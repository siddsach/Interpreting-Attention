Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'anneal': 7.732698473565949, 'lr': 6.371976747761653, 'dropout': 0.35353854866768, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.516284465789795 and batch: 50, loss is 7.373254470825195 and perplexity is 1592.8091077762315
At time: 3.3588836193084717 and batch: 100, loss is 6.623839788436889 and perplexity is 752.8302632930443
At time: 4.200726747512817 and batch: 150, loss is 6.244241638183594 and perplexity is 515.0384912610795
At time: 5.076269865036011 and batch: 200, loss is 6.016874513626099 and perplexity is 410.29422062791997
At time: 5.92181134223938 and batch: 250, loss is 5.894843854904175 and perplexity is 363.16012579887035
At time: 6.773074626922607 and batch: 300, loss is 5.820147609710693 and perplexity is 337.021797648679
At time: 7.617932081222534 and batch: 350, loss is 5.767832727432251 and perplexity is 319.8437922630644
At time: 8.465855121612549 and batch: 400, loss is 5.683536739349365 and perplexity is 293.987350043294
At time: 9.314467906951904 and batch: 450, loss is 5.6142965507507325 and perplexity is 274.3203408691651
At time: 10.162534952163696 and batch: 500, loss is 5.581289405822754 and perplexity is 265.4136111026137
At time: 11.009756088256836 and batch: 550, loss is 5.558584251403809 and perplexity is 259.45525277151364
At time: 11.860914468765259 and batch: 600, loss is 5.552326860427857 and perplexity is 257.83680871361616
At time: 12.711745977401733 and batch: 650, loss is 5.502320632934571 and perplexity is 245.2604318053055
At time: 13.560602903366089 and batch: 700, loss is 5.496830263137817 and perplexity is 243.9175511671636
At time: 14.414106607437134 and batch: 750, loss is 5.460697278976441 and perplexity is 235.2614100218951
At time: 15.261946201324463 and batch: 800, loss is 5.461350717544556 and perplexity is 235.41518913792348
At time: 16.11236047744751 and batch: 850, loss is 5.500302038192749 and perplexity is 244.76584973560952
At time: 16.962650537490845 and batch: 900, loss is 5.465000305175781 and perplexity is 236.2759272145405
At time: 17.812411069869995 and batch: 950, loss is 5.426208753585815 and perplexity is 227.2859130024606
At time: 18.670621871948242 and batch: 1000, loss is 5.41173092842102 and perplexity is 224.019013157654
At time: 19.518041610717773 and batch: 1050, loss is 5.3847236156463625 and perplexity is 218.04983021708796
At time: 20.37007761001587 and batch: 1100, loss is 5.35924880027771 and perplexity is 212.5652075342758
At time: 21.218279361724854 and batch: 1150, loss is 5.386334753036499 and perplexity is 218.40142160642625
At time: 22.070077896118164 and batch: 1200, loss is 5.361076803207397 and perplexity is 212.95413272629077
At time: 22.918956518173218 and batch: 1250, loss is 5.345756359100342 and perplexity is 209.71644556889262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.873779742387089 and perplexity of 130.8144284757501
Finished 1 epochs...
Completing Train Step...
At time: 25.38607907295227 and batch: 50, loss is 5.0871970748901365 and perplexity is 161.9353327349521
At time: 26.261900901794434 and batch: 100, loss is 5.073459854125977 and perplexity is 159.7260011015481
At time: 27.105252504348755 and batch: 150, loss is 4.939979963302612 and perplexity is 139.76744905386522
At time: 27.955446243286133 and batch: 200, loss is 4.9651217269897465 and perplexity is 143.32599585355106
At time: 28.795603036880493 and batch: 250, loss is 4.949926795959473 and perplexity is 141.16462972195373
At time: 29.695395708084106 and batch: 300, loss is 4.938499002456665 and perplexity is 139.56061213077456
At time: 30.54427433013916 and batch: 350, loss is 4.921925354003906 and perplexity is 137.26664581288657
At time: 31.392603874206543 and batch: 400, loss is 4.901379375457764 and perplexity is 134.47514352503669
At time: 32.24652123451233 and batch: 450, loss is 4.83278190612793 and perplexity is 125.55977074700775
At time: 33.09039568901062 and batch: 500, loss is 4.831933364868164 and perplexity is 125.45327329109497
At time: 33.93838977813721 and batch: 550, loss is 4.822453355789184 and perplexity is 124.26959462835225
At time: 34.783369064331055 and batch: 600, loss is 4.829947462081909 and perplexity is 125.20438250437296
At time: 35.63004779815674 and batch: 650, loss is 4.807004623413086 and perplexity is 122.36454014562221
At time: 36.48308086395264 and batch: 700, loss is 4.794491376876831 and perplexity is 120.84290265496546
At time: 37.32824373245239 and batch: 750, loss is 4.787415800094604 and perplexity is 119.99088722353572
At time: 38.19645833969116 and batch: 800, loss is 4.802911882400513 and perplexity is 121.86475721237325
At time: 39.05953907966614 and batch: 850, loss is 4.849175491333008 and perplexity is 127.63511019543226
At time: 39.900511503219604 and batch: 900, loss is 4.815365095138549 and perplexity is 123.39185385343244
At time: 40.74718236923218 and batch: 950, loss is 4.788475542068482 and perplexity is 120.11811400508275
At time: 41.620609998703 and batch: 1000, loss is 4.772109336853028 and perplexity is 118.16823587712071
At time: 42.490445137023926 and batch: 1050, loss is 4.7426924228668215 and perplexity is 114.74272208883477
At time: 43.33730459213257 and batch: 1100, loss is 4.722775011062622 and perplexity is 112.47995309587988
At time: 44.185054540634155 and batch: 1150, loss is 4.732191429138184 and perplexity is 113.54411378718052
At time: 45.02894949913025 and batch: 1200, loss is 4.722674999237061 and perplexity is 112.4687043329457
At time: 45.878986120224 and batch: 1250, loss is 4.71954496383667 and perplexity is 112.1172236673686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.611091112568431 and perplexity of 100.59384899127483
Finished 2 epochs...
Completing Train Step...
At time: 48.457919120788574 and batch: 50, loss is 4.649702835083008 and perplexity is 104.55391120589167
At time: 49.306246280670166 and batch: 100, loss is 4.658893013000489 and perplexity is 105.51920908745174
At time: 50.188554763793945 and batch: 150, loss is 4.556378526687622 and perplexity is 95.23795284476955
At time: 51.0347626209259 and batch: 200, loss is 4.608180675506592 and perplexity is 100.30150255928636
At time: 51.879212379455566 and batch: 250, loss is 4.610414981842041 and perplexity is 100.5258573872825
At time: 52.7243766784668 and batch: 300, loss is 4.610132827758789 and perplexity is 100.49749760725025
At time: 53.5725793838501 and batch: 350, loss is 4.600106658935547 and perplexity is 99.49492709903713
At time: 54.41957497596741 and batch: 400, loss is 4.592731790542603 and perplexity is 98.76386416583532
At time: 55.26619076728821 and batch: 450, loss is 4.519308853149414 and perplexity is 91.77214802298657
At time: 56.11211919784546 and batch: 500, loss is 4.536618003845215 and perplexity is 93.37447340310968
At time: 56.958587884902954 and batch: 550, loss is 4.531749992370606 and perplexity is 92.92102997451182
At time: 57.80506944656372 and batch: 600, loss is 4.545877265930176 and perplexity is 94.24306718748707
At time: 58.653098583221436 and batch: 650, loss is 4.544212779998779 and perplexity is 94.08633140645965
At time: 59.50025415420532 and batch: 700, loss is 4.52831922531128 and perplexity is 92.60278578874006
At time: 60.354044675827026 and batch: 750, loss is 4.523839807510376 and perplexity is 92.18890688198199
At time: 61.199485540390015 and batch: 800, loss is 4.550885524749756 and perplexity is 94.71624476876494
At time: 62.04515624046326 and batch: 850, loss is 4.596685085296631 and perplexity is 99.15507961741552
At time: 62.89100885391235 and batch: 900, loss is 4.562481021881103 and perplexity is 95.8209189590381
At time: 63.739553451538086 and batch: 950, loss is 4.54759708404541 and perplexity is 94.40528757646894
At time: 64.5867919921875 and batch: 1000, loss is 4.533841409683228 and perplexity is 93.11556998657153
At time: 65.43292951583862 and batch: 1050, loss is 4.510549697875977 and perplexity is 90.97181178038299
At time: 66.27934384346008 and batch: 1100, loss is 4.490189151763916 and perplexity is 89.1383049517608
At time: 67.13070869445801 and batch: 1150, loss is 4.493238258361816 and perplexity is 89.410511928666
At time: 67.97885036468506 and batch: 1200, loss is 4.496623725891113 and perplexity is 89.71372127667944
At time: 68.82656359672546 and batch: 1250, loss is 4.510235233306885 and perplexity is 90.94320886632919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.534896070939781 and perplexity of 93.21382717553574
Finished 3 epochs...
Completing Train Step...
At time: 71.39606380462646 and batch: 50, loss is 4.4491560173034665 and perplexity is 85.55470683085493
At time: 72.24164986610413 and batch: 100, loss is 4.45815354347229 and perplexity is 86.32796101522128
At time: 73.08908319473267 and batch: 150, loss is 4.370144281387329 and perplexity is 79.0550370471755
At time: 73.94065070152283 and batch: 200, loss is 4.427312316894532 and perplexity is 83.70613873280757
At time: 74.79417896270752 and batch: 250, loss is 4.434456014633179 and perplexity is 84.30625104517209
At time: 75.6432101726532 and batch: 300, loss is 4.434529457092285 and perplexity is 84.31244293093776
At time: 76.48735904693604 and batch: 350, loss is 4.422639722824097 and perplexity is 83.31592628744802
At time: 77.33116173744202 and batch: 400, loss is 4.42194130897522 and perplexity is 83.2577576059677
At time: 78.17785143852234 and batch: 450, loss is 4.347666940689087 and perplexity is 77.29791177610514
At time: 79.02496910095215 and batch: 500, loss is 4.374787044525147 and perplexity is 79.42292420467908
At time: 79.87651133537292 and batch: 550, loss is 4.369160585403442 and perplexity is 78.97730916129858
At time: 80.72474026679993 and batch: 600, loss is 4.39315691947937 and perplexity is 80.89539655004732
At time: 81.57934236526489 and batch: 650, loss is 4.401763706207276 and perplexity is 81.59465082509323
At time: 82.42598271369934 and batch: 700, loss is 4.378755264282226 and perplexity is 79.73871797682185
At time: 83.27378296852112 and batch: 750, loss is 4.371143865585327 and perplexity is 79.1340987207968
At time: 84.12048745155334 and batch: 800, loss is 4.408852310180664 and perplexity is 82.17509784010979
At time: 84.9685959815979 and batch: 850, loss is 4.457431354522705 and perplexity is 86.26563842278843
At time: 85.81526684761047 and batch: 900, loss is 4.417482194900512 and perplexity is 82.88732827431345
At time: 86.66650581359863 and batch: 950, loss is 4.408918809890747 and perplexity is 82.18056264199409
At time: 87.51702427864075 and batch: 1000, loss is 4.39398302078247 and perplexity is 80.96225195341795
At time: 88.36494779586792 and batch: 1050, loss is 4.373059597015381 and perplexity is 79.28584370581477
At time: 89.21189284324646 and batch: 1100, loss is 4.352664880752563 and perplexity is 77.68520914450652
At time: 90.0582628250122 and batch: 1150, loss is 4.354994945526123 and perplexity is 77.86643176198815
At time: 90.90598368644714 and batch: 1200, loss is 4.355967130661011 and perplexity is 77.94216915887439
At time: 91.75431776046753 and batch: 1250, loss is 4.37880497932434 and perplexity is 79.74268228908635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.502019060789234 and perplexity of 90.19906496644958
Finished 4 epochs...
Completing Train Step...
At time: 94.25986814498901 and batch: 50, loss is 4.321359395980835 and perplexity is 75.29090891531101
At time: 95.13757348060608 and batch: 100, loss is 4.333078393936157 and perplexity is 76.17843321334475
At time: 95.98318195343018 and batch: 150, loss is 4.253266921043396 and perplexity is 70.33481571084732
At time: 96.83172965049744 and batch: 200, loss is 4.307198276519776 and perplexity is 74.23221916799474
At time: 97.68220138549805 and batch: 250, loss is 4.320659847259521 and perplexity is 75.23825767445402
At time: 98.53218030929565 and batch: 300, loss is 4.319337377548218 and perplexity is 75.13882312163246
At time: 99.38158345222473 and batch: 350, loss is 4.308751411437989 and perplexity is 74.34760139853256
At time: 100.23210883140564 and batch: 400, loss is 4.312162551879883 and perplexity is 74.60164455052124
At time: 101.08288192749023 and batch: 450, loss is 4.2342118072509765 and perplexity is 69.0072662656783
At time: 101.93119502067566 and batch: 500, loss is 4.267854766845703 and perplexity is 71.36836948980834
At time: 102.77905821800232 and batch: 550, loss is 4.263588547706604 and perplexity is 71.06454493784015
At time: 103.62507319450378 and batch: 600, loss is 4.287955083847046 and perplexity is 72.81741064692197
At time: 104.47175788879395 and batch: 650, loss is 4.30395263671875 and perplexity is 73.99167868798048
At time: 105.31823658943176 and batch: 700, loss is 4.275850009918213 and perplexity is 71.94126411566016
At time: 106.16591191291809 and batch: 750, loss is 4.2649063205719 and perplexity is 71.15825359664711
At time: 107.01336002349854 and batch: 800, loss is 4.3044541072845455 and perplexity is 74.02879264195627
At time: 107.86595487594604 and batch: 850, loss is 4.355757055282592 and perplexity is 77.92579714793202
At time: 108.71355032920837 and batch: 900, loss is 4.308564782142639 and perplexity is 74.33372725277393
At time: 109.55987858772278 and batch: 950, loss is 4.304678726196289 and perplexity is 74.04542277644862
At time: 110.40536403656006 and batch: 1000, loss is 4.294781036376953 and perplexity is 73.31615911549385
At time: 111.25655388832092 and batch: 1050, loss is 4.278488845825195 and perplexity is 72.1313560069484
At time: 112.10210680961609 and batch: 1100, loss is 4.248962459564209 and perplexity is 70.03271286745671
At time: 112.94976162910461 and batch: 1150, loss is 4.259390487670898 and perplexity is 70.7668370468071
At time: 113.8134093284607 and batch: 1200, loss is 4.255741529464721 and perplexity is 70.50908237005943
At time: 114.71068739891052 and batch: 1250, loss is 4.282700605392456 and perplexity is 72.43579660096535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.48640430756729 and perplexity of 88.80156800598546
Finished 5 epochs...
Completing Train Step...
At time: 117.3132848739624 and batch: 50, loss is 4.2268883419036865 and perplexity is 68.50373997162622
At time: 118.18815112113953 and batch: 100, loss is 4.236679744720459 and perplexity is 69.17778220855722
At time: 119.07335638999939 and batch: 150, loss is 4.166748046875 and perplexity is 64.50534230945631
At time: 119.94325590133667 and batch: 200, loss is 4.2210509395599365 and perplexity is 68.10502095323054
At time: 120.80265593528748 and batch: 250, loss is 4.233113451004028 and perplexity is 68.93151331317048
At time: 121.68890976905823 and batch: 300, loss is 4.232494106292725 and perplexity is 68.8888341628751
At time: 122.53952980041504 and batch: 350, loss is 4.2251998233795165 and perplexity is 68.38816773809675
At time: 123.4058244228363 and batch: 400, loss is 4.23009428024292 and perplexity is 68.72371115660383
At time: 124.27425980567932 and batch: 450, loss is 4.150116186141968 and perplexity is 63.441370878058905
At time: 125.13666534423828 and batch: 500, loss is 4.189703192710876 and perplexity is 66.00319782264147
At time: 125.98618626594543 and batch: 550, loss is 4.1852485466003415 and perplexity is 65.70983084234476
At time: 126.84024834632874 and batch: 600, loss is 4.212863774299621 and perplexity is 67.5497102039217
At time: 127.6923303604126 and batch: 650, loss is 4.226916437149048 and perplexity is 68.50566462804561
At time: 128.54329228401184 and batch: 700, loss is 4.200944967269898 and perplexity is 66.74937722470698
At time: 129.3955361843109 and batch: 750, loss is 4.187061328887939 and perplexity is 65.82905649224023
At time: 130.24459075927734 and batch: 800, loss is 4.2273414516448975 and perplexity is 68.5347867167769
At time: 131.0954132080078 and batch: 850, loss is 4.278939123153687 and perplexity is 72.16384243463374
At time: 131.94296407699585 and batch: 900, loss is 4.234345407485962 and perplexity is 69.01648626855064
At time: 132.79603719711304 and batch: 950, loss is 4.227352170944214 and perplexity is 68.53552136560678
At time: 133.64504885673523 and batch: 1000, loss is 4.221490926742554 and perplexity is 68.13499288267073
At time: 134.49117922782898 and batch: 1050, loss is 4.203790192604065 and perplexity is 66.93956467860548
At time: 135.34064960479736 and batch: 1100, loss is 4.176284132003784 and perplexity is 65.12341304879327
At time: 136.19143962860107 and batch: 1150, loss is 4.181765313148499 and perplexity is 65.48134632491217
At time: 137.0736563205719 and batch: 1200, loss is 4.1813638877868655 and perplexity is 65.45506572698564
At time: 137.92379927635193 and batch: 1250, loss is 4.213203296661377 and perplexity is 67.57264873491783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.489455174355611 and perplexity of 89.07290345425854
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 140.39128184318542 and batch: 50, loss is 4.192339453697205 and perplexity is 66.17742903660316
At time: 141.26964163780212 and batch: 100, loss is 4.2099776935577395 and perplexity is 67.35503734210786
At time: 142.1191918849945 and batch: 150, loss is 4.1334867095947265 and perplexity is 62.39509768786661
At time: 142.97038578987122 and batch: 200, loss is 4.187208261489868 and perplexity is 65.83872963742596
At time: 143.82335424423218 and batch: 250, loss is 4.192526865005493 and perplexity is 66.18983259740554
At time: 144.67147612571716 and batch: 300, loss is 4.178194808959961 and perplexity is 65.24796180173857
At time: 145.52556943893433 and batch: 350, loss is 4.16022038936615 and perplexity is 64.0856448383291
At time: 146.37626552581787 and batch: 400, loss is 4.152114896774292 and perplexity is 63.56829862422528
At time: 147.2239146232605 and batch: 450, loss is 4.063930473327637 and perplexity is 58.20262596119162
At time: 148.07405591011047 and batch: 500, loss is 4.088590440750122 and perplexity is 59.65574410665154
At time: 148.92129969596863 and batch: 550, loss is 4.069383325576783 and perplexity is 58.520863142708166
At time: 149.77014756202698 and batch: 600, loss is 4.082849230766296 and perplexity is 59.31422924532905
At time: 150.61954355239868 and batch: 650, loss is 4.0888089847564695 and perplexity is 59.66878293669745
At time: 151.47546315193176 and batch: 700, loss is 4.051928644180298 and perplexity is 57.508263135354184
At time: 152.32386994361877 and batch: 750, loss is 4.030768218040467 and perplexity is 56.30414849981334
At time: 153.17296147346497 and batch: 800, loss is 4.051459689140319 and perplexity is 57.481300668103046
At time: 154.02523136138916 and batch: 850, loss is 4.084483170509339 and perplexity is 59.41122434231339
At time: 154.87276887893677 and batch: 900, loss is 4.028655471801758 and perplexity is 56.18531769571592
At time: 155.72350883483887 and batch: 950, loss is 4.002816672325134 and perplexity is 54.75215191595048
At time: 156.57346105575562 and batch: 1000, loss is 3.9749089431762696 and perplexity is 53.245268339977756
At time: 157.42545771598816 and batch: 1050, loss is 3.9450613403320314 and perplexity is 51.67950805306662
At time: 158.27803897857666 and batch: 1100, loss is 3.8905841445922853 and perplexity is 48.939465900029994
At time: 159.1834020614624 and batch: 1150, loss is 3.875153970718384 and perplexity is 48.190117586930874
At time: 160.03585815429688 and batch: 1200, loss is 3.8586444950103758 and perplexity is 47.40105543481322
At time: 160.88504815101624 and batch: 1250, loss is 3.884587960243225 and perplexity is 48.64689387504701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3570556640625 and perplexity of 78.02705800717966
Finished 7 epochs...
Completing Train Step...
At time: 163.39763760566711 and batch: 50, loss is 4.080072565078735 and perplexity is 59.1497619012834
At time: 164.24979901313782 and batch: 100, loss is 4.096828622817993 and perplexity is 60.14922890584048
At time: 165.09727597236633 and batch: 150, loss is 4.020189700126648 and perplexity is 55.71167333592258
At time: 165.94492864608765 and batch: 200, loss is 4.077257041931152 and perplexity is 58.983458602729144
At time: 166.79615211486816 and batch: 250, loss is 4.087982406616211 and perplexity is 59.61948240324475
At time: 167.64642643928528 and batch: 300, loss is 4.081741433143616 and perplexity is 59.24855746541693
At time: 168.49750900268555 and batch: 350, loss is 4.068437347412109 and perplexity is 58.46552986016221
At time: 169.34639954566956 and batch: 400, loss is 4.066773014068604 and perplexity is 58.36830465938483
At time: 170.19492888450623 and batch: 450, loss is 3.9819712352752688 and perplexity is 53.62263293894633
At time: 171.0428819656372 and batch: 500, loss is 4.0121154832839965 and perplexity is 55.26365633192205
At time: 171.88896703720093 and batch: 550, loss is 3.994220142364502 and perplexity is 54.28349071779269
At time: 172.74069213867188 and batch: 600, loss is 4.0130540466308595 and perplexity is 55.31554912269515
At time: 173.58652138710022 and batch: 650, loss is 4.02605926990509 and perplexity is 56.03963845553562
At time: 174.43227791786194 and batch: 700, loss is 3.9931404399871826 and perplexity is 54.22491233311678
At time: 175.2856948375702 and batch: 750, loss is 3.975859832763672 and perplexity is 53.29592279079883
At time: 176.13445806503296 and batch: 800, loss is 4.004023976325989 and perplexity is 54.81829432697873
At time: 176.98233771324158 and batch: 850, loss is 4.041151618957519 and perplexity is 56.891822795892665
At time: 177.82798171043396 and batch: 900, loss is 3.989660906791687 and perplexity is 54.03656282484533
At time: 178.6785113811493 and batch: 950, loss is 3.970632677078247 and perplexity is 53.01806354440838
At time: 179.5281310081482 and batch: 1000, loss is 3.9490328884124755 and perplexity is 51.88516381963492
At time: 180.37698411941528 and batch: 1050, loss is 3.9285027408599853 and perplexity is 50.830813764375606
At time: 181.28776025772095 and batch: 1100, loss is 3.8821063947677614 and perplexity is 48.52632308674553
At time: 182.1392421722412 and batch: 1150, loss is 3.8751713705062865 and perplexity is 48.19095609205076
At time: 182.9895350933075 and batch: 1200, loss is 3.8691774225234985 and perplexity is 47.90296596913018
At time: 183.84472846984863 and batch: 1250, loss is 3.9015997219085694 and perplexity is 49.48154253257791
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.347205837277601 and perplexity of 77.26227766142172
Finished 8 epochs...
Completing Train Step...
At time: 186.30772495269775 and batch: 50, loss is 4.0429921722412105 and perplexity is 56.996631650707926
At time: 187.19028854370117 and batch: 100, loss is 4.053928532600403 and perplexity is 57.623388325255235
At time: 188.0400366783142 and batch: 150, loss is 3.977076001167297 and perplexity is 53.3607790382046
At time: 188.88439273834229 and batch: 200, loss is 4.034030313491821 and perplexity is 56.48811790627004
At time: 189.74211621284485 and batch: 250, loss is 4.045274391174316 and perplexity is 57.12685898970941
At time: 190.58993124961853 and batch: 300, loss is 4.041183972358704 and perplexity is 56.893663469635584
At time: 191.43990850448608 and batch: 350, loss is 4.0281829214096065 and perplexity is 56.15877357401716
At time: 192.2959086894989 and batch: 400, loss is 4.028489441871643 and perplexity is 56.17599002570302
At time: 193.14578795433044 and batch: 450, loss is 3.944564118385315 and perplexity is 51.653818254762164
At time: 193.99565625190735 and batch: 500, loss is 3.9770040273666383 and perplexity is 53.35693859833828
At time: 194.84582924842834 and batch: 550, loss is 3.960239624977112 and perplexity is 52.469897540802485
At time: 195.70310521125793 and batch: 600, loss is 3.9814258146286012 and perplexity is 53.59339402229202
At time: 196.5514714717865 and batch: 650, loss is 3.9970693302154543 and perplexity is 54.43837512265781
At time: 197.43742895126343 and batch: 700, loss is 3.964626989364624 and perplexity is 52.700607835677424
At time: 198.32602405548096 and batch: 750, loss is 3.9495066022872924 and perplexity is 51.909748364193625
At time: 199.20198678970337 and batch: 800, loss is 3.980285530090332 and perplexity is 53.53231713286928
At time: 200.0871307849884 and batch: 850, loss is 4.019110054969787 and perplexity is 55.65155695564376
At time: 200.9728283882141 and batch: 900, loss is 3.9692776823043823 and perplexity is 52.94627299427942
At time: 201.86699271202087 and batch: 950, loss is 3.954257230758667 and perplexity is 52.15693898321806
At time: 202.791250705719 and batch: 1000, loss is 3.9358815574645996 and perplexity is 51.20727221786751
At time: 203.673184633255 and batch: 1050, loss is 3.919490122795105 and perplexity is 50.374753290465115
At time: 204.52813291549683 and batch: 1100, loss is 3.87604043006897 and perplexity is 48.23285510701016
At time: 205.3819191455841 and batch: 1150, loss is 3.872221450805664 and perplexity is 48.04900611473797
At time: 206.22884154319763 and batch: 1200, loss is 3.8708910655975344 and perplexity is 47.98512493045099
At time: 207.07750058174133 and batch: 1250, loss is 3.904896464347839 and perplexity is 49.64493962487969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.343417647981296 and perplexity of 76.97014720026807
Finished 9 epochs...
Completing Train Step...
At time: 209.5345528125763 and batch: 50, loss is 4.015302419662476 and perplexity is 55.44005903148052
At time: 210.412371635437 and batch: 100, loss is 4.0237240219116215 and perplexity is 55.908924686214064
At time: 211.263445854187 and batch: 150, loss is 3.947587332725525 and perplexity is 51.81021511032483
At time: 212.116614818573 and batch: 200, loss is 4.004131865501404 and perplexity is 54.8242089466073
At time: 212.9647662639618 and batch: 250, loss is 4.01686776638031 and perplexity is 55.52690990405185
At time: 213.81353330612183 and batch: 300, loss is 4.013472294807434 and perplexity is 55.338689589143534
At time: 214.65964722633362 and batch: 350, loss is 4.000411930084229 and perplexity is 54.62064528660387
At time: 215.50896906852722 and batch: 400, loss is 4.002279357910156 and perplexity is 54.72274069771833
At time: 216.35904669761658 and batch: 450, loss is 3.9193166303634643 and perplexity is 50.36601441011018
At time: 217.2048683166504 and batch: 500, loss is 3.953041048049927 and perplexity is 52.09354517292764
At time: 218.055025100708 and batch: 550, loss is 3.937450385093689 and perplexity is 51.287670650484564
At time: 218.91313195228577 and batch: 600, loss is 3.9597411251068113 and perplexity is 52.44374782204094
At time: 219.76066064834595 and batch: 650, loss is 3.9771801710128782 and perplexity is 53.36633791184557
At time: 220.6095254421234 and batch: 700, loss is 3.945129437446594 and perplexity is 51.68302739827433
At time: 221.45697712898254 and batch: 750, loss is 3.9309642124176025 and perplexity is 50.95608648107107
At time: 222.30790662765503 and batch: 800, loss is 3.9632581996917726 and perplexity is 52.62852113493977
At time: 223.15789461135864 and batch: 850, loss is 4.003038992881775 and perplexity is 54.764325798043686
At time: 224.00747418403625 and batch: 900, loss is 3.954248285293579 and perplexity is 52.15647241722813
At time: 224.91305494308472 and batch: 950, loss is 3.9416844129562376 and perplexity is 51.505284443358455
At time: 225.76366186141968 and batch: 1000, loss is 3.924776315689087 and perplexity is 50.64174902708798
At time: 226.61345624923706 and batch: 1050, loss is 3.9108490705490113 and perplexity is 49.941337695642844
At time: 227.46271634101868 and batch: 1100, loss is 3.8693692255020142 and perplexity is 47.91215478187545
At time: 228.3131709098816 and batch: 1150, loss is 3.8670455741882326 and perplexity is 47.800952887719035
At time: 229.16836380958557 and batch: 1200, loss is 3.868522062301636 and perplexity is 47.87158255556846
At time: 230.0185890197754 and batch: 1250, loss is 3.9027978706359865 and perplexity is 49.54086431085386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.341656538691834 and perplexity of 76.83471365066757
Finished 10 epochs...
Completing Train Step...
At time: 232.553151845932 and batch: 50, loss is 3.99265851020813 and perplexity is 54.198786029124555
At time: 233.40145564079285 and batch: 100, loss is 3.999855647087097 and perplexity is 54.590269199972134
At time: 234.24989438056946 and batch: 150, loss is 3.9246027088165283 and perplexity is 50.63295803452893
At time: 235.1001832485199 and batch: 200, loss is 3.981149320602417 and perplexity is 53.57857781739237
At time: 235.95696568489075 and batch: 250, loss is 3.9946089601516723 and perplexity is 54.304601208334624
At time: 236.80673480033875 and batch: 300, loss is 3.9916500282287597 and perplexity is 54.14415508190346
At time: 237.65752005577087 and batch: 350, loss is 3.9786435890197756 and perplexity is 53.444492344061736
At time: 238.50582599639893 and batch: 400, loss is 3.981612620353699 and perplexity is 53.603406510288714
At time: 239.36159372329712 and batch: 450, loss is 3.899401044845581 and perplexity is 49.3728681137256
At time: 240.21240782737732 and batch: 500, loss is 3.9341868591308593 and perplexity is 51.120564831148386
At time: 241.0616753101349 and batch: 550, loss is 3.9194345903396606 and perplexity is 50.37195593439524
At time: 241.90717887878418 and batch: 600, loss is 3.942442297935486 and perplexity is 51.54433432057852
At time: 242.75346326828003 and batch: 650, loss is 3.961135196685791 and perplexity is 52.51690914458634
At time: 243.60904479026794 and batch: 700, loss is 3.9294051933288574 and perplexity is 50.876706862807346
At time: 244.4553451538086 and batch: 750, loss is 3.9160973691940306 and perplexity is 50.20413376350649
At time: 245.307923078537 and batch: 800, loss is 3.9490610265731814 and perplexity is 51.88662379325312
At time: 246.15608382225037 and batch: 850, loss is 3.9895904541015623 and perplexity is 54.03275593773353
At time: 247.05802965164185 and batch: 900, loss is 3.9414402055740356 and perplexity is 51.492708008366606
At time: 247.90845656394958 and batch: 950, loss is 3.930641140937805 and perplexity is 50.939626681795964
At time: 248.75993967056274 and batch: 1000, loss is 3.914609975814819 and perplexity is 50.12951597409723
At time: 249.61122679710388 and batch: 1050, loss is 3.902602934837341 and perplexity is 49.53120796411826
At time: 250.46246099472046 and batch: 1100, loss is 3.862084245681763 and perplexity is 47.56438399078425
At time: 251.31146216392517 and batch: 1150, loss is 3.860868773460388 and perplexity is 47.50660592424741
At time: 252.16028666496277 and batch: 1200, loss is 3.864298930168152 and perplexity is 47.669840827819826
At time: 253.010094165802 and batch: 1250, loss is 3.898400321006775 and perplexity is 49.323484221554324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340900059164006 and perplexity of 76.77661174198732
Finished 11 epochs...
Completing Train Step...
At time: 255.3556387424469 and batch: 50, loss is 3.9733303022384643 and perplexity is 53.16127949119178
At time: 256.2328085899353 and batch: 100, loss is 3.9797141551971436 and perplexity is 53.50173884754952
At time: 257.08717226982117 and batch: 150, loss is 3.9052879428863525 and perplexity is 49.66437835796377
At time: 257.9335434436798 and batch: 200, loss is 3.9617105388641356 and perplexity is 52.54713303119818
At time: 258.7805814743042 and batch: 250, loss is 3.9760592365264893 and perplexity is 53.3065512579891
At time: 259.6305663585663 and batch: 300, loss is 3.9734221839904786 and perplexity is 53.1661642670982
At time: 260.4826374053955 and batch: 350, loss is 3.9604851675033568 and perplexity is 52.4827827138607
At time: 261.3314096927643 and batch: 400, loss is 3.9642172718048094 and perplexity is 52.67901989401563
At time: 262.1796205043793 and batch: 450, loss is 3.88268150806427 and perplexity is 48.55423924709144
At time: 263.0275011062622 and batch: 500, loss is 3.918211030960083 and perplexity is 50.31036054573632
At time: 263.8770971298218 and batch: 550, loss is 3.9042166233062745 and perplexity is 49.611200427361766
At time: 264.72321581840515 and batch: 600, loss is 3.927797303199768 and perplexity is 50.79496843885452
At time: 265.5730814933777 and batch: 650, loss is 3.947445158958435 and perplexity is 51.80284958047383
At time: 266.41871643066406 and batch: 700, loss is 3.9157415342330935 and perplexity is 50.18627255553945
At time: 267.2677183151245 and batch: 750, loss is 3.9032089853286744 and perplexity is 49.56123547521609
At time: 268.1772630214691 and batch: 800, loss is 3.936603002548218 and perplexity is 51.24422878211745
At time: 269.02877974510193 and batch: 850, loss is 3.9776851749420166 and perplexity is 53.39329492830149
At time: 269.87488770484924 and batch: 900, loss is 3.9298961925506593 and perplexity is 50.90169341997291
At time: 270.72682881355286 and batch: 950, loss is 3.920394458770752 and perplexity is 50.42032959717081
At time: 271.5754814147949 and batch: 1000, loss is 3.905034804344177 and perplexity is 49.6518079807188
At time: 272.4304928779602 and batch: 1050, loss is 3.894607672691345 and perplexity is 49.13677188313681
At time: 273.2777268886566 and batch: 1100, loss is 3.854461178779602 and perplexity is 47.20317601492773
At time: 274.12503266334534 and batch: 1150, loss is 3.854157819747925 and perplexity is 47.18885867691645
At time: 274.9985387325287 and batch: 1200, loss is 3.859011516571045 and perplexity is 47.41845583712235
At time: 275.8773114681244 and batch: 1250, loss is 3.892959785461426 and perplexity is 49.055866703845425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340720517791971 and perplexity of 76.76282840115199
Finished 12 epochs...
Completing Train Step...
At time: 278.3719322681427 and batch: 50, loss is 3.956343832015991 and perplexity is 52.26588333986663
At time: 279.22147583961487 and batch: 100, loss is 3.9621187353134157 and perplexity is 52.56858696273281
At time: 280.07403445243835 and batch: 150, loss is 3.8884470224380494 and perplexity is 48.83498796405946
At time: 280.92262387275696 and batch: 200, loss is 3.944737014770508 and perplexity is 51.66274978531224
At time: 281.76956057548523 and batch: 250, loss is 3.9598985385894774 and perplexity is 52.45200382481569
At time: 282.6208782196045 and batch: 300, loss is 3.9574513721466062 and perplexity is 52.323801970811154
At time: 283.4784924983978 and batch: 350, loss is 3.9443667936325073 and perplexity is 51.64362668340119
At time: 284.3292336463928 and batch: 400, loss is 3.948926043510437 and perplexity is 51.87962045053501
At time: 285.1777913570404 and batch: 450, loss is 3.867891855239868 and perplexity is 47.84142305054864
At time: 286.0270218849182 and batch: 500, loss is 3.904035267829895 and perplexity is 49.602203980276634
At time: 286.87538266181946 and batch: 550, loss is 3.8907697868347166 and perplexity is 48.94855197557669
At time: 287.7251658439636 and batch: 600, loss is 3.914805917739868 and perplexity is 50.13933941033903
At time: 288.5731735229492 and batch: 650, loss is 3.935034155845642 and perplexity is 51.16389747299778
At time: 289.420978307724 and batch: 700, loss is 3.9035281801223753 and perplexity is 49.57705768859889
At time: 290.2978446483612 and batch: 750, loss is 3.891604743003845 and perplexity is 48.98943893805402
At time: 291.15103244781494 and batch: 800, loss is 3.925296874046326 and perplexity is 50.66811787543597
At time: 291.9975368976593 and batch: 850, loss is 3.966839699745178 and perplexity is 52.81734812629249
At time: 292.84372425079346 and batch: 900, loss is 3.91925546169281 and perplexity is 50.36293368218553
At time: 293.69078159332275 and batch: 950, loss is 3.910699486732483 and perplexity is 49.933867838446595
At time: 294.5384590625763 and batch: 1000, loss is 3.895824613571167 and perplexity is 49.19660482873851
At time: 295.3835942745209 and batch: 1050, loss is 3.8867407846450805 and perplexity is 48.75173490692975
At time: 296.23057103157043 and batch: 1100, loss is 3.8468440055847166 and perplexity is 46.84498717297717
At time: 297.07718420028687 and batch: 1150, loss is 3.847238607406616 and perplexity is 46.86347593787273
At time: 297.92402243614197 and batch: 1200, loss is 3.853184251785278 and perplexity is 47.142939472274975
At time: 298.77521657943726 and batch: 1250, loss is 3.8868608379364016 and perplexity is 48.75758806450132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340936591155337 and perplexity of 76.77941659573507
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 301.22891902923584 and batch: 50, loss is 3.9588967514038087 and perplexity is 52.399484390561746
At time: 302.10337138175964 and batch: 100, loss is 3.9750907897949217 and perplexity is 53.25495169240037
At time: 302.9497401714325 and batch: 150, loss is 3.902932114601135 and perplexity is 49.54751531933473
At time: 303.79403424263 and batch: 200, loss is 3.9633314180374146 and perplexity is 52.6323746492631
At time: 304.6398675441742 and batch: 250, loss is 3.9795906066894533 and perplexity is 53.49512919587078
At time: 305.48295402526855 and batch: 300, loss is 3.972863402366638 and perplexity is 53.13646429016911
At time: 306.3310525417328 and batch: 350, loss is 3.955375380516052 and perplexity is 52.21529086888192
At time: 307.17650532722473 and batch: 400, loss is 3.9541329622268675 and perplexity is 52.15045791969193
At time: 308.02240538597107 and batch: 450, loss is 3.873594937324524 and perplexity is 48.115046119027134
At time: 308.86187720298767 and batch: 500, loss is 3.9045400524139406 and perplexity is 49.62724872875023
At time: 309.6999566555023 and batch: 550, loss is 3.8889485931396486 and perplexity is 48.85948830704865
At time: 310.5489192008972 and batch: 600, loss is 3.9074602842330934 and perplexity is 49.772383610194844
At time: 311.3934543132782 and batch: 650, loss is 3.9272943687438966 and perplexity is 50.76942832208243
At time: 312.298730134964 and batch: 700, loss is 3.891295900344849 and perplexity is 48.97431124562827
At time: 313.1457681655884 and batch: 750, loss is 3.8772263383865355 and perplexity is 48.290088781299005
At time: 313.9887185096741 and batch: 800, loss is 3.9033867835998537 and perplexity is 49.570048160617986
At time: 314.8044774532318 and batch: 850, loss is 3.9403359508514404 and perplexity is 51.435878325366815
At time: 315.6383924484253 and batch: 900, loss is 3.8902948904037475 and perplexity is 48.92531200166949
At time: 316.4844410419464 and batch: 950, loss is 3.8758658266067503 and perplexity is 48.22443421869526
At time: 317.3295969963074 and batch: 1000, loss is 3.856273264884949 and perplexity is 47.288789780643086
At time: 318.15556359291077 and batch: 1050, loss is 3.840437021255493 and perplexity is 46.54581150458241
At time: 319.0013692378998 and batch: 1100, loss is 3.7942464542388916 and perplexity is 44.444732643917924
At time: 319.8469784259796 and batch: 1150, loss is 3.792189750671387 and perplexity is 44.35341694058779
At time: 320.6927864551544 and batch: 1200, loss is 3.7976618671417235 and perplexity is 44.59678927753482
At time: 321.54199957847595 and batch: 1250, loss is 3.833822708129883 and perplexity is 46.23895886105009
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.329779631900092 and perplexity of 75.92755271492872
Finished 14 epochs...
Completing Train Step...
At time: 324.045330286026 and batch: 50, loss is 3.948100447654724 and perplexity is 51.83680652682593
At time: 324.9351153373718 and batch: 100, loss is 3.9594936084747316 and perplexity is 52.43076872854341
At time: 325.78459644317627 and batch: 150, loss is 3.884471197128296 and perplexity is 48.64121404379038
At time: 326.6413929462433 and batch: 200, loss is 3.943889846801758 and perplexity is 51.61900129229407
At time: 327.48773288726807 and batch: 250, loss is 3.9602342891693114 and perplexity is 52.46961757226083
At time: 328.33850860595703 and batch: 300, loss is 3.955469765663147 and perplexity is 52.22021944937992
At time: 329.1855092048645 and batch: 350, loss is 3.939198408126831 and perplexity is 51.377401082680706
At time: 330.0327785015106 and batch: 400, loss is 3.94000150680542 and perplexity is 51.41867877841241
At time: 330.88079833984375 and batch: 450, loss is 3.860160207748413 and perplexity is 47.47295629509176
At time: 331.72788429260254 and batch: 500, loss is 3.892530155181885 and perplexity is 49.034795344891364
At time: 332.5790123939514 and batch: 550, loss is 3.8778595209121702 and perplexity is 48.320674903953986
At time: 333.428973197937 and batch: 600, loss is 3.8966904973983763 and perplexity is 49.23922182123292
At time: 334.35506200790405 and batch: 650, loss is 3.9174352264404297 and perplexity is 50.271344676940124
At time: 335.2029712200165 and batch: 700, loss is 3.882778401374817 and perplexity is 48.55894405600176
At time: 336.0534064769745 and batch: 750, loss is 3.869905071258545 and perplexity is 47.937835186455004
At time: 336.90088176727295 and batch: 800, loss is 3.89727746963501 and perplexity is 49.268132361407574
At time: 337.75072956085205 and batch: 850, loss is 3.935884938240051 and perplexity is 51.207445338449006
At time: 338.59603357315063 and batch: 900, loss is 3.886793713569641 and perplexity is 48.754315352118326
At time: 339.4462950229645 and batch: 950, loss is 3.8738429880142213 and perplexity is 48.126982569762994
At time: 340.29359793663025 and batch: 1000, loss is 3.855824055671692 and perplexity is 47.26755199105235
At time: 341.14243149757385 and batch: 1050, loss is 3.841767168045044 and perplexity is 46.607765461115925
At time: 341.99434995651245 and batch: 1100, loss is 3.79702513217926 and perplexity is 44.56840198113934
At time: 342.8443355560303 and batch: 1150, loss is 3.796596727371216 and perplexity is 44.54931275269435
At time: 343.6939444541931 and batch: 1200, loss is 3.80376916885376 and perplexity is 44.86998873161291
At time: 344.5405766963959 and batch: 1250, loss is 3.8400399780273435 and perplexity is 46.52733447365853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328745598340556 and perplexity of 75.84908165517466
Finished 15 epochs...
Completing Train Step...
At time: 347.0683317184448 and batch: 50, loss is 3.943414902687073 and perplexity is 51.5944909724012
At time: 347.9156484603882 and batch: 100, loss is 3.953638505935669 and perplexity is 52.12467817168919
At time: 348.7710151672363 and batch: 150, loss is 3.877751836776733 and perplexity is 48.31547181400337
At time: 349.62036204338074 and batch: 200, loss is 3.936550483703613 and perplexity is 51.24153756509958
At time: 350.4712986946106 and batch: 250, loss is 3.9528573513031007 and perplexity is 52.08397663703009
At time: 351.3201344013214 and batch: 300, loss is 3.9484380197525026 and perplexity is 51.854308140209305
At time: 352.1616852283478 and batch: 350, loss is 3.932555160522461 and perplexity is 51.03721949238133
At time: 353.0281620025635 and batch: 400, loss is 3.93390793800354 and perplexity is 51.106308213904796
At time: 353.9136073589325 and batch: 450, loss is 3.854028973579407 and perplexity is 47.1827789649614
At time: 354.7970564365387 and batch: 500, loss is 3.886864786148071 and perplexity is 48.757780570159525
At time: 355.74386382102966 and batch: 550, loss is 3.872569899559021 and perplexity is 48.06575164832941
At time: 356.6257553100586 and batch: 600, loss is 3.8914730024337767 and perplexity is 48.982985466542324
At time: 357.50184655189514 and batch: 650, loss is 3.912743625640869 and perplexity is 50.03604399609244
At time: 358.376846075058 and batch: 700, loss is 3.878781123161316 and perplexity is 48.36522787353076
At time: 359.2617585659027 and batch: 750, loss is 3.866389265060425 and perplexity is 47.76959097870024
At time: 360.11354756355286 and batch: 800, loss is 3.8944463205337523 and perplexity is 49.12884419856803
At time: 360.97255396842957 and batch: 850, loss is 3.9339338064193727 and perplexity is 51.10763027023702
At time: 361.8506410121918 and batch: 900, loss is 3.8852357482910156 and perplexity is 48.678416960498055
At time: 362.72923827171326 and batch: 950, loss is 3.8730277824401855 and perplexity is 48.08776517260284
At time: 363.57870745658875 and batch: 1000, loss is 3.855831308364868 and perplexity is 47.26789480934731
At time: 364.42997336387634 and batch: 1050, loss is 3.8428234004974366 and perplexity is 46.65702010312597
At time: 365.2793245315552 and batch: 1100, loss is 3.7986043548583983 and perplexity is 44.63884101714323
At time: 366.12964630126953 and batch: 1150, loss is 3.7989807748794555 and perplexity is 44.65564713349977
At time: 366.98579573631287 and batch: 1200, loss is 3.806789903640747 and perplexity is 45.00573398945636
At time: 367.8385081291199 and batch: 1250, loss is 3.8427241706848143 and perplexity is 46.652390565461495
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328369586137089 and perplexity of 75.8205668361465
Finished 16 epochs...
Completing Train Step...
At time: 370.3736057281494 and batch: 50, loss is 3.9393731784820556 and perplexity is 51.386381114017205
At time: 371.2901141643524 and batch: 100, loss is 3.9490173053741455 and perplexity is 51.884355297438
At time: 372.13878560066223 and batch: 150, loss is 3.8727212381362914 and perplexity is 48.0730264012608
At time: 372.9968864917755 and batch: 200, loss is 3.9312228631973265 and perplexity is 50.9692680172049
At time: 373.84421253204346 and batch: 250, loss is 3.9475993871688844 and perplexity is 51.8108396573926
At time: 374.69518518447876 and batch: 300, loss is 3.943423156738281 and perplexity is 51.59491683772931
At time: 375.5470118522644 and batch: 350, loss is 3.9277918481826783 and perplexity is 50.794691352189375
At time: 376.39959192276 and batch: 400, loss is 3.929535036087036 and perplexity is 50.88331326364083
At time: 377.2547113895416 and batch: 450, loss is 3.849606337547302 and perplexity is 46.974567467830475
At time: 378.1596648693085 and batch: 500, loss is 3.882760806083679 and perplexity is 48.55808965476048
At time: 379.0114252567291 and batch: 550, loss is 3.8687706279754637 and perplexity is 47.88348326673646
At time: 379.86143589019775 and batch: 600, loss is 3.8877921056747438 and perplexity is 48.80301558257305
At time: 380.7119162082672 and batch: 650, loss is 3.909481415748596 and perplexity is 49.873081871246434
At time: 381.5651550292969 and batch: 700, loss is 3.8760321283340455 and perplexity is 48.23245469229448
At time: 382.41450548171997 and batch: 750, loss is 3.8639141178131102 and perplexity is 47.65150041314181
At time: 383.264155626297 and batch: 800, loss is 3.892417483329773 and perplexity is 49.02927081491728
At time: 384.11552119255066 and batch: 850, loss is 3.932494339942932 and perplexity is 51.034115473509345
At time: 384.9631984233856 and batch: 900, loss is 3.88403742313385 and perplexity is 48.62011932558075
At time: 385.8122625350952 and batch: 950, loss is 3.8722829151153566 and perplexity is 48.051959504493325
At time: 386.65740633010864 and batch: 1000, loss is 3.855640516281128 and perplexity is 47.25887732946186
At time: 387.5073220729828 and batch: 1050, loss is 3.84336811542511 and perplexity is 46.682441801617635
At time: 388.36211013793945 and batch: 1100, loss is 3.799411430358887 and perplexity is 44.674882474230515
At time: 389.21130871772766 and batch: 1150, loss is 3.8003219842910765 and perplexity is 44.7155798899216
At time: 390.06007266044617 and batch: 1200, loss is 3.8084992933273316 and perplexity is 45.082732118129506
At time: 390.90906500816345 and batch: 1250, loss is 3.844111967086792 and perplexity is 46.71717953178055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.32820162807938 and perplexity of 75.807833230392
Finished 17 epochs...
Completing Train Step...
At time: 393.43377017974854 and batch: 50, loss is 3.935782217979431 and perplexity is 51.20218556646535
At time: 394.28389024734497 and batch: 100, loss is 3.9450568342208863 and perplexity is 51.67927517998408
At time: 395.13224387168884 and batch: 150, loss is 3.868510670661926 and perplexity is 47.87103722285378
At time: 395.9821529388428 and batch: 200, loss is 3.926835594177246 and perplexity is 50.74614194161286
At time: 396.8365886211395 and batch: 250, loss is 3.943327474594116 and perplexity is 51.58998036162833
At time: 397.6861786842346 and batch: 300, loss is 3.9393580389022826 and perplexity is 51.38560315169011
At time: 398.53626775741577 and batch: 350, loss is 3.923913125991821 and perplexity is 50.5980544521428
At time: 399.3827373981476 and batch: 400, loss is 3.9259652614593508 and perplexity is 50.70199512796896
At time: 400.2653889656067 and batch: 450, loss is 3.8459958362579347 and perplexity is 46.805271536916635
At time: 401.11017179489136 and batch: 500, loss is 3.879390935897827 and perplexity is 48.39473060014727
At time: 401.96033906936646 and batch: 550, loss is 3.8656752777099608 and perplexity is 47.735496268049054
At time: 402.8108744621277 and batch: 600, loss is 3.8848369312286377 and perplexity is 48.65900704800408
At time: 403.66352343559265 and batch: 650, loss is 3.906874942779541 and perplexity is 49.743258295784194
At time: 404.5133593082428 and batch: 700, loss is 3.8738253355026244 and perplexity is 48.126133015143466
At time: 405.36048412323 and batch: 750, loss is 3.8618862628936768 and perplexity is 47.55496799356161
At time: 406.2090628147125 and batch: 800, loss is 3.890698781013489 and perplexity is 48.9450764668379
At time: 407.0560746192932 and batch: 850, loss is 3.931192603111267 and perplexity is 50.96772570610365
At time: 407.9067761898041 and batch: 900, loss is 3.88292827129364 and perplexity is 48.56622212637385
At time: 408.7573010921478 and batch: 950, loss is 3.8714769411087038 and perplexity is 48.01324647710651
At time: 409.6045563220978 and batch: 1000, loss is 3.855245819091797 and perplexity is 47.24022806405742
At time: 410.4571042060852 and batch: 1050, loss is 3.843528151512146 and perplexity is 46.68991327477358
At time: 411.3070294857025 and batch: 1100, loss is 3.799723563194275 and perplexity is 44.68882914846208
At time: 412.1559782028198 and batch: 1150, loss is 3.8010187005996703 and perplexity is 44.74674481897564
At time: 413.0063474178314 and batch: 1200, loss is 3.8094750118255614 and perplexity is 45.12674164076905
At time: 413.8634126186371 and batch: 1250, loss is 3.8448185110092163 and perplexity is 46.75019893450708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328116980782391 and perplexity of 75.80141657379865
Finished 18 epochs...
Completing Train Step...
At time: 416.42450165748596 and batch: 50, loss is 3.9324989318847656 and perplexity is 51.03434981973717
At time: 417.32016825675964 and batch: 100, loss is 3.9415144777297972 and perplexity is 51.49653262482589
At time: 418.1680393218994 and batch: 150, loss is 3.86481867313385 and perplexity is 47.694623331974256
At time: 419.0186140537262 and batch: 200, loss is 3.923019223213196 and perplexity is 50.552844920148566
At time: 419.86935544013977 and batch: 250, loss is 3.939650845527649 and perplexity is 51.40065139974723
At time: 420.7221541404724 and batch: 300, loss is 3.9358514976501464 and perplexity is 51.205732959901006
At time: 421.56857776641846 and batch: 350, loss is 3.920558671951294 and perplexity is 50.428609959711665
At time: 422.47960233688354 and batch: 400, loss is 3.9228640174865723 and perplexity is 50.544999437967476
At time: 423.32892870903015 and batch: 450, loss is 3.84286762714386 and perplexity is 46.659083632288386
At time: 424.1763026714325 and batch: 500, loss is 3.876474962234497 and perplexity is 48.25381838826951
At time: 425.0307161808014 and batch: 550, loss is 3.863005156517029 and perplexity is 47.60820672269136
At time: 425.8808972835541 and batch: 600, loss is 3.882291917800903 and perplexity is 48.53532667255225
At time: 426.73074650764465 and batch: 650, loss is 3.9046239185333254 and perplexity is 49.631410948049
At time: 427.5785069465637 and batch: 700, loss is 3.8718928146362304 and perplexity is 48.03321806782705
At time: 428.42549562454224 and batch: 750, loss is 3.8600761795043947 and perplexity is 47.468967393528466
At time: 429.27730679512024 and batch: 800, loss is 3.889124813079834 and perplexity is 48.86809908182843
At time: 430.1259288787842 and batch: 850, loss is 3.929935321807861 and perplexity is 50.90368520439502
At time: 430.97474002838135 and batch: 900, loss is 3.881834988594055 and perplexity is 48.51315453016713
At time: 431.8308367729187 and batch: 950, loss is 3.870600490570068 and perplexity is 47.97118367704449
At time: 432.6777412891388 and batch: 1000, loss is 3.8546894454956053 and perplexity is 47.21395215877227
At time: 433.52248907089233 and batch: 1050, loss is 3.843426523208618 and perplexity is 46.68516849920143
At time: 434.4050760269165 and batch: 1100, loss is 3.799718098640442 and perplexity is 44.6885849446167
At time: 435.2731046676636 and batch: 1150, loss is 3.8013228750228882 and perplexity is 44.76035770451268
At time: 436.15979266166687 and batch: 1200, loss is 3.810006937980652 and perplexity is 45.15075212027667
At time: 437.0504205226898 and batch: 1250, loss is 3.8451273155212404 and perplexity is 46.76463783616033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328082676351506 and perplexity of 75.7988162939436
Finished 19 epochs...
Completing Train Step...
At time: 439.79735016822815 and batch: 50, loss is 3.9294429874420165 and perplexity is 50.878629739160154
At time: 440.7222774028778 and batch: 100, loss is 3.9382685136795046 and perplexity is 51.32964772892614
At time: 441.5938606262207 and batch: 150, loss is 3.8614767742156983 and perplexity is 47.53549875907422
At time: 442.4542257785797 and batch: 200, loss is 3.919588465690613 and perplexity is 50.37970753316749
At time: 443.3187098503113 and batch: 250, loss is 3.9363654708862303 and perplexity is 51.232058100805865
At time: 444.2368366718292 and batch: 300, loss is 3.932712616920471 and perplexity is 51.04525626183084
At time: 445.11354207992554 and batch: 350, loss is 3.9175478410720825 and perplexity is 50.27700628468756
At time: 445.98269987106323 and batch: 400, loss is 3.9200773525238035 and perplexity is 50.40434353045711
At time: 446.84507727622986 and batch: 450, loss is 3.840057921409607 and perplexity is 46.528169338896824
At time: 447.7098877429962 and batch: 500, loss is 3.8738479471206664 and perplexity is 48.12722123718422
At time: 448.5557062625885 and batch: 550, loss is 3.8606060409545897 and perplexity is 47.49412603413914
At time: 449.40843296051025 and batch: 600, loss is 3.8800027561187744 and perplexity is 48.42434853441739
At time: 450.2525360584259 and batch: 650, loss is 3.9025873708724976 and perplexity is 49.530437068137964
At time: 451.1086983680725 and batch: 700, loss is 3.8701218223571776 and perplexity is 47.94822689106375
At time: 451.97297501564026 and batch: 750, loss is 3.8583965253829957 and perplexity is 47.38930286995942
At time: 452.84178948402405 and batch: 800, loss is 3.887626624107361 and perplexity is 48.79494025123911
At time: 453.70859241485596 and batch: 850, loss is 3.9286918115615843 and perplexity is 50.84042529059725
At time: 454.5643780231476 and batch: 900, loss is 3.8807289457321166 and perplexity is 48.459526564770144
At time: 455.42505145072937 and batch: 950, loss is 3.869664149284363 and perplexity is 47.92628729968956
At time: 456.2745244503021 and batch: 1000, loss is 3.854016833305359 and perplexity is 47.18220615657146
At time: 457.12876868247986 and batch: 1050, loss is 3.8431359910964966 and perplexity is 46.67160692872386
At time: 457.98019433021545 and batch: 1100, loss is 3.7994890213012695 and perplexity is 44.678348974945315
At time: 458.8290297985077 and batch: 1150, loss is 3.8013576078414917 and perplexity is 44.76191238489652
At time: 459.7061047554016 and batch: 1200, loss is 3.8102340030670168 and perplexity is 45.16100544374814
At time: 460.56191754341125 and batch: 1250, loss is 3.84516797542572 and perplexity is 46.76653932052461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3280764391822535 and perplexity of 75.79834352537162
Finished 20 epochs...
Completing Train Step...
At time: 463.2047915458679 and batch: 50, loss is 3.92656466960907 and perplexity is 50.73239542723904
At time: 464.05389976501465 and batch: 100, loss is 3.935240740776062 and perplexity is 51.1744682550418
At time: 464.90012979507446 and batch: 150, loss is 3.8583910036087037 and perplexity is 47.389041197647565
At time: 465.75168347358704 and batch: 200, loss is 3.9164379119873045 and perplexity is 50.221233330854176
At time: 466.6657392978668 and batch: 250, loss is 3.933357343673706 and perplexity is 51.07817711550577
At time: 467.5218267440796 and batch: 300, loss is 3.9298375940322874 and perplexity is 50.89871074354694
At time: 468.3685336112976 and batch: 350, loss is 3.9147805547714234 and perplexity is 50.138067743982425
At time: 469.22208189964294 and batch: 400, loss is 3.917510094642639 and perplexity is 50.275108543033916
At time: 470.07000970840454 and batch: 450, loss is 3.837479658126831 and perplexity is 46.408361982097766
At time: 470.9210515022278 and batch: 500, loss is 3.8714265632629394 and perplexity is 48.01082773410689
At time: 471.7795238494873 and batch: 550, loss is 3.858392162322998 and perplexity is 47.38909610803881
At time: 472.6274833679199 and batch: 600, loss is 3.8778865194320677 and perplexity is 48.32197950826796
At time: 473.4751465320587 and batch: 650, loss is 3.900691952705383 and perplexity is 49.436645093480074
At time: 474.32340955734253 and batch: 700, loss is 3.868452301025391 and perplexity is 47.868243089357584
At time: 475.17499804496765 and batch: 750, loss is 3.856798095703125 and perplexity is 47.31361490880054
At time: 476.02878308296204 and batch: 800, loss is 3.8861769437789917 and perplexity is 48.72425443453121
At time: 476.87811851501465 and batch: 850, loss is 3.9274462509155272 and perplexity is 50.7771398787176
At time: 477.72896575927734 and batch: 900, loss is 3.879611306190491 and perplexity is 48.40539653627758
At time: 478.57688570022583 and batch: 950, loss is 3.8686799097061155 and perplexity is 47.87913955703404
At time: 479.42622804641724 and batch: 1000, loss is 3.853260202407837 and perplexity is 47.14652014385254
At time: 480.27336406707764 and batch: 1050, loss is 3.8427103090286256 and perplexity is 46.6517438905451
At time: 481.1192355155945 and batch: 1100, loss is 3.799100203514099 and perplexity is 44.660980614944904
At time: 481.96675848960876 and batch: 1150, loss is 3.8011969900131226 and perplexity is 44.754723401090544
At time: 482.8247787952423 and batch: 1200, loss is 3.8102451944351197 and perplexity is 45.1615108600121
At time: 483.673063993454 and batch: 1250, loss is 3.845023999214172 and perplexity is 46.75980653605806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.328087576984489 and perplexity of 75.799187757033
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 486.1836609840393 and batch: 50, loss is 3.927696399688721 and perplexity is 50.78984330677177
At time: 487.0746977329254 and batch: 100, loss is 3.9409920883178713 and perplexity is 51.46963840667078
At time: 487.9316749572754 and batch: 150, loss is 3.8662591123580934 and perplexity is 47.763374041929424
At time: 488.8454341888428 and batch: 200, loss is 3.9247706937789917 and perplexity is 50.64146432452817
At time: 489.70008754730225 and batch: 250, loss is 3.941153335571289 and perplexity is 51.477938413656936
At time: 490.5553641319275 and batch: 300, loss is 3.937770276069641 and perplexity is 51.30407973792282
At time: 491.404171705246 and batch: 350, loss is 3.921801218986511 and perplexity is 50.4913088245847
At time: 492.2542428970337 and batch: 400, loss is 3.923949375152588 and perplexity is 50.599888622396485
At time: 493.1037218570709 and batch: 450, loss is 3.8426891803741454 and perplexity is 46.6507582123806
At time: 493.9556314945221 and batch: 500, loss is 3.8765363073349 and perplexity is 48.25677861440012
At time: 494.80797004699707 and batch: 550, loss is 3.8612842988967895 and perplexity is 47.526350229252415
At time: 495.6569004058838 and batch: 600, loss is 3.879982852935791 and perplexity is 48.42338474533892
At time: 496.5114789009094 and batch: 650, loss is 3.902442898750305 and perplexity is 49.52328181766116
At time: 497.36171674728394 and batch: 700, loss is 3.871287431716919 and perplexity is 48.004148378083855
At time: 498.20893931388855 and batch: 750, loss is 3.8570530366897584 and perplexity is 47.32567862616925
At time: 499.05866861343384 and batch: 800, loss is 3.884716119766235 and perplexity is 48.65312883728832
At time: 499.90682101249695 and batch: 850, loss is 3.9229665184020996 and perplexity is 50.550180612218206
At time: 500.75443720817566 and batch: 900, loss is 3.8749022340774535 and perplexity is 48.17798789541145
At time: 501.6008551120758 and batch: 950, loss is 3.8621506977081297 and perplexity is 47.56754484550477
At time: 502.4494996070862 and batch: 1000, loss is 3.8452691650390625 and perplexity is 46.771271847993084
At time: 503.30020475387573 and batch: 1050, loss is 3.8330420017242433 and perplexity is 46.20287389739112
At time: 504.1518533229828 and batch: 1100, loss is 3.786930809020996 and perplexity is 44.12077716457154
At time: 505.0122227668762 and batch: 1150, loss is 3.7879918241500854 and perplexity is 44.16761481998687
At time: 505.8629503250122 and batch: 1200, loss is 3.79763870716095 and perplexity is 44.59575642871301
At time: 506.7118113040924 and batch: 1250, loss is 3.8343831634521486 and perplexity is 46.26488099506024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.327086956831661 and perplexity of 75.72337949615729
Finished 22 epochs...
Completing Train Step...
At time: 509.27641201019287 and batch: 50, loss is 3.9260892772674563 and perplexity is 50.70828336677977
At time: 510.128089427948 and batch: 100, loss is 3.938163938522339 and perplexity is 51.324280203607415
At time: 511.0092179775238 and batch: 150, loss is 3.862609453201294 and perplexity is 47.58937172421643
At time: 511.85789036750793 and batch: 200, loss is 3.920870666503906 and perplexity is 50.44434586594576
At time: 512.7134728431702 and batch: 250, loss is 3.93674081325531 and perplexity is 51.25129127215251
At time: 513.5649733543396 and batch: 300, loss is 3.933296213150024 and perplexity is 51.07505477522611
At time: 514.4143302440643 and batch: 350, loss is 3.917440595626831 and perplexity is 50.271614593884955
At time: 515.260678768158 and batch: 400, loss is 3.919681987762451 and perplexity is 50.38441936812145
At time: 516.1101818084717 and batch: 450, loss is 3.839082818031311 and perplexity is 46.48282167671098
At time: 516.9589364528656 and batch: 500, loss is 3.873261170387268 and perplexity is 48.09898958716685
At time: 517.807599067688 and batch: 550, loss is 3.858320779800415 and perplexity is 47.38571347554753
At time: 518.6589798927307 and batch: 600, loss is 3.8775478172302247 and perplexity is 48.30561551882715
At time: 519.5133776664734 and batch: 650, loss is 3.9004534101486206 and perplexity is 49.42485375618549
At time: 520.3660161495209 and batch: 700, loss is 3.868782649040222 and perplexity is 47.884058880649405
At time: 521.2173209190369 and batch: 750, loss is 3.855300168991089 and perplexity is 47.24279563546821
At time: 522.0663595199585 and batch: 800, loss is 3.88324089050293 and perplexity is 48.58140723378765
At time: 522.9200553894043 and batch: 850, loss is 3.9219599103927614 and perplexity is 50.49932199717949
At time: 523.7671799659729 and batch: 900, loss is 3.8741089630126955 and perplexity is 48.139784846345826
At time: 524.6193602085114 and batch: 950, loss is 3.8617704916000366 and perplexity is 47.54946281207512
At time: 525.4702801704407 and batch: 1000, loss is 3.8455734062194824 and perplexity is 46.7855037598069
At time: 526.3189845085144 and batch: 1050, loss is 3.8336316394805907 and perplexity is 46.2301248896102
At time: 527.1678330898285 and batch: 1100, loss is 3.7882930994033814 and perplexity is 44.18092343400661
At time: 528.02019572258 and batch: 1150, loss is 3.790158314704895 and perplexity is 44.2634072695732
At time: 528.8690235614777 and batch: 1200, loss is 3.8002193546295167 and perplexity is 44.710990980574174
At time: 529.7184746265411 and batch: 1250, loss is 3.8368584156036376 and perplexity is 46.37954008782287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326864200786953 and perplexity of 75.70651353421606
Finished 23 epochs...
Completing Train Step...
At time: 532.2147448062897 and batch: 50, loss is 3.9255955410003662 and perplexity is 50.68325302794122
At time: 533.1145355701447 and batch: 100, loss is 3.937112331390381 and perplexity is 51.270335593742274
At time: 533.9974603652954 and batch: 150, loss is 3.8611985301971434 and perplexity is 47.52227413079766
At time: 534.8669488430023 and batch: 200, loss is 3.9191142606735228 and perplexity is 50.355822886652916
At time: 535.7338831424713 and batch: 250, loss is 3.9348052835464475 and perplexity is 51.152188814092334
At time: 536.6057758331299 and batch: 300, loss is 3.9313572025299073 and perplexity is 50.976115654595674
At time: 537.4919452667236 and batch: 350, loss is 3.915607852935791 and perplexity is 50.17956403792911
At time: 538.3775238990784 and batch: 400, loss is 3.9178209114074707 and perplexity is 50.29073731833969
At time: 539.2781662940979 and batch: 450, loss is 3.8374022912979124 and perplexity is 46.404771653183886
At time: 540.1749832630157 and batch: 500, loss is 3.87176100730896 and perplexity is 48.02688735495967
At time: 541.0657529830933 and batch: 550, loss is 3.8569611930847167 and perplexity is 47.32133226482898
At time: 541.9545137882233 and batch: 600, loss is 3.876397180557251 and perplexity is 48.250065271305495
At time: 542.8348217010498 and batch: 650, loss is 3.8994698333740234 and perplexity is 49.376264517483584
At time: 543.7111139297485 and batch: 700, loss is 3.86773627281189 and perplexity is 47.833980344784635
At time: 544.5934784412384 and batch: 750, loss is 3.8545805549621583 and perplexity is 47.20881128623682
At time: 545.485622882843 and batch: 800, loss is 3.8826281595230103 and perplexity is 48.551649018348705
At time: 546.3788692951202 and batch: 850, loss is 3.9216011571884155 and perplexity is 50.481208452936
At time: 547.2546639442444 and batch: 900, loss is 3.8739883136749267 and perplexity is 48.13397716353737
At time: 548.1197142601013 and batch: 950, loss is 3.8618228912353514 and perplexity is 47.551954451865825
At time: 548.9852390289307 and batch: 1000, loss is 3.845984878540039 and perplexity is 46.80475866076508
At time: 549.8647615909576 and batch: 1050, loss is 3.834231610298157 and perplexity is 46.257869937713735
At time: 550.7239809036255 and batch: 1100, loss is 3.789205117225647 and perplexity is 44.22123560349686
At time: 551.5856873989105 and batch: 1150, loss is 3.791482057571411 and perplexity is 44.32203943758933
At time: 552.440776348114 and batch: 1200, loss is 3.8017055416107177 and perplexity is 44.77748927549634
At time: 553.2869765758514 and batch: 1250, loss is 3.8381330013275146 and perplexity is 46.43869247688601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326759950958029 and perplexity of 75.6986215545076
Finished 24 epochs...
Completing Train Step...
At time: 555.7734587192535 and batch: 50, loss is 3.925136103630066 and perplexity is 50.65997259581157
At time: 556.6840240955353 and batch: 100, loss is 3.936347165107727 and perplexity is 51.23112026668193
At time: 557.5532939434052 and batch: 150, loss is 3.860253944396973 and perplexity is 47.47740645948057
At time: 558.4244134426117 and batch: 200, loss is 3.9179513168334963 and perplexity is 50.29729593099487
At time: 559.285879611969 and batch: 250, loss is 3.933584909439087 and perplexity is 51.089802082647424
At time: 560.139199256897 and batch: 300, loss is 3.9301578521728517 and perplexity is 50.915014080505635
At time: 560.9858796596527 and batch: 350, loss is 3.9144721460342407 and perplexity is 50.122607110044555
At time: 561.8441169261932 and batch: 400, loss is 3.9166794061660766 and perplexity is 50.23336293090928
At time: 562.7061522006989 and batch: 450, loss is 3.8363354444503783 and perplexity is 46.35529126752493
At time: 563.561146736145 and batch: 500, loss is 3.8708170557022097 and perplexity is 47.98157368779292
At time: 564.4311711788177 and batch: 550, loss is 3.8561002588272095 and perplexity is 47.28060924120971
At time: 565.2929217815399 and batch: 600, loss is 3.875647497177124 and perplexity is 48.2139065547695
At time: 566.129075050354 and batch: 650, loss is 3.898829770088196 and perplexity is 49.34467069547602
At time: 566.9557428359985 and batch: 700, loss is 3.8670999002456665 and perplexity is 47.803549795570255
At time: 567.7773938179016 and batch: 750, loss is 3.8541581439971924 and perplexity is 47.18887397787179
At time: 568.6012423038483 and batch: 800, loss is 3.882272205352783 and perplexity is 48.53436993187311
At time: 569.420706987381 and batch: 850, loss is 3.9214176082611085 and perplexity is 50.47194353158451
At time: 570.2393381595612 and batch: 900, loss is 3.8739818954467773 and perplexity is 48.133668229681604
At time: 571.0598332881927 and batch: 950, loss is 3.8619117307662965 and perplexity is 47.55617913285141
At time: 571.9246656894684 and batch: 1000, loss is 3.8462859773635865 and perplexity is 46.818853640418666
At time: 572.7938017845154 and batch: 1050, loss is 3.8346662378311156 and perplexity is 46.27797925131881
At time: 573.6487975120544 and batch: 1100, loss is 3.7897924470901487 and perplexity is 44.247215684503864
At time: 574.5032839775085 and batch: 1150, loss is 3.792312388420105 and perplexity is 44.35885667734105
At time: 575.3661668300629 and batch: 1200, loss is 3.8026137590408324 and perplexity is 44.81817544488053
At time: 576.2824642658234 and batch: 1250, loss is 3.8388643074035644 and perplexity is 46.472665795791336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326704261946852 and perplexity of 75.69440609050439
Finished 25 epochs...
Completing Train Step...
At time: 578.768180847168 and batch: 50, loss is 3.924660668373108 and perplexity is 50.63589278337247
At time: 579.6204073429108 and batch: 100, loss is 3.9356939220428466 and perplexity is 51.19766482112025
At time: 580.4699912071228 and batch: 150, loss is 3.8594986963272095 and perplexity is 47.44156277703444
At time: 581.3170125484467 and batch: 200, loss is 3.9170555210113527 and perplexity is 50.252259997946695
At time: 582.167539358139 and batch: 250, loss is 3.9326788139343263 and perplexity is 51.04353080890356
At time: 583.0193703174591 and batch: 300, loss is 3.929278349876404 and perplexity is 50.870253894926414
At time: 583.8666553497314 and batch: 350, loss is 3.9136351442337034 and perplexity is 50.08067194999628
At time: 584.7201125621796 and batch: 400, loss is 3.9158554124832152 and perplexity is 50.19198800586478
At time: 585.5673050880432 and batch: 450, loss is 3.835548129081726 and perplexity is 46.3188093975446
At time: 586.4174520969391 and batch: 500, loss is 3.870122108459473 and perplexity is 47.94824060916347
At time: 587.2750577926636 and batch: 550, loss is 3.8554622173309325 and perplexity is 47.250451872394
At time: 588.1212403774261 and batch: 600, loss is 3.875078363418579 and perplexity is 48.1864741999979
At time: 588.973667383194 and batch: 650, loss is 3.8983481884002686 and perplexity is 49.32091292678461
At time: 589.8277781009674 and batch: 700, loss is 3.8666458463668825 and perplexity is 47.78184933532783
At time: 590.6767427921295 and batch: 750, loss is 3.8538461828231814 and perplexity is 47.17415517731192
At time: 591.5299336910248 and batch: 800, loss is 3.8820139503479005 and perplexity is 48.52183730630567
At time: 592.3760936260223 and batch: 850, loss is 3.9212833404541017 and perplexity is 50.46516722934094
At time: 593.2286276817322 and batch: 900, loss is 3.8739752674102785 and perplexity is 48.13334919902903
At time: 594.0739336013794 and batch: 950, loss is 3.8619710969924927 and perplexity is 47.55900244754278
At time: 594.9250690937042 and batch: 1000, loss is 3.846481285095215 and perplexity is 46.827998617533915
At time: 595.7715079784393 and batch: 1050, loss is 3.8349604988098145 and perplexity is 46.291599058576146
At time: 596.6201975345612 and batch: 1100, loss is 3.7901772689819335 and perplexity is 44.26424625840845
At time: 597.4693379402161 and batch: 1150, loss is 3.792858018875122 and perplexity is 44.383066824790205
At time: 598.3746554851532 and batch: 1200, loss is 3.803200616836548 and perplexity is 44.84448505977632
At time: 599.2353084087372 and batch: 1250, loss is 3.8393182611465453 and perplexity is 46.49376702550437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326673076100593 and perplexity of 75.69204553320155
Finished 26 epochs...
Completing Train Step...
At time: 601.7238342761993 and batch: 50, loss is 3.924180498123169 and perplexity is 50.61158477053816
At time: 602.6124420166016 and batch: 100, loss is 3.935102677345276 and perplexity is 51.16740342009475
At time: 603.4634404182434 and batch: 150, loss is 3.8588449573516845 and perplexity is 47.410558513839106
At time: 604.3127205371857 and batch: 200, loss is 3.9163066577911376 and perplexity is 50.21464201582116
At time: 605.1635229587555 and batch: 250, loss is 3.931938815116882 and perplexity is 51.005772628694174
At time: 606.0189774036407 and batch: 300, loss is 3.9285675191879275 and perplexity is 50.83410660615044
At time: 606.8687002658844 and batch: 350, loss is 3.9129562902450563 and perplexity is 50.04668602313508
At time: 607.7198252677917 and batch: 400, loss is 3.91519907951355 and perplexity is 50.15905615763453
At time: 608.5675055980682 and batch: 450, loss is 3.8349108028411867 and perplexity is 46.28929860988358
At time: 609.4169504642487 and batch: 500, loss is 3.869559950828552 and perplexity is 47.92129371472663
At time: 610.2662615776062 and batch: 550, loss is 3.8549443769454954 and perplexity is 47.22599001439995
At time: 611.117226600647 and batch: 600, loss is 3.8746060132980347 and perplexity is 48.163718687808476
At time: 611.9672350883484 and batch: 650, loss is 3.8979510974884035 and perplexity is 49.301331928472024
At time: 612.8206932544708 and batch: 700, loss is 3.8662865924835206 and perplexity is 47.76468660347352
At time: 613.6718807220459 and batch: 750, loss is 3.85358238697052 and perplexity is 47.16171247206251
At time: 614.5220422744751 and batch: 800, loss is 3.881798424720764 and perplexity is 48.511380733760575
At time: 615.3744869232178 and batch: 850, loss is 3.9211609983444213 and perplexity is 50.458993591972344
At time: 616.2293126583099 and batch: 900, loss is 3.873945984840393 and perplexity is 48.131939751503516
At time: 617.0800268650055 and batch: 950, loss is 3.861993365287781 and perplexity is 47.56006151724469
At time: 617.9326810836792 and batch: 1000, loss is 3.8465986013412476 and perplexity is 46.83349262480281
At time: 618.7832505702972 and batch: 1050, loss is 3.8351553440093995 and perplexity is 46.30061963321312
At time: 619.6359796524048 and batch: 1100, loss is 3.7904346418380737 and perplexity is 44.2756401400679
At time: 620.5504806041718 and batch: 1150, loss is 3.793231954574585 and perplexity is 44.39966634131013
At time: 621.4024977684021 and batch: 1200, loss is 3.8035983180999757 and perplexity is 44.86232331505557
At time: 622.2516129016876 and batch: 1250, loss is 3.839617075920105 and perplexity is 46.507662125897276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326662829322537 and perplexity of 75.69126993758402
Finished 27 epochs...
Completing Train Step...
At time: 624.7957625389099 and batch: 50, loss is 3.923705315589905 and perplexity is 50.58754074257786
At time: 625.6484541893005 and batch: 100, loss is 3.934552855491638 and perplexity is 51.13927819614039
At time: 626.49702501297 and batch: 150, loss is 3.858254837989807 and perplexity is 47.38258887882593
At time: 627.3523426055908 and batch: 200, loss is 3.915649108886719 and perplexity is 50.181634286265385
At time: 628.2030353546143 and batch: 250, loss is 3.9312973976135255 and perplexity is 50.97306712342097
At time: 629.0498299598694 and batch: 300, loss is 3.927955985069275 and perplexity is 50.80302931894879
At time: 629.9011607170105 and batch: 350, loss is 3.912367811203003 and perplexity is 50.017243261360065
At time: 630.75315117836 and batch: 400, loss is 3.914640574455261 and perplexity is 50.13104989259984
At time: 631.6067819595337 and batch: 450, loss is 3.834362473487854 and perplexity is 46.2639237862267
At time: 632.4565224647522 and batch: 500, loss is 3.869076385498047 and perplexity is 47.898126240438366
At time: 633.3090109825134 and batch: 550, loss is 3.8544975185394286 and perplexity is 47.20489139817528
At time: 634.1646695137024 and batch: 600, loss is 3.874190912246704 and perplexity is 48.14373002648936
At time: 635.0122287273407 and batch: 650, loss is 3.897603406906128 and perplexity is 49.28419329930949
At time: 635.8621504306793 and batch: 700, loss is 3.865980176925659 and perplexity is 47.750053002477834
At time: 636.7152409553528 and batch: 750, loss is 3.8533414793014527 and perplexity is 47.15035222228224
At time: 637.5650234222412 and batch: 800, loss is 3.881603231430054 and perplexity is 48.50191256181014
At time: 638.4193217754364 and batch: 850, loss is 3.9210390424728394 and perplexity is 50.45284019665866
At time: 639.2650153636932 and batch: 900, loss is 3.8738941192626952 and perplexity is 48.12944342537984
At time: 640.1150629520416 and batch: 950, loss is 3.8619836568832397 and perplexity is 47.559599787168814
At time: 640.9636397361755 and batch: 1000, loss is 3.846657567024231 and perplexity is 46.83625427510243
At time: 641.8117091655731 and batch: 1050, loss is 3.8352863502502443 and perplexity is 46.30668570067772
At time: 642.7178704738617 and batch: 1100, loss is 3.7906106901168823 and perplexity is 44.28343547646536
At time: 643.5659739971161 and batch: 1150, loss is 3.793497295379639 and perplexity is 44.411448947655245
At time: 644.420095205307 and batch: 1200, loss is 3.8038785362243654 and perplexity is 44.87489631265931
At time: 645.2743558883667 and batch: 1250, loss is 3.839819588661194 and perplexity is 46.51708147377282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326653919080748 and perplexity of 75.69059551307222
Finished 28 epochs...
Completing Train Step...
At time: 647.7839980125427 and batch: 50, loss is 3.9232364082336426 and perplexity is 50.563825433164496
At time: 648.6637871265411 and batch: 100, loss is 3.9340315771102907 and perplexity is 51.11262734283936
At time: 649.515659570694 and batch: 150, loss is 3.8577068042755127 and perplexity is 47.35662873681605
At time: 650.3754179477692 and batch: 200, loss is 3.9150513553619386 and perplexity is 50.151647000887195
At time: 651.2291812896729 and batch: 250, loss is 3.930718183517456 and perplexity is 50.943551353223455
At time: 652.0763847827911 and batch: 300, loss is 3.9274051904678347 and perplexity is 50.775054989425165
At time: 652.9288356304169 and batch: 350, loss is 3.9118359756469725 and perplexity is 49.990649385390284
At time: 653.7765254974365 and batch: 400, loss is 3.914141035079956 and perplexity is 50.106013713052484
At time: 654.6258895397186 and batch: 450, loss is 3.8338700103759766 and perplexity is 46.24114611939196
At time: 655.4750347137451 and batch: 500, loss is 3.868640561103821 and perplexity is 47.87725561687884
At time: 656.3291039466858 and batch: 550, loss is 3.854093909263611 and perplexity is 47.185842910474754
At time: 657.1818635463715 and batch: 600, loss is 3.873811569213867 and perplexity is 48.12547050145979
At time: 658.0294525623322 and batch: 650, loss is 3.8972852277755736 and perplexity is 49.268514591986445
At time: 658.8824706077576 and batch: 700, loss is 3.8657051372528075 and perplexity is 47.736921649425696
At time: 659.7340004444122 and batch: 750, loss is 3.853113303184509 and perplexity is 47.13959486533227
At time: 660.5935180187225 and batch: 800, loss is 3.881417655944824 and perplexity is 48.492912630961
At time: 661.4431581497192 and batch: 850, loss is 3.9209143829345705 and perplexity is 50.446551160897684
At time: 662.2890465259552 and batch: 900, loss is 3.873824324607849 and perplexity is 48.12608436471163
At time: 663.1380717754364 and batch: 950, loss is 3.861951513290405 and perplexity is 47.558071075327156
At time: 664.0436494350433 and batch: 1000, loss is 3.84667950630188 and perplexity is 46.83728183996097
At time: 664.8922839164734 and batch: 1050, loss is 3.835372738838196 and perplexity is 46.31068624266622
At time: 665.74049949646 and batch: 1100, loss is 3.790731954574585 and perplexity is 44.28880580886226
At time: 666.5973379611969 and batch: 1150, loss is 3.7936915588378906 and perplexity is 44.42007730737415
At time: 667.4469974040985 and batch: 1200, loss is 3.8040833473205566 and perplexity is 44.8840881306256
At time: 668.298597574234 and batch: 1250, loss is 3.8399605703353883 and perplexity is 46.52363999210221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326654810104927 and perplexity of 75.69066295525299
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 670.814934015274 and batch: 50, loss is 3.9233212089538574 and perplexity is 50.568113463789516
At time: 671.6650686264038 and batch: 100, loss is 3.934821457862854 and perplexity is 51.15301617247007
At time: 672.5136461257935 and batch: 150, loss is 3.8590244960784914 and perplexity is 47.419071309317246
At time: 673.370445728302 and batch: 200, loss is 3.916651291847229 and perplexity is 50.23195067397947
At time: 674.2207198143005 and batch: 250, loss is 3.9319280672073362 and perplexity is 51.005224426209665
At time: 675.0698199272156 and batch: 300, loss is 3.928440647125244 and perplexity is 50.82765758729947
At time: 675.9199285507202 and batch: 350, loss is 3.9126317358016967 and perplexity is 50.03044578437423
At time: 676.769537448883 and batch: 400, loss is 3.9148404598236084 and perplexity is 50.141071357512
At time: 677.6184711456299 and batch: 450, loss is 3.83432767868042 and perplexity is 46.26231406991246
At time: 678.467157125473 and batch: 500, loss is 3.8691084146499635 and perplexity is 47.89966040136904
At time: 679.3174338340759 and batch: 550, loss is 3.8535610103607176 and perplexity is 47.16070432531281
At time: 680.1726820468903 and batch: 600, loss is 3.873427362442017 and perplexity is 48.10698392135669
At time: 681.0254521369934 and batch: 650, loss is 3.8969284009933474 and perplexity is 49.25093740265198
At time: 681.8728015422821 and batch: 700, loss is 3.8655862474441527 and perplexity is 47.73124655330725
At time: 682.722261428833 and batch: 750, loss is 3.8526923274993896 and perplexity is 47.119754418553434
At time: 683.5724487304688 and batch: 800, loss is 3.880606665611267 and perplexity is 48.453601290284524
At time: 684.4203612804413 and batch: 850, loss is 3.918935375213623 and perplexity is 50.346815767762536
At time: 685.2673630714417 and batch: 900, loss is 3.8717287015914916 and perplexity is 48.025335836967486
At time: 686.1749770641327 and batch: 950, loss is 3.85997043132782 and perplexity is 47.46394790218841
At time: 687.0322103500366 and batch: 1000, loss is 3.844270873069763 and perplexity is 46.72460376097729
At time: 687.880692243576 and batch: 1050, loss is 3.8328873205184935 and perplexity is 46.19572773385041
At time: 688.7271428108215 and batch: 1100, loss is 3.7880845880508422 and perplexity is 44.17171217026486
At time: 689.5760045051575 and batch: 1150, loss is 3.790944595336914 and perplexity is 44.29822441564648
At time: 690.4278094768524 and batch: 1200, loss is 3.801496858596802 and perplexity is 44.76814594900942
At time: 691.2769601345062 and batch: 1250, loss is 3.837819652557373 and perplexity is 46.42414324932173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326598675581661 and perplexity of 75.68641421522392
Finished 30 epochs...
Completing Train Step...
At time: 693.7838115692139 and batch: 50, loss is 3.9231669807434084 and perplexity is 50.56031503552849
At time: 694.6309442520142 and batch: 100, loss is 3.934534878730774 and perplexity is 51.138358885828644
At time: 695.4868950843811 and batch: 150, loss is 3.8586831283569336 and perplexity is 47.40288673158938
At time: 696.3371210098267 and batch: 200, loss is 3.916301999092102 and perplexity is 50.214408081461755
At time: 697.1849851608276 and batch: 250, loss is 3.931596612930298 and perplexity is 50.98832132787917
At time: 698.0317893028259 and batch: 300, loss is 3.928177218437195 and perplexity is 50.814269887574135
At time: 698.8786342144012 and batch: 350, loss is 3.9123537158966064 and perplexity is 50.0165382579598
At time: 699.7266964912415 and batch: 400, loss is 3.914589033126831 and perplexity is 50.12846613827841
At time: 700.5759515762329 and batch: 450, loss is 3.834117064476013 and perplexity is 46.25257159542837
At time: 701.4234714508057 and batch: 500, loss is 3.8689122200012207 and perplexity is 47.89026366614642
At time: 702.279135465622 and batch: 550, loss is 3.853427543640137 and perplexity is 47.15441036079297
At time: 703.128214597702 and batch: 600, loss is 3.873281674385071 and perplexity is 48.099975818854475
At time: 703.9796268939972 and batch: 650, loss is 3.8967917490005495 and perplexity is 49.24420762373806
At time: 704.8266055583954 and batch: 700, loss is 3.865455193519592 and perplexity is 47.72499159599958
At time: 705.6724326610565 and batch: 750, loss is 3.8525803089141846 and perplexity is 47.114476425950414
At time: 706.5203778743744 and batch: 800, loss is 3.8805837488174437 and perplexity is 48.45249090181707
At time: 707.3661212921143 and batch: 850, loss is 3.9190130186080934 and perplexity is 50.35072501720125
At time: 708.2424492835999 and batch: 900, loss is 3.87181746006012 and perplexity is 48.029598681410704
At time: 709.0961620807648 and batch: 950, loss is 3.860015573501587 and perplexity is 47.4660905763344
At time: 709.9428267478943 and batch: 1000, loss is 3.8443530082702635 and perplexity is 46.72844165328637
At time: 710.7904794216156 and batch: 1050, loss is 3.8329866790771483 and perplexity is 46.20031790280657
At time: 711.6389219760895 and batch: 1100, loss is 3.788250374794006 and perplexity is 44.17903586163424
At time: 712.4874274730682 and batch: 1150, loss is 3.791146283149719 and perplexity is 44.3071597286816
At time: 713.3380155563354 and batch: 1200, loss is 3.8017182970046997 and perplexity is 44.77806043365624
At time: 714.1857023239136 and batch: 1250, loss is 3.837993516921997 and perplexity is 46.432215455205124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326577736513458 and perplexity of 75.68482942882666
Finished 31 epochs...
Completing Train Step...
At time: 716.7185306549072 and batch: 50, loss is 3.923040580749512 and perplexity is 50.553924615899604
At time: 717.5973880290985 and batch: 100, loss is 3.934320330619812 and perplexity is 51.127388424419955
At time: 718.445121049881 and batch: 150, loss is 3.858450436592102 and perplexity is 47.39185775344365
At time: 719.2974622249603 and batch: 200, loss is 3.916042699813843 and perplexity is 50.201389209652994
At time: 720.1468181610107 and batch: 250, loss is 3.9313480710983275 and perplexity is 50.97565017180864
At time: 720.995543718338 and batch: 300, loss is 3.92795428276062 and perplexity is 50.80294283658589
At time: 721.8448970317841 and batch: 350, loss is 3.9121410512924193 and perplexity is 50.00590264159802
At time: 722.6972763538361 and batch: 400, loss is 3.9143860816955565 and perplexity is 50.118293526636016
At time: 723.544328212738 and batch: 450, loss is 3.8339529514312742 and perplexity is 46.24498156790515
At time: 724.3964984416962 and batch: 500, loss is 3.8687629079818726 and perplexity is 47.88311360797941
At time: 725.2494459152222 and batch: 550, loss is 3.8533129930496215 and perplexity is 47.149009104605184
At time: 726.0984523296356 and batch: 600, loss is 3.873170742988586 and perplexity is 48.0946403173088
At time: 726.9477367401123 and batch: 650, loss is 3.896705946922302 and perplexity is 49.239982549644964
At time: 727.7993116378784 and batch: 700, loss is 3.8653782320022585 and perplexity is 47.721318749567345
At time: 728.6486325263977 and batch: 750, loss is 3.8525175428390503 and perplexity is 47.11151932798684
At time: 729.5011603832245 and batch: 800, loss is 3.8805633926391603 and perplexity is 48.45150460431265
At time: 730.3789041042328 and batch: 850, loss is 3.919073405265808 and perplexity is 50.353765621003575
At time: 731.2290968894958 and batch: 900, loss is 3.871891279220581 and perplexity is 48.03314431692896
At time: 732.0787410736084 and batch: 950, loss is 3.860067410469055 and perplexity is 47.46855113830094
At time: 732.9254651069641 and batch: 1000, loss is 3.844427080154419 and perplexity is 46.73190304519761
At time: 733.7749953269958 and batch: 1050, loss is 3.8330833101272583 and perplexity is 46.204782503747
At time: 734.6235859394073 and batch: 1100, loss is 3.7883830308914184 and perplexity is 44.18489686885965
At time: 735.4715068340302 and batch: 1150, loss is 3.791316800117493 and perplexity is 44.31471549538411
At time: 736.3238327503204 and batch: 1200, loss is 3.8019064044952393 and perplexity is 44.78648431450836
At time: 737.178591966629 and batch: 1250, loss is 3.8381355810165405 and perplexity is 46.43881227442589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326561252566149 and perplexity of 75.68358185436881
Finished 32 epochs...
Completing Train Step...
At time: 739.6559388637543 and batch: 50, loss is 3.9229269456863403 and perplexity is 50.548180243869524
At time: 740.5023803710938 and batch: 100, loss is 3.9341425704956055 and perplexity is 51.11830082123394
At time: 741.3494160175323 and batch: 150, loss is 3.8582604789733885 and perplexity is 47.38285616398572
At time: 742.2009663581848 and batch: 200, loss is 3.915824599266052 and perplexity is 50.19044145306576
At time: 743.0496370792389 and batch: 250, loss is 3.93114116191864 and perplexity is 50.965103932942
At time: 743.905752658844 and batch: 300, loss is 3.9277619743347167 and perplexity is 50.79317394196821
At time: 744.7565386295319 and batch: 350, loss is 3.9119624423980714 and perplexity is 49.996971940191436
At time: 745.6080858707428 and batch: 400, loss is 3.9142119216918947 and perplexity is 50.10956568449447
At time: 746.4540991783142 and batch: 450, loss is 3.833810815811157 and perplexity is 46.23840897588347
At time: 747.2998526096344 and batch: 500, loss is 3.8686374616622925 and perplexity is 47.87710722435449
At time: 748.1476571559906 and batch: 550, loss is 3.853211832046509 and perplexity is 47.144239704791104
At time: 748.9950108528137 and batch: 600, loss is 3.8730781650543213 and perplexity is 48.090188020954386
At time: 749.8495030403137 and batch: 650, loss is 3.8966394376754763 and perplexity is 49.23670774439549
At time: 750.6971225738525 and batch: 700, loss is 3.8653205537796023 and perplexity is 47.71856634809664
At time: 751.5445737838745 and batch: 750, loss is 3.852474455833435 and perplexity is 47.10948947741941
At time: 752.4488141536713 and batch: 800, loss is 3.8805474853515625 and perplexity is 48.45073387842446
At time: 753.2943613529205 and batch: 850, loss is 3.9191212606430055 and perplexity is 50.35617537711011
At time: 754.1459848880768 and batch: 900, loss is 3.871950969696045 and perplexity is 48.03601152372288
At time: 754.9949150085449 and batch: 950, loss is 3.8601156759262083 and perplexity is 47.470842284913196
At time: 755.8452496528625 and batch: 1000, loss is 3.844493179321289 and perplexity is 46.734992087145564
At time: 756.6996088027954 and batch: 1050, loss is 3.8331715726852416 and perplexity is 46.20886083602121
At time: 757.5466754436493 and batch: 1100, loss is 3.7884962129592896 and perplexity is 44.18989808987488
At time: 758.3973610401154 and batch: 1150, loss is 3.7914645719528197 and perplexity is 44.321264446088165
At time: 759.2488312721252 and batch: 1200, loss is 3.802067937850952 and perplexity is 44.79371940994915
At time: 760.0952055454254 and batch: 1250, loss is 3.838254008293152 and perplexity is 46.44431222215811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326549223739735 and perplexity of 75.6826714751757
Finished 33 epochs...
Completing Train Step...
At time: 762.5550103187561 and batch: 50, loss is 3.9228226852416994 and perplexity is 50.5429103428474
At time: 763.4385890960693 and batch: 100, loss is 3.933988175392151 and perplexity is 51.110409015134
At time: 764.2920501232147 and batch: 150, loss is 3.8580945777893065 and perplexity is 47.37499594407085
At time: 765.1394090652466 and batch: 200, loss is 3.9156333112716677 and perplexity is 50.180841542386034
At time: 765.9908077716827 and batch: 250, loss is 3.9309611558914184 and perplexity is 50.95593073269652
At time: 766.8392775058746 and batch: 300, loss is 3.9275922346115113 and perplexity is 50.78455305435681
At time: 767.6880235671997 and batch: 350, loss is 3.9118061017990113 and perplexity is 49.98915599463784
At time: 768.5372798442841 and batch: 400, loss is 3.9140584754943846 and perplexity is 50.10187715208442
At time: 769.3843495845795 and batch: 450, loss is 3.833683133125305 and perplexity is 46.23250550852922
At time: 770.2332606315613 and batch: 500, loss is 3.8685267686843874 and perplexity is 47.87180785808907
At time: 771.0892465114594 and batch: 550, loss is 3.8531209135055544 and perplexity is 47.13995361414824
At time: 771.9382388591766 and batch: 600, loss is 3.872997312545776 and perplexity is 48.08629996579811
At time: 772.7864880561829 and batch: 650, loss is 3.896583008766174 and perplexity is 49.23392944906867
At time: 773.6377000808716 and batch: 700, loss is 3.8652722215652466 and perplexity is 47.71626005985362
At time: 774.5150866508484 and batch: 750, loss is 3.8524410009384153 and perplexity is 47.107913460757395
At time: 775.3616163730621 and batch: 800, loss is 3.880534443855286 and perplexity is 48.45010201247923
At time: 776.2098631858826 and batch: 850, loss is 3.9191588878631594 and perplexity is 50.35807017565491
At time: 777.0562264919281 and batch: 900, loss is 3.8719992446899414 and perplexity is 48.03833051786027
At time: 777.9113788604736 and batch: 950, loss is 3.8601578855514527 and perplexity is 47.472846053664945
At time: 778.7624757289886 and batch: 1000, loss is 3.844551739692688 and perplexity is 46.73772898577564
At time: 779.610680103302 and batch: 1050, loss is 3.8332510375976563 and perplexity is 46.2125329650011
At time: 780.4652488231659 and batch: 1100, loss is 3.788594813346863 and perplexity is 44.19425544576833
At time: 781.3129909038544 and batch: 1150, loss is 3.7915941429138185 and perplexity is 44.32700756697793
At time: 782.1607565879822 and batch: 1200, loss is 3.8022075510025024 and perplexity is 44.79997363886177
At time: 783.0089573860168 and batch: 1250, loss is 3.838353600502014 and perplexity is 46.4489379441405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326538531449589 and perplexity of 75.68186225841949
Finished 34 epochs...
Completing Train Step...
At time: 785.5032453536987 and batch: 50, loss is 3.922725477218628 and perplexity is 50.53799740524504
At time: 786.3545520305634 and batch: 100, loss is 3.9338501834869386 and perplexity is 51.10335667901167
At time: 787.2023363113403 and batch: 150, loss is 3.8579452896118163 and perplexity is 47.367923945163795
At time: 788.0521218776703 and batch: 200, loss is 3.9154612827301025 and perplexity is 50.17220974787978
At time: 788.9031505584717 and batch: 250, loss is 3.9308001708984377 and perplexity is 50.947728252801
At time: 789.7494413852692 and batch: 300, loss is 3.9274394369125365 and perplexity is 50.776793884313406
At time: 790.608268737793 and batch: 350, loss is 3.911665596961975 and perplexity is 49.98213276983132
At time: 791.4560921192169 and batch: 400, loss is 3.913920531272888 and perplexity is 50.09496636430773
At time: 792.3045506477356 and batch: 450, loss is 3.8335663652420044 and perplexity is 46.227107351893096
At time: 793.1547782421112 and batch: 500, loss is 3.868426365852356 and perplexity is 47.867001634288926
At time: 794.0049557685852 and batch: 550, loss is 3.8530374431610106 and perplexity is 47.13601899019278
At time: 794.8499352931976 and batch: 600, loss is 3.872924418449402 and perplexity is 48.08279488616549
At time: 795.7258722782135 and batch: 650, loss is 3.896532196998596 and perplexity is 49.23142784964444
At time: 796.5732305049896 and batch: 700, loss is 3.8652292680740357 and perplexity is 47.71421052391419
At time: 797.4255292415619 and batch: 750, loss is 3.8524131393432617 and perplexity is 47.106600977428045
At time: 798.2783036231995 and batch: 800, loss is 3.8805227708816528 and perplexity is 48.44953645901678
At time: 799.1279816627502 and batch: 850, loss is 3.9191880512237547 and perplexity is 50.35953880762934
At time: 799.9753658771515 and batch: 900, loss is 3.872037925720215 and perplexity is 48.04018872591578
At time: 800.8232743740082 and batch: 950, loss is 3.8601935577392577 and perplexity is 47.47453954415008
At time: 801.6739778518677 and batch: 1000, loss is 3.8446035861968992 and perplexity is 46.74015223645632
At time: 802.5241770744324 and batch: 1050, loss is 3.833322219848633 and perplexity is 46.2158225942011
At time: 803.3751380443573 and batch: 1100, loss is 3.788681592941284 and perplexity is 44.19809077174326
At time: 804.2308356761932 and batch: 1150, loss is 3.791708245277405 and perplexity is 44.332065671877416
At time: 805.0787341594696 and batch: 1200, loss is 3.802329077720642 and perplexity is 44.80541836346391
At time: 805.9312834739685 and batch: 1250, loss is 3.8384385442733766 and perplexity is 46.45288365968487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326533185304516 and perplexity of 75.68145765328597
Finished 35 epochs...
Completing Train Step...
At time: 808.3808643817902 and batch: 50, loss is 3.9226337575912478 and perplexity is 50.53336229152318
At time: 809.2625389099121 and batch: 100, loss is 3.9337244701385496 and perplexity is 51.0969327087275
At time: 810.1147110462189 and batch: 150, loss is 3.8578085231781007 and perplexity is 47.361446046122985
At time: 810.9628708362579 and batch: 200, loss is 3.9153042602539063 and perplexity is 50.16433220176102
At time: 811.8118963241577 and batch: 250, loss is 3.930653657913208 and perplexity is 50.94026429584066
At time: 812.6674063205719 and batch: 300, loss is 3.9272995138168336 and perplexity is 50.7696895351661
At time: 813.5178949832916 and batch: 350, loss is 3.9115370178222655 and perplexity is 49.97570652334839
At time: 814.3720850944519 and batch: 400, loss is 3.913794527053833 and perplexity is 50.08865458485617
At time: 815.2194383144379 and batch: 450, loss is 3.8334580135345457 and perplexity is 46.2220988372261
At time: 816.0717344284058 and batch: 500, loss is 3.8683334302902224 and perplexity is 47.86255329429209
At time: 816.9252870082855 and batch: 550, loss is 3.852960076332092 and perplexity is 47.132372366941325
At time: 817.8284327983856 and batch: 600, loss is 3.8728572797775267 and perplexity is 48.079566779543384
At time: 818.6771228313446 and batch: 650, loss is 3.896485085487366 and perplexity is 49.229108537311994
At time: 819.5360651016235 and batch: 700, loss is 3.8651898288726807 and perplexity is 47.712328750665904
At time: 820.3866677284241 and batch: 750, loss is 3.8523882627487183 and perplexity is 47.10542914019093
At time: 821.2343616485596 and batch: 800, loss is 3.880511360168457 and perplexity is 48.44898361840593
At time: 822.0851767063141 and batch: 850, loss is 3.919209885597229 and perplexity is 50.360638388611946
At time: 822.9329252243042 and batch: 900, loss is 3.872068905830383 and perplexity is 48.04167703930896
At time: 823.783008813858 and batch: 950, loss is 3.8602235603332518 and perplexity is 47.475963924852536
At time: 824.634200334549 and batch: 1000, loss is 3.8446494913101197 and perplexity is 46.74229789768471
At time: 825.4892020225525 and batch: 1050, loss is 3.833385953903198 and perplexity is 46.218768209827104
At time: 826.3392612934113 and batch: 1100, loss is 3.7887581968307495 and perplexity is 44.20147664708728
At time: 827.1874828338623 and batch: 1150, loss is 3.7918095493316653 and perplexity is 44.336556917350556
At time: 828.0418450832367 and batch: 1200, loss is 3.8024357652664182 and perplexity is 44.81019879858849
At time: 828.8886439800262 and batch: 1250, loss is 3.8385113191604616 and perplexity is 46.45626438606248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326526057111086 and perplexity of 75.68091818313947
Finished 36 epochs...
Completing Train Step...
At time: 831.3889718055725 and batch: 50, loss is 3.922546329498291 and perplexity is 50.52894444915192
At time: 832.2393589019775 and batch: 100, loss is 3.9336077785491943 and perplexity is 51.090970474316606
At time: 833.0896844863892 and batch: 150, loss is 3.8576816368103026 and perplexity is 47.3554369055076
At time: 833.9388298988342 and batch: 200, loss is 3.915159168243408 and perplexity is 50.157054285943055
At time: 834.7973465919495 and batch: 250, loss is 3.9305182600021364 and perplexity is 50.933367557378105
At time: 835.6496922969818 and batch: 300, loss is 3.9271697473526 and perplexity is 50.7631017595103
At time: 836.50212931633 and batch: 350, loss is 3.911417956352234 and perplexity is 49.96975669646846
At time: 837.3493719100952 and batch: 400, loss is 3.9136782693862915 and perplexity is 50.08283173318599
At time: 838.2066423892975 and batch: 450, loss is 3.833356509208679 and perplexity is 46.21740733235153
At time: 839.0540432929993 and batch: 500, loss is 3.868246831893921 and perplexity is 47.85840865339613
At time: 839.9618332386017 and batch: 550, loss is 3.8528870964050292 and perplexity is 47.128932775355786
At time: 840.8108196258545 and batch: 600, loss is 3.872794370651245 and perplexity is 48.07654223114213
At time: 841.6640250682831 and batch: 650, loss is 3.8964403533935545 and perplexity is 49.22690646546267
At time: 842.5179381370544 and batch: 700, loss is 3.865152521133423 and perplexity is 47.710548744749694
At time: 843.3670027256012 and batch: 750, loss is 3.852364950180054 and perplexity is 47.104331004439864
At time: 844.2141442298889 and batch: 800, loss is 3.8804995012283325 and perplexity is 48.4484090682169
At time: 845.0619604587555 and batch: 850, loss is 3.919225797653198 and perplexity is 50.36143973628416
At time: 845.9085993766785 and batch: 900, loss is 3.8720934629440307 and perplexity is 48.04285681871775
At time: 846.7624394893646 and batch: 950, loss is 3.860248851776123 and perplexity is 47.47716467566619
At time: 847.6103432178497 and batch: 1000, loss is 3.844689955711365 and perplexity is 46.74418933504965
At time: 848.4653429985046 and batch: 1050, loss is 3.8334432363510134 and perplexity is 46.22141580983495
At time: 849.3141458034515 and batch: 1100, loss is 3.788826389312744 and perplexity is 44.20449095826318
At time: 850.1644897460938 and batch: 1150, loss is 3.7919001054763792 and perplexity is 44.34057204680946
At time: 851.013664484024 and batch: 1200, loss is 3.802530121803284 and perplexity is 44.81442713324575
At time: 851.8643515110016 and batch: 1250, loss is 3.838574447631836 and perplexity is 46.45919719158972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326520265453923 and perplexity of 75.68047986647687
Finished 37 epochs...
Completing Train Step...
At time: 854.3549015522003 and batch: 50, loss is 3.922462124824524 and perplexity is 50.52468985499966
At time: 855.2051017284393 and batch: 100, loss is 3.9334984111785887 and perplexity is 51.085383094758214
At time: 856.0601797103882 and batch: 150, loss is 3.8575625658035277 and perplexity is 47.349798581646105
At time: 856.9127435684204 and batch: 200, loss is 3.915023756027222 and perplexity is 50.150262867895684
At time: 857.7608876228333 and batch: 250, loss is 3.9303919076919556 and perplexity is 50.92693241527803
At time: 858.6116054058075 and batch: 300, loss is 3.9270484542846678 and perplexity is 50.75694492055863
At time: 859.4581792354584 and batch: 350, loss is 3.9113065671920775 and perplexity is 49.964190917226304
At time: 860.3087410926819 and batch: 400, loss is 3.9135697269439698 and perplexity is 50.07739591532509
At time: 861.1599013805389 and batch: 450, loss is 3.833260612487793 and perplexity is 46.21297544704554
At time: 862.0422332286835 and batch: 500, loss is 3.868164930343628 and perplexity is 47.85448913604233
At time: 862.8939356803894 and batch: 550, loss is 3.8528178691864015 and perplexity is 47.12567028335078
At time: 863.7420279979706 and batch: 600, loss is 3.8727344512939452 and perplexity is 48.07366160193403
At time: 864.5945067405701 and batch: 650, loss is 3.8963972759246825 and perplexity is 49.224785940605486
At time: 865.4417796134949 and batch: 700, loss is 3.8651168060302736 and perplexity is 47.70884478800865
At time: 866.2904849052429 and batch: 750, loss is 3.852342505455017 and perplexity is 47.10327377254711
At time: 867.1448497772217 and batch: 800, loss is 3.8804868268966675 and perplexity is 48.447795020903044
At time: 867.9935004711151 and batch: 850, loss is 3.9192367792129517 and perplexity is 50.36199278648057
At time: 868.8444294929504 and batch: 900, loss is 3.8721128416061403 and perplexity is 48.04378783402771
At time: 869.6926758289337 and batch: 950, loss is 3.860269627571106 and perplexity is 47.47815106175231
At time: 870.5426285266876 and batch: 1000, loss is 3.8447260570526125 and perplexity is 46.745876893441555
At time: 871.3937735557556 and batch: 1050, loss is 3.833494634628296 and perplexity is 46.223791572035665
At time: 872.2572138309479 and batch: 1100, loss is 3.7888873386383057 and perplexity is 44.20718527428142
At time: 873.1052823066711 and batch: 1150, loss is 3.791981554031372 and perplexity is 44.34418366940896
At time: 873.9508934020996 and batch: 1200, loss is 3.8026141595840453 and perplexity is 44.818193396500114
At time: 874.8057427406311 and batch: 1250, loss is 3.8386294269561767 and perplexity is 46.461751557078735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326515810333029 and perplexity of 75.68014270154077
Finished 38 epochs...
Completing Train Step...
At time: 877.292813539505 and batch: 50, loss is 3.9223809480667113 and perplexity is 50.52058859095366
At time: 878.1712734699249 and batch: 100, loss is 3.9333949279785156 and perplexity is 51.08009688936006
At time: 879.0186898708344 and batch: 150, loss is 3.8574501991271974 and perplexity is 47.34447834106905
At time: 879.8661253452301 and batch: 200, loss is 3.9148964881896973 and perplexity is 50.14388075851644
At time: 880.7138786315918 and batch: 250, loss is 3.9302729749679566 and perplexity is 50.92087589664724
At time: 881.5672628879547 and batch: 300, loss is 3.9269340372085573 and perplexity is 50.75113779155221
At time: 882.416512966156 and batch: 350, loss is 3.911201486587524 and perplexity is 49.958940925679684
At time: 883.263097524643 and batch: 400, loss is 3.9134676837921143 and perplexity is 50.07228612072339
At time: 884.1422979831696 and batch: 450, loss is 3.833169298171997 and perplexity is 46.20875573347476
At time: 884.9914450645447 and batch: 500, loss is 3.8680870008468626 and perplexity is 47.85076000509253
At time: 885.8402016162872 and batch: 550, loss is 3.8527516078948976 and perplexity is 47.12254777902655
At time: 886.6859328746796 and batch: 600, loss is 3.8726769876480103 and perplexity is 48.07089919343473
At time: 887.5328323841095 and batch: 650, loss is 3.896355447769165 and perplexity is 49.22272700166496
At time: 888.3792200088501 and batch: 700, loss is 3.8650822257995605 and perplexity is 47.70719503367343
At time: 889.2340557575226 and batch: 750, loss is 3.8523204708099366 and perplexity is 47.102235880062246
At time: 890.0824410915375 and batch: 800, loss is 3.880473256111145 and perplexity is 48.44713755072898
At time: 890.9313676357269 and batch: 850, loss is 3.9192436218261717 and perplexity is 50.36233739529721
At time: 891.7799160480499 and batch: 900, loss is 3.8721278381347655 and perplexity is 48.04450832946968
At time: 892.6290001869202 and batch: 950, loss is 3.860286874771118 and perplexity is 47.47896993398149
At time: 893.4818825721741 and batch: 1000, loss is 3.8447579193115233 and perplexity is 46.747366346402686
At time: 894.3399188518524 and batch: 1050, loss is 3.8335409641265867 and perplexity is 46.22593314671695
At time: 895.1913805007935 and batch: 1100, loss is 3.7889419317245485 and perplexity is 44.20959874683851
At time: 896.0414798259735 and batch: 1150, loss is 3.792055244445801 and perplexity is 44.34745153108462
At time: 896.8886194229126 and batch: 1200, loss is 3.8026896572113036 and perplexity is 44.82157719149219
At time: 897.7377026081085 and batch: 1250, loss is 3.838678035736084 and perplexity is 46.4640100610254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326510909700046 and perplexity of 75.67977182184607
Finished 39 epochs...
Completing Train Step...
At time: 900.2137448787689 and batch: 50, loss is 3.922302188873291 and perplexity is 50.51660978683087
At time: 901.0679285526276 and batch: 100, loss is 3.9332962083816527 and perplexity is 51.075054531681275
At time: 901.9180090427399 and batch: 150, loss is 3.857343301773071 and perplexity is 47.33941761209599
At time: 902.7672617435455 and batch: 200, loss is 3.9147761154174803 and perplexity is 50.137845163847736
At time: 903.6181390285492 and batch: 250, loss is 3.9301602602005006 and perplexity is 50.9151366854149
At time: 904.4680693149567 and batch: 300, loss is 3.926825637817383 and perplexity is 50.74563669727723
At time: 905.3517959117889 and batch: 350, loss is 3.9111017894744875 and perplexity is 49.953960411774574
At time: 906.1991548538208 and batch: 400, loss is 3.9133710527420043 and perplexity is 50.06744781690308
At time: 907.2273960113525 and batch: 450, loss is 3.833082022666931 and perplexity is 46.20472301696089
At time: 908.0862867832184 and batch: 500, loss is 3.8680125188827517 and perplexity is 47.847196119227405
At time: 908.9346299171448 and batch: 550, loss is 3.85268807888031 and perplexity is 47.11955422509106
At time: 909.7845480442047 and batch: 600, loss is 3.8726214838027953 and perplexity is 48.068231147730636
At time: 910.635466337204 and batch: 650, loss is 3.896314549446106 and perplexity is 49.220713915840385
At time: 911.484973192215 and batch: 700, loss is 3.865048232078552 and perplexity is 47.70557331615963
At time: 912.3330857753754 and batch: 750, loss is 3.852298393249512 and perplexity is 47.10119598908263
At time: 913.1830551624298 and batch: 800, loss is 3.880458769798279 and perplexity is 48.44643573542032
At time: 914.0374276638031 and batch: 850, loss is 3.91924711227417 and perplexity is 50.362513182723745
At time: 914.8938493728638 and batch: 900, loss is 3.87213933467865 and perplexity is 48.04506067844314
At time: 915.7420241832733 and batch: 950, loss is 3.860300884246826 and perplexity is 47.479635094116674
At time: 916.5944368839264 and batch: 1000, loss is 3.844786286354065 and perplexity is 46.74869244974127
At time: 917.4500105381012 and batch: 1050, loss is 3.8335827589035034 and perplexity is 46.22786518965495
At time: 918.3011255264282 and batch: 1100, loss is 3.7889911794662474 and perplexity is 44.211776023350744
At time: 919.1496679782867 and batch: 1150, loss is 3.792122273445129 and perplexity is 44.350424196009776
At time: 919.9984059333801 and batch: 1200, loss is 3.8027577924728395 and perplexity is 44.824631225419104
At time: 920.8480432033539 and batch: 1250, loss is 3.8387211847305296 and perplexity is 46.46601497959224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.32650779111542 and perplexity of 75.67953580844117
Finished 40 epochs...
Completing Train Step...
At time: 923.4099831581116 and batch: 50, loss is 3.922225308418274 and perplexity is 50.512726196172565
At time: 924.2609720230103 and batch: 100, loss is 3.933201718330383 and perplexity is 51.07022867516128
At time: 925.1119060516357 and batch: 150, loss is 3.857241168022156 and perplexity is 47.33458290670662
At time: 925.9622535705566 and batch: 200, loss is 3.9146617078781127 and perplexity is 50.1321093444701
At time: 926.8109102249146 and batch: 250, loss is 3.9300527477264406 and perplexity is 50.90966296735451
At time: 927.7211928367615 and batch: 300, loss is 3.9267222023010255 and perplexity is 50.7403880675946
At time: 928.5814204216003 and batch: 350, loss is 3.9110063982009886 and perplexity is 49.949195467145266
At time: 929.4314765930176 and batch: 400, loss is 3.9132790851593016 and perplexity is 50.062843446484926
At time: 930.2825214862823 and batch: 450, loss is 3.832998113632202 and perplexity is 46.20084618590547
At time: 931.1348011493683 and batch: 500, loss is 3.8679409551620485 and perplexity is 47.84377211836648
At time: 931.9836874008179 and batch: 550, loss is 3.8526262617111207 and perplexity is 47.11664151766399
At time: 932.8381533622742 and batch: 600, loss is 3.8725674247741697 and perplexity is 48.06563269608256
At time: 933.6928808689117 and batch: 650, loss is 3.896274404525757 and perplexity is 49.2187379938626
At time: 934.5493977069855 and batch: 700, loss is 3.865014910697937 and perplexity is 47.70398372707749
At time: 935.3971831798553 and batch: 750, loss is 3.85227632522583 and perplexity is 47.10015657024311
At time: 936.2551980018616 and batch: 800, loss is 3.8804432582855224 and perplexity is 48.445684263742656
At time: 937.1047794818878 and batch: 850, loss is 3.9192476654052735 and perplexity is 50.36254103980394
At time: 937.9547863006592 and batch: 900, loss is 3.8721478843688963 and perplexity is 48.0454714505858
At time: 938.8090407848358 and batch: 950, loss is 3.860312275886536 and perplexity is 47.48017596809393
At time: 939.6612825393677 and batch: 1000, loss is 3.8448115587234497 and perplexity is 46.74987391489426
At time: 940.513388633728 and batch: 1050, loss is 3.8336204957962035 and perplexity is 46.22960971855971
At time: 941.3630652427673 and batch: 1100, loss is 3.78903573513031 and perplexity is 44.21374595227627
At time: 942.2182877063751 and batch: 1150, loss is 3.7921835660934446 and perplexity is 44.353142634271954
At time: 943.0710496902466 and batch: 1200, loss is 3.8028197383880613 and perplexity is 44.82740801422932
At time: 943.9199829101562 and batch: 1250, loss is 3.8387596940994264 and perplexity is 46.46780439095858
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326505118042883 and perplexity of 75.6793335118228
Finished 41 epochs...
Completing Train Step...
At time: 946.4346024990082 and batch: 50, loss is 3.9221503019332884 and perplexity is 50.50893755622161
At time: 947.2836134433746 and batch: 100, loss is 3.933110361099243 and perplexity is 51.065563253589076
At time: 948.1342408657074 and batch: 150, loss is 3.8571430253982544 and perplexity is 47.32993759449417
At time: 948.9912488460541 and batch: 200, loss is 3.914552330970764 and perplexity is 50.12662634925314
At time: 949.8408081531525 and batch: 250, loss is 3.9299500036239623 and perplexity is 50.90443256842638
At time: 950.6889359951019 and batch: 300, loss is 3.926623387336731 and perplexity is 50.73537440567589
At time: 951.5369307994843 and batch: 350, loss is 3.910915021896362 and perplexity is 49.94463150276669
At time: 952.3851916790009 and batch: 400, loss is 3.913190932273865 and perplexity is 50.058430456893696
At time: 953.2341859340668 and batch: 450, loss is 3.8329172945022583 and perplexity is 46.19711242459576
At time: 954.0841376781464 and batch: 500, loss is 3.867871923446655 and perplexity is 47.840469494700464
At time: 954.933541059494 and batch: 550, loss is 3.8525664043426513 and perplexity is 47.11382132389716
At time: 955.7905476093292 and batch: 600, loss is 3.872514772415161 and perplexity is 48.06310199375817
At time: 956.6410005092621 and batch: 650, loss is 3.8962347078323365 and perplexity is 49.216784211489525
At time: 957.4893367290497 and batch: 700, loss is 3.864982175827026 and perplexity is 47.7024221688871
At time: 958.3376371860504 and batch: 750, loss is 3.852253866195679 and perplexity is 47.09909875828533
At time: 959.1883840560913 and batch: 800, loss is 3.880426969528198 and perplexity is 48.444895150175135
At time: 960.0379185676575 and batch: 850, loss is 3.9192459630966185 and perplexity is 50.36245530728742
At time: 960.8885962963104 and batch: 900, loss is 3.8721538591384888 and perplexity is 48.045758512065234
At time: 961.7391493320465 and batch: 950, loss is 3.860321168899536 and perplexity is 47.48059821179358
At time: 962.5886845588684 and batch: 1000, loss is 3.8448339891433716 and perplexity is 46.75092254595804
At time: 963.4370994567871 and batch: 1050, loss is 3.833654832839966 and perplexity is 46.23119713394516
At time: 964.2929248809814 and batch: 1100, loss is 3.789076256752014 and perplexity is 44.215537601263875
At time: 965.1423304080963 and batch: 1150, loss is 3.7922399282455443 and perplexity is 44.355642543292674
At time: 966.0485162734985 and batch: 1200, loss is 3.8028762912750245 and perplexity is 44.82994320525311
At time: 966.8988137245178 and batch: 1250, loss is 3.8387943506240845 and perplexity is 46.46941483147324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326502890482436 and perplexity of 75.67916493172056
Finished 42 epochs...
Completing Train Step...
At time: 969.3866457939148 and batch: 50, loss is 3.922076873779297 and perplexity is 50.50522891433782
At time: 970.2634460926056 and batch: 100, loss is 3.9330221223831177 and perplexity is 51.06105749264356
At time: 971.1162927150726 and batch: 150, loss is 3.8570486211776736 and perplexity is 47.3254696595247
At time: 971.9733107089996 and batch: 200, loss is 3.914447422027588 and perplexity is 50.12136789369218
At time: 972.8215112686157 and batch: 250, loss is 3.929851002693176 and perplexity is 50.89939323167459
At time: 973.6720457077026 and batch: 300, loss is 3.926528148651123 and perplexity is 50.73054266539163
At time: 974.521336555481 and batch: 350, loss is 3.910826826095581 and perplexity is 49.94022679023801
At time: 975.3695664405823 and batch: 400, loss is 3.913106331825256 and perplexity is 50.05419567035534
At time: 976.2202205657959 and batch: 450, loss is 3.8328389978408812 and perplexity is 46.19349548652659
At time: 977.0703990459442 and batch: 500, loss is 3.8678050947189333 and perplexity is 47.83727248381779
At time: 977.9271893501282 and batch: 550, loss is 3.8525081157684324 and perplexity is 47.111075206460605
At time: 978.7773230075836 and batch: 600, loss is 3.8724631166458128 and perplexity is 48.06061932137015
At time: 979.6252648830414 and batch: 650, loss is 3.89619544506073 and perplexity is 49.21485186206677
At time: 980.4728453159332 and batch: 700, loss is 3.8649495744705202 and perplexity is 47.70086703056572
At time: 981.3207275867462 and batch: 750, loss is 3.8522313117980955 and perplexity is 47.0980364784657
At time: 982.1683690547943 and batch: 800, loss is 3.8804098558425903 and perplexity is 48.44406608656442
At time: 983.0193996429443 and batch: 850, loss is 3.919242334365845 and perplexity is 50.36227255582759
At time: 983.8710107803345 and batch: 900, loss is 3.872157645225525 and perplexity is 48.045940417833044
At time: 984.7259984016418 and batch: 950, loss is 3.860328116416931 and perplexity is 47.480928085221485
At time: 985.5763649940491 and batch: 1000, loss is 3.8448538398742675 and perplexity is 46.751850595151836
At time: 986.4256269931793 and batch: 1050, loss is 3.833685865402222 and perplexity is 46.232631828709394
At time: 987.2763025760651 and batch: 1100, loss is 3.7891131162643434 and perplexity is 44.21716739445375
At time: 988.1814646720886 and batch: 1150, loss is 3.7922918128967287 and perplexity is 44.35794398003819
At time: 989.0290882587433 and batch: 1200, loss is 3.8029282236099244 and perplexity is 44.832271389330685
At time: 989.8784408569336 and batch: 1250, loss is 3.838825807571411 and perplexity is 46.47087664039979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326500217409899 and perplexity of 75.67896263609356
Finished 43 epochs...
Completing Train Step...
At time: 992.3825523853302 and batch: 50, loss is 3.922004709243774 and perplexity is 50.50158435945713
At time: 993.2328681945801 and batch: 100, loss is 3.9329362964630126 and perplexity is 51.05667531845747
At time: 994.0827658176422 and batch: 150, loss is 3.8569571447372435 and perplexity is 47.321140692020855
At time: 994.9324467182159 and batch: 200, loss is 3.9143463277816775 and perplexity is 50.11630116791369
At time: 995.7797315120697 and batch: 250, loss is 3.9297555446624757 and perplexity is 50.8945347077291
At time: 996.6300644874573 and batch: 300, loss is 3.9264363384246828 and perplexity is 50.72588529658239
At time: 997.4806697368622 and batch: 350, loss is 3.910741715431213 and perplexity is 49.93597652523122
At time: 998.3294370174408 and batch: 400, loss is 3.9130246448516846 and perplexity is 50.05010706159178
At time: 999.1852984428406 and batch: 450, loss is 3.83276309967041 and perplexity is 46.189989617777684
At time: 1000.036514043808 and batch: 500, loss is 3.867740125656128 and perplexity is 47.83416464201524
At time: 1000.8869385719299 and batch: 550, loss is 3.852451195716858 and perplexity is 47.108393717946086
At time: 1001.7391803264618 and batch: 600, loss is 3.872412333488464 and perplexity is 48.0581787133483
At time: 1002.5891618728638 and batch: 650, loss is 3.896156668663025 and perplexity is 49.21294352439743
At time: 1003.4369430541992 and batch: 700, loss is 3.8649174880981447 and perplexity is 47.69933650733814
At time: 1004.2878220081329 and batch: 750, loss is 3.852208385467529 and perplexity is 47.09695670569003
At time: 1005.1428053379059 and batch: 800, loss is 3.880391993522644 and perplexity is 48.44320077088478
At time: 1005.9978992938995 and batch: 850, loss is 3.9192370271682737 and perplexity is 50.36200527400626
At time: 1006.8517422676086 and batch: 900, loss is 3.8721596813201904 and perplexity is 48.04603824401561
At time: 1007.7059283256531 and batch: 950, loss is 3.860333261489868 and perplexity is 47.48117237868806
At time: 1008.5576415061951 and batch: 1000, loss is 3.8448715209960938 and perplexity is 46.75267722762568
At time: 1009.4633803367615 and batch: 1050, loss is 3.833714122772217 and perplexity is 46.233938259750886
At time: 1010.3244926929474 and batch: 1100, loss is 3.789147129058838 and perplexity is 44.21867136945853
At time: 1011.1811439990997 and batch: 1150, loss is 3.7923399782180787 and perplexity is 44.3600805461182
At time: 1012.0291576385498 and batch: 1200, loss is 3.802976007461548 and perplexity is 44.834413699118194
At time: 1012.8796904087067 and batch: 1250, loss is 3.838854441642761 and perplexity is 46.47220730984836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326497544337363 and perplexity of 75.67876034100738
Finished 44 epochs...
Completing Train Step...
At time: 1015.4260172843933 and batch: 50, loss is 3.9219336795806883 and perplexity is 50.49799737632738
At time: 1016.2781667709351 and batch: 100, loss is 3.9328528451919555 and perplexity is 51.05241475178352
At time: 1017.1270568370819 and batch: 150, loss is 3.856868405342102 and perplexity is 47.316941628932355
At time: 1017.9749913215637 and batch: 200, loss is 3.91424870967865 and perplexity is 50.11140914844163
At time: 1018.8442268371582 and batch: 250, loss is 3.9296631050109863 and perplexity is 50.88983025212047
At time: 1019.6995921134949 and batch: 300, loss is 3.9263476848602297 and perplexity is 50.72138846537381
At time: 1020.5564408302307 and batch: 350, loss is 3.910659074783325 and perplexity is 49.93184995429185
At time: 1021.4089477062225 and batch: 400, loss is 3.9129456377029417 and perplexity is 50.046152901544076
At time: 1022.2633883953094 and batch: 450, loss is 3.8326892137527464 and perplexity is 46.18657695408335
At time: 1023.1157348155975 and batch: 500, loss is 3.8676770973205565 and perplexity is 47.83114982924471
At time: 1023.9668090343475 and batch: 550, loss is 3.8523952388763427 and perplexity is 47.10575775482265
At time: 1024.8189225196838 and batch: 600, loss is 3.872362356185913 and perplexity is 48.05577695522788
At time: 1025.6695709228516 and batch: 650, loss is 3.896117992401123 and perplexity is 49.21104018851191
At time: 1026.5205805301666 and batch: 700, loss is 3.864885563850403 and perplexity is 47.697813766208675
At time: 1027.3759684562683 and batch: 750, loss is 3.8521851921081542 and perplexity is 47.095864381715074
At time: 1028.2287831306458 and batch: 800, loss is 3.8803736352920533 and perplexity is 48.442311447597696
At time: 1029.080106973648 and batch: 850, loss is 3.919230160713196 and perplexity is 50.36165946674665
At time: 1029.9354009628296 and batch: 900, loss is 3.87216016292572 and perplexity is 48.04606138325888
At time: 1030.786468744278 and batch: 950, loss is 3.8603369331359865 and perplexity is 47.48134671307036
At time: 1031.6939806938171 and batch: 1000, loss is 3.8448872137069703 and perplexity is 46.75341090962883
At time: 1032.5467557907104 and batch: 1050, loss is 3.8337399196624755 and perplexity is 46.2351309669664
At time: 1033.3990619182587 and batch: 1100, loss is 3.789177989959717 and perplexity is 44.22003601854969
At time: 1034.2558891773224 and batch: 1150, loss is 3.7923846101760863 and perplexity is 44.3620604675539
At time: 1035.1074178218842 and batch: 1200, loss is 3.803020329475403 and perplexity is 44.83640089466128
At time: 1035.9551260471344 and batch: 1250, loss is 3.838880581855774 and perplexity is 46.47342211912423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326497544337363 and perplexity of 75.67876034100738
Finished 45 epochs...
Completing Train Step...
At time: 1038.4557304382324 and batch: 50, loss is 3.921863760948181 and perplexity is 50.49446674883623
At time: 1039.3395931720734 and batch: 100, loss is 3.9327713203430177 and perplexity is 51.04825288103322
At time: 1040.1904973983765 and batch: 150, loss is 3.8567821550369263 and perplexity is 47.31286070426991
At time: 1041.0476324558258 and batch: 200, loss is 3.9141542100906372 and perplexity is 50.10667386466708
At time: 1041.8949909210205 and batch: 250, loss is 3.9295734119415284 and perplexity is 50.885265991735295
At time: 1042.7469775676727 and batch: 300, loss is 3.9262616205215455 and perplexity is 50.71702335046145
At time: 1043.5949172973633 and batch: 350, loss is 3.9105788183212282 and perplexity is 49.92784276147227
At time: 1044.4439506530762 and batch: 400, loss is 3.9128688764572144 and perplexity is 50.04231144394292
At time: 1045.2950809001923 and batch: 450, loss is 3.8326171159744264 and perplexity is 46.18324712453485
At time: 1046.1474194526672 and batch: 500, loss is 3.8676154613494873 and perplexity is 47.82820180073086
At time: 1047.0059187412262 and batch: 550, loss is 3.8523404169082642 and perplexity is 47.10317539526036
At time: 1047.8551790714264 and batch: 600, loss is 3.872312994003296 and perplexity is 48.05340487573601
At time: 1048.7059404850006 and batch: 650, loss is 3.896079683303833 and perplexity is 49.20915499409587
At time: 1049.552231311798 and batch: 700, loss is 3.864853982925415 and perplexity is 47.696307448915604
At time: 1050.4055407047272 and batch: 750, loss is 3.8521615839004517 and perplexity is 47.0947525458911
At time: 1051.2581419944763 and batch: 800, loss is 3.880354475975037 and perplexity is 48.441383334886694
At time: 1052.1167917251587 and batch: 850, loss is 3.919222187995911 and perplexity is 50.361257949074314
At time: 1052.966481924057 and batch: 900, loss is 3.8721589756011965 and perplexity is 48.046004337025806
At time: 1053.8735954761505 and batch: 950, loss is 3.8603391218185426 and perplexity is 47.48145063477937
At time: 1054.7316043376923 and batch: 1000, loss is 3.844900813102722 and perplexity is 46.75404673208993
At time: 1055.583794593811 and batch: 1050, loss is 3.833763303756714 and perplexity is 46.2362121462672
At time: 1056.4465401172638 and batch: 1100, loss is 3.7892065763473513 and perplexity is 44.221300127708595
At time: 1057.3048853874207 and batch: 1150, loss is 3.7924261474609375 and perplexity is 44.36390318536662
At time: 1058.1504802703857 and batch: 1200, loss is 3.8030612659454346 and perplexity is 44.83823637621164
At time: 1059.0055587291718 and batch: 1250, loss is 3.838904504776001 and perplexity is 46.47453391239288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326496207801095 and perplexity of 75.67865919366703
Finished 46 epochs...
Completing Train Step...
At time: 1061.5633418560028 and batch: 50, loss is 3.921794681549072 and perplexity is 50.490978741891
At time: 1062.4128589630127 and batch: 100, loss is 3.932691602706909 and perplexity is 51.04418359718507
At time: 1063.2665905952454 and batch: 150, loss is 3.8566981410980223 and perplexity is 47.30888593145182
At time: 1064.1176772117615 and batch: 200, loss is 3.9140623903274534 and perplexity is 50.10207329295383
At time: 1064.9704399108887 and batch: 250, loss is 3.9294860887527467 and perplexity is 50.88082272204995
At time: 1065.8296239376068 and batch: 300, loss is 3.9261780071258543 and perplexity is 50.71278290520126
At time: 1066.679574728012 and batch: 350, loss is 3.9105006504058837 and perplexity is 49.92394015861711
At time: 1067.5364022254944 and batch: 400, loss is 3.912794284820557 and perplexity is 50.03857884524224
At time: 1068.3867704868317 and batch: 450, loss is 3.832546715736389 and perplexity is 46.179995927387814
At time: 1069.2394952774048 and batch: 500, loss is 3.8675550985336304 and perplexity is 47.825314842926126
At time: 1070.088790178299 and batch: 550, loss is 3.8522864961624146 and perplexity is 47.10063562538492
At time: 1070.9405221939087 and batch: 600, loss is 3.8722642707824706 and perplexity is 48.051063616116146
At time: 1071.7896354198456 and batch: 650, loss is 3.8960417127609253 and perplexity is 49.20728653123821
At time: 1072.640124797821 and batch: 700, loss is 3.8648225927352904 and perplexity is 47.694810276254934
At time: 1073.4897747039795 and batch: 750, loss is 3.8521378898620604 and perplexity is 47.09363669423581
At time: 1074.3481931686401 and batch: 800, loss is 3.880335030555725 and perplexity is 48.440441381034084
At time: 1075.1970484256744 and batch: 850, loss is 3.9192131614685057 and perplexity is 50.36080336385095
At time: 1076.1083028316498 and batch: 900, loss is 3.872156720161438 and perplexity is 48.04589597227959
At time: 1076.9570727348328 and batch: 950, loss is 3.8603402471542356 and perplexity is 47.4815040673806
At time: 1077.8103482723236 and batch: 1000, loss is 3.8449129915237426 and perplexity is 46.7546161260226
At time: 1078.6578159332275 and batch: 1050, loss is 3.833784861564636 and perplexity is 46.23720890839167
At time: 1079.5098068714142 and batch: 1100, loss is 3.7892328548431395 and perplexity is 44.2224622122266
At time: 1080.3576622009277 and batch: 1150, loss is 3.792465100288391 and perplexity is 44.36563131849017
At time: 1081.2089326381683 and batch: 1200, loss is 3.8030995988845824 and perplexity is 44.83995519054154
At time: 1082.0619051456451 and batch: 1250, loss is 3.838926591873169 and perplexity is 46.475560411275396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326493980240648 and perplexity of 75.67849061506688
Finished 47 epochs...
Completing Train Step...
At time: 1084.5422871112823 and batch: 50, loss is 3.921726598739624 and perplexity is 50.48754129122345
At time: 1085.4273459911346 and batch: 100, loss is 3.932613344192505 and perplexity is 51.04018911151108
At time: 1086.2802531719208 and batch: 150, loss is 3.8566160821914672 and perplexity is 47.305003975278666
At time: 1087.1278853416443 and batch: 200, loss is 3.9139731407165526 and perplexity is 50.09760190194503
At time: 1087.976586818695 and batch: 250, loss is 3.929401078224182 and perplexity is 50.87649750026385
At time: 1088.8300566673279 and batch: 300, loss is 3.9260965728759767 and perplexity is 50.70865331591345
At time: 1089.6896765232086 and batch: 350, loss is 3.9104242753982543 and perplexity is 49.92012736290961
At time: 1090.539806842804 and batch: 400, loss is 3.912721586227417 and perplexity is 50.03494124318336
At time: 1091.3903863430023 and batch: 450, loss is 3.832477903366089 and perplexity is 46.17681828173952
At time: 1092.248150587082 and batch: 500, loss is 3.8674958896636964 and perplexity is 47.82248324390875
At time: 1093.0992848873138 and batch: 550, loss is 3.8522332906723022 and perplexity is 47.09812967964749
At time: 1093.954915046692 and batch: 600, loss is 3.8722160243988037 and perplexity is 48.04874538198898
At time: 1094.7983746528625 and batch: 650, loss is 3.8960039472579955 and perplexity is 49.20542822840464
At time: 1095.6603841781616 and batch: 700, loss is 3.8647912883758546 and perplexity is 47.693317244140154
At time: 1096.5085666179657 and batch: 750, loss is 3.8521138620376587 and perplexity is 47.09250515019721
At time: 1097.41508603096 and batch: 800, loss is 3.880315008163452 and perplexity is 48.4394714972246
At time: 1098.2647886276245 and batch: 850, loss is 3.919203200340271 and perplexity is 50.36030171592913
At time: 1099.1137418746948 and batch: 900, loss is 3.8721533966064454 and perplexity is 48.04573628936752
At time: 1099.9615986347198 and batch: 950, loss is 3.8603401517868043 and perplexity is 47.48149953919174
At time: 1100.8091886043549 and batch: 1000, loss is 3.844923567771912 and perplexity is 46.75511061706073
At time: 1101.6577289104462 and batch: 1050, loss is 3.8338043594360354 and perplexity is 46.23811044433381
At time: 1102.5049958229065 and batch: 1100, loss is 3.7892570972442625 and perplexity is 44.223534283888945
At time: 1103.3550662994385 and batch: 1150, loss is 3.792501459121704 and perplexity is 44.36724443040936
At time: 1104.2032787799835 and batch: 1200, loss is 3.803135323524475 and perplexity is 44.84155711040735
At time: 1105.0531933307648 and batch: 1250, loss is 3.838947033882141 and perplexity is 46.47651047480888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.32649219819229 and perplexity of 75.67835575245712
Finished 48 epochs...
Completing Train Step...
At time: 1107.552559375763 and batch: 50, loss is 3.9216591548919677 and perplexity is 50.484136332003125
At time: 1108.398496389389 and batch: 100, loss is 3.9325366735458376 and perplexity is 51.036275977219056
At time: 1109.244511604309 and batch: 150, loss is 3.8565356969833373 and perplexity is 47.30120150552173
At time: 1110.0912153720856 and batch: 200, loss is 3.9138860177993773 and perplexity is 50.09323744284881
At time: 1110.9387440681458 and batch: 250, loss is 3.9293180179595946 and perplexity is 50.872271860413996
At time: 1111.7862582206726 and batch: 300, loss is 3.9260171699523925 and perplexity is 50.70462706043949
At time: 1112.6385436058044 and batch: 350, loss is 3.9103496217727662 and perplexity is 49.9164007835202
At time: 1113.4867441654205 and batch: 400, loss is 3.912650632858276 and perplexity is 50.03139122147188
At time: 1114.337546825409 and batch: 450, loss is 3.8324104595184325 and perplexity is 46.173704044461324
At time: 1115.1830887794495 and batch: 500, loss is 3.8674379205703735 and perplexity is 47.81971109826481
At time: 1116.0306086540222 and batch: 550, loss is 3.8521808624267577 and perplexity is 47.09566047206864
At time: 1116.876838684082 and batch: 600, loss is 3.872168264389038 and perplexity is 48.04645062823948
At time: 1117.7260587215424 and batch: 650, loss is 3.8959663200378416 and perplexity is 49.2035767997562
At time: 1118.5808656215668 and batch: 700, loss is 3.8647604322433473 and perplexity is 47.69184563552775
At time: 1119.4892251491547 and batch: 750, loss is 3.85208966255188 and perplexity is 47.091365549577475
At time: 1120.3369188308716 and batch: 800, loss is 3.880294690132141 and perplexity is 48.4384873125244
At time: 1121.1849224567413 and batch: 850, loss is 3.9191924142837524 and perplexity is 50.35975852979795
At time: 1122.0316083431244 and batch: 900, loss is 3.872149095535278 and perplexity is 48.04552964168086
At time: 1122.8782587051392 and batch: 950, loss is 3.860339250564575 and perplexity is 47.481456747828155
At time: 1123.7270996570587 and batch: 1000, loss is 3.84493278503418 and perplexity is 46.755541573163775
At time: 1124.581447839737 and batch: 1050, loss is 3.8338222885131836 and perplexity is 46.23893945841487
At time: 1125.427550792694 and batch: 1100, loss is 3.7892795515060427 and perplexity is 44.2245273018533
At time: 1126.2759251594543 and batch: 1150, loss is 3.7925353336334227 and perplexity is 44.36874737460636
At time: 1127.1260194778442 and batch: 1200, loss is 3.803168730735779 and perplexity is 44.84305516680374
At time: 1127.975397825241 and batch: 1250, loss is 3.8389659309387207 and perplexity is 46.477388752355346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326493089216469 and perplexity of 75.67842318373195
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1130.4504821300507 and batch: 50, loss is 3.9216628742218016 and perplexity is 50.48432409950671
At time: 1131.32866024971 and batch: 100, loss is 3.93261908531189 and perplexity is 51.04048214017135
At time: 1132.1774756908417 and batch: 150, loss is 3.8566775512695313 and perplexity is 47.30791185963241
At time: 1133.0314726829529 and batch: 200, loss is 3.9140676975250246 and perplexity is 50.10233919526112
At time: 1133.8827641010284 and batch: 250, loss is 3.9294428539276125 and perplexity is 50.87862294613068
At time: 1134.7326843738556 and batch: 300, loss is 3.9261152648925783 and perplexity is 50.70960117176172
At time: 1135.5843074321747 and batch: 350, loss is 3.910424394607544 and perplexity is 49.92013331385288
At time: 1136.4333431720734 and batch: 400, loss is 3.91270893573761 and perplexity is 50.03430828067281
At time: 1137.2823226451874 and batch: 450, loss is 3.832452292442322 and perplexity is 46.17563566591071
At time: 1138.1313989162445 and batch: 500, loss is 3.8674859952926637 and perplexity is 47.8220100728567
At time: 1138.9791617393494 and batch: 550, loss is 3.8520622491836547 and perplexity is 47.090074634327685
At time: 1139.8374140262604 and batch: 600, loss is 3.87206392288208 and perplexity is 48.0414376507123
At time: 1140.6890301704407 and batch: 650, loss is 3.8958753967285156 and perplexity is 49.199103251100894
At time: 1141.602287054062 and batch: 700, loss is 3.8647112226486207 and perplexity is 47.68949879687622
At time: 1142.4575831890106 and batch: 750, loss is 3.8520050954818728 and perplexity is 47.08738333915461
At time: 1143.3067405223846 and batch: 800, loss is 3.880128049850464 and perplexity is 48.430416181861055
At time: 1144.15478181839 and batch: 850, loss is 3.91886682510376 and perplexity is 50.34336460630049
At time: 1145.006050825119 and batch: 900, loss is 3.871789746284485 and perplexity is 48.028267618333494
At time: 1145.858335018158 and batch: 950, loss is 3.860034556388855 and perplexity is 47.46699162833313
At time: 1146.711659669876 and batch: 1000, loss is 3.8445725584030153 and perplexity is 46.73870201514532
At time: 1147.5606064796448 and batch: 1050, loss is 3.8334752464294435 and perplexity is 46.2228953846607
At time: 1148.408527135849 and batch: 1100, loss is 3.788863787651062 and perplexity is 44.20614416368458
At time: 1149.2604098320007 and batch: 1150, loss is 3.792114567756653 and perplexity is 44.35008244677385
At time: 1150.1093633174896 and batch: 1200, loss is 3.802789134979248 and perplexity is 44.82603616372757
At time: 1150.9619834423065 and batch: 1250, loss is 3.838663868904114 and perplexity is 46.463351817864826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.326495762289006 and perplexity of 75.67862547791695
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -75.67835575245712
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f15b303a898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'anneal': 5.400371833329866, 'lr': 15.300871416778174, 'dropout': 0.12688207441098331, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.5181779861450195 and batch: 50, loss is 7.46585563659668 and perplexity is 1747.3500053867026
At time: 2.3425192832946777 and batch: 100, loss is 6.572293167114258 and perplexity is 715.007596500833
At time: 3.174654483795166 and batch: 150, loss is 6.088218297958374 and perplexity is 440.63562993837473
At time: 3.9983317852020264 and batch: 200, loss is 5.958021812438965 and perplexity is 386.8441166146574
At time: 4.822812080383301 and batch: 250, loss is 5.934828653335571 and perplexity is 377.97522582852844
At time: 5.645711421966553 and batch: 300, loss is 5.923319320678711 and perplexity is 373.64992164650454
At time: 6.5300962924957275 and batch: 350, loss is 5.949233016967773 and perplexity is 383.4591196065795
At time: 7.356918811798096 and batch: 400, loss is 5.917430944442749 and perplexity is 371.45619540861674
At time: 8.182251214981079 and batch: 450, loss is 5.899511022567749 and perplexity is 364.8590164145383
At time: 9.008217334747314 and batch: 500, loss is 5.90662202835083 and perplexity is 367.4627776991532
At time: 9.83808183670044 and batch: 550, loss is 5.9189918518066404 and perplexity is 372.0364568687722
At time: 10.663644790649414 and batch: 600, loss is 5.936083688735962 and perplexity is 378.44989591899247
At time: 11.496385097503662 and batch: 650, loss is 5.920140466690063 and perplexity is 372.46402899115077
At time: 12.321873664855957 and batch: 700, loss is 5.9567376899719235 and perplexity is 386.3476802040615
At time: 13.148818016052246 and batch: 750, loss is 5.907196388244629 and perplexity is 367.6738942037506
At time: 13.974761486053467 and batch: 800, loss is 5.93874529838562 and perplexity is 379.45852350490014
At time: 14.800116300582886 and batch: 850, loss is 5.965387735366821 and perplexity is 389.70410084331155
At time: 15.62533187866211 and batch: 900, loss is 5.958545379638672 and perplexity is 387.04670853613106
At time: 16.456187963485718 and batch: 950, loss is 5.942138051986694 and perplexity is 380.74811918014467
At time: 17.284205436706543 and batch: 1000, loss is 5.933454761505127 and perplexity is 377.45628531934375
At time: 18.110156059265137 and batch: 1050, loss is 5.947826261520386 and perplexity is 382.9200656486656
At time: 18.936306476593018 and batch: 1100, loss is 5.927248878479004 and perplexity is 375.1210892366525
At time: 19.763204097747803 and batch: 1150, loss is 5.959159088134766 and perplexity is 387.28431529272996
At time: 20.596657276153564 and batch: 1200, loss is 5.9502925682067875 and perplexity is 383.86562951287675
At time: 21.421764135360718 and batch: 1250, loss is 5.968356313705445 and perplexity is 390.8626868213342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.559730696852189 and perplexity of 259.75287463613006
Finished 1 epochs...
Completing Train Step...
At time: 23.94115710258484 and batch: 50, loss is 5.8526684761047365 and perplexity is 348.16220451334993
At time: 24.761357307434082 and batch: 100, loss is 5.82368763923645 and perplexity is 338.21697900367326
At time: 25.580009937286377 and batch: 150, loss is 5.731400833129883 and perplexity is 308.4009842418851
At time: 26.400902032852173 and batch: 200, loss is 5.741767463684082 and perplexity is 311.61469218898475
At time: 27.220945835113525 and batch: 250, loss is 5.749671564102173 and perplexity is 314.0874857381345
At time: 28.046226024627686 and batch: 300, loss is 5.755285224914551 and perplexity is 315.85562457101423
At time: 28.869221448898315 and batch: 350, loss is 5.771932554244995 and perplexity is 321.1577881557375
At time: 29.69545865058899 and batch: 400, loss is 5.734431295394898 and perplexity is 309.3369993500157
At time: 30.516209363937378 and batch: 450, loss is 5.730102338790894 and perplexity is 308.00078719274023
At time: 31.335115671157837 and batch: 500, loss is 5.730639047622681 and perplexity is 308.16613830415577
At time: 32.15575194358826 and batch: 550, loss is 5.727154750823974 and perplexity is 307.09426446269777
At time: 33.01304054260254 and batch: 600, loss is 5.737848949432373 and perplexity is 310.3960148382578
At time: 33.83367991447449 and batch: 650, loss is 5.731269569396972 and perplexity is 308.3605050342443
At time: 34.65582776069641 and batch: 700, loss is 5.782873582839966 and perplexity is 324.69087726966745
At time: 35.47631216049194 and batch: 750, loss is 5.73340256690979 and perplexity is 309.0189391941076
At time: 36.29715609550476 and batch: 800, loss is 5.749415845870971 and perplexity is 314.0071781103388
At time: 37.125800371170044 and batch: 850, loss is 5.774045524597168 and perplexity is 321.8371024732448
At time: 37.94712853431702 and batch: 900, loss is 5.7385780620574955 and perplexity is 310.6224110155902
At time: 38.77175498008728 and batch: 950, loss is 5.719190473556519 and perplexity is 304.6581942392121
At time: 39.595473527908325 and batch: 1000, loss is 5.723240394592285 and perplexity is 305.894537725666
At time: 40.41969966888428 and batch: 1050, loss is 5.739166946411133 and perplexity is 310.8053855634627
At time: 41.241002559661865 and batch: 1100, loss is 5.703205089569092 and perplexity is 299.8268344702753
At time: 42.065725803375244 and batch: 1150, loss is 5.734406747817993 and perplexity is 309.3294059694347
At time: 42.89411807060242 and batch: 1200, loss is 5.74165132522583 and perplexity is 311.57850384053563
At time: 43.71796703338623 and batch: 1250, loss is 5.754812498092651 and perplexity is 315.7063464321016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.495732439695484 and perplexity of 243.6499196944275
Finished 2 epochs...
Completing Train Step...
At time: 46.23682904243469 and batch: 50, loss is 5.744471950531006 and perplexity is 312.4585906671117
At time: 47.05898857116699 and batch: 100, loss is 5.7280248928070066 and perplexity is 307.36159636644413
At time: 47.88208532333374 and batch: 150, loss is 5.636826028823853 and perplexity is 280.5707801617176
At time: 48.70289587974548 and batch: 200, loss is 5.678124160766601 and perplexity is 292.4004189776221
At time: 49.52485466003418 and batch: 250, loss is 5.687502346038818 and perplexity is 295.15550293182736
At time: 50.353278398513794 and batch: 300, loss is 5.683987064361572 and perplexity is 294.1197697139981
At time: 51.17555093765259 and batch: 350, loss is 5.74928575515747 and perplexity is 313.96633134944375
At time: 52.000391721725464 and batch: 400, loss is 5.703765888214111 and perplexity is 299.99502410863926
At time: 52.881760358810425 and batch: 450, loss is 5.694864158630371 and perplexity is 297.336400258776
At time: 53.70874810218811 and batch: 500, loss is 5.672270431518554 and perplexity is 290.6937860497627
At time: 54.53970170021057 and batch: 550, loss is 5.672152452468872 and perplexity is 290.6594922961479
At time: 55.36552143096924 and batch: 600, loss is 5.690901403427124 and perplexity is 296.1604604112674
At time: 56.19705152511597 and batch: 650, loss is 5.679549245834351 and perplexity is 292.81741150284995
At time: 57.027752161026 and batch: 700, loss is 5.728879585266113 and perplexity is 307.62440830076804
At time: 57.84505653381348 and batch: 750, loss is 5.67713604927063 and perplexity is 292.11163745986863
At time: 58.66965436935425 and batch: 800, loss is 5.686757192611695 and perplexity is 294.9356487200159
At time: 59.5010449886322 and batch: 850, loss is 5.707922697067261 and perplexity is 301.2446414917048
At time: 60.337931871414185 and batch: 900, loss is 5.676659955978393 and perplexity is 291.9725981691527
At time: 61.164456605911255 and batch: 950, loss is 5.650661144256592 and perplexity is 284.47948564101074
At time: 61.99251747131348 and batch: 1000, loss is 5.648717555999756 and perplexity is 283.9271116211217
At time: 62.822224378585815 and batch: 1050, loss is 5.67697958946228 and perplexity is 292.06593730426687
At time: 63.64397311210632 and batch: 1100, loss is 5.649279851913452 and perplexity is 284.0868075697557
At time: 64.46682453155518 and batch: 1150, loss is 5.683652820587159 and perplexity is 294.02147843957755
At time: 65.29089093208313 and batch: 1200, loss is 5.678332185745239 and perplexity is 292.4612518956974
At time: 66.12040996551514 and batch: 1250, loss is 5.701963214874268 and perplexity is 299.45471822041924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.499889513001825 and perplexity of 244.66489848061508
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 68.5997965335846 and batch: 50, loss is 5.627474679946899 and perplexity is 277.95929439965033
At time: 69.45416164398193 and batch: 100, loss is 5.56394778251648 and perplexity is 260.8505877082532
At time: 70.2803258895874 and batch: 150, loss is 5.426963768005371 and perplexity is 227.45758194224695
At time: 71.10377359390259 and batch: 200, loss is 5.444392080307007 and perplexity is 231.4565299515542
At time: 71.93054866790771 and batch: 250, loss is 5.439720945358276 and perplexity is 230.3778864713024
At time: 72.75913000106812 and batch: 300, loss is 5.422817525863647 and perplexity is 226.5164401796511
At time: 73.584725856781 and batch: 350, loss is 5.4441592979431155 and perplexity is 231.40265722392766
At time: 74.4671356678009 and batch: 400, loss is 5.4141294956207275 and perplexity is 224.55698273489878
At time: 75.29049444198608 and batch: 450, loss is 5.383856611251831 and perplexity is 217.86086198604352
At time: 76.12050557136536 and batch: 500, loss is 5.346491003036499 and perplexity is 209.87056908996564
At time: 76.94284152984619 and batch: 550, loss is 5.349520406723022 and perplexity is 210.5073157600472
At time: 77.77403736114502 and batch: 600, loss is 5.364983539581299 and perplexity is 213.7877156166654
At time: 78.59913229942322 and batch: 650, loss is 5.346914110183715 and perplexity is 209.95938561586635
At time: 79.42390465736389 and batch: 700, loss is 5.361007308959961 and perplexity is 212.93933415331222
At time: 80.25247287750244 and batch: 750, loss is 5.323096494674683 and perplexity is 205.01773645216463
At time: 81.07977414131165 and batch: 800, loss is 5.316578702926636 and perplexity is 203.68581883729664
At time: 81.90677070617676 and batch: 850, loss is 5.348304204940796 and perplexity is 210.2514520099608
At time: 82.73368954658508 and batch: 900, loss is 5.301458826065064 and perplexity is 200.6292798144184
At time: 83.55955600738525 and batch: 950, loss is 5.277574291229248 and perplexity is 195.89411648236015
At time: 84.390291929245 and batch: 1000, loss is 5.264455442428589 and perplexity is 193.34099481267364
At time: 85.22128319740295 and batch: 1050, loss is 5.250599775314331 and perplexity is 190.68059968529204
At time: 86.04488635063171 and batch: 1100, loss is 5.185731697082519 and perplexity is 178.7041592760956
At time: 86.87216234207153 and batch: 1150, loss is 5.218864240646362 and perplexity is 184.72426257403515
At time: 87.69799089431763 and batch: 1200, loss is 5.231501216888428 and perplexity is 187.07343061981703
At time: 88.52322840690613 and batch: 1250, loss is 5.273535232543946 and perplexity is 195.1044844084298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.104855279852874 and perplexity of 164.82021599965836
Finished 4 epochs...
Completing Train Step...
At time: 91.0171308517456 and batch: 50, loss is 5.318679542541504 and perplexity is 204.1141798758679
At time: 91.84226131439209 and batch: 100, loss is 5.324638175964355 and perplexity is 205.33405222690854
At time: 92.66947507858276 and batch: 150, loss is 5.223254432678223 and perplexity is 185.53702033559378
At time: 93.4967360496521 and batch: 200, loss is 5.26211669921875 and perplexity is 192.88934822243516
At time: 94.32258439064026 and batch: 250, loss is 5.269734687805176 and perplexity is 194.3643883616068
At time: 95.15281438827515 and batch: 300, loss is 5.268530588150025 and perplexity is 194.13049511225668
At time: 96.03578519821167 and batch: 350, loss is 5.294278745651245 and perplexity is 199.19390467300627
At time: 96.86524701118469 and batch: 400, loss is 5.267588014602661 and perplexity is 193.9475990528621
At time: 97.69841480255127 and batch: 450, loss is 5.243478946685791 and perplexity is 189.32761870204433
At time: 98.5277795791626 and batch: 500, loss is 5.228855571746826 and perplexity is 186.5791548344444
At time: 99.35550832748413 and batch: 550, loss is 5.225537357330322 and perplexity is 185.96107122714153
At time: 100.18366622924805 and batch: 600, loss is 5.245098390579224 and perplexity is 189.63447255724768
At time: 101.0119194984436 and batch: 650, loss is 5.243639183044434 and perplexity is 189.3579583009442
At time: 101.83401703834534 and batch: 700, loss is 5.2530741691589355 and perplexity is 191.15300280209206
At time: 102.65985774993896 and batch: 750, loss is 5.217786607742309 and perplexity is 184.5253048514566
At time: 103.48312473297119 and batch: 800, loss is 5.2224327754974365 and perplexity is 185.3846351233085
At time: 104.31673002243042 and batch: 850, loss is 5.255049304962158 and perplexity is 191.53092904678587
At time: 105.14094066619873 and batch: 900, loss is 5.215043458938599 and perplexity is 184.01981811184817
At time: 105.97415733337402 and batch: 950, loss is 5.197211542129517 and perplexity is 180.76747595191708
At time: 106.79834604263306 and batch: 1000, loss is 5.18646388053894 and perplexity is 178.835051417794
At time: 107.62492561340332 and batch: 1050, loss is 5.18741096496582 and perplexity is 179.00450354003897
At time: 108.45394396781921 and batch: 1100, loss is 5.147789754867554 and perplexity is 172.05079532430852
At time: 109.2785849571228 and batch: 1150, loss is 5.194389572143555 and perplexity is 180.2580746559787
At time: 110.11036586761475 and batch: 1200, loss is 5.212183618545533 and perplexity is 183.49430260629106
At time: 110.93428182601929 and batch: 1250, loss is 5.243624563217163 and perplexity is 189.35518994053808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.078257762602646 and perplexity of 160.4941932202735
Finished 5 epochs...
Completing Train Step...
At time: 113.43402218818665 and batch: 50, loss is 5.255909948348999 and perplexity is 191.69583982875324
At time: 114.28939723968506 and batch: 100, loss is 5.258932809829712 and perplexity is 192.27618651088093
At time: 115.1157169342041 and batch: 150, loss is 5.163583164215088 and perplexity is 174.78963484101536
At time: 115.94068694114685 and batch: 200, loss is 5.205858974456787 and perplexity is 182.33742871415737
At time: 116.76549696922302 and batch: 250, loss is 5.216056928634644 and perplexity is 184.2064111582041
At time: 117.62302994728088 and batch: 300, loss is 5.21934211730957 and perplexity is 184.81255908398802
At time: 118.45242094993591 and batch: 350, loss is 5.24136510848999 and perplexity is 188.92783343965468
At time: 119.28471994400024 and batch: 400, loss is 5.215917139053345 and perplexity is 184.1806628208323
At time: 120.11626529693604 and batch: 450, loss is 5.1859887504577635 and perplexity is 178.75010168798073
At time: 120.93986129760742 and batch: 500, loss is 5.1854645538330075 and perplexity is 178.65642604237476
At time: 121.77128148078918 and batch: 550, loss is 5.180454177856445 and perplexity is 177.76352892034856
At time: 122.60271668434143 and batch: 600, loss is 5.203547830581665 and perplexity is 181.91650727494084
At time: 123.43114972114563 and batch: 650, loss is 5.203241348266602 and perplexity is 181.86076162560713
At time: 124.25795531272888 and batch: 700, loss is 5.211581449508667 and perplexity is 183.38384128035335
At time: 125.08257937431335 and batch: 750, loss is 5.177265529632568 and perplexity is 177.19760630316105
At time: 125.90715456008911 and batch: 800, loss is 5.182337341308593 and perplexity is 178.09860210089732
At time: 126.73756217956543 and batch: 850, loss is 5.214219188690185 and perplexity is 183.86819854699274
At time: 127.56586861610413 and batch: 900, loss is 5.17802167892456 and perplexity is 177.33164481788737
At time: 128.3920829296112 and batch: 950, loss is 5.164130954742432 and perplexity is 174.88540917700124
At time: 129.2193102836609 and batch: 1000, loss is 5.153732576370239 and perplexity is 173.07630668763204
At time: 130.0487461090088 and batch: 1050, loss is 5.155484962463379 and perplexity is 173.3798691021171
At time: 130.87718057632446 and batch: 1100, loss is 5.129640798568726 and perplexity is 168.95641775664353
At time: 131.70447826385498 and batch: 1150, loss is 5.17388876914978 and perplexity is 176.60026154124762
At time: 132.5302176475525 and batch: 1200, loss is 5.192774171829224 and perplexity is 179.96712077228707
At time: 133.35911750793457 and batch: 1250, loss is 5.21989483833313 and perplexity is 184.91473710618084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.064606381158759 and perplexity of 158.3181128153873
Finished 6 epochs...
Completing Train Step...
At time: 135.83549976348877 and batch: 50, loss is 5.218251781463623 and perplexity is 184.61116114168777
At time: 136.66158080101013 and batch: 100, loss is 5.218768348693848 and perplexity is 184.7065498530874
At time: 137.48496651649475 and batch: 150, loss is 5.125114679336548 and perplexity is 168.19342885673538
At time: 138.3074917793274 and batch: 200, loss is 5.171463031768798 and perplexity is 176.17239484126446
At time: 139.18987011909485 and batch: 250, loss is 5.18299295425415 and perplexity is 178.21540413429906
At time: 140.01489067077637 and batch: 300, loss is 5.185196475982666 and perplexity is 178.6085386307956
At time: 140.83976411819458 and batch: 350, loss is 5.207358074188233 and perplexity is 182.61097569044603
At time: 141.66401505470276 and batch: 400, loss is 5.18262809753418 and perplexity is 178.15039290711127
At time: 142.49333953857422 and batch: 450, loss is 5.149940052032471 and perplexity is 172.42115370930415
At time: 143.31806445121765 and batch: 500, loss is 5.150238580703736 and perplexity is 172.47263405101563
At time: 144.14210104942322 and batch: 550, loss is 5.147589387893677 and perplexity is 172.01632548052078
At time: 144.97187662124634 and batch: 600, loss is 5.172504215240479 and perplexity is 176.35591815107873
At time: 145.79767751693726 and batch: 650, loss is 5.174652299880981 and perplexity is 176.73515275831411
At time: 146.62245297431946 and batch: 700, loss is 5.183679151535034 and perplexity is 178.33773702747678
At time: 147.44511198997498 and batch: 750, loss is 5.15114595413208 and perplexity is 172.629202158398
At time: 148.27456665039062 and batch: 800, loss is 5.158424234390258 and perplexity is 173.89022936041457
At time: 149.09852814674377 and batch: 850, loss is 5.192001762390137 and perplexity is 179.82816614132014
At time: 149.92579197883606 and batch: 900, loss is 5.154985942840576 and perplexity is 173.2933707292239
At time: 150.75098538398743 and batch: 950, loss is 5.142031116485596 and perplexity is 171.06286430957655
At time: 151.5761456489563 and batch: 1000, loss is 5.1300174999237065 and perplexity is 169.02007585743874
At time: 152.4069676399231 and batch: 1050, loss is 5.136983699798584 and perplexity is 170.20161412646854
At time: 153.23257637023926 and batch: 1100, loss is 5.1104504489898686 and perplexity is 165.74499772787073
At time: 154.06173300743103 and batch: 1150, loss is 5.154258403778076 and perplexity is 173.16733888485027
At time: 154.88710641860962 and batch: 1200, loss is 5.173100299835205 and perplexity is 176.46107253441784
At time: 155.71904635429382 and batch: 1250, loss is 5.201004314422607 and perplexity is 181.45438765259033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0525958207401915 and perplexity of 156.42799695272194
Finished 7 epochs...
Completing Train Step...
At time: 158.21000862121582 and batch: 50, loss is 5.1889645385742185 and perplexity is 179.28281634626077
At time: 159.0371437072754 and batch: 100, loss is 5.189102954864502 and perplexity is 179.30763372613626
At time: 159.8949272632599 and batch: 150, loss is 5.099484920501709 and perplexity is 163.93744473251678
At time: 160.722403049469 and batch: 200, loss is 5.14770956993103 and perplexity is 172.0369999953028
At time: 161.5534553527832 and batch: 250, loss is 5.159011316299439 and perplexity is 173.99234714107303
At time: 162.37925386428833 and batch: 300, loss is 5.161755924224853 and perplexity is 174.47054384704964
At time: 163.20808506011963 and batch: 350, loss is 5.18057430267334 and perplexity is 177.78488401432335
At time: 164.03404235839844 and batch: 400, loss is 5.160305099487305 and perplexity is 174.21760119815696
At time: 164.85914373397827 and batch: 450, loss is 5.124333295822144 and perplexity is 168.06205661693105
At time: 165.6835114955902 and batch: 500, loss is 5.125236854553223 and perplexity is 168.21397918069385
At time: 166.51264595985413 and batch: 550, loss is 5.121177530288696 and perplexity is 167.53252814448825
At time: 167.33798670768738 and batch: 600, loss is 5.148013610839843 and perplexity is 172.0893142335621
At time: 168.1622350215912 and batch: 650, loss is 5.150716533660889 and perplexity is 172.55508755936776
At time: 168.99414920806885 and batch: 700, loss is 5.161692276000976 and perplexity is 174.45943946020589
At time: 169.8183867931366 and batch: 750, loss is 5.130199747085571 and perplexity is 169.05088209365124
At time: 170.64811515808105 and batch: 800, loss is 5.138849105834961 and perplexity is 170.51940555773558
At time: 171.47393608093262 and batch: 850, loss is 5.172363233566284 and perplexity is 176.33105695101167
At time: 172.30410957336426 and batch: 900, loss is 5.135144681930542 and perplexity is 169.88889795037153
At time: 173.1316282749176 and batch: 950, loss is 5.124654045104981 and perplexity is 168.11597104710822
At time: 173.96296954154968 and batch: 1000, loss is 5.112508783340454 and perplexity is 166.08650770041137
At time: 174.79009294509888 and batch: 1050, loss is 5.120719547271729 and perplexity is 167.4558186589701
At time: 175.6138563156128 and batch: 1100, loss is 5.093524866104126 and perplexity is 162.96327458138126
At time: 176.44426679611206 and batch: 1150, loss is 5.136487770080566 and perplexity is 170.11722701473641
At time: 177.2738652229309 and batch: 1200, loss is 5.156536483764649 and perplexity is 173.56227761409997
At time: 178.1050500869751 and batch: 1250, loss is 5.182024965286255 and perplexity is 178.04297705640636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.043382185218978 and perplexity of 154.993345729299
Finished 8 epochs...
Completing Train Step...
At time: 180.59779596328735 and batch: 50, loss is 5.164359979629516 and perplexity is 174.9254668750205
At time: 181.47842860221863 and batch: 100, loss is 5.163022241592407 and perplexity is 174.69161887287981
At time: 182.3118257522583 and batch: 150, loss is 5.076507396697998 and perplexity is 160.2135153726954
At time: 183.13635778427124 and batch: 200, loss is 5.121569385528565 and perplexity is 167.5981895075245
At time: 183.9622447490692 and batch: 250, loss is 5.135198612213134 and perplexity is 169.8980603537106
At time: 184.78507924079895 and batch: 300, loss is 5.139933309555054 and perplexity is 170.70438359040477
At time: 185.61222863197327 and batch: 350, loss is 5.157364292144775 and perplexity is 173.70601340662142
At time: 186.4371953010559 and batch: 400, loss is 5.137941560745239 and perplexity is 170.36472171041942
At time: 187.26197504997253 and batch: 450, loss is 5.100461330413818 and perplexity is 164.09759305099877
At time: 188.08879780769348 and batch: 500, loss is 5.100683603286743 and perplexity is 164.13407154838634
At time: 188.91990661621094 and batch: 550, loss is 5.098357019424438 and perplexity is 163.75264375020282
At time: 189.74614095687866 and batch: 600, loss is 5.124233179092407 and perplexity is 168.04523163567484
At time: 190.57231783866882 and batch: 650, loss is 5.126925268173218 and perplexity is 168.49823385642108
At time: 191.39832544326782 and batch: 700, loss is 5.138137702941894 and perplexity is 170.39814069849797
At time: 192.22307896614075 and batch: 750, loss is 5.108852643966674 and perplexity is 165.48038099719935
At time: 193.04834604263306 and batch: 800, loss is 5.115829639434814 and perplexity is 166.6389739143758
At time: 193.87320852279663 and batch: 850, loss is 5.1518659591674805 and perplexity is 172.7535408100654
At time: 194.69766640663147 and batch: 900, loss is 5.113721323013306 and perplexity is 166.2880163240348
At time: 195.52846932411194 and batch: 950, loss is 5.104058904647827 and perplexity is 164.68900951807453
At time: 196.35713648796082 and batch: 1000, loss is 5.095180320739746 and perplexity is 163.23327631587205
At time: 197.18079900741577 and batch: 1050, loss is 5.101302928924561 and perplexity is 164.23575547141567
At time: 198.00716400146484 and batch: 1100, loss is 5.072510404586792 and perplexity is 159.57442129348271
At time: 198.8341588973999 and batch: 1150, loss is 5.117646360397339 and perplexity is 166.9419855919528
At time: 199.6576910018921 and batch: 1200, loss is 5.137881031036377 and perplexity is 170.35440989550264
At time: 200.48899245262146 and batch: 1250, loss is 5.1601427555084225 and perplexity is 174.18932031526475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.030504212762318 and perplexity of 153.01014291578215
Finished 9 epochs...
Completing Train Step...
At time: 202.98005509376526 and batch: 50, loss is 5.137021884918213 and perplexity is 170.20811341955235
At time: 203.8023064136505 and batch: 100, loss is 5.137507419586182 and perplexity is 170.29077542539798
At time: 204.62891793251038 and batch: 150, loss is 5.05066780090332 and perplexity is 156.12669122659952
At time: 205.45524144172668 and batch: 200, loss is 5.099710731506348 and perplexity is 163.9744677915595
At time: 206.28849840164185 and batch: 250, loss is 5.110035495758057 and perplexity is 165.67623557293277
At time: 207.1142861843109 and batch: 300, loss is 5.1150454330444335 and perplexity is 166.5083457925441
At time: 207.94566679000854 and batch: 350, loss is 5.13411283493042 and perplexity is 169.7136890106224
At time: 208.7753710746765 and batch: 400, loss is 5.115630369186402 and perplexity is 166.6057710329318
At time: 209.60660552978516 and batch: 450, loss is 5.077300176620484 and perplexity is 160.3405797913987
At time: 210.43463230133057 and batch: 500, loss is 5.078399982452392 and perplexity is 160.5170203035122
At time: 211.26202416419983 and batch: 550, loss is 5.0730446434021 and perplexity is 159.6596949194803
At time: 212.08817529678345 and batch: 600, loss is 5.100201034545899 and perplexity is 164.0548846842389
At time: 212.91235041618347 and batch: 650, loss is 5.102735214233398 and perplexity is 164.47115647161772
At time: 213.7382836341858 and batch: 700, loss is 5.113592166900634 and perplexity is 166.2665405971531
At time: 214.57070636749268 and batch: 750, loss is 5.086020269393921 and perplexity is 161.74487843117342
At time: 215.3941707611084 and batch: 800, loss is 5.092372303009033 and perplexity is 162.77555732401524
At time: 216.22391963005066 and batch: 850, loss is 5.132863111495972 and perplexity is 169.5017263112826
At time: 217.04858255386353 and batch: 900, loss is 5.091032218933106 and perplexity is 162.5575704846634
At time: 217.8752956390381 and batch: 950, loss is 5.08246413230896 and perplexity is 161.17071298128636
At time: 218.70267939567566 and batch: 1000, loss is 5.0759202575683595 and perplexity is 160.11947535871064
At time: 219.53095149993896 and batch: 1050, loss is 5.079622144699097 and perplexity is 160.71331807510956
At time: 220.3616898059845 and batch: 1100, loss is 5.050831089019775 and perplexity is 156.1521869414552
At time: 221.18716502189636 and batch: 1150, loss is 5.0959374141693115 and perplexity is 163.35690595054402
At time: 222.0145812034607 and batch: 1200, loss is 5.1168428325653075 and perplexity is 166.80789693939178
At time: 222.83909797668457 and batch: 1250, loss is 5.13987253189087 and perplexity is 170.69400889198232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.022016316434763 and perplexity of 151.71690488501673
Finished 10 epochs...
Completing Train Step...
At time: 225.30166840553284 and batch: 50, loss is 5.111799468994141 and perplexity is 165.96874192918355
At time: 226.16142964363098 and batch: 100, loss is 5.1136323261260985 and perplexity is 166.27321786672016
At time: 226.9873034954071 and batch: 150, loss is 5.0294556808471675 and perplexity is 152.84979097934595
At time: 227.813303232193 and batch: 200, loss is 5.080407629013061 and perplexity is 160.83960545738856
At time: 228.63879704475403 and batch: 250, loss is 5.089062814712524 and perplexity is 162.23774395649266
At time: 229.4662115573883 and batch: 300, loss is 5.096622943878174 and perplexity is 163.46893035636293
At time: 230.29302287101746 and batch: 350, loss is 5.116686182022095 and perplexity is 166.78176843829942
At time: 231.11692142486572 and batch: 400, loss is 5.096868352890015 and perplexity is 163.50905202793692
At time: 231.93941640853882 and batch: 450, loss is 5.0543403148651125 and perplexity is 156.70112283836445
At time: 232.76454305648804 and batch: 500, loss is 5.056316137313843 and perplexity is 157.011042506905
At time: 233.59441566467285 and batch: 550, loss is 5.050284042358398 and perplexity is 156.06678776971776
At time: 234.4185426235199 and batch: 600, loss is 5.078138675689697 and perplexity is 160.47508160025123
At time: 235.24356842041016 and batch: 650, loss is 5.08626425743103 and perplexity is 161.78434706131532
At time: 236.06894183158875 and batch: 700, loss is 5.095783348083496 and perplexity is 163.33174013010284
At time: 236.89878487586975 and batch: 750, loss is 5.066788587570191 and perplexity is 158.66397284795954
At time: 237.72358465194702 and batch: 800, loss is 5.0761461353302 and perplexity is 160.15564687244824
At time: 238.5515820980072 and batch: 850, loss is 5.111282615661621 and perplexity is 165.88298259623105
At time: 239.3786175251007 and batch: 900, loss is 5.071989231109619 and perplexity is 159.4912770056501
At time: 240.20769429206848 and batch: 950, loss is 5.060456171035766 and perplexity is 157.6624209515826
At time: 241.03318881988525 and batch: 1000, loss is 5.057041778564453 and perplexity is 157.12501754364223
At time: 241.85653257369995 and batch: 1050, loss is 5.0603006649017335 and perplexity is 157.6379053842281
At time: 242.6870038509369 and batch: 1100, loss is 5.03020920753479 and perplexity is 152.96501078119243
At time: 243.511470079422 and batch: 1150, loss is 5.075720567703247 and perplexity is 160.0875043145273
At time: 244.33599519729614 and batch: 1200, loss is 5.0942629528045655 and perplexity is 163.08360000684556
At time: 245.21826362609863 and batch: 1250, loss is 5.120177412033081 and perplexity is 167.36505956283358
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0035525134010035 and perplexity of 148.9413364622661
Finished 11 epochs...
Completing Train Step...
At time: 247.70456433296204 and batch: 50, loss is 5.087785263061523 and perplexity is 162.03060919969428
At time: 248.53140950202942 and batch: 100, loss is 5.083796606063843 and perplexity is 161.3856118681572
At time: 249.35787725448608 and batch: 150, loss is 5.001949186325073 and perplexity is 148.70272612111665
At time: 250.18514227867126 and batch: 200, loss is 5.055697355270386 and perplexity is 156.91391694605284
At time: 251.0171537399292 and batch: 250, loss is 5.06370888710022 and perplexity is 158.1760869929757
At time: 251.8457543849945 and batch: 300, loss is 5.067389698028564 and perplexity is 158.75937609247688
At time: 252.6710124015808 and batch: 350, loss is 5.0914945220947265 and perplexity is 162.63273873736551
At time: 253.49748253822327 and batch: 400, loss is 5.07273078918457 and perplexity is 159.6095929136345
At time: 254.33237481117249 and batch: 450, loss is 5.028957319259644 and perplexity is 152.77363549292156
At time: 255.1619701385498 and batch: 500, loss is 5.029579544067383 and perplexity is 152.86872461923252
At time: 255.99310874938965 and batch: 550, loss is 5.023911294937133 and perplexity is 152.0046777337864
At time: 256.81813073158264 and batch: 600, loss is 5.052366638183594 and perplexity is 156.39215049230552
At time: 257.64448714256287 and batch: 650, loss is 5.059408988952637 and perplexity is 157.49740610455433
At time: 258.4717490673065 and batch: 700, loss is 5.0707114315032955 and perplexity is 159.28760926567116
At time: 259.2992513179779 and batch: 750, loss is 5.044958715438843 and perplexity is 155.23789013789408
At time: 260.12736773490906 and batch: 800, loss is 5.05221155166626 and perplexity is 156.3678980590086
At time: 260.95997405052185 and batch: 850, loss is 5.092420253753662 and perplexity is 162.78336272033212
At time: 261.78667664527893 and batch: 900, loss is 5.048703937530518 and perplexity is 155.82038061072953
At time: 262.6147994995117 and batch: 950, loss is 5.040191812515259 and perplexity is 154.49964714982028
At time: 263.4431109428406 and batch: 1000, loss is 5.034285507202148 and perplexity is 153.5898145823645
At time: 264.27048444747925 and batch: 1050, loss is 5.040726442337036 and perplexity is 154.58226935281942
At time: 265.0979096889496 and batch: 1100, loss is 5.0100893306732175 and perplexity is 149.9181278381808
At time: 265.9239959716797 and batch: 1150, loss is 5.050765943527222 and perplexity is 156.1420146616663
At time: 266.80696725845337 and batch: 1200, loss is 5.072569503784179 and perplexity is 159.58385229238422
At time: 267.63512682914734 and batch: 1250, loss is 5.098702716827392 and perplexity is 163.80926239977038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.99189791714188 and perplexity of 147.2155614591867
Finished 12 epochs...
Completing Train Step...
At time: 270.0756006240845 and batch: 50, loss is 5.063973159790039 and perplexity is 158.2178941369502
At time: 270.9425218105316 and batch: 100, loss is 5.059446640014649 and perplexity is 157.50333616079408
At time: 271.76646518707275 and batch: 150, loss is 4.979712724685669 and perplexity is 145.43259645710842
At time: 272.59220910072327 and batch: 200, loss is 5.033359603881836 and perplexity is 153.44767107880241
At time: 273.4183728694916 and batch: 250, loss is 5.044363317489624 and perplexity is 155.14548932682126
At time: 274.24871015548706 and batch: 300, loss is 5.048262872695923 and perplexity is 155.75166887460813
At time: 275.0741364955902 and batch: 350, loss is 5.068717603683472 and perplexity is 158.9703336005946
At time: 275.8991162776947 and batch: 400, loss is 5.049065027236939 and perplexity is 155.87665590582208
At time: 276.7242043018341 and batch: 450, loss is 5.004758615493774 and perplexity is 149.12108329459906
At time: 277.5508506298065 and batch: 500, loss is 5.007632493972778 and perplexity is 149.55025556573798
At time: 278.3765149116516 and batch: 550, loss is 5.001181859970092 and perplexity is 148.58866636643074
At time: 279.20979142189026 and batch: 600, loss is 5.029961757659912 and perplexity is 152.92716429116155
At time: 280.0337085723877 and batch: 650, loss is 5.038052768707275 and perplexity is 154.16951884158098
At time: 280.8609664440155 and batch: 700, loss is 5.048334465026856 and perplexity is 155.7628198987887
At time: 281.693651676178 and batch: 750, loss is 5.023819150924683 and perplexity is 151.99067205814828
At time: 282.55075907707214 and batch: 800, loss is 5.034385061264038 and perplexity is 153.60510583341144
At time: 283.3787190914154 and batch: 850, loss is 5.074020662307739 and perplexity is 159.8156018718803
At time: 284.2121937274933 and batch: 900, loss is 5.02956109046936 and perplexity is 152.86590366726656
At time: 285.03672099113464 and batch: 950, loss is 5.021049842834473 and perplexity is 151.57034533610326
At time: 285.8768651485443 and batch: 1000, loss is 5.014466314315796 and perplexity is 150.57575519364246
At time: 286.7053818702698 and batch: 1050, loss is 5.021674709320068 and perplexity is 151.66508616221475
At time: 287.56219935417175 and batch: 1100, loss is 4.99191312789917 and perplexity is 147.2178007363919
At time: 288.38833475112915 and batch: 1150, loss is 5.033139600753784 and perplexity is 153.4139158244397
At time: 289.21809697151184 and batch: 1200, loss is 5.0571113967895505 and perplexity is 157.1359566892596
At time: 290.04475140571594 and batch: 1250, loss is 5.0836328125 and perplexity is 161.35918010837153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.984718489820939 and perplexity of 146.16242302419775
Finished 13 epochs...
Completing Train Step...
At time: 292.50039315223694 and batch: 50, loss is 5.046400499343872 and perplexity is 155.4618710565876
At time: 293.3567032814026 and batch: 100, loss is 5.047076549530029 and perplexity is 155.5670066179093
At time: 294.18154859542847 and batch: 150, loss is 4.96059741973877 and perplexity is 142.67900969695265
At time: 295.0121569633484 and batch: 200, loss is 5.0147725772857665 and perplexity is 150.62187803413212
At time: 295.83905243873596 and batch: 250, loss is 5.024563245773315 and perplexity is 152.1038096215858
At time: 296.66615056991577 and batch: 300, loss is 5.032145099639893 and perplexity is 153.26142135479432
At time: 297.5012822151184 and batch: 350, loss is 5.050594663619995 and perplexity is 156.1152729621041
At time: 298.32517313957214 and batch: 400, loss is 5.035184230804443 and perplexity is 153.72791141995032
At time: 299.1529610157013 and batch: 450, loss is 4.987035150527954 and perplexity is 146.50142429030043
At time: 299.9783842563629 and batch: 500, loss is 4.98994592666626 and perplexity is 146.9284783682402
At time: 300.8042562007904 and batch: 550, loss is 4.983831186294555 and perplexity is 146.03279011119903
At time: 301.6391816139221 and batch: 600, loss is 5.013276224136352 and perplexity is 150.39666305515937
At time: 302.4656791687012 and batch: 650, loss is 5.020204105377197 and perplexity is 151.4422108093623
At time: 303.28991055488586 and batch: 700, loss is 5.033113298416137 and perplexity is 153.4098807328924
At time: 304.1178300380707 and batch: 750, loss is 5.0112472248077395 and perplexity is 150.0918176968917
At time: 304.9482319355011 and batch: 800, loss is 5.020773191452026 and perplexity is 151.5284189902862
At time: 305.7724027633667 and batch: 850, loss is 5.057818117141724 and perplexity is 157.24704711821997
At time: 306.5994164943695 and batch: 900, loss is 5.013412036895752 and perplexity is 150.41709022807734
At time: 307.43354415893555 and batch: 950, loss is 5.007590045928955 and perplexity is 149.54390758466664
At time: 308.25805401802063 and batch: 1000, loss is 5.001659212112426 and perplexity is 148.6596124164246
At time: 309.1138405799866 and batch: 1050, loss is 5.010218744277954 and perplexity is 149.93753053897933
At time: 309.9395604133606 and batch: 1100, loss is 4.9766222095489505 and perplexity is 144.98382863545066
At time: 310.76922726631165 and batch: 1150, loss is 5.021711225509644 and perplexity is 151.67062449437176
At time: 311.5913338661194 and batch: 1200, loss is 5.043655986785889 and perplexity is 155.03578896046685
At time: 312.417094707489 and batch: 1250, loss is 5.069949140548706 and perplexity is 159.16623203021172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.978067885350137 and perplexity of 145.19357982765237
Finished 14 epochs...
Completing Train Step...
At time: 314.9012122154236 and batch: 50, loss is 5.029557256698609 and perplexity is 152.86531761555958
At time: 315.7324860095978 and batch: 100, loss is 5.02380069732666 and perplexity is 151.9878673092618
At time: 316.5604155063629 and batch: 150, loss is 4.943010034561158 and perplexity is 140.19159665833646
At time: 317.38695430755615 and batch: 200, loss is 4.999450025558471 and perplexity is 148.33155809957353
At time: 318.21484088897705 and batch: 250, loss is 5.009245567321777 and perplexity is 149.7916857672887
At time: 319.04167127609253 and batch: 300, loss is 5.016599092483521 and perplexity is 150.89724258568995
At time: 319.8697953224182 and batch: 350, loss is 5.036791172027588 and perplexity is 153.9751417270439
At time: 320.7000799179077 and batch: 400, loss is 5.01839168548584 and perplexity is 151.1679825175726
At time: 321.52346324920654 and batch: 450, loss is 4.969913539886474 and perplexity is 144.014435328197
At time: 322.356064081192 and batch: 500, loss is 4.974094467163086 and perplexity is 144.61780966226945
At time: 323.18199276924133 and batch: 550, loss is 4.966685857772827 and perplexity is 143.55035187101726
At time: 324.0097465515137 and batch: 600, loss is 4.994634580612183 and perplexity is 147.61899268445504
At time: 324.83608078956604 and batch: 650, loss is 5.004694995880127 and perplexity is 149.11159657066722
At time: 325.66226077079773 and batch: 700, loss is 5.01698857307434 and perplexity is 150.9560255795598
At time: 326.4902241230011 and batch: 750, loss is 4.993553256988525 and perplexity is 147.45945505181538
At time: 327.317097902298 and batch: 800, loss is 5.004609708786011 and perplexity is 149.09887981819185
At time: 328.15390062332153 and batch: 850, loss is 5.0446446514129635 and perplexity is 155.18914315638568
At time: 328.9818015098572 and batch: 900, loss is 5.003521995544434 and perplexity is 148.9367911612794
At time: 329.8129997253418 and batch: 950, loss is 4.9940874385833744 and perplexity is 147.53824622121329
At time: 330.71428179740906 and batch: 1000, loss is 4.98825662612915 and perplexity is 146.68048154037976
At time: 331.55209159851074 and batch: 1050, loss is 4.994731721878051 and perplexity is 147.63333327679103
At time: 332.3876438140869 and batch: 1100, loss is 4.965551204681397 and perplexity is 143.38756439161418
At time: 333.21603178977966 and batch: 1150, loss is 5.007932434082031 and perplexity is 149.59511841347853
At time: 334.0480651855469 and batch: 1200, loss is 5.030725908279419 and perplexity is 153.0440683389558
At time: 334.88014006614685 and batch: 1250, loss is 5.058383169174195 and perplexity is 157.33592498974033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.975354716725593 and perplexity of 144.8001790849826
Finished 15 epochs...
Completing Train Step...
At time: 337.31770396232605 and batch: 50, loss is 5.012686805725098 and perplexity is 150.30804261278922
At time: 338.1705768108368 and batch: 100, loss is 5.010148344039917 and perplexity is 149.92697527268965
At time: 339.00291085243225 and batch: 150, loss is 4.928991975784302 and perplexity is 138.24009272154677
At time: 339.83272433280945 and batch: 200, loss is 4.983060445785522 and perplexity is 145.9202800877992
At time: 340.66110825538635 and batch: 250, loss is 4.994060707092285 and perplexity is 147.53430235661205
At time: 341.49626088142395 and batch: 300, loss is 5.00194860458374 and perplexity is 148.70263961461967
At time: 342.3226091861725 and batch: 350, loss is 5.022848138809204 and perplexity is 151.8431589042602
At time: 343.14871430397034 and batch: 400, loss is 5.005899181365967 and perplexity is 149.29126274501724
At time: 343.9775323867798 and batch: 450, loss is 4.9573744201660155 and perplexity is 142.21989556940608
At time: 344.81135082244873 and batch: 500, loss is 4.960719966888428 and perplexity is 142.69649567431352
At time: 345.63536834716797 and batch: 550, loss is 4.953617668151855 and perplexity is 141.68661302260574
At time: 346.4614977836609 and batch: 600, loss is 4.981040000915527 and perplexity is 145.62575384367995
At time: 347.28833651542664 and batch: 650, loss is 4.993760051727295 and perplexity is 147.4899520445022
At time: 348.1156716346741 and batch: 700, loss is 5.0052737045288085 and perplexity is 149.19791371505144
At time: 348.94813656806946 and batch: 750, loss is 4.981438083648682 and perplexity is 145.68373648196552
At time: 349.77692794799805 and batch: 800, loss is 4.991750593185425 and perplexity is 147.19387467775127
At time: 350.6040096282959 and batch: 850, loss is 5.031862602233887 and perplexity is 153.21813151574239
At time: 351.4303729534149 and batch: 900, loss is 4.986677007675171 and perplexity is 146.4489652467448
At time: 352.3108661174774 and batch: 950, loss is 4.9805311107635495 and perplexity is 145.55166518476787
At time: 353.13632822036743 and batch: 1000, loss is 4.974904155731201 and perplexity is 144.73495246769414
At time: 353.96125316619873 and batch: 1050, loss is 4.983330154418946 and perplexity is 145.9596413549286
At time: 354.7919933795929 and batch: 1100, loss is 4.953038673400879 and perplexity is 141.60460096194956
At time: 355.6236882209778 and batch: 1150, loss is 4.996931734085083 and perplexity is 147.95848595094458
At time: 356.45640659332275 and batch: 1200, loss is 5.015921363830566 and perplexity is 150.7950098476788
At time: 357.2856276035309 and batch: 1250, loss is 5.046864175796509 and perplexity is 155.53397177989143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.972776538264142 and perplexity of 144.4273392124383
Finished 16 epochs...
Completing Train Step...
At time: 359.7853105068207 and batch: 50, loss is 4.996321048736572 and perplexity is 147.86815745533806
At time: 360.6144902706146 and batch: 100, loss is 4.992284326553345 and perplexity is 147.27245792960656
At time: 361.4541382789612 and batch: 150, loss is 4.913306884765625 and perplexity is 136.0887007812913
At time: 362.2794497013092 and batch: 200, loss is 4.966309862136841 and perplexity is 143.49638771094956
At time: 363.1058807373047 and batch: 250, loss is 4.980048389434814 and perplexity is 145.48142124703776
At time: 363.935298204422 and batch: 300, loss is 4.990984926223755 and perplexity is 147.0812163258431
At time: 364.76124238967896 and batch: 350, loss is 5.008724660873413 and perplexity is 149.71367863123515
At time: 365.5852189064026 and batch: 400, loss is 4.994368524551391 and perplexity is 147.57972298096638
At time: 366.4147264957428 and batch: 450, loss is 4.944621419906616 and perplexity is 140.41768144868684
At time: 367.2434871196747 and batch: 500, loss is 4.944629011154174 and perplexity is 140.41874739811425
At time: 368.07646656036377 and batch: 550, loss is 4.937223072052002 and perplexity is 139.38265605638918
At time: 368.905974149704 and batch: 600, loss is 4.963015375137329 and perplexity is 143.02441860208754
At time: 369.73798751831055 and batch: 650, loss is 4.979157075881958 and perplexity is 145.35180945554544
At time: 370.56553745269775 and batch: 700, loss is 4.990679683685303 and perplexity is 147.03632773331574
At time: 371.39210295677185 and batch: 750, loss is 4.967175226211548 and perplexity is 143.62061807421904
At time: 372.21776938438416 and batch: 800, loss is 4.9783098506927494 and perplexity is 145.22871589262414
At time: 373.10029578208923 and batch: 850, loss is 5.018306922912598 and perplexity is 151.1551696734151
At time: 373.9259693622589 and batch: 900, loss is 4.973259840011597 and perplexity is 144.4971580682884
At time: 374.75321555137634 and batch: 950, loss is 4.969252767562867 and perplexity is 143.9193060080044
At time: 375.58984756469727 and batch: 1000, loss is 4.961488866806031 and perplexity is 142.8062571904824
At time: 376.41367650032043 and batch: 1050, loss is 4.972051687240601 and perplexity is 144.3226888403295
At time: 377.2381896972656 and batch: 1100, loss is 4.940093069076538 and perplexity is 139.7832584534105
At time: 378.06742763519287 and batch: 1150, loss is 4.980726184844971 and perplexity is 145.58006131173732
At time: 378.8936417102814 and batch: 1200, loss is 5.003786687850952 and perplexity is 148.9762188019237
At time: 379.7176196575165 and batch: 1250, loss is 5.032585458755493 and perplexity is 153.32892628087174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.967061954693203 and perplexity of 143.60435087006692
Finished 17 epochs...
Completing Train Step...
At time: 382.1812493801117 and batch: 50, loss is 4.980812969207764 and perplexity is 145.59269593282946
At time: 383.0359561443329 and batch: 100, loss is 4.974354867935181 and perplexity is 144.65547315515263
At time: 383.8582594394684 and batch: 150, loss is 4.898239440917969 and perplexity is 134.05356259177583
At time: 384.68691182136536 and batch: 200, loss is 4.952523202896118 and perplexity is 141.5316267764511
At time: 385.50918674468994 and batch: 250, loss is 4.968952541351318 and perplexity is 143.87610414548323
At time: 386.3391327857971 and batch: 300, loss is 4.973274974822998 and perplexity is 144.49934502207333
At time: 387.16437101364136 and batch: 350, loss is 4.9897450351715085 and perplexity is 146.8989646512259
At time: 387.9927475452423 and batch: 400, loss is 4.976445045471191 and perplexity is 144.95814498433782
At time: 388.8168115615845 and batch: 450, loss is 4.927117881774902 and perplexity is 137.98126040572117
At time: 389.64448857307434 and batch: 500, loss is 4.924168796539306 and perplexity is 137.57494133713607
At time: 390.47079277038574 and batch: 550, loss is 4.92086051940918 and perplexity is 137.12055733356013
At time: 391.2966799736023 and batch: 600, loss is 4.9440820598602295 and perplexity is 140.34196618223754
At time: 392.1256465911865 and batch: 650, loss is 4.958194952011109 and perplexity is 142.33663941219694
At time: 392.95117259025574 and batch: 700, loss is 4.971465892791748 and perplexity is 144.2381701680648
At time: 393.78093004226685 and batch: 750, loss is 4.953078451156617 and perplexity is 141.61023378720773
At time: 394.669438123703 and batch: 800, loss is 4.964375553131103 and perplexity is 143.2190896324551
At time: 395.4958543777466 and batch: 850, loss is 5.005753736495972 and perplexity is 149.26955067570873
At time: 396.3213310241699 and batch: 900, loss is 4.962709236145019 and perplexity is 142.98063995221887
At time: 397.14517855644226 and batch: 950, loss is 4.956328182220459 and perplexity is 142.07117752884267
At time: 397.9694800376892 and batch: 1000, loss is 4.948082551956177 and perplexity is 140.90452761970508
At time: 398.7941563129425 and batch: 1050, loss is 4.959121189117432 and perplexity is 142.46853796444668
At time: 399.6185805797577 and batch: 1100, loss is 4.921644296646118 and perplexity is 137.2280714331625
At time: 400.44434332847595 and batch: 1150, loss is 4.958996782302856 and perplexity is 142.45081500991233
At time: 401.2686026096344 and batch: 1200, loss is 4.983684186935425 and perplexity is 146.01132496236085
At time: 402.0984756946564 and batch: 1250, loss is 5.0099515533447265 and perplexity is 149.8974739418869
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9531944998859485 and perplexity of 141.6266684284903
Finished 18 epochs...
Completing Train Step...
At time: 404.5373523235321 and batch: 50, loss is 4.956303844451904 and perplexity is 142.06771987548157
At time: 405.39183378219604 and batch: 100, loss is 4.957914142608643 and perplexity is 142.29667555691037
At time: 406.2176513671875 and batch: 150, loss is 4.877821092605591 and perplexity is 131.3441650994698
At time: 407.04761576652527 and batch: 200, loss is 4.929374275207519 and perplexity is 138.29295193263883
At time: 407.87391328811646 and batch: 250, loss is 4.949406461715698 and perplexity is 141.09119603778672
At time: 408.69960594177246 and batch: 300, loss is 4.954404544830322 and perplexity is 141.79814678994956
At time: 409.5292468070984 and batch: 350, loss is 4.966002883911133 and perplexity is 143.45234420499912
At time: 410.3566963672638 and batch: 400, loss is 4.956535081863404 and perplexity is 142.1005750458091
At time: 411.18036913871765 and batch: 450, loss is 4.905785741806031 and perplexity is 135.06899768076863
At time: 412.0061511993408 and batch: 500, loss is 4.900838422775268 and perplexity is 134.40241850758795
At time: 412.8315079212189 and batch: 550, loss is 4.891879634857178 and perplexity is 133.20371323497466
At time: 413.65911841392517 and batch: 600, loss is 4.919680471420288 and perplexity is 136.95884392919743
At time: 414.4857921600342 and batch: 650, loss is 4.937860202789307 and perplexity is 139.4714893270084
At time: 415.31919956207275 and batch: 700, loss is 4.9503703212738035 and perplexity is 141.22725369535567
At time: 416.20506739616394 and batch: 750, loss is 4.933698320388794 and perplexity is 138.89223162754024
At time: 417.03486013412476 and batch: 800, loss is 4.94417929649353 and perplexity is 140.35561322602544
At time: 417.8638231754303 and batch: 850, loss is 4.984144334793091 and perplexity is 146.07852722103917
At time: 418.69348192214966 and batch: 900, loss is 4.9440991497039795 and perplexity is 140.3443646250056
At time: 419.5214214324951 and batch: 950, loss is 4.939882879257202 and perplexity is 139.753880523149
At time: 420.3501923084259 and batch: 1000, loss is 4.937761459350586 and perplexity is 139.4577181124685
At time: 421.1808567047119 and batch: 1050, loss is 4.945507612228393 and perplexity is 140.54217367370808
At time: 422.0057330131531 and batch: 1100, loss is 4.905365905761719 and perplexity is 135.01230274919385
At time: 422.83679461479187 and batch: 1150, loss is 4.940610666275024 and perplexity is 139.8556286040591
At time: 423.66285610198975 and batch: 1200, loss is 4.9635806655883785 and perplexity is 143.1052917964589
At time: 424.4889748096466 and batch: 1250, loss is 4.9882010650634765 and perplexity is 146.67233204291134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9437500891024175 and perplexity of 140.29538448567953
Finished 19 epochs...
Completing Train Step...
At time: 426.949343919754 and batch: 50, loss is 4.935617694854736 and perplexity is 139.15907383358166
At time: 427.7782154083252 and batch: 100, loss is 4.935962028503418 and perplexity is 139.2069992359128
At time: 428.607031583786 and batch: 150, loss is 4.8583447456359865 and perplexity is 128.81081089086976
At time: 429.4400396347046 and batch: 200, loss is 4.914934673309326 and perplexity is 136.31040480403044
At time: 430.27283787727356 and batch: 250, loss is 4.932960548400879 and perplexity is 138.7897986204543
At time: 431.09512209892273 and batch: 300, loss is 4.935460634231568 and perplexity is 139.1372191390267
At time: 431.91568422317505 and batch: 350, loss is 4.9491586494445805 and perplexity is 141.05623623997423
At time: 432.7475383281708 and batch: 400, loss is 4.943295307159424 and perplexity is 140.2315951843408
At time: 433.5768504142761 and batch: 450, loss is 4.889543685913086 and perplexity is 132.89291930239827
At time: 434.4005525112152 and batch: 500, loss is 4.884595079421997 and perplexity is 132.2369090463606
At time: 435.2251546382904 and batch: 550, loss is 4.874818325042725 and perplexity is 130.95036064845166
At time: 436.0547294616699 and batch: 600, loss is 4.901547651290894 and perplexity is 134.49777434590536
At time: 436.8869435787201 and batch: 650, loss is 4.918513832092285 and perplexity is 136.7991555230511
At time: 437.76884055137634 and batch: 700, loss is 4.932227277755738 and perplexity is 138.68806543881706
At time: 438.5943727493286 and batch: 750, loss is 4.913090600967407 and perplexity is 136.05927018298732
At time: 439.41885471343994 and batch: 800, loss is 4.9285921955108645 and perplexity is 138.18483810506726
At time: 440.24667501449585 and batch: 850, loss is 4.9707786655426025 and perplexity is 144.13907981985196
At time: 441.0741231441498 and batch: 900, loss is 4.930450944900513 and perplexity is 138.44192794732805
At time: 441.90929412841797 and batch: 950, loss is 4.924362859725952 and perplexity is 137.60164215939406
At time: 442.7353720664978 and batch: 1000, loss is 4.9239654636383055 and perplexity is 137.54697066898385
At time: 443.56053018569946 and batch: 1050, loss is 4.929941349029541 and perplexity is 138.37139648527395
At time: 444.38689398765564 and batch: 1100, loss is 4.88931718826294 and perplexity is 132.86282277697998
At time: 445.21711325645447 and batch: 1150, loss is 4.922491788864136 and perplexity is 137.3444204512755
At time: 446.04145789146423 and batch: 1200, loss is 4.946866006851196 and perplexity is 140.733215132172
At time: 446.8701310157776 and batch: 1250, loss is 4.973918943405152 and perplexity is 144.59242802845833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.935401192546761 and perplexity of 139.12894883410468
Finished 20 epochs...
Completing Train Step...
At time: 449.3093752861023 and batch: 50, loss is 4.919903841018677 and perplexity is 136.98943978812676
At time: 450.1712956428528 and batch: 100, loss is 4.919682598114013 and perplexity is 136.95913519902112
At time: 450.994256734848 and batch: 150, loss is 4.8416448593139645 and perplexity is 126.67754720042787
At time: 451.8212549686432 and batch: 200, loss is 4.897275171279907 and perplexity is 133.92436111403677
At time: 452.6483166217804 and batch: 250, loss is 4.918381299972534 and perplexity is 136.7810264423588
At time: 453.4752881526947 and batch: 300, loss is 4.9221593856811525 and perplexity is 137.29877431563318
At time: 454.3045198917389 and batch: 350, loss is 4.933109359741211 and perplexity is 138.81045365322484
At time: 455.1292905807495 and batch: 400, loss is 4.926907300949097 and perplexity is 137.95220725708467
At time: 455.9538507461548 and batch: 450, loss is 4.873489789962768 and perplexity is 130.77650401348797
At time: 456.7790901660919 and batch: 500, loss is 4.868168048858642 and perplexity is 130.08239389196058
At time: 457.60798597335815 and batch: 550, loss is 4.860114297866821 and perplexity is 129.03895014126576
At time: 458.49070167541504 and batch: 600, loss is 4.885343208312988 and perplexity is 132.33587631399365
At time: 459.3156979084015 and batch: 650, loss is 4.902115058898926 and perplexity is 134.57411106130107
At time: 460.14226770401 and batch: 700, loss is 4.914325809478759 and perplexity is 136.2274355898553
At time: 460.96732902526855 and batch: 750, loss is 4.897864828109741 and perplexity is 134.0033538152529
At time: 461.7964491844177 and batch: 800, loss is 4.916935634613037 and perplexity is 136.583429714381
At time: 462.62309288978577 and batch: 850, loss is 4.955396680831909 and perplexity is 141.9388996477813
At time: 463.4458727836609 and batch: 900, loss is 4.916495790481568 and perplexity is 136.52336750433872
At time: 464.27472734451294 and batch: 950, loss is 4.909762411117554 and perplexity is 135.60719181959627
At time: 465.10324239730835 and batch: 1000, loss is 4.910110521316528 and perplexity is 135.65440628356436
At time: 465.92919516563416 and batch: 1050, loss is 4.915437755584716 and perplexity is 136.37899740505284
At time: 466.7608473300934 and batch: 1100, loss is 4.874023303985596 and perplexity is 130.84629372747568
At time: 467.5920498371124 and batch: 1150, loss is 4.90747820854187 and perplexity is 135.29779102433443
At time: 468.42012667655945 and batch: 1200, loss is 4.9346489906311035 and perplexity is 139.02433512251795
At time: 469.2521047592163 and batch: 1250, loss is 4.957972221374511 and perplexity is 142.30494021221207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.931850015682025 and perplexity of 138.63575355991486
Finished 21 epochs...
Completing Train Step...
At time: 471.78064227104187 and batch: 50, loss is 4.905442428588867 and perplexity is 135.02263466760883
At time: 472.6067337989807 and batch: 100, loss is 4.9031170463562015 and perplexity is 134.7090202100088
At time: 473.43731474876404 and batch: 150, loss is 4.827571611404419 and perplexity is 124.90726867617924
At time: 474.2607047557831 and batch: 200, loss is 4.882098493576049 and perplexity is 131.90718002059808
At time: 475.08497881889343 and batch: 250, loss is 4.903894481658935 and perplexity is 134.81378847791487
At time: 475.9125986099243 and batch: 300, loss is 4.906951627731323 and perplexity is 135.22656455876245
At time: 476.7427372932434 and batch: 350, loss is 4.91942868232727 and perplexity is 136.92436352718042
At time: 477.5686709880829 and batch: 400, loss is 4.912464237213134 and perplexity is 135.97407427231258
At time: 478.3957133293152 and batch: 450, loss is 4.8576812839508055 and perplexity is 128.7253781969999
At time: 479.2228970527649 and batch: 500, loss is 4.853945922851563 and perplexity is 128.24543935727914
At time: 480.1048789024353 and batch: 550, loss is 4.845274238586426 and perplexity is 127.13814339807362
At time: 480.9308588504791 and batch: 600, loss is 4.870507030487061 and perplexity is 130.3870103287037
At time: 481.76259875297546 and batch: 650, loss is 4.890258922576904 and perplexity is 132.9880031903331
At time: 482.5908360481262 and batch: 700, loss is 4.90602970123291 and perplexity is 135.10195305575579
At time: 483.41610980033875 and batch: 750, loss is 4.887562942504883 and perplexity is 132.62995304880548
At time: 484.24088406562805 and batch: 800, loss is 4.899912233352661 and perplexity is 134.27799403840157
At time: 485.0653762817383 and batch: 850, loss is 4.940916471481323 and perplexity is 139.89840372351017
At time: 485.8910253047943 and batch: 900, loss is 4.904813690185547 and perplexity is 134.9377674343159
At time: 486.7176926136017 and batch: 950, loss is 4.896480331420898 and perplexity is 133.81795498724847
At time: 487.55046224594116 and batch: 1000, loss is 4.896373157501221 and perplexity is 133.80361396099497
At time: 488.38327383995056 and batch: 1050, loss is 4.90072039604187 and perplexity is 134.3865563652675
At time: 489.2072899341583 and batch: 1100, loss is 4.859845571517944 and perplexity is 129.00427863411375
At time: 490.03418612480164 and batch: 1150, loss is 4.892170572280884 and perplexity is 133.24247281816457
At time: 490.8597056865692 and batch: 1200, loss is 4.9176224517822265 and perplexity is 136.67726978073108
At time: 491.68563866615295 and batch: 1250, loss is 4.944064054489136 and perplexity is 140.33943929580525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.925762092980155 and perplexity of 137.79431371772696
Finished 22 epochs...
Completing Train Step...
At time: 494.19564628601074 and batch: 50, loss is 4.890467367172241 and perplexity is 133.01572671015134
At time: 495.08040022850037 and batch: 100, loss is 4.892764978408813 and perplexity is 133.3216965036976
At time: 495.90559935569763 and batch: 150, loss is 4.81420334815979 and perplexity is 123.24858697605907
At time: 496.73250436782837 and batch: 200, loss is 4.869596662521363 and perplexity is 130.26836418537556
At time: 497.5583040714264 and batch: 250, loss is 4.890237560272217 and perplexity is 132.9851622904334
At time: 498.3873209953308 and batch: 300, loss is 4.88955062866211 and perplexity is 132.89384194778685
At time: 499.21360969543457 and batch: 350, loss is 4.903380041122436 and perplexity is 134.74445263635576
At time: 500.039754152298 and batch: 400, loss is 4.897722806930542 and perplexity is 133.9843238522883
At time: 500.87022614479065 and batch: 450, loss is 4.843665533065796 and perplexity is 126.93377998925168
At time: 501.7537090778351 and batch: 500, loss is 4.840132932662964 and perplexity is 126.48616475529899
At time: 502.5830931663513 and batch: 550, loss is 4.8330412769317626 and perplexity is 125.59234150944529
At time: 503.4062523841858 and batch: 600, loss is 4.86080379486084 and perplexity is 129.12795278950267
At time: 504.23015570640564 and batch: 650, loss is 4.877389297485352 and perplexity is 131.2874635725103
At time: 505.0561225414276 and batch: 700, loss is 4.89196515083313 and perplexity is 133.21510476758445
At time: 505.89940905570984 and batch: 750, loss is 4.875588245391846 and perplexity is 131.05122081800587
At time: 506.739351272583 and batch: 800, loss is 4.883995265960693 and perplexity is 132.15761535132356
At time: 507.57824516296387 and batch: 850, loss is 4.9271426486969 and perplexity is 137.984677819154
At time: 508.4100639820099 and batch: 900, loss is 4.889520139694214 and perplexity is 132.88979021347305
At time: 509.2436828613281 and batch: 950, loss is 4.885819349288941 and perplexity is 132.3989018506249
At time: 510.0713086128235 and batch: 1000, loss is 4.882174034118652 and perplexity is 131.91714473691528
At time: 510.9008824825287 and batch: 1050, loss is 4.889424724578857 and perplexity is 132.87711112370818
At time: 511.7253153324127 and batch: 1100, loss is 4.848516941070557 and perplexity is 127.55108373097696
At time: 512.5546045303345 and batch: 1150, loss is 4.880652627944946 and perplexity is 131.71659777387222
At time: 513.3795671463013 and batch: 1200, loss is 4.906278781890869 and perplexity is 135.13560853040428
At time: 514.2064611911774 and batch: 1250, loss is 4.931838912963867 and perplexity is 138.63421433476125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.927040267164689 and perplexity of 137.97055145956884
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 516.7183966636658 and batch: 50, loss is 4.883256540298462 and perplexity is 132.06002318076284
At time: 517.5733904838562 and batch: 100, loss is 4.888838586807251 and perplexity is 132.7992496509008
At time: 518.403612613678 and batch: 150, loss is 4.808393230438233 and perplexity is 122.53457443377874
At time: 519.2312939167023 and batch: 200, loss is 4.863458976745606 and perplexity is 129.47126656913392
At time: 520.0622708797455 and batch: 250, loss is 4.881092948913574 and perplexity is 131.77460812443798
At time: 520.8906517028809 and batch: 300, loss is 4.870750141143799 and perplexity is 130.41871265384523
At time: 521.7200803756714 and batch: 350, loss is 4.884854545593262 and perplexity is 132.27122450250826
At time: 522.6060445308685 and batch: 400, loss is 4.863779048919678 and perplexity is 129.51271335153163
At time: 523.4386909008026 and batch: 450, loss is 4.81299280166626 and perplexity is 123.0994791004795
At time: 524.2655305862427 and batch: 500, loss is 4.796086730957032 and perplexity is 121.03584373652814
At time: 525.090754032135 and batch: 550, loss is 4.794762792587281 and perplexity is 120.87570576868119
At time: 525.9173171520233 and batch: 600, loss is 4.816677007675171 and perplexity is 123.55383940562511
At time: 526.7496898174286 and batch: 650, loss is 4.83479902267456 and perplexity is 125.81329504624138
At time: 527.5748338699341 and batch: 700, loss is 4.840708513259887 and perplexity is 126.55898869352374
At time: 528.4039154052734 and batch: 750, loss is 4.814066724777222 and perplexity is 123.23174948743318
At time: 529.2385120391846 and batch: 800, loss is 4.811072368621826 and perplexity is 122.86330164714161
At time: 530.0667405128479 and batch: 850, loss is 4.854152498245239 and perplexity is 128.27193444592402
At time: 530.8940529823303 and batch: 900, loss is 4.812242307662964 and perplexity is 123.00712833828698
At time: 531.7234537601471 and batch: 950, loss is 4.803164119720459 and perplexity is 121.89549992919568
At time: 532.5557107925415 and batch: 1000, loss is 4.792470092773438 and perplexity is 120.59889150782489
At time: 533.3818929195404 and batch: 1050, loss is 4.790297441482544 and perplexity is 120.33715660280774
At time: 534.2133991718292 and batch: 1100, loss is 4.729169578552246 and perplexity is 113.20151833723662
At time: 535.0445923805237 and batch: 1150, loss is 4.762948083877563 and perplexity is 117.09061051083394
At time: 535.8719487190247 and batch: 1200, loss is 4.796025867462158 and perplexity is 121.0284772962493
At time: 536.6986925601959 and batch: 1250, loss is 4.845040540695191 and perplexity is 127.10843495360156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8710973140967155 and perplexity of 130.46399836396483
Finished 24 epochs...
Completing Train Step...
At time: 539.2219207286835 and batch: 50, loss is 4.837225742340088 and perplexity is 126.11897939861586
At time: 540.0493965148926 and batch: 100, loss is 4.84473105430603 and perplexity is 127.06910270974691
At time: 540.8797767162323 and batch: 150, loss is 4.768308172225952 and perplexity is 117.71991157591245
At time: 541.7126989364624 and batch: 200, loss is 4.81790114402771 and perplexity is 123.70517876304314
At time: 542.5381197929382 and batch: 250, loss is 4.8431307888031006 and perplexity is 126.86592102387345
At time: 543.3632833957672 and batch: 300, loss is 4.8370449829101565 and perplexity is 126.09618426407603
At time: 544.2509303092957 and batch: 350, loss is 4.850573587417602 and perplexity is 127.81368114383787
At time: 545.0769808292389 and batch: 400, loss is 4.8355811595916744 and perplexity is 125.91173676139225
At time: 545.9076733589172 and batch: 450, loss is 4.784146862030029 and perplexity is 119.59928485555247
At time: 546.7366244792938 and batch: 500, loss is 4.771733198165894 and perplexity is 118.12379659020841
At time: 547.574497461319 and batch: 550, loss is 4.769304399490356 and perplexity is 117.8372457974587
At time: 548.401296377182 and batch: 600, loss is 4.794136161804199 and perplexity is 120.79998505746188
At time: 549.2304549217224 and batch: 650, loss is 4.81346622467041 and perplexity is 123.15777102296437
At time: 550.0703685283661 and batch: 700, loss is 4.821369237899781 and perplexity is 124.13494473907157
At time: 550.8976385593414 and batch: 750, loss is 4.796878881454468 and perplexity is 121.13176032552285
At time: 551.7245199680328 and batch: 800, loss is 4.798770523071289 and perplexity is 121.36111506456622
At time: 552.5529084205627 and batch: 850, loss is 4.844264478683471 and perplexity is 127.00982919290642
At time: 553.3809752464294 and batch: 900, loss is 4.804528684616089 and perplexity is 122.0619477879591
At time: 554.2071168422699 and batch: 950, loss is 4.796348123550415 and perplexity is 121.06748574492795
At time: 555.0392978191376 and batch: 1000, loss is 4.789836444854736 and perplexity is 120.2816943643486
At time: 555.8676483631134 and batch: 1050, loss is 4.791413068771362 and perplexity is 120.4714829337007
At time: 556.6966874599457 and batch: 1100, loss is 4.735998249053955 and perplexity is 113.97717955957314
At time: 557.5249698162079 and batch: 1150, loss is 4.773383045196534 and perplexity is 118.31884363991549
At time: 558.3511111736298 and batch: 1200, loss is 4.807665882110595 and perplexity is 122.44548152070502
At time: 559.1781344413757 and batch: 1250, loss is 4.848358354568481 and perplexity is 127.53085745462175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.867178144246123 and perplexity of 129.9536884438077
Finished 25 epochs...
Completing Train Step...
At time: 561.6408896446228 and batch: 50, loss is 4.825474596023559 and perplexity is 124.64561065891361
At time: 562.4964144229889 and batch: 100, loss is 4.831367053985596 and perplexity is 125.3822478503146
At time: 563.329820394516 and batch: 150, loss is 4.755472011566162 and perplexity is 116.21849669463246
At time: 564.1550438404083 and batch: 200, loss is 4.804170408248901 and perplexity is 122.01822370985347
At time: 564.9826629161835 and batch: 250, loss is 4.830075101852417 and perplexity is 125.2203645829751
At time: 565.8374192714691 and batch: 300, loss is 4.824104309082031 and perplexity is 124.47492737555355
At time: 566.6749515533447 and batch: 350, loss is 4.837381315231323 and perplexity is 126.13860161918376
At time: 567.5015847682953 and batch: 400, loss is 4.8243381309509275 and perplexity is 124.50403573865654
At time: 568.3310372829437 and batch: 450, loss is 4.772084674835205 and perplexity is 118.165321645917
At time: 569.1585891246796 and batch: 500, loss is 4.7627183628082275 and perplexity is 117.063715419881
At time: 569.9832534790039 and batch: 550, loss is 4.76065920829773 and perplexity is 116.82291115394024
At time: 570.8097624778748 and batch: 600, loss is 4.786590251922608 and perplexity is 119.89186984336429
At time: 571.6341652870178 and batch: 650, loss is 4.806526470184326 and perplexity is 122.30604513157029
At time: 572.468968629837 and batch: 700, loss is 4.814673538208008 and perplexity is 123.30655086106522
At time: 573.2979145050049 and batch: 750, loss is 4.790586338043213 and perplexity is 120.37192661569779
At time: 574.1298105716705 and batch: 800, loss is 4.794566698074341 and perplexity is 120.85200502990061
At time: 574.959321975708 and batch: 850, loss is 4.840567674636841 and perplexity is 126.5411655549446
At time: 575.7858648300171 and batch: 900, loss is 4.801867074966431 and perplexity is 121.73749850008606
At time: 576.6127481460571 and batch: 950, loss is 4.794246616363526 and perplexity is 120.81332870349775
At time: 577.4398484230042 and batch: 1000, loss is 4.7892108917236325 and perplexity is 120.20647530303704
At time: 578.2696063518524 and batch: 1050, loss is 4.792768869400025 and perplexity is 120.63492902111497
At time: 579.0967445373535 and batch: 1100, loss is 4.739039573669434 and perplexity is 114.32434882098114
At time: 579.9293410778046 and batch: 1150, loss is 4.777032804489136 and perplexity is 118.7514679460313
At time: 580.765275478363 and batch: 1200, loss is 4.8113282203674315 and perplexity is 122.89474045900486
At time: 581.62255692482 and batch: 1250, loss is 4.8476605796813965 and perplexity is 127.44190066447796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.865821114421761 and perplexity of 129.77745701550043
Finished 26 epochs...
Completing Train Step...
At time: 584.1155595779419 and batch: 50, loss is 4.817428693771363 and perplexity is 123.64674802352086
At time: 584.9376788139343 and batch: 100, loss is 4.8227166652679445 and perplexity is 124.30232029884014
At time: 585.7694597244263 and batch: 150, loss is 4.747217874526978 and perplexity is 115.26316145439424
At time: 586.594694852829 and batch: 200, loss is 4.795993375778198 and perplexity is 121.02454494109958
At time: 587.4527206420898 and batch: 250, loss is 4.8229140949249265 and perplexity is 124.32686368601534
At time: 588.2859048843384 and batch: 300, loss is 4.816456260681153 and perplexity is 123.52656827709747
At time: 589.1144554615021 and batch: 350, loss is 4.8296035957336425 and perplexity is 125.16133633206337
At time: 589.9408483505249 and batch: 400, loss is 4.817147855758667 and perplexity is 123.61202819208206
At time: 590.7673692703247 and batch: 450, loss is 4.765245094299316 and perplexity is 117.35987800018768
At time: 591.5957064628601 and batch: 500, loss is 4.75715030670166 and perplexity is 116.41370939873485
At time: 592.4259378910065 and batch: 550, loss is 4.75501651763916 and perplexity is 116.16557192955234
At time: 593.2520189285278 and batch: 600, loss is 4.7809391212463375 and perplexity is 119.21625600888176
At time: 594.0796070098877 and batch: 650, loss is 4.802069597244262 and perplexity is 121.76215555229672
At time: 594.9087498188019 and batch: 700, loss is 4.8099026870727535 and perplexity is 122.71967472540233
At time: 595.7328991889954 and batch: 750, loss is 4.786240062713623 and perplexity is 119.84989235476091
At time: 596.5629048347473 and batch: 800, loss is 4.791553821563721 and perplexity is 120.48844082473042
At time: 597.3944473266602 and batch: 850, loss is 4.83758994102478 and perplexity is 126.16492013029676
At time: 598.2198915481567 and batch: 900, loss is 4.799795608520508 and perplexity is 121.48558436265377
At time: 599.0504956245422 and batch: 950, loss is 4.791801824569702 and perplexity is 120.51832602589764
At time: 599.8755521774292 and batch: 1000, loss is 4.788537626266479 and perplexity is 120.12557167335503
At time: 600.7018280029297 and batch: 1050, loss is 4.792494421005249 and perplexity is 120.60182550130297
At time: 601.5316841602325 and batch: 1100, loss is 4.740435180664062 and perplexity is 114.48401206951033
At time: 602.3626124858856 and batch: 1150, loss is 4.778310623168945 and perplexity is 118.9033077812558
At time: 603.1908943653107 and batch: 1200, loss is 4.812006168365478 and perplexity is 122.97808495069297
At time: 604.0188400745392 and batch: 1250, loss is 4.845443916320801 and perplexity is 127.15971774048906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.864550068430657 and perplexity of 129.61260868612277
Finished 27 epochs...
Completing Train Step...
At time: 606.5034742355347 and batch: 50, loss is 4.811236486434937 and perplexity is 122.88346735825048
At time: 607.3572506904602 and batch: 100, loss is 4.816664218902588 and perplexity is 123.55225931377491
At time: 608.1836218833923 and batch: 150, loss is 4.741215915679931 and perplexity is 114.57342864727485
At time: 609.0417218208313 and batch: 200, loss is 4.789548273086548 and perplexity is 120.24703756959768
At time: 609.8692259788513 and batch: 250, loss is 4.816904411315918 and perplexity is 123.58193919341018
At time: 610.698831319809 and batch: 300, loss is 4.8108930492401125 and perplexity is 122.84127185109874
At time: 611.5220477581024 and batch: 350, loss is 4.823611183166504 and perplexity is 124.4135606950251
At time: 612.3488962650299 and batch: 400, loss is 4.811954174041748 and perplexity is 122.97169095455955
At time: 613.1786272525787 and batch: 450, loss is 4.759777545928955 and perplexity is 116.71995818091516
At time: 614.0072877407074 and batch: 500, loss is 4.752421703338623 and perplexity is 115.86453457919883
At time: 614.8326127529144 and batch: 550, loss is 4.750432367324829 and perplexity is 115.63427020038817
At time: 615.6550490856171 and batch: 600, loss is 4.7765802478790285 and perplexity is 118.69773834301289
At time: 616.4817364215851 and batch: 650, loss is 4.7987069892883305 and perplexity is 121.35340477875656
At time: 617.3079769611359 and batch: 700, loss is 4.8070470809936525 and perplexity is 122.36973555823552
At time: 618.1340591907501 and batch: 750, loss is 4.7833113193511965 and perplexity is 119.49939628509091
At time: 618.9645702838898 and batch: 800, loss is 4.789054126739502 and perplexity is 120.18763261381926
At time: 619.7948477268219 and batch: 850, loss is 4.835442419052124 and perplexity is 125.89426891087578
At time: 620.6233465671539 and batch: 900, loss is 4.798200731277466 and perplexity is 121.29198419408492
At time: 621.4483244419098 and batch: 950, loss is 4.789997501373291 and perplexity is 120.30106807537811
At time: 622.2746980190277 and batch: 1000, loss is 4.786897420883179 and perplexity is 119.9287025610492
At time: 623.099707365036 and batch: 1050, loss is 4.791533041000366 and perplexity is 120.48593703306756
At time: 623.9285252094269 and batch: 1100, loss is 4.740037717819214 and perplexity is 114.43851797008955
At time: 624.7531378269196 and batch: 1150, loss is 4.778377790451049 and perplexity is 118.91129446149147
At time: 625.583878993988 and batch: 1200, loss is 4.81154034614563 and perplexity is 122.92081236662536
At time: 626.4106814861298 and batch: 1250, loss is 4.843112411499024 and perplexity is 126.86358959168862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.863400647239963 and perplexity of 129.46371479440782
Finished 28 epochs...
Completing Train Step...
At time: 628.859442949295 and batch: 50, loss is 4.806180744171143 and perplexity is 122.26376805876177
At time: 629.719898223877 and batch: 100, loss is 4.81134744644165 and perplexity is 122.89710326511971
At time: 630.5514149665833 and batch: 150, loss is 4.736116895675659 and perplexity is 113.99070336914055
At time: 631.3841195106506 and batch: 200, loss is 4.784333848953247 and perplexity is 119.62165044881826
At time: 632.210352897644 and batch: 250, loss is 4.812031517028808 and perplexity is 122.98120232027578
At time: 633.0348761081696 and batch: 300, loss is 4.806005744934082 and perplexity is 122.24237386466976
At time: 633.863762140274 and batch: 350, loss is 4.818278770446778 and perplexity is 123.75190192811512
At time: 634.6894850730896 and batch: 400, loss is 4.807295150756836 and perplexity is 122.40009555509923
At time: 635.518748998642 and batch: 450, loss is 4.755215883255005 and perplexity is 116.18873365908954
At time: 636.3449232578278 and batch: 500, loss is 4.748726596832276 and perplexity is 115.43719280653954
At time: 637.1720943450928 and batch: 550, loss is 4.747069101333619 and perplexity is 115.24601466131118
At time: 638.009202003479 and batch: 600, loss is 4.773452835083008 and perplexity is 118.32710138673114
At time: 638.8371248245239 and batch: 650, loss is 4.795546827316284 and perplexity is 120.97051368138806
At time: 639.6640572547913 and batch: 700, loss is 4.803930988311768 and perplexity is 121.98901361128323
At time: 640.4921748638153 and batch: 750, loss is 4.780243787765503 and perplexity is 119.13338976779373
At time: 641.3196489810944 and batch: 800, loss is 4.786750688552856 and perplexity is 119.9111064340418
At time: 642.1442470550537 and batch: 850, loss is 4.83318323135376 and perplexity is 125.61017116316074
At time: 642.9700441360474 and batch: 900, loss is 4.79670654296875 and perplexity is 121.11088646011287
At time: 643.7956912517548 and batch: 950, loss is 4.787903728485108 and perplexity is 120.04944846969929
At time: 644.6212966442108 and batch: 1000, loss is 4.78533314704895 and perplexity is 119.7412478831083
At time: 645.4519510269165 and batch: 1050, loss is 4.790131492614746 and perplexity is 120.31718844480794
At time: 646.2775349617004 and batch: 1100, loss is 4.73970817565918 and perplexity is 114.40081186690317
At time: 647.1063232421875 and batch: 1150, loss is 4.777748775482178 and perplexity is 118.83652099658707
At time: 647.9349956512451 and batch: 1200, loss is 4.810011014938355 and perplexity is 122.7329694059118
At time: 648.7652173042297 and batch: 1250, loss is 4.840396175384521 and perplexity is 126.51946570047188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.862214694057938 and perplexity of 129.310267898293
Finished 29 epochs...
Completing Train Step...
At time: 651.2525427341461 and batch: 50, loss is 4.801164674758911 and perplexity is 121.65202007942119
At time: 652.0826818943024 and batch: 100, loss is 4.806767587661743 and perplexity is 122.33553881222332
At time: 652.9114120006561 and batch: 150, loss is 4.73185302734375 and perplexity is 113.50569675589026
At time: 653.7394576072693 and batch: 200, loss is 4.779810285568237 and perplexity is 119.08175637396506
At time: 654.5666415691376 and batch: 250, loss is 4.807788228988647 and perplexity is 122.46046325956642
At time: 655.3921883106232 and batch: 300, loss is 4.8014138889312745 and perplexity is 121.6823412649993
At time: 656.2186124324799 and batch: 350, loss is 4.813770351409912 and perplexity is 123.19523229050769
At time: 657.0506575107574 and batch: 400, loss is 4.80289810180664 and perplexity is 121.86307785521805
At time: 657.8809585571289 and batch: 450, loss is 4.750987129211426 and perplexity is 115.69843748341559
At time: 658.7115659713745 and batch: 500, loss is 4.745444049835205 and perplexity is 115.05888604050313
At time: 659.536262512207 and batch: 550, loss is 4.743875713348388 and perplexity is 114.87857642142922
At time: 660.3617610931396 and batch: 600, loss is 4.770405731201172 and perplexity is 117.96709518346897
At time: 661.1873004436493 and batch: 650, loss is 4.792811536788941 and perplexity is 120.64007630835826
At time: 662.0151214599609 and batch: 700, loss is 4.800886611938477 and perplexity is 121.61819787817268
At time: 662.8423931598663 and batch: 750, loss is 4.777691450119018 and perplexity is 118.82970884512072
At time: 663.6687972545624 and batch: 800, loss is 4.784462594985962 and perplexity is 119.63705225317966
At time: 664.4933171272278 and batch: 850, loss is 4.831186103820801 and perplexity is 125.35956196447299
At time: 665.3166728019714 and batch: 900, loss is 4.794237852096558 and perplexity is 120.81226986787165
At time: 666.1496386528015 and batch: 950, loss is 4.785587348937988 and perplexity is 119.7716902035997
At time: 666.9889400005341 and batch: 1000, loss is 4.783344554901123 and perplexity is 119.50336797924267
At time: 667.8145201206207 and batch: 1050, loss is 4.788413228988648 and perplexity is 120.11062930865492
At time: 668.6410479545593 and batch: 1100, loss is 4.738162088394165 and perplexity is 114.22407488916943
At time: 669.4667594432831 and batch: 1150, loss is 4.776715707778931 and perplexity is 118.71381821583186
At time: 670.2907526493073 and batch: 1200, loss is 4.807815351486206 and perplexity is 122.46378473822546
At time: 671.1171417236328 and batch: 1250, loss is 4.837299385070801 and perplexity is 126.12826748664935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.860953003820712 and perplexity of 129.14722127450835
Finished 30 epochs...
Completing Train Step...
At time: 673.57670378685 and batch: 50, loss is 4.796746292114258 and perplexity is 121.11570061003985
At time: 674.4577612876892 and batch: 100, loss is 4.802355871200562 and perplexity is 121.79701787614383
At time: 675.2835676670074 and batch: 150, loss is 4.727771492004394 and perplexity is 113.04336340015938
At time: 676.1099193096161 and batch: 200, loss is 4.775487079620361 and perplexity is 118.5680526401424
At time: 676.9369518756866 and batch: 250, loss is 4.80372031211853 and perplexity is 121.96331613729686
At time: 677.7609624862671 and batch: 300, loss is 4.797115545272828 and perplexity is 121.1604312229943
At time: 678.5869660377502 and batch: 350, loss is 4.809961099624633 and perplexity is 122.72684330413435
At time: 679.4157063961029 and batch: 400, loss is 4.798496046066284 and perplexity is 121.32780880029023
At time: 680.2452783584595 and batch: 450, loss is 4.747075691223144 and perplexity is 115.24677412231844
At time: 681.0769398212433 and batch: 500, loss is 4.742407321929932 and perplexity is 114.710013494113
At time: 681.9013564586639 and batch: 550, loss is 4.740568962097168 and perplexity is 114.49932892924555
At time: 682.7274785041809 and batch: 600, loss is 4.767467803955078 and perplexity is 117.62102505374146
At time: 683.5533084869385 and batch: 650, loss is 4.789857225418091 and perplexity is 120.28419391168964
At time: 684.379474401474 and batch: 700, loss is 4.7980391311645505 and perplexity is 121.27238497940375
At time: 685.20916056633 and batch: 750, loss is 4.775079641342163 and perplexity is 118.5197533170901
At time: 686.0335655212402 and batch: 800, loss is 4.781780843734741 and perplexity is 119.31664525656429
At time: 686.8575730323792 and batch: 850, loss is 4.828727207183838 and perplexity is 125.0516944214988
At time: 687.6854531764984 and batch: 900, loss is 4.791894588470459 and perplexity is 120.52950629448718
At time: 688.5172002315521 and batch: 950, loss is 4.78322317123413 and perplexity is 119.48886310256478
At time: 689.3553991317749 and batch: 1000, loss is 4.781610317230225 and perplexity is 119.29630034084107
At time: 690.1837513446808 and batch: 1050, loss is 4.786817531585694 and perplexity is 119.91912192395361
At time: 691.0115892887115 and batch: 1100, loss is 4.736569948196411 and perplexity is 114.04235884508253
At time: 691.8348274230957 and batch: 1150, loss is 4.775173931121826 and perplexity is 118.5309290453863
At time: 692.6601507663727 and batch: 1200, loss is 4.805868167877197 and perplexity is 122.22555727546172
At time: 693.5150136947632 and batch: 1250, loss is 4.834448966979981 and perplexity is 125.76926109347423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.859819175553148 and perplexity of 129.00087348665764
Finished 31 epochs...
Completing Train Step...
At time: 696.0002179145813 and batch: 50, loss is 4.792793235778809 and perplexity is 120.63786849330208
At time: 696.824588060379 and batch: 100, loss is 4.7985138320922855 and perplexity is 121.32996675904293
At time: 697.6507685184479 and batch: 150, loss is 4.723761806488037 and perplexity is 112.59100258159664
At time: 698.4757232666016 and batch: 200, loss is 4.771929035186767 and perplexity is 118.14693186792492
At time: 699.3050935268402 and batch: 250, loss is 4.800345811843872 and perplexity is 121.55244452656643
At time: 700.1300160884857 and batch: 300, loss is 4.793667125701904 and perplexity is 120.7433387889242
At time: 700.9594655036926 and batch: 350, loss is 4.806065893173217 and perplexity is 122.24972674933474
At time: 701.7850472927094 and batch: 400, loss is 4.794515810012817 and perplexity is 120.8458552621096
At time: 702.6096851825714 and batch: 450, loss is 4.743024816513062 and perplexity is 114.78086818001735
At time: 703.4371764659882 and batch: 500, loss is 4.73927869796753 and perplexity is 114.35168981946649
At time: 704.2736964225769 and batch: 550, loss is 4.737591819763184 and perplexity is 114.15895505202226
At time: 705.0977826118469 and batch: 600, loss is 4.764498863220215 and perplexity is 117.272333080217
At time: 705.9289808273315 and batch: 650, loss is 4.787335186004639 and perplexity is 119.98121465723588
At time: 706.7548694610596 and batch: 700, loss is 4.795337524414062 and perplexity is 120.94519685132676
At time: 707.5856783390045 and batch: 750, loss is 4.772151708602905 and perplexity is 118.17324297813384
At time: 708.4105536937714 and batch: 800, loss is 4.77954740524292 and perplexity is 119.05045623738548
At time: 709.2341239452362 and batch: 850, loss is 4.826678895950318 and perplexity is 124.79581178414944
At time: 710.0605766773224 and batch: 900, loss is 4.7897536563873295 and perplexity is 120.27173683940488
At time: 710.886069059372 and batch: 950, loss is 4.781155138015747 and perplexity is 119.2420115010749
At time: 711.7113180160522 and batch: 1000, loss is 4.779694061279297 and perplexity is 119.06791698575692
At time: 712.5373010635376 and batch: 1050, loss is 4.785067329406738 and perplexity is 119.70942277694519
At time: 713.3629584312439 and batch: 1100, loss is 4.735291690826416 and perplexity is 113.89667648900615
At time: 714.1874725818634 and batch: 1150, loss is 4.773799686431885 and perplexity is 118.36815042000059
At time: 715.0733773708344 and batch: 1200, loss is 4.803724575042724 and perplexity is 121.9638360587762
At time: 715.8974816799164 and batch: 1250, loss is 4.8316820526123045 and perplexity is 125.42174930732685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.858761975364963 and perplexity of 128.86456580388113
Finished 32 epochs...
Completing Train Step...
At time: 718.3669652938843 and batch: 50, loss is 4.789053573608398 and perplexity is 120.1875661343198
At time: 719.2610709667206 and batch: 100, loss is 4.79488052368164 and perplexity is 120.88993743554126
At time: 720.0872116088867 and batch: 150, loss is 4.720478458404541 and perplexity is 112.22193335198398
At time: 720.9165325164795 and batch: 200, loss is 4.767941913604736 and perplexity is 117.67680353823614
At time: 721.7500507831573 and batch: 250, loss is 4.796821117401123 and perplexity is 121.12476346614305
At time: 722.5819504261017 and batch: 300, loss is 4.78998366355896 and perplexity is 120.29940338305217
At time: 723.4086256027222 and batch: 350, loss is 4.802469644546509 and perplexity is 121.81087591871696
At time: 724.2426691055298 and batch: 400, loss is 4.790684423446655 and perplexity is 120.3837339237358
At time: 725.067389011383 and batch: 450, loss is 4.7397489166259765 and perplexity is 114.40547276152498
At time: 725.8924057483673 and batch: 500, loss is 4.7362383842468265 and perplexity is 114.00455277807467
At time: 726.719723701477 and batch: 550, loss is 4.734499387741089 and perplexity is 113.80647154036376
At time: 727.5603756904602 and batch: 600, loss is 4.761468439102173 and perplexity is 116.91748611363505
At time: 728.406147480011 and batch: 650, loss is 4.784543056488037 and perplexity is 119.64667881738643
At time: 729.2347090244293 and batch: 700, loss is 4.792966146469116 and perplexity is 120.65872987394633
At time: 730.0663166046143 and batch: 750, loss is 4.769236640930176 and perplexity is 117.82926158585023
At time: 730.897599697113 and batch: 800, loss is 4.777373113632202 and perplexity is 118.7918870334283
At time: 731.7227909564972 and batch: 850, loss is 4.8244895744323735 and perplexity is 124.52289249111296
At time: 732.5501959323883 and batch: 900, loss is 4.787692747116089 and perplexity is 120.02412294441211
At time: 733.3788540363312 and batch: 950, loss is 4.778866243362427 and perplexity is 118.96939121707648
At time: 734.2118492126465 and batch: 1000, loss is 4.7777868747711185 and perplexity is 118.84104866978723
At time: 735.0435049533844 and batch: 1050, loss is 4.783012638092041 and perplexity is 119.46370938470962
At time: 735.9290199279785 and batch: 1100, loss is 4.733580894470215 and perplexity is 113.70198905264841
At time: 736.7584209442139 and batch: 1150, loss is 4.77194390296936 and perplexity is 118.14868846388023
At time: 737.5886290073395 and batch: 1200, loss is 4.801128749847412 and perplexity is 121.64764981986724
At time: 738.6746666431427 and batch: 1250, loss is 4.828347463607788 and perplexity is 125.00421585927809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.857325644388686 and perplexity of 128.6796064994599
Finished 33 epochs...
Completing Train Step...
At time: 741.1348407268524 and batch: 50, loss is 4.785468702316284 and perplexity is 119.75748054016219
At time: 741.9662787914276 and batch: 100, loss is 4.791231880187988 and perplexity is 120.4496568537489
At time: 742.7944116592407 and batch: 150, loss is 4.717049160003662 and perplexity is 111.83774997162489
At time: 743.6220469474792 and batch: 200, loss is 4.764670667648315 and perplexity is 117.29248271718266
At time: 744.4509601593018 and batch: 250, loss is 4.793658685684204 and perplexity is 120.74231971730819
At time: 745.2781074047089 and batch: 300, loss is 4.786331195831298 and perplexity is 119.86081514681075
At time: 746.1614317893982 and batch: 350, loss is 4.798980216979981 and perplexity is 121.38656641955114
At time: 746.9932954311371 and batch: 400, loss is 4.787055568695068 and perplexity is 119.9476705227733
At time: 747.8298242092133 and batch: 450, loss is 4.7361435699462895 and perplexity is 113.99374402856509
At time: 748.6577441692352 and batch: 500, loss is 4.733198661804199 and perplexity is 113.65853674321654
At time: 749.4867906570435 and batch: 550, loss is 4.731490545272827 and perplexity is 113.46456043190904
At time: 750.3155710697174 and batch: 600, loss is 4.758557682037353 and perplexity is 116.57766252681166
At time: 751.1402525901794 and batch: 650, loss is 4.781567106246948 and perplexity is 119.29114554177481
At time: 751.969925403595 and batch: 700, loss is 4.790241460800171 and perplexity is 120.33042023522135
At time: 752.7974905967712 and batch: 750, loss is 4.766730804443359 and perplexity is 117.53437035186774
At time: 753.6352264881134 and batch: 800, loss is 4.775062770843506 and perplexity is 118.51775384661694
At time: 754.4643177986145 and batch: 850, loss is 4.8223238277435305 and perplexity is 124.25349927304833
At time: 755.290367603302 and batch: 900, loss is 4.785676393508911 and perplexity is 119.7823556972077
At time: 756.1194922924042 and batch: 950, loss is 4.776618251800537 and perplexity is 118.70224940826265
At time: 756.947009563446 and batch: 1000, loss is 4.77572265625 and perplexity is 118.59598779267158
At time: 757.7752549648285 and batch: 1050, loss is 4.780882015228271 and perplexity is 119.20944823759653
At time: 758.603111743927 and batch: 1100, loss is 4.731395711898804 and perplexity is 113.45380071500944
At time: 759.4306764602661 and batch: 1150, loss is 4.769972848892212 and perplexity is 117.9160403660855
At time: 760.2622365951538 and batch: 1200, loss is 4.798792839050293 and perplexity is 121.36382338688125
At time: 761.0885770320892 and batch: 1250, loss is 4.825659818649292 and perplexity is 124.6686999844727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.856202953923358 and perplexity of 128.53522019788863
Finished 34 epochs...
Completing Train Step...
At time: 763.6308467388153 and batch: 50, loss is 4.782347173690796 and perplexity is 119.38423698483271
At time: 764.4569540023804 and batch: 100, loss is 4.7877606010437015 and perplexity is 120.03226732887326
At time: 765.2859878540039 and batch: 150, loss is 4.7136768627166745 and perplexity is 111.4612350477637
At time: 766.1145811080933 and batch: 200, loss is 4.76095983505249 and perplexity is 116.85803652615088
At time: 766.9399511814117 and batch: 250, loss is 4.7904442691802975 and perplexity is 120.35482672766611
At time: 767.7750806808472 and batch: 300, loss is 4.782751245498657 and perplexity is 119.43248653678668
At time: 768.6017892360687 and batch: 350, loss is 4.7954128551483155 and perplexity is 120.95430808498361
At time: 769.4293482303619 and batch: 400, loss is 4.783640756607055 and perplexity is 119.53877032356135
At time: 770.2548160552979 and batch: 450, loss is 4.732768907546997 and perplexity is 113.60970199740534
At time: 771.0827813148499 and batch: 500, loss is 4.730330114364624 and perplexity is 113.33296901513857
At time: 771.9104251861572 and batch: 550, loss is 4.728388423919678 and perplexity is 113.1131249757105
At time: 772.7383100986481 and batch: 600, loss is 4.7554841899871825 and perplexity is 116.21991206103398
At time: 773.5647399425507 and batch: 650, loss is 4.778310127258301 and perplexity is 118.90324881585448
At time: 774.4437968730927 and batch: 700, loss is 4.787227478027344 and perplexity is 119.96829241921827
At time: 775.277939081192 and batch: 750, loss is 4.763809356689453 and perplexity is 117.19150091103722
At time: 776.1050701141357 and batch: 800, loss is 4.772733106613159 and perplexity is 118.24196864301305
At time: 776.9339122772217 and batch: 850, loss is 4.819810209274292 and perplexity is 123.94156558778455
At time: 777.7657070159912 and batch: 900, loss is 4.783344230651855 and perplexity is 119.50332923036943
At time: 778.5971639156342 and batch: 950, loss is 4.774132270812988 and perplexity is 118.407524365267
At time: 779.4242599010468 and batch: 1000, loss is 4.773448095321656 and perplexity is 118.32654054583818
At time: 780.2516856193542 and batch: 1050, loss is 4.778687887191772 and perplexity is 118.94817418418448
At time: 781.0875363349915 and batch: 1100, loss is 4.729219274520874 and perplexity is 113.20714413612919
At time: 781.9147763252258 and batch: 1150, loss is 4.7677207279205325 and perplexity is 117.65077799227582
At time: 782.7443337440491 and batch: 1200, loss is 4.795851812362671 and perplexity is 121.00741350577599
At time: 783.5716416835785 and batch: 1250, loss is 4.822888584136963 and perplexity is 124.32369205021327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.854832558736314 and perplexity of 128.35919678912737
Finished 35 epochs...
Completing Train Step...
At time: 786.0383610725403 and batch: 50, loss is 4.779298057556153 and perplexity is 119.020774982144
At time: 786.8933997154236 and batch: 100, loss is 4.784090738296509 and perplexity is 119.59257268551781
At time: 787.71768450737 and batch: 150, loss is 4.710500450134277 and perplexity is 111.10774988288131
At time: 788.5468139648438 and batch: 200, loss is 4.75749174118042 and perplexity is 116.4534638393061
At time: 789.3786742687225 and batch: 250, loss is 4.787170190811157 and perplexity is 119.96141996656861
At time: 790.2108781337738 and batch: 300, loss is 4.7790062046051025 and perplexity is 118.98604348622052
At time: 791.0371336936951 and batch: 350, loss is 4.79178334236145 and perplexity is 120.51609860168178
At time: 791.8625721931458 and batch: 400, loss is 4.780369091033935 and perplexity is 119.14831850620138
At time: 792.69336104393 and batch: 450, loss is 4.72913028717041 and perplexity is 113.19707058053504
At time: 793.5206394195557 and batch: 500, loss is 4.727246246337891 and perplexity is 112.98400345404836
At time: 794.351879119873 and batch: 550, loss is 4.725439767837525 and perplexity is 112.78008452396004
At time: 795.23792719841 and batch: 600, loss is 4.75234712600708 and perplexity is 115.85589403358784
At time: 796.0679748058319 and batch: 650, loss is 4.775349531173706 and perplexity is 118.55174491025602
At time: 796.9049680233002 and batch: 700, loss is 4.784510459899902 and perplexity is 119.64277880743921
At time: 797.7312717437744 and batch: 750, loss is 4.761098251342774 and perplexity is 116.87421270155394
At time: 798.5601282119751 and batch: 800, loss is 4.770379343032837 and perplexity is 117.96398228897526
At time: 799.3871080875397 and batch: 850, loss is 4.817493667602539 and perplexity is 123.65478208745161
At time: 800.2175397872925 and batch: 900, loss is 4.780968503952026 and perplexity is 119.2197589565082
At time: 801.0465705394745 and batch: 950, loss is 4.771709728240967 and perplexity is 118.12102426610359
At time: 801.877438545227 and batch: 1000, loss is 4.7708570003509525 and perplexity is 118.020342107658
At time: 802.7332620620728 and batch: 1050, loss is 4.776261596679688 and perplexity is 118.65992119189113
At time: 803.5900223255157 and batch: 1100, loss is 4.726190242767334 and perplexity is 112.86475491750691
At time: 804.4196543693542 and batch: 1150, loss is 4.764795122146606 and perplexity is 117.3070812026773
At time: 805.2441589832306 and batch: 1200, loss is 4.792752819061279 and perplexity is 120.63299280517818
At time: 806.0744013786316 and batch: 1250, loss is 4.81940333366394 and perplexity is 123.8911470453713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.853143622405337 and perplexity of 128.14258924791767
Finished 36 epochs...
Completing Train Step...
At time: 808.5610473155975 and batch: 50, loss is 4.776158943176269 and perplexity is 118.64774096044782
At time: 809.3827130794525 and batch: 100, loss is 4.780375957489014 and perplexity is 119.14913663558688
At time: 810.2139687538147 and batch: 150, loss is 4.70717526435852 and perplexity is 110.73890954456893
At time: 811.042151927948 and batch: 200, loss is 4.753902769088745 and perplexity is 116.0362647134363
At time: 811.8674004077911 and batch: 250, loss is 4.783573875427246 and perplexity is 119.53077569691715
At time: 812.6930265426636 and batch: 300, loss is 4.775156831741333 and perplexity is 118.52890225725874
At time: 813.5190424919128 and batch: 350, loss is 4.788141527175903 and perplexity is 120.07799946593632
At time: 814.344334602356 and batch: 400, loss is 4.777023038864136 and perplexity is 118.7503082693896
At time: 815.1708750724792 and batch: 450, loss is 4.725292234420777 and perplexity is 112.76344692008051
At time: 816.0033140182495 and batch: 500, loss is 4.724228601455689 and perplexity is 112.64357176356486
At time: 816.8858866691589 and batch: 550, loss is 4.72224268913269 and perplexity is 112.4200934838748
At time: 817.7152440547943 and batch: 600, loss is 4.7487430763244625 and perplexity is 115.43909516853141
At time: 818.5425732135773 and batch: 650, loss is 4.7718010425567625 and perplexity is 118.13181089909507
At time: 819.3675560951233 and batch: 700, loss is 4.7810225009918215 and perplexity is 119.2261966443834
At time: 820.1927816867828 and batch: 750, loss is 4.758793926239013 and perplexity is 116.60520657705966
At time: 821.020991563797 and batch: 800, loss is 4.768089942932129 and perplexity is 117.69422444568438
At time: 821.8507678508759 and batch: 850, loss is 4.814764528274536 and perplexity is 123.31777104278626
At time: 822.6776239871979 and batch: 900, loss is 4.777962408065796 and perplexity is 118.86191106157176
At time: 823.5039422512054 and batch: 950, loss is 4.7690029716491695 and perplexity is 117.80173172358079
At time: 824.3296217918396 and batch: 1000, loss is 4.767787504196167 and perplexity is 117.65863453536805
At time: 825.160701751709 and batch: 1050, loss is 4.7734777545928955 and perplexity is 118.33005007684392
At time: 825.9941368103027 and batch: 1100, loss is 4.72330641746521 and perplexity is 112.53974154769384
At time: 826.8234574794769 and batch: 1150, loss is 4.761494359970093 and perplexity is 116.92051675562836
At time: 827.6489546298981 and batch: 1200, loss is 4.789654388427734 and perplexity is 120.25979830205875
At time: 828.4759950637817 and batch: 1250, loss is 4.816413774490356 and perplexity is 123.52132021523506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.850475004989735 and perplexity of 127.80108158189397
Finished 37 epochs...
Completing Train Step...
At time: 830.968337059021 and batch: 50, loss is 4.772910137176513 and perplexity is 118.26290293828443
At time: 831.8424806594849 and batch: 100, loss is 4.776627511978149 and perplexity is 118.70334861726457
At time: 832.6695477962494 and batch: 150, loss is 4.703606271743775 and perplexity is 110.34438763590391
At time: 833.4957838058472 and batch: 200, loss is 4.750091724395752 and perplexity is 115.59488691208554
At time: 834.3202555179596 and batch: 250, loss is 4.780033016204834 and perplexity is 119.10828248334833
At time: 835.1470513343811 and batch: 300, loss is 4.770865030288697 and perplexity is 118.02128980746264
At time: 835.9767808914185 and batch: 350, loss is 4.7844174861907955 and perplexity is 119.63165569161234
At time: 836.8092594146729 and batch: 400, loss is 4.773227987289428 and perplexity is 118.30049878994335
At time: 837.6378331184387 and batch: 450, loss is 4.72154016494751 and perplexity is 112.3411433847263
At time: 838.5221197605133 and batch: 500, loss is 4.7212095546722415 and perplexity is 112.30400838733199
At time: 839.3468792438507 and batch: 550, loss is 4.7189364528656 and perplexity is 112.04901986020818
At time: 840.1748814582825 and batch: 600, loss is 4.745041112899781 and perplexity is 115.01253390468952
At time: 841.0020937919617 and batch: 650, loss is 4.768376064300537 and perplexity is 117.72790409624066
At time: 841.8269197940826 and batch: 700, loss is 4.778058118820191 and perplexity is 118.8732879691858
At time: 842.6532759666443 and batch: 750, loss is 4.756002063751221 and perplexity is 116.280114891737
At time: 843.4789338111877 and batch: 800, loss is 4.765321073532104 and perplexity is 117.36879525243693
At time: 844.3107995986938 and batch: 850, loss is 4.812046899795532 and perplexity is 122.98309412597303
At time: 845.1397309303284 and batch: 900, loss is 4.774544248580932 and perplexity is 118.45631568264174
At time: 845.9666562080383 and batch: 950, loss is 4.76616886138916 and perplexity is 117.46834128284482
At time: 846.7977809906006 and batch: 1000, loss is 4.764941539764404 and perplexity is 117.32425828354079
At time: 847.6242077350616 and batch: 1050, loss is 4.77068172454834 and perplexity is 117.99965781025186
At time: 848.4511721134186 and batch: 1100, loss is 4.719612321853638 and perplexity is 112.12477591557213
At time: 849.2808523178101 and batch: 1150, loss is 4.757937803268432 and perplexity is 116.50542090171889
At time: 850.1107368469238 and batch: 1200, loss is 4.786205701828003 and perplexity is 119.8457742770689
At time: 850.9402267932892 and batch: 1250, loss is 4.812609004974365 and perplexity is 123.05224299273752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8477818844092155 and perplexity of 127.45736090723211
Finished 38 epochs...
Completing Train Step...
At time: 853.4347422122955 and batch: 50, loss is 4.769514007568359 and perplexity is 117.86194802487024
At time: 854.2600796222687 and batch: 100, loss is 4.7730252552032475 and perplexity is 118.27651791395607
At time: 855.0863857269287 and batch: 150, loss is 4.6998491287231445 and perplexity is 109.93058583308266
At time: 855.9146518707275 and batch: 200, loss is 4.745996799468994 and perplexity is 115.12250237796003
At time: 856.7435269355774 and batch: 250, loss is 4.776511964797973 and perplexity is 118.68963357243703
At time: 857.5677487850189 and batch: 300, loss is 4.766821727752686 and perplexity is 117.54505745162567
At time: 858.3991904258728 and batch: 350, loss is 4.7804931640625 and perplexity is 119.16310251605634
At time: 859.228084564209 and batch: 400, loss is 4.7698219871521 and perplexity is 117.89825268882248
At time: 860.1095819473267 and batch: 450, loss is 4.718227376937866 and perplexity is 111.96959675933134
At time: 860.9403586387634 and batch: 500, loss is 4.718246383666992 and perplexity is 111.97172495535222
At time: 861.766256570816 and batch: 550, loss is 4.715937786102295 and perplexity is 111.71352545784637
At time: 862.5958585739136 and batch: 600, loss is 4.7412364959716795 and perplexity is 114.57578662612686
At time: 863.4200258255005 and batch: 650, loss is 4.764663486480713 and perplexity is 117.29164042323016
At time: 864.2476160526276 and batch: 700, loss is 4.7745413494110105 and perplexity is 118.45597225815209
At time: 865.0817492008209 and batch: 750, loss is 4.753334083557129 and perplexity is 115.97029532824637
At time: 865.9100255966187 and batch: 800, loss is 4.762775888442993 and perplexity is 117.07044977811586
At time: 866.7395792007446 and batch: 850, loss is 4.809020614624023 and perplexity is 122.61147480850815
At time: 867.5671899318695 and batch: 900, loss is 4.771126623153687 and perplexity is 118.05216737329256
At time: 868.3971743583679 and batch: 950, loss is 4.7630965042114255 and perplexity is 117.10799042807274
At time: 869.2232823371887 and batch: 1000, loss is 4.762047777175903 and perplexity is 116.98524048927516
At time: 870.0514807701111 and batch: 1050, loss is 4.76721492767334 and perplexity is 117.59128524667601
At time: 870.8791568279266 and batch: 1100, loss is 4.71592643737793 and perplexity is 111.7122576590321
At time: 871.7202343940735 and batch: 1150, loss is 4.754094552993775 and perplexity is 116.05852073551331
At time: 872.5559189319611 and batch: 1200, loss is 4.783067073822021 and perplexity is 119.4702126559397
At time: 873.3875768184662 and batch: 1250, loss is 4.809424476623535 and perplexity is 122.66100292447622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8447800239507295 and perplexity of 127.07532539091886
Finished 39 epochs...
Completing Train Step...
At time: 875.8901898860931 and batch: 50, loss is 4.7661732578277585 and perplexity is 117.46885772632974
At time: 876.7272279262543 and batch: 100, loss is 4.769271316528321 and perplexity is 117.8333474568143
At time: 877.554131269455 and batch: 150, loss is 4.696112613677979 and perplexity is 109.52059499067767
At time: 878.3839783668518 and batch: 200, loss is 4.741284694671631 and perplexity is 114.58130916317661
At time: 879.211508512497 and batch: 250, loss is 4.772759065628052 and perplexity is 118.24503812787829
At time: 880.0396037101746 and batch: 300, loss is 4.762295618057251 and perplexity is 117.01423780759471
At time: 880.8684871196747 and batch: 350, loss is 4.776649122238159 and perplexity is 118.70591385520986
At time: 881.758672952652 and batch: 400, loss is 4.766576194763184 and perplexity is 117.51619980516547
At time: 882.5883634090424 and batch: 450, loss is 4.714775972366333 and perplexity is 111.58381051636799
At time: 883.4169933795929 and batch: 500, loss is 4.714941291809082 and perplexity is 111.60225901464814
At time: 884.2452573776245 and batch: 550, loss is 4.712401523590088 and perplexity is 111.3191747803732
At time: 885.0737895965576 and batch: 600, loss is 4.737550868988037 and perplexity is 114.15428025004196
At time: 885.9020917415619 and batch: 650, loss is 4.760907945632934 and perplexity is 116.85197298778321
At time: 886.7342441082001 and batch: 700, loss is 4.771803970336914 and perplexity is 118.13215676357261
At time: 887.5609409809113 and batch: 750, loss is 4.750301933288574 and perplexity is 115.61918853939883
At time: 888.3875341415405 and batch: 800, loss is 4.759887046813965 and perplexity is 116.73273981942191
At time: 889.2154624462128 and batch: 850, loss is 4.80601209640503 and perplexity is 122.2431502860216
At time: 890.0428838729858 and batch: 900, loss is 4.76755428314209 and perplexity is 117.63119726419947
At time: 890.8713004589081 and batch: 950, loss is 4.759885654449463 and perplexity is 116.73257728501191
At time: 891.7030439376831 and batch: 1000, loss is 4.759239330291748 and perplexity is 116.657154576705
At time: 892.5322735309601 and batch: 1050, loss is 4.764053421020508 and perplexity is 117.22010666699272
At time: 893.3633120059967 and batch: 1100, loss is 4.712954778671264 and perplexity is 111.38077971950425
At time: 894.1908478736877 and batch: 1150, loss is 4.750970478057861 and perplexity is 115.69651098700513
At time: 895.0272800922394 and batch: 1200, loss is 4.779483737945557 and perplexity is 119.04287685786886
At time: 895.8549778461456 and batch: 1250, loss is 4.806364593505859 and perplexity is 122.28624823759891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.842463361085766 and perplexity of 126.78127544234279
Finished 40 epochs...
Completing Train Step...
At time: 898.3207340240479 and batch: 50, loss is 4.762655429840088 and perplexity is 117.05634848462213
At time: 899.1734504699707 and batch: 100, loss is 4.766058692932129 and perplexity is 117.4554006897713
At time: 900.0038669109344 and batch: 150, loss is 4.692646903991699 and perplexity is 109.14168537838952
At time: 900.831591129303 and batch: 200, loss is 4.737740831375122 and perplexity is 114.17596732941374
At time: 901.6574597358704 and batch: 250, loss is 4.769231595993042 and perplexity is 117.82866714613247
At time: 902.5463690757751 and batch: 300, loss is 4.758474445343017 and perplexity is 116.56795939138355
At time: 903.3702943325043 and batch: 350, loss is 4.773060741424561 and perplexity is 118.28071517511897
At time: 904.1972198486328 and batch: 400, loss is 4.763496217727661 and perplexity is 117.15480943117682
At time: 905.0268137454987 and batch: 450, loss is 4.711296806335449 and perplexity is 111.19626646918702
At time: 905.8535461425781 and batch: 500, loss is 4.711910095214844 and perplexity is 111.26448281886896
At time: 906.6853530406952 and batch: 550, loss is 4.709058876037598 and perplexity is 110.94769522175453
At time: 907.5125186443329 and batch: 600, loss is 4.733669147491455 and perplexity is 113.71202403950578
At time: 908.3397443294525 and batch: 650, loss is 4.757076959609986 and perplexity is 116.4051711048528
At time: 909.1680662631989 and batch: 700, loss is 4.768537082672119 and perplexity is 117.74686197789065
At time: 909.9999840259552 and batch: 750, loss is 4.747242822647094 and perplexity is 115.2660370894619
At time: 910.8259479999542 and batch: 800, loss is 4.756907863616943 and perplexity is 116.38548912096876
At time: 911.6509020328522 and batch: 850, loss is 4.802785816192627 and perplexity is 121.84939515289526
At time: 912.4842176437378 and batch: 900, loss is 4.7638843059539795 and perplexity is 117.20028465700277
At time: 913.312983751297 and batch: 950, loss is 4.756612253189087 and perplexity is 116.35108944143595
At time: 914.1417255401611 and batch: 1000, loss is 4.756439609527588 and perplexity is 116.33100389720619
At time: 914.9704833030701 and batch: 1050, loss is 4.760296468734741 and perplexity is 116.78054254705074
At time: 915.7949426174164 and batch: 1100, loss is 4.709391145706177 and perplexity is 110.98456590084044
At time: 916.6332187652588 and batch: 1150, loss is 4.747327003479004 and perplexity is 115.27574068877782
At time: 917.4600656032562 and batch: 1200, loss is 4.776198539733887 and perplexity is 118.65243909557334
At time: 918.2854189872742 and batch: 1250, loss is 4.802938861846924 and perplexity is 121.86804510041236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.840111502765739 and perplexity of 126.48345419883138
Finished 41 epochs...
Completing Train Step...
At time: 920.7916858196259 and batch: 50, loss is 4.7591156578063964 and perplexity is 116.6427281885565
At time: 921.619161605835 and batch: 100, loss is 4.76245346069336 and perplexity is 117.03270910109303
At time: 922.4451582431793 and batch: 150, loss is 4.689387187957764 and perplexity is 108.78649370310514
At time: 923.2709393501282 and batch: 200, loss is 4.7339479351043705 and perplexity is 113.74372996265146
At time: 924.1277658939362 and batch: 250, loss is 4.765411739349365 and perplexity is 117.3794370725968
At time: 924.9606502056122 and batch: 300, loss is 4.7545262050628665 and perplexity is 116.10862844989582
At time: 925.7897748947144 and batch: 350, loss is 4.769376249313354 and perplexity is 117.84571268688043
At time: 926.613284111023 and batch: 400, loss is 4.760451097488403 and perplexity is 116.7986015729831
At time: 927.4392609596252 and batch: 450, loss is 4.707802810668945 and perplexity is 110.80842514852316
At time: 928.2669901847839 and batch: 500, loss is 4.708923063278198 and perplexity is 110.93262813229211
At time: 929.0916962623596 and batch: 550, loss is 4.705804691314698 and perplexity is 110.58723774262903
At time: 929.9178583621979 and batch: 600, loss is 4.729917659759521 and perplexity is 113.2862339488675
At time: 930.742965221405 and batch: 650, loss is 4.753365888595581 and perplexity is 115.97398382660465
At time: 931.5752613544464 and batch: 700, loss is 4.765174198150635 and perplexity is 117.35155793176162
At time: 932.4036977291107 and batch: 750, loss is 4.743919172286987 and perplexity is 114.88356903091422
At time: 933.2298443317413 and batch: 800, loss is 4.753885326385498 and perplexity is 116.03424074495679
At time: 934.0572574138641 and batch: 850, loss is 4.7997229957580565 and perplexity is 121.47676327904064
At time: 934.884514093399 and batch: 900, loss is 4.7603346824646 and perplexity is 116.78500525242416
At time: 935.711234331131 and batch: 950, loss is 4.753254117965699 and perplexity is 115.96102206576799
At time: 936.5382361412048 and batch: 1000, loss is 4.753407497406005 and perplexity is 115.97880946650577
At time: 937.3673644065857 and batch: 1050, loss is 4.756792135238648 and perplexity is 116.37202079640436
At time: 938.1945955753326 and batch: 1100, loss is 4.706017055511475 and perplexity is 110.61072500638473
At time: 939.0265712738037 and batch: 1150, loss is 4.743810691833496 and perplexity is 114.87110708519803
At time: 939.8515083789825 and batch: 1200, loss is 4.773121385574341 and perplexity is 118.28788842603181
At time: 940.6883385181427 and batch: 1250, loss is 4.799814748764038 and perplexity is 121.48790964857702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.838116945141423 and perplexity of 126.23142708576175
Finished 42 epochs...
Completing Train Step...
At time: 943.2892832756042 and batch: 50, loss is 4.755784416198731 and perplexity is 116.25480956324887
At time: 944.1793596744537 and batch: 100, loss is 4.75932656288147 and perplexity is 116.66733132627401
At time: 945.0066962242126 and batch: 150, loss is 4.686512289047241 and perplexity is 108.47419266293696
At time: 945.8693046569824 and batch: 200, loss is 4.7306100940704345 and perplexity is 113.36470438888507
At time: 946.6974198818207 and batch: 250, loss is 4.761901874542236 and perplexity is 116.96817327969327
At time: 947.5219419002533 and batch: 300, loss is 4.750672645568848 and perplexity is 115.66205793804103
At time: 948.3476374149323 and batch: 350, loss is 4.765966682434082 and perplexity is 117.44459405702504
At time: 949.1744270324707 and batch: 400, loss is 4.757102098464966 and perplexity is 116.40809743435025
At time: 950.0170431137085 and batch: 450, loss is 4.704496784210205 and perplexity is 110.44269445392123
At time: 950.8544626235962 and batch: 500, loss is 4.706020812988282 and perplexity is 110.61114062439938
At time: 951.6906414031982 and batch: 550, loss is 4.702225751876831 and perplexity is 110.19216011730042
At time: 952.5259971618652 and batch: 600, loss is 4.726347856521606 and perplexity is 112.88254535722663
At time: 953.3562183380127 and batch: 650, loss is 4.7495423412323 and perplexity is 115.53139846877272
At time: 954.1830198764801 and batch: 700, loss is 4.76206865310669 and perplexity is 116.9876826905502
At time: 955.0149409770966 and batch: 750, loss is 4.740821771621704 and perplexity is 114.5282791094383
At time: 955.8435318470001 and batch: 800, loss is 4.751169185638428 and perplexity is 115.71950304505583
At time: 956.6747515201569 and batch: 850, loss is 4.796751594543457 and perplexity is 121.11634281916983
At time: 957.5067245960236 and batch: 900, loss is 4.757140407562256 and perplexity is 116.41255700890079
At time: 958.3373572826385 and batch: 950, loss is 4.750001049041748 and perplexity is 115.5844057799911
At time: 959.1669387817383 and batch: 1000, loss is 4.750196933746338 and perplexity is 115.60704921485141
At time: 959.9976930618286 and batch: 1050, loss is 4.753375930786133 and perplexity is 115.97514846529707
At time: 960.8282179832458 and batch: 1100, loss is 4.7026060962677 and perplexity is 110.23407905863048
At time: 961.6567392349243 and batch: 1150, loss is 4.74054365158081 and perplexity is 114.49643092878287
At time: 962.4933574199677 and batch: 1200, loss is 4.770114107131958 and perplexity is 117.93269815488271
At time: 963.3216300010681 and batch: 1250, loss is 4.796586618423462 and perplexity is 121.096363162992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8363772204322535 and perplexity of 126.01201007092945
Finished 43 epochs...
Completing Train Step...
At time: 965.8803684711456 and batch: 50, loss is 4.752326850891113 and perplexity is 115.85354506571379
At time: 966.7123651504517 and batch: 100, loss is 4.756287288665772 and perplexity is 116.31328560790342
At time: 967.5942883491516 and batch: 150, loss is 4.683361415863037 and perplexity is 108.13294213898759
At time: 968.425023317337 and batch: 200, loss is 4.7270400428771975 and perplexity is 112.9607081634005
At time: 969.2503333091736 and batch: 250, loss is 4.758224563598633 and perplexity is 116.53883482535174
At time: 970.0812344551086 and batch: 300, loss is 4.746906061172485 and perplexity is 115.22722646416776
At time: 970.9161751270294 and batch: 350, loss is 4.762758455276489 and perplexity is 117.0684088872618
At time: 971.7430474758148 and batch: 400, loss is 4.75408299446106 and perplexity is 116.0571792770572
At time: 972.5737397670746 and batch: 450, loss is 4.7013623237609865 and perplexity is 110.09705817086494
At time: 973.4050765037537 and batch: 500, loss is 4.703180093765258 and perplexity is 110.29737130721419
At time: 974.2335817813873 and batch: 550, loss is 4.698856468200684 and perplexity is 109.82151622381903
At time: 975.0607008934021 and batch: 600, loss is 4.722998924255371 and perplexity is 112.50514166121864
At time: 975.8912541866302 and batch: 650, loss is 4.746072254180908 and perplexity is 115.13118924094005
At time: 976.72008061409 and batch: 700, loss is 4.759098424911499 and perplexity is 116.6407181140008
At time: 977.5507204532623 and batch: 750, loss is 4.7381000995635985 and perplexity is 114.21699449179957
At time: 978.3790814876556 and batch: 800, loss is 4.7483368015289305 and perplexity is 115.39220469959822
At time: 979.2042367458344 and batch: 850, loss is 4.793755598068238 and perplexity is 120.75402171039056
At time: 980.0336351394653 and batch: 900, loss is 4.7537803459167485 and perplexity is 116.02206005535088
At time: 980.862028837204 and batch: 950, loss is 4.746781234741211 and perplexity is 115.21284395837972
At time: 981.6992394924164 and batch: 1000, loss is 4.747193717956543 and perplexity is 115.2603771253461
At time: 982.5262517929077 and batch: 1050, loss is 4.749684448242188 and perplexity is 115.54781745695627
At time: 983.3543381690979 and batch: 1100, loss is 4.699606552124023 and perplexity is 109.9039224795154
At time: 984.1832592487335 and batch: 1150, loss is 4.737662343978882 and perplexity is 114.16700630669314
At time: 985.0113575458527 and batch: 1200, loss is 4.767391853332519 and perplexity is 117.61209200290227
At time: 985.8412535190582 and batch: 1250, loss is 4.793433713912964 and perplexity is 120.71515915907169
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.834356823106752 and perplexity of 125.75767276052915
Finished 44 epochs...
Completing Train Step...
At time: 988.3789436817169 and batch: 50, loss is 4.7490267658233645 and perplexity is 115.47184867328647
At time: 989.2385985851288 and batch: 100, loss is 4.753287000656128 and perplexity is 115.96483523885179
At time: 990.0842986106873 and batch: 150, loss is 4.680485801696777 and perplexity is 107.82244017462284
At time: 990.921790599823 and batch: 200, loss is 4.723801012039185 and perplexity is 112.59541686043869
At time: 991.7496159076691 and batch: 250, loss is 4.754847793579102 and perplexity is 116.14597365601401
At time: 992.5737490653992 and batch: 300, loss is 4.743430452346802 and perplexity is 114.82743685750259
At time: 993.3996775150299 and batch: 350, loss is 4.759655179977417 and perplexity is 116.70567650598211
At time: 994.2260227203369 and batch: 400, loss is 4.751227130889893 and perplexity is 115.7262086350358
At time: 995.057443857193 and batch: 450, loss is 4.698264722824097 and perplexity is 109.75654907324504
At time: 995.8850798606873 and batch: 500, loss is 4.700237255096436 and perplexity is 109.97326107383334
At time: 996.7154316902161 and batch: 550, loss is 4.695789499282837 and perplexity is 109.48521302639048
At time: 997.5425033569336 and batch: 600, loss is 4.719714088439941 and perplexity is 112.13618705188347
At time: 998.368978023529 and batch: 650, loss is 4.742745475769043 and perplexity is 114.74880968473127
At time: 999.1974859237671 and batch: 700, loss is 4.756250648498535 and perplexity is 116.30902394774141
At time: 1000.0257141590118 and batch: 750, loss is 4.735361089706421 and perplexity is 113.90458106507201
At time: 1000.8550925254822 and batch: 800, loss is 4.745562858581543 and perplexity is 115.0725568545987
At time: 1001.6838772296906 and batch: 850, loss is 4.790729560852051 and perplexity is 120.38916785577283
At time: 1002.5136358737946 and batch: 900, loss is 4.7504751491546635 and perplexity is 115.63921735188225
At time: 1003.3425016403198 and batch: 950, loss is 4.7435909938812255 and perplexity is 114.84587291024613
At time: 1004.176016330719 and batch: 1000, loss is 4.744017715454102 and perplexity is 114.89489057947738
At time: 1005.0055725574493 and batch: 1050, loss is 4.7464730930328365 and perplexity is 115.17734754505932
At time: 1005.8326554298401 and batch: 1100, loss is 4.696840410232544 and perplexity is 109.60033271526672
At time: 1006.6624529361725 and batch: 1150, loss is 4.734999370574951 and perplexity is 113.8633870497196
At time: 1007.4861192703247 and batch: 1200, loss is 4.764314794540406 and perplexity is 117.25074890323518
At time: 1008.3125469684601 and batch: 1250, loss is 4.790446672439575 and perplexity is 120.35511597186763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8333094241845345 and perplexity of 125.62602326642372
Finished 45 epochs...
Completing Train Step...
At time: 1010.7792413234711 and batch: 50, loss is 4.745867681503296 and perplexity is 115.1076389542351
At time: 1011.6411228179932 and batch: 100, loss is 4.750656995773316 and perplexity is 115.66024786464712
At time: 1012.4718546867371 and batch: 150, loss is 4.677770538330078 and perplexity is 107.53007096204288
At time: 1013.3040065765381 and batch: 200, loss is 4.7207597064971925 and perplexity is 112.25349999551396
At time: 1014.1311275959015 and batch: 250, loss is 4.7515329742431645 and perplexity is 115.76160813982185
At time: 1014.959311246872 and batch: 300, loss is 4.739926052093506 and perplexity is 114.42573982338527
At time: 1015.7847032546997 and batch: 350, loss is 4.7563434600830075 and perplexity is 116.31981927350029
At time: 1016.6108019351959 and batch: 400, loss is 4.747806100845337 and perplexity is 115.3309822245244
At time: 1017.4393372535706 and batch: 450, loss is 4.69521731376648 and perplexity is 109.42258509234858
At time: 1018.2711255550385 and batch: 500, loss is 4.697113819122315 and perplexity is 109.63030251737139
At time: 1019.1020555496216 and batch: 550, loss is 4.692760696411133 and perplexity is 109.1541055814789
At time: 1019.9279747009277 and batch: 600, loss is 4.716657667160034 and perplexity is 111.7939748622437
At time: 1020.7596294879913 and batch: 650, loss is 4.739545030593872 and perplexity is 114.38214946136095
At time: 1021.5887207984924 and batch: 700, loss is 4.753795642852783 and perplexity is 116.02383485095655
At time: 1022.4173920154572 and batch: 750, loss is 4.732823753356934 and perplexity is 113.6159331844038
At time: 1023.2480938434601 and batch: 800, loss is 4.742854204177856 and perplexity is 114.76128681851866
At time: 1024.0763449668884 and batch: 850, loss is 4.787907638549805 and perplexity is 120.04991787172732
At time: 1024.907065153122 and batch: 900, loss is 4.747250690460205 and perplexity is 115.26694398466732
At time: 1025.7385139465332 and batch: 950, loss is 4.740829362869262 and perplexity is 114.52914852525745
At time: 1026.5646483898163 and batch: 1000, loss is 4.741310825347901 and perplexity is 114.58430328939204
At time: 1027.3954486846924 and batch: 1050, loss is 4.743199462890625 and perplexity is 114.80091599344661
At time: 1028.229996919632 and batch: 1100, loss is 4.693979749679565 and perplexity is 109.28725139007847
At time: 1029.0999701023102 and batch: 1150, loss is 4.732301206588745 and perplexity is 113.55657905470841
At time: 1029.933716058731 and batch: 1200, loss is 4.761850728988647 and perplexity is 116.96219103070264
At time: 1030.8161313533783 and batch: 1250, loss is 4.787953662872314 and perplexity is 120.05544321501381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.832200099081889 and perplexity of 125.4867404345231
Finished 46 epochs...
Completing Train Step...
At time: 1033.2834589481354 and batch: 50, loss is 4.742757778167725 and perplexity is 114.75022137901989
At time: 1034.109633922577 and batch: 100, loss is 4.7477563285827635 and perplexity is 115.32524208344535
At time: 1034.9383380413055 and batch: 150, loss is 4.674940958023071 and perplexity is 107.22623605619248
At time: 1035.7690427303314 and batch: 200, loss is 4.717565116882324 and perplexity is 111.89546831682497
At time: 1036.5955300331116 and batch: 250, loss is 4.748157291412354 and perplexity is 115.3714924905617
At time: 1037.4224021434784 and batch: 300, loss is 4.73647852897644 and perplexity is 114.03193365813175
At time: 1038.2518355846405 and batch: 350, loss is 4.7529944896698 and perplexity is 115.93091921116391
At time: 1039.0777282714844 and batch: 400, loss is 4.744867925643921 and perplexity is 114.99261692428644
At time: 1039.906537771225 and batch: 450, loss is 4.6919637966156005 and perplexity is 109.067155346973
At time: 1040.7334189414978 and batch: 500, loss is 4.694197320938111 and perplexity is 109.3110317417731
At time: 1041.56422996521 and batch: 550, loss is 4.689836311340332 and perplexity is 108.83536323453794
At time: 1042.389420747757 and batch: 600, loss is 4.713597021102905 and perplexity is 111.45233615814034
At time: 1043.216876745224 and batch: 650, loss is 4.736843509674072 and perplexity is 114.07356070890351
At time: 1044.0474388599396 and batch: 700, loss is 4.750865917205811 and perplexity is 115.68441429366835
At time: 1044.877275466919 and batch: 750, loss is 4.730178260803223 and perplexity is 113.31576030680438
At time: 1045.7062892913818 and batch: 800, loss is 4.740064516067505 and perplexity is 114.44158476300028
At time: 1046.533626317978 and batch: 850, loss is 4.7853713226318355 and perplexity is 119.7458191622967
At time: 1047.3607621192932 and batch: 900, loss is 4.7443350887298585 and perplexity is 114.93136093432018
At time: 1048.1965601444244 and batch: 950, loss is 4.737999658584595 and perplexity is 114.20552300116752
At time: 1049.0263335704803 and batch: 1000, loss is 4.738267822265625 and perplexity is 114.2361528813366
At time: 1049.8568692207336 and batch: 1050, loss is 4.740563879013061 and perplexity is 114.49874692100566
At time: 1050.6890647411346 and batch: 1100, loss is 4.69146577835083 and perplexity is 109.01285143481608
At time: 1051.5182139873505 and batch: 1150, loss is 4.730017623901367 and perplexity is 113.29755907607105
At time: 1052.4031150341034 and batch: 1200, loss is 4.758938093185424 and perplexity is 116.62201840545337
At time: 1053.2339684963226 and batch: 1250, loss is 4.784605693817139 and perplexity is 119.65417340050138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8311322066035585 and perplexity of 125.35280561500427
Finished 47 epochs...
Completing Train Step...
At time: 1055.6736195087433 and batch: 50, loss is 4.739728078842163 and perplexity is 114.40308882985455
At time: 1056.5306215286255 and batch: 100, loss is 4.7449714469909665 and perplexity is 115.00452173108094
At time: 1057.3550879955292 and batch: 150, loss is 4.672381324768066 and perplexity is 106.95212717533462
At time: 1058.1852657794952 and batch: 200, loss is 4.714641532897949 and perplexity is 111.56881025653803
At time: 1059.009470462799 and batch: 250, loss is 4.74497769355774 and perplexity is 115.00524011674887
At time: 1059.834231853485 and batch: 300, loss is 4.733050956726074 and perplexity is 113.6417500399385
At time: 1060.6602654457092 and batch: 350, loss is 4.749851951599121 and perplexity is 115.56717372534133
At time: 1061.4886796474457 and batch: 400, loss is 4.742065753936767 and perplexity is 114.6708389157799
At time: 1062.3212387561798 and batch: 450, loss is 4.6891457462310795 and perplexity is 108.7602312747761
At time: 1063.1470375061035 and batch: 500, loss is 4.691408376693726 and perplexity is 109.00659409609062
At time: 1063.9736170768738 and batch: 550, loss is 4.686851825714111 and perplexity is 108.5110298821946
At time: 1064.8010799884796 and batch: 600, loss is 4.710517511367798 and perplexity is 111.1096455343191
At time: 1065.626505613327 and batch: 650, loss is 4.733688936233521 and perplexity is 113.71427427968388
At time: 1066.45285820961 and batch: 700, loss is 4.748409490585328 and perplexity is 115.40059275492965
At time: 1067.279198884964 and batch: 750, loss is 4.727375392913818 and perplexity is 112.99859589349057
At time: 1068.1051902770996 and batch: 800, loss is 4.737238168716431 and perplexity is 114.11858975610717
At time: 1068.9325232505798 and batch: 850, loss is 4.782849225997925 and perplexity is 119.44418916475166
At time: 1069.7600214481354 and batch: 900, loss is 4.741345672607422 and perplexity is 114.58829630791833
At time: 1070.5963878631592 and batch: 950, loss is 4.7352431106567385 and perplexity is 113.89114350353442
At time: 1071.4245183467865 and batch: 1000, loss is 4.735709953308105 and perplexity is 113.9443251597016
At time: 1072.2517096996307 and batch: 1050, loss is 4.737682704925537 and perplexity is 114.16933087868345
At time: 1073.0760567188263 and batch: 1100, loss is 4.688836660385132 and perplexity is 108.7266202213051
At time: 1073.9340813159943 and batch: 1150, loss is 4.727709875106812 and perplexity is 113.03639823340256
At time: 1074.7623822689056 and batch: 1200, loss is 4.756792707443237 and perplexity is 116.3720873850278
At time: 1075.5884585380554 and batch: 1250, loss is 4.781971282958985 and perplexity is 119.33936998969789
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.830071887830748 and perplexity of 125.21996212265664
Finished 48 epochs...
Completing Train Step...
At time: 1078.0688307285309 and batch: 50, loss is 4.7368574047088625 and perplexity is 114.07514577601047
At time: 1078.8971111774445 and batch: 100, loss is 4.742581853866577 and perplexity is 114.730035802136
At time: 1079.7228820323944 and batch: 150, loss is 4.669747295379639 and perplexity is 106.67078282650803
At time: 1080.546787261963 and batch: 200, loss is 4.711903982162475 and perplexity is 111.2638026553377
At time: 1081.3715403079987 and batch: 250, loss is 4.74191722869873 and perplexity is 114.65380866687619
At time: 1082.195907831192 and batch: 300, loss is 4.729869747161866 and perplexity is 113.28080624114922
At time: 1083.0208477973938 and batch: 350, loss is 4.746887063980102 and perplexity is 115.22503749117112
At time: 1083.8453283309937 and batch: 400, loss is 4.7392269802093505 and perplexity is 114.34577595935214
At time: 1084.6758632659912 and batch: 450, loss is 4.686219549179077 and perplexity is 108.4424425895605
At time: 1085.5060205459595 and batch: 500, loss is 4.688600292205811 and perplexity is 108.70092374507395
At time: 1086.3299086093903 and batch: 550, loss is 4.684149808883667 and perplexity is 108.2182270104514
At time: 1087.156344652176 and batch: 600, loss is 4.707597093582153 and perplexity is 110.7856323066285
At time: 1087.98361992836 and batch: 650, loss is 4.730768461227417 and perplexity is 113.38265905649855
At time: 1088.8101451396942 and batch: 700, loss is 4.745694417953491 and perplexity is 115.0876967237809
At time: 1089.6332263946533 and batch: 750, loss is 4.724366655349732 and perplexity is 112.65912372076514
At time: 1090.4646670818329 and batch: 800, loss is 4.734480981826782 and perplexity is 113.8043768474785
At time: 1091.2919013500214 and batch: 850, loss is 4.7802122116088865 and perplexity is 119.12962805261073
At time: 1092.11891579628 and batch: 900, loss is 4.738405447006226 and perplexity is 114.25187568414266
At time: 1092.9467811584473 and batch: 950, loss is 4.732365856170654 and perplexity is 113.5639206773812
At time: 1093.7726836204529 and batch: 1000, loss is 4.733008737564087 and perplexity is 113.63695228176447
At time: 1094.598670244217 and batch: 1050, loss is 4.734913787841797 and perplexity is 113.85364272682851
At time: 1095.4812428951263 and batch: 1100, loss is 4.686086235046386 and perplexity is 108.42798664299224
At time: 1096.3101723194122 and batch: 1150, loss is 4.725153007507324 and perplexity is 112.74774830627571
At time: 1097.1360495090485 and batch: 1200, loss is 4.754419260025024 and perplexity is 116.09621187218994
At time: 1097.965677022934 and batch: 1250, loss is 4.7790232849121095 and perplexity is 118.98807582172923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.829163043168339 and perplexity of 125.10620832854883
Finished 49 epochs...
Completing Train Step...
At time: 1100.4304127693176 and batch: 50, loss is 4.733947629928589 and perplexity is 113.74369525082507
At time: 1101.284696817398 and batch: 100, loss is 4.739867992401123 and perplexity is 114.4190964929872
At time: 1102.1088089942932 and batch: 150, loss is 4.667262744903565 and perplexity is 106.40608284864707
At time: 1102.9361526966095 and batch: 200, loss is 4.709066457748413 and perplexity is 110.94853639828413
At time: 1103.7592775821686 and batch: 250, loss is 4.739129734039307 and perplexity is 114.33465681123546
At time: 1104.5916783809662 and batch: 300, loss is 4.726751070022583 and perplexity is 112.92807030105854
At time: 1105.4178006649017 and batch: 350, loss is 4.743811368942261 and perplexity is 114.87118486545778
At time: 1106.2467527389526 and batch: 400, loss is 4.736445293426514 and perplexity is 114.02814380708669
At time: 1107.0698175430298 and batch: 450, loss is 4.683015670776367 and perplexity is 108.09556216787891
At time: 1107.8951394557953 and batch: 500, loss is 4.6856722736358645 and perplexity is 108.38311092974907
At time: 1108.7230336666107 and batch: 550, loss is 4.681373996734619 and perplexity is 107.91825007359371
At time: 1109.5482330322266 and batch: 600, loss is 4.704782447814941 and perplexity is 110.47424841883084
At time: 1110.3811168670654 and batch: 650, loss is 4.728126392364502 and perplexity is 113.08348965052708
At time: 1111.210896730423 and batch: 700, loss is 4.7430575084686275 and perplexity is 114.7846206523972
At time: 1112.0352683067322 and batch: 750, loss is 4.721539669036865 and perplexity is 112.34108767357122
At time: 1112.8632786273956 and batch: 800, loss is 4.732319250106811 and perplexity is 113.55862803337948
At time: 1113.6932663917542 and batch: 850, loss is 4.777923221588135 and perplexity is 118.857253373209
At time: 1114.5249230861664 and batch: 900, loss is 4.735821113586426 and perplexity is 113.95699194660817
At time: 1115.3520903587341 and batch: 950, loss is 4.729727306365967 and perplexity is 113.2646715820923
At time: 1116.2336609363556 and batch: 1000, loss is 4.730551347732544 and perplexity is 113.35804482327067
At time: 1117.0601501464844 and batch: 1050, loss is 4.732095460891724 and perplexity is 113.53321768053301
At time: 1117.8919870853424 and batch: 1100, loss is 4.683597555160523 and perplexity is 108.15847959105868
At time: 1118.720012664795 and batch: 1150, loss is 4.723007802963257 and perplexity is 112.50614056594154
At time: 1119.5465791225433 and batch: 1200, loss is 4.75211579322815 and perplexity is 115.82909586743227
At time: 1120.3751838207245 and batch: 1250, loss is 4.776221742630005 and perplexity is 118.65519220773182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.827932093265283 and perplexity of 124.95230359747548
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f15b303a898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'anneal': 3.8987434387266164, 'lr': 7.751153807508898, 'dropout': 0.7574826006389305, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4805476665496826 and batch: 50, loss is 7.465387392044067 and perplexity is 1746.5320097911094
At time: 2.305222749710083 and batch: 100, loss is 6.667973041534424 and perplexity is 786.7991778097878
At time: 3.1259660720825195 and batch: 150, loss is 6.4115167903900145 and perplexity is 608.8164279849815
At time: 3.9495253562927246 and batch: 200, loss is 6.333590087890625 and perplexity is 563.1748159750339
At time: 4.77120566368103 and batch: 250, loss is 6.330388326644897 and perplexity is 561.3745482263498
At time: 5.593361854553223 and batch: 300, loss is 6.299501237869262 and perplexity is 544.3003660032865
At time: 6.423138618469238 and batch: 350, loss is 6.306205444335937 and perplexity is 547.9617275851193
At time: 7.243969917297363 and batch: 400, loss is 6.253662214279175 and perplexity is 519.9133766217358
At time: 8.065990686416626 and batch: 450, loss is 6.211537580490113 and perplexity is 498.46709564347987
At time: 8.890079259872437 and batch: 500, loss is 6.194993524551392 and perplexity is 490.288270151242
At time: 9.767663717269897 and batch: 550, loss is 6.2002155971527095 and perplexity is 492.85528783635243
At time: 10.593943119049072 and batch: 600, loss is 6.216429080963135 and perplexity is 500.91132076824897
At time: 11.420877933502197 and batch: 650, loss is 6.177271986007691 and perplexity is 481.676143132151
At time: 12.252981662750244 and batch: 700, loss is 6.1806133842468265 and perplexity is 483.28830688924006
At time: 13.077031373977661 and batch: 750, loss is 6.133793935775757 and perplexity is 461.1825427024538
At time: 13.907690048217773 and batch: 800, loss is 6.125940799713135 and perplexity is 457.57499725780286
At time: 14.735782861709595 and batch: 850, loss is 6.1720396995544435 and perplexity is 479.1624574700315
At time: 15.565648794174194 and batch: 900, loss is 6.147446699142456 and perplexity is 467.52213684250177
At time: 16.38841438293457 and batch: 950, loss is 6.1244964504241945 and perplexity is 456.91457619003774
At time: 17.216022491455078 and batch: 1000, loss is 6.111866674423218 and perplexity is 451.1801360140699
At time: 18.04497742652893 and batch: 1050, loss is 6.100764112472534 and perplexity is 446.1985857597856
At time: 18.870941877365112 and batch: 1100, loss is 6.097936582565308 and perplexity is 444.938727896254
At time: 19.699279308319092 and batch: 1150, loss is 6.126609048843384 and perplexity is 457.8808735411416
At time: 20.525216102600098 and batch: 1200, loss is 6.118438968658447 and perplexity is 454.15519037840807
At time: 21.358050107955933 and batch: 1250, loss is 6.101307640075683 and perplexity is 446.44117292807107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.455195740191606 and perplexity of 233.9706640501718
Finished 1 epochs...
Completing Train Step...
At time: 23.835660457611084 and batch: 50, loss is 5.615072326660156 and perplexity is 274.5332345492933
At time: 24.678809642791748 and batch: 100, loss is 5.506585865020752 and perplexity is 246.308758557682
At time: 25.503549337387085 and batch: 150, loss is 5.316114845275879 and perplexity is 203.59135952141023
At time: 26.330629587173462 and batch: 200, loss is 5.301467056274414 and perplexity is 200.63093104218805
At time: 27.15462899208069 and batch: 250, loss is 5.27647481918335 and perplexity is 195.67885473615283
At time: 27.984118223190308 and batch: 300, loss is 5.247465467453003 and perplexity is 190.08388361713307
At time: 28.806536436080933 and batch: 350, loss is 5.233317461013794 and perplexity is 187.413510379639
At time: 29.62781524658203 and batch: 400, loss is 5.1956007766723635 and perplexity is 180.47653632653146
At time: 30.452031135559082 and batch: 450, loss is 5.126182050704956 and perplexity is 168.3730495509913
At time: 31.273799180984497 and batch: 500, loss is 5.114440374374389 and perplexity is 166.40762894716056
At time: 32.096893548965454 and batch: 550, loss is 5.096613626480103 and perplexity is 163.46740725836227
At time: 32.92452311515808 and batch: 600, loss is 5.091505918502808 and perplexity is 162.6345921769848
At time: 33.752548933029175 and batch: 650, loss is 5.066512861251831 and perplexity is 158.6202310455301
At time: 34.57736301422119 and batch: 700, loss is 5.06736478805542 and perplexity is 158.7554214499372
At time: 35.400310039520264 and batch: 750, loss is 5.05062798500061 and perplexity is 156.12047502520386
At time: 36.225865840911865 and batch: 800, loss is 5.060581502914428 and perplexity is 157.68218231733354
At time: 37.05157995223999 and batch: 850, loss is 5.084837512969971 and perplexity is 161.553686726065
At time: 37.87629199028015 and batch: 900, loss is 5.049013366699219 and perplexity is 155.86860344195912
At time: 38.75299382209778 and batch: 950, loss is 5.0196926975250244 and perplexity is 151.36478187417424
At time: 39.57934355735779 and batch: 1000, loss is 5.004154052734375 and perplexity is 149.03095748709637
At time: 40.41007995605469 and batch: 1050, loss is 4.983041133880615 and perplexity is 145.91746211643638
At time: 41.234153747558594 and batch: 1100, loss is 4.952292003631592 and perplexity is 141.4989085507938
At time: 42.06142735481262 and batch: 1150, loss is 4.959179182052612 and perplexity is 142.47680037271243
At time: 42.88369345664978 and batch: 1200, loss is 4.953051605224609 and perplexity is 141.60643217952904
At time: 43.709643602371216 and batch: 1250, loss is 4.938937883377076 and perplexity is 139.62187606348175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.736676960966013 and perplexity of 114.05456348676901
Finished 2 epochs...
Completing Train Step...
At time: 46.20010471343994 and batch: 50, loss is 4.867260236740112 and perplexity is 129.96435710409762
At time: 47.027503490448 and batch: 100, loss is 4.85824990272522 and perplexity is 128.7985946779464
At time: 47.84972262382507 and batch: 150, loss is 4.750182256698609 and perplexity is 115.605352457124
At time: 48.67786407470703 and batch: 200, loss is 4.800261564254761 and perplexity is 121.54220445752112
At time: 49.50878882408142 and batch: 250, loss is 4.787959547042846 and perplexity is 120.05614964379338
At time: 50.33084583282471 and batch: 300, loss is 4.788625297546386 and perplexity is 120.13610369764447
At time: 51.155685901641846 and batch: 350, loss is 4.7806206226348875 and perplexity is 119.17829184296866
At time: 51.97827768325806 and batch: 400, loss is 4.772194290161133 and perplexity is 118.17827508609744
At time: 52.806499004364014 and batch: 450, loss is 4.704199666976929 and perplexity is 110.40988490049276
At time: 53.62972092628479 and batch: 500, loss is 4.719380826950073 and perplexity is 112.09882260552975
At time: 54.45167565345764 and batch: 550, loss is 4.708117666244507 and perplexity is 110.84331929201996
At time: 55.278489112854004 and batch: 600, loss is 4.720085935592651 and perplexity is 112.17789232726788
At time: 56.10191106796265 and batch: 650, loss is 4.72507794380188 and perplexity is 112.73928536014137
At time: 56.93843746185303 and batch: 700, loss is 4.719493503570557 and perplexity is 112.11145423365237
At time: 57.76059556007385 and batch: 750, loss is 4.717972745895386 and perplexity is 111.94108945375889
At time: 58.586058378219604 and batch: 800, loss is 4.738647193908691 and perplexity is 114.27949905998838
At time: 59.41020369529724 and batch: 850, loss is 4.767164688110352 and perplexity is 117.58537766029254
At time: 60.29149532318115 and batch: 900, loss is 4.7235292625427245 and perplexity is 112.56482326968785
At time: 61.117077350616455 and batch: 950, loss is 4.715486831665039 and perplexity is 111.66315910515839
At time: 61.94091796875 and batch: 1000, loss is 4.705253238677979 and perplexity is 110.52627093048804
At time: 62.76584196090698 and batch: 1050, loss is 4.688434476852417 and perplexity is 108.6829009572577
At time: 63.589534521102905 and batch: 1100, loss is 4.658850116729736 and perplexity is 105.51468280397019
At time: 64.41875910758972 and batch: 1150, loss is 4.660901622772217 and perplexity is 105.73136900384029
At time: 65.24826884269714 and batch: 1200, loss is 4.671888427734375 and perplexity is 106.89942377884385
At time: 66.07763719558716 and batch: 1250, loss is 4.680395908355713 and perplexity is 107.81274809086726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.631146284785584 and perplexity of 102.6316417983746
Finished 3 epochs...
Completing Train Step...
At time: 68.59028363227844 and batch: 50, loss is 4.622097654342651 and perplexity is 101.70715497768153
At time: 69.4627685546875 and batch: 100, loss is 4.619019718170166 and perplexity is 101.3945881235519
At time: 70.29090070724487 and batch: 150, loss is 4.531378335952759 and perplexity is 92.88650169409483
At time: 71.11890268325806 and batch: 200, loss is 4.583852167129517 and perplexity is 97.89076039839458
At time: 71.94690489768982 and batch: 250, loss is 4.576749877929688 and perplexity is 97.19797500132621
At time: 72.77980136871338 and batch: 300, loss is 4.585630903244018 and perplexity is 98.06503717944385
At time: 73.60514760017395 and batch: 350, loss is 4.57889799118042 and perplexity is 97.4069916747401
At time: 74.43266749382019 and batch: 400, loss is 4.568777780532837 and perplexity is 96.42618376285296
At time: 75.25423097610474 and batch: 450, loss is 4.503118724822998 and perplexity is 90.29830819117382
At time: 76.08341479301453 and batch: 500, loss is 4.5297935104370115 and perplexity is 92.73940938475131
At time: 76.90992164611816 and batch: 550, loss is 4.519394769668579 and perplexity is 91.78003310522556
At time: 77.73900651931763 and batch: 600, loss is 4.537797994613648 and perplexity is 93.48471945160037
At time: 78.5660662651062 and batch: 650, loss is 4.55475640296936 and perplexity is 95.083590333974
At time: 79.3929533958435 and batch: 700, loss is 4.541311235427856 and perplexity is 93.81373139416273
At time: 80.2244279384613 and batch: 750, loss is 4.5427431297302245 and perplexity is 93.94815896169565
At time: 81.04899644851685 and batch: 800, loss is 4.566684722900391 and perplexity is 96.22456927193916
At time: 81.93391251564026 and batch: 850, loss is 4.597268114089966 and perplexity is 99.21290673963846
At time: 82.76348948478699 and batch: 900, loss is 4.5539859104156495 and perplexity is 95.01035735199818
At time: 83.59399557113647 and batch: 950, loss is 4.551686391830445 and perplexity is 94.79213027425234
At time: 84.42363262176514 and batch: 1000, loss is 4.5438778114318845 and perplexity is 94.0548207207033
At time: 85.2535469532013 and batch: 1050, loss is 4.530352554321289 and perplexity is 92.79126927902709
At time: 86.08271384239197 and batch: 1100, loss is 4.501228761672974 and perplexity is 90.12780888563668
At time: 86.91147947311401 and batch: 1150, loss is 4.500967540740967 and perplexity is 90.10426869012959
At time: 87.73782277107239 and batch: 1200, loss is 4.512052335739136 and perplexity is 91.10861222426733
At time: 88.57281470298767 and batch: 1250, loss is 4.541065559387207 and perplexity is 93.7906864389879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.594418268134124 and perplexity of 98.93056774101342
Finished 4 epochs...
Completing Train Step...
At time: 91.07769894599915 and batch: 50, loss is 4.477840719223022 and perplexity is 88.04435479190028
At time: 91.90486979484558 and batch: 100, loss is 4.475910215377808 and perplexity is 87.87454878473672
At time: 92.7314224243164 and batch: 150, loss is 4.395213956832886 and perplexity is 81.06197267039155
At time: 93.5576696395874 and batch: 200, loss is 4.4520777893066406 and perplexity is 85.8050437135721
At time: 94.38324189186096 and batch: 250, loss is 4.444319381713867 and perplexity is 85.14190897206284
At time: 95.21730852127075 and batch: 300, loss is 4.456048316955567 and perplexity is 86.14641227022511
At time: 96.04551982879639 and batch: 350, loss is 4.4491948223114015 and perplexity is 85.55802684634857
At time: 96.875248670578 and batch: 400, loss is 4.446118469238281 and perplexity is 85.29522459105478
At time: 97.70659613609314 and batch: 450, loss is 4.37871452331543 and perplexity is 79.73546941053566
At time: 98.53599238395691 and batch: 500, loss is 4.4099783611297605 and perplexity is 82.26768330521705
At time: 99.37279915809631 and batch: 550, loss is 4.394423713684082 and perplexity is 80.99793930615628
At time: 100.22646713256836 and batch: 600, loss is 4.418529615402222 and perplexity is 82.97419164457592
At time: 101.06297850608826 and batch: 650, loss is 4.44452579498291 and perplexity is 85.15948520574787
At time: 101.8915662765503 and batch: 700, loss is 4.4222633838653564 and perplexity is 83.28457715782193
At time: 102.78138303756714 and batch: 750, loss is 4.428150081634522 and perplexity is 83.77629416713003
At time: 103.60653162002563 and batch: 800, loss is 4.456239128112793 and perplexity is 86.16285153518902
At time: 104.43069362640381 and batch: 850, loss is 4.488034296035766 and perplexity is 88.94643156871267
At time: 105.26001167297363 and batch: 900, loss is 4.439028148651123 and perplexity is 84.69259305420134
At time: 106.08865141868591 and batch: 950, loss is 4.442049970626831 and perplexity is 84.94890606417381
At time: 106.91522192955017 and batch: 1000, loss is 4.43794921875 and perplexity is 84.60126496031668
At time: 107.74229693412781 and batch: 1050, loss is 4.427549085617065 and perplexity is 83.72596007478789
At time: 108.56758189201355 and batch: 1100, loss is 4.391901702880859 and perplexity is 80.79391900702649
At time: 109.39387083053589 and batch: 1150, loss is 4.392021493911743 and perplexity is 80.80359797358862
At time: 110.22820258140564 and batch: 1200, loss is 4.405164575576782 and perplexity is 81.87261596745908
At time: 111.05515646934509 and batch: 1250, loss is 4.436517715454102 and perplexity is 84.48024461186891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.579060129875685 and perplexity of 97.42278639771318
Finished 5 epochs...
Completing Train Step...
At time: 113.54382467269897 and batch: 50, loss is 4.375594167709351 and perplexity is 79.48705416506944
At time: 114.39941453933716 and batch: 100, loss is 4.3723290920257565 and perplexity is 79.22794615118208
At time: 115.22705364227295 and batch: 150, loss is 4.299297666549682 and perplexity is 73.64805004197683
At time: 116.0540521144867 and batch: 200, loss is 4.3524136734008785 and perplexity is 77.66569649981477
At time: 116.88020610809326 and batch: 250, loss is 4.353581695556641 and perplexity is 77.75646475341549
At time: 117.71093964576721 and batch: 300, loss is 4.360817785263062 and perplexity is 78.32115812959862
At time: 118.53756046295166 and batch: 350, loss is 4.356542930603028 and perplexity is 77.9870611785276
At time: 119.36583375930786 and batch: 400, loss is 4.355718688964844 and perplexity is 77.92280747938948
At time: 120.18893694877625 and batch: 450, loss is 4.286884937286377 and perplexity is 72.73952702623853
At time: 121.01567363739014 and batch: 500, loss is 4.324997673034668 and perplexity is 75.56533702160245
At time: 121.84475874900818 and batch: 550, loss is 4.308262782096863 and perplexity is 74.31128185316732
At time: 122.66848611831665 and batch: 600, loss is 4.334238109588623 and perplexity is 76.26682978225753
At time: 123.49544739723206 and batch: 650, loss is 4.361836643218994 and perplexity is 78.40099692996873
At time: 124.3752989768982 and batch: 700, loss is 4.337872247695923 and perplexity is 76.5444982118065
At time: 125.20079350471497 and batch: 750, loss is 4.3490205574035645 and perplexity is 77.40261436908243
At time: 126.02376961708069 and batch: 800, loss is 4.3803040885925295 and perplexity is 79.86231493197356
At time: 126.84692859649658 and batch: 850, loss is 4.407684183120727 and perplexity is 82.07916292766218
At time: 127.67115521430969 and batch: 900, loss is 4.356768236160279 and perplexity is 78.00463407636623
At time: 128.49797010421753 and batch: 950, loss is 4.359158391952515 and perplexity is 78.19130029613206
At time: 129.32421898841858 and batch: 1000, loss is 4.35374529838562 and perplexity is 77.76918697168765
At time: 130.15417408943176 and batch: 1050, loss is 4.34836636543274 and perplexity is 77.35199475951768
At time: 130.98095750808716 and batch: 1100, loss is 4.3081515026092525 and perplexity is 74.30301299188488
At time: 131.80811667442322 and batch: 1150, loss is 4.3149059391021725 and perplexity is 74.80658673829669
At time: 132.63412594795227 and batch: 1200, loss is 4.321904649734497 and perplexity is 75.33197276010412
At time: 133.46065664291382 and batch: 1250, loss is 4.362225761413574 and perplexity is 78.43151012058124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.572306612112226 and perplexity of 96.76705661290332
Finished 6 epochs...
Completing Train Step...
At time: 135.9698040485382 and batch: 50, loss is 4.299942588806152 and perplexity is 73.69556262790975
At time: 136.79721641540527 and batch: 100, loss is 4.293421106338501 and perplexity is 73.21652203349493
At time: 137.62287878990173 and batch: 150, loss is 4.22786997795105 and perplexity is 68.57101872838548
At time: 138.45210480690002 and batch: 200, loss is 4.282228832244873 and perplexity is 72.40163139695179
At time: 139.28400087356567 and batch: 250, loss is 4.2861789894104 and perplexity is 72.68819483269314
At time: 140.11042737960815 and batch: 300, loss is 4.295316066741943 and perplexity is 73.35539598238667
At time: 140.93616437911987 and batch: 350, loss is 4.287573490142822 and perplexity is 72.78962928239525
At time: 141.76302337646484 and batch: 400, loss is 4.287488565444947 and perplexity is 72.78344790759937
At time: 142.58867764472961 and batch: 450, loss is 4.216766214370727 and perplexity is 67.81383392811982
At time: 143.41505122184753 and batch: 500, loss is 4.254674615859986 and perplexity is 70.43389538696111
At time: 144.24144101142883 and batch: 550, loss is 4.239475064277649 and perplexity is 69.37142673916777
At time: 145.06719160079956 and batch: 600, loss is 4.269209775924683 and perplexity is 71.46513982595476
At time: 145.94922423362732 and batch: 650, loss is 4.294978408813477 and perplexity is 73.33063113259666
At time: 146.78127765655518 and batch: 700, loss is 4.278419361114502 and perplexity is 72.12634415466995
At time: 147.60672545433044 and batch: 750, loss is 4.282145643234253 and perplexity is 72.39560862738622
At time: 148.43360900878906 and batch: 800, loss is 4.311587319374085 and perplexity is 74.55874359978344
At time: 149.25836968421936 and batch: 850, loss is 4.346648464202881 and perplexity is 77.21922574726513
At time: 150.08892583847046 and batch: 900, loss is 4.296747078895569 and perplexity is 73.46044358984666
At time: 150.91643261909485 and batch: 950, loss is 4.295549383163452 and perplexity is 73.3725129976384
At time: 151.74220037460327 and batch: 1000, loss is 4.286615648269653 and perplexity is 72.71994170770263
At time: 152.5721402168274 and batch: 1050, loss is 4.287896757125854 and perplexity is 72.81316356997351
At time: 153.40541243553162 and batch: 1100, loss is 4.246656179428101 and perplexity is 69.8713839192751
At time: 154.23012971878052 and batch: 1150, loss is 4.2498670673370365 and perplexity is 70.09609366692864
At time: 155.0586874485016 and batch: 1200, loss is 4.256884469985962 and perplexity is 70.5897161284328
At time: 155.88312244415283 and batch: 1250, loss is 4.304465589523315 and perplexity is 74.02964266310927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.573697055343294 and perplexity of 96.9016992965727
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 158.36829328536987 and batch: 50, loss is 4.2498655605316165 and perplexity is 70.09598804583435
At time: 159.19573783874512 and batch: 100, loss is 4.227961587905884 and perplexity is 68.57730080406003
At time: 160.02981090545654 and batch: 150, loss is 4.151477699279785 and perplexity is 63.52780596589213
At time: 160.85546350479126 and batch: 200, loss is 4.204478969573975 and perplexity is 66.98568699130188
At time: 161.6788945198059 and batch: 250, loss is 4.198361577987671 and perplexity is 66.57716014664616
At time: 162.50892734527588 and batch: 300, loss is 4.191164331436157 and perplexity is 66.0997081412843
At time: 163.3413565158844 and batch: 350, loss is 4.1772315979003904 and perplexity is 65.18514450133266
At time: 164.16973686218262 and batch: 400, loss is 4.172314019203186 and perplexity is 64.86537830547249
At time: 164.9940755367279 and batch: 450, loss is 4.087969441413879 and perplexity is 59.61870942960335
At time: 165.82009840011597 and batch: 500, loss is 4.113633379936219 and perplexity is 61.16856293060318
At time: 166.65187120437622 and batch: 550, loss is 4.096865992546082 and perplexity is 60.151476708169064
At time: 167.5064513683319 and batch: 600, loss is 4.1125580596923825 and perplexity is 61.10282248894613
At time: 168.334712266922 and batch: 650, loss is 4.133275203704834 and perplexity is 62.38190215272108
At time: 169.1618263721466 and batch: 700, loss is 4.105864744186402 and perplexity is 60.69520768872989
At time: 169.98900604248047 and batch: 750, loss is 4.0951706981658935 and perplexity is 60.04958863728505
At time: 170.8225827217102 and batch: 800, loss is 4.113246088027954 and perplexity is 61.14487742803778
At time: 171.6502068042755 and batch: 850, loss is 4.135080337524414 and perplexity is 62.494611531176766
At time: 172.47459292411804 and batch: 900, loss is 4.076042098999023 and perplexity is 58.91184058130125
At time: 173.2998719215393 and batch: 950, loss is 4.073794283866882 and perplexity is 58.779566374257264
At time: 174.13768696784973 and batch: 1000, loss is 4.047565932273865 and perplexity is 57.2579176407643
At time: 174.97135853767395 and batch: 1050, loss is 4.032964653968811 and perplexity is 56.427952868902594
At time: 175.79755115509033 and batch: 1100, loss is 3.9734023714065554 and perplexity is 53.1651109184416
At time: 176.62656545639038 and batch: 1150, loss is 3.9750085401535036 and perplexity is 53.25057167184996
At time: 177.46036219596863 and batch: 1200, loss is 3.975263056755066 and perplexity is 53.26412655128088
At time: 178.2894003391266 and batch: 1250, loss is 4.038548789024353 and perplexity is 56.74393560266499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.425758528883439 and perplexity of 83.5761781297497
Finished 8 epochs...
Completing Train Step...
At time: 180.77337050437927 and batch: 50, loss is 4.122181005477906 and perplexity is 61.693649827559696
At time: 181.6277539730072 and batch: 100, loss is 4.112058072090149 and perplexity is 61.07227947144152
At time: 182.45276498794556 and batch: 150, loss is 4.044883360862732 and perplexity is 57.104525023152384
At time: 183.2798728942871 and batch: 200, loss is 4.096381244659423 and perplexity is 60.12232547302604
At time: 184.10859322547913 and batch: 250, loss is 4.097309427261353 and perplexity is 60.17815587591367
At time: 184.93531727790833 and batch: 300, loss is 4.101847252845764 and perplexity is 60.45185437964311
At time: 185.76400756835938 and batch: 350, loss is 4.089525327682495 and perplexity is 59.71154156034906
At time: 186.59152269363403 and batch: 400, loss is 4.090006489753723 and perplexity is 59.74027940258782
At time: 187.4169511795044 and batch: 450, loss is 4.010915536880493 and perplexity is 55.19738267663138
At time: 188.24541521072388 and batch: 500, loss is 4.0421887874603275 and perplexity is 56.950859812938944
At time: 189.1004297733307 and batch: 550, loss is 4.026673622131348 and perplexity is 56.074077109828245
At time: 189.92631220817566 and batch: 600, loss is 4.048321776390075 and perplexity is 57.301212060774674
At time: 190.75833249092102 and batch: 650, loss is 4.076170310974121 and perplexity is 58.9193942689651
At time: 191.58306193351746 and batch: 700, loss is 4.052412333488465 and perplexity is 57.53608599563159
At time: 192.4103217124939 and batch: 750, loss is 4.046235647201538 and perplexity is 57.18179892867282
At time: 193.23567867279053 and batch: 800, loss is 4.068864755630493 and perplexity is 58.490523849054256
At time: 194.06349730491638 and batch: 850, loss is 4.093791708946228 and perplexity is 59.96683797116045
At time: 194.8883912563324 and batch: 900, loss is 4.042254209518433 and perplexity is 56.95458577727756
At time: 195.71509289741516 and batch: 950, loss is 4.0457892322540285 and perplexity is 57.15627781583254
At time: 196.54621028900146 and batch: 1000, loss is 4.02301242351532 and perplexity is 55.869154137076144
At time: 197.3717451095581 and batch: 1050, loss is 4.015953464508057 and perplexity is 55.47616474809671
At time: 198.20038485527039 and batch: 1100, loss is 3.9613136053085327 and perplexity is 52.52627944986426
At time: 199.02886271476746 and batch: 1150, loss is 3.967811017036438 and perplexity is 52.868675453330745
At time: 199.85664081573486 and batch: 1200, loss is 3.9732111215591432 and perplexity is 53.154944071325765
At time: 200.69119501113892 and batch: 1250, loss is 4.035435662269593 and perplexity is 56.56755922203147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.41964699404083 and perplexity of 83.06695705126555
Finished 9 epochs...
Completing Train Step...
At time: 203.24041938781738 and batch: 50, loss is 4.0762943983078 and perplexity is 58.92670587313124
At time: 204.07131123542786 and batch: 100, loss is 4.067425675392151 and perplexity is 58.40641182854208
At time: 204.8991084098816 and batch: 150, loss is 4.001307983398437 and perplexity is 54.66961023116133
At time: 205.72626948356628 and batch: 200, loss is 4.053609104156494 and perplexity is 57.60498471546443
At time: 206.5531415939331 and batch: 250, loss is 4.055916786193848 and perplexity is 57.738072206698845
At time: 207.3844437599182 and batch: 300, loss is 4.0623007488250735 and perplexity is 58.107848966721555
At time: 208.21107935905457 and batch: 350, loss is 4.048654332160949 and perplexity is 57.32027107844107
At time: 209.03780841827393 and batch: 400, loss is 4.052328720092773 and perplexity is 57.53127540922422
At time: 209.92065453529358 and batch: 450, loss is 3.9744157457351683 and perplexity is 53.21901438460571
At time: 210.74733877182007 and batch: 500, loss is 4.007031006813049 and perplexity is 54.98338269775332
At time: 211.57356333732605 and batch: 550, loss is 3.993428874015808 and perplexity is 54.24055489884855
At time: 212.3990559577942 and batch: 600, loss is 4.016457858085633 and perplexity is 55.504153627416294
At time: 213.22735905647278 and batch: 650, loss is 4.046096820831298 and perplexity is 57.17386113808386
At time: 214.06207966804504 and batch: 700, loss is 4.023054566383362 and perplexity is 55.87150867307963
At time: 214.88722896575928 and batch: 750, loss is 4.018931541442871 and perplexity is 55.641623286606766
At time: 215.71485352516174 and batch: 800, loss is 4.04247793674469 and perplexity is 56.9673294942812
At time: 216.5479302406311 and batch: 850, loss is 4.069161276817322 and perplexity is 58.50787010024264
At time: 217.37456059455872 and batch: 900, loss is 4.021011357307434 and perplexity is 55.75746804357145
At time: 218.20644855499268 and batch: 950, loss is 4.026952047348022 and perplexity is 56.089691720547165
At time: 219.0344021320343 and batch: 1000, loss is 4.004746375083923 and perplexity is 54.85790930189414
At time: 219.86585974693298 and batch: 1050, loss is 4.000849227905274 and perplexity is 54.644535999069305
At time: 220.69109225273132 and batch: 1100, loss is 3.947765564918518 and perplexity is 51.819450181552455
At time: 221.51548433303833 and batch: 1150, loss is 3.9562163305282594 and perplexity is 52.259219786798674
At time: 222.34442520141602 and batch: 1200, loss is 3.9638222312927245 and perplexity is 52.65821365694442
At time: 223.1705358028412 and batch: 1250, loss is 4.024246616363525 and perplexity is 55.93815001589943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.418739040402601 and perplexity of 82.99157033439506
Finished 10 epochs...
Completing Train Step...
At time: 225.65632677078247 and batch: 50, loss is 4.043147573471069 and perplexity is 57.00548968562117
At time: 226.5188398361206 and batch: 100, loss is 4.035430879592895 and perplexity is 56.56728867833112
At time: 227.3456211090088 and batch: 150, loss is 3.9703135871887207 and perplexity is 53.001148715188116
At time: 228.17787170410156 and batch: 200, loss is 4.022721447944641 and perplexity is 55.85289994296914
At time: 229.00327372550964 and batch: 250, loss is 4.026181387901306 and perplexity is 56.04648232176358
At time: 229.83538603782654 and batch: 300, loss is 4.033529782295227 and perplexity is 56.45985091586649
At time: 230.66399836540222 and batch: 350, loss is 4.019259004592896 and perplexity is 55.6598468514508
At time: 231.5451934337616 and batch: 400, loss is 4.025664472579956 and perplexity is 56.01751852290433
At time: 232.37087869644165 and batch: 450, loss is 3.9476750564575194 and perplexity is 51.81476029510712
At time: 233.2012128829956 and batch: 500, loss is 3.9803886365890504 and perplexity is 53.53783694721661
At time: 234.02840161323547 and batch: 550, loss is 3.9678906917572023 and perplexity is 52.87288791809593
At time: 234.85467314720154 and batch: 600, loss is 3.9927212238311767 and perplexity is 54.20218513794527
At time: 235.68076634407043 and batch: 650, loss is 4.022641944885254 and perplexity is 55.848459643059066
At time: 236.51275968551636 and batch: 700, loss is 4.000019783973694 and perplexity is 54.59923021219331
At time: 237.33857774734497 and batch: 750, loss is 3.9967410230636595 and perplexity is 54.42050554828799
At time: 238.16516757011414 and batch: 800, loss is 4.02100827217102 and perplexity is 55.7572960244418
At time: 238.99966645240784 and batch: 850, loss is 4.04888792514801 and perplexity is 57.33366225576309
At time: 239.82530283927917 and batch: 900, loss is 4.002977709770203 and perplexity is 54.76096977259053
At time: 240.652437210083 and batch: 950, loss is 4.009816317558289 and perplexity is 55.136741981875765
At time: 241.47922277450562 and batch: 1000, loss is 3.9882319450378416 and perplexity is 53.95940178647553
At time: 242.3045527935028 and batch: 1050, loss is 3.9862612962722777 and perplexity is 53.853171463660104
At time: 243.12978649139404 and batch: 1100, loss is 3.9336236238479616 and perplexity is 51.091780032421916
At time: 243.9569308757782 and batch: 1150, loss is 3.9431534814834595 and perplexity is 51.581004841328486
At time: 244.78063106536865 and batch: 1200, loss is 3.9520246505737306 and perplexity is 52.040624323952315
At time: 245.61600756645203 and batch: 1250, loss is 4.011403799057007 and perplexity is 55.22434005143265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.418770226248859 and perplexity of 82.99415853710569
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 248.1066448688507 and batch: 50, loss is 4.034823303222656 and perplexity is 56.53293016916776
At time: 248.95091104507446 and batch: 100, loss is 4.037056131362915 and perplexity is 56.65929951451452
At time: 249.780535697937 and batch: 150, loss is 3.970204429626465 and perplexity is 52.995363554750355
At time: 250.61129546165466 and batch: 200, loss is 4.020291948318482 and perplexity is 55.717370045019386
At time: 251.4392294883728 and batch: 250, loss is 4.027316017150879 and perplexity is 56.11011039024917
At time: 252.27131175994873 and batch: 300, loss is 4.026231546401977 and perplexity is 56.04929359978888
At time: 253.13351607322693 and batch: 350, loss is 4.006262421607971 and perplexity is 54.941139519107246
At time: 253.96012210845947 and batch: 400, loss is 4.00917188167572 and perplexity is 55.10122133351421
At time: 254.79585576057434 and batch: 450, loss is 3.9313108205795286 and perplexity is 50.97375133776013
At time: 255.62068557739258 and batch: 500, loss is 3.956407012939453 and perplexity is 52.26918565096203
At time: 256.45053243637085 and batch: 550, loss is 3.935624055862427 and perplexity is 51.194087960785026
At time: 257.2767126560211 and batch: 600, loss is 3.958375039100647 and perplexity is 52.372154064779366
At time: 258.11284136772156 and batch: 650, loss is 3.988168339729309 and perplexity is 53.95596979122439
At time: 258.94397592544556 and batch: 700, loss is 3.9654164028167727 and perplexity is 52.74222682957482
At time: 259.7732791900635 and batch: 750, loss is 3.9560028886795044 and perplexity is 52.24806667262589
At time: 260.6054744720459 and batch: 800, loss is 3.9674806213378906 and perplexity is 52.85121075566244
At time: 261.43275928497314 and batch: 850, loss is 3.993548970222473 and perplexity is 54.24706937491342
At time: 262.26122522354126 and batch: 900, loss is 3.944650511741638 and perplexity is 51.65828099426085
At time: 263.0915324687958 and batch: 950, loss is 3.945228877067566 and perplexity is 51.68816699446499
At time: 263.9236240386963 and batch: 1000, loss is 3.9223578786849975 and perplexity is 50.51942312565438
At time: 264.7588219642639 and batch: 1050, loss is 3.9133409309387206 and perplexity is 50.06593971780248
At time: 265.5981168746948 and batch: 1100, loss is 3.85665696144104 and perplexity is 47.30693780786876
At time: 266.4294652938843 and batch: 1150, loss is 3.861942253112793 and perplexity is 47.557630681181166
At time: 267.2582597732544 and batch: 1200, loss is 3.8705601406097414 and perplexity is 47.96924808073718
At time: 268.09083318710327 and batch: 1250, loss is 3.931864686012268 and perplexity is 51.00199175657712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.378788634808394 and perplexity of 79.74137894419539
Finished 12 epochs...
Completing Train Step...
At time: 270.5830645561218 and batch: 50, loss is 4.007172856330872 and perplexity is 54.991182617271576
At time: 271.4517230987549 and batch: 100, loss is 4.003745484352112 and perplexity is 54.803029997582286
At time: 272.2820394039154 and batch: 150, loss is 3.93647412776947 and perplexity is 51.237625119003006
At time: 273.1083619594574 and batch: 200, loss is 3.9868761110305786 and perplexity is 53.886291368517156
At time: 273.9454720020294 and batch: 250, loss is 3.9952646493911743 and perplexity is 54.34021982709878
At time: 274.81679344177246 and batch: 300, loss is 3.9981950426101687 and perplexity is 54.499691582155556
At time: 275.64590549468994 and batch: 350, loss is 3.979750146865845 and perplexity is 53.503664499062566
At time: 276.48001194000244 and batch: 400, loss is 3.9848851013183593 and perplexity is 53.77910997405229
At time: 277.3140881061554 and batch: 450, loss is 3.9086966466903688 and perplexity is 49.83395837321869
At time: 278.1436378955841 and batch: 500, loss is 3.9352800846099854 and perplexity is 51.17648169442989
At time: 278.9691631793976 and batch: 550, loss is 3.9152539491653444 and perplexity is 50.16180844308803
At time: 279.79485154151917 and batch: 600, loss is 3.9393994855880736 and perplexity is 51.38773295877454
At time: 280.6219835281372 and batch: 650, loss is 3.97083655834198 and perplexity is 53.02887403619541
At time: 281.4504437446594 and batch: 700, loss is 3.949943714141846 and perplexity is 51.932443690407204
At time: 282.2813801765442 and batch: 750, loss is 3.9437480068206785 and perplexity is 51.61168017335339
At time: 283.1107642650604 and batch: 800, loss is 3.9573005056381225 and perplexity is 52.315908656930546
At time: 283.9425027370453 and batch: 850, loss is 3.9860833168029783 and perplexity is 53.843587557677466
At time: 284.7719986438751 and batch: 900, loss is 3.938420100212097 and perplexity is 51.33742920201292
At time: 285.6005127429962 and batch: 950, loss is 3.941301999092102 and perplexity is 51.48559187410681
At time: 286.4266378879547 and batch: 1000, loss is 3.920470938682556 and perplexity is 50.424185886994
At time: 287.254677772522 and batch: 1050, loss is 3.914880185127258 and perplexity is 50.143063266361324
At time: 288.0845470428467 and batch: 1100, loss is 3.860097451210022 and perplexity is 47.469977150168866
At time: 288.9104673862457 and batch: 1150, loss is 3.8675501346588135 and perplexity is 47.82507744463937
At time: 289.7389862537384 and batch: 1200, loss is 3.877157254219055 and perplexity is 48.286752815950514
At time: 290.56568598747253 and batch: 1250, loss is 3.9368103313446046 and perplexity is 51.254854287841205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.377020842837592 and perplexity of 79.60053730079883
Finished 13 epochs...
Completing Train Step...
At time: 293.10412335395813 and batch: 50, loss is 3.9942222452163696 and perplexity is 54.28360486805255
At time: 293.96532106399536 and batch: 100, loss is 3.98971022605896 and perplexity is 54.0392279342499
At time: 294.7952229976654 and batch: 150, loss is 3.9218555593490603 and perplexity is 50.49405261516043
At time: 295.65442943573 and batch: 200, loss is 3.9718147802352903 and perplexity is 53.080773422174396
At time: 296.4795162677765 and batch: 250, loss is 3.9809716510772706 and perplexity is 53.56905938250648
At time: 297.309800863266 and batch: 300, loss is 3.9850079298019407 and perplexity is 53.785715986273686
At time: 298.13604736328125 and batch: 350, loss is 3.9667496204376222 and perplexity is 52.81259059042733
At time: 298.96583104133606 and batch: 400, loss is 3.972701292037964 and perplexity is 53.12785101864745
At time: 299.79832315444946 and batch: 450, loss is 3.897336721420288 and perplexity is 49.27105167269366
At time: 300.6281096935272 and batch: 500, loss is 3.924231128692627 and perplexity is 50.6141473287676
At time: 301.4565839767456 and batch: 550, loss is 3.9048445081710814 and perplexity is 49.64236033062713
At time: 302.2834208011627 and batch: 600, loss is 3.9298624563217164 and perplexity is 50.89997621775624
At time: 303.11358165740967 and batch: 650, loss is 3.96182493686676 and perplexity is 52.553144662113326
At time: 303.9378921985626 and batch: 700, loss is 3.9420208883285524 and perplexity is 51.52261761904755
At time: 304.76916909217834 and batch: 750, loss is 3.9368970012664795 and perplexity is 51.259296734568515
At time: 305.59748339653015 and batch: 800, loss is 3.951120162010193 and perplexity is 51.99357545520134
At time: 306.42451000213623 and batch: 850, loss is 3.9813737058639527 and perplexity is 53.59060140949663
At time: 307.25823307037354 and batch: 900, loss is 3.9340056133270265 and perplexity is 51.11130028288879
At time: 308.0889503955841 and batch: 950, loss is 3.9378626775741576 and perplexity is 51.30882053110329
At time: 308.9171073436737 and batch: 1000, loss is 3.9179996919631956 and perplexity is 50.2997291280617
At time: 309.7415783405304 and batch: 1050, loss is 3.9140267515182496 and perplexity is 50.100287746540594
At time: 310.5704975128174 and batch: 1100, loss is 3.860197777748108 and perplexity is 47.47473988754985
At time: 311.40323781967163 and batch: 1150, loss is 3.868418502807617 and perplexity is 47.8666252553933
At time: 312.2310881614685 and batch: 1200, loss is 3.8782581615448 and perplexity is 48.33994132830145
At time: 313.0604045391083 and batch: 1250, loss is 3.936857237815857 and perplexity is 51.25725852857719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.376426975222399 and perplexity of 79.5532791534745
Finished 14 epochs...
Completing Train Step...
At time: 315.57477140426636 and batch: 50, loss is 3.9836887550354003 and perplexity is 53.71481000591701
At time: 316.4072701931 and batch: 100, loss is 3.978847885131836 and perplexity is 53.45541196143837
At time: 317.29011130332947 and batch: 150, loss is 3.910939998626709 and perplexity is 49.94587897193878
At time: 318.1176998615265 and batch: 200, loss is 3.9608204555511475 and perplexity is 52.500382513955884
At time: 318.9504702091217 and batch: 250, loss is 3.9706833076477053 and perplexity is 53.02074794711306
At time: 319.7757441997528 and batch: 300, loss is 3.9752287817001344 and perplexity is 53.2623009517039
At time: 320.60367345809937 and batch: 350, loss is 3.957013268470764 and perplexity is 52.30088374148053
At time: 321.42869448661804 and batch: 400, loss is 3.9636252450942995 and perplexity is 52.64784173721634
At time: 322.2690372467041 and batch: 450, loss is 3.888794322013855 and perplexity is 48.85195128016959
At time: 323.1094627380371 and batch: 500, loss is 3.9158465385437013 and perplexity is 50.19154260717536
At time: 323.9567565917969 and batch: 550, loss is 3.896927146911621 and perplexity is 49.2508756379901
At time: 324.79509568214417 and batch: 600, loss is 3.922506055831909 and perplexity is 50.52690950427817
At time: 325.6240077018738 and batch: 650, loss is 3.954737801551819 and perplexity is 52.18201010849637
At time: 326.44896245002747 and batch: 700, loss is 3.9357143878936767 and perplexity is 51.1987126356135
At time: 327.2883446216583 and batch: 750, loss is 3.9312202310562134 and perplexity is 50.969133859075605
At time: 328.1239137649536 and batch: 800, loss is 3.945731453895569 and perplexity is 51.714150798360315
At time: 328.95295214653015 and batch: 850, loss is 3.977088747024536 and perplexity is 53.361459171410814
At time: 329.79206442832947 and batch: 900, loss is 3.9297363805770873 and perplexity is 50.89355936986592
At time: 330.6185612678528 and batch: 950, loss is 3.9342052841186526 and perplexity is 51.12150673560865
At time: 331.44405126571655 and batch: 1000, loss is 3.914938735961914 and perplexity is 50.145999270519674
At time: 332.275936126709 and batch: 1050, loss is 3.9120136976242064 and perplexity is 49.99953461196889
At time: 333.10432505607605 and batch: 1100, loss is 3.8589090061187745 and perplexity is 47.413595198905895
At time: 333.931608915329 and batch: 1150, loss is 3.867407650947571 and perplexity is 47.81826363555453
At time: 334.7591655254364 and batch: 1200, loss is 3.8774161195755004 and perplexity is 48.299254201447795
At time: 335.5837914943695 and batch: 1250, loss is 3.9352524375915525 and perplexity is 51.17506683685554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3761681326984485 and perplexity of 79.532690046693
Finished 15 epochs...
Completing Train Step...
At time: 338.05140948295593 and batch: 50, loss is 3.9744514083862303 and perplexity is 53.2209123495886
At time: 338.9077227115631 and batch: 100, loss is 3.969519257545471 and perplexity is 52.959065048002294
At time: 339.73274183273315 and batch: 150, loss is 3.901842083930969 and perplexity is 49.493536432671796
At time: 340.5603425502777 and batch: 200, loss is 3.9517099618911744 and perplexity is 52.02425030493877
At time: 341.38649439811707 and batch: 250, loss is 3.962201952934265 and perplexity is 52.57296177749958
At time: 342.21224212646484 and batch: 300, loss is 3.9670813131332396 and perplexity is 52.83011104650582
At time: 343.0408208370209 and batch: 350, loss is 3.9487907123565673 and perplexity is 51.87259999669088
At time: 343.86969351768494 and batch: 400, loss is 3.9560149335861206 and perplexity is 52.248695999499915
At time: 344.6958386898041 and batch: 450, loss is 3.8814482927322387 and perplexity is 48.49439832077466
At time: 345.5222342014313 and batch: 500, loss is 3.9086220979690554 and perplexity is 49.83024345381695
At time: 346.3496346473694 and batch: 550, loss is 3.8901105880737306 and perplexity is 48.9162957835513
At time: 347.1755533218384 and batch: 600, loss is 3.9160930252075197 and perplexity is 50.20391567790031
At time: 348.0008633136749 and batch: 650, loss is 3.948488917350769 and perplexity is 51.856947467120534
At time: 348.8325090408325 and batch: 700, loss is 3.93005877494812 and perplexity is 50.909969812103554
At time: 349.6654145717621 and batch: 750, loss is 3.9259991312026976 and perplexity is 50.70371242061308
At time: 350.49401092529297 and batch: 800, loss is 3.9406576585769653 and perplexity is 51.45242830677894
At time: 351.3205785751343 and batch: 850, loss is 3.9729367446899415 and perplexity is 53.14036158482886
At time: 352.15575194358826 and batch: 900, loss is 3.925509896278381 and perplexity is 50.678912460702335
At time: 352.98707580566406 and batch: 950, loss is 3.9304325771331787 and perplexity is 50.929003627279805
At time: 353.8169357776642 and batch: 1000, loss is 3.9115880346298217 and perplexity is 49.97825618938794
At time: 354.64441323280334 and batch: 1050, loss is 3.9094408798217772 and perplexity is 49.871060260623686
At time: 355.4690592288971 and batch: 1100, loss is 3.85690625667572 and perplexity is 47.31873267217222
At time: 356.29719734191895 and batch: 1150, loss is 3.865586757659912 and perplexity is 47.73127090654768
At time: 357.1246728897095 and batch: 1200, loss is 3.8756269311904905 and perplexity is 48.21291499840794
At time: 357.950421333313 and batch: 1250, loss is 3.932890248298645 and perplexity is 51.05432430641066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.376091504619069 and perplexity of 79.52659584290338
Finished 16 epochs...
Completing Train Step...
At time: 360.47846007347107 and batch: 50, loss is 3.9661471891403197 and perplexity is 52.78078421450412
At time: 361.3093936443329 and batch: 100, loss is 3.961181640625 and perplexity is 52.51934829336351
At time: 362.14098048210144 and batch: 150, loss is 3.8938632154464723 and perplexity is 49.100205270147555
At time: 362.9708523750305 and batch: 200, loss is 3.9437131214141847 and perplexity is 51.609879710315845
At time: 363.79980516433716 and batch: 250, loss is 3.9546931648254393 and perplexity is 52.179680926373145
At time: 364.63128447532654 and batch: 300, loss is 3.9599392080307005 and perplexity is 52.45413706188077
At time: 365.4654018878937 and batch: 350, loss is 3.941453537940979 and perplexity is 51.49339453262117
At time: 366.2937207221985 and batch: 400, loss is 3.9493214511871337 and perplexity is 51.90013810687734
At time: 367.12174224853516 and batch: 450, loss is 3.8749219608306884 and perplexity is 48.17893830006418
At time: 367.94869565963745 and batch: 500, loss is 3.9020810985565184 and perplexity is 49.50536752559516
At time: 368.77983498573303 and batch: 550, loss is 3.8838820552825926 and perplexity is 48.61256590890751
At time: 369.60768604278564 and batch: 600, loss is 3.910138502120972 and perplexity is 49.90586356271429
At time: 370.43587374687195 and batch: 650, loss is 3.9427278232574463 and perplexity is 51.559053634499065
At time: 371.2628560066223 and batch: 700, loss is 3.9247762823104857 and perplexity is 50.64174733673726
At time: 372.0917077064514 and batch: 750, loss is 3.9210210418701172 and perplexity is 50.45193202329994
At time: 372.9250626564026 and batch: 800, loss is 3.9357604455947874 and perplexity is 51.20107078492237
At time: 373.75033378601074 and batch: 850, loss is 3.968831295967102 and perplexity is 52.92264377567741
At time: 374.57731342315674 and batch: 900, loss is 3.9213253927230833 and perplexity is 50.467289448749234
At time: 375.4052882194519 and batch: 950, loss is 3.9266216802597045 and perplexity is 50.73528779655774
At time: 376.2325210571289 and batch: 1000, loss is 3.9080831718444826 and perplexity is 49.80339586890812
At time: 377.061425447464 and batch: 1050, loss is 3.906550154685974 and perplexity is 49.72710490111163
At time: 377.89354705810547 and batch: 1100, loss is 3.8544796657562257 and perplexity is 47.2040486670056
At time: 378.7230179309845 and batch: 1150, loss is 3.8632479381561278 and perplexity is 47.61976652435107
At time: 379.55674409866333 and batch: 1200, loss is 3.8733252811431886 and perplexity is 48.10207334859838
At time: 380.3891134262085 and batch: 1250, loss is 3.930118050575256 and perplexity is 50.91298762193205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.37611422573563 and perplexity of 79.52840279648512
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 382.8866879940033 and batch: 50, loss is 3.9686485433578493 and perplexity is 52.9129729081539
At time: 383.7460505962372 and batch: 100, loss is 3.9725384950637816 and perplexity is 53.11920266923847
At time: 384.57637882232666 and batch: 150, loss is 3.903356785774231 and perplexity is 49.56856118926022
At time: 385.41119265556335 and batch: 200, loss is 3.9570705127716064 and perplexity is 52.30387775469802
At time: 386.2393388748169 and batch: 250, loss is 3.9700848340988157 and perplexity is 52.98902592526669
At time: 387.0673050880432 and batch: 300, loss is 3.975259962081909 and perplexity is 53.263961716473275
At time: 387.89553141593933 and batch: 350, loss is 3.9470296239852907 and perplexity is 51.78132815652644
At time: 388.72645139694214 and batch: 400, loss is 3.9520474672317505 and perplexity is 52.041811730626925
At time: 389.5537655353546 and batch: 450, loss is 3.8757393980026245 and perplexity is 48.21833765619029
At time: 390.3784303665161 and batch: 500, loss is 3.902078266143799 and perplexity is 49.50522730616108
At time: 391.2150559425354 and batch: 550, loss is 3.879725556373596 and perplexity is 48.410927177627514
At time: 392.04732060432434 and batch: 600, loss is 3.9041250467300417 and perplexity is 49.60665741150392
At time: 392.87561798095703 and batch: 650, loss is 3.9363609027862547 and perplexity is 51.23182406817705
At time: 393.70166754722595 and batch: 700, loss is 3.9212192153930663 and perplexity is 50.46193125116696
At time: 394.5294806957245 and batch: 750, loss is 3.913269557952881 and perplexity is 50.06236648971344
At time: 395.355810880661 and batch: 800, loss is 3.9241422319412234 and perplexity is 50.60964809548158
At time: 396.1815779209137 and batch: 850, loss is 3.9576749753952027 and perplexity is 52.33550305106177
At time: 397.0095372200012 and batch: 900, loss is 3.9082927560806273 and perplexity is 49.81383496948596
At time: 397.83812618255615 and batch: 950, loss is 3.9106457138061526 and perplexity is 49.931182820441215
At time: 398.67770648002625 and batch: 1000, loss is 3.8922469806671143 and perplexity is 49.020911906323455
At time: 399.5081651210785 and batch: 1050, loss is 3.887523341178894 and perplexity is 48.78990082716331
At time: 400.337443113327 and batch: 1100, loss is 3.831742081642151 and perplexity is 46.142852873480614
At time: 401.17310881614685 and batch: 1150, loss is 3.8356453943252564 and perplexity is 46.323314826927984
At time: 402.0053644180298 and batch: 1200, loss is 3.848668961524963 and perplexity is 46.930555265840496
At time: 402.89375019073486 and batch: 1250, loss is 3.908157534599304 and perplexity is 49.80709952432969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.370306084625913 and perplexity of 79.06782944309371
Finished 18 epochs...
Completing Train Step...
At time: 405.3738272190094 and batch: 50, loss is 3.9645016479492186 and perplexity is 52.69400268085695
At time: 406.22895789146423 and batch: 100, loss is 3.9618557405471804 and perplexity is 52.5547635173198
At time: 407.05489706993103 and batch: 150, loss is 3.891957750320435 and perplexity is 49.006735621183005
At time: 407.8812446594238 and batch: 200, loss is 3.9441403102874757 and perplexity is 51.631931586502695
At time: 408.7119562625885 and batch: 250, loss is 3.9581250762939453 and perplexity is 52.35906461016288
At time: 409.5360312461853 and batch: 300, loss is 3.964914541244507 and perplexity is 52.715764173543825
At time: 410.37053894996643 and batch: 350, loss is 3.9384714794158935 and perplexity is 51.34006694601229
At time: 411.2017242908478 and batch: 400, loss is 3.9447917985916137 and perplexity is 51.66558014568259
At time: 412.02933382987976 and batch: 450, loss is 3.869036331176758 and perplexity is 47.896207751922915
At time: 412.85547256469727 and batch: 500, loss is 3.8961268949508665 and perplexity is 49.21147829419525
At time: 413.68432879447937 and batch: 550, loss is 3.8742522239685058 and perplexity is 48.146681891962295
At time: 414.51088213920593 and batch: 600, loss is 3.8991113042831422 and perplexity is 49.358564863365075
At time: 415.3370759487152 and batch: 650, loss is 3.931772618293762 and perplexity is 50.99729633570849
At time: 416.16292691230774 and batch: 700, loss is 3.9170267295837404 and perplexity is 50.250813184468626
At time: 416.9957377910614 and batch: 750, loss is 3.9101049280166627 and perplexity is 49.904188046172536
At time: 417.824782371521 and batch: 800, loss is 3.9214706420898438 and perplexity is 50.474620322973315
At time: 418.6545968055725 and batch: 850, loss is 3.956278510093689 and perplexity is 52.26246934340166
At time: 419.47889161109924 and batch: 900, loss is 3.9071702480316164 and perplexity is 49.757949910362996
At time: 420.3111639022827 and batch: 950, loss is 3.9102946138381958 and perplexity is 49.91365506093087
At time: 421.1387610435486 and batch: 1000, loss is 3.892587308883667 and perplexity is 49.037597945050166
At time: 421.9729163646698 and batch: 1050, loss is 3.8889534997940065 and perplexity is 48.859728044258034
At time: 422.80183959007263 and batch: 1100, loss is 3.833406934738159 and perplexity is 46.219737928347726
At time: 423.6852822303772 and batch: 1150, loss is 3.838066649436951 and perplexity is 46.43561128406748
At time: 424.5203802585602 and batch: 1200, loss is 3.8515426254272462 and perplexity is 47.06561186912404
At time: 425.3604009151459 and batch: 1250, loss is 3.910321822166443 and perplexity is 49.915013146517325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.36979641879562 and perplexity of 79.02754153970714
Finished 19 epochs...
Completing Train Step...
At time: 427.96187233924866 and batch: 50, loss is 3.9610488414764404 and perplexity is 52.51237423171222
At time: 428.7905418872833 and batch: 100, loss is 3.9578846883773804 and perplexity is 52.34647963640637
At time: 429.62303471565247 and batch: 150, loss is 3.8876646184921264 and perplexity is 48.796794220193604
At time: 430.45126128196716 and batch: 200, loss is 3.939630331993103 and perplexity is 51.3995970015238
At time: 431.28540086746216 and batch: 250, loss is 3.9538041400909423 and perplexity is 52.13331251377828
At time: 432.11271619796753 and batch: 300, loss is 3.960994629859924 and perplexity is 52.509527528180904
At time: 432.93857407569885 and batch: 350, loss is 3.9348621463775633 and perplexity is 51.15509755506491
At time: 433.76606726646423 and batch: 400, loss is 3.9412520790100096 and perplexity is 51.48302177328423
At time: 434.5931279659271 and batch: 450, loss is 3.8655845594406126 and perplexity is 47.7311659828621
At time: 435.42118525505066 and batch: 500, loss is 3.8930369234085083 and perplexity is 49.05965091864671
At time: 436.248407125473 and batch: 550, loss is 3.8713442420959474 and perplexity is 48.00687558941438
At time: 437.07626581192017 and batch: 600, loss is 3.896426911354065 and perplexity is 49.22624475988936
At time: 437.91002130508423 and batch: 650, loss is 3.929346833229065 and perplexity is 50.87373777975656
At time: 438.73881673812866 and batch: 700, loss is 3.914860963821411 and perplexity is 50.14209946046901
At time: 439.5679988861084 and batch: 750, loss is 3.9083693504333494 and perplexity is 49.81765057405707
At time: 440.4035279750824 and batch: 800, loss is 3.9200122594833373 and perplexity is 50.40106266526592
At time: 441.22954750061035 and batch: 850, loss is 3.9554492616653443 and perplexity is 52.21914873709205
At time: 442.05698108673096 and batch: 900, loss is 3.9064754247665405 and perplexity is 49.723388937417276
At time: 442.88405871391296 and batch: 950, loss is 3.9099404048919677 and perplexity is 49.89597832858253
At time: 443.7103385925293 and batch: 1000, loss is 3.892577347755432 and perplexity is 49.03710947768156
At time: 444.54021096229553 and batch: 1050, loss is 3.8894945335388185 and perplexity is 48.88616995823118
At time: 445.4240369796753 and batch: 1100, loss is 3.8339051008224487 and perplexity is 46.24276877032428
At time: 446.24915194511414 and batch: 1150, loss is 3.838928518295288 and perplexity is 46.47564994290921
At time: 447.0802481174469 and batch: 1200, loss is 3.8525996255874633 and perplexity is 47.11538652968829
At time: 447.9102716445923 and batch: 1250, loss is 3.9108938789367675 and perplexity is 49.94357553660402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369602621036725 and perplexity of 79.01222766321112
Finished 20 epochs...
Completing Train Step...
At time: 450.39674949645996 and batch: 50, loss is 3.958098177909851 and perplexity is 52.357656254873504
At time: 451.2516405582428 and batch: 100, loss is 3.9547419214248656 and perplexity is 52.1822250921962
At time: 452.078156709671 and batch: 150, loss is 3.8843502235412597 and perplexity is 48.63533009755792
At time: 452.9051425457001 and batch: 200, loss is 3.9362322759628294 and perplexity is 51.22523470518237
At time: 453.7337119579315 and batch: 250, loss is 3.950611596107483 and perplexity is 51.96714001821555
At time: 454.5607719421387 and batch: 300, loss is 3.9580884647369383 and perplexity is 52.35714769837485
At time: 455.39705777168274 and batch: 350, loss is 3.9321553754806517 and perplexity is 51.016819653499674
At time: 456.2303192615509 and batch: 400, loss is 3.938521909713745 and perplexity is 51.3426561061656
At time: 457.0560781955719 and batch: 450, loss is 3.862914175987244 and perplexity is 47.603875499853075
At time: 457.8834593296051 and batch: 500, loss is 3.8906359004974367 and perplexity is 48.94199887193284
At time: 458.7105395793915 and batch: 550, loss is 3.869072618484497 and perplexity is 47.897945807887645
At time: 459.5369362831116 and batch: 600, loss is 3.8943396377563477 and perplexity is 49.12360327658128
At time: 460.3664288520813 and batch: 650, loss is 3.927440938949585 and perplexity is 50.7768701529963
At time: 461.19652485847473 and batch: 700, loss is 3.913143849372864 and perplexity is 50.05607361625211
At time: 462.0250496864319 and batch: 750, loss is 3.906907768249512 and perplexity is 49.74489116841541
At time: 462.8566105365753 and batch: 800, loss is 3.918725061416626 and perplexity is 50.336228251161124
At time: 463.68639302253723 and batch: 850, loss is 3.9546027612686157 and perplexity is 52.17496391084418
At time: 464.5127532482147 and batch: 900, loss is 3.9057243061065674 and perplexity is 49.68605479509002
At time: 465.3401234149933 and batch: 950, loss is 3.9093804025650023 and perplexity is 49.868044286906496
At time: 466.1647837162018 and batch: 1000, loss is 3.8922675800323487 and perplexity is 49.021921716392626
At time: 467.04874086380005 and batch: 1050, loss is 3.889591360092163 and perplexity is 48.89090366674509
At time: 467.87176752090454 and batch: 1100, loss is 3.8339032793045043 and perplexity is 46.242684538367875
At time: 468.7000620365143 and batch: 1150, loss is 3.8391593742370604 and perplexity is 46.48638036138904
At time: 469.524667263031 and batch: 1200, loss is 3.852982497215271 and perplexity is 47.13342912820283
At time: 470.3502149581909 and batch: 1250, loss is 3.9109422016143798 and perplexity is 49.94598900221557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369507281449589 and perplexity of 79.0046950291319
Finished 21 epochs...
Completing Train Step...
At time: 472.84854650497437 and batch: 50, loss is 3.955444784164429 and perplexity is 52.21891492632923
At time: 473.66943883895874 and batch: 100, loss is 3.952009916305542 and perplexity is 52.03985754908577
At time: 474.50173568725586 and batch: 150, loss is 3.881509141921997 and perplexity is 48.49734925540038
At time: 475.3504660129547 and batch: 200, loss is 3.933349814414978 and perplexity is 51.07779253614271
At time: 476.18117117881775 and batch: 250, loss is 3.9479496669769287 and perplexity is 51.8289911272235
At time: 477.0131821632385 and batch: 300, loss is 3.955653805732727 and perplexity is 52.229830946624055
At time: 477.83961153030396 and batch: 350, loss is 3.929854054450989 and perplexity is 50.899548564532566
At time: 478.6694531440735 and batch: 400, loss is 3.936167769432068 and perplexity is 51.22193044957828
At time: 479.49849939346313 and batch: 450, loss is 3.8606019973754884 and perplexity is 47.49393398827195
At time: 480.32688188552856 and batch: 500, loss is 3.8885455560684203 and perplexity is 48.83980008978689
At time: 481.15451884269714 and batch: 550, loss is 3.867095913887024 and perplexity is 47.8033592338562
At time: 481.9801025390625 and batch: 600, loss is 3.8925133085250856 and perplexity is 49.0339692794812
At time: 482.8075420856476 and batch: 650, loss is 3.9257471466064455 and perplexity is 50.690937475722556
At time: 483.63257217407227 and batch: 700, loss is 3.9115962171554566 and perplexity is 49.97866513942351
At time: 484.4659252166748 and batch: 750, loss is 3.9055427265167237 and perplexity is 49.677033640692926
At time: 485.2935862541199 and batch: 800, loss is 3.917475266456604 and perplexity is 50.273357582692206
At time: 486.1253433227539 and batch: 850, loss is 3.953714566230774 and perplexity is 52.12864294087202
At time: 486.9546194076538 and batch: 900, loss is 3.9049085092544558 and perplexity is 49.64553759714271
At time: 487.8408601284027 and batch: 950, loss is 3.9086900091171266 and perplexity is 49.833627597767816
At time: 488.67175364494324 and batch: 1000, loss is 3.8917811012268064 and perplexity is 48.99807939033401
At time: 489.50053095817566 and batch: 1050, loss is 3.889438118934631 and perplexity is 48.88341214209404
At time: 490.58379077911377 and batch: 1100, loss is 3.833638615608215 and perplexity is 46.23044739798646
At time: 491.41149640083313 and batch: 1150, loss is 3.8390630674362183 and perplexity is 46.481903622387364
At time: 492.24701976776123 and batch: 1200, loss is 3.8530177545547484 and perplexity is 47.13509095680999
At time: 493.08176040649414 and batch: 1250, loss is 3.9107316160202026 and perplexity is 49.935472203826755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369449810390055 and perplexity of 79.00015467607106
Finished 22 epochs...
Completing Train Step...
At time: 495.61091470718384 and batch: 50, loss is 3.952983546257019 and perplexity is 52.09054978680212
At time: 496.44340896606445 and batch: 100, loss is 3.9495279598236084 and perplexity is 51.91085704036872
At time: 497.2703137397766 and batch: 150, loss is 3.878955297470093 and perplexity is 48.373652587327534
At time: 498.1539969444275 and batch: 200, loss is 3.930773892402649 and perplexity is 50.94638944072972
At time: 498.98915672302246 and batch: 250, loss is 3.9455989599227905 and perplexity is 51.70729943896411
At time: 499.8170442581177 and batch: 300, loss is 3.9534969758987426 and perplexity is 52.117301486086376
At time: 500.64640855789185 and batch: 350, loss is 3.927788953781128 and perplexity is 50.79454433216873
At time: 501.47503900527954 and batch: 400, loss is 3.9340393495559693 and perplexity is 51.113024614502756
At time: 502.3020498752594 and batch: 450, loss is 3.8585002183914185 and perplexity is 47.394217064120454
At time: 503.1335687637329 and batch: 500, loss is 3.8866330099105837 and perplexity is 48.74648098476901
At time: 503.9681634902954 and batch: 550, loss is 3.8652884006500243 and perplexity is 47.717032071515604
At time: 504.80007457733154 and batch: 600, loss is 3.8908314657211305 and perplexity is 48.95157116086315
At time: 505.6278350353241 and batch: 650, loss is 3.924166369438171 and perplexity is 50.6108697004512
At time: 506.45773696899414 and batch: 700, loss is 3.910133638381958 and perplexity is 49.905620834208946
At time: 507.2909381389618 and batch: 750, loss is 3.9042248058319093 and perplexity is 49.61160637394187
At time: 508.1186988353729 and batch: 800, loss is 3.9162365913391115 and perplexity is 50.21112377727204
At time: 508.94806814193726 and batch: 850, loss is 3.9527940797805785 and perplexity is 52.080681308780576
At time: 509.7761824131012 and batch: 900, loss is 3.9040454864501952 and perplexity is 49.60271084895491
At time: 510.6091706752777 and batch: 950, loss is 3.9079178953170777 and perplexity is 49.795165216771466
At time: 511.43581438064575 and batch: 1000, loss is 3.8911812257766725 and perplexity is 48.96869545963452
At time: 512.2663929462433 and batch: 1050, loss is 3.889127812385559 and perplexity is 48.86824565241759
At time: 513.0977530479431 and batch: 1100, loss is 3.8332160329818725 and perplexity is 46.21091534135258
At time: 513.9285433292389 and batch: 1150, loss is 3.8387725353240967 and perplexity is 46.46840109830592
At time: 514.7545309066772 and batch: 1200, loss is 3.85284294128418 and perplexity is 47.126851837575956
At time: 515.582617521286 and batch: 1250, loss is 3.9103687620162964 and perplexity is 49.917356204730815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369416842495438 and perplexity of 78.99755025022844
Finished 23 epochs...
Completing Train Step...
At time: 518.0982639789581 and batch: 50, loss is 3.9506601428985597 and perplexity is 51.96966291734371
At time: 518.9247641563416 and batch: 100, loss is 3.947218027114868 and perplexity is 51.791084839870784
At time: 519.758270740509 and batch: 150, loss is 3.876597843170166 and perplexity is 48.25974822694726
At time: 520.5934474468231 and batch: 200, loss is 3.928406095504761 and perplexity is 50.8259014397035
At time: 521.4211344718933 and batch: 250, loss is 3.9434512948989866 and perplexity is 51.59636864421636
At time: 522.2512984275818 and batch: 300, loss is 3.951523289680481 and perplexity is 52.01453972950016
At time: 523.0772612094879 and batch: 350, loss is 3.9258796262741087 and perplexity is 50.69765343912735
At time: 523.906320810318 and batch: 400, loss is 3.9320636892318728 and perplexity is 51.01214232710755
At time: 524.7365112304688 and batch: 450, loss is 3.8565406608581543 and perplexity is 47.30143630334745
At time: 525.5660552978516 and batch: 500, loss is 3.884837899208069 and perplexity is 48.65905414894484
At time: 526.455310344696 and batch: 550, loss is 3.8635907220840453 and perplexity is 47.63609261296733
At time: 527.2850828170776 and batch: 600, loss is 3.8892416048049925 and perplexity is 48.873806804726335
At time: 528.112954378128 and batch: 650, loss is 3.922655687332153 and perplexity is 50.534470487216495
At time: 528.9399826526642 and batch: 700, loss is 3.9087225008010864 and perplexity is 49.835246802551495
At time: 529.7705118656158 and batch: 750, loss is 3.902935018539429 and perplexity is 49.54765920247074
At time: 530.5959718227386 and batch: 800, loss is 3.9150036716461183 and perplexity is 50.1492556410186
At time: 531.431976556778 and batch: 850, loss is 3.951851963996887 and perplexity is 52.031638382579075
At time: 532.2583374977112 and batch: 900, loss is 3.90315016746521 and perplexity is 49.55832047496265
At time: 533.0896835327148 and batch: 950, loss is 3.9070927047729493 and perplexity is 49.7540916663747
At time: 533.9199194908142 and batch: 1000, loss is 3.8905061006546022 and perplexity is 48.9356466204408
At time: 534.7452676296234 and batch: 1050, loss is 3.88871223449707 and perplexity is 48.847941309385426
At time: 535.5761349201202 and batch: 1100, loss is 3.832690715789795 and perplexity is 46.186646328085175
At time: 536.4027109146118 and batch: 1150, loss is 3.8383558130264284 and perplexity is 46.449040713663415
At time: 537.233648777008 and batch: 1200, loss is 3.852529287338257 and perplexity is 47.11207263243734
At time: 538.0616843700409 and batch: 1250, loss is 3.9099076890945437 and perplexity is 49.89434596856538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369397685475593 and perplexity of 78.9960369070862
Finished 24 epochs...
Completing Train Step...
At time: 540.5503370761871 and batch: 50, loss is 3.9484427070617674 and perplexity is 51.85455119795791
At time: 541.3814160823822 and batch: 100, loss is 3.945035901069641 and perplexity is 51.67819338122326
At time: 542.2076239585876 and batch: 150, loss is 3.8743851518630983 and perplexity is 48.15308235440838
At time: 543.0365478992462 and batch: 200, loss is 3.9261901664733885 and perplexity is 50.713399543301975
At time: 543.8628387451172 and batch: 250, loss is 3.9414480543136596 and perplexity is 51.49311216281035
At time: 544.6974580287933 and batch: 300, loss is 3.9496811294555663 and perplexity is 51.91880881620595
At time: 545.5341875553131 and batch: 350, loss is 3.9240836191177366 and perplexity is 50.60668180804312
At time: 546.3612101078033 and batch: 400, loss is 3.9301995706558226 and perplexity is 50.91713822196119
At time: 547.2465641498566 and batch: 450, loss is 3.8546839475631716 and perplexity is 47.213692580366946
At time: 548.07546210289 and batch: 500, loss is 3.88312716960907 and perplexity is 48.57588282685825
At time: 548.9045445919037 and batch: 550, loss is 3.861970987319946 and perplexity is 47.55899723162616
At time: 549.7455043792725 and batch: 600, loss is 3.887716760635376 and perplexity is 48.79933865596356
At time: 550.5952847003937 and batch: 650, loss is 3.9211934709548952 and perplexity is 50.460632153820214
At time: 551.4447679519653 and batch: 700, loss is 3.9073469734191892 and perplexity is 49.76674418040824
At time: 552.2738511562347 and batch: 750, loss is 3.901665754318237 and perplexity is 49.48481002594446
At time: 553.1066353321075 and batch: 800, loss is 3.913776521682739 and perplexity is 50.08775272816195
At time: 553.9330637454987 and batch: 850, loss is 3.9508953952789305 and perplexity is 51.98189034246113
At time: 554.7627949714661 and batch: 900, loss is 3.902232174873352 and perplexity is 49.51284717916945
At time: 555.5875029563904 and batch: 950, loss is 3.906231484413147 and perplexity is 49.71126087566993
At time: 556.4170320034027 and batch: 1000, loss is 3.889778380393982 and perplexity is 48.900048113376975
At time: 557.243323802948 and batch: 1050, loss is 3.888222827911377 and perplexity is 48.82404065425713
At time: 558.0697984695435 and batch: 1100, loss is 3.8320947742462157 and perplexity is 46.15912998665952
At time: 558.8963718414307 and batch: 1150, loss is 3.8378520393371582 and perplexity is 46.42564680217334
At time: 559.7330138683319 and batch: 1200, loss is 3.8521177339553834 and perplexity is 47.0926874888556
At time: 560.5600888729095 and batch: 1250, loss is 3.909378056526184 and perplexity is 49.867927294676036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3693883297217155 and perplexity of 78.99529784306486
Finished 25 epochs...
Completing Train Step...
At time: 563.0277707576752 and batch: 50, loss is 3.946310806274414 and perplexity is 51.7441201952277
At time: 563.8821504116058 and batch: 100, loss is 3.9429538583755495 and perplexity is 51.570709108500225
At time: 564.7099814414978 and batch: 150, loss is 3.872284188270569 and perplexity is 48.05202068213498
At time: 565.5383260250092 and batch: 200, loss is 3.924090890884399 and perplexity is 50.607049809362806
At time: 566.3679676055908 and batch: 250, loss is 3.939554796218872 and perplexity is 51.39571463979958
At time: 567.202244758606 and batch: 300, loss is 3.9479387474060057 and perplexity is 51.828425179968974
At time: 568.0305514335632 and batch: 350, loss is 3.9223749256134033 and perplexity is 50.52028433398396
At time: 568.9155974388123 and batch: 400, loss is 3.9284206438064575 and perplexity is 50.82664087563041
At time: 569.7437365055084 and batch: 450, loss is 3.8529060792922976 and perplexity is 47.129827427065244
At time: 570.573849439621 and batch: 500, loss is 3.88148163318634 and perplexity is 48.496015172989196
At time: 571.4041945934296 and batch: 550, loss is 3.860410060882568 and perplexity is 47.48481904392047
At time: 572.2370383739471 and batch: 600, loss is 3.886241383552551 and perplexity is 48.72739431561935
At time: 573.0641865730286 and batch: 650, loss is 3.919767727851868 and perplexity is 50.38873951794565
At time: 573.892986536026 and batch: 700, loss is 3.9059988117218016 and perplexity is 49.69969576830633
At time: 574.7297568321228 and batch: 750, loss is 3.9004171228408815 and perplexity is 49.42306029384744
At time: 575.5580635070801 and batch: 800, loss is 3.9125566387176516 and perplexity is 50.02668878485397
At time: 576.3896963596344 and batch: 850, loss is 3.9499289703369143 and perplexity is 51.93167801423231
At time: 577.2223830223083 and batch: 900, loss is 3.901297826766968 and perplexity is 49.46660654995211
At time: 578.0500380992889 and batch: 950, loss is 3.905345468521118 and perplexity is 49.66723541503027
At time: 578.8789553642273 and batch: 1000, loss is 3.889013199806213 and perplexity is 48.86264505769069
At time: 579.7099750041962 and batch: 1050, loss is 3.8876795864105222 and perplexity is 48.79752461209368
At time: 580.5373661518097 and batch: 1100, loss is 3.8314477968215943 and perplexity is 46.12927573017434
At time: 581.3632621765137 and batch: 1150, loss is 3.8372847747802736 and perplexity is 46.39931864643284
At time: 582.1904990673065 and batch: 1200, loss is 3.85163432598114 and perplexity is 47.06992800969501
At time: 583.0243127346039 and batch: 1250, loss is 3.9087982082366945 and perplexity is 49.83901984411167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369389220745894 and perplexity of 78.99536822981662
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 585.5401618480682 and batch: 50, loss is 3.948465666770935 and perplexity is 51.85574177704006
At time: 586.3677325248718 and batch: 100, loss is 3.9504848194122313 and perplexity is 51.96055221354125
At time: 587.197502374649 and batch: 150, loss is 3.8805102109909058 and perplexity is 48.44892794195357
At time: 588.0209386348724 and batch: 200, loss is 3.9333310174942016 and perplexity is 51.07683243994643
At time: 588.8495590686798 and batch: 250, loss is 3.949017395973206 and perplexity is 51.88435999811204
At time: 589.67644739151 and batch: 300, loss is 3.9569662284851073 and perplexity is 52.29842356652324
At time: 590.5373730659485 and batch: 350, loss is 3.930000514984131 and perplexity is 50.90700388549384
At time: 591.3651859760284 and batch: 400, loss is 3.9333722734451295 and perplexity is 51.07893970670747
At time: 592.1940512657166 and batch: 450, loss is 3.855133476257324 and perplexity is 47.234921261031786
At time: 593.0232517719269 and batch: 500, loss is 3.8826930713653565 and perplexity is 48.55480069762498
At time: 593.8485908508301 and batch: 550, loss is 3.861353998184204 and perplexity is 47.52966289744077
At time: 594.6755664348602 and batch: 600, loss is 3.8865955448150635 and perplexity is 48.74465472741331
At time: 595.5064287185669 and batch: 650, loss is 3.919554443359375 and perplexity is 50.377993527227545
At time: 596.3368237018585 and batch: 700, loss is 3.9057461071014403 and perplexity is 49.68713801232343
At time: 597.1624937057495 and batch: 750, loss is 3.8986078834533693 and perplexity is 49.33372298716876
At time: 597.9988367557526 and batch: 800, loss is 3.9105419015884397 and perplexity is 49.92599962266388
At time: 598.8255531787872 and batch: 850, loss is 3.9455376958847044 and perplexity is 51.70413173803602
At time: 599.6501863002777 and batch: 900, loss is 3.8980195188522337 and perplexity is 49.304705308245524
At time: 600.4790382385254 and batch: 950, loss is 3.901465344429016 and perplexity is 49.47489377433966
At time: 601.3047525882721 and batch: 1000, loss is 3.8840726470947264 and perplexity is 48.6218319489242
At time: 602.1328220367432 and batch: 1050, loss is 3.8806862449645996 and perplexity is 48.457457349971165
At time: 602.9607267379761 and batch: 1100, loss is 3.82364305973053 and perplexity is 45.77065017044195
At time: 603.7915141582489 and batch: 1150, loss is 3.8279623413085937 and perplexity is 45.96877406496331
At time: 604.6285762786865 and batch: 1200, loss is 3.8413422536849975 and perplexity is 46.58796535925353
At time: 605.4539666175842 and batch: 1250, loss is 3.9013653421401977 and perplexity is 49.46994641910073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368624722000456 and perplexity of 78.93499944877844
Finished 27 epochs...
Completing Train Step...
At time: 607.9642336368561 and batch: 50, loss is 3.9469718980789184 and perplexity is 51.77833911869874
At time: 608.7933311462402 and batch: 100, loss is 3.947032542228699 and perplexity is 51.7814792672665
At time: 609.6210334300995 and batch: 150, loss is 3.8769371509552 and perplexity is 48.27612591360562
At time: 610.4477553367615 and batch: 200, loss is 3.9278174448013305 and perplexity is 50.79599154117362
At time: 611.3151285648346 and batch: 250, loss is 3.942756667137146 and perplexity is 51.56054081908752
At time: 612.1396548748016 and batch: 300, loss is 3.951053748130798 and perplexity is 51.99012247481594
At time: 612.9648728370667 and batch: 350, loss is 3.9253944063186648 and perplexity is 50.67305989310668
At time: 613.7911643981934 and batch: 400, loss is 3.9297718954086305 and perplexity is 50.89536687815006
At time: 614.6174087524414 and batch: 450, loss is 3.852496094703674 and perplexity is 47.110508884578614
At time: 615.4462015628815 and batch: 500, loss is 3.880634317398071 and perplexity is 48.45494113746178
At time: 616.2719283103943 and batch: 550, loss is 3.8590995502471923 and perplexity is 47.42263044185714
At time: 617.1048817634583 and batch: 600, loss is 3.8850984716415407 and perplexity is 48.671735009164415
At time: 617.9337813854218 and batch: 650, loss is 3.917945556640625 and perplexity is 50.29700620970384
At time: 618.7681593894958 and batch: 700, loss is 3.9041192388534545 and perplexity is 49.60636930299642
At time: 619.5947630405426 and batch: 750, loss is 3.8974849224090575 and perplexity is 49.27835423237918
At time: 620.4222638607025 and batch: 800, loss is 3.909958076477051 and perplexity is 49.8968600773998
At time: 621.2537348270416 and batch: 850, loss is 3.945323452949524 and perplexity is 51.693055679617494
At time: 622.0884845256805 and batch: 900, loss is 3.897730059623718 and perplexity is 49.29043567162345
At time: 622.9143166542053 and batch: 950, loss is 3.9014078330993653 and perplexity is 49.472048489233224
At time: 623.7404897212982 and batch: 1000, loss is 3.8843968677520753 and perplexity is 48.63759870705641
At time: 624.5737063884735 and batch: 1050, loss is 3.8815313673019407 and perplexity is 48.498427139391985
At time: 625.4069440364838 and batch: 1100, loss is 3.8250966358184812 and perplexity is 45.837229670521424
At time: 626.2311582565308 and batch: 1150, loss is 3.8294974422454833 and perplexity is 46.039394964334114
At time: 627.0602195262909 and batch: 1200, loss is 3.842942199707031 and perplexity is 46.66256324949044
At time: 627.8867163658142 and batch: 1250, loss is 3.902787322998047 and perplexity is 49.54034177450967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368508443345118 and perplexity of 78.92582152679198
Finished 28 epochs...
Completing Train Step...
At time: 630.3705041408539 and batch: 50, loss is 3.946215896606445 and perplexity is 51.73920941100477
At time: 631.2584178447723 and batch: 100, loss is 3.9458623838424685 and perplexity is 51.72092217265646
At time: 632.0857594013214 and batch: 150, loss is 3.87561505317688 and perplexity is 48.21234232814849
At time: 632.9420208930969 and batch: 200, loss is 3.9259733963012695 and perplexity is 50.702407582361914
At time: 633.7703192234039 and batch: 250, loss is 3.940933976173401 and perplexity is 51.46664748251321
At time: 634.5983667373657 and batch: 300, loss is 3.949328451156616 and perplexity is 51.90050140753176
At time: 635.4245417118073 and batch: 350, loss is 3.9239799690246584 and perplexity is 50.60143669259639
At time: 636.256573677063 and batch: 400, loss is 3.9285360050201414 and perplexity is 50.8325046368281
At time: 637.0853006839752 and batch: 450, loss is 3.851394295692444 and perplexity is 47.05863115713356
At time: 637.9205338954926 and batch: 500, loss is 3.87968719959259 and perplexity is 48.409070325907116
At time: 638.7514359951019 and batch: 550, loss is 3.858148903846741 and perplexity is 47.377569710732125
At time: 639.5803394317627 and batch: 600, loss is 3.884310746192932 and perplexity is 48.633410141588264
At time: 640.4065749645233 and batch: 650, loss is 3.9172128677368163 and perplexity is 50.260167648609674
At time: 641.2348532676697 and batch: 700, loss is 3.9033643436431884 and perplexity is 49.56893582336581
At time: 642.0626180171967 and batch: 750, loss is 3.8969804859161377 and perplexity is 49.25350270073004
At time: 642.8890833854675 and batch: 800, loss is 3.9096340274810792 and perplexity is 49.880693669485176
At time: 643.7217350006104 and batch: 850, loss is 3.9452010345458985 and perplexity is 51.68672788558981
At time: 644.5489263534546 and batch: 900, loss is 3.89761070728302 and perplexity is 49.284553093808704
At time: 645.3786242008209 and batch: 950, loss is 3.901453762054443 and perplexity is 49.47432074090657
At time: 646.2068486213684 and batch: 1000, loss is 3.884619336128235 and perplexity is 48.64842023834069
At time: 647.0345194339752 and batch: 1050, loss is 3.8819376373291017 and perplexity is 48.51813459970732
At time: 647.8588652610779 and batch: 1100, loss is 3.8256489658355712 and perplexity is 45.86255394140238
At time: 648.6924941539764 and batch: 1150, loss is 3.830072898864746 and perplexity is 46.06589626335584
At time: 649.5196781158447 and batch: 1200, loss is 3.843518238067627 and perplexity is 46.68945041920298
At time: 650.3531062602997 and batch: 1250, loss is 3.9032130813598633 and perplexity is 49.56143847999811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368442507555885 and perplexity of 78.9206176620211
Finished 29 epochs...
Completing Train Step...
At time: 652.8358595371246 and batch: 50, loss is 3.94546320438385 and perplexity is 51.70028036311161
At time: 653.6626582145691 and batch: 100, loss is 3.945006613731384 and perplexity is 51.67667988665643
At time: 654.5188462734222 and batch: 150, loss is 3.8746792793273928 and perplexity is 48.16724758150829
At time: 655.3480970859528 and batch: 200, loss is 3.924853205680847 and perplexity is 50.64564302045604
At time: 656.1748185157776 and batch: 250, loss is 3.9398937368392946 and perplexity is 51.4131376877292
At time: 657.0005431175232 and batch: 300, loss is 3.9483407402038573 and perplexity is 51.849264021866865
At time: 657.8395545482635 and batch: 350, loss is 3.9231200313568113 and perplexity is 50.5579413154742
At time: 658.6693036556244 and batch: 400, loss is 3.9277267122268675 and perplexity is 50.79138289916883
At time: 659.4933524131775 and batch: 450, loss is 3.8505957460403444 and perplexity is 47.021067503812034
At time: 660.3192338943481 and batch: 500, loss is 3.878995113372803 and perplexity is 48.37557866631671
At time: 661.1461238861084 and batch: 550, loss is 3.857487440109253 and perplexity is 47.346241528768594
At time: 661.9720287322998 and batch: 600, loss is 3.883729863166809 and perplexity is 48.60516802261243
At time: 662.8012893199921 and batch: 650, loss is 3.9166800498962404 and perplexity is 50.23339526765064
At time: 663.6289968490601 and batch: 700, loss is 3.9028205108642577 and perplexity is 49.54198594002754
At time: 664.4603226184845 and batch: 750, loss is 3.8965902614593504 and perplexity is 49.23428652894772
At time: 665.2890059947968 and batch: 800, loss is 3.90934440612793 and perplexity is 49.8662492472961
At time: 666.1187646389008 and batch: 850, loss is 3.9450366401672365 and perplexity is 51.67823157646585
At time: 666.9432377815247 and batch: 900, loss is 3.897473106384277 and perplexity is 49.277771961564504
At time: 667.7742547988892 and batch: 950, loss is 3.901412482261658 and perplexity is 49.47227849335025
At time: 668.6057970523834 and batch: 1000, loss is 3.8846881437301635 and perplexity is 48.65176773464018
At time: 669.4316074848175 and batch: 1050, loss is 3.8821108818054197 and perplexity is 48.52654082667314
At time: 670.2573430538177 and batch: 1100, loss is 3.8258781814575196 and perplexity is 45.873067560124994
At time: 671.0922815799713 and batch: 1150, loss is 3.8303254318237303 and perplexity is 46.07753088944893
At time: 671.9220430850983 and batch: 1200, loss is 3.8437703752517702 and perplexity is 46.70122404998356
At time: 672.7479920387268 and batch: 1250, loss is 3.9033488607406617 and perplexity is 49.568168358305414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368406866588732 and perplexity of 78.91780490500425
Finished 30 epochs...
Completing Train Step...
At time: 675.2313523292542 and batch: 50, loss is 3.944751749038696 and perplexity is 51.66351100373091
At time: 676.0829381942749 and batch: 100, loss is 3.9442801904678344 and perplexity is 51.639154375556025
At time: 676.9085674285889 and batch: 150, loss is 3.8739065980911254 and perplexity is 48.130044028194185
At time: 677.7371356487274 and batch: 200, loss is 3.923977246284485 and perplexity is 50.60129891821944
At time: 678.5690574645996 and batch: 250, loss is 3.9390918159484865 and perplexity is 51.371924945443496
At time: 679.3970115184784 and batch: 300, loss is 3.9475788068771362 and perplexity is 51.809773386168864
At time: 680.2242579460144 and batch: 350, loss is 3.922441453933716 and perplexity is 50.52364547544673
At time: 681.0547556877136 and batch: 400, loss is 3.9270615863800047 and perplexity is 50.757611469974925
At time: 681.8828587532043 and batch: 450, loss is 3.84991925239563 and perplexity is 46.98926880749871
At time: 682.71098279953 and batch: 500, loss is 3.878409538269043 and perplexity is 48.34725942415054
At time: 683.5383372306824 and batch: 550, loss is 3.8569279718399048 and perplexity is 47.31976021737781
At time: 684.3642756938934 and batch: 600, loss is 3.8832364654541016 and perplexity is 48.58119225916413
At time: 685.1941487789154 and batch: 650, loss is 3.916216731071472 and perplexity is 50.21012658081768
At time: 686.0276837348938 and batch: 700, loss is 3.902351040840149 and perplexity is 49.51873292141857
At time: 686.8542478084564 and batch: 750, loss is 3.8962346506118775 and perplexity is 49.21678139528262
At time: 687.6803538799286 and batch: 800, loss is 3.9090575265884397 and perplexity is 49.85194569247265
At time: 688.5069501399994 and batch: 850, loss is 3.944840154647827 and perplexity is 51.66807854978641
At time: 689.331401348114 and batch: 900, loss is 3.897315435409546 and perplexity is 49.27000289972062
At time: 690.1580948829651 and batch: 950, loss is 3.9013138484954832 and perplexity is 49.4673990968418
At time: 690.9862282276154 and batch: 1000, loss is 3.884669065475464 and perplexity is 48.65083955267782
At time: 691.8187682628632 and batch: 1050, loss is 3.8821682119369507 and perplexity is 48.52932293939016
At time: 692.6452162265778 and batch: 1100, loss is 3.8259680557250975 and perplexity is 45.87719055374625
At time: 693.481960773468 and batch: 1150, loss is 3.8304388666152955 and perplexity is 46.08275798102263
At time: 694.3079302310944 and batch: 1200, loss is 3.8438865089416505 and perplexity is 46.7066479503971
At time: 695.1344277858734 and batch: 1250, loss is 3.9033707666397093 and perplexity is 49.56925420549063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368388600593065 and perplexity of 78.91636340588711
Finished 31 epochs...
Completing Train Step...
At time: 697.5983180999756 and batch: 50, loss is 3.944080843925476 and perplexity is 51.62886131465809
At time: 698.4539339542389 and batch: 100, loss is 3.943624954223633 and perplexity is 51.60532961280467
At time: 699.2849924564362 and batch: 150, loss is 3.8732219457626345 and perplexity is 48.09710295935641
At time: 700.1199641227722 and batch: 200, loss is 3.9232146835327146 and perplexity is 50.56272696111122
At time: 700.9483067989349 and batch: 250, loss is 3.9383978605270387 and perplexity is 51.3362874864515
At time: 701.782222032547 and batch: 300, loss is 3.946917676925659 and perplexity is 51.77553171354893
At time: 702.6173157691956 and batch: 350, loss is 3.921846127510071 and perplexity is 50.493576365632194
At time: 703.4442174434662 and batch: 400, loss is 3.926465907096863 and perplexity is 50.72738521583097
At time: 704.2728576660156 and batch: 450, loss is 3.8493077468872072 and perplexity is 46.96054339455542
At time: 705.1068291664124 and batch: 500, loss is 3.8778799676895144 and perplexity is 48.321662916135665
At time: 705.9367182254791 and batch: 550, loss is 3.856419448852539 and perplexity is 47.29570314885524
At time: 706.7660579681396 and batch: 600, loss is 3.8827866983413695 and perplexity is 48.55934694960781
At time: 707.5942311286926 and batch: 650, loss is 3.9157871627807617 and perplexity is 50.188562534512855
At time: 708.4207899570465 and batch: 700, loss is 3.901914315223694 and perplexity is 49.49711154390575
At time: 709.2535202503204 and batch: 750, loss is 3.895892777442932 and perplexity is 49.19995837409536
At time: 710.0815663337708 and batch: 800, loss is 3.90876736164093 and perplexity is 49.837482503724225
At time: 710.9115915298462 and batch: 850, loss is 3.944623336791992 and perplexity is 51.6568772021501
At time: 711.7387962341309 and batch: 900, loss is 3.897141752243042 and perplexity is 49.261446272695885
At time: 712.5707776546478 and batch: 950, loss is 3.901179609298706 and perplexity is 49.46075907860572
At time: 713.4017186164856 and batch: 1000, loss is 3.8845988273620606 and perplexity is 48.6474225294962
At time: 714.227617263794 and batch: 1050, loss is 3.8821617364883423 and perplexity is 48.52900869127092
At time: 715.0674140453339 and batch: 1100, loss is 3.825985760688782 and perplexity is 45.87800281492946
At time: 715.8967325687408 and batch: 1150, loss is 3.8304782342910766 and perplexity is 46.08457218780824
At time: 716.726396560669 and batch: 1200, loss is 3.843931646347046 and perplexity is 46.70875621488074
At time: 717.5623409748077 and batch: 1250, loss is 3.9033360719680785 and perplexity is 49.5675344463264
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368379244839188 and perplexity of 78.91562508726795
Finished 32 epochs...
Completing Train Step...
At time: 720.0909390449524 and batch: 50, loss is 3.9434421062469482 and perplexity is 51.59589454531661
At time: 720.9258379936218 and batch: 100, loss is 3.9430147886276243 and perplexity is 51.573851420535846
At time: 721.7585470676422 and batch: 150, loss is 3.872592496871948 and perplexity is 48.06683781743269
At time: 722.5892009735107 and batch: 200, loss is 3.9225173807144165 and perplexity is 50.52748171883189
At time: 723.4301190376282 and batch: 250, loss is 3.93776695728302 and perplexity is 51.30390947091192
At time: 724.2592523097992 and batch: 300, loss is 3.946314854621887 and perplexity is 51.744329673829974
At time: 725.0926783084869 and batch: 350, loss is 3.921298174858093 and perplexity is 50.465915855571815
At time: 725.9251415729523 and batch: 400, loss is 3.9259118700027464 and perplexity is 50.699288146861804
At time: 726.7559626102448 and batch: 450, loss is 3.8487370300292967 and perplexity is 46.93374986726962
At time: 727.5824995040894 and batch: 500, loss is 3.877383852005005 and perplexity is 48.29769572700219
At time: 728.4123961925507 and batch: 550, loss is 3.8559404850006103 and perplexity is 47.27305564079719
At time: 729.2426652908325 and batch: 600, loss is 3.882362084388733 and perplexity is 48.538732350295064
At time: 730.0696892738342 and batch: 650, loss is 3.915376749038696 and perplexity is 50.16796868504266
At time: 730.9064755439758 and batch: 700, loss is 3.9014955139160157 and perplexity is 49.476386429020394
At time: 731.7382175922394 and batch: 750, loss is 3.8955577659606933 and perplexity is 49.18347858372799
At time: 732.5726017951965 and batch: 800, loss is 3.9084733295440675 and perplexity is 49.822830838376795
At time: 733.4015960693359 and batch: 850, loss is 3.9443929052352904 and perplexity is 51.6449751988733
At time: 734.2315158843994 and batch: 900, loss is 3.8969560480117797 and perplexity is 49.252299063049
At time: 735.0585060119629 and batch: 950, loss is 3.90102180480957 and perplexity is 49.45295456459691
At time: 735.8834738731384 and batch: 1000, loss is 3.8844961309432984 and perplexity is 48.642426869942966
At time: 736.7142655849457 and batch: 1050, loss is 3.8821160125732423 and perplexity is 48.52678980572608
At time: 737.5439701080322 and batch: 1100, loss is 3.825959997177124 and perplexity is 45.87682085169492
At time: 738.3759543895721 and batch: 1150, loss is 3.8304713106155397 and perplexity is 46.08425311428774
At time: 739.2329556941986 and batch: 1200, loss is 3.8439327812194826 and perplexity is 46.70880922339079
At time: 740.0722868442535 and batch: 1250, loss is 3.9032682943344117 and perplexity is 49.56417498998422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368373007669936 and perplexity of 78.91513287869265
Finished 33 epochs...
Completing Train Step...
At time: 742.5397217273712 and batch: 50, loss is 3.9428282308578493 and perplexity is 51.56423081526336
At time: 743.3941373825073 and batch: 100, loss is 3.9424363374710083 and perplexity is 51.544027093320395
At time: 744.222941160202 and batch: 150, loss is 3.872000951766968 and perplexity is 48.03841252306067
At time: 745.0505232810974 and batch: 200, loss is 3.921863250732422 and perplexity is 50.494440985770126
At time: 745.8773946762085 and batch: 250, loss is 3.9371787023544313 and perplexity is 51.27373856827092
At time: 746.706689119339 and batch: 300, loss is 3.9457507610321043 and perplexity is 51.71514926016928
At time: 747.5334782600403 and batch: 350, loss is 3.9207815408706663 and perplexity is 50.43985018202041
At time: 748.3699929714203 and batch: 400, loss is 3.925386552810669 and perplexity is 50.67266193338832
At time: 749.198233127594 and batch: 450, loss is 3.8481943893432615 and perplexity is 46.90828861382423
At time: 750.028811454773 and batch: 500, loss is 3.876910099983215 and perplexity is 48.27482001513897
At time: 750.8567073345184 and batch: 550, loss is 3.8554810523986816 and perplexity is 47.25134184623752
At time: 751.6841025352478 and batch: 600, loss is 3.881953773498535 and perplexity is 48.518917502864326
At time: 752.5125386714935 and batch: 650, loss is 3.9149789142608644 and perplexity is 50.14801409194532
At time: 753.3381021022797 and batch: 700, loss is 3.901087908744812 and perplexity is 49.45622370755337
At time: 754.171660900116 and batch: 750, loss is 3.895226535797119 and perplexity is 49.16719022981755
At time: 754.9996223449707 and batch: 800, loss is 3.908176074028015 and perplexity is 49.80802292806028
At time: 755.8398950099945 and batch: 850, loss is 3.9441526460647585 and perplexity is 51.63256851043989
At time: 756.669154882431 and batch: 900, loss is 3.8967607688903807 and perplexity is 49.24268205639195
At time: 757.4970600605011 and batch: 950, loss is 3.9008469915390016 and perplexity is 49.44431028745939
At time: 758.3244915008545 and batch: 1000, loss is 3.8843714952468873 and perplexity is 48.636364664986324
At time: 759.1501574516296 and batch: 1050, loss is 3.882043890953064 and perplexity is 48.52329010122694
At time: 759.9789757728577 and batch: 1100, loss is 3.825904731750488 and perplexity is 45.874285519676604
At time: 760.8626708984375 and batch: 1150, loss is 3.8304324388504027 and perplexity is 46.0824617728407
At time: 761.6925036907196 and batch: 1200, loss is 3.843904118537903 and perplexity is 46.70747044285166
At time: 762.5209774971008 and batch: 1250, loss is 3.9031784534454346 and perplexity is 49.55972230046149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368372116645757 and perplexity of 78.9150625634325
Finished 34 epochs...
Completing Train Step...
At time: 765.0174930095673 and batch: 50, loss is 3.942234392166138 and perplexity is 51.533619070015874
At time: 765.8504822254181 and batch: 100, loss is 3.9418816900253297 and perplexity is 51.51544625722967
At time: 766.6762592792511 and batch: 150, loss is 3.871437568664551 and perplexity is 48.011356115455314
At time: 767.5050895214081 and batch: 200, loss is 3.9212407779693605 and perplexity is 50.463019352140606
At time: 768.3317074775696 and batch: 250, loss is 3.9366214418411256 and perplexity is 51.24517369817356
At time: 769.1610357761383 and batch: 300, loss is 3.945215120315552 and perplexity is 51.687455938060516
At time: 769.9900732040405 and batch: 350, loss is 3.9202877044677735 and perplexity is 50.41494729732567
At time: 770.8168413639069 and batch: 400, loss is 3.924882559776306 and perplexity is 50.647129699315784
At time: 771.6568217277527 and batch: 450, loss is 3.8476726245880126 and perplexity is 46.883819906106254
At time: 772.4938805103302 and batch: 500, loss is 3.876452827453613 and perplexity is 48.25275031239353
At time: 773.3329238891602 and batch: 550, loss is 3.8550358819961548 and perplexity is 47.23031162873041
At time: 774.2003314495087 and batch: 600, loss is 3.88155704498291 and perplexity is 48.49967248252028
At time: 775.0324747562408 and batch: 650, loss is 3.9145899629592895 and perplexity is 50.128512749375
At time: 775.8594744205475 and batch: 700, loss is 3.9006883764266966 and perplexity is 49.43646829457603
At time: 776.6875703334808 and batch: 750, loss is 3.894897937774658 and perplexity is 49.15103664250184
At time: 777.5146398544312 and batch: 800, loss is 3.9078762674331666 and perplexity is 49.793092392558435
At time: 778.3455986976624 and batch: 850, loss is 3.943905577659607 and perplexity is 51.61981330985255
At time: 779.175695180893 and batch: 900, loss is 3.8965580558776853 and perplexity is 49.23270093564481
At time: 780.0071053504944 and batch: 950, loss is 3.900659441947937 and perplexity is 49.43503789682822
At time: 780.8329100608826 and batch: 1000, loss is 3.8842307901382447 and perplexity is 48.629521761439186
At time: 781.661602973938 and batch: 1050, loss is 3.881953010559082 and perplexity is 48.51888048588205
At time: 782.5198030471802 and batch: 1100, loss is 3.825828542709351 and perplexity is 45.87079053499145
At time: 783.3454909324646 and batch: 1150, loss is 3.8303699111938476 and perplexity is 46.07958043458036
At time: 784.1712219715118 and batch: 1200, loss is 3.8438532972335815 and perplexity is 46.705096768599326
At time: 784.9996016025543 and batch: 1250, loss is 3.9030727672576906 and perplexity is 49.55448479911655
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368371225621578 and perplexity of 78.914992248235
Finished 35 epochs...
Completing Train Step...
At time: 787.4679524898529 and batch: 50, loss is 3.9416565752029418 and perplexity is 51.50385067191327
At time: 788.3220574855804 and batch: 100, loss is 3.941345520019531 and perplexity is 51.487832623573794
At time: 789.149427652359 and batch: 150, loss is 3.870895953178406 and perplexity is 47.985359462202865
At time: 789.9896950721741 and batch: 200, loss is 3.920643014907837 and perplexity is 50.43286343714292
At time: 790.8151957988739 and batch: 250, loss is 3.936088175773621 and perplexity is 51.21785367098609
At time: 791.6474199295044 and batch: 300, loss is 3.9447011137008667 and perplexity is 51.66089507062767
At time: 792.4745516777039 and batch: 350, loss is 3.919811210632324 and perplexity is 50.39093060808058
At time: 793.3012175559998 and batch: 400, loss is 3.924395327568054 and perplexity is 50.62245879718789
At time: 794.1264388561249 and batch: 450, loss is 3.847166724205017 and perplexity is 46.8601073622577
At time: 794.9508562088013 and batch: 500, loss is 3.876008176803589 and perplexity is 48.231299465021706
At time: 795.7794518470764 and batch: 550, loss is 3.8546017932891847 and perplexity is 47.20981393305668
At time: 796.6102576255798 and batch: 600, loss is 3.8811694765090943 and perplexity is 48.480879180556585
At time: 797.4392910003662 and batch: 650, loss is 3.9142080307006837 and perplexity is 50.10937070899413
At time: 798.2675824165344 and batch: 700, loss is 3.900294904708862 and perplexity is 49.41702026884745
At time: 799.100433588028 and batch: 750, loss is 3.8945712661743164 and perplexity is 49.134983016977905
At time: 799.9317643642426 and batch: 800, loss is 3.9075745010375975 and perplexity is 49.778068777467986
At time: 800.7581582069397 and batch: 850, loss is 3.9436529636383058 and perplexity is 51.60677506812421
At time: 801.5978989601135 and batch: 900, loss is 3.8963492488861085 and perplexity is 49.22242187668227
At time: 802.4279162883759 and batch: 950, loss is 3.900461893081665 and perplexity is 49.42527302568895
At time: 803.2554326057434 and batch: 1000, loss is 3.8840778398513796 and perplexity is 48.62208443092108
At time: 804.1373474597931 and batch: 1050, loss is 3.881848168373108 and perplexity is 48.51379392703855
At time: 804.9656064510345 and batch: 1100, loss is 3.8257362508773802 and perplexity is 45.866557231051736
At time: 805.7927052974701 and batch: 1150, loss is 3.8302893161773683 and perplexity is 46.075866799688114
At time: 806.6207373142242 and batch: 1200, loss is 3.843785037994385 and perplexity is 46.70190882303191
At time: 807.4498913288116 and batch: 1250, loss is 3.902955231666565 and perplexity is 49.54866072572742
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3683756807424725 and perplexity of 78.915343824849
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 809.9465367794037 and batch: 50, loss is 3.9423824071884157 and perplexity is 51.54124738432921
At time: 810.7724506855011 and batch: 100, loss is 3.943904700279236 and perplexity is 51.619768019661464
At time: 811.600061416626 and batch: 150, loss is 3.8741709756851197 and perplexity is 48.142770215618484
At time: 812.4299387931824 and batch: 200, loss is 3.924391188621521 and perplexity is 50.622249273971164
At time: 813.2627546787262 and batch: 250, loss is 3.939284019470215 and perplexity is 51.38179975929258
At time: 814.0901775360107 and batch: 300, loss is 3.946952633857727 and perplexity is 51.77734165892872
At time: 814.9150567054749 and batch: 350, loss is 3.9218293476104735 and perplexity is 50.49272909559904
At time: 815.740128993988 and batch: 400, loss is 3.9256415033340453 and perplexity is 50.68558260206475
At time: 816.5687167644501 and batch: 450, loss is 3.8477738666534425 and perplexity is 46.88856676115544
At time: 817.396910905838 and batch: 500, loss is 3.8763321304321288 and perplexity is 48.24692670060576
At time: 818.2232692241669 and batch: 550, loss is 3.8547933769226073 and perplexity is 47.218859427199824
At time: 819.0512347221375 and batch: 600, loss is 3.8809223794937133 and perplexity is 48.46890117993307
At time: 819.8834421634674 and batch: 650, loss is 3.913692579269409 and perplexity is 50.08354841778211
At time: 820.7103583812714 and batch: 700, loss is 3.8997345066070555 and perplexity is 49.38933482265214
At time: 821.5387711524963 and batch: 750, loss is 3.893676447868347 and perplexity is 49.09103580003031
At time: 822.3663971424103 and batch: 800, loss is 3.906338891983032 and perplexity is 49.716600528150934
At time: 823.1933884620667 and batch: 850, loss is 3.9408530235290526 and perplexity is 51.46248128993818
At time: 824.0204000473022 and batch: 900, loss is 3.8938641929626465 and perplexity is 49.10025326641582
At time: 824.9057786464691 and batch: 950, loss is 3.897658095359802 and perplexity is 49.28688864933319
At time: 825.7301836013794 and batch: 1000, loss is 3.8812657499313357 and perplexity is 48.48554682539003
At time: 826.5578677654266 and batch: 1050, loss is 3.8780806922912596 and perplexity is 48.33136323619404
At time: 827.3934421539307 and batch: 1100, loss is 3.8219898986816405 and perplexity is 45.69504642420043
At time: 828.2242424488068 and batch: 1150, loss is 3.8268611907958983 and perplexity is 45.91818338492224
At time: 829.0520639419556 and batch: 1200, loss is 3.8399129152297973 and perplexity is 46.52142295595288
At time: 829.8768441677094 and batch: 1250, loss is 3.9000363826751707 and perplexity is 49.404246531486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368437161410812 and perplexity of 78.92019574207762
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 832.3377051353455 and batch: 50, loss is 3.9419204235076903 and perplexity is 51.51744166850293
At time: 833.1973268985748 and batch: 100, loss is 3.9434643507003786 and perplexity is 51.59704228055535
At time: 834.0252256393433 and batch: 150, loss is 3.874104323387146 and perplexity is 48.13956149628824
At time: 834.8548314571381 and batch: 200, loss is 3.924113655090332 and perplexity is 50.60820185177893
At time: 835.6886515617371 and batch: 250, loss is 3.938753113746643 and perplexity is 51.35452810769147
At time: 836.5155789852142 and batch: 300, loss is 3.9462631511688233 and perplexity is 51.74165438247086
At time: 837.3455526828766 and batch: 350, loss is 3.9214479064941408 and perplexity is 50.47347276545764
At time: 838.175607919693 and batch: 400, loss is 3.925254611968994 and perplexity is 50.665976580768145
At time: 839.0012743473053 and batch: 450, loss is 3.8475446462631226 and perplexity is 46.87782017729604
At time: 839.8288226127625 and batch: 500, loss is 3.8761638498306272 and perplexity is 48.23880836185861
At time: 840.6558570861816 and batch: 550, loss is 3.854416160583496 and perplexity is 47.201051060924215
At time: 841.4855105876923 and batch: 600, loss is 3.8806558561325075 and perplexity is 48.45598480681069
At time: 842.3118808269501 and batch: 650, loss is 3.913458547592163 and perplexity is 50.07182865239513
At time: 843.144118309021 and batch: 700, loss is 3.8995530223846435 and perplexity is 49.38037225093368
At time: 843.9722709655762 and batch: 750, loss is 3.893341999053955 and perplexity is 49.074620106567345
At time: 844.7979300022125 and batch: 800, loss is 3.905983862876892 and perplexity is 49.69895282081535
At time: 845.6260211467743 and batch: 850, loss is 3.940239596366882 and perplexity is 51.43092248658545
At time: 846.4914839267731 and batch: 900, loss is 3.8931222677230837 and perplexity is 49.06383805959946
At time: 847.3268978595734 and batch: 950, loss is 3.896795992851257 and perplexity is 49.24441660924689
At time: 848.1589238643646 and batch: 1000, loss is 3.8804437255859376 and perplexity is 48.44570690243632
At time: 848.9977164268494 and batch: 1050, loss is 3.877063536643982 and perplexity is 48.2822277106127
At time: 849.8269209861755 and batch: 1100, loss is 3.8210345554351806 and perplexity is 45.65141281605745
At time: 850.6606643199921 and batch: 1150, loss is 3.826059045791626 and perplexity is 45.88136511228568
At time: 851.4876523017883 and batch: 1200, loss is 3.838930220603943 and perplexity is 46.47572905887769
At time: 852.3178119659424 and batch: 1250, loss is 3.8992403030395506 and perplexity is 49.36493246754823
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368311081489507 and perplexity of 78.91024611724623
Finished 38 epochs...
Completing Train Step...
At time: 854.793112039566 and batch: 50, loss is 3.941652760505676 and perplexity is 51.50365420068968
At time: 855.6477477550507 and batch: 100, loss is 3.9431092596054076 and perplexity is 51.57872388285711
At time: 856.4726514816284 and batch: 150, loss is 3.873705072402954 and perplexity is 48.120345565227225
At time: 857.3138620853424 and batch: 200, loss is 3.9236120653152464 and perplexity is 50.58282366044733
At time: 858.1395938396454 and batch: 250, loss is 3.9382719802856445 and perplexity is 51.329825668906544
At time: 858.9658789634705 and batch: 300, loss is 3.9458625078201295 and perplexity is 51.720928584895816
At time: 859.7913243770599 and batch: 350, loss is 3.9210875749588014 and perplexity is 50.45528885783658
At time: 860.6214175224304 and batch: 400, loss is 3.9249848127365112 and perplexity is 50.65230878303725
At time: 861.448873758316 and batch: 450, loss is 3.847324557304382 and perplexity is 46.867504021942786
At time: 862.2787399291992 and batch: 500, loss is 3.875964322090149 and perplexity is 48.229184341584244
At time: 863.1053469181061 and batch: 550, loss is 3.8542105197906493 and perplexity is 47.19134559731467
At time: 863.9327516555786 and batch: 600, loss is 3.88055184841156 and perplexity is 48.45094527234447
At time: 864.7675669193268 and batch: 650, loss is 3.9133535480499266 and perplexity is 50.06657140931658
At time: 865.5948956012726 and batch: 700, loss is 3.8994072914123534 and perplexity is 49.37317652560627
At time: 866.4230673313141 and batch: 750, loss is 3.8932243680953977 and perplexity is 49.06884775147386
At time: 867.2501721382141 and batch: 800, loss is 3.905958924293518 and perplexity is 49.697713414791416
At time: 868.1073846817017 and batch: 850, loss is 3.9402571725845337 and perplexity is 51.43182645561725
At time: 868.9351243972778 and batch: 900, loss is 3.8931344413757323 and perplexity is 49.064435349357204
At time: 869.7659909725189 and batch: 950, loss is 3.896836585998535 and perplexity is 49.24641563567603
At time: 870.5939173698425 and batch: 1000, loss is 3.8805277013778685 and perplexity is 48.44977533986184
At time: 871.4202973842621 and batch: 1050, loss is 3.877206416130066 and perplexity is 48.28912674334839
At time: 872.2564060688019 and batch: 1100, loss is 3.8212644386291506 and perplexity is 45.661908514991026
At time: 873.0838730335236 and batch: 1150, loss is 3.826338138580322 and perplexity is 45.89417205750395
At time: 873.913580417633 and batch: 1200, loss is 3.839152374267578 and perplexity is 46.48605495928407
At time: 874.7435615062714 and batch: 1250, loss is 3.8994119501113893 and perplexity is 49.37340654091193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368291033445484 and perplexity of 78.90866413701605
Finished 39 epochs...
Completing Train Step...
At time: 877.2555298805237 and batch: 50, loss is 3.9414469814300537 and perplexity is 51.49305691672413
At time: 878.0823304653168 and batch: 100, loss is 3.9428375577926635 and perplexity is 51.56471175372575
At time: 878.9217958450317 and batch: 150, loss is 3.87344247341156 and perplexity is 48.107710870017975
At time: 879.7470226287842 and batch: 200, loss is 3.9232240962982177 and perplexity is 50.56320289844324
At time: 880.578156709671 and batch: 250, loss is 3.9378778600692748 and perplexity is 51.309599532934044
At time: 881.4062597751617 and batch: 300, loss is 3.94550950050354 and perplexity is 51.70267394088555
At time: 882.2313854694366 and batch: 350, loss is 3.9207992124557496 and perplexity is 50.44074154200034
At time: 883.056788444519 and batch: 400, loss is 3.924756951332092 and perplexity is 50.6407683916757
At time: 883.886397600174 and batch: 450, loss is 3.847151961326599 and perplexity is 46.859415577296446
At time: 884.7144501209259 and batch: 500, loss is 3.8758087062835695 and perplexity is 48.22167970209767
At time: 885.5404005050659 and batch: 550, loss is 3.8540394353866576 and perplexity is 47.183272584682555
At time: 886.3731491565704 and batch: 600, loss is 3.8804689693450927 and perplexity is 48.44692986962954
At time: 887.2009394168854 and batch: 650, loss is 3.913291392326355 and perplexity is 50.06345958205383
At time: 888.0336186885834 and batch: 700, loss is 3.8993088245391845 and perplexity is 49.36831514264189
At time: 888.8610324859619 and batch: 750, loss is 3.893147053718567 and perplexity is 49.065054170739195
At time: 889.7225625514984 and batch: 800, loss is 3.905940866470337 and perplexity is 49.69681599037285
At time: 890.5486183166504 and batch: 850, loss is 3.9402800464630126 and perplexity is 51.43300291442058
At time: 891.3753712177277 and batch: 900, loss is 3.8931532955169676 and perplexity is 49.065360425871646
At time: 892.2016913890839 and batch: 950, loss is 3.8968865633010865 and perplexity is 49.248876900193004
At time: 893.0300436019897 and batch: 1000, loss is 3.880601568222046 and perplexity is 48.453354304049064
At time: 893.8565080165863 and batch: 1050, loss is 3.87733274936676 and perplexity is 48.29522765039256
At time: 894.6838636398315 and batch: 1100, loss is 3.821459341049194 and perplexity is 45.670808998799146
At time: 895.5170266628265 and batch: 1150, loss is 3.8265791082382203 and perplexity is 45.90523249300533
At time: 896.3444225788116 and batch: 1200, loss is 3.8393439865112304 and perplexity is 46.49496311000148
At time: 897.1696832180023 and batch: 1250, loss is 3.8995588207244873 and perplexity is 49.38065857594371
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368287914860858 and perplexity of 78.90841805405293
Finished 40 epochs...
Completing Train Step...
At time: 899.6463408470154 and batch: 50, loss is 3.9412738418579103 and perplexity is 51.48414220264837
At time: 900.5032916069031 and batch: 100, loss is 3.9426091480255128 and perplexity is 51.552935214910335
At time: 901.3323519229889 and batch: 150, loss is 3.873224682807922 and perplexity is 48.097234603485575
At time: 902.1626205444336 and batch: 200, loss is 3.922893319129944 and perplexity is 50.546480511214064
At time: 902.9873983860016 and batch: 250, loss is 3.9375389051437377 and perplexity is 51.2922108386033
At time: 903.8207399845123 and batch: 300, loss is 3.9452056789398195 and perplexity is 51.68696793967205
At time: 904.648788690567 and batch: 350, loss is 3.9205512237548827 and perplexity is 50.428234358918715
At time: 905.4779114723206 and batch: 400, loss is 3.924559416770935 and perplexity is 50.630766077648666
At time: 906.30446600914 and batch: 450, loss is 3.8470045852661134 and perplexity is 46.85251013009331
At time: 907.1330673694611 and batch: 500, loss is 3.875678791999817 and perplexity is 48.21541542403623
At time: 907.9612777233124 and batch: 550, loss is 3.853894305229187 and perplexity is 47.176425365783366
At time: 908.7909848690033 and batch: 600, loss is 3.8803984928131103 and perplexity is 48.44351561834085
At time: 909.6174991130829 and batch: 650, loss is 3.913241672515869 and perplexity is 50.06097049821003
At time: 910.4892067909241 and batch: 700, loss is 3.8992315912246704 and perplexity is 49.36450241126829
At time: 911.3185238838196 and batch: 750, loss is 3.893089141845703 and perplexity is 49.06221280383524
At time: 912.1499092578888 and batch: 800, loss is 3.905930461883545 and perplexity is 49.69629891822755
At time: 912.9810688495636 and batch: 850, loss is 3.9403062295913696 and perplexity is 51.434349608967935
At time: 913.8115155696869 and batch: 900, loss is 3.893174753189087 and perplexity is 49.066413265583776
At time: 914.6379930973053 and batch: 950, loss is 3.8969373321533203 and perplexity is 49.251377272617006
At time: 915.4628319740295 and batch: 1000, loss is 3.8806692457199095 and perplexity is 48.45663361679805
At time: 916.2893948554993 and batch: 1050, loss is 3.877443804740906 and perplexity is 48.300591392799426
At time: 917.1219651699066 and batch: 1100, loss is 3.8216322231292725 and perplexity is 45.678705345806506
At time: 917.9501640796661 and batch: 1150, loss is 3.826790566444397 and perplexity is 45.9149405575112
At time: 918.7828094959259 and batch: 1200, loss is 3.839510192871094 and perplexity is 46.50269151080887
At time: 919.614417552948 and batch: 1250, loss is 3.8996862602233886 and perplexity is 49.38695202333642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368292369981752 and perplexity of 78.90876960137804
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 922.1857552528381 and batch: 50, loss is 3.9412161684036255 and perplexity is 51.48117301994899
At time: 923.0353260040283 and batch: 100, loss is 3.9426148319244385 and perplexity is 51.55322823741617
At time: 923.8865697383881 and batch: 150, loss is 3.8733095359802245 and perplexity is 48.10131597957706
At time: 924.7457547187805 and batch: 200, loss is 3.922927489280701 and perplexity is 50.54820772158271
At time: 925.5741617679596 and batch: 250, loss is 3.9375037956237793 and perplexity is 51.290410025316184
At time: 926.4187037944794 and batch: 300, loss is 3.9451110315322877 and perplexity is 51.68207613365541
At time: 927.2463409900665 and batch: 350, loss is 3.9205312633514406 and perplexity is 50.427227801061726
At time: 928.0781712532043 and batch: 400, loss is 3.9245058155059813 and perplexity is 50.62805227727355
At time: 928.9082326889038 and batch: 450, loss is 3.8469844961166384 and perplexity is 46.85156891246819
At time: 929.7350885868073 and batch: 500, loss is 3.8756915807723997 and perplexity is 48.21603204396197
At time: 930.5683135986328 and batch: 550, loss is 3.8538219690322877 and perplexity is 47.17301292601204
At time: 931.3992409706116 and batch: 600, loss is 3.880308861732483 and perplexity is 48.43917376827185
At time: 932.2613253593445 and batch: 650, loss is 3.9131667709350584 and perplexity is 50.0572209928065
At time: 933.0911254882812 and batch: 700, loss is 3.899193596839905 and perplexity is 49.3626268730001
At time: 933.9211587905884 and batch: 750, loss is 3.8929950046539306 and perplexity is 49.05759444228295
At time: 934.7460691928864 and batch: 800, loss is 3.9057902097702026 and perplexity is 49.68932939603551
At time: 935.5774192810059 and batch: 850, loss is 3.9401293706893923 and perplexity is 51.4252537907341
At time: 936.4107897281647 and batch: 900, loss is 3.8929691982269286 and perplexity is 49.056328457388425
At time: 937.243597984314 and batch: 950, loss is 3.896700282096863 and perplexity is 49.23970361452925
At time: 938.075433254242 and batch: 1000, loss is 3.8804037141799927 and perplexity is 48.443768560369314
At time: 938.9040913581848 and batch: 1050, loss is 3.8771697807312013 and perplexity is 48.2873576843346
At time: 939.7327170372009 and batch: 1100, loss is 3.82129017829895 and perplexity is 45.66308385256495
At time: 940.5595650672913 and batch: 1150, loss is 3.826492762565613 and perplexity is 45.901268945949546
At time: 941.3909068107605 and batch: 1200, loss is 3.8392209720611574 and perplexity is 46.4892439094627
At time: 942.2167901992798 and batch: 1250, loss is 3.899470205307007 and perplexity is 49.37628288214838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368284350764142 and perplexity of 78.90813681732048
Finished 42 epochs...
Completing Train Step...
At time: 944.7206575870514 and batch: 50, loss is 3.9411755418777465 and perplexity is 51.479081561225655
At time: 945.5777378082275 and batch: 100, loss is 3.9425535297393797 and perplexity is 51.550068008743835
At time: 946.4056408405304 and batch: 150, loss is 3.8732335710525514 and perplexity is 48.09766210537259
At time: 947.2408111095428 and batch: 200, loss is 3.922833342552185 and perplexity is 50.54344899720607
At time: 948.0683333873749 and batch: 250, loss is 3.9374153232574463 and perplexity is 51.285872442099375
At time: 948.8957855701447 and batch: 300, loss is 3.9450416660308836 and perplexity is 51.678491304863954
At time: 949.7209401130676 and batch: 350, loss is 3.920461835861206 and perplexity is 50.42372688672726
At time: 950.5517358779907 and batch: 400, loss is 3.9244574069976808 and perplexity is 50.62560150810416
At time: 951.3783774375916 and batch: 450, loss is 3.846939196586609 and perplexity is 46.84944660648541
At time: 952.2159411907196 and batch: 500, loss is 3.875652747154236 and perplexity is 48.21415967733982
At time: 953.0539991855621 and batch: 550, loss is 3.8537871742248537 and perplexity is 47.171371578666545
At time: 953.911566734314 and batch: 600, loss is 3.880290207862854 and perplexity is 48.438270198667006
At time: 954.7443995475769 and batch: 650, loss is 3.913139181137085 and perplexity is 50.055839943243726
At time: 955.5747177600861 and batch: 700, loss is 3.899161558151245 and perplexity is 49.361045384500834
At time: 956.4048929214478 and batch: 750, loss is 3.8929727268218994 and perplexity is 49.0565015576077
At time: 957.2345254421234 and batch: 800, loss is 3.905792541503906 and perplexity is 49.68944525845465
At time: 958.0615928173065 and batch: 850, loss is 3.9401368999481203 and perplexity is 51.425640986232686
At time: 958.8874833583832 and batch: 900, loss is 3.8929736948013307 and perplexity is 49.056549043315165
At time: 959.7231514453888 and batch: 950, loss is 3.8967075729370118 and perplexity is 49.240062614645986
At time: 960.5497500896454 and batch: 1000, loss is 3.880425443649292 and perplexity is 48.44482122918792
At time: 961.379759311676 and batch: 1050, loss is 3.8771983098983767 and perplexity is 48.288735302085485
At time: 962.2062029838562 and batch: 1100, loss is 3.8213482332229614 and perplexity is 45.665734896380464
At time: 963.0395791530609 and batch: 1150, loss is 3.826552891731262 and perplexity is 45.90402903393357
At time: 963.8865401744843 and batch: 1200, loss is 3.83926504611969 and perplexity is 46.49129292427377
At time: 964.7299077510834 and batch: 1250, loss is 3.8995095109939575 and perplexity is 49.37822368900825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368279450131158 and perplexity of 78.90775011845004
Finished 43 epochs...
Completing Train Step...
At time: 967.1666760444641 and batch: 50, loss is 3.941136131286621 and perplexity is 51.47705278016871
At time: 968.0523526668549 and batch: 100, loss is 3.942496962547302 and perplexity is 51.547152048619786
At time: 968.8801159858704 and batch: 150, loss is 3.8731705379486083 and perplexity is 48.09463045598582
At time: 969.7074222564697 and batch: 200, loss is 3.9227484369277956 and perplexity is 50.539157756287985
At time: 970.536406993866 and batch: 250, loss is 3.937332730293274 and perplexity is 51.281636764795195
At time: 971.3674960136414 and batch: 300, loss is 3.944972424507141 and perplexity is 51.6749131312618
At time: 972.1965386867523 and batch: 350, loss is 3.9203989791870115 and perplexity is 50.42055751856368
At time: 973.022055387497 and batch: 400, loss is 3.924411072731018 and perplexity is 50.62325586232624
At time: 973.8559346199036 and batch: 450, loss is 3.8468995761871336 and perplexity is 46.84759044946674
At time: 974.6854484081268 and batch: 500, loss is 3.8756183433532714 and perplexity is 48.21250095551994
At time: 975.5597620010376 and batch: 550, loss is 3.853753757476807 and perplexity is 47.169795291164824
At time: 976.3878788948059 and batch: 600, loss is 3.8802729177474977 and perplexity is 48.43743270262783
At time: 977.2202913761139 and batch: 650, loss is 3.913119692802429 and perplexity is 50.05486444778886
At time: 978.0477876663208 and batch: 700, loss is 3.8991373920440675 and perplexity is 49.359852534600996
At time: 978.8811836242676 and batch: 750, loss is 3.8929559087753294 and perplexity is 49.05567653001764
At time: 979.7151396274567 and batch: 800, loss is 3.905793375968933 and perplexity is 49.68948672257622
At time: 980.5508830547333 and batch: 850, loss is 3.940144348144531 and perplexity is 51.42602401593374
At time: 981.3880298137665 and batch: 900, loss is 3.892979245185852 and perplexity is 49.05682132678128
At time: 982.222972869873 and batch: 950, loss is 3.8967171335220336 and perplexity is 49.24053338070149
At time: 983.0546741485596 and batch: 1000, loss is 3.880445408821106 and perplexity is 48.445788448022554
At time: 983.879983663559 and batch: 1050, loss is 3.8772265768051146 and perplexity is 48.29010029455473
At time: 984.7076299190521 and batch: 1100, loss is 3.8213988828659056 and perplexity is 45.66804790812387
At time: 985.5331063270569 and batch: 1150, loss is 3.8266082191467286 and perplexity is 45.90656885547975
At time: 986.3639581203461 and batch: 1200, loss is 3.839306926727295 and perplexity is 46.493240048642875
At time: 987.191264629364 and batch: 1250, loss is 3.899544768333435 and perplexity is 49.379964664494565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368276331546533 and perplexity of 78.90750403833742
Finished 44 epochs...
Completing Train Step...
At time: 989.7334597110748 and batch: 50, loss is 3.941097660064697 and perplexity is 51.47507243314066
At time: 990.5655026435852 and batch: 100, loss is 3.9424433374404906 and perplexity is 51.54438790119986
At time: 991.3944718837738 and batch: 150, loss is 3.8731144523620604 and perplexity is 48.091933116068546
At time: 992.2264096736908 and batch: 200, loss is 3.9226693630218508 and perplexity is 50.535161585679525
At time: 993.0533859729767 and batch: 250, loss is 3.937254033088684 and perplexity is 51.277601202130846
At time: 993.8816282749176 and batch: 300, loss is 3.9449041986465456 and perplexity is 51.67138768610687
At time: 994.7143199443817 and batch: 350, loss is 3.9203401947021486 and perplexity is 50.41759365917878
At time: 995.5508849620819 and batch: 400, loss is 3.9243663597106933 and perplexity is 50.62099239426158
At time: 996.4204125404358 and batch: 450, loss is 3.8468633222579958 and perplexity is 46.845892071028935
At time: 997.2474315166473 and batch: 500, loss is 3.8755868339538573 and perplexity is 48.21098183250403
At time: 998.0789039134979 and batch: 550, loss is 3.853721232414246 and perplexity is 47.1682611155717
At time: 998.9095895290375 and batch: 600, loss is 3.8802562665939333 and perplexity is 48.4366261702125
At time: 999.7386009693146 and batch: 650, loss is 3.91310435295105 and perplexity is 50.05409661949661
At time: 1000.5679180622101 and batch: 700, loss is 3.8991169691085816 and perplexity is 49.358844471810926
At time: 1001.3959364891052 and batch: 750, loss is 3.892941813468933 and perplexity is 49.054985080099584
At time: 1002.2287874221802 and batch: 800, loss is 3.9057929849624635 and perplexity is 49.689467293669246
At time: 1003.0657181739807 and batch: 850, loss is 3.9401510667800905 and perplexity is 51.42636952980806
At time: 1003.8983380794525 and batch: 900, loss is 3.892985019683838 and perplexity is 49.05710460611513
At time: 1004.7259387969971 and batch: 950, loss is 3.8967278718948366 and perplexity is 49.24106214674499
At time: 1005.557291507721 and batch: 1000, loss is 3.8804635572433472 and perplexity is 48.44666767062536
At time: 1006.4010539054871 and batch: 1050, loss is 3.877254133224487 and perplexity is 48.29143101514485
At time: 1007.2534425258636 and batch: 1100, loss is 3.821444511413574 and perplexity is 45.67013172236512
At time: 1008.1135792732239 and batch: 1150, loss is 3.8266602849960325 and perplexity is 45.90895908219989
At time: 1008.9404139518738 and batch: 1200, loss is 3.839346857070923 and perplexity is 46.49509657676005
At time: 1009.7700221538544 and batch: 1250, loss is 3.8995770168304444 and perplexity is 49.38155711981437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368274995010265 and perplexity of 78.9073985756669
Finished 45 epochs...
Completing Train Step...
At time: 1012.2357518672943 and batch: 50, loss is 3.9410598421096803 and perplexity is 51.473125787976194
At time: 1013.0909955501556 and batch: 100, loss is 3.9423922300338745 and perplexity is 51.5417536685236
At time: 1013.9175300598145 and batch: 150, loss is 3.87306303024292 and perplexity is 48.08946019053623
At time: 1014.7455203533173 and batch: 200, loss is 3.922594361305237 and perplexity is 50.531371503944335
At time: 1015.5749826431274 and batch: 250, loss is 3.937178707122803 and perplexity is 51.273738812763156
At time: 1016.4027230739594 and batch: 300, loss is 3.94483763217926 and perplexity is 51.66794821884673
At time: 1017.2327103614807 and batch: 350, loss is 3.9202842855453492 and perplexity is 50.41477493282648
At time: 1018.1002492904663 and batch: 400, loss is 3.92432297706604 and perplexity is 50.61879636937158
At time: 1018.9315247535706 and batch: 450, loss is 3.846829304695129 and perplexity is 46.844298515054966
At time: 1019.7589337825775 and batch: 500, loss is 3.8755573844909668 and perplexity is 48.20956206588943
At time: 1020.585866689682 and batch: 550, loss is 3.8536897325515747 and perplexity is 47.16677534522503
At time: 1021.414665222168 and batch: 600, loss is 3.8802401638031006 and perplexity is 48.43584621163241
At time: 1022.2434887886047 and batch: 650, loss is 3.913091049194336 and perplexity is 50.05343071640216
At time: 1023.071852684021 and batch: 700, loss is 3.8990989351272582 and perplexity is 49.35795434335788
At time: 1023.8988399505615 and batch: 750, loss is 3.8929293394088744 and perplexity is 49.054373169086034
At time: 1024.7330911159515 and batch: 800, loss is 3.9057918643951415 and perplexity is 49.68941161330714
At time: 1025.5614502429962 and batch: 850, loss is 3.9401575469970704 and perplexity is 51.42670278492089
At time: 1026.3898544311523 and batch: 900, loss is 3.892990593910217 and perplexity is 49.05737806228387
At time: 1027.2158498764038 and batch: 950, loss is 3.89673912525177 and perplexity is 49.241616277111014
At time: 1028.0463120937347 and batch: 1000, loss is 3.880480794906616 and perplexity is 48.44750278516685
At time: 1028.8750867843628 and batch: 1050, loss is 3.877280673980713 and perplexity is 48.29271272325189
At time: 1029.701758146286 and batch: 1100, loss is 3.8214872026443483 and perplexity is 45.672081478116404
At time: 1030.5288434028625 and batch: 1150, loss is 3.8267099189758302 and perplexity is 45.91123778309754
At time: 1031.3643057346344 and batch: 1200, loss is 3.83938524723053 and perplexity is 46.49688156520134
At time: 1032.192626953125 and batch: 1250, loss is 3.8996073246002196 and perplexity is 49.38305378735891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368275440522354 and perplexity of 78.90743372987475
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1034.7042269706726 and batch: 50, loss is 3.941046996116638 and perplexity is 51.47246456880748
At time: 1035.5339341163635 and batch: 100, loss is 3.9423953914642333 and perplexity is 51.54191661444596
At time: 1036.3613543510437 and batch: 150, loss is 3.873083863258362 and perplexity is 48.09046204943882
At time: 1037.1969993114471 and batch: 200, loss is 3.9226028966903685 and perplexity is 50.53180281050203
At time: 1038.0275225639343 and batch: 250, loss is 3.9371711301803587 and perplexity is 51.27335031606709
At time: 1038.854507446289 and batch: 300, loss is 3.944816393852234 and perplexity is 51.66685088971844
At time: 1039.7111766338348 and batch: 350, loss is 3.920278687477112 and perplexity is 50.414492708266195
At time: 1040.5406863689423 and batch: 400, loss is 3.9243103075027466 and perplexity is 50.61815505538972
At time: 1041.369785785675 and batch: 450, loss is 3.84682409286499 and perplexity is 46.84405437116435
At time: 1042.1986391544342 and batch: 500, loss is 3.8755631256103515 and perplexity is 48.20983884353524
At time: 1043.027266740799 and batch: 550, loss is 3.85367431640625 and perplexity is 47.16604822096655
At time: 1043.8580076694489 and batch: 600, loss is 3.8802156925201414 and perplexity is 48.43466093883703
At time: 1044.6880869865417 and batch: 650, loss is 3.913064546585083 and perplexity is 50.05210418746443
At time: 1045.5163569450378 and batch: 700, loss is 3.899080920219421 and perplexity is 49.35706517236856
At time: 1046.3432743549347 and batch: 750, loss is 3.892896361351013 and perplexity is 49.052755477803615
At time: 1047.1734223365784 and batch: 800, loss is 3.905749740600586 and perplexity is 49.68731855082493
At time: 1048.0099091529846 and batch: 850, loss is 3.9401079034805297 and perplexity is 51.424149845919516
At time: 1048.8423883914948 and batch: 900, loss is 3.8929335927963256 and perplexity is 49.054581816785024
At time: 1049.6705386638641 and batch: 950, loss is 3.8966777420043943 and perplexity is 49.238593759564814
At time: 1050.4971220493317 and batch: 1000, loss is 3.8804095125198366 and perplexity is 48.4440494546171
At time: 1051.32368350029 and batch: 1050, loss is 3.87720751285553 and perplexity is 48.289179703292355
At time: 1052.15243268013 and batch: 1100, loss is 3.82139009475708 and perplexity is 45.66764657411248
At time: 1052.9865682125092 and batch: 1150, loss is 3.8266250896453857 and perplexity is 45.907343328720835
At time: 1053.812742471695 and batch: 1200, loss is 3.8393067646026613 and perplexity is 46.49323251094397
At time: 1054.6411499977112 and batch: 1250, loss is 3.899548134803772 and perplexity is 49.38013090096065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368276331546533 and perplexity of 78.90750403833742
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1057.1448209285736 and batch: 50, loss is 3.9410441732406616 and perplexity is 51.47231926862888
At time: 1058.0045704841614 and batch: 100, loss is 3.9423964166641237 and perplexity is 51.541969455240306
At time: 1058.8329882621765 and batch: 150, loss is 3.8730882453918456 and perplexity is 48.090672788724554
At time: 1059.6620185375214 and batch: 200, loss is 3.9226049089431765 and perplexity is 50.53190449336643
At time: 1060.501221895218 and batch: 250, loss is 3.9371694850921632 and perplexity is 51.27326596695312
At time: 1061.3623378276825 and batch: 300, loss is 3.944811668395996 and perplexity is 51.666606740852465
At time: 1062.1947448253632 and batch: 350, loss is 3.9202775144577027 and perplexity is 50.41443357112243
At time: 1063.0214748382568 and batch: 400, loss is 3.924307827949524 and perplexity is 50.61802954513583
At time: 1063.8495614528656 and batch: 450, loss is 3.8468227100372316 and perplexity is 46.843989593950425
At time: 1064.6791501045227 and batch: 500, loss is 3.875564694404602 and perplexity is 48.20991447491256
At time: 1065.5103869438171 and batch: 550, loss is 3.853671226501465 and perplexity is 47.16590248259362
At time: 1066.3371527194977 and batch: 600, loss is 3.8802100896835325 and perplexity is 48.43438956810581
At time: 1067.1719341278076 and batch: 650, loss is 3.9130570983886717 and perplexity is 50.05173139094998
At time: 1067.999747991562 and batch: 700, loss is 3.8990756368637083 and perplexity is 49.35680440212517
At time: 1068.8259358406067 and batch: 750, loss is 3.8928874254226686 and perplexity is 49.05231714785401
At time: 1069.6549882888794 and batch: 800, loss is 3.905739064216614 and perplexity is 49.686788072765346
At time: 1070.484162569046 and batch: 850, loss is 3.940095191001892 and perplexity is 51.423496121668364
At time: 1071.3119509220123 and batch: 900, loss is 3.8929187726974486 and perplexity is 49.05385482841916
At time: 1072.138640165329 and batch: 950, loss is 3.896661410331726 and perplexity is 49.23778961753539
At time: 1072.9661145210266 and batch: 1000, loss is 3.8803908252716064 and perplexity is 48.44314417709827
At time: 1073.7983992099762 and batch: 1050, loss is 3.877187924385071 and perplexity is 48.28823380138666
At time: 1074.6240208148956 and batch: 1100, loss is 3.821364302635193 and perplexity is 45.66646872379544
At time: 1075.4503545761108 and batch: 1150, loss is 3.82660231590271 and perplexity is 45.90629785860162
At time: 1076.2748448848724 and batch: 1200, loss is 3.8392863035202027 and perplexity is 46.49228121881207
At time: 1077.103091955185 and batch: 1250, loss is 3.8995326280593874 and perplexity is 49.37936518183001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368276331546533 and perplexity of 78.90750403833742
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1079.603008031845 and batch: 50, loss is 3.941043567657471 and perplexity is 51.47228809786698
At time: 1080.4583113193512 and batch: 100, loss is 3.9423967218399047 and perplexity is 51.5419851846035
At time: 1081.3107368946075 and batch: 150, loss is 3.873089656829834 and perplexity is 48.09074066577491
At time: 1082.1710000038147 and batch: 200, loss is 3.922605767250061 and perplexity is 50.53194786526657
At time: 1082.9979557991028 and batch: 250, loss is 3.9371692752838134 and perplexity is 51.273255209394925
At time: 1083.8272438049316 and batch: 300, loss is 3.9448107194900515 and perplexity is 51.66655771412546
At time: 1084.653865814209 and batch: 350, loss is 3.920277237892151 and perplexity is 50.414419628228714
At time: 1085.4830617904663 and batch: 400, loss is 3.9243076515197752 and perplexity is 50.618020614610394
At time: 1086.3089995384216 and batch: 450, loss is 3.846822772026062 and perplexity is 46.84399249775465
At time: 1087.1384344100952 and batch: 500, loss is 3.8755655097961426 and perplexity is 48.209953784885016
At time: 1087.9697914123535 and batch: 550, loss is 3.853670983314514 and perplexity is 47.165891012463
At time: 1088.7978780269623 and batch: 600, loss is 3.8802090883255005 and perplexity is 48.434341067965065
At time: 1089.6281833648682 and batch: 650, loss is 3.913055729866028 and perplexity is 50.05166289406909
At time: 1090.4547481536865 and batch: 700, loss is 3.899074592590332 and perplexity is 49.35675286015531
At time: 1091.2802157402039 and batch: 750, loss is 3.8928855895996093 and perplexity is 49.05222709656174
At time: 1092.1059849262238 and batch: 800, loss is 3.9057366943359373 and perplexity is 49.68667032114594
At time: 1092.9315927028656 and batch: 850, loss is 3.940092725753784 and perplexity is 51.423369350148114
At time: 1093.7594816684723 and batch: 900, loss is 3.89291570186615 and perplexity is 49.05370419253773
At time: 1094.5930016040802 and batch: 950, loss is 3.8966582298278807 and perplexity is 49.23763301680522
At time: 1095.420132637024 and batch: 1000, loss is 3.880386872291565 and perplexity is 48.442952682694674
At time: 1096.2478501796722 and batch: 1050, loss is 3.8771837091445924 and perplexity is 48.28803025529791
At time: 1097.0756413936615 and batch: 1100, loss is 3.8213586139678957 and perplexity is 45.66620894318714
At time: 1097.9032690525055 and batch: 1150, loss is 3.826597366333008 and perplexity is 45.90607064274292
At time: 1098.7288982868195 and batch: 1200, loss is 3.8392820167541504 and perplexity is 46.49208191770642
At time: 1099.554996728897 and batch: 1250, loss is 3.8995294713974 and perplexity is 49.379209308111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368276331546533 and perplexity of 78.90750403833742
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1102.0389404296875 and batch: 50, loss is 3.9410435247421263 and perplexity is 51.47228588891605
At time: 1102.8647124767303 and batch: 100, loss is 3.9423969078063963 and perplexity is 51.54199476968654
At time: 1103.7263622283936 and batch: 150, loss is 3.873089909553528 and perplexity is 48.090752819446074
At time: 1104.5516510009766 and batch: 200, loss is 3.922605957984924 and perplexity is 50.53195750347164
At time: 1105.3771529197693 and batch: 250, loss is 3.9371691846847536 and perplexity is 51.273250564086425
At time: 1106.2027022838593 and batch: 300, loss is 3.9448104429244997 and perplexity is 51.666543424937394
At time: 1107.0290451049805 and batch: 350, loss is 3.920277132987976 and perplexity is 50.414414339545914
At time: 1107.8541357517242 and batch: 400, loss is 3.924307556152344 and perplexity is 50.61801578730002
At time: 1108.6812300682068 and batch: 450, loss is 3.846822681427002 and perplexity is 46.843988253733144
At time: 1109.5068545341492 and batch: 500, loss is 3.8755656099319458 and perplexity is 48.209958612427705
At time: 1110.338549375534 and batch: 550, loss is 3.853670930862427 and perplexity is 47.16588853851364
At time: 1111.1626398563385 and batch: 600, loss is 3.8802089977264402 and perplexity is 48.434336679859484
At time: 1111.9860169887543 and batch: 650, loss is 3.9130555152893067 and perplexity is 50.05165215414853
At time: 1112.8116726875305 and batch: 700, loss is 3.899074425697327 and perplexity is 49.35674462285919
At time: 1113.6364562511444 and batch: 750, loss is 3.8928853034973145 and perplexity is 49.05221306260901
At time: 1114.4635434150696 and batch: 800, loss is 3.9057364082336425 and perplexity is 49.68665610567758
At time: 1115.290182352066 and batch: 850, loss is 3.9400924825668335 and perplexity is 51.423356844657256
At time: 1116.1216237545013 and batch: 900, loss is 3.892915201187134 and perplexity is 49.05367963238353
At time: 1116.948811531067 and batch: 950, loss is 3.8966578006744386 and perplexity is 49.23761188631006
At time: 1117.7764399051666 and batch: 1000, loss is 3.8803864097595215 and perplexity is 48.44293027628196
At time: 1118.603106021881 and batch: 1050, loss is 3.877183051109314 and perplexity is 48.28799848008094
At time: 1119.4301006793976 and batch: 1100, loss is 3.821357569694519 and perplexity is 45.66616125520582
At time: 1120.2576863765717 and batch: 1150, loss is 3.826596598625183 and perplexity is 45.906035400306806
At time: 1121.0832500457764 and batch: 1200, loss is 3.8392813777923585 and perplexity is 46.492052211051934
At time: 1121.9110984802246 and batch: 1250, loss is 3.899529104232788 and perplexity is 49.379191177816104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368276331546533 and perplexity of 78.90750403833742
Annealing...
Model not improving. Stopping early with 78.9073985756669loss at 49 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f15b303a898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'anneal': 7.428288555337209, 'lr': 21.9743528860023, 'dropout': 0.2134645447692738, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.479954481124878 and batch: 50, loss is 7.534838771820068 and perplexity is 1872.1424922957137
At time: 2.3544483184814453 and batch: 100, loss is 6.464574880599976 and perplexity is 641.9913824340667
At time: 3.1827590465545654 and batch: 150, loss is 6.146452255249024 and perplexity is 467.05744340254637
At time: 4.020533561706543 and batch: 200, loss is 6.156125087738037 and perplexity is 471.5971322444048
At time: 4.8428261280059814 and batch: 250, loss is 6.1727159881591795 and perplexity is 479.48661918091625
At time: 5.670547723770142 and batch: 300, loss is 6.234579973220825 and perplexity is 510.0863235310662
At time: 6.494157314300537 and batch: 350, loss is 6.298877601623535 and perplexity is 543.9610263897237
At time: 7.31987452507019 and batch: 400, loss is 6.306757144927978 and perplexity is 548.2641218025082
At time: 8.15419602394104 and batch: 450, loss is 6.333257522583008 and perplexity is 562.9875547091432
At time: 8.984821557998657 and batch: 500, loss is 6.3509097671508785 and perplexity is 573.0137810656273
At time: 9.808282136917114 and batch: 550, loss is 6.375791721343994 and perplexity is 587.4503438524513
At time: 10.628707885742188 and batch: 600, loss is 6.420837020874023 and perplexity is 614.5172626953147
At time: 11.457934379577637 and batch: 650, loss is 6.430241985321045 and perplexity is 620.3240391638218
At time: 12.279563188552856 and batch: 700, loss is 6.476988506317139 and perplexity is 650.0104933125452
At time: 13.10436224937439 and batch: 750, loss is 6.42389100074768 and perplexity is 616.3968547048048
At time: 13.928509712219238 and batch: 800, loss is 6.479443025588989 and perplexity is 651.6079162459789
At time: 14.75354266166687 and batch: 850, loss is 6.526399536132812 and perplexity is 682.9348971948222
At time: 15.578823804855347 and batch: 900, loss is 6.544645357131958 and perplexity is 695.5099775535317
At time: 16.408114433288574 and batch: 950, loss is 6.50359808921814 and perplexity is 667.5391826949234
At time: 17.232688188552856 and batch: 1000, loss is 6.514834775924682 and perplexity is 675.0824124407641
At time: 18.055557250976562 and batch: 1050, loss is 6.4895091724395755 and perplexity is 658.2002211093842
At time: 18.878685235977173 and batch: 1100, loss is 6.506449937820435 and perplexity is 669.4456205240035
At time: 19.70046353340149 and batch: 1150, loss is 6.563735389709473 and perplexity is 708.9148281089806
At time: 20.52203392982483 and batch: 1200, loss is 6.564397716522217 and perplexity is 709.3845169341703
At time: 21.347203254699707 and batch: 1250, loss is 6.568071908950806 and perplexity is 711.9957262604736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.090632751910356 and perplexity of 441.70080977283055
Finished 1 epochs...
Completing Train Step...
At time: 23.806116342544556 and batch: 50, loss is 6.4302827262878415 and perplexity is 620.3493122797266
At time: 24.6303288936615 and batch: 100, loss is 6.4166623878479 and perplexity is 611.9572259602273
At time: 25.450738430023193 and batch: 150, loss is 6.387994470596314 and perplexity is 594.6627693081223
At time: 26.327263832092285 and batch: 200, loss is 6.4135970115661625 and perplexity is 610.0842189965391
At time: 27.145721435546875 and batch: 250, loss is 6.415596256256103 and perplexity is 611.3051466917218
At time: 27.968390464782715 and batch: 300, loss is 6.44190752029419 and perplexity is 627.6028238519547
At time: 28.78808569908142 and batch: 350, loss is 6.458121089935303 and perplexity is 637.8614456518377
At time: 29.606502532958984 and batch: 400, loss is 6.4316857719421385 and perplexity is 621.2203015624497
At time: 30.42853045463562 and batch: 450, loss is 6.4808017349243165 and perplexity is 652.4938637410265
At time: 31.245429039001465 and batch: 500, loss is 6.439867877960205 and perplexity is 626.3240431344675
At time: 32.06809067726135 and batch: 550, loss is 6.449804954528808 and perplexity is 632.5788991295085
At time: 32.883383989334106 and batch: 600, loss is 6.458600559234619 and perplexity is 638.1673539632066
At time: 33.70593976974487 and batch: 650, loss is 6.437826747894287 and perplexity is 625.0469381111393
At time: 34.52608919143677 and batch: 700, loss is 6.49912844657898 and perplexity is 664.5621791278204
At time: 35.34236788749695 and batch: 750, loss is 6.422847909927368 and perplexity is 615.7542320191715
At time: 36.17036461830139 and batch: 800, loss is 6.453369932174683 and perplexity is 634.8380532885504
At time: 36.987667083740234 and batch: 850, loss is 6.465240383148194 and perplexity is 642.4187715335039
At time: 37.81076407432556 and batch: 900, loss is 6.47842679977417 and perplexity is 650.9460718090085
At time: 38.62639260292053 and batch: 950, loss is 6.448571634292603 and perplexity is 631.7992076756705
At time: 39.44718360900879 and batch: 1000, loss is 6.4425929927825925 and perplexity is 628.0331758017093
At time: 40.2653968334198 and batch: 1050, loss is 6.4961808490753175 and perplexity is 662.6062014412495
At time: 41.08597111701965 and batch: 1100, loss is 6.453685607910156 and perplexity is 635.0384878924348
At time: 41.90896272659302 and batch: 1150, loss is 6.473277988433838 and perplexity is 647.603086877177
At time: 42.72613453865051 and batch: 1200, loss is 6.44450798034668 and perplexity is 629.237003813152
At time: 43.54725408554077 and batch: 1250, loss is 6.476027336120605 and perplexity is 649.3860227582582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.121641284357892 and perplexity of 455.6118698034046
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 46.0002326965332 and batch: 50, loss is 6.356487655639649 and perplexity is 576.2189186789657
At time: 46.82613778114319 and batch: 100, loss is 6.270958948135376 and perplexity is 528.9844033404984
At time: 47.64888858795166 and batch: 150, loss is 6.125859594345092 and perplexity is 457.53784122039883
At time: 48.47429704666138 and batch: 200, loss is 6.133611583709717 and perplexity is 461.09845278018895
At time: 49.30221915245056 and batch: 250, loss is 6.099981374740601 and perplexity is 445.84946594322764
At time: 50.12854337692261 and batch: 300, loss is 6.0818390369415285 and perplexity is 437.8336470340428
At time: 50.95353126525879 and batch: 350, loss is 6.097018404006958 and perplexity is 444.5303821923431
At time: 51.77719736099243 and batch: 400, loss is 6.037231187820435 and perplexity is 418.7320379704815
At time: 52.59949445724487 and batch: 450, loss is 6.059505357742309 and perplexity is 428.16359666293533
At time: 53.4190137386322 and batch: 500, loss is 6.009019718170166 and perplexity is 407.08406750116785
At time: 54.24332237243652 and batch: 550, loss is 6.014097213745117 and perplexity is 409.1562914545229
At time: 55.06719756126404 and batch: 600, loss is 6.023954992294311 and perplexity is 413.2096090979053
At time: 55.89071702957153 and batch: 650, loss is 5.993996372222901 and perplexity is 401.01401315629175
At time: 56.72036027908325 and batch: 700, loss is 6.039412412643433 and perplexity is 419.64638351997354
At time: 57.54171347618103 and batch: 750, loss is 5.975213918685913 and perplexity is 393.5522802736591
At time: 58.36951231956482 and batch: 800, loss is 5.959158840179444 and perplexity is 387.28421926353474
At time: 59.19355773925781 and batch: 850, loss is 5.98186092376709 and perplexity is 396.1769376721869
At time: 60.02250671386719 and batch: 900, loss is 5.927754201889038 and perplexity is 375.31069460662275
At time: 60.847110748291016 and batch: 950, loss is 5.913544216156006 and perplexity is 370.0152482057533
At time: 61.66972351074219 and batch: 1000, loss is 5.881471338272095 and perplexity is 358.33608762952684
At time: 62.492950439453125 and batch: 1050, loss is 5.844138460159302 and perplexity is 345.20500576424405
At time: 63.31482172012329 and batch: 1100, loss is 5.781321115493775 and perplexity is 324.1871963603448
At time: 64.14510464668274 and batch: 1150, loss is 5.79539758682251 and perplexity is 328.7828777388279
At time: 65.00077962875366 and batch: 1200, loss is 5.793565063476563 and perplexity is 328.18092715232814
At time: 65.82355546951294 and batch: 1250, loss is 5.826890249252319 and perplexity is 339.3018924381797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.5019655993385035 and perplexity of 245.17337156779777
Finished 3 epochs...
Completing Train Step...
At time: 68.23178744316101 and batch: 50, loss is 5.90090271949768 and perplexity is 365.3671430846911
At time: 69.08369827270508 and batch: 100, loss is 5.9354673194885255 and perplexity is 378.21670291533337
At time: 69.91107130050659 and batch: 150, loss is 5.844877767562866 and perplexity is 345.4603127442946
At time: 70.73417472839355 and batch: 200, loss is 5.856809635162353 and perplexity is 349.6069890566483
At time: 71.56303119659424 and batch: 250, loss is 5.865620403289795 and perplexity is 352.70090504621635
At time: 72.38935995101929 and batch: 300, loss is 5.8643170261383055 and perplexity is 352.2415021978931
At time: 73.21147084236145 and batch: 350, loss is 5.893084487915039 and perplexity is 362.52175559017155
At time: 74.03642964363098 and batch: 400, loss is 5.861982860565186 and perplexity is 351.42027102766275
At time: 74.8589346408844 and batch: 450, loss is 5.884175214767456 and perplexity is 359.3062952239684
At time: 75.68602299690247 and batch: 500, loss is 5.842509040832519 and perplexity is 344.64298006826385
At time: 76.5135383605957 and batch: 550, loss is 5.831566562652588 and perplexity is 340.8922901253118
At time: 77.34194803237915 and batch: 600, loss is 5.848062753677368 and perplexity is 346.5623531035137
At time: 78.20099401473999 and batch: 650, loss is 5.834516134262085 and perplexity is 341.89926068199327
At time: 79.0364818572998 and batch: 700, loss is 5.8740831470489505 and perplexity is 355.69838800488094
At time: 79.8577938079834 and batch: 750, loss is 5.834461975097656 and perplexity is 341.8807442051389
At time: 80.67899370193481 and batch: 800, loss is 5.830941419601441 and perplexity is 340.679250276079
At time: 81.50139546394348 and batch: 850, loss is 5.853865232467651 and perplexity is 348.5791192698619
At time: 82.3281741142273 and batch: 900, loss is 5.8157548999786375 and perplexity is 335.5446055316388
At time: 83.15434455871582 and batch: 950, loss is 5.801965141296387 and perplexity is 330.94928340515094
At time: 83.9832353591919 and batch: 1000, loss is 5.776707849502563 and perplexity is 322.6950790060093
At time: 84.8031325340271 and batch: 1050, loss is 5.766703329086304 and perplexity is 319.48276512322377
At time: 85.6263267993927 and batch: 1100, loss is 5.7304814910888675 and perplexity is 308.1175885403326
At time: 86.50546503067017 and batch: 1150, loss is 5.768041505813598 and perplexity is 319.9105757035239
At time: 87.32745456695557 and batch: 1200, loss is 5.769293375015259 and perplexity is 320.3113126833387
At time: 88.15097451210022 and batch: 1250, loss is 5.78082106590271 and perplexity is 324.0251272100598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.461463649777601 and perplexity of 235.44177660218813
Finished 4 epochs...
Completing Train Step...
At time: 90.59738206863403 and batch: 50, loss is 5.81439772605896 and perplexity is 335.0895220277066
At time: 91.42013764381409 and batch: 100, loss is 5.8433733081817625 and perplexity is 344.940972497097
At time: 92.24914193153381 and batch: 150, loss is 5.753042707443237 and perplexity is 315.1481064219086
At time: 93.07188177108765 and batch: 200, loss is 5.764064226150513 and perplexity is 318.6407288188163
At time: 93.89595246315002 and batch: 250, loss is 5.781134634017945 and perplexity is 324.12674709003204
At time: 94.71748900413513 and batch: 300, loss is 5.78381986618042 and perplexity is 324.99827225606435
At time: 95.54097747802734 and batch: 350, loss is 5.808726606369018 and perplexity is 333.19456757691836
At time: 96.36260557174683 and batch: 400, loss is 5.7737274169921875 and perplexity is 321.73473992539897
At time: 97.18278694152832 and batch: 450, loss is 5.777981309890747 and perplexity is 323.1062801752151
At time: 98.01203322410583 and batch: 500, loss is 5.747137603759765 and perplexity is 313.2926080248515
At time: 98.83248686790466 and batch: 550, loss is 5.746634511947632 and perplexity is 313.1350327196988
At time: 99.65259623527527 and batch: 600, loss is 5.757853078842163 and perplexity is 316.6677379251126
At time: 100.47709727287292 and batch: 650, loss is 5.74816593170166 and perplexity is 313.6149412714101
At time: 101.30514025688171 and batch: 700, loss is 5.7794045639038085 and perplexity is 323.56646989086755
At time: 102.12901759147644 and batch: 750, loss is 5.7442519474029545 and perplexity is 312.38985636093685
At time: 102.9504177570343 and batch: 800, loss is 5.742251396179199 and perplexity is 311.76552915900294
At time: 103.77353000640869 and batch: 850, loss is 5.76528959274292 and perplexity is 319.03141984387014
At time: 104.59666347503662 and batch: 900, loss is 5.744166774749756 and perplexity is 312.36325042110354
At time: 105.42108392715454 and batch: 950, loss is 5.725988836288452 and perplexity is 306.7364274402138
At time: 106.25219130516052 and batch: 1000, loss is 5.707607173919678 and perplexity is 301.1496068278355
At time: 107.13143301010132 and batch: 1050, loss is 5.707421598434448 and perplexity is 301.0937260286351
At time: 107.95261788368225 and batch: 1100, loss is 5.670982370376587 and perplexity is 290.3195957214863
At time: 108.77564907073975 and batch: 1150, loss is 5.716102561950684 and perplexity is 303.7188876596251
At time: 109.60008645057678 and batch: 1200, loss is 5.719590396881103 and perplexity is 304.7800585236749
At time: 110.4217164516449 and batch: 1250, loss is 5.7220984554290775 and perplexity is 305.545424144399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.4115025875342155 and perplexity of 223.96786629721038
Finished 5 epochs...
Completing Train Step...
At time: 112.87656712532043 and batch: 50, loss is 5.73997018814087 and perplexity is 311.05513771121963
At time: 113.7009768486023 and batch: 100, loss is 5.7625635147094725 and perplexity is 318.1628996630149
At time: 114.52093625068665 and batch: 150, loss is 5.674078054428101 and perplexity is 291.21972600443553
At time: 115.344322681427 and batch: 200, loss is 5.694794797897339 and perplexity is 297.31577750331
At time: 116.16863918304443 and batch: 250, loss is 5.7115246868133545 and perplexity is 302.33167817361704
At time: 116.99428224563599 and batch: 300, loss is 5.713992061614991 and perplexity is 303.07856478376317
At time: 117.81862020492554 and batch: 350, loss is 5.74088303565979 and perplexity is 311.3392132610307
At time: 118.64061450958252 and batch: 400, loss is 5.7068626403808596 and perplexity is 300.9254742927985
At time: 119.46564078330994 and batch: 450, loss is 5.70083233833313 and perplexity is 299.11626331587684
At time: 120.29474449157715 and batch: 500, loss is 5.68488187789917 and perplexity is 294.38306985030806
At time: 121.11635780334473 and batch: 550, loss is 5.689186601638794 and perplexity is 295.65303911193865
At time: 121.93821167945862 and batch: 600, loss is 5.703178625106812 and perplexity is 299.81889981931744
At time: 122.75875210762024 and batch: 650, loss is 5.691491317749024 and perplexity is 296.3352212503373
At time: 123.58264255523682 and batch: 700, loss is 5.7233424949646 and perplexity is 305.92577126630783
At time: 124.40515422821045 and batch: 750, loss is 5.688565855026245 and perplexity is 295.4695704390899
At time: 125.2284767627716 and batch: 800, loss is 5.695249671936035 and perplexity is 297.45104949531725
At time: 126.05060720443726 and batch: 850, loss is 5.720098066329956 and perplexity is 304.93482532987247
At time: 126.87720680236816 and batch: 900, loss is 5.706089696884155 and perplexity is 300.69296577400087
At time: 127.70293068885803 and batch: 950, loss is 5.690435819625854 and perplexity is 296.0226049924385
At time: 128.58785247802734 and batch: 1000, loss is 5.676603889465332 and perplexity is 291.9562287425565
At time: 129.40777254104614 and batch: 1050, loss is 5.675821332931519 and perplexity is 291.72784586107974
At time: 130.22946572303772 and batch: 1100, loss is 5.644642152786255 and perplexity is 282.77234881831856
At time: 131.0514919757843 and batch: 1150, loss is 5.693509912490844 and perplexity is 296.934006118439
At time: 131.87220454216003 and batch: 1200, loss is 5.69550687789917 and perplexity is 297.5275655187551
At time: 132.69423937797546 and batch: 1250, loss is 5.693270053863525 and perplexity is 296.86279247627175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.393541322137318 and perplexity of 219.98103149047452
Finished 6 epochs...
Completing Train Step...
At time: 135.1383957862854 and batch: 50, loss is 5.701828575134277 and perplexity is 299.41440242913376
At time: 135.99696803092957 and batch: 100, loss is 5.721934928894043 and perplexity is 305.49546344495434
At time: 136.82012128829956 and batch: 150, loss is 5.636898345947266 and perplexity is 280.5910709671302
At time: 137.64316964149475 and batch: 200, loss is 5.663156108856201 and perplexity is 288.05634658275875
At time: 138.46865487098694 and batch: 250, loss is 5.675731039047241 and perplexity is 291.70150580991555
At time: 139.29171347618103 and batch: 300, loss is 5.681690454483032 and perplexity is 293.4450664070944
At time: 140.11806869506836 and batch: 350, loss is 5.712100315093994 and perplexity is 302.50575893580515
At time: 140.94330215454102 and batch: 400, loss is 5.681988124847412 and perplexity is 293.53242930896556
At time: 141.76761603355408 and batch: 450, loss is 5.6654302024841305 and perplexity is 288.71215909190335
At time: 142.590149641037 and batch: 500, loss is 5.659514560699463 and perplexity is 287.0092831478651
At time: 143.41520023345947 and batch: 550, loss is 5.6650083637237545 and perplexity is 288.5903947968618
At time: 144.23968386650085 and batch: 600, loss is 5.678052091598511 and perplexity is 292.3793466820177
At time: 145.06154203414917 and batch: 650, loss is 5.669686603546142 and perplexity is 289.94365283888214
At time: 145.88501739501953 and batch: 700, loss is 5.699788780212402 and perplexity is 298.8042809244437
At time: 146.72003960609436 and batch: 750, loss is 5.663962354660034 and perplexity is 288.28868445158895
At time: 147.54417252540588 and batch: 800, loss is 5.672605657577515 and perplexity is 290.7912505174265
At time: 148.37487196922302 and batch: 850, loss is 5.698123788833618 and perplexity is 298.30718831505175
At time: 149.2116048336029 and batch: 900, loss is 5.687870607376099 and perplexity is 295.264217308465
At time: 150.12044286727905 and batch: 950, loss is 5.676526441574096 and perplexity is 291.93361822388715
At time: 150.94892978668213 and batch: 1000, loss is 5.662021083831787 and perplexity is 287.72958089964027
At time: 151.77855587005615 and batch: 1050, loss is 5.662231311798096 and perplexity is 287.7900760629445
At time: 152.59873175621033 and batch: 1100, loss is 5.632797784805298 and perplexity is 279.4428459171621
At time: 153.4269676208496 and batch: 1150, loss is 5.682152242660522 and perplexity is 293.58060716265453
At time: 154.2509207725525 and batch: 1200, loss is 5.682962923049927 and perplexity is 293.81870370064814
At time: 155.07480931282043 and batch: 1250, loss is 5.6782198333740235 and perplexity is 292.42839502636645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.388009398523038 and perplexity of 218.7674729789653
Finished 7 epochs...
Completing Train Step...
At time: 157.5083110332489 and batch: 50, loss is 5.678135967254638 and perplexity is 292.4038712200502
At time: 158.33127403259277 and batch: 100, loss is 5.6969436931610105 and perplexity is 297.9553649264725
At time: 159.16286873817444 and batch: 150, loss is 5.612311096191406 and perplexity is 273.77623062949
At time: 159.98438692092896 and batch: 200, loss is 5.644311494827271 and perplexity is 282.6788633473104
At time: 160.80621361732483 and batch: 250, loss is 5.654292764663697 and perplexity is 285.51448537201753
At time: 161.63320422172546 and batch: 300, loss is 5.659372472763062 and perplexity is 286.96850548817014
At time: 162.45357155799866 and batch: 350, loss is 5.689934530258179 and perplexity is 295.8742491956627
At time: 163.27661609649658 and batch: 400, loss is 5.659391250610351 and perplexity is 286.9738941895371
At time: 164.1001136302948 and batch: 450, loss is 5.6366743564605715 and perplexity is 280.5282285554557
At time: 164.92175388336182 and batch: 500, loss is 5.633260364532471 and perplexity is 279.572140414788
At time: 165.75204706192017 and batch: 550, loss is 5.642288188934327 and perplexity is 282.1074957579148
At time: 166.57626128196716 and batch: 600, loss is 5.654246091842651 and perplexity is 285.50115991650665
At time: 167.39710187911987 and batch: 650, loss is 5.646745195388794 and perplexity is 283.36765687439527
At time: 168.2239284515381 and batch: 700, loss is 5.67625901222229 and perplexity is 291.85555704398564
At time: 169.04916334152222 and batch: 750, loss is 5.639943952560425 and perplexity is 281.44694365264706
At time: 169.8736608028412 and batch: 800, loss is 5.649126214981079 and perplexity is 284.0431646967775
At time: 170.69620895385742 and batch: 850, loss is 5.674662427902222 and perplexity is 291.389956821796
At time: 171.57558822631836 and batch: 900, loss is 5.664111518859864 and perplexity is 288.3316900098901
At time: 172.40382313728333 and batch: 950, loss is 5.651230144500732 and perplexity is 284.64140059824996
At time: 173.22911095619202 and batch: 1000, loss is 5.639796056747437 and perplexity is 281.40532190601937
At time: 174.04815649986267 and batch: 1050, loss is 5.639736394882203 and perplexity is 281.38853324045425
At time: 174.86833477020264 and batch: 1100, loss is 5.614356241226196 and perplexity is 274.3367156694458
At time: 175.69210195541382 and batch: 1150, loss is 5.662973299026489 and perplexity is 288.0036918641391
At time: 176.51335883140564 and batch: 1200, loss is 5.66409197807312 and perplexity is 288.3260558368725
At time: 177.33751964569092 and batch: 1250, loss is 5.656409673690796 and perplexity is 286.11953375370916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.369398576499772 and perplexity of 214.7336829776308
Finished 8 epochs...
Completing Train Step...
At time: 179.77838015556335 and batch: 50, loss is 5.6523888969421385 and perplexity is 284.97142068498243
At time: 180.62869262695312 and batch: 100, loss is 5.672120866775512 and perplexity is 290.6503117595398
At time: 181.45328164100647 and batch: 150, loss is 5.584497566223145 and perplexity is 266.2664678586262
At time: 182.27618265151978 and batch: 200, loss is 5.61871826171875 and perplexity is 275.5359917769606
At time: 183.1019217967987 and batch: 250, loss is 5.630494565963745 and perplexity is 278.7999685171742
At time: 183.92929673194885 and batch: 300, loss is 5.638489894866943 and perplexity is 281.0380009441195
At time: 184.75339198112488 and batch: 350, loss is 5.669106988906861 and perplexity is 289.7756459474603
At time: 185.57604789733887 and batch: 400, loss is 5.640598354339599 and perplexity is 281.63118331012146
At time: 186.40568852424622 and batch: 450, loss is 5.618804597854615 and perplexity is 275.55978151672457
At time: 187.22732591629028 and batch: 500, loss is 5.61488941192627 and perplexity is 274.4830229681117
At time: 188.0474934577942 and batch: 550, loss is 5.623540983200074 and perplexity is 276.8680345773035
At time: 188.87736248970032 and batch: 600, loss is 5.635502605438233 and perplexity is 280.19971182434654
At time: 189.70079469680786 and batch: 650, loss is 5.628690567016601 and perplexity is 278.2974670598345
At time: 190.5250232219696 and batch: 700, loss is 5.655298357009888 and perplexity is 285.8017409600382
At time: 191.34802842140198 and batch: 750, loss is 5.620121107101441 and perplexity is 275.922797421611
At time: 192.23221492767334 and batch: 800, loss is 5.63083701133728 and perplexity is 278.89545862567445
At time: 193.05884647369385 and batch: 850, loss is 5.655269031524658 and perplexity is 285.7933598081964
At time: 193.88402795791626 and batch: 900, loss is 5.6432952976226805 and perplexity is 282.3917517822723
At time: 194.71234440803528 and batch: 950, loss is 5.6296209049224855 and perplexity is 278.55649821712075
At time: 195.54189896583557 and batch: 1000, loss is 5.619454536437988 and perplexity is 275.7389366643732
At time: 196.3697235584259 and batch: 1050, loss is 5.6180463218688965 and perplexity is 275.3509103527727
At time: 197.1957266330719 and batch: 1100, loss is 5.595523405075073 and perplexity is 269.2185235636424
At time: 198.01963233947754 and batch: 1150, loss is 5.643958435058594 and perplexity is 282.5790784292638
At time: 198.84270119667053 and batch: 1200, loss is 5.640060968399048 and perplexity is 281.4798793297469
At time: 199.6724853515625 and batch: 1250, loss is 5.631685447692871 and perplexity is 279.1321840812227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3553711828524175 and perplexity of 211.7425569743782
Finished 9 epochs...
Completing Train Step...
At time: 202.16868257522583 and batch: 50, loss is 5.625574588775635 and perplexity is 277.4316478454192
At time: 202.99744987487793 and batch: 100, loss is 5.6469643020629885 and perplexity is 283.4297514216837
At time: 203.82279300689697 and batch: 150, loss is 5.560553798675537 and perplexity is 259.9667657158558
At time: 204.6483118534088 and batch: 200, loss is 5.595624046325684 and perplexity is 269.24561941599876
At time: 205.47120690345764 and batch: 250, loss is 5.6055317306518555 and perplexity is 271.9264786429629
At time: 206.2954957485199 and batch: 300, loss is 5.613382387161255 and perplexity is 274.0696817908469
At time: 207.11728072166443 and batch: 350, loss is 5.641524858474732 and perplexity is 281.89223668088005
At time: 207.94076228141785 and batch: 400, loss is 5.615504264831543 and perplexity is 274.65184154621005
At time: 208.7667055130005 and batch: 450, loss is 5.595004692077636 and perplexity is 269.07891262845294
At time: 209.59127402305603 and batch: 500, loss is 5.58798056602478 and perplexity is 267.195490867651
At time: 210.4175248146057 and batch: 550, loss is 5.597207546234131 and perplexity is 269.67230757086185
At time: 211.2428138256073 and batch: 600, loss is 5.612713890075684 and perplexity is 273.88652823295695
At time: 212.06703925132751 and batch: 650, loss is 5.607613153457642 and perplexity is 272.49306206238106
At time: 212.88922476768494 and batch: 700, loss is 5.632058210372925 and perplexity is 279.23625353764976
At time: 213.77153491973877 and batch: 750, loss is 5.599557266235352 and perplexity is 270.30670702439255
At time: 214.59392833709717 and batch: 800, loss is 5.6094852066040035 and perplexity is 273.0036613419508
At time: 215.42091751098633 and batch: 850, loss is 5.634303188323974 and perplexity is 279.86383696186914
At time: 216.24596452713013 and batch: 900, loss is 5.6230081367492675 and perplexity is 276.72054572568254
At time: 217.07580375671387 and batch: 950, loss is 5.608836584091186 and perplexity is 272.8266424365526
At time: 217.90227007865906 and batch: 1000, loss is 5.599133501052856 and perplexity is 270.1921847203569
At time: 218.72472023963928 and batch: 1050, loss is 5.596876468658447 and perplexity is 269.58303989512046
At time: 219.54767894744873 and batch: 1100, loss is 5.573340797424317 and perplexity is 263.31230453927554
At time: 220.37499356269836 and batch: 1150, loss is 5.623342800140381 and perplexity is 276.8131694599278
At time: 221.2026002407074 and batch: 1200, loss is 5.621861324310303 and perplexity is 276.4033810606795
At time: 222.02426171302795 and batch: 1250, loss is 5.614100542068481 and perplexity is 274.26657696990424
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.346078246179288 and perplexity of 209.78396144862543
Finished 10 epochs...
Completing Train Step...
At time: 224.48353695869446 and batch: 50, loss is 5.6081964302062985 and perplexity is 272.65204729133865
At time: 225.37163710594177 and batch: 100, loss is 5.627046842575073 and perplexity is 277.8403984615361
At time: 226.20502710342407 and batch: 150, loss is 5.544013519287109 and perplexity is 255.70220848787216
At time: 227.03612351417542 and batch: 200, loss is 5.576827430725098 and perplexity is 264.2319803432514
At time: 227.85953330993652 and batch: 250, loss is 5.58590030670166 and perplexity is 266.6402326974097
At time: 228.68836379051208 and batch: 300, loss is 5.595361051559448 and perplexity is 269.1748185377987
At time: 229.51138758659363 and batch: 350, loss is 5.624269170761108 and perplexity is 277.0697198595756
At time: 230.3378038406372 and batch: 400, loss is 5.597424783706665 and perplexity is 269.7308968650357
At time: 231.16400861740112 and batch: 450, loss is 5.574074926376343 and perplexity is 263.5056806982962
At time: 231.99027252197266 and batch: 500, loss is 5.569761352539063 and perplexity is 262.3714774813909
At time: 232.81906700134277 and batch: 550, loss is 5.575157985687256 and perplexity is 263.7912275834487
At time: 233.64597725868225 and batch: 600, loss is 5.5950741481781 and perplexity is 269.09760244949473
At time: 234.47624158859253 and batch: 650, loss is 5.586544885635376 and perplexity is 266.81215877829976
At time: 235.36387753486633 and batch: 700, loss is 5.609072675704956 and perplexity is 272.8910621230196
At time: 236.1846091747284 and batch: 750, loss is 5.578059988021851 and perplexity is 264.5578621914272
At time: 237.009916305542 and batch: 800, loss is 5.584576902389526 and perplexity is 266.287593257415
At time: 237.83666372299194 and batch: 850, loss is 5.613101673126221 and perplexity is 273.99275738197673
At time: 238.66291403770447 and batch: 900, loss is 5.5987411308288575 and perplexity is 270.08619014823233
At time: 239.48919343948364 and batch: 950, loss is 5.585625429153442 and perplexity is 266.5669493564235
At time: 240.31913661956787 and batch: 1000, loss is 5.577501192092895 and perplexity is 264.41006963184753
At time: 241.1466941833496 and batch: 1050, loss is 5.576154155731201 and perplexity is 264.05413943293524
At time: 241.97355937957764 and batch: 1100, loss is 5.553631315231323 and perplexity is 258.17336464052704
At time: 242.8018879890442 and batch: 1150, loss is 5.600118503570557 and perplexity is 270.45845581989255
At time: 243.6271243095398 and batch: 1200, loss is 5.600083246231079 and perplexity is 270.44892034239973
At time: 244.45376420021057 and batch: 1250, loss is 5.593518524169922 and perplexity is 268.6793131933395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.328685454208485 and perplexity of 206.1667802736134
Finished 11 epochs...
Completing Train Step...
At time: 246.8939564228058 and batch: 50, loss is 5.588036184310913 and perplexity is 267.21035223619367
At time: 247.7442708015442 and batch: 100, loss is 5.604380960464478 and perplexity is 271.61373374144966
At time: 248.57036018371582 and batch: 150, loss is 5.521708278656006 and perplexity is 250.0618478474909
At time: 249.40245056152344 and batch: 200, loss is 5.55593710899353 and perplexity is 258.7693460152976
At time: 250.22956943511963 and batch: 250, loss is 5.564104318618774 and perplexity is 260.891423438584
At time: 251.0564408302307 and batch: 300, loss is 5.572202863693238 and perplexity is 263.01284300213854
At time: 251.88499116897583 and batch: 350, loss is 5.599151105880737 and perplexity is 270.1969414491342
At time: 252.71100187301636 and batch: 400, loss is 5.571026172637939 and perplexity is 262.7035401549845
At time: 253.53552317619324 and batch: 450, loss is 5.548016996383667 and perplexity is 256.7279583360597
At time: 254.3608386516571 and batch: 500, loss is 5.544058332443237 and perplexity is 255.71366756762026
At time: 255.18428087234497 and batch: 550, loss is 5.551250820159912 and perplexity is 257.55951514116254
At time: 256.0131125450134 and batch: 600, loss is 5.569502363204956 and perplexity is 262.30353486573557
At time: 256.8944947719574 and batch: 650, loss is 5.564803619384765 and perplexity is 261.07392881647286
At time: 257.71902418136597 and batch: 700, loss is 5.585807685852051 and perplexity is 266.6155373961848
At time: 258.5464963912964 and batch: 750, loss is 5.557932863235473 and perplexity is 259.2863017219771
At time: 259.3699154853821 and batch: 800, loss is 5.5650639247894285 and perplexity is 261.1418966169696
At time: 260.19510221481323 and batch: 850, loss is 5.5927454662323 and perplexity is 268.47168878079054
At time: 261.02109718322754 and batch: 900, loss is 5.581841564178466 and perplexity is 265.56020191264975
At time: 261.84782004356384 and batch: 950, loss is 5.56863881111145 and perplexity is 262.0771198737087
At time: 262.6722500324249 and batch: 1000, loss is 5.558940029144287 and perplexity is 259.547577597685
At time: 263.5006740093231 and batch: 1050, loss is 5.558226556777954 and perplexity is 259.36246361802756
At time: 264.3256254196167 and batch: 1100, loss is 5.53685510635376 and perplexity is 253.87832234504774
At time: 265.1519601345062 and batch: 1150, loss is 5.582427921295166 and perplexity is 265.7159606876239
At time: 265.97621154785156 and batch: 1200, loss is 5.582246789932251 and perplexity is 265.6678355521334
At time: 266.7993459701538 and batch: 1250, loss is 5.577504816055298 and perplexity is 264.41102784573513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.319627747918568 and perplexity of 204.30781382662994
Finished 12 epochs...
Completing Train Step...
At time: 269.2455904483795 and batch: 50, loss is 5.570671920776367 and perplexity is 262.61049341880585
At time: 270.07310461997986 and batch: 100, loss is 5.586032609939576 and perplexity is 266.67551239731193
At time: 270.89975786209106 and batch: 150, loss is 5.503146114349366 and perplexity is 245.46297331917177
At time: 271.7259051799774 and batch: 200, loss is 5.5373631191253665 and perplexity is 254.0073285408562
At time: 272.5515854358673 and batch: 250, loss is 5.548742380142212 and perplexity is 256.91425218649914
At time: 273.37864565849304 and batch: 300, loss is 5.555406513214112 and perplexity is 258.6320805119278
At time: 274.201406955719 and batch: 350, loss is 5.581238555908203 and perplexity is 265.40011518630445
At time: 275.02479457855225 and batch: 400, loss is 5.555595769882202 and perplexity is 258.68103298989206
At time: 275.84949564933777 and batch: 450, loss is 5.5273806095123295 and perplexity is 251.48431191240945
At time: 276.6711890697479 and batch: 500, loss is 5.52572919845581 and perplexity is 251.06935066927403
At time: 277.53238439559937 and batch: 550, loss is 5.53544174194336 and perplexity is 253.51975321378737
At time: 278.35808062553406 and batch: 600, loss is 5.5542615699768065 and perplexity is 258.33613091546994
At time: 279.18242716789246 and batch: 650, loss is 5.550190448760986 and perplexity is 257.2865511449694
At time: 280.0050599575043 and batch: 700, loss is 5.57165228843689 and perplexity is 262.8680744953157
At time: 280.83383560180664 and batch: 750, loss is 5.546267452239991 and perplexity is 256.2791941209059
At time: 281.6595323085785 and batch: 800, loss is 5.550873050689697 and perplexity is 257.46223539539477
At time: 282.488068819046 and batch: 850, loss is 5.582522163391113 and perplexity is 265.7410034967105
At time: 283.31546664237976 and batch: 900, loss is 5.567823925018311 and perplexity is 261.8636438645222
At time: 284.1425516605377 and batch: 950, loss is 5.553173685073853 and perplexity is 258.05524375291367
At time: 284.97152304649353 and batch: 1000, loss is 5.544401035308838 and perplexity is 255.8013163921629
At time: 285.79543828964233 and batch: 1050, loss is 5.545175065994263 and perplexity is 255.99939110848743
At time: 286.62109780311584 and batch: 1100, loss is 5.523235425949097 and perplexity is 250.44402086450606
At time: 287.4472668170929 and batch: 1150, loss is 5.570249805450439 and perplexity is 262.4996648976386
At time: 288.27329301834106 and batch: 1200, loss is 5.571127462387085 and perplexity is 262.73015067833035
At time: 289.09573316574097 and batch: 1250, loss is 5.564596128463745 and perplexity is 261.0197639660801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.309719113537865 and perplexity of 202.2933989300697
Finished 13 epochs...
Completing Train Step...
At time: 291.54436016082764 and batch: 50, loss is 5.556173934936523 and perplexity is 258.8306365669953
At time: 292.3986005783081 and batch: 100, loss is 5.572098798751831 and perplexity is 262.9854740101434
At time: 293.2232618331909 and batch: 150, loss is 5.488486576080322 and perplexity is 241.89084630571998
At time: 294.04621934890747 and batch: 200, loss is 5.52413272857666 and perplexity is 250.668845795401
At time: 294.8678641319275 and batch: 250, loss is 5.530951328277588 and perplexity is 252.38389679061487
At time: 295.6956114768982 and batch: 300, loss is 5.54156494140625 and perplexity is 255.0768676264203
At time: 296.51824855804443 and batch: 350, loss is 5.567474136352539 and perplexity is 261.77206294783775
At time: 297.3452479839325 and batch: 400, loss is 5.5422821140289305 and perplexity is 255.25986738592616
At time: 298.16997814178467 and batch: 450, loss is 5.515924386978149 and perplexity is 248.61969186418776
At time: 299.05990076065063 and batch: 500, loss is 5.514738388061524 and perplexity is 248.3250039632929
At time: 299.90634751319885 and batch: 550, loss is 5.525977592468262 and perplexity is 251.13172253876925
At time: 300.76843547821045 and batch: 600, loss is 5.544710130691528 and perplexity is 255.88039561882803
At time: 301.6101379394531 and batch: 650, loss is 5.541380233764649 and perplexity is 255.02975733072373
At time: 302.4468562602997 and batch: 700, loss is 5.560368499755859 and perplexity is 259.91859861781
At time: 303.2935609817505 and batch: 750, loss is 5.534581136703491 and perplexity is 253.30166664244481
At time: 304.1311445236206 and batch: 800, loss is 5.537771978378296 and perplexity is 254.1112030210162
At time: 304.96248149871826 and batch: 850, loss is 5.571404113769531 and perplexity is 262.80284539280876
At time: 305.8073580265045 and batch: 900, loss is 5.556625213623047 and perplexity is 258.94746767640936
At time: 306.6541540622711 and batch: 950, loss is 5.542566022872925 and perplexity is 255.33234820828022
At time: 307.48149585723877 and batch: 1000, loss is 5.532113704681397 and perplexity is 252.67743244335358
At time: 308.3224906921387 and batch: 1050, loss is 5.532211532592774 and perplexity is 252.70215255796052
At time: 309.15511894226074 and batch: 1100, loss is 5.512230968475341 and perplexity is 247.70312896121527
At time: 309.9812822341919 and batch: 1150, loss is 5.560591497421265 and perplexity is 259.9765663215883
At time: 310.8060796260834 and batch: 1200, loss is 5.560283842086792 and perplexity is 259.8965954464856
At time: 311.63819551467896 and batch: 1250, loss is 5.555443143844604 and perplexity is 258.6415545416213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.307255431683394 and perplexity of 201.79562578296063
Finished 14 epochs...
Completing Train Step...
At time: 314.1506977081299 and batch: 50, loss is 5.5454637241363525 and perplexity is 256.0732980835127
At time: 314.9745862483978 and batch: 100, loss is 5.562738237380981 and perplexity is 260.5352678840212
At time: 315.79990553855896 and batch: 150, loss is 5.478777770996094 and perplexity is 239.55373884579814
At time: 316.62429451942444 and batch: 200, loss is 5.512862386703492 and perplexity is 247.8595826206594
At time: 317.44947934150696 and batch: 250, loss is 5.520639820098877 and perplexity is 249.79480981128842
At time: 318.2719657421112 and batch: 300, loss is 5.529666891098023 and perplexity is 252.05993362975667
At time: 319.10264229774475 and batch: 350, loss is 5.553976936340332 and perplexity is 258.26261022682155
At time: 319.9300606250763 and batch: 400, loss is 5.530492334365845 and perplexity is 252.2680807000395
At time: 320.8124783039093 and batch: 450, loss is 5.503949117660523 and perplexity is 245.66016005972057
At time: 321.6367464065552 and batch: 500, loss is 5.503550710678101 and perplexity is 245.56230683064916
At time: 322.4627318382263 and batch: 550, loss is 5.513926725387574 and perplexity is 248.12352960223757
At time: 323.2916588783264 and batch: 600, loss is 5.532150993347168 and perplexity is 252.68685462334903
At time: 324.1167240142822 and batch: 650, loss is 5.528553857803344 and perplexity is 251.77953860481867
At time: 324.9420280456543 and batch: 700, loss is 5.548415098190308 and perplexity is 256.8301825465599
At time: 325.7690885066986 and batch: 750, loss is 5.524683237075806 and perplexity is 250.8068791162545
At time: 326.6000785827637 and batch: 800, loss is 5.529516515731811 and perplexity is 252.0220328746712
At time: 327.42782258987427 and batch: 850, loss is 5.562313423156739 and perplexity is 260.42461230200405
At time: 328.2568826675415 and batch: 900, loss is 5.549215354919434 and perplexity is 257.0357948887426
At time: 329.0846543312073 and batch: 950, loss is 5.5341282653808594 and perplexity is 253.18697955285467
At time: 329.91138434410095 and batch: 1000, loss is 5.521843519210815 and perplexity is 250.09566863745005
At time: 330.73715710639954 and batch: 1050, loss is 5.522750129699707 and perplexity is 250.32251080707076
At time: 331.56430983543396 and batch: 1100, loss is 5.503915557861328 and perplexity is 245.6519158924159
At time: 332.3888919353485 and batch: 1150, loss is 5.552165870666504 and perplexity is 257.79530296840295
At time: 333.21507382392883 and batch: 1200, loss is 5.553140096664428 and perplexity is 258.0465762332973
At time: 334.0410907268524 and batch: 1250, loss is 5.546507024765015 and perplexity is 256.3405989297115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.302632352731524 and perplexity of 200.86486182820474
Finished 15 epochs...
Completing Train Step...
At time: 336.51988434791565 and batch: 50, loss is 5.536307468414306 and perplexity is 253.73932700676096
At time: 337.37327122688293 and batch: 100, loss is 5.551184759140015 and perplexity is 257.54250105889815
At time: 338.19604778289795 and batch: 150, loss is 5.469482040405273 and perplexity is 237.33722982362613
At time: 339.0205111503601 and batch: 200, loss is 5.503180866241455 and perplexity is 245.4715037701563
At time: 339.8424005508423 and batch: 250, loss is 5.511200590133667 and perplexity is 247.44803246748893
At time: 340.67483592033386 and batch: 300, loss is 5.5223354244232175 and perplexity is 250.2187222633307
At time: 341.49802327156067 and batch: 350, loss is 5.546613883972168 and perplexity is 256.367992746489
At time: 342.37960386276245 and batch: 400, loss is 5.523251600265503 and perplexity is 250.44807165810087
At time: 343.21112537384033 and batch: 450, loss is 5.497492551803589 and perplexity is 244.0791485028195
At time: 344.036598443985 and batch: 500, loss is 5.495704832077027 and perplexity is 243.64319319325924
At time: 344.8599350452423 and batch: 550, loss is 5.508309545516968 and perplexity is 246.73368227200777
At time: 345.6828796863556 and batch: 600, loss is 5.526051349639893 and perplexity is 251.15024598744066
At time: 346.5055413246155 and batch: 650, loss is 5.52573392868042 and perplexity is 251.07053828650427
At time: 347.33404302597046 and batch: 700, loss is 5.54395902633667 and perplexity is 255.68827489974245
At time: 348.1588706970215 and batch: 750, loss is 5.518583011627197 and perplexity is 249.28155774183034
At time: 348.98542618751526 and batch: 800, loss is 5.521610984802246 and perplexity is 250.0375195501515
At time: 349.81268787384033 and batch: 850, loss is 5.555041561126709 and perplexity is 258.53770941578733
At time: 350.6375172138214 and batch: 900, loss is 5.542196302413941 and perplexity is 255.23796406425683
At time: 351.46692991256714 and batch: 950, loss is 5.523407888412476 and perplexity is 250.48721678201292
At time: 352.29249358177185 and batch: 1000, loss is 5.513409051895142 and perplexity is 247.9951158691744
At time: 353.117192029953 and batch: 1050, loss is 5.515486879348755 and perplexity is 248.5109426432211
At time: 353.9515492916107 and batch: 1100, loss is 5.496198720932007 and perplexity is 243.76355557134113
At time: 354.7744085788727 and batch: 1150, loss is 5.543265495300293 and perplexity is 255.5110086223996
At time: 355.605073928833 and batch: 1200, loss is 5.5438294506073 and perplexity is 255.6551460514246
At time: 356.43316555023193 and batch: 1250, loss is 5.540166501998901 and perplexity is 254.72040738494496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.298638782361998 and perplexity of 200.06429349471168
Finished 16 epochs...
Completing Train Step...
At time: 358.9079222679138 and batch: 50, loss is 5.528697519302368 and perplexity is 251.81571222907453
At time: 359.79045701026917 and batch: 100, loss is 5.54091781616211 and perplexity is 254.91185434407188
At time: 360.6177616119385 and batch: 150, loss is 5.460959148406983 and perplexity is 235.3230258606662
At time: 361.44981026649475 and batch: 200, loss is 5.493542156219482 and perplexity is 243.1168413108924
At time: 362.2738869190216 and batch: 250, loss is 5.502664861679077 and perplexity is 245.34487202836507
At time: 363.15728068351746 and batch: 300, loss is 5.514001951217652 and perplexity is 248.14219560278767
At time: 363.9828038215637 and batch: 350, loss is 5.5377004528045655 and perplexity is 254.093028221418
At time: 364.80644369125366 and batch: 400, loss is 5.515754566192627 and perplexity is 248.57747465760025
At time: 365.6373245716095 and batch: 450, loss is 5.489484786987305 and perplexity is 242.13242493995583
At time: 366.46233892440796 and batch: 500, loss is 5.48842493057251 and perplexity is 241.8759352812679
At time: 367.2872061729431 and batch: 550, loss is 5.4997251605987545 and perplexity is 244.6246905208308
At time: 368.11754846572876 and batch: 600, loss is 5.517575159072876 and perplexity is 249.03044525055378
At time: 368.94085121154785 and batch: 650, loss is 5.515217714309692 and perplexity is 248.44406118711572
At time: 369.7651324272156 and batch: 700, loss is 5.535158643722534 and perplexity is 253.4479923808697
At time: 370.5910382270813 and batch: 750, loss is 5.51198655128479 and perplexity is 247.64259345660403
At time: 371.4177145957947 and batch: 800, loss is 5.51600043296814 and perplexity is 248.63859911368783
At time: 372.24299359321594 and batch: 850, loss is 5.547949514389038 and perplexity is 256.7106344058875
At time: 373.07675099372864 and batch: 900, loss is 5.5353508567810055 and perplexity is 253.49671307687566
At time: 373.9022250175476 and batch: 950, loss is 5.517678260803223 and perplexity is 249.05612204400637
At time: 374.7276792526245 and batch: 1000, loss is 5.506308708190918 and perplexity is 246.24050186234138
At time: 375.5565242767334 and batch: 1050, loss is 5.508484296798706 and perplexity is 246.77680306682998
At time: 376.3885250091553 and batch: 1100, loss is 5.487451343536377 and perplexity is 241.6405626028117
At time: 377.2188720703125 and batch: 1150, loss is 5.536462459564209 and perplexity is 253.77865740468235
At time: 378.0479507446289 and batch: 1200, loss is 5.538377628326416 and perplexity is 254.26515207285445
At time: 378.8761579990387 and batch: 1250, loss is 5.531829948425293 and perplexity is 252.60574381265135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.296874554487911 and perplexity of 199.71164565855793
Finished 17 epochs...
Completing Train Step...
At time: 381.3836052417755 and batch: 50, loss is 5.51936149597168 and perplexity is 249.47569508853852
At time: 382.20832109451294 and batch: 100, loss is 5.5317491436004635 and perplexity is 252.58533287443387
At time: 383.03335881233215 and batch: 150, loss is 5.453061771392822 and perplexity is 233.47191030499306
At time: 383.85908579826355 and batch: 200, loss is 5.484982566833496 and perplexity is 241.0447417882801
At time: 384.7048604488373 and batch: 250, loss is 5.493827886581421 and perplexity is 243.18631709915195
At time: 385.5273697376251 and batch: 300, loss is 5.5052801132202145 and perplexity is 245.98735033805426
At time: 386.620224237442 and batch: 350, loss is 5.529422054290771 and perplexity is 251.99822763462842
At time: 387.44729471206665 and batch: 400, loss is 5.506611013412476 and perplexity is 246.3149529047158
At time: 388.28047132492065 and batch: 450, loss is 5.47918779373169 and perplexity is 239.65198146459923
At time: 389.1074023246765 and batch: 500, loss is 5.4781941795349125 and perplexity is 239.41397811486493
At time: 389.93278455734253 and batch: 550, loss is 5.490166606903077 and perplexity is 242.29757194339425
At time: 390.7569806575775 and batch: 600, loss is 5.5075494575500485 and perplexity is 246.54621422420826
At time: 391.5816171169281 and batch: 650, loss is 5.507484512329102 and perplexity is 246.53020274579197
At time: 392.406311750412 and batch: 700, loss is 5.526916494369507 and perplexity is 251.36762131609964
At time: 393.22964453697205 and batch: 750, loss is 5.500416564941406 and perplexity is 244.7938835778438
At time: 394.0582983493805 and batch: 800, loss is 5.508521699905396 and perplexity is 246.78603345854532
At time: 394.88554549217224 and batch: 850, loss is 5.538784227371216 and perplexity is 254.36855706157328
At time: 395.70963525772095 and batch: 900, loss is 5.52233401298523 and perplexity is 250.2183690953701
At time: 396.54136061668396 and batch: 950, loss is 5.5062265300750735 and perplexity is 246.22026711329184
At time: 397.3689441680908 and batch: 1000, loss is 5.496105194091797 and perplexity is 243.7407582023297
At time: 398.1936559677124 and batch: 1050, loss is 5.496256608963012 and perplexity is 243.77766697204063
At time: 399.02046608924866 and batch: 1100, loss is 5.477270946502686 and perplexity is 239.19304522385679
At time: 399.8466281890869 and batch: 1150, loss is 5.524449243545532 and perplexity is 250.74819879486867
At time: 400.67623233795166 and batch: 1200, loss is 5.525344676971436 and perplexity is 250.97282766864683
At time: 401.50220489501953 and batch: 1250, loss is 5.523569793701172 and perplexity is 250.52777527038893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2907229235572535 and perplexity of 198.4868643854964
Finished 18 epochs...
Completing Train Step...
At time: 403.95978808403015 and batch: 50, loss is 5.5070607948303225 and perplexity is 246.42576571237282
At time: 404.7837088108063 and batch: 100, loss is 5.51968936920166 and perplexity is 249.55750490137967
At time: 405.6079754829407 and batch: 150, loss is 5.439819850921631 and perplexity is 230.4006732527994
At time: 406.4938497543335 and batch: 200, loss is 5.472199087142944 and perplexity is 237.9829630160263
At time: 407.3167676925659 and batch: 250, loss is 5.4791520404815675 and perplexity is 239.64341328053476
At time: 408.14123344421387 and batch: 300, loss is 5.490618515014648 and perplexity is 242.40709292641603
At time: 408.96517395973206 and batch: 350, loss is 5.518228874206543 and perplexity is 249.1932934437229
At time: 409.7886517047882 and batch: 400, loss is 5.492473955154419 and perplexity is 242.8572822973575
At time: 410.6175377368927 and batch: 450, loss is 5.463103141784668 and perplexity is 235.82809811203697
At time: 411.454713344574 and batch: 500, loss is 5.46442177772522 and perplexity is 236.13927463718503
At time: 412.27965474128723 and batch: 550, loss is 5.475554504394531 and perplexity is 238.78283635965286
At time: 413.1052157878876 and batch: 600, loss is 5.492556171417236 and perplexity is 242.87724993632705
At time: 413.9300329685211 and batch: 650, loss is 5.49242392539978 and perplexity is 242.8451325110399
At time: 414.75232577323914 and batch: 700, loss is 5.5129781913757325 and perplexity is 247.8882875804387
At time: 415.5802149772644 and batch: 750, loss is 5.487360172271728 and perplexity is 241.61853293138049
At time: 416.402872800827 and batch: 800, loss is 5.496565647125244 and perplexity is 243.85301521637672
At time: 417.2378799915314 and batch: 850, loss is 5.526398544311523 and perplexity is 251.23745915372342
At time: 418.0603325366974 and batch: 900, loss is 5.509338397979736 and perplexity is 246.98766546187204
At time: 418.8867871761322 and batch: 950, loss is 5.492212381362915 and perplexity is 242.79376550475888
At time: 419.71071672439575 and batch: 1000, loss is 5.482425298690796 and perplexity is 240.4291132482641
At time: 420.5413603782654 and batch: 1050, loss is 5.482518291473388 and perplexity is 240.45147246012786
At time: 421.36363530158997 and batch: 1100, loss is 5.462612180709839 and perplexity is 235.71234411319207
At time: 422.1884481906891 and batch: 1150, loss is 5.512179851531982 and perplexity is 247.69046745801373
At time: 423.0140986442566 and batch: 1200, loss is 5.5091531944274905 and perplexity is 246.94192670448862
At time: 423.8460741043091 and batch: 1250, loss is 5.506897687911987 and perplexity is 246.38557524288998
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.283005317632299 and perplexity of 196.960916889569
Finished 19 epochs...
Completing Train Step...
At time: 426.32112431526184 and batch: 50, loss is 5.4918771171569825 and perplexity is 242.71237908949985
At time: 427.14397406578064 and batch: 100, loss is 5.503987598419189 and perplexity is 245.6696134309391
At time: 427.9691789150238 and batch: 150, loss is 5.427407703399658 and perplexity is 227.55858083040198
At time: 428.79616808891296 and batch: 200, loss is 5.458214750289917 and perplexity is 234.6780911751493
At time: 429.6248435974121 and batch: 250, loss is 5.462664871215821 and perplexity is 235.72476424307806
At time: 430.44789361953735 and batch: 300, loss is 5.477828893661499 and perplexity is 239.32653954177192
At time: 431.2712519168854 and batch: 350, loss is 5.504151659011841 and perplexity is 245.70992143970258
At time: 432.095427274704 and batch: 400, loss is 5.48068546295166 and perplexity is 240.01116976623683
At time: 432.92680191993713 and batch: 450, loss is 5.455630970001221 and perplexity is 234.07251722090217
At time: 433.75382471084595 and batch: 500, loss is 5.45639331817627 and perplexity is 234.25103001303964
At time: 434.57989144325256 and batch: 550, loss is 5.46612440109253 and perplexity is 236.5416733536403
At time: 435.4032175540924 and batch: 600, loss is 5.482746553421021 and perplexity is 240.506364646203
At time: 436.2286274433136 and batch: 650, loss is 5.483904514312744 and perplexity is 240.7850229172268
At time: 437.0530025959015 and batch: 700, loss is 5.502252893447876 and perplexity is 245.2438185522415
At time: 437.8877737522125 and batch: 750, loss is 5.477488946914673 and perplexity is 239.2451950903997
At time: 438.7087345123291 and batch: 800, loss is 5.485945796966552 and perplexity is 241.2770352050343
At time: 439.5355591773987 and batch: 850, loss is 5.51555908203125 and perplexity is 248.52888644769695
At time: 440.3619990348816 and batch: 900, loss is 5.499793767929077 and perplexity is 244.64147414351163
At time: 441.1881160736084 and batch: 950, loss is 5.480077562332153 and perplexity is 239.8653111657058
At time: 442.01477670669556 and batch: 1000, loss is 5.473012819290161 and perplexity is 238.1766962162986
At time: 442.84093022346497 and batch: 1050, loss is 5.469871234893799 and perplexity is 237.42961814274162
At time: 443.74913454055786 and batch: 1100, loss is 5.450621900558471 and perplexity is 232.90296336117055
At time: 444.57074546813965 and batch: 1150, loss is 5.502414560317993 and perplexity is 245.28346955784295
At time: 445.40186500549316 and batch: 1200, loss is 5.49858395576477 and perplexity is 244.34568287421453
At time: 446.2304587364197 and batch: 1250, loss is 5.49872239112854 and perplexity is 244.37951129917988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.277118766394845 and perplexity of 195.80490216859943
Finished 20 epochs...
Completing Train Step...
At time: 448.69932889938354 and batch: 50, loss is 5.483275375366211 and perplexity is 240.63358332483563
At time: 449.5790786743164 and batch: 100, loss is 5.494065313339234 and perplexity is 243.24406289289217
At time: 450.40448784828186 and batch: 150, loss is 5.416461229324341 and perplexity is 225.0812007508177
At time: 451.22385573387146 and batch: 200, loss is 5.4466562080383305 and perplexity is 231.9811708024421
At time: 452.0613169670105 and batch: 250, loss is 5.454061985015869 and perplexity is 233.70554891506097
At time: 452.9207253456116 and batch: 300, loss is 5.468125333786011 and perplexity is 237.01545116190414
At time: 453.75453305244446 and batch: 350, loss is 5.4944438648223874 and perplexity is 243.3361607244542
At time: 454.5804913043976 and batch: 400, loss is 5.470909595489502 and perplexity is 237.67628374423154
At time: 455.4324297904968 and batch: 450, loss is 5.443916749954224 and perplexity is 231.34653778089796
At time: 456.26071643829346 and batch: 500, loss is 5.445056314468384 and perplexity is 231.61032235705198
At time: 457.0916533470154 and batch: 550, loss is 5.456710138320923 and perplexity is 234.32525721596926
At time: 457.91640305519104 and batch: 600, loss is 5.473663692474365 and perplexity is 238.33176950201525
At time: 458.74080419540405 and batch: 650, loss is 5.474147787094116 and perplexity is 238.44717256009304
At time: 459.5701515674591 and batch: 700, loss is 5.493696689605713 and perplexity is 243.15441388266342
At time: 460.3933243751526 and batch: 750, loss is 5.469284725189209 and perplexity is 237.29040419669974
At time: 461.22856283187866 and batch: 800, loss is 5.478461761474609 and perplexity is 239.4780495433156
At time: 462.05645847320557 and batch: 850, loss is 5.506312580108642 and perplexity is 246.24145528715073
At time: 462.88518619537354 and batch: 900, loss is 5.492725143432617 and perplexity is 242.9182928621942
At time: 463.7122759819031 and batch: 950, loss is 5.474698295593262 and perplexity is 238.5784758936691
At time: 464.613028049469 and batch: 1000, loss is 5.466313343048096 and perplexity is 236.58637022240066
At time: 465.44343733787537 and batch: 1050, loss is 5.4595622634887695 and perplexity is 234.99453615950264
At time: 466.27067613601685 and batch: 1100, loss is 5.439066619873047 and perplexity is 230.22719365540226
At time: 467.098268032074 and batch: 1150, loss is 5.491624021530152 and perplexity is 242.65095742088036
At time: 467.92373037338257 and batch: 1200, loss is 5.492309312820435 and perplexity is 242.8173009989725
At time: 468.7534284591675 and batch: 1250, loss is 5.4916097450256345 and perplexity is 242.6474932381189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.272642260920392 and perplexity of 194.93033940507013
Finished 21 epochs...
Completing Train Step...
At time: 471.2119941711426 and batch: 50, loss is 5.474778127670288 and perplexity is 238.59752286920315
At time: 472.0936770439148 and batch: 100, loss is 5.483753261566162 and perplexity is 240.7486062753026
At time: 472.9211699962616 and batch: 150, loss is 5.4098748683929445 and perplexity is 223.6036060513208
At time: 473.74416756629944 and batch: 200, loss is 5.43864556312561 and perplexity is 230.13027534755594
At time: 474.5741970539093 and batch: 250, loss is 5.447432746887207 and perplexity is 232.16138315566891
At time: 475.3964591026306 and batch: 300, loss is 5.459033470153809 and perplexity is 234.87030546410605
At time: 476.22567343711853 and batch: 350, loss is 5.482590389251709 and perplexity is 240.46880910204538
At time: 477.0589153766632 and batch: 400, loss is 5.46004584312439 and perplexity is 235.1082022127507
At time: 477.88445377349854 and batch: 450, loss is 5.436231517791748 and perplexity is 229.57540044622246
At time: 478.71235060691833 and batch: 500, loss is 5.436761465072632 and perplexity is 229.6970955485928
At time: 479.53574800491333 and batch: 550, loss is 5.448640203475952 and perplexity is 232.44187725555787
At time: 480.36349391937256 and batch: 600, loss is 5.463853750228882 and perplexity is 236.00517912480043
At time: 481.1894721984863 and batch: 650, loss is 5.466719923019409 and perplexity is 236.68258105939296
At time: 482.01707553863525 and batch: 700, loss is 5.487764358520508 and perplexity is 241.71621155869084
At time: 482.8440511226654 and batch: 750, loss is 5.4562147426605225 and perplexity is 234.20920224935918
At time: 483.67809104919434 and batch: 800, loss is 5.468668231964111 and perplexity is 237.1441613536245
At time: 484.5065267086029 and batch: 850, loss is 5.497813758850097 and perplexity is 244.1575610378796
At time: 485.33253717422485 and batch: 900, loss is 5.483411817550659 and perplexity is 240.6664181365719
At time: 486.21537017822266 and batch: 950, loss is 5.463798828125 and perplexity is 235.99221757977685
At time: 487.0413749217987 and batch: 1000, loss is 5.455018711090088 and perplexity is 233.92924809975594
At time: 487.87241101264954 and batch: 1050, loss is 5.448012781143189 and perplexity is 232.2960837725247
At time: 488.6981384754181 and batch: 1100, loss is 5.431863327026367 and perplexity is 228.57475839083867
At time: 489.525532245636 and batch: 1150, loss is 5.483798313140869 and perplexity is 240.75945262344456
At time: 490.3549816608429 and batch: 1200, loss is 5.484651956558228 and perplexity is 240.9650630918691
At time: 491.18030285835266 and batch: 1250, loss is 5.485933704376221 and perplexity is 241.2741175583321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.272063095204151 and perplexity of 194.81747512214804
Finished 22 epochs...
Completing Train Step...
At time: 493.65254187583923 and batch: 50, loss is 5.470512952804565 and perplexity is 237.58202987869032
At time: 494.4756672382355 and batch: 100, loss is 5.480346775054931 and perplexity is 239.92989465218287
At time: 495.3058521747589 and batch: 150, loss is 5.40242018699646 and perplexity is 221.94291007950036
At time: 496.13235330581665 and batch: 200, loss is 5.4273614501953125 and perplexity is 227.5480557602732
At time: 496.9651143550873 and batch: 250, loss is 5.4363789463043215 and perplexity is 229.60924890108598
At time: 497.79355692863464 and batch: 300, loss is 5.45238600730896 and perplexity is 233.31419166971475
At time: 498.61983132362366 and batch: 350, loss is 5.474065752029419 and perplexity is 238.4276123331883
At time: 499.4508891105652 and batch: 400, loss is 5.447536249160766 and perplexity is 232.18541363024102
At time: 500.2747800350189 and batch: 450, loss is 5.425820455551148 and perplexity is 227.19767546147477
At time: 501.1052656173706 and batch: 500, loss is 5.428434638977051 and perplexity is 227.79238886534466
At time: 501.9283812046051 and batch: 550, loss is 5.439299736022949 and perplexity is 230.28086958851048
At time: 502.7611434459686 and batch: 600, loss is 5.456583318710327 and perplexity is 234.2955420623682
At time: 503.591210603714 and batch: 650, loss is 5.460138359069824 and perplexity is 235.12995447655788
At time: 504.415301322937 and batch: 700, loss is 5.479957323074341 and perplexity is 239.83647167257084
At time: 505.2407510280609 and batch: 750, loss is 5.448477296829224 and perplexity is 232.4040140129457
At time: 506.0644073486328 and batch: 800, loss is 5.463701028823852 and perplexity is 235.9691388343821
At time: 506.89928913116455 and batch: 850, loss is 5.488074007034302 and perplexity is 241.7910702136975
At time: 507.78246092796326 and batch: 900, loss is 5.474226951599121 and perplexity is 238.4660498596747
At time: 508.6088402271271 and batch: 950, loss is 5.4569142723083495 and perplexity is 234.3730958476587
At time: 509.43804001808167 and batch: 1000, loss is 5.447290897369385 and perplexity is 232.1284535109936
At time: 510.2710952758789 and batch: 1050, loss is 5.439155597686767 and perplexity is 230.24767967914002
At time: 511.1103427410126 and batch: 1100, loss is 5.424189147949218 and perplexity is 226.82734830722984
At time: 511.9419288635254 and batch: 1150, loss is 5.475029716491699 and perplexity is 238.65755889065258
At time: 512.7716245651245 and batch: 1200, loss is 5.477858238220215 and perplexity is 239.33356257650706
At time: 513.6007533073425 and batch: 1250, loss is 5.4779089736938475 and perplexity is 239.34570558619873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.268964113110173 and perplexity of 194.21467377287064
Finished 23 epochs...
Completing Train Step...
At time: 516.0871660709381 and batch: 50, loss is 5.461856498718261 and perplexity is 235.53428782499174
At time: 516.9735553264618 and batch: 100, loss is 5.471229705810547 and perplexity is 237.75237855444297
At time: 517.8072414398193 and batch: 150, loss is 5.3993513298034665 and perplexity is 221.26284303152752
At time: 518.6376075744629 and batch: 200, loss is 5.425900402069092 and perplexity is 227.2158398505931
At time: 519.4629166126251 and batch: 250, loss is 5.429601125717163 and perplexity is 228.05826070429436
At time: 520.2903761863708 and batch: 300, loss is 5.445079383850097 and perplexity is 231.61566552561877
At time: 521.1181671619415 and batch: 350, loss is 5.464783515930176 and perplexity is 236.2247106863217
At time: 521.9433856010437 and batch: 400, loss is 5.438532800674438 and perplexity is 230.1043267566597
At time: 522.7738864421844 and batch: 450, loss is 5.418683090209961 and perplexity is 225.5818558538858
At time: 523.5994069576263 and batch: 500, loss is 5.419006290435791 and perplexity is 225.65477574386873
At time: 524.4305245876312 and batch: 550, loss is 5.43057110786438 and perplexity is 228.27958046645935
At time: 525.2664129734039 and batch: 600, loss is 5.446160726547241 and perplexity is 231.8662568972357
At time: 526.094241142273 and batch: 650, loss is 5.451263952255249 and perplexity is 233.05254711909532
At time: 526.9188113212585 and batch: 700, loss is 5.469135932922363 and perplexity is 237.25509984613078
At time: 527.7481698989868 and batch: 750, loss is 5.435816068649292 and perplexity is 229.48004335236087
At time: 528.628499507904 and batch: 800, loss is 5.4483017730712895 and perplexity is 232.36322516685763
At time: 529.4936621189117 and batch: 850, loss is 5.476657953262329 and perplexity is 239.0464664344876
At time: 530.3377590179443 and batch: 900, loss is 5.464119777679444 and perplexity is 236.06797133277755
At time: 531.1631002426147 and batch: 950, loss is 5.4487933254241945 and perplexity is 232.47747193375014
At time: 531.989176273346 and batch: 1000, loss is 5.437112274169922 and perplexity is 229.77768951505425
At time: 532.8150250911713 and batch: 1050, loss is 5.429086923599243 and perplexity is 227.94102280820155
At time: 533.6416680812836 and batch: 1100, loss is 5.413121948242187 and perplexity is 224.3308448770446
At time: 534.4714324474335 and batch: 1150, loss is 5.466353197097778 and perplexity is 236.59579933524643
At time: 535.3008980751038 and batch: 1200, loss is 5.468510551452637 and perplexity is 237.1067712888888
At time: 536.1265785694122 and batch: 1250, loss is 5.467651271820069 and perplexity is 236.9031177797634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.262232425439096 and perplexity of 192.91167186934072
Finished 24 epochs...
Completing Train Step...
At time: 538.6255166530609 and batch: 50, loss is 5.450676164627075 and perplexity is 232.9156019664604
At time: 539.448881149292 and batch: 100, loss is 5.460929679870605 and perplexity is 235.31609133769385
At time: 540.2741873264313 and batch: 150, loss is 5.386339960098266 and perplexity is 218.4025588390795
At time: 541.1024947166443 and batch: 200, loss is 5.414705457687378 and perplexity is 224.68635629230994
At time: 541.92711353302 and batch: 250, loss is 5.424368104934692 and perplexity is 226.8679442780639
At time: 542.7525084018707 and batch: 300, loss is 5.437707576751709 and perplexity is 229.9145174898421
At time: 543.5780148506165 and batch: 350, loss is 5.456979341506958 and perplexity is 234.38834681335968
At time: 544.4030170440674 and batch: 400, loss is 5.432594919204712 and perplexity is 228.74204308086874
At time: 545.2273862361908 and batch: 450, loss is 5.408016204833984 and perplexity is 223.18838817180733
At time: 546.0535349845886 and batch: 500, loss is 5.407827472686767 and perplexity is 223.14626932278915
At time: 546.8816995620728 and batch: 550, loss is 5.42227334022522 and perplexity is 226.39320672001836
At time: 547.7059009075165 and batch: 600, loss is 5.439533834457397 and perplexity is 230.33478428999186
At time: 548.5391895771027 and batch: 650, loss is 5.442571868896485 and perplexity is 231.0356133295296
At time: 549.3650987148285 and batch: 700, loss is 5.461957769393921 and perplexity is 235.55814174929188
At time: 550.244877576828 and batch: 750, loss is 5.426803426742554 and perplexity is 227.42111403005464
At time: 551.0699343681335 and batch: 800, loss is 5.441453905105591 and perplexity is 230.7774682047581
At time: 551.897896528244 and batch: 850, loss is 5.4716079139709475 and perplexity is 237.84231545052884
At time: 552.7242884635925 and batch: 900, loss is 5.45411509513855 and perplexity is 233.71796137504583
At time: 553.5508809089661 and batch: 950, loss is 5.439990167617798 and perplexity is 230.43991767615782
At time: 554.3781945705414 and batch: 1000, loss is 5.428527030944824 and perplexity is 227.81343602467498
At time: 555.2121648788452 and batch: 1050, loss is 5.420618648529053 and perplexity is 226.01890552273477
At time: 556.0359692573547 and batch: 1100, loss is 5.404510097503662 and perplexity is 222.40723593000735
At time: 556.8606934547424 and batch: 1150, loss is 5.458067979812622 and perplexity is 234.64364988724074
At time: 557.6862394809723 and batch: 1200, loss is 5.461946840286255 and perplexity is 235.55556732306724
At time: 558.512354850769 and batch: 1250, loss is 5.4628520011901855 and perplexity is 235.76887953968648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.259545096515739 and perplexity of 192.39395070896765
Finished 25 epochs...
Completing Train Step...
At time: 560.9310791492462 and batch: 50, loss is 5.444610242843628 and perplexity is 231.50703060372078
At time: 561.7853231430054 and batch: 100, loss is 5.452916803359986 and perplexity is 233.43806679460636
At time: 562.6115758419037 and batch: 150, loss is 5.3788450241088865 and perplexity is 216.77176462381277
At time: 563.4414863586426 and batch: 200, loss is 5.404590177536011 and perplexity is 222.42504702180202
At time: 564.2763228416443 and batch: 250, loss is 5.417430820465088 and perplexity is 225.29954332342103
At time: 565.0997693538666 and batch: 300, loss is 5.431233720779419 and perplexity is 228.43089158953208
At time: 565.9241087436676 and batch: 350, loss is 5.4507372760772705 and perplexity is 232.92983621160295
At time: 566.7501151561737 and batch: 400, loss is 5.425686616897583 and perplexity is 227.16726966527867
At time: 567.5802690982819 and batch: 450, loss is 5.39958532333374 and perplexity is 221.31462316315853
At time: 568.4014120101929 and batch: 500, loss is 5.400021486282348 and perplexity is 221.41117345606702
At time: 569.2236621379852 and batch: 550, loss is 5.413174686431884 and perplexity is 224.34267599166967
At time: 570.047355890274 and batch: 600, loss is 5.427312917709351 and perplexity is 227.5370125554307
At time: 570.8766489028931 and batch: 650, loss is 5.435038709640503 and perplexity is 229.30172429126773
At time: 571.7589502334595 and batch: 700, loss is 5.454841175079346 and perplexity is 233.8877209206463
At time: 572.5820190906525 and batch: 750, loss is 5.420297260284424 and perplexity is 225.94627737497748
At time: 573.4117727279663 and batch: 800, loss is 5.4361605548858645 and perplexity is 229.55910968671398
At time: 574.23761677742 and batch: 850, loss is 5.462421188354492 and perplexity is 235.66732915629865
At time: 575.0620431900024 and batch: 900, loss is 5.445080900192261 and perplexity is 231.61601673448448
At time: 575.8858687877655 and batch: 950, loss is 5.432269849777222 and perplexity is 228.667698120173
At time: 576.7150721549988 and batch: 1000, loss is 5.418931617736816 and perplexity is 225.63792612183846
At time: 577.5421793460846 and batch: 1050, loss is 5.4100343704223635 and perplexity is 223.63927412476082
At time: 578.3684031963348 and batch: 1100, loss is 5.394847097396851 and perplexity is 220.26846489983808
At time: 579.1990916728973 and batch: 1150, loss is 5.4500141429901126 and perplexity is 232.76145782737984
At time: 580.0240557193756 and batch: 1200, loss is 5.449029130935669 and perplexity is 232.5322978668003
At time: 580.8510448932648 and batch: 1250, loss is 5.449622354507446 and perplexity is 232.67028243089584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.251211347371123 and perplexity of 190.79725027831753
Finished 26 epochs...
Completing Train Step...
At time: 583.3549430370331 and batch: 50, loss is 5.43339916229248 and perplexity is 228.92608128370497
At time: 584.2079236507416 and batch: 100, loss is 5.440752716064453 and perplexity is 230.61570629257918
At time: 585.038017988205 and batch: 150, loss is 5.364124593734741 and perplexity is 213.60416238871792
At time: 585.8601677417755 and batch: 200, loss is 5.397782163619995 and perplexity is 220.91591712397067
At time: 586.6920955181122 and batch: 250, loss is 5.408420658111572 and perplexity is 223.27867570423632
At time: 587.5159261226654 and batch: 300, loss is 5.420310945510864 and perplexity is 225.94936952210497
At time: 588.3411862850189 and batch: 350, loss is 5.43837387084961 and perplexity is 230.06775922222886
At time: 589.16743516922 and batch: 400, loss is 5.414816417694092 and perplexity is 224.71128887514675
At time: 589.9923498630524 and batch: 450, loss is 5.38270432472229 and perplexity is 217.6099684278572
At time: 590.8159589767456 and batch: 500, loss is 5.3889045238494875 and perplexity is 218.96338495443644
At time: 591.6399691104889 and batch: 550, loss is 5.400102691650391 and perplexity is 221.42915396194314
At time: 592.4658567905426 and batch: 600, loss is 5.414825048446655 and perplexity is 224.7132283110486
At time: 593.3513491153717 and batch: 650, loss is 5.421582374572754 and perplexity is 226.2368308216321
At time: 594.1767590045929 and batch: 700, loss is 5.4398568248748775 and perplexity is 230.40919223400948
At time: 595.0024492740631 and batch: 750, loss is 5.402906446456909 and perplexity is 222.05085816246742
At time: 595.8261594772339 and batch: 800, loss is 5.417398204803467 and perplexity is 225.292195149586
At time: 596.6582629680634 and batch: 850, loss is 5.449320678710937 and perplexity is 232.60010202451681
At time: 597.4823553562164 and batch: 900, loss is 5.430144100189209 and perplexity is 228.1821241422954
At time: 598.3069498538971 and batch: 950, loss is 5.41612117767334 and perplexity is 225.00467452906446
At time: 599.1373536586761 and batch: 1000, loss is 5.400092554092407 and perplexity is 221.42690922243378
At time: 599.9751381874084 and batch: 1050, loss is 5.394071817398071 and perplexity is 220.0977613447156
At time: 600.8080847263336 and batch: 1100, loss is 5.380472135543823 and perplexity is 217.12476354721588
At time: 601.6469128131866 and batch: 1150, loss is 5.435462741851807 and perplexity is 229.39897622599398
At time: 602.4679753780365 and batch: 1200, loss is 5.435781755447388 and perplexity is 229.4721692923933
At time: 603.3049681186676 and batch: 1250, loss is 5.433836526870728 and perplexity is 229.02622734126817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.236146801579608 and perplexity of 187.94451787342553
Finished 27 epochs...
Completing Train Step...
At time: 605.7789862155914 and batch: 50, loss is 5.413940601348877 and perplexity is 224.5145692131178
At time: 606.6116104125977 and batch: 100, loss is 5.417305154800415 and perplexity is 225.27123268543355
At time: 607.4441690444946 and batch: 150, loss is 5.344531602859497 and perplexity is 209.45975126943438
At time: 608.269003868103 and batch: 200, loss is 5.375324964523315 and perplexity is 216.01005651135438
At time: 609.0926425457001 and batch: 250, loss is 5.387444067001343 and perplexity is 218.64383178299437
At time: 609.9227373600006 and batch: 300, loss is 5.395832023620605 and perplexity is 220.48551996124834
At time: 610.7500140666962 and batch: 350, loss is 5.413836746215821 and perplexity is 224.4912534334118
At time: 611.5765900611877 and batch: 400, loss is 5.389290533065796 and perplexity is 219.04792315427537
At time: 612.399254322052 and batch: 450, loss is 5.358312959671021 and perplexity is 212.36637343451972
At time: 613.2318694591522 and batch: 500, loss is 5.365180768966675 and perplexity is 213.8298849948009
At time: 614.1118569374084 and batch: 550, loss is 5.373325023651123 and perplexity is 215.57848087725782
At time: 614.9358034133911 and batch: 600, loss is 5.387126035690308 and perplexity is 218.57430725459722
At time: 615.7602701187134 and batch: 650, loss is 5.393850088119507 and perplexity is 220.0489646369083
At time: 616.5945053100586 and batch: 700, loss is 5.4138194274902345 and perplexity is 224.48736556466366
At time: 617.4202969074249 and batch: 750, loss is 5.37616084098816 and perplexity is 216.19068971676725
At time: 618.2452034950256 and batch: 800, loss is 5.393847455978394 and perplexity is 220.04838543774386
At time: 619.0721321105957 and batch: 850, loss is 5.423877115249634 and perplexity is 226.75658179870447
At time: 619.8996376991272 and batch: 900, loss is 5.403274183273315 and perplexity is 222.13252945397826
At time: 620.7310583591461 and batch: 950, loss is 5.390427474975586 and perplexity is 219.2971095467461
At time: 621.5563678741455 and batch: 1000, loss is 5.37498291015625 and perplexity is 215.93618196347077
At time: 622.3812186717987 and batch: 1050, loss is 5.366993436813354 and perplexity is 214.21783906169452
At time: 623.2080872058868 and batch: 1100, loss is 5.353047142028808 and perplexity is 211.25103001347074
At time: 624.0404486656189 and batch: 1150, loss is 5.40838475227356 and perplexity is 223.2706588402019
At time: 624.8639523983002 and batch: 1200, loss is 5.411352930068969 and perplexity is 223.93435034206274
At time: 625.6893680095673 and batch: 1250, loss is 5.40832763671875 and perplexity is 223.25790697681774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.219330502252509 and perplexity of 184.8104124880351
Finished 28 epochs...
Completing Train Step...
At time: 628.1399788856506 and batch: 50, loss is 5.387882280349731 and perplexity is 218.73966542488148
At time: 629.0157668590546 and batch: 100, loss is 5.396977310180664 and perplexity is 220.73818372253461
At time: 629.841037273407 and batch: 150, loss is 5.3243554592132565 and perplexity is 205.27600905604794
At time: 630.6717505455017 and batch: 200, loss is 5.35559344291687 and perplexity is 211.78962411911823
At time: 631.5056285858154 and batch: 250, loss is 5.3668603515625 and perplexity is 214.1893317238407
At time: 632.3301792144775 and batch: 300, loss is 5.376763114929199 and perplexity is 216.32093495319788
At time: 633.1566443443298 and batch: 350, loss is 5.391794910430908 and perplexity is 219.59718931268634
At time: 633.9816007614136 and batch: 400, loss is 5.370679798126221 and perplexity is 215.00898073725142
At time: 634.8094041347504 and batch: 450, loss is 5.342310342788696 and perplexity is 208.99500304192887
At time: 635.6654005050659 and batch: 500, loss is 5.347714195251465 and perplexity is 210.12743820435003
At time: 636.4902038574219 and batch: 550, loss is 5.357141151428222 and perplexity is 212.11766651450444
At time: 637.3231093883514 and batch: 600, loss is 5.373135509490967 and perplexity is 215.53762957357918
At time: 638.1499638557434 and batch: 650, loss is 5.37834264755249 and perplexity is 216.6628909212647
At time: 638.9826440811157 and batch: 700, loss is 5.397663288116455 and perplexity is 220.8896571939446
At time: 639.8162760734558 and batch: 750, loss is 5.361746025085449 and perplexity is 213.09669398816993
At time: 640.6411221027374 and batch: 800, loss is 5.379815015792847 and perplexity is 216.98213344432412
At time: 641.4649269580841 and batch: 850, loss is 5.409040040969849 and perplexity is 223.4170135262015
At time: 642.2887108325958 and batch: 900, loss is 5.392725296020508 and perplexity is 219.80159444619005
At time: 643.114107131958 and batch: 950, loss is 5.379948482513428 and perplexity is 217.01109527077665
At time: 643.937522649765 and batch: 1000, loss is 5.3645023727417 and perplexity is 213.6848728014573
At time: 644.7646379470825 and batch: 1050, loss is 5.3556838226318355 and perplexity is 211.80876647000582
At time: 645.5908579826355 and batch: 1100, loss is 5.343626184463501 and perplexity is 209.27018838722458
At time: 646.4183006286621 and batch: 1150, loss is 5.3991889476776125 and perplexity is 221.22691681766997
At time: 647.2500231266022 and batch: 1200, loss is 5.397616167068481 and perplexity is 220.87924888703813
At time: 648.0797290802002 and batch: 1250, loss is 5.3974262046813966 and perplexity is 220.83729412270188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.209153669593978 and perplexity of 182.9391656794746
Finished 29 epochs...
Completing Train Step...
At time: 650.5586307048798 and batch: 50, loss is 5.379372835159302 and perplexity is 216.88620935653964
At time: 651.3856892585754 and batch: 100, loss is 5.386053276062012 and perplexity is 218.3399552861316
At time: 652.209710597992 and batch: 150, loss is 5.314315433502197 and perplexity is 203.22534423693315
At time: 653.0336186885834 and batch: 200, loss is 5.342988138198852 and perplexity is 209.1367069134281
At time: 653.8577535152435 and batch: 250, loss is 5.358839025497437 and perplexity is 212.4781215171101
At time: 654.6815106868744 and batch: 300, loss is 5.3668239402771 and perplexity is 214.18153295693608
At time: 655.5149581432343 and batch: 350, loss is 5.382146940231324 and perplexity is 217.48870980335346
At time: 656.339447259903 and batch: 400, loss is 5.355345840454102 and perplexity is 211.73719097815282
At time: 657.1966569423676 and batch: 450, loss is 5.328514490127564 and perplexity is 206.13153617233093
At time: 658.0205266475677 and batch: 500, loss is 5.337804403305054 and perplexity is 208.05540268677115
At time: 658.8452341556549 and batch: 550, loss is 5.34756817817688 and perplexity is 210.09675825048657
At time: 659.6713562011719 and batch: 600, loss is 5.360446424484253 and perplexity is 212.81993327469786
At time: 660.4985182285309 and batch: 650, loss is 5.367157020568848 and perplexity is 214.25288448665486
At time: 661.3210723400116 and batch: 700, loss is 5.383626041412353 and perplexity is 217.8106356326305
At time: 662.1432032585144 and batch: 750, loss is 5.3488943481445315 and perplexity is 210.3755670946748
At time: 662.9660222530365 and batch: 800, loss is 5.369221420288086 and perplexity is 214.69564494126976
At time: 663.7983238697052 and batch: 850, loss is 5.402823905944825 and perplexity is 222.03253072731417
At time: 664.6204085350037 and batch: 900, loss is 5.385771427154541 and perplexity is 218.27842507979506
At time: 665.4492557048798 and batch: 950, loss is 5.3690206336975095 and perplexity is 214.65254126217565
At time: 666.2782199382782 and batch: 1000, loss is 5.3543171787261965 and perplexity is 211.51949701933736
At time: 667.1071305274963 and batch: 1050, loss is 5.3446546459198 and perplexity is 209.4855254238736
At time: 667.9368612766266 and batch: 1100, loss is 5.333780031204224 and perplexity is 207.21979285897527
At time: 668.7596299648285 and batch: 1150, loss is 5.388345575332641 and perplexity is 218.841029893448
At time: 669.5840384960175 and batch: 1200, loss is 5.388553695678711 and perplexity is 218.88657990410127
At time: 670.4099147319794 and batch: 1250, loss is 5.388812551498413 and perplexity is 218.94324730319067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.205885838418111 and perplexity of 182.3423270855848
Finished 30 epochs...
Completing Train Step...
At time: 672.865526676178 and batch: 50, loss is 5.369080200195312 and perplexity is 214.66532774312225
At time: 673.7197458744049 and batch: 100, loss is 5.377762374877929 and perplexity is 216.53720383598912
At time: 674.5426499843597 and batch: 150, loss is 5.306274156570435 and perplexity is 201.59770588029457
At time: 675.3747191429138 and batch: 200, loss is 5.333142318725586 and perplexity is 207.08768833807602
At time: 676.2349848747253 and batch: 250, loss is 5.347936315536499 and perplexity is 210.17411695477426
At time: 677.0908241271973 and batch: 300, loss is 5.3544287014007566 and perplexity is 211.5430875547819
At time: 677.9434652328491 and batch: 350, loss is 5.372872304916382 and perplexity is 215.48090654868744
At time: 678.846971988678 and batch: 400, loss is 5.345554285049438 and perplexity is 209.67407159867946
At time: 679.6817142963409 and batch: 450, loss is 5.3191510200500485 and perplexity is 204.210437810797
At time: 680.5057299137115 and batch: 500, loss is 5.328261022567749 and perplexity is 206.07929513584017
At time: 681.3302385807037 and batch: 550, loss is 5.335677280426025 and perplexity is 207.6133136351703
At time: 682.1553716659546 and batch: 600, loss is 5.350440673828125 and perplexity is 210.70112788390747
At time: 682.9796025753021 and batch: 650, loss is 5.3582938766479495 and perplexity is 212.36232088078356
At time: 683.8061575889587 and batch: 700, loss is 5.377190189361572 and perplexity is 216.4133398241779
At time: 684.637186050415 and batch: 750, loss is 5.339487104415894 and perplexity is 208.40579246194355
At time: 685.4711623191833 and batch: 800, loss is 5.360352039337158 and perplexity is 212.7998471819204
At time: 686.2990367412567 and batch: 850, loss is 5.388336219787598 and perplexity is 218.8389825259126
At time: 687.1280252933502 and batch: 900, loss is 5.374551076889038 and perplexity is 215.8429536674927
At time: 687.9505724906921 and batch: 950, loss is 5.3579122257232665 and perplexity is 212.28128806875898
At time: 688.7775640487671 and batch: 1000, loss is 5.344940786361694 and perplexity is 209.54547628146233
At time: 689.6099762916565 and batch: 1050, loss is 5.3407770538330075 and perplexity is 208.67479885747298
At time: 690.4372882843018 and batch: 1100, loss is 5.323192701339722 and perplexity is 205.03746147368656
At time: 691.2751936912537 and batch: 1150, loss is 5.3808933448791505 and perplexity is 217.21623778810232
At time: 692.1035614013672 and batch: 1200, loss is 5.379352550506592 and perplexity is 216.88180993972583
At time: 692.9298100471497 and batch: 1250, loss is 5.380295028686524 and perplexity is 217.08631266775978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.201949293596031 and perplexity of 181.62593931345563
Finished 31 epochs...
Completing Train Step...
At time: 695.372555732727 and batch: 50, loss is 5.359964237213135 and perplexity is 212.71733894865955
At time: 696.2320308685303 and batch: 100, loss is 5.368112602233887 and perplexity is 214.45771846696326
At time: 697.0627326965332 and batch: 150, loss is 5.298017826080322 and perplexity is 199.94010087801047
At time: 697.8988392353058 and batch: 200, loss is 5.324906949996948 and perplexity is 205.38924810543134
At time: 698.7328855991364 and batch: 250, loss is 5.338473424911499 and perplexity is 208.19464281864117
At time: 699.6152064800262 and batch: 300, loss is 5.3479725360870365 and perplexity is 210.18172971486746
At time: 700.4437429904938 and batch: 350, loss is 5.364463510513306 and perplexity is 213.67656869248535
At time: 701.2714309692383 and batch: 400, loss is 5.335486612319946 and perplexity is 207.57373217144402
At time: 702.0965042114258 and batch: 450, loss is 5.311532354354858 and perplexity is 202.6605383335026
At time: 702.9226088523865 and batch: 500, loss is 5.320980796813965 and perplexity is 204.58443939020316
At time: 703.7459161281586 and batch: 550, loss is 5.328516283035278 and perplexity is 206.13190574748367
At time: 704.5786037445068 and batch: 600, loss is 5.347989768981933 and perplexity is 210.18535178573416
At time: 705.4110119342804 and batch: 650, loss is 5.3525730228424075 and perplexity is 211.15089558669666
At time: 706.2375390529633 and batch: 700, loss is 5.368435945510864 and perplexity is 214.5270731405049
At time: 707.0629906654358 and batch: 750, loss is 5.335351533889771 and perplexity is 207.54569533118544
At time: 707.8953959941864 and batch: 800, loss is 5.35539999961853 and perplexity is 211.74865879803568
At time: 708.7213108539581 and batch: 850, loss is 5.383514566421509 and perplexity is 217.78635654729808
At time: 709.5496287345886 and batch: 900, loss is 5.369050722122193 and perplexity is 214.65899991616138
At time: 710.3770153522491 and batch: 950, loss is 5.3534821510314945 and perplexity is 211.34294610406732
At time: 711.2043943405151 and batch: 1000, loss is 5.341590461730957 and perplexity is 208.8446056386926
At time: 712.0384283065796 and batch: 1050, loss is 5.337397289276123 and perplexity is 207.9707176529449
At time: 712.8657147884369 and batch: 1100, loss is 5.317158336639404 and perplexity is 203.80391622801565
At time: 713.6913895606995 and batch: 1150, loss is 5.373393268585205 and perplexity is 215.5931935185006
At time: 714.5156366825104 and batch: 1200, loss is 5.372650032043457 and perplexity is 215.4330163110759
At time: 715.3403050899506 and batch: 1250, loss is 5.372444257736206 and perplexity is 215.38869029211992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.199895037351734 and perplexity of 181.25321605910432
Finished 32 epochs...
Completing Train Step...
At time: 717.8301241397858 and batch: 50, loss is 5.350970087051391 and perplexity is 210.8127053798612
At time: 718.6518542766571 and batch: 100, loss is 5.36029619216919 and perplexity is 212.7879632449564
At time: 719.4791812896729 and batch: 150, loss is 5.2885025691986085 and perplexity is 198.04664211659806
At time: 720.3079173564911 and batch: 200, loss is 5.3166430377960205 and perplexity is 203.6989233593814
At time: 721.1653661727905 and batch: 250, loss is 5.331733226776123 and perplexity is 206.79608823753165
At time: 721.991724729538 and batch: 300, loss is 5.339646558761597 and perplexity is 208.43902632079252
At time: 722.8222622871399 and batch: 350, loss is 5.355493507385254 and perplexity is 211.76845986798904
At time: 723.6513617038727 and batch: 400, loss is 5.325603895187378 and perplexity is 205.53244304778133
At time: 724.4776237010956 and batch: 450, loss is 5.302363319396973 and perplexity is 200.81082975317344
At time: 725.3165085315704 and batch: 500, loss is 5.3100173282623295 and perplexity is 202.3537347963674
At time: 726.1496870517731 and batch: 550, loss is 5.318082475662232 and perplexity is 203.99234643444998
At time: 726.9762845039368 and batch: 600, loss is 5.334034128189087 and perplexity is 207.272453473713
At time: 727.8050937652588 and batch: 650, loss is 5.341711645126343 and perplexity is 208.86991567065877
At time: 728.6323328018188 and batch: 700, loss is 5.358522100448608 and perplexity is 212.41079254775534
At time: 729.457338809967 and batch: 750, loss is 5.324803228378296 and perplexity is 205.36794590493255
At time: 730.289226770401 and batch: 800, loss is 5.343142566680908 and perplexity is 209.16900607150959
At time: 731.1176702976227 and batch: 850, loss is 5.3711159038543705 and perplexity is 215.1027678344132
At time: 731.946585893631 and batch: 900, loss is 5.352992219924927 and perplexity is 211.23942798106216
At time: 732.7738857269287 and batch: 950, loss is 5.3384652519226075 and perplexity is 208.1929412530915
At time: 733.6019434928894 and batch: 1000, loss is 5.32775574684143 and perplexity is 205.97519457226988
At time: 734.4264078140259 and batch: 1050, loss is 5.323815746307373 and perplexity is 205.16524883674066
At time: 735.2604403495789 and batch: 1100, loss is 5.305725717544556 and perplexity is 201.48717214414003
At time: 736.084263086319 and batch: 1150, loss is 5.361400699615478 and perplexity is 213.023118976563
At time: 736.9118041992188 and batch: 1200, loss is 5.360384111404419 and perplexity is 212.80667222237827
At time: 737.7392375469208 and batch: 1250, loss is 5.360265121459961 and perplexity is 212.7813518747336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.190720161382299 and perplexity of 179.5978458032696
Finished 33 epochs...
Completing Train Step...
At time: 740.223245382309 and batch: 50, loss is 5.3394076538085935 and perplexity is 208.38923515292038
At time: 741.0815141201019 and batch: 100, loss is 5.348056850433349 and perplexity is 210.19945179711763
At time: 741.9053404331207 and batch: 150, loss is 5.27853645324707 and perplexity is 196.08268906489482
At time: 742.7627727985382 and batch: 200, loss is 5.306323776245117 and perplexity is 201.60770934105918
At time: 743.5904557704926 and batch: 250, loss is 5.32030198097229 and perplexity is 204.44561135645012
At time: 744.4198145866394 and batch: 300, loss is 5.329019050598145 and perplexity is 206.2355682402512
At time: 745.2452638149261 and batch: 350, loss is 5.340451822280884 and perplexity is 208.60694226390228
At time: 746.0706241130829 and batch: 400, loss is 5.312936267852783 and perplexity is 202.94525601149712
At time: 746.8991405963898 and batch: 450, loss is 5.2905041694641115 and perplexity is 198.44344932026192
At time: 747.724452495575 and batch: 500, loss is 5.299937686920166 and perplexity is 200.32432676009586
At time: 748.5524032115936 and batch: 550, loss is 5.308886985778809 and perplexity is 202.12513499560097
At time: 749.3782384395599 and batch: 600, loss is 5.327657947540283 and perplexity is 205.95505132720072
At time: 750.2158672809601 and batch: 650, loss is 5.332518558502198 and perplexity is 206.95855555347146
At time: 751.0483920574188 and batch: 700, loss is 5.347706451416015 and perplexity is 210.12581101834553
At time: 751.8804247379303 and batch: 750, loss is 5.311806621551514 and perplexity is 202.7161290942367
At time: 752.707117319107 and batch: 800, loss is 5.333222322463989 and perplexity is 207.1042567900805
At time: 753.5687186717987 and batch: 850, loss is 5.361904544830322 and perplexity is 213.13047669927678
At time: 754.4135890007019 and batch: 900, loss is 5.34442777633667 and perplexity is 209.43800492073123
At time: 755.2459380626678 and batch: 950, loss is 5.330347509384155 and perplexity is 206.50972575602853
At time: 756.0721752643585 and batch: 1000, loss is 5.317709312438965 and perplexity is 203.91623819421406
At time: 756.899423122406 and batch: 1050, loss is 5.314563217163086 and perplexity is 203.275706395916
At time: 757.7350625991821 and batch: 1100, loss is 5.294663591384888 and perplexity is 199.27057835020977
At time: 758.5610029697418 and batch: 1150, loss is 5.3501998233795165 and perplexity is 210.65038653351814
At time: 759.3849604129791 and batch: 1200, loss is 5.350107460021973 and perplexity is 210.63093105505055
At time: 760.2103793621063 and batch: 1250, loss is 5.34983865737915 and perplexity is 210.57432051299529
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.185817300838275 and perplexity of 178.71945767808984
Finished 34 epochs...
Completing Train Step...
At time: 762.6960854530334 and batch: 50, loss is 5.327536001205444 and perplexity is 205.92993739485692
At time: 763.5214898586273 and batch: 100, loss is 5.336098051071167 and perplexity is 207.7006896044209
At time: 764.3818802833557 and batch: 150, loss is 5.267041234970093 and perplexity is 193.84158144269077
At time: 765.2058255672455 and batch: 200, loss is 5.294778470993042 and perplexity is 199.29347179113688
At time: 766.0358788967133 and batch: 250, loss is 5.311222620010376 and perplexity is 202.5977771246622
At time: 766.87056016922 and batch: 300, loss is 5.31911636352539 and perplexity is 204.20336070935818
At time: 767.6946637630463 and batch: 350, loss is 5.331034355163574 and perplexity is 206.65161481194454
At time: 768.5199229717255 and batch: 400, loss is 5.305537185668945 and perplexity is 201.4491889702962
At time: 769.3450088500977 and batch: 450, loss is 5.280286598205566 and perplexity is 196.42616267123853
At time: 770.170512676239 and batch: 500, loss is 5.28930772781372 and perplexity is 198.20616528880245
At time: 770.9956800937653 and batch: 550, loss is 5.296523475646973 and perplexity is 199.64154343190268
At time: 771.8182191848755 and batch: 600, loss is 5.312590684890747 and perplexity is 202.87513370602835
At time: 772.6431102752686 and batch: 650, loss is 5.319896106719971 and perplexity is 204.36264898408518
At time: 773.4748337268829 and batch: 700, loss is 5.341590900421142 and perplexity is 208.84469725679148
At time: 774.3023800849915 and batch: 750, loss is 5.3064720153808596 and perplexity is 201.6375977089091
At time: 775.127640247345 and batch: 800, loss is 5.324311208724976 and perplexity is 205.266925693388
At time: 775.9555583000183 and batch: 850, loss is 5.351564235687256 and perplexity is 210.9379966783269
At time: 776.7803678512573 and batch: 900, loss is 5.33291262626648 and perplexity is 207.04012732010375
At time: 777.6037263870239 and batch: 950, loss is 5.320572338104248 and perplexity is 204.50089215802066
At time: 778.4265172481537 and batch: 1000, loss is 5.307248125076294 and perplexity is 201.79415134699494
At time: 779.252922296524 and batch: 1050, loss is 5.303439903259277 and perplexity is 201.02713586680517
At time: 780.0779402256012 and batch: 1100, loss is 5.285645208358765 and perplexity is 197.4815591044554
At time: 780.9134664535522 and batch: 1150, loss is 5.34081618309021 and perplexity is 208.68296430710205
At time: 781.7416565418243 and batch: 1200, loss is 5.340398855209351 and perplexity is 208.59589325768837
At time: 782.5664577484131 and batch: 1250, loss is 5.34183219909668 and perplexity is 208.89509728611822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.180925578096487 and perplexity of 177.8473464448008
Finished 35 epochs...
Completing Train Step...
At time: 785.0432357788086 and batch: 50, loss is 5.316507148742676 and perplexity is 203.67124478616887
At time: 785.8966698646545 and batch: 100, loss is 5.326101446151734 and perplexity is 205.63473135773967
At time: 786.7280087471008 and batch: 150, loss is 5.258183107376099 and perplexity is 192.13209060335916
At time: 787.5563836097717 and batch: 200, loss is 5.28460111618042 and perplexity is 197.27547775591052
At time: 788.3900241851807 and batch: 250, loss is 5.302194509506226 and perplexity is 200.77693376001196
At time: 789.2151246070862 and batch: 300, loss is 5.312337455749511 and perplexity is 202.82376631427667
At time: 790.0441846847534 and batch: 350, loss is 5.324370565414429 and perplexity is 205.2791100201585
At time: 790.8710842132568 and batch: 400, loss is 5.299795417785645 and perplexity is 200.29582881874094
At time: 791.6939764022827 and batch: 450, loss is 5.272318506240845 and perplexity is 194.86724001041534
At time: 792.5219945907593 and batch: 500, loss is 5.2786236667633055 and perplexity is 196.09979087142457
At time: 793.3461384773254 and batch: 550, loss is 5.285086612701416 and perplexity is 197.37127756739474
At time: 794.1714475154877 and batch: 600, loss is 5.304450721740722 and perplexity is 201.23044054576414
At time: 795.0036406517029 and batch: 650, loss is 5.311326417922974 and perplexity is 202.61880744245732
At time: 795.8290650844574 and batch: 700, loss is 5.332809267044067 and perplexity is 207.01872891941574
At time: 796.6557548046112 and batch: 750, loss is 5.298075952529907 and perplexity is 199.95172302397796
At time: 797.4838876724243 and batch: 800, loss is 5.314470624923706 and perplexity is 203.25688551439634
At time: 798.3114476203918 and batch: 850, loss is 5.341598148345947 and perplexity is 208.84621095293875
At time: 799.1379375457764 and batch: 900, loss is 5.32537733078003 and perplexity is 205.48588198637023
At time: 799.9647676944733 and batch: 950, loss is 5.3097654056549075 and perplexity is 202.30276373652666
At time: 800.7946033477783 and batch: 1000, loss is 5.2961496925354 and perplexity is 199.56693473920282
At time: 801.6202223300934 and batch: 1050, loss is 5.295687627792359 and perplexity is 199.47474319565413
At time: 802.4547019004822 and batch: 1100, loss is 5.27526294708252 and perplexity is 195.44186062365696
At time: 803.2804298400879 and batch: 1150, loss is 5.3300449085235595 and perplexity is 206.44724518910684
At time: 804.1048624515533 and batch: 1200, loss is 5.330778188705445 and perplexity is 206.5986843795016
At time: 804.9295969009399 and batch: 1250, loss is 5.334245262145996 and perplexity is 207.3162203471471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1740557816776915 and perplexity of 176.6297584604664
Finished 36 epochs...
Completing Train Step...
At time: 807.3674139976501 and batch: 50, loss is 5.307258605957031 and perplexity is 201.79626633851223
At time: 808.2227716445923 and batch: 100, loss is 5.314329414367676 and perplexity is 203.22818552299455
At time: 809.0491282939911 and batch: 150, loss is 5.246017608642578 and perplexity is 189.80886813135817
At time: 809.8836591243744 and batch: 200, loss is 5.272601394653321 and perplexity is 194.92237349252932
At time: 810.7102327346802 and batch: 250, loss is 5.289993124008179 and perplexity is 198.34206160629904
At time: 811.5346443653107 and batch: 300, loss is 5.29866247177124 and perplexity is 200.06903295577234
At time: 812.3629515171051 and batch: 350, loss is 5.3134829044342045 and perplexity is 203.05622363917803
At time: 813.1906630992889 and batch: 400, loss is 5.285583724975586 and perplexity is 197.4694176433392
At time: 814.0143237113953 and batch: 450, loss is 5.256388177871704 and perplexity is 191.78753636298637
At time: 814.8406357765198 and batch: 500, loss is 5.2679593372344975 and perplexity is 194.01962955823151
At time: 815.6706130504608 and batch: 550, loss is 5.274318733215332 and perplexity is 195.25740880331432
At time: 816.4977459907532 and batch: 600, loss is 5.2868748474121094 and perplexity is 197.72453950032187
At time: 817.3264083862305 and batch: 650, loss is 5.295074920654297 and perplexity is 199.3525610313997
At time: 818.1528885364532 and batch: 700, loss is 5.313440885543823 and perplexity is 203.04769162122992
At time: 818.9817764759064 and batch: 750, loss is 5.280685043334961 and perplexity is 196.50444331327515
At time: 819.8062236309052 and batch: 800, loss is 5.299610900878906 and perplexity is 200.2588742614498
At time: 820.6344468593597 and batch: 850, loss is 5.326301326751709 and perplexity is 205.6758378592785
At time: 821.4580714702606 and batch: 900, loss is 5.312928590774536 and perplexity is 202.94369799086743
At time: 822.2819118499756 and batch: 950, loss is 5.296740665435791 and perplexity is 199.68490824558677
At time: 823.1110665798187 and batch: 1000, loss is 5.283982496261597 and perplexity is 197.15347695582858
At time: 823.9352152347565 and batch: 1050, loss is 5.279770822525024 and perplexity is 196.32487695611022
At time: 824.7678384780884 and batch: 1100, loss is 5.2602341270446775 and perplexity is 192.52656169585194
At time: 825.6067469120026 and batch: 1150, loss is 5.3146543788909915 and perplexity is 203.29423820523522
At time: 826.4341578483582 and batch: 1200, loss is 5.316530103683472 and perplexity is 203.6759201011954
At time: 827.3152089118958 and batch: 1250, loss is 5.319557552337646 and perplexity is 204.293472824295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.162676957401916 and perplexity of 174.6313110309431
Finished 37 epochs...
Completing Train Step...
At time: 829.7907910346985 and batch: 50, loss is 5.29262191772461 and perplexity is 198.8641478994782
At time: 830.6193873882294 and batch: 100, loss is 5.300951919555664 and perplexity is 200.52760529841845
At time: 831.4526352882385 and batch: 150, loss is 5.231198530197144 and perplexity is 187.01681455097315
At time: 832.2823498249054 and batch: 200, loss is 5.258400802612305 and perplexity is 192.1739213972228
At time: 833.1085450649261 and batch: 250, loss is 5.274588041305542 and perplexity is 195.31000028451624
At time: 833.9355013370514 and batch: 300, loss is 5.2854351043701175 and perplexity is 197.44007179968006
At time: 834.7601912021637 and batch: 350, loss is 5.299283256530762 and perplexity is 200.19327132093554
At time: 835.5899152755737 and batch: 400, loss is 5.2699395942687985 and perplexity is 194.4042189617145
At time: 836.4191591739655 and batch: 450, loss is 5.241373586654663 and perplexity is 188.9294352077278
At time: 837.242837190628 and batch: 500, loss is 5.251581602096557 and perplexity is 190.86790694150756
At time: 838.0672824382782 and batch: 550, loss is 5.258747568130493 and perplexity is 192.24057224209798
At time: 838.8969879150391 and batch: 600, loss is 5.273453817367554 and perplexity is 195.08860058901763
At time: 839.7257573604584 and batch: 650, loss is 5.283039054870605 and perplexity is 196.96756191905095
At time: 840.552362203598 and batch: 700, loss is 5.300822725296021 and perplexity is 200.50169995636065
At time: 841.3827149868011 and batch: 750, loss is 5.272372169494629 and perplexity is 194.87769750115913
At time: 842.2075095176697 and batch: 800, loss is 5.288702297210693 and perplexity is 198.08620152915796
At time: 843.035635471344 and batch: 850, loss is 5.312893552780151 and perplexity is 202.93658737528833
At time: 843.8633046150208 and batch: 900, loss is 5.300524702072144 and perplexity is 200.44195469652652
At time: 844.6869003772736 and batch: 950, loss is 5.28235894203186 and perplexity is 196.83364729506283
At time: 845.5173153877258 and batch: 1000, loss is 5.270319156646728 and perplexity is 194.4780214947896
At time: 846.3452234268188 and batch: 1050, loss is 5.271350622177124 and perplexity is 194.67872236059938
At time: 847.1773769855499 and batch: 1100, loss is 5.248101177215577 and perplexity is 190.20476021480226
At time: 848.0073688030243 and batch: 1150, loss is 5.301778907775879 and perplexity is 200.6935078561034
At time: 848.8956985473633 and batch: 1200, loss is 5.304530515670776 and perplexity is 201.24649815410302
At time: 849.7223119735718 and batch: 1250, loss is 5.3105326175689695 and perplexity is 202.45803238147403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.161764994154882 and perplexity of 174.47212628985065
Finished 38 epochs...
Completing Train Step...
At time: 852.1698112487793 and batch: 50, loss is 5.281767330169678 and perplexity is 196.7172326139951
At time: 853.0555713176727 and batch: 100, loss is 5.28951771736145 and perplexity is 198.2477908821255
At time: 853.8802332878113 and batch: 150, loss is 5.21874062538147 and perplexity is 184.70142924668792
At time: 854.7042782306671 and batch: 200, loss is 5.245828971862793 and perplexity is 189.7730665745506
At time: 855.5289835929871 and batch: 250, loss is 5.262163543701172 and perplexity is 192.8983842357594
At time: 856.3654828071594 and batch: 300, loss is 5.270304841995239 and perplexity is 194.4752376296147
At time: 857.1920499801636 and batch: 350, loss is 5.283441066741943 and perplexity is 197.0467611356569
At time: 858.0182263851166 and batch: 400, loss is 5.25526683807373 and perplexity is 191.57259789775676
At time: 858.8420796394348 and batch: 450, loss is 5.227069454193115 and perplexity is 186.24619996762067
At time: 859.6657822132111 and batch: 500, loss is 5.237674579620362 and perplexity is 188.231874833646
At time: 860.4885723590851 and batch: 550, loss is 5.243201274871826 and perplexity is 189.27505505678477
At time: 861.3134593963623 and batch: 600, loss is 5.258193273544311 and perplexity is 192.13404386043973
At time: 862.1454594135284 and batch: 650, loss is 5.265432481765747 and perplexity is 193.52998888237664
At time: 862.9808166027069 and batch: 700, loss is 5.283697261810302 and perplexity is 197.0972500113203
At time: 863.8045012950897 and batch: 750, loss is 5.251226873397827 and perplexity is 190.80021262451712
At time: 864.6296298503876 and batch: 800, loss is 5.271792478561402 and perplexity is 194.7647614040079
At time: 865.4569129943848 and batch: 850, loss is 5.2979111480712895 and perplexity is 199.9187728037615
At time: 866.282933473587 and batch: 900, loss is 5.279196710586548 and perplexity is 196.21219684902212
At time: 867.1099457740784 and batch: 950, loss is 5.264642152786255 and perplexity is 193.3770969491832
At time: 867.9373743534088 and batch: 1000, loss is 5.2486168575286865 and perplexity is 190.30287035966577
At time: 868.7688300609589 and batch: 1050, loss is 5.250385284423828 and perplexity is 190.63970481960874
At time: 869.5928726196289 and batch: 1100, loss is 5.23004487991333 and perplexity is 186.80118695312322
At time: 870.477205991745 and batch: 1150, loss is 5.282450551986694 and perplexity is 196.85168004257844
At time: 871.3029220104218 and batch: 1200, loss is 5.287324752807617 and perplexity is 197.8135168516617
At time: 872.1291811466217 and batch: 1250, loss is 5.29069263458252 and perplexity is 198.48085251292326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1452391687100825 and perplexity of 171.61252410921702
Finished 39 epochs...
Completing Train Step...
At time: 874.5966029167175 and batch: 50, loss is 5.262212409973144 and perplexity is 192.90781069098242
At time: 875.4182369709015 and batch: 100, loss is 5.271644973754883 and perplexity is 194.73603478426972
At time: 876.2516732215881 and batch: 150, loss is 5.200725240707397 and perplexity is 181.4037555678576
At time: 877.0824575424194 and batch: 200, loss is 5.227012519836426 and perplexity is 186.2355964618944
At time: 877.9166903495789 and batch: 250, loss is 5.246350688934326 and perplexity is 189.87210025463412
At time: 878.7453279495239 and batch: 300, loss is 5.256858682632446 and perplexity is 191.87779454368285
At time: 879.5726454257965 and batch: 350, loss is 5.271305408477783 and perplexity is 194.66992041436427
At time: 880.3988883495331 and batch: 400, loss is 5.239703788757324 and perplexity is 188.61422447642053
At time: 881.2223329544067 and batch: 450, loss is 5.2103658962249755 and perplexity is 183.16106387624228
At time: 882.0448553562164 and batch: 500, loss is 5.224542407989502 and perplexity is 185.77614139506878
At time: 882.8738744258881 and batch: 550, loss is 5.2316468143463135 and perplexity is 187.10067001869874
At time: 883.6979157924652 and batch: 600, loss is 5.2409285545349125 and perplexity is 188.84537424699522
At time: 884.5353755950928 and batch: 650, loss is 5.2492758083343505 and perplexity is 190.42831191477728
At time: 885.3670220375061 and batch: 700, loss is 5.269709882736206 and perplexity is 194.35956719934302
At time: 886.1913392543793 and batch: 750, loss is 5.237315864562988 and perplexity is 188.1643653349263
At time: 887.0158727169037 and batch: 800, loss is 5.257051868438721 and perplexity is 191.9148661908707
At time: 887.8426485061646 and batch: 850, loss is 5.283763914108277 and perplexity is 197.1103874337728
At time: 888.6670589447021 and batch: 900, loss is 5.271351375579834 and perplexity is 194.67886903213164
At time: 889.496579170227 and batch: 950, loss is 5.249601144790649 and perplexity is 190.49027526587653
At time: 890.3210661411285 and batch: 1000, loss is 5.23550313949585 and perplexity is 187.8235840378578
At time: 891.1464712619781 and batch: 1050, loss is 5.238408765792847 and perplexity is 188.37012281703872
At time: 892.028167963028 and batch: 1100, loss is 5.217230033874512 and perplexity is 184.42263146414157
At time: 892.859897851944 and batch: 1150, loss is 5.270683135986328 and perplexity is 194.5488203605005
At time: 893.6844778060913 and batch: 1200, loss is 5.2781416606903075 and perplexity is 196.00529235756926
At time: 894.5128195285797 and batch: 1250, loss is 5.278911085128784 and perplexity is 196.15616165339844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.136085677320939 and perplexity of 170.04883785977944
Finished 40 epochs...
Completing Train Step...
At time: 896.9524471759796 and batch: 50, loss is 5.250849123001099 and perplexity is 190.72815137994115
At time: 897.8102943897247 and batch: 100, loss is 5.258344488143921 and perplexity is 192.16309952971883
At time: 898.6318991184235 and batch: 150, loss is 5.189701824188233 and perplexity is 179.4150477277505
At time: 899.4532747268677 and batch: 200, loss is 5.213436412811279 and perplexity is 183.72432727309996
At time: 900.2750120162964 and batch: 250, loss is 5.230946369171143 and perplexity is 186.96966214438598
At time: 901.1212677955627 and batch: 300, loss is 5.24091854095459 and perplexity is 188.8434832381396
At time: 901.9875373840332 and batch: 350, loss is 5.253689832687378 and perplexity is 191.27072496917353
At time: 902.8143537044525 and batch: 400, loss is 5.225303897857666 and perplexity is 185.91766192087232
At time: 903.6609132289886 and batch: 450, loss is 5.197381458282471 and perplexity is 180.79819387567184
At time: 904.4906339645386 and batch: 500, loss is 5.208924074172973 and perplexity is 182.89716850607599
At time: 905.316020488739 and batch: 550, loss is 5.214145755767822 and perplexity is 183.85469706357654
At time: 906.1406121253967 and batch: 600, loss is 5.228198051452637 and perplexity is 186.4565155769868
At time: 906.9686059951782 and batch: 650, loss is 5.2363298225402835 and perplexity is 187.97891880759045
At time: 907.7959861755371 and batch: 700, loss is 5.254406042098999 and perplexity is 191.40776393100325
At time: 908.6198556423187 and batch: 750, loss is 5.22327844619751 and perplexity is 185.54147578590533
At time: 909.4490315914154 and batch: 800, loss is 5.2456045246124265 and perplexity is 189.7304773112646
At time: 910.2765872478485 and batch: 850, loss is 5.273815746307373 and perplexity is 195.15922157851864
At time: 911.100017786026 and batch: 900, loss is 5.258436183929444 and perplexity is 192.18072088396832
At time: 911.9267649650574 and batch: 950, loss is 5.237396869659424 and perplexity is 188.17960822485358
At time: 912.8185746669769 and batch: 1000, loss is 5.224632940292358 and perplexity is 185.79296089830768
At time: 913.6444320678711 and batch: 1050, loss is 5.229425096511841 and perplexity is 186.68544654876698
At time: 914.4726896286011 and batch: 1100, loss is 5.208735303878784 and perplexity is 182.86264621226505
At time: 915.3049914836884 and batch: 1150, loss is 5.25854115486145 and perplexity is 192.20089533219993
At time: 916.1323802471161 and batch: 1200, loss is 5.2667155361175535 and perplexity is 193.77845774225685
At time: 916.9620099067688 and batch: 1250, loss is 5.270641202926636 and perplexity is 194.54066250424643
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.130311395130018 and perplexity of 169.06975734772203
Finished 41 epochs...
Completing Train Step...
At time: 919.4024074077606 and batch: 50, loss is 5.240431900024414 and perplexity is 188.75160662707182
At time: 920.2256941795349 and batch: 100, loss is 5.241008758544922 and perplexity is 188.860521010689
At time: 921.0556235313416 and batch: 150, loss is 5.171107835769654 and perplexity is 176.10983022346585
At time: 921.8876760005951 and batch: 200, loss is 5.202689828872681 and perplexity is 181.76048954211848
At time: 922.717052936554 and batch: 250, loss is 5.220094156265259 and perplexity is 184.95159760255794
At time: 923.5453765392303 and batch: 300, loss is 5.2276756477355955 and perplexity is 186.35913543827067
At time: 924.3741648197174 and batch: 350, loss is 5.242705841064453 and perplexity is 189.18130502100342
At time: 925.2008304595947 and batch: 400, loss is 5.214570569992065 and perplexity is 183.93281774629685
At time: 926.0292544364929 and batch: 450, loss is 5.183285398483276 and perplexity is 178.26752982233637
At time: 926.8559198379517 and batch: 500, loss is 5.197977600097656 and perplexity is 180.90600737202487
At time: 927.6811301708221 and batch: 550, loss is 5.201723079681397 and perplexity is 181.58485764552208
At time: 928.5113079547882 and batch: 600, loss is 5.216472835540771 and perplexity is 184.28303981084608
At time: 929.3388769626617 and batch: 650, loss is 5.221490135192871 and perplexity is 185.20996643220204
At time: 930.1627745628357 and batch: 700, loss is 5.238273992538452 and perplexity is 188.34473727324053
At time: 930.9925413131714 and batch: 750, loss is 5.210500230789185 and perplexity is 183.1856703906539
At time: 931.8175528049469 and batch: 800, loss is 5.233329515457154 and perplexity is 187.41576955880132
At time: 932.6451950073242 and batch: 850, loss is 5.260093307495117 and perplexity is 192.49945210098073
At time: 933.4716289043427 and batch: 900, loss is 5.243034725189209 and perplexity is 189.24353398142318
At time: 934.3500146865845 and batch: 950, loss is 5.222881727218628 and perplexity is 185.46788255997123
At time: 935.1821043491364 and batch: 1000, loss is 5.212519721984863 and perplexity is 183.55598603792072
At time: 936.0103685855865 and batch: 1050, loss is 5.216641292572022 and perplexity is 184.31408619956025
At time: 936.8429698944092 and batch: 1100, loss is 5.19315541267395 and perplexity is 180.03574466971838
At time: 937.6708879470825 and batch: 1150, loss is 5.245358943939209 and perplexity is 189.6838888937582
At time: 938.4959719181061 and batch: 1200, loss is 5.25315375328064 and perplexity is 191.1682161512937
At time: 939.323591709137 and batch: 1250, loss is 5.260121412277222 and perplexity is 192.5048623321637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.125498973540146 and perplexity of 168.25807703769325
Finished 42 epochs...
Completing Train Step...
At time: 941.8201744556427 and batch: 50, loss is 5.2288987255096435 and perplexity is 186.58720660076958
At time: 942.6590857505798 and batch: 100, loss is 5.235429010391235 and perplexity is 187.8096613597918
At time: 943.4932465553284 and batch: 150, loss is 5.164815969467163 and perplexity is 175.00524929886603
At time: 944.3309624195099 and batch: 200, loss is 5.193715953826905 and perplexity is 180.13669040308432
At time: 945.1577343940735 and batch: 250, loss is 5.210517921447754 and perplexity is 183.18891109446855
At time: 945.9961950778961 and batch: 300, loss is 5.221843233108521 and perplexity is 185.27537523248267
At time: 946.8208844661713 and batch: 350, loss is 5.231489934921265 and perplexity is 187.07132007542126
At time: 947.6503326892853 and batch: 400, loss is 5.202463083267212 and perplexity is 181.71928082199148
At time: 948.4859368801117 and batch: 450, loss is 5.175252151489258 and perplexity is 176.84119942688773
At time: 949.3137512207031 and batch: 500, loss is 5.188827085494995 and perplexity is 179.25817506465094
At time: 950.1426453590393 and batch: 550, loss is 5.194026288986206 and perplexity is 180.1926018267881
At time: 950.9675357341766 and batch: 600, loss is 5.207568778991699 and perplexity is 182.6494567541198
At time: 951.7984347343445 and batch: 650, loss is 5.212476539611816 and perplexity is 183.54805982599413
At time: 952.627768278122 and batch: 700, loss is 5.232194385528564 and perplexity is 187.20314900849104
At time: 953.4583249092102 and batch: 750, loss is 5.203477201461792 and perplexity is 181.90365912587362
At time: 954.2822995185852 and batch: 800, loss is 5.2275214195251465 and perplexity is 186.33039581859822
At time: 955.1106474399567 and batch: 850, loss is 5.253945045471191 and perplexity is 191.31954593295688
At time: 955.9965319633484 and batch: 900, loss is 5.236267690658569 and perplexity is 187.96723968646896
At time: 956.8224561214447 and batch: 950, loss is 5.215090265274048 and perplexity is 184.0284316067655
At time: 957.6525495052338 and batch: 1000, loss is 5.208103437423706 and perplexity is 182.74713793698194
At time: 958.4784061908722 and batch: 1050, loss is 5.21434398651123 and perplexity is 183.89114632941775
At time: 959.3142807483673 and batch: 1100, loss is 5.18750002861023 and perplexity is 179.02044704347273
At time: 960.1441378593445 and batch: 1150, loss is 5.240534219741821 and perplexity is 188.7709206262067
At time: 960.9718906879425 and batch: 1200, loss is 5.248364315032959 and perplexity is 190.2548168658711
At time: 961.8090326786041 and batch: 1250, loss is 5.254030284881591 and perplexity is 191.33585459331067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.124782144588275 and perplexity of 168.13750799553947
Finished 43 epochs...
Completing Train Step...
At time: 964.2667739391327 and batch: 50, loss is 5.221238031387329 and perplexity is 185.16328017997827
At time: 965.1292250156403 and batch: 100, loss is 5.227287225723266 and perplexity is 186.28676350420386
At time: 965.9532806873322 and batch: 150, loss is 5.157147159576416 and perplexity is 173.6683002683145
At time: 966.7794861793518 and batch: 200, loss is 5.1899630737304685 and perplexity is 179.46192595003004
At time: 967.6050333976746 and batch: 250, loss is 5.208374519348144 and perplexity is 182.79668409805043
At time: 968.431881904602 and batch: 300, loss is 5.213247041702271 and perplexity is 183.68953848759188
At time: 969.2675349712372 and batch: 350, loss is 5.225769319534302 and perplexity is 186.00421217041855
At time: 970.0935969352722 and batch: 400, loss is 5.194807472229004 and perplexity is 180.33342026315168
At time: 970.921147108078 and batch: 450, loss is 5.169453592300415 and perplexity is 175.8187425182982
At time: 971.7496318817139 and batch: 500, loss is 5.181281929016113 and perplexity is 177.91073380366043
At time: 972.5796427726746 and batch: 550, loss is 5.18465027809143 and perplexity is 178.51100966112222
At time: 973.4057505130768 and batch: 600, loss is 5.197161273956299 and perplexity is 180.75838932950967
At time: 974.231965303421 and batch: 650, loss is 5.204153985977173 and perplexity is 182.02681037438953
At time: 975.0611820220947 and batch: 700, loss is 5.225644702911377 and perplexity is 185.98103439784597
At time: 975.889710187912 and batch: 750, loss is 5.196590871810913 and perplexity is 180.6553137564983
At time: 976.7194592952728 and batch: 800, loss is 5.222370948791504 and perplexity is 185.3731737563012
At time: 977.6019878387451 and batch: 850, loss is 5.248577117919922 and perplexity is 190.29530794831552
At time: 978.4314153194427 and batch: 900, loss is 5.226687955856323 and perplexity is 186.17516090360314
At time: 979.2586686611176 and batch: 950, loss is 5.208007678985596 and perplexity is 182.7296391943236
At time: 980.0859100818634 and batch: 1000, loss is 5.201026077270508 and perplexity is 181.45833665980038
At time: 980.9129524230957 and batch: 1050, loss is 5.209164581298828 and perplexity is 182.94116186854689
At time: 981.7411994934082 and batch: 1100, loss is 5.17950065612793 and perplexity is 177.59410831893427
At time: 982.5707573890686 and batch: 1150, loss is 5.231663846969605 and perplexity is 187.10385686106878
At time: 983.3956065177917 and batch: 1200, loss is 5.245206079483032 and perplexity is 189.65489518534716
At time: 984.2276566028595 and batch: 1250, loss is 5.251368894577026 and perplexity is 190.8273122200182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.121791421932025 and perplexity of 167.6354065383642
Finished 44 epochs...
Completing Train Step...
At time: 986.7906994819641 and batch: 50, loss is 5.218206987380982 and perplexity is 184.6028918392884
At time: 987.6166586875916 and batch: 100, loss is 5.222194137573243 and perplexity is 185.34040059703293
At time: 988.4424529075623 and batch: 150, loss is 5.151289949417114 and perplexity is 172.6540617393563
At time: 989.2672805786133 and batch: 200, loss is 5.179870586395264 and perplexity is 177.6598179081353
At time: 990.0949428081512 and batch: 250, loss is 5.199066038131714 and perplexity is 181.10301954936168
At time: 990.9230978488922 and batch: 300, loss is 5.208570346832276 and perplexity is 182.8324842180143
At time: 991.752682685852 and batch: 350, loss is 5.220931625366211 and perplexity is 185.10655372714828
At time: 992.5834097862244 and batch: 400, loss is 5.18884536743164 and perplexity is 179.26145228120748
At time: 993.416216135025 and batch: 450, loss is 5.165315027236939 and perplexity is 175.09260882519212
At time: 994.2402889728546 and batch: 500, loss is 5.177457752227784 and perplexity is 177.23167096080414
At time: 995.0679984092712 and batch: 550, loss is 5.177770891189575 and perplexity is 177.28717779246767
At time: 995.8942110538483 and batch: 600, loss is 5.195265283584595 and perplexity is 180.41599785177723
At time: 996.7167015075684 and batch: 650, loss is 5.196338472366333 and perplexity is 180.6097222095283
At time: 997.5461783409119 and batch: 700, loss is 5.220231380462646 and perplexity is 184.97697917853782
At time: 998.3716683387756 and batch: 750, loss is 5.191177387237548 and perplexity is 179.67998135773655
At time: 999.2536165714264 and batch: 800, loss is 5.216851625442505 and perplexity is 184.35285758768683
At time: 1000.0826232433319 and batch: 850, loss is 5.243321189880371 and perplexity is 189.29775333753435
At time: 1000.9066545963287 and batch: 900, loss is 5.220373935699463 and perplexity is 185.0033504952503
At time: 1001.7339043617249 and batch: 950, loss is 5.2034532165527345 and perplexity is 181.8992962354743
At time: 1002.5617694854736 and batch: 1000, loss is 5.197784280776977 and perplexity is 180.87103812579738
At time: 1003.389942407608 and batch: 1050, loss is 5.202157278060913 and perplexity is 181.66371861587018
At time: 1004.2195734977722 and batch: 1100, loss is 5.1750279235839844 and perplexity is 176.8015511404664
At time: 1005.0467712879181 and batch: 1150, loss is 5.223461513519287 and perplexity is 185.57544547623132
At time: 1005.8749122619629 and batch: 1200, loss is 5.233551254272461 and perplexity is 187.45733151729263
At time: 1006.706120967865 and batch: 1250, loss is 5.243172740936279 and perplexity is 189.2696543716149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.114521555656935 and perplexity of 166.4211386802651
Finished 45 epochs...
Completing Train Step...
At time: 1009.1508512496948 and batch: 50, loss is 5.210387582778931 and perplexity is 183.16503605160779
At time: 1010.0037758350372 and batch: 100, loss is 5.2094331550598145 and perplexity is 182.99030166296347
At time: 1010.830180644989 and batch: 150, loss is 5.140815467834472 and perplexity is 170.85503831660915
At time: 1011.6559159755707 and batch: 200, loss is 5.1708571815490725 and perplexity is 176.06569308304586
At time: 1012.4809601306915 and batch: 250, loss is 5.191316499710083 and perplexity is 179.70497882289743
At time: 1013.315904378891 and batch: 300, loss is 5.199204607009888 and perplexity is 181.12811653040458
At time: 1014.1405673027039 and batch: 350, loss is 5.208966360092163 and perplexity is 182.90490264448496
At time: 1014.9646420478821 and batch: 400, loss is 5.1821799659729 and perplexity is 178.07057597897327
At time: 1015.7895722389221 and batch: 450, loss is 5.158619565963745 and perplexity is 173.92419893008622
At time: 1016.6209673881531 and batch: 500, loss is 5.171280775070191 and perplexity is 176.14028916802073
At time: 1017.4476120471954 and batch: 550, loss is 5.170982437133789 and perplexity is 176.08774767558313
At time: 1018.2719521522522 and batch: 600, loss is 5.183045310974121 and perplexity is 178.22473515257647
At time: 1019.095457315445 and batch: 650, loss is 5.184654722213745 and perplexity is 178.51180298764646
At time: 1019.9184722900391 and batch: 700, loss is 5.208996448516846 and perplexity is 182.91040604766619
At time: 1020.7986626625061 and batch: 750, loss is 5.183257265090942 and perplexity is 178.2625146225271
At time: 1021.6273679733276 and batch: 800, loss is 5.206649465560913 and perplexity is 182.48162181367772
At time: 1022.4527823925018 and batch: 850, loss is 5.2335396671295165 and perplexity is 187.45515943498046
At time: 1023.2770037651062 and batch: 900, loss is 5.2151390552520756 and perplexity is 184.03741056893995
At time: 1024.1123764514923 and batch: 950, loss is 5.193711862564087 and perplexity is 180.13595341804842
At time: 1024.936282634735 and batch: 1000, loss is 5.187258520126343 and perplexity is 178.97721730710666
At time: 1025.7623119354248 and batch: 1050, loss is 5.188428068161011 and perplexity is 179.18666221392417
At time: 1026.5853703022003 and batch: 1100, loss is 5.162638883590699 and perplexity is 174.62466227794673
At time: 1027.4131755828857 and batch: 1150, loss is 5.212864627838135 and perplexity is 183.619306491078
At time: 1028.2441341876984 and batch: 1200, loss is 5.223298768997193 and perplexity is 185.54524654646673
At time: 1029.078988790512 and batch: 1250, loss is 5.235337629318237 and perplexity is 187.79249989554572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.113173881586451 and perplexity of 166.19700828816684
Finished 46 epochs...
Completing Train Step...
At time: 1031.5370349884033 and batch: 50, loss is 5.202779550552368 and perplexity is 181.7767981301469
At time: 1032.3639676570892 and batch: 100, loss is 5.199075918197632 and perplexity is 181.1048088679721
At time: 1033.188753604889 and batch: 150, loss is 5.127367944717407 and perplexity is 168.57284058439254
At time: 1034.0130054950714 and batch: 200, loss is 5.164116458892822 and perplexity is 174.88287408278512
At time: 1034.8395297527313 and batch: 250, loss is 5.182638912200928 and perplexity is 178.15231955465956
At time: 1035.6683626174927 and batch: 300, loss is 5.18737380027771 and perplexity is 178.99785101711484
At time: 1036.4945805072784 and batch: 350, loss is 5.197011137008667 and perplexity is 180.73125285382173
At time: 1037.3212249279022 and batch: 400, loss is 5.173722372055054 and perplexity is 176.5708782155171
At time: 1038.145084619522 and batch: 450, loss is 5.153119020462036 and perplexity is 172.97014726779378
At time: 1038.9696691036224 and batch: 500, loss is 5.159314937591553 and perplexity is 174.04518294296133
At time: 1039.7940018177032 and batch: 550, loss is 5.162872800827026 and perplexity is 174.66551477420663
At time: 1040.6169171333313 and batch: 600, loss is 5.173161849975586 and perplexity is 176.47193407246527
At time: 1041.4839940071106 and batch: 650, loss is 5.174883508682251 and perplexity is 176.7760202054003
At time: 1042.3111493587494 and batch: 700, loss is 5.197810897827148 and perplexity is 180.87585244336483
At time: 1043.1463174819946 and batch: 750, loss is 5.177511329650879 and perplexity is 177.241166831405
At time: 1043.9724371433258 and batch: 800, loss is 5.1955927467346195 and perplexity is 180.47508711699905
At time: 1044.795988559723 and batch: 850, loss is 5.22465485572815 and perplexity is 185.79703267662998
At time: 1045.623782634735 and batch: 900, loss is 5.208418006896973 and perplexity is 182.8046336506279
At time: 1046.4485311508179 and batch: 950, loss is 5.182568445205688 and perplexity is 178.1397661383115
At time: 1047.2755460739136 and batch: 1000, loss is 5.179003286361694 and perplexity is 177.50580034147436
At time: 1048.101152420044 and batch: 1050, loss is 5.181547403335571 and perplexity is 177.95797080446914
At time: 1048.9266929626465 and batch: 1100, loss is 5.154655027389526 and perplexity is 173.23603476248695
At time: 1049.750342130661 and batch: 1150, loss is 5.205939865112304 and perplexity is 182.35217870485153
At time: 1050.5818889141083 and batch: 1200, loss is 5.216099882125855 and perplexity is 184.21432363659986
At time: 1051.413923740387 and batch: 1250, loss is 5.227293882369995 and perplexity is 186.28800355350606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.105477660241788 and perplexity of 164.92282879857765
Finished 47 epochs...
Completing Train Step...
At time: 1053.8538918495178 and batch: 50, loss is 5.192332105636597 and perplexity is 179.8875809746323
At time: 1054.7055578231812 and batch: 100, loss is 5.190009727478027 and perplexity is 179.47029871672862
At time: 1055.53955411911 and batch: 150, loss is 5.120392827987671 and perplexity is 167.40111655039004
At time: 1056.3719458580017 and batch: 200, loss is 5.156064682006836 and perplexity is 173.48040994060634
At time: 1057.2010791301727 and batch: 250, loss is 5.175861644744873 and perplexity is 176.94901579859132
At time: 1058.031890153885 and batch: 300, loss is 5.179883556365967 and perplexity is 177.66212216571165
At time: 1058.8711924552917 and batch: 350, loss is 5.189335975646973 and perplexity is 179.34942099971258
At time: 1059.6962747573853 and batch: 400, loss is 5.172293491363526 and perplexity is 176.3187596635101
At time: 1060.5312943458557 and batch: 450, loss is 5.1457865524291995 and perplexity is 171.7064877256417
At time: 1061.3562002182007 and batch: 500, loss is 5.148156414031982 and perplexity is 172.11389089173852
At time: 1062.1786088943481 and batch: 550, loss is 5.152371196746826 and perplexity is 172.8408444435288
At time: 1063.0396490097046 and batch: 600, loss is 5.167975196838379 and perplexity is 175.55900493196074
At time: 1063.8642222881317 and batch: 650, loss is 5.166463594436646 and perplexity is 175.29382998849294
At time: 1064.6894295215607 and batch: 700, loss is 5.190755825042725 and perplexity is 179.60425103407184
At time: 1065.5205445289612 and batch: 750, loss is 5.171297588348389 and perplexity is 176.1432506886008
At time: 1066.3460528850555 and batch: 800, loss is 5.18993350982666 and perplexity is 179.45662043334022
At time: 1067.1697487831116 and batch: 850, loss is 5.22068642616272 and perplexity is 185.06117131170694
At time: 1067.9947328567505 and batch: 900, loss is 5.198867254257202 and perplexity is 181.06702276735825
At time: 1068.8197100162506 and batch: 950, loss is 5.178111276626587 and perplexity is 177.34753403756778
At time: 1069.6492097377777 and batch: 1000, loss is 5.173519344329834 and perplexity is 176.5350330706751
At time: 1070.4734518527985 and batch: 1050, loss is 5.173972969055176 and perplexity is 176.61513189259452
At time: 1071.2970576286316 and batch: 1100, loss is 5.147267990112304 and perplexity is 171.96104869854483
At time: 1072.122078180313 and batch: 1150, loss is 5.197260570526123 and perplexity is 180.77633890868813
At time: 1072.9498391151428 and batch: 1200, loss is 5.20664776802063 and perplexity is 182.48131204403663
At time: 1073.7758331298828 and batch: 1250, loss is 5.219867458343506 and perplexity is 184.90967421190888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1003502616046985 and perplexity of 164.07936793889868
Finished 48 epochs...
Completing Train Step...
At time: 1076.2329199314117 and batch: 50, loss is 5.1872684764862065 and perplexity is 178.97899927756052
At time: 1077.0637259483337 and batch: 100, loss is 5.180972785949707 and perplexity is 177.85574243440396
At time: 1077.8873398303986 and batch: 150, loss is 5.111401119232178 and perplexity is 165.9026414867646
At time: 1078.710368156433 and batch: 200, loss is 5.146880512237549 and perplexity is 171.89443050422602
At time: 1079.5355994701385 and batch: 250, loss is 5.166274967193604 and perplexity is 175.26076791492184
At time: 1080.368007183075 and batch: 300, loss is 5.171504526138306 and perplexity is 176.17970515538042
At time: 1081.1923580169678 and batch: 350, loss is 5.1797856521606445 and perplexity is 177.64472914826382
At time: 1082.0188827514648 and batch: 400, loss is 5.164828786849975 and perplexity is 175.00749242251587
At time: 1082.8436179161072 and batch: 450, loss is 5.1371681690216064 and perplexity is 170.23301398204802
At time: 1083.669540643692 and batch: 500, loss is 5.139994535446167 and perplexity is 170.71483543836567
At time: 1084.5230026245117 and batch: 550, loss is 5.141702890396118 and perplexity is 171.00672622806488
At time: 1085.3474440574646 and batch: 600, loss is 5.159130163192749 and perplexity is 174.01302681982403
At time: 1086.1705300807953 and batch: 650, loss is 5.15679817199707 and perplexity is 173.60770276309808
At time: 1087.0057818889618 and batch: 700, loss is 5.1810745525360105 and perplexity is 177.87384312717327
At time: 1087.8285381793976 and batch: 750, loss is 5.1611562347412105 and perplexity is 174.36594706262957
At time: 1088.6569418907166 and batch: 800, loss is 5.184383230209351 and perplexity is 178.4633450387158
At time: 1089.480523109436 and batch: 850, loss is 5.212787895202637 and perplexity is 183.60521743831458
At time: 1090.304770708084 and batch: 900, loss is 5.187805881500244 and perplexity is 179.07520933874892
At time: 1091.1295020580292 and batch: 950, loss is 5.17099100112915 and perplexity is 176.08925569669466
At time: 1091.9539620876312 and batch: 1000, loss is 5.166137456893921 and perplexity is 175.2366694111374
At time: 1092.7825095653534 and batch: 1050, loss is 5.166477375030517 and perplexity is 175.2962456582168
At time: 1093.6124229431152 and batch: 1100, loss is 5.1395683860778805 and perplexity is 170.64210091807212
At time: 1094.4375941753387 and batch: 1150, loss is 5.191062498092651 and perplexity is 179.65933926412217
At time: 1095.264582157135 and batch: 1200, loss is 5.19669768333435 and perplexity is 180.67461085633568
At time: 1096.0886249542236 and batch: 1250, loss is 5.211603355407715 and perplexity is 183.38785851226777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.102976555371806 and perplexity of 164.5108549179973
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 1098.5337085723877 and batch: 50, loss is 5.187136964797974 and perplexity is 178.95546299488936
At time: 1099.3613021373749 and batch: 100, loss is 5.191722478866577 and perplexity is 179.77795011002632
At time: 1100.1840195655823 and batch: 150, loss is 5.1112705326080325 and perplexity is 165.88097823537228
At time: 1101.0109622478485 and batch: 200, loss is 5.146764945983887 and perplexity is 171.87456645669621
At time: 1101.8433754444122 and batch: 250, loss is 5.161212329864502 and perplexity is 174.37572841626843
At time: 1102.6677422523499 and batch: 300, loss is 5.158401346206665 and perplexity is 173.8862493744673
At time: 1103.4915149211884 and batch: 350, loss is 5.1668189525604244 and perplexity is 175.35613314433823
At time: 1104.318852186203 and batch: 400, loss is 5.13686990737915 and perplexity is 170.18224757490984
At time: 1105.1429715156555 and batch: 450, loss is 5.104660682678222 and perplexity is 164.78814557180792
At time: 1105.993651151657 and batch: 500, loss is 5.1003701114654545 and perplexity is 164.08262492383042
At time: 1106.8197703361511 and batch: 550, loss is 5.1104843807220455 and perplexity is 165.75062183816067
At time: 1107.649605512619 and batch: 600, loss is 5.11614541053772 and perplexity is 166.69160199573216
At time: 1108.4745454788208 and batch: 650, loss is 5.120259504318238 and perplexity is 167.3787995069924
At time: 1109.2999966144562 and batch: 700, loss is 5.135863332748413 and perplexity is 170.0110326266669
At time: 1110.1232697963715 and batch: 750, loss is 5.107165155410766 and perplexity is 165.20137022798895
At time: 1110.9480788707733 and batch: 800, loss is 5.1098963737487795 and perplexity is 165.65318796540228
At time: 1111.774282693863 and batch: 850, loss is 5.139451274871826 and perplexity is 170.6221179859654
At time: 1112.5988516807556 and batch: 900, loss is 5.106680145263672 and perplexity is 165.12126531453274
At time: 1113.422609090805 and batch: 950, loss is 5.0834830856323245 and perplexity is 161.33502211235884
At time: 1114.2519476413727 and batch: 1000, loss is 5.074517393112183 and perplexity is 159.8950069241958
At time: 1115.0737342834473 and batch: 1050, loss is 5.054742631912231 and perplexity is 156.76417905482984
At time: 1115.8968093395233 and batch: 1100, loss is 5.009842576980591 and perplexity is 149.8811395502311
At time: 1116.72043800354 and batch: 1150, loss is 5.05675181388855 and perplexity is 157.079463443713
At time: 1117.5446865558624 and batch: 1200, loss is 5.073957452774048 and perplexity is 159.80550032146982
At time: 1118.3657670021057 and batch: 1250, loss is 5.117551040649414 and perplexity is 166.92607348234927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.032238145814325 and perplexity of 153.27568240719728
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f15b303a898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'anneal': 4.321431559402063, 'lr': 4.99481944811914, 'dropout': 0.2336529334756795, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.4438042640686035 and batch: 50, loss is 7.46312894821167 and perplexity is 1742.592016147369
At time: 2.2701985836029053 and batch: 100, loss is 6.665740947723389 and perplexity is 785.0449267892363
At time: 3.0943291187286377 and batch: 150, loss is 6.2629107475280765 and perplexity is 524.7441169701215
At time: 3.933164358139038 and batch: 200, loss is 6.045224018096924 and perplexity is 422.09230320688016
At time: 4.763643264770508 and batch: 250, loss is 5.918655290603637 and perplexity is 371.91126489984896
At time: 5.622908115386963 and batch: 300, loss is 5.8037240600585935 and perplexity is 331.5319085540136
At time: 6.4521355628967285 and batch: 350, loss is 5.749926881790161 and perplexity is 314.16768806692005
At time: 7.276952028274536 and batch: 400, loss is 5.662500953674316 and perplexity is 287.86768678209137
At time: 8.103742361068726 and batch: 450, loss is 5.569889974594116 and perplexity is 262.4052264103939
At time: 8.930933952331543 and batch: 500, loss is 5.537436351776123 and perplexity is 254.02593085197688
At time: 9.762860774993896 and batch: 550, loss is 5.503720626831055 and perplexity is 245.60403537821307
At time: 10.5947847366333 and batch: 600, loss is 5.489858713150024 and perplexity is 242.22298151816537
At time: 11.421382904052734 and batch: 650, loss is 5.434730672836304 and perplexity is 229.23110179864156
At time: 12.264741897583008 and batch: 700, loss is 5.4173215389251705 and perplexity is 225.2749235876497
At time: 13.089972257614136 and batch: 750, loss is 5.381705093383789 and perplexity is 217.39263432943815
At time: 13.926137208938599 and batch: 800, loss is 5.376108903884887 and perplexity is 216.17946169016702
At time: 14.755197763442993 and batch: 850, loss is 5.414100093841553 and perplexity is 224.55038045714005
At time: 15.587719678878784 and batch: 900, loss is 5.372747583389282 and perplexity is 215.4540331168444
At time: 16.413954257965088 and batch: 950, loss is 5.3349937915802 and perplexity is 207.4714607340161
At time: 17.24153184890747 and batch: 1000, loss is 5.311131734848022 and perplexity is 202.57936482951052
At time: 18.0720157623291 and batch: 1050, loss is 5.276613817214966 and perplexity is 195.7060556021797
At time: 18.90401577949524 and batch: 1100, loss is 5.248010292053222 and perplexity is 190.18747420982217
At time: 19.733399868011475 and batch: 1150, loss is 5.2642473697662355 and perplexity is 193.3007700221257
At time: 20.563342094421387 and batch: 1200, loss is 5.248615455627442 and perplexity is 190.3026035740219
At time: 21.400639057159424 and batch: 1250, loss is 5.244238929748535 and perplexity is 189.4715591748003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.833306305599908 and perplexity of 125.62563149164981
Finished 1 epochs...
Completing Train Step...
At time: 23.909212350845337 and batch: 50, loss is 5.045791339874268 and perplexity is 155.36719882384213
At time: 24.730791568756104 and batch: 100, loss is 5.051623220443726 and perplexity is 156.27592899916593
At time: 25.560713291168213 and batch: 150, loss is 4.920444831848145 and perplexity is 137.06356986882508
At time: 26.386627197265625 and batch: 200, loss is 4.9475994968414305 and perplexity is 140.83647940379
At time: 27.210337162017822 and batch: 250, loss is 4.94524956703186 and perplexity is 140.50591211964237
At time: 28.033008813858032 and batch: 300, loss is 4.929055299758911 and perplexity is 138.24884691084029
At time: 28.859771013259888 and batch: 350, loss is 4.911409797668457 and perplexity is 135.8307733956819
At time: 29.684595108032227 and batch: 400, loss is 4.898547830581665 and perplexity is 134.09490970004353
At time: 30.506550073623657 and batch: 450, loss is 4.83022253036499 and perplexity is 125.23882699597903
At time: 31.385943174362183 and batch: 500, loss is 4.8258515739440915 and perplexity is 124.69260815997681
At time: 32.21711850166321 and batch: 550, loss is 4.821303215026855 and perplexity is 124.12674926393687
At time: 33.04183912277222 and batch: 600, loss is 4.8236344623565675 and perplexity is 124.41645697566241
At time: 33.867148876190186 and batch: 650, loss is 4.808901824951172 and perplexity is 122.596910696578
At time: 34.69136047363281 and batch: 700, loss is 4.794668684005737 and perplexity is 120.86433086271475
At time: 35.51894950866699 and batch: 750, loss is 4.78312502861023 and perplexity is 119.47713672745101
At time: 36.34325170516968 and batch: 800, loss is 4.793831825256348 and perplexity is 120.76322680075315
At time: 37.17309498786926 and batch: 850, loss is 4.842087230682373 and perplexity is 126.73359811707584
At time: 37.99864196777344 and batch: 900, loss is 4.799717845916748 and perplexity is 121.47613769459797
At time: 38.824934244155884 and batch: 950, loss is 4.776154689788818 and perplexity is 118.64723630670855
At time: 39.64674711227417 and batch: 1000, loss is 4.758814392089843 and perplexity is 116.60759302624375
At time: 40.477073669433594 and batch: 1050, loss is 4.730413074493408 and perplexity is 113.34237152285482
At time: 41.300376653671265 and batch: 1100, loss is 4.707962207794189 and perplexity is 110.82608910069882
At time: 42.12881112098694 and batch: 1150, loss is 4.720937738418579 and perplexity is 112.27348648086391
At time: 42.950682401657104 and batch: 1200, loss is 4.7141721725463865 and perplexity is 111.51645656786658
At time: 43.77572751045227 and batch: 1250, loss is 4.7203750324249265 and perplexity is 112.2103272887874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.606749597257071 and perplexity of 100.15806591958682
Finished 2 epochs...
Completing Train Step...
At time: 46.27459526062012 and batch: 50, loss is 4.651573905944824 and perplexity is 104.74972211355353
At time: 47.10133147239685 and batch: 100, loss is 4.667774124145508 and perplexity is 106.46051062606402
At time: 47.93076682090759 and batch: 150, loss is 4.567977371215821 and perplexity is 96.34903422668745
At time: 48.75666379928589 and batch: 200, loss is 4.617493638992309 and perplexity is 101.23996996365413
At time: 49.580034255981445 and batch: 250, loss is 4.621403665542602 and perplexity is 101.63659583770051
At time: 50.40673756599426 and batch: 300, loss is 4.6226691627502445 and perplexity is 101.76529808491841
At time: 51.25883412361145 and batch: 350, loss is 4.609180917739868 and perplexity is 100.40187854999108
At time: 52.08675456047058 and batch: 400, loss is 4.609330406188965 and perplexity is 100.41688859298799
At time: 52.909183502197266 and batch: 450, loss is 4.541242237091065 and perplexity is 93.807258626036
At time: 53.73682117462158 and batch: 500, loss is 4.5585266208648685 and perplexity is 95.44275282279759
At time: 54.56504678726196 and batch: 550, loss is 4.551001863479614 and perplexity is 94.72726457738116
At time: 55.391587018966675 and batch: 600, loss is 4.569339542388916 and perplexity is 96.48036753258079
At time: 56.217992305755615 and batch: 650, loss is 4.563260707855225 and perplexity is 95.89565831840677
At time: 57.0423789024353 and batch: 700, loss is 4.545476188659668 and perplexity is 94.20527601443214
At time: 57.8696448802948 and batch: 750, loss is 4.541165466308594 and perplexity is 93.80005724582118
At time: 58.69753360748291 and batch: 800, loss is 4.562802734375 and perplexity is 95.85175070505696
At time: 59.53597569465637 and batch: 850, loss is 4.608873462677002 and perplexity is 100.37101422904855
At time: 60.39765524864197 and batch: 900, loss is 4.570141038894653 and perplexity is 96.55772720764371
At time: 61.257792234420776 and batch: 950, loss is 4.551079244613647 and perplexity is 94.73459496415116
At time: 62.08877491950989 and batch: 1000, loss is 4.53600341796875 and perplexity is 93.3171044014287
At time: 62.911906242370605 and batch: 1050, loss is 4.516069192886352 and perplexity is 91.47531851477397
At time: 63.73718571662903 and batch: 1100, loss is 4.497415285110474 and perplexity is 89.7847631130601
At time: 64.56512427330017 and batch: 1150, loss is 4.505651378631592 and perplexity is 90.52729239177927
At time: 65.38667964935303 and batch: 1200, loss is 4.502140045166016 and perplexity is 90.20997830426921
At time: 66.2114417552948 and batch: 1250, loss is 4.522142210006714 and perplexity is 92.0325399853725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.529853319599681 and perplexity of 92.74495621704713
Finished 3 epochs...
Completing Train Step...
At time: 68.66020274162292 and batch: 50, loss is 4.457323312759399 and perplexity is 86.25631863457279
At time: 69.51394319534302 and batch: 100, loss is 4.475889558792114 and perplexity is 87.87273361533713
At time: 70.3472945690155 and batch: 150, loss is 4.38819372177124 and perplexity is 80.49489141851495
At time: 71.18419146537781 and batch: 200, loss is 4.440772676467896 and perplexity is 84.8404705893919
At time: 72.00636768341064 and batch: 250, loss is 4.448947172164917 and perplexity is 85.53684101191295
At time: 72.86594223976135 and batch: 300, loss is 4.451973648071289 and perplexity is 85.79610833559909
At time: 73.68888974189758 and batch: 350, loss is 4.43815541267395 and perplexity is 84.61871102568453
At time: 74.51775121688843 and batch: 400, loss is 4.447690725326538 and perplexity is 85.42943600692772
At time: 75.33963775634766 and batch: 450, loss is 4.3726793479919435 and perplexity is 79.2557010723899
At time: 76.16378116607666 and batch: 500, loss is 4.401132326126099 and perplexity is 81.54314984788816
At time: 76.98893594741821 and batch: 550, loss is 4.392424697875977 and perplexity is 80.83618487375809
At time: 77.8164598941803 and batch: 600, loss is 4.415158472061157 and perplexity is 82.69494470612256
At time: 78.64237380027771 and batch: 650, loss is 4.416689891815185 and perplexity is 82.8216823975366
At time: 79.46938395500183 and batch: 700, loss is 4.397846031188965 and perplexity is 81.27561484761902
At time: 80.29816913604736 and batch: 750, loss is 4.395079565048218 and perplexity is 81.05107933921919
At time: 81.12599420547485 and batch: 800, loss is 4.420993595123291 and perplexity is 83.1788904534534
At time: 81.95158243179321 and batch: 850, loss is 4.467760238647461 and perplexity is 87.16128375170591
At time: 82.78320503234863 and batch: 900, loss is 4.428234539031982 and perplexity is 83.78336999370302
At time: 83.61124324798584 and batch: 950, loss is 4.412946624755859 and perplexity is 82.51223824939197
At time: 84.43576288223267 and batch: 1000, loss is 4.396965160369873 and perplexity is 81.20405305317152
At time: 85.26317524909973 and batch: 1050, loss is 4.384632720947265 and perplexity is 80.20875880544736
At time: 86.08905291557312 and batch: 1100, loss is 4.3652552795410156 and perplexity is 78.66948008741133
At time: 86.91595649719238 and batch: 1150, loss is 4.367831296920777 and perplexity is 78.87239527969443
At time: 87.75742244720459 and batch: 1200, loss is 4.366951189041138 and perplexity is 78.80300960104282
At time: 88.60455560684204 and batch: 1250, loss is 4.393259601593018 and perplexity is 80.90370348682903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.494153544850593 and perplexity of 89.49238562541733
Finished 4 epochs...
Completing Train Step...
At time: 91.1473114490509 and batch: 50, loss is 4.330893144607544 and perplexity is 76.01214609888589
At time: 91.98289895057678 and batch: 100, loss is 4.349305753707886 and perplexity is 77.42469245679014
At time: 92.82096576690674 and batch: 150, loss is 4.271325883865356 and perplexity is 71.61652789607858
At time: 93.65626645088196 and batch: 200, loss is 4.322697448730469 and perplexity is 75.39171955294218
At time: 94.51033735275269 and batch: 250, loss is 4.334054460525513 and perplexity is 76.2528247364678
At time: 95.33648705482483 and batch: 300, loss is 4.335356254577636 and perplexity is 76.35215484981508
At time: 96.16057395935059 and batch: 350, loss is 4.3244272804260255 and perplexity is 75.52224740206428
At time: 96.98338103294373 and batch: 400, loss is 4.3346723937988285 and perplexity is 76.29995845530354
At time: 97.80861043930054 and batch: 450, loss is 4.2569113540649415 and perplexity is 70.59161389344608
At time: 98.6373074054718 and batch: 500, loss is 4.294541797637939 and perplexity is 73.29862114800196
At time: 99.46505761146545 and batch: 550, loss is 4.283268251419067 and perplexity is 72.47692616550283
At time: 100.29130291938782 and batch: 600, loss is 4.310013246536255 and perplexity is 74.44147502551213
At time: 101.11831593513489 and batch: 650, loss is 4.313504600524903 and perplexity is 74.70183079887886
At time: 101.94191265106201 and batch: 700, loss is 4.296849098205566 and perplexity is 73.46793835591272
At time: 102.76922678947449 and batch: 750, loss is 4.2922115135192875 and perplexity is 73.12801339469391
At time: 103.59239935874939 and batch: 800, loss is 4.323476142883301 and perplexity is 75.45044950749849
At time: 104.41727495193481 and batch: 850, loss is 4.365831203460694 and perplexity is 78.71480077212281
At time: 105.2404088973999 and batch: 900, loss is 4.324581241607666 and perplexity is 75.53387579165175
At time: 106.07058334350586 and batch: 950, loss is 4.313027667999267 and perplexity is 74.66621156070816
At time: 106.89781045913696 and batch: 1000, loss is 4.299072313308716 and perplexity is 73.63145508514287
At time: 107.72338461875916 and batch: 1050, loss is 4.29205018043518 and perplexity is 73.1162163784084
At time: 108.54871654510498 and batch: 1100, loss is 4.267233533859253 and perplexity is 71.32404687325328
At time: 109.3734040260315 and batch: 1150, loss is 4.270465078353882 and perplexity is 71.55490651996816
At time: 110.19809913635254 and batch: 1200, loss is 4.270905179977417 and perplexity is 71.58640488123102
At time: 111.02150130271912 and batch: 1250, loss is 4.2996478843688966 and perplexity is 73.67384741853871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4811316719890515 and perplexity of 88.334581904757
Finished 5 epochs...
Completing Train Step...
At time: 113.46103262901306 and batch: 50, loss is 4.238770027160644 and perplexity is 69.32253454588786
At time: 114.31680178642273 and batch: 100, loss is 4.257918148040772 and perplexity is 70.66272089410172
At time: 115.13967323303223 and batch: 150, loss is 4.185128917694092 and perplexity is 65.70197051732167
At time: 115.99203681945801 and batch: 200, loss is 4.236180143356323 and perplexity is 69.14322952620026
At time: 116.8154661655426 and batch: 250, loss is 4.245641918182373 and perplexity is 69.80055200948317
At time: 117.63793110847473 and batch: 300, loss is 4.2511172866821285 and perplexity is 70.18378396387722
At time: 118.46032071113586 and batch: 350, loss is 4.239038438796997 and perplexity is 69.34114401820918
At time: 119.28175663948059 and batch: 400, loss is 4.248989524841309 and perplexity is 70.0346083478873
At time: 120.1062400341034 and batch: 450, loss is 4.172248167991638 and perplexity is 64.86110698236102
At time: 120.94118547439575 and batch: 500, loss is 4.213150882720948 and perplexity is 67.56910707894924
At time: 121.76527190208435 and batch: 550, loss is 4.201691341400147 and perplexity is 66.79921582988686
At time: 122.58910202980042 and batch: 600, loss is 4.231265501976013 and perplexity is 68.80424901532795
At time: 123.41683864593506 and batch: 650, loss is 4.237890272140503 and perplexity is 69.26157451698971
At time: 124.24294662475586 and batch: 700, loss is 4.217776870727539 and perplexity is 67.88240505554525
At time: 125.06678676605225 and batch: 750, loss is 4.2137628221511845 and perplexity is 67.610467933715
At time: 125.89291429519653 and batch: 800, loss is 4.244500637054443 and perplexity is 69.72093539786572
At time: 126.717125415802 and batch: 850, loss is 4.289846935272217 and perplexity is 72.9553007617098
At time: 127.54487586021423 and batch: 900, loss is 4.248812990188599 and perplexity is 70.02224590385688
At time: 128.36790823936462 and batch: 950, loss is 4.239161491394043 and perplexity is 69.34967715106407
At time: 129.19383263587952 and batch: 1000, loss is 4.226246957778931 and perplexity is 68.45981684762498
At time: 130.02710676193237 and batch: 1050, loss is 4.22273428440094 and perplexity is 68.21976173595633
At time: 130.85677814483643 and batch: 1100, loss is 4.191115026473999 and perplexity is 66.09644917801789
At time: 131.6849753856659 and batch: 1150, loss is 4.194754543304444 and perplexity is 66.33744660831316
At time: 132.52127385139465 and batch: 1200, loss is 4.197452039718628 and perplexity is 66.51663320160759
At time: 133.3448555469513 and batch: 1250, loss is 4.229243364334106 and perplexity is 68.66525793038498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.475856808850365 and perplexity of 87.86985583555366
Finished 6 epochs...
Completing Train Step...
At time: 135.7416548728943 and batch: 50, loss is 4.1692490386962895 and perplexity is 64.6668715504605
At time: 136.59655857086182 and batch: 100, loss is 4.188324308395385 and perplexity is 65.91224976622597
At time: 137.42616081237793 and batch: 150, loss is 4.114973068237305 and perplexity is 61.2505646548664
At time: 138.25015664100647 and batch: 200, loss is 4.16698233127594 and perplexity is 64.52045667539765
At time: 139.07373595237732 and batch: 250, loss is 4.176036148071289 and perplexity is 65.10726549097815
At time: 139.89629578590393 and batch: 300, loss is 4.182842187881469 and perplexity is 65.55189951390552
At time: 140.72057676315308 and batch: 350, loss is 4.172497763633728 and perplexity is 64.87729805253116
At time: 141.5460364818573 and batch: 400, loss is 4.18266411781311 and perplexity is 65.54022772190724
At time: 142.37217330932617 and batch: 450, loss is 4.102674045562744 and perplexity is 60.501856200294334
At time: 143.20153093338013 and batch: 500, loss is 4.146893582344055 and perplexity is 63.23725354661571
At time: 144.03221106529236 and batch: 550, loss is 4.136458868980408 and perplexity is 62.58082172708359
At time: 144.85701775550842 and batch: 600, loss is 4.170046977996826 and perplexity is 64.71849238111915
At time: 145.68259048461914 and batch: 650, loss is 4.178286504745484 and perplexity is 65.25394503916438
At time: 146.50755834579468 and batch: 700, loss is 4.157052402496338 and perplexity is 63.88294360389059
At time: 147.33114385604858 and batch: 750, loss is 4.15213369846344 and perplexity is 63.56949382685155
At time: 148.15687108039856 and batch: 800, loss is 4.1817132139205935 and perplexity is 65.47793488619388
At time: 148.98267674446106 and batch: 850, loss is 4.229356861114502 and perplexity is 68.67305165835825
At time: 149.81867814064026 and batch: 900, loss is 4.187764511108399 and perplexity is 65.87536259326062
At time: 150.64434599876404 and batch: 950, loss is 4.181356883049011 and perplexity is 65.45460723301478
At time: 151.46838116645813 and batch: 1000, loss is 4.1662244844436644 and perplexity is 64.47157857508533
At time: 152.29127955436707 and batch: 1050, loss is 4.164787912368775 and perplexity is 64.37902700010123
At time: 153.11616325378418 and batch: 1100, loss is 4.129295749664307 and perplexity is 62.13414952718895
At time: 153.9386646747589 and batch: 1150, loss is 4.135216708183289 and perplexity is 62.50313454365859
At time: 154.76391100883484 and batch: 1200, loss is 4.13713050365448 and perplexity is 62.622867294937514
At time: 155.59260630607605 and batch: 1250, loss is 4.169635119438172 and perplexity is 64.69184300439245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.474790252908303 and perplexity of 87.77618767872804
Finished 7 epochs...
Completing Train Step...
At time: 158.07349491119385 and batch: 50, loss is 4.114098963737487 and perplexity is 61.197048653381536
At time: 158.89885711669922 and batch: 100, loss is 4.129196252822876 and perplexity is 62.12796768310809
At time: 159.72466444969177 and batch: 150, loss is 4.057681384086609 and perplexity is 57.840046632714824
At time: 160.54845786094666 and batch: 200, loss is 4.109990649223327 and perplexity is 60.94614767314178
At time: 161.37300658226013 and batch: 250, loss is 4.121698479652405 and perplexity is 61.663888229195884
At time: 162.19540309906006 and batch: 300, loss is 4.1290634250640865 and perplexity is 62.11971591244684
At time: 163.0187919139862 and batch: 350, loss is 4.118629260063171 and perplexity is 61.47491835885146
At time: 163.8521068096161 and batch: 400, loss is 4.128786478042603 and perplexity is 62.10251442420988
At time: 164.67730116844177 and batch: 450, loss is 4.04803994178772 and perplexity is 57.28506487198419
At time: 165.50132131576538 and batch: 500, loss is 4.094474310874939 and perplexity is 60.00778542423353
At time: 166.32834148406982 and batch: 550, loss is 4.082285323143005 and perplexity is 59.28079092823504
At time: 167.15459871292114 and batch: 600, loss is 4.118457770347595 and perplexity is 61.464376946484755
At time: 167.98141288757324 and batch: 650, loss is 4.129535403251648 and perplexity is 62.14904198346355
At time: 168.80577874183655 and batch: 700, loss is 4.106634149551391 and perplexity is 60.74192487707778
At time: 169.63460755348206 and batch: 750, loss is 4.099340181350708 and perplexity is 60.300487082306624
At time: 170.46408653259277 and batch: 800, loss is 4.131058297157288 and perplexity is 62.243760485550325
At time: 171.29335236549377 and batch: 850, loss is 4.178573746681213 and perplexity is 65.27269140089312
At time: 172.12060451507568 and batch: 900, loss is 4.138781881332397 and perplexity is 62.72636673492732
At time: 172.9451723098755 and batch: 950, loss is 4.131069445610047 and perplexity is 62.244454411041744
At time: 173.76983547210693 and batch: 1000, loss is 4.119129409790039 and perplexity is 61.50567271272762
At time: 174.5944492816925 and batch: 1050, loss is 4.116092300415039 and perplexity is 61.31915663574826
At time: 175.41834998130798 and batch: 1100, loss is 4.0797167491912845 and perplexity is 59.12871922014213
At time: 176.2422571182251 and batch: 1150, loss is 4.084392395019531 and perplexity is 59.40583150409611
At time: 177.06990432739258 and batch: 1200, loss is 4.089541602134704 and perplexity is 59.71251334088608
At time: 177.89970707893372 and batch: 1250, loss is 4.120413484573365 and perplexity is 61.58470132455445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4793215563697535 and perplexity of 88.17483072599795
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 180.35398960113525 and batch: 50, loss is 4.090413208007813 and perplexity is 59.764581806505916
At time: 181.2051877975464 and batch: 100, loss is 4.105372157096863 and perplexity is 60.66531737542113
At time: 182.03227519989014 and batch: 150, loss is 4.028030552864075 and perplexity is 56.15021739523165
At time: 182.8605513572693 and batch: 200, loss is 4.074050545692444 and perplexity is 58.7946312634376
At time: 183.68666052818298 and batch: 250, loss is 4.082216815948486 and perplexity is 59.27672990666587
At time: 184.51415419578552 and batch: 300, loss is 4.077424883842468 and perplexity is 58.99335933001231
At time: 185.34068536758423 and batch: 350, loss is 4.0615719079971315 and perplexity is 58.06551302392662
At time: 186.17721581459045 and batch: 400, loss is 4.068352546691894 and perplexity is 58.46057215133395
At time: 187.00143146514893 and batch: 450, loss is 3.97955361366272 and perplexity is 53.49315028572949
At time: 187.8279812335968 and batch: 500, loss is 4.011892886161804 and perplexity is 55.25135617010183
At time: 188.6554136276245 and batch: 550, loss is 3.9890856742858887 and perplexity is 54.005488175837854
At time: 189.47837257385254 and batch: 600, loss is 4.015807528495788 and perplexity is 55.4680693685555
At time: 190.30616807937622 and batch: 650, loss is 4.019837093353272 and perplexity is 55.69203248550018
At time: 191.13172602653503 and batch: 700, loss is 3.9869007539749144 and perplexity is 53.88761930175784
At time: 191.95863676071167 and batch: 750, loss is 3.9741317558288576 and perplexity is 53.20390286755739
At time: 192.7851278781891 and batch: 800, loss is 3.995285725593567 and perplexity is 54.34136512463913
At time: 193.61002278327942 and batch: 850, loss is 4.0315427541732785 and perplexity is 56.34777499021924
At time: 194.44354009628296 and batch: 900, loss is 3.9744350051879884 and perplexity is 53.22003936357262
At time: 195.27095246315002 and batch: 950, loss is 3.9580775403976443 and perplexity is 52.3565757342531
At time: 196.09554767608643 and batch: 1000, loss is 3.9334806776046753 and perplexity is 51.084477176373824
At time: 196.92074728012085 and batch: 1050, loss is 3.9259292602539064 and perplexity is 50.70016982788262
At time: 197.74615859985352 and batch: 1100, loss is 3.8747520780563356 and perplexity is 48.17075422354689
At time: 198.5722839832306 and batch: 1150, loss is 3.86947461605072 and perplexity is 47.917204536251106
At time: 199.3989486694336 and batch: 1200, loss is 3.870019292831421 and perplexity is 47.94331103411428
At time: 200.27974128723145 and batch: 1250, loss is 3.8972154378890993 and perplexity is 49.26507626792783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.373005442375685 and perplexity of 79.28155012577557
Finished 9 epochs...
Completing Train Step...
At time: 202.73125672340393 and batch: 50, loss is 4.005653333663941 and perplexity is 54.90768572258117
At time: 203.55415153503418 and batch: 100, loss is 4.01786250114441 and perplexity is 55.582171932650134
At time: 204.38189387321472 and batch: 150, loss is 3.9409105730056764 and perplexity is 51.465443014024196
At time: 205.2043764591217 and batch: 200, loss is 3.990368447303772 and perplexity is 54.07480941108679
At time: 206.0283544063568 and batch: 250, loss is 4.002027897834778 and perplexity is 54.708981843191374
At time: 206.8485562801361 and batch: 300, loss is 4.00228590965271 and perplexity is 54.72309922820172
At time: 207.68352937698364 and batch: 350, loss is 3.989192771911621 and perplexity is 54.011272345127864
At time: 208.53827476501465 and batch: 400, loss is 4.00134539604187 and perplexity is 54.671655604056696
At time: 209.39359307289124 and batch: 450, loss is 3.914802975654602 and perplexity is 50.1391918963443
At time: 210.22013425827026 and batch: 500, loss is 3.951863250732422 and perplexity is 52.03222565323511
At time: 211.06333875656128 and batch: 550, loss is 3.932177448272705 and perplexity is 51.017945749579106
At time: 211.89610886573792 and batch: 600, loss is 3.9648867893218993 and perplexity is 52.71430123003613
At time: 212.72173285484314 and batch: 650, loss is 3.973562021255493 and perplexity is 53.173599397942695
At time: 213.5492091178894 and batch: 700, loss is 3.9440471839904787 and perplexity is 51.627123519789436
At time: 214.37485790252686 and batch: 750, loss is 3.9355233573913573 and perplexity is 51.18893305394955
At time: 215.2003300189972 and batch: 800, loss is 3.960061492919922 and perplexity is 52.460551802425506
At time: 216.02759790420532 and batch: 850, loss is 4.001028671264648 and perplexity is 54.65434247800787
At time: 216.85078811645508 and batch: 900, loss is 3.948205509185791 and perplexity is 51.84225286718068
At time: 217.68417143821716 and batch: 950, loss is 3.9361326122283935 and perplexity is 51.22012966139239
At time: 218.50831055641174 and batch: 1000, loss is 3.91573052406311 and perplexity is 50.185719999189665
At time: 219.33546566963196 and batch: 1050, loss is 3.9142826318740847 and perplexity is 50.11310906628859
At time: 220.1634612083435 and batch: 1100, loss is 3.8671429920196534 and perplexity is 47.805609779717685
At time: 220.98836302757263 and batch: 1150, loss is 3.866655158996582 and perplexity is 47.782294312068984
At time: 221.84469842910767 and batch: 1200, loss is 3.8730486249923706 and perplexity is 48.088767454802934
At time: 222.66861844062805 and batch: 1250, loss is 3.9037338399887087 and perplexity is 49.58725474818334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368753474994297 and perplexity of 78.94516322057028
Finished 10 epochs...
Completing Train Step...
At time: 225.08978581428528 and batch: 50, loss is 3.9718614768981935 and perplexity is 53.083252175031824
At time: 225.94485783576965 and batch: 100, loss is 3.9824933433532714 and perplexity is 53.65063705871995
At time: 226.76666831970215 and batch: 150, loss is 3.9055457639694215 and perplexity is 49.67718453256194
At time: 227.5899829864502 and batch: 200, loss is 3.9558417415618896 and perplexity is 52.23964772564339
At time: 228.41413688659668 and batch: 250, loss is 3.9689723014831544 and perplexity is 52.93010668651728
At time: 229.2400505542755 and batch: 300, loss is 3.9702046394348143 and perplexity is 52.99537467362127
At time: 230.0632848739624 and batch: 350, loss is 3.956353282928467 and perplexity is 52.26637730248974
At time: 230.88792777061462 and batch: 400, loss is 3.9704209327697755 and perplexity is 53.006838459672515
At time: 231.71180844306946 and batch: 450, loss is 3.8855447959899903 and perplexity is 48.69346323813841
At time: 232.54789066314697 and batch: 500, loss is 3.9239823484420775 and perplexity is 50.60155709467953
At time: 233.3761384487152 and batch: 550, loss is 3.905867967605591 and perplexity is 49.69319328095296
At time: 234.20429253578186 and batch: 600, loss is 3.9403865575790404 and perplexity is 51.4384813927159
At time: 235.03179359436035 and batch: 650, loss is 3.9500140571594238 and perplexity is 51.9360969036941
At time: 235.85874438285828 and batch: 700, loss is 3.9219385814666747 and perplexity is 50.49824491235976
At time: 236.68651795387268 and batch: 750, loss is 3.915141282081604 and perplexity is 50.15615717677743
At time: 237.51303625106812 and batch: 800, loss is 3.9409613466262816 and perplexity is 51.46805616724114
At time: 238.33543968200684 and batch: 850, loss is 3.983394494056702 and perplexity is 53.69900615869587
At time: 239.16876697540283 and batch: 900, loss is 3.9324659442901613 and perplexity is 51.03266634706143
At time: 239.99508047103882 and batch: 950, loss is 3.92217405796051 and perplexity is 50.51013746216958
At time: 240.82056546211243 and batch: 1000, loss is 3.9029246091842653 and perplexity is 49.54714344597293
At time: 241.64577341079712 and batch: 1050, loss is 3.90381956577301 and perplexity is 49.59150583669933
At time: 242.52577829360962 and batch: 1100, loss is 3.8587163305282592 and perplexity is 47.40446063648436
At time: 243.3484456539154 and batch: 1150, loss is 3.8597149658203125 and perplexity is 47.45182404932832
At time: 244.17763090133667 and batch: 1200, loss is 3.8685584592819215 and perplexity is 47.87332496832407
At time: 245.0087673664093 and batch: 1250, loss is 3.8999880743026734 and perplexity is 49.401859950387994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368311972513686 and perplexity of 78.9103164282148
Finished 11 epochs...
Completing Train Step...
At time: 247.44207763671875 and batch: 50, loss is 3.947169532775879 and perplexity is 51.788573326343545
At time: 248.29696416854858 and batch: 100, loss is 3.9571606874465943 and perplexity is 52.30859445253531
At time: 249.12209057807922 and batch: 150, loss is 3.8806158971786497 and perplexity is 48.45404859503442
At time: 249.94856667518616 and batch: 200, loss is 3.931621541976929 and perplexity is 50.989592433962855
At time: 250.78110194206238 and batch: 250, loss is 3.9458214139938352 and perplexity is 51.7188032177108
At time: 251.6093876361847 and batch: 300, loss is 3.947386884689331 and perplexity is 51.79983089523372
At time: 252.44487166404724 and batch: 350, loss is 3.932713074684143 and perplexity is 51.04527962850013
At time: 253.27022433280945 and batch: 400, loss is 3.948023247718811 and perplexity is 51.83280488314942
At time: 254.09829115867615 and batch: 450, loss is 3.864034848213196 and perplexity is 47.65725374514542
At time: 254.92996859550476 and batch: 500, loss is 3.90356650352478 and perplexity is 49.5789576865377
At time: 255.7543170452118 and batch: 550, loss is 3.886676664352417 and perplexity is 48.748609031636754
At time: 256.5819127559662 and batch: 600, loss is 3.922131977081299 and perplexity is 50.50801199589714
At time: 257.40563321113586 and batch: 650, loss is 3.9323038911819457 and perplexity is 51.02439701491295
At time: 258.2342965602875 and batch: 700, loss is 3.9049482107162476 and perplexity is 49.64750863668308
At time: 259.05891013145447 and batch: 750, loss is 3.8995356512069703 and perplexity is 49.379514463164156
At time: 259.8875095844269 and batch: 800, loss is 3.925736355781555 and perplexity is 50.69039048164398
At time: 260.71159625053406 and batch: 850, loss is 3.9689832496643067 and perplexity is 52.93068617808588
At time: 261.5440080165863 and batch: 900, loss is 3.9192226123809815 and perplexity is 50.36127932164486
At time: 262.3732633590698 and batch: 950, loss is 3.9098420906066895 and perplexity is 49.89107308226674
At time: 263.2064151763916 and batch: 1000, loss is 3.8909746694564817 and perplexity is 48.95858171066118
At time: 264.06403708457947 and batch: 1050, loss is 3.8934445858001707 and perplexity is 49.07965477040632
At time: 264.88932633399963 and batch: 1100, loss is 3.849020185470581 and perplexity is 46.947041295605736
At time: 266.0111563205719 and batch: 1150, loss is 3.8510467052459716 and perplexity is 47.0422768689811
At time: 266.840713262558 and batch: 1200, loss is 3.8614942979812623 and perplexity is 47.536331767309136
At time: 267.6659324169159 and batch: 1250, loss is 3.893364291191101 and perplexity is 49.075714096922795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.369141961536268 and perplexity of 78.9758383120795
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 270.15754413604736 and batch: 50, loss is 3.9487366390228273 and perplexity is 51.86979514811374
At time: 270.99234771728516 and batch: 100, loss is 3.9686267709732057 and perplexity is 52.91182087909636
At time: 271.82219314575195 and batch: 150, loss is 3.8939019441604614 and perplexity is 49.10210689477777
At time: 272.6517162322998 and batch: 200, loss is 3.945778727531433 and perplexity is 51.716595572080394
At time: 273.48001766204834 and batch: 250, loss is 3.960765724182129 and perplexity is 52.4975091747785
At time: 274.3374285697937 and batch: 300, loss is 3.9589244747161865 and perplexity is 52.40093709797277
At time: 275.167272567749 and batch: 350, loss is 3.935675654411316 and perplexity is 51.196729569586516
At time: 275.9937069416046 and batch: 400, loss is 3.947111325263977 and perplexity is 51.78555893007637
At time: 276.8190805912018 and batch: 450, loss is 3.8665269374847413 and perplexity is 47.77616798682482
At time: 277.6511631011963 and batch: 500, loss is 3.9015713930130005 and perplexity is 49.48014079498185
At time: 278.4847629070282 and batch: 550, loss is 3.880510439872742 and perplexity is 48.44893903103442
At time: 279.3115818500519 and batch: 600, loss is 3.910140385627747 and perplexity is 49.90595756083494
At time: 280.1385521888733 and batch: 650, loss is 3.9216074705123902 and perplexity is 50.48152715816565
At time: 280.96501779556274 and batch: 700, loss is 3.8882446336746215 and perplexity is 48.82510531133606
At time: 281.79506635665894 and batch: 750, loss is 3.8777831745147706 and perplexity is 48.31698593532668
At time: 282.61992025375366 and batch: 800, loss is 3.89771915435791 and perplexity is 49.28989814925156
At time: 283.4501919746399 and batch: 850, loss is 3.9399827909469605 and perplexity is 51.41771644270372
At time: 284.295282125473 and batch: 900, loss is 3.884646425247192 and perplexity is 48.64973809903337
At time: 285.1522822380066 and batch: 950, loss is 3.8671122407913208 and perplexity is 47.804139721098935
At time: 286.0097932815552 and batch: 1000, loss is 3.848062348365784 and perplexity is 46.90209520644464
At time: 286.85208463668823 and batch: 1050, loss is 3.8469210386276247 and perplexity is 46.84859592387884
At time: 287.7045114040375 and batch: 1100, loss is 3.7978378915786744 and perplexity is 44.604640093204644
At time: 288.53531074523926 and batch: 1150, loss is 3.7929710721969605 and perplexity is 44.388084761569985
At time: 289.3629221916199 and batch: 1200, loss is 3.8029893970489503 and perplexity is 44.835014017438056
At time: 290.2010090351105 and batch: 1250, loss is 3.8365817642211915 and perplexity is 46.36671089862865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.349644570455064 and perplexity of 77.45092968379461
Finished 13 epochs...
Completing Train Step...
At time: 292.6239449977875 and batch: 50, loss is 3.930586242675781 and perplexity is 50.936830261583
At time: 293.45365810394287 and batch: 100, loss is 3.945507354736328 and perplexity is 51.70256299910207
At time: 294.2949080467224 and batch: 150, loss is 3.8694071674346926 and perplexity is 47.91397269611403
At time: 295.1218571662903 and batch: 200, loss is 3.9203952312469483 and perplexity is 50.42036854569028
At time: 295.9475836753845 and batch: 250, loss is 3.936392779350281 and perplexity is 51.23345718872613
At time: 296.77817940711975 and batch: 300, loss is 3.9358883571624754 and perplexity is 51.20762041303145
At time: 297.6072278022766 and batch: 350, loss is 3.9131670427322387 and perplexity is 50.057234598219864
At time: 298.434978723526 and batch: 400, loss is 3.9270412969589232 and perplexity is 50.756581637870106
At time: 299.2635962963104 and batch: 450, loss is 3.846523389816284 and perplexity is 46.829970338863035
At time: 300.0919189453125 and batch: 500, loss is 3.882926445007324 and perplexity is 48.56613343062796
At time: 300.91699838638306 and batch: 550, loss is 3.863987135887146 and perplexity is 47.654979960960276
At time: 301.79962706565857 and batch: 600, loss is 3.8945653295516967 and perplexity is 49.13469132199215
At time: 302.62438702583313 and batch: 650, loss is 3.9076888132095338 and perplexity is 49.78375934186892
At time: 303.44917249679565 and batch: 700, loss is 3.8757983589172365 and perplexity is 48.221180737294084
At time: 304.2893261909485 and batch: 750, loss is 3.867687673568726 and perplexity is 47.83165570603066
At time: 305.11666202545166 and batch: 800, loss is 3.889709234237671 and perplexity is 48.896666979904055
At time: 305.9442434310913 and batch: 850, loss is 3.933755621910095 and perplexity is 51.09852449349525
At time: 306.7715563774109 and batch: 900, loss is 3.8799536275863646 and perplexity is 48.42196957567885
At time: 307.5964434146881 and batch: 950, loss is 3.864745020866394 and perplexity is 47.69111064418069
At time: 308.4279537200928 and batch: 1000, loss is 3.8470032167434693 and perplexity is 46.852446011416134
At time: 309.2546045780182 and batch: 1050, loss is 3.848321318626404 and perplexity is 46.914243027158136
At time: 310.08186292648315 and batch: 1100, loss is 3.8007404232025146 and perplexity is 44.73429454369167
At time: 310.9082815647125 and batch: 1150, loss is 3.798083462715149 and perplexity is 44.61559505041996
At time: 311.7359793186188 and batch: 1200, loss is 3.809690823554993 and perplexity is 45.13648157188404
At time: 312.56256771087646 and batch: 1250, loss is 3.8427980518341065 and perplexity is 46.655837425021105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348583806170164 and perplexity of 77.36881606305148
Finished 14 epochs...
Completing Train Step...
At time: 315.0161635875702 and batch: 50, loss is 3.92152587890625 and perplexity is 50.477408457312386
At time: 315.8782844543457 and batch: 100, loss is 3.935237832069397 and perplexity is 51.17431940374138
At time: 316.70165061950684 and batch: 150, loss is 3.8585147857666016 and perplexity is 47.39490747849068
At time: 317.5270731449127 and batch: 200, loss is 3.9089505195617678 and perplexity is 49.84661146939505
At time: 318.3570647239685 and batch: 250, loss is 3.9255266618728637 and perplexity is 50.67976212992005
At time: 319.18574714660645 and batch: 300, loss is 3.9256738805770874 and perplexity is 50.68722368805817
At time: 320.01046562194824 and batch: 350, loss is 3.902743353843689 and perplexity is 49.53816357546238
At time: 320.8354034423828 and batch: 400, loss is 3.9172595739364624 and perplexity is 50.262515164855465
At time: 321.6609606742859 and batch: 450, loss is 3.8367365026474 and perplexity is 46.373886165632
At time: 322.48692202568054 and batch: 500, loss is 3.873836498260498 and perplexity is 48.12667023851215
At time: 323.3424668312073 and batch: 550, loss is 3.8556684112548827 and perplexity is 47.26019563299158
At time: 324.1708354949951 and batch: 600, loss is 3.886948752403259 and perplexity is 48.76187475028935
At time: 324.9939832687378 and batch: 650, loss is 3.900862855911255 and perplexity is 49.44509469662565
At time: 325.8260660171509 and batch: 700, loss is 3.86947416305542 and perplexity is 47.91718282998758
At time: 326.6511766910553 and batch: 750, loss is 3.8623374128341674 and perplexity is 47.57642725485019
At time: 327.47525358200073 and batch: 800, loss is 3.8852606868743895 and perplexity is 48.67963094639541
At time: 328.30141258239746 and batch: 850, loss is 3.9299462461471557 and perplexity is 50.904241296561
At time: 329.125456571579 and batch: 900, loss is 3.876852331161499 and perplexity is 48.27203131621884
At time: 329.94997119903564 and batch: 950, loss is 3.862552809715271 and perplexity is 47.58667617264776
At time: 330.77903628349304 and batch: 1000, loss is 3.845556826591492 and perplexity is 46.78472807998948
At time: 331.60803484916687 and batch: 1050, loss is 3.8481403589248657 and perplexity is 46.905754207832324
At time: 332.4315321445465 and batch: 1100, loss is 3.800982036590576 and perplexity is 44.7451042539917
At time: 333.26387548446655 and batch: 1150, loss is 3.799352025985718 and perplexity is 44.672228669665316
At time: 334.09267592430115 and batch: 1200, loss is 3.8116228914260866 and perplexity is 45.22377261670089
At time: 334.9188952445984 and batch: 1250, loss is 3.8444457244873047 and perplexity is 46.73277433847668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348225614450274 and perplexity of 77.34110815642761
Finished 15 epochs...
Completing Train Step...
At time: 337.41336154937744 and batch: 50, loss is 3.913839979171753 and perplexity is 50.09093127203056
At time: 338.2402777671814 and batch: 100, loss is 3.927121934890747 and perplexity is 50.760674708665995
At time: 339.0664961338043 and batch: 150, loss is 3.8501889753341674 and perplexity is 47.00194460056042
At time: 339.89133071899414 and batch: 200, loss is 3.9005446529388426 and perplexity is 49.42936362349183
At time: 340.71863293647766 and batch: 250, loss is 3.917574324607849 and perplexity is 50.278337815213476
At time: 341.5430123806 and batch: 300, loss is 3.918153095245361 and perplexity is 50.30744586347312
At time: 342.3774325847626 and batch: 350, loss is 3.895028443336487 and perplexity is 49.15745154473444
At time: 343.20320868492126 and batch: 400, loss is 3.909936909675598 and perplexity is 49.89580393164707
At time: 344.06368494033813 and batch: 450, loss is 3.8295488929748536 and perplexity is 46.04176378572307
At time: 344.8902952671051 and batch: 500, loss is 3.867216787338257 and perplexity is 47.809137740094286
At time: 345.71399569511414 and batch: 550, loss is 3.8495662069320677 and perplexity is 46.97268238736259
At time: 346.54130244255066 and batch: 600, loss is 3.88125159740448 and perplexity is 48.48486063724213
At time: 347.36822533607483 and batch: 650, loss is 3.895756893157959 and perplexity is 49.193273327137696
At time: 348.1966962814331 and batch: 700, loss is 3.8646596384048464 and perplexity is 47.68703883359303
At time: 349.03021359443665 and batch: 750, loss is 3.8580803155899046 and perplexity is 47.37432027725029
At time: 349.85952043533325 and batch: 800, loss is 3.8815274333953855 and perplexity is 48.49823635148682
At time: 350.6904618740082 and batch: 850, loss is 3.926695146560669 and perplexity is 50.739015267400596
At time: 351.52000737190247 and batch: 900, loss is 3.8738958072662353 and perplexity is 48.129524668119295
At time: 352.34961128234863 and batch: 950, loss is 3.860120425224304 and perplexity is 47.471067738629436
At time: 353.17559909820557 and batch: 1000, loss is 3.8436590909957884 and perplexity is 46.696027228179304
At time: 354.00620198249817 and batch: 1050, loss is 3.8470704174041748 and perplexity is 46.85559463253729
At time: 354.8351833820343 and batch: 1100, loss is 3.800050754547119 and perplexity is 44.70345333925211
At time: 355.6644649505615 and batch: 1150, loss is 3.7991152572631837 and perplexity is 44.66165293520142
At time: 356.4935007095337 and batch: 1200, loss is 3.811841287612915 and perplexity is 45.23365039478965
At time: 357.33045291900635 and batch: 1250, loss is 3.8445181322097777 and perplexity is 46.736158274741456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348154778028056 and perplexity of 77.33562978307194
Finished 16 epochs...
Completing Train Step...
At time: 359.7858672142029 and batch: 50, loss is 3.907071590423584 and perplexity is 49.7530411521914
At time: 360.6643271446228 and batch: 100, loss is 3.920146498680115 and perplexity is 50.407828917572814
At time: 361.4930028915405 and batch: 150, loss is 3.843232250213623 and perplexity is 46.67609971263393
At time: 362.32317543029785 and batch: 200, loss is 3.8935923099517824 and perplexity is 49.08690555631358
At time: 363.1512689590454 and batch: 250, loss is 3.9109864234924316 and perplexity is 49.94819775648767
At time: 363.98083996772766 and batch: 300, loss is 3.91186222076416 and perplexity is 49.99196141305877
At time: 364.82069396972656 and batch: 350, loss is 3.8884890985488894 and perplexity is 48.837042793655215
At time: 365.7208433151245 and batch: 400, loss is 3.903802919387817 and perplexity is 49.59068032426179
At time: 366.55322098731995 and batch: 450, loss is 3.8235694932937623 and perplexity is 45.76728311065318
At time: 367.38131618499756 and batch: 500, loss is 3.861682252883911 and perplexity is 47.54526729363043
At time: 368.2176299095154 and batch: 550, loss is 3.8444304132461546 and perplexity is 46.73205880717702
At time: 369.0459372997284 and batch: 600, loss is 3.8763967895507814 and perplexity is 48.25004640522151
At time: 369.87405133247375 and batch: 650, loss is 3.891354675292969 and perplexity is 48.97718979282335
At time: 370.699524641037 and batch: 700, loss is 3.8604641342163086 and perplexity is 47.48738677581052
At time: 371.52798295021057 and batch: 750, loss is 3.8542635011672974 and perplexity is 47.19384592600514
At time: 372.35340118408203 and batch: 800, loss is 3.8780843734741213 and perplexity is 48.331541153107544
At time: 373.17877745628357 and batch: 850, loss is 3.923582339286804 and perplexity is 50.581320056340715
At time: 374.0119936466217 and batch: 900, loss is 3.870995512008667 and perplexity is 47.99013706628298
At time: 374.83647871017456 and batch: 950, loss is 3.857537937164307 and perplexity is 47.34863243490002
At time: 375.66251611709595 and batch: 1000, loss is 3.8414989423751833 and perplexity is 46.59526573845259
At time: 376.487416267395 and batch: 1050, loss is 3.8455456924438476 and perplexity is 46.78420717481946
At time: 377.31721925735474 and batch: 1100, loss is 3.7985733270645143 and perplexity is 44.63745599387215
At time: 378.1451852321625 and batch: 1150, loss is 3.7981991243362425 and perplexity is 44.620755660906084
At time: 378.9703142642975 and batch: 1200, loss is 3.8112533712387084 and perplexity is 45.20706460693319
At time: 379.8020420074463 and batch: 1250, loss is 3.8437930059432985 and perplexity is 46.70228094293809
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.348238979812956 and perplexity of 77.34214185529625
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 382.2745678424835 and batch: 50, loss is 3.908861165046692 and perplexity is 49.842157648586486
At time: 383.106947183609 and batch: 100, loss is 3.929754056930542 and perplexity is 50.89445899036089
At time: 383.9337980747223 and batch: 150, loss is 3.8572856473922728 and perplexity is 47.33668836596346
At time: 384.7602870464325 and batch: 200, loss is 3.909993133544922 and perplexity is 49.89860934567201
At time: 385.5835974216461 and batch: 250, loss is 3.9270540475845337 and perplexity is 50.757228820165814
At time: 386.40834069252014 and batch: 300, loss is 3.9282467412948607 and perplexity is 50.817802763633324
At time: 387.2641706466675 and batch: 350, loss is 3.9003978967666626 and perplexity is 49.42211009155647
At time: 388.0872759819031 and batch: 400, loss is 3.914291777610779 and perplexity is 50.11356738968489
At time: 388.9089663028717 and batch: 450, loss is 3.8319930171966554 and perplexity is 46.154433208750994
At time: 389.73356199264526 and batch: 500, loss is 3.8682258701324463 and perplexity is 47.85740546736369
At time: 390.56287813186646 and batch: 550, loss is 3.850629529953003 and perplexity is 47.02265608628381
At time: 391.38927698135376 and batch: 600, loss is 3.8812591552734377 and perplexity is 48.48522708085003
At time: 392.21362018585205 and batch: 650, loss is 3.8958436679840087 and perplexity is 49.19754225008831
At time: 393.04354786872864 and batch: 700, loss is 3.86315420627594 and perplexity is 47.615303243279776
At time: 393.8717997074127 and batch: 750, loss is 3.853101863861084 and perplexity is 47.139055623344774
At time: 394.7006661891937 and batch: 800, loss is 3.8756449317932127 and perplexity is 48.213782867747966
At time: 395.529794216156 and batch: 850, loss is 3.9203075838088988 and perplexity is 50.415949523222565
At time: 396.3536584377289 and batch: 900, loss is 3.8670029211044312 and perplexity is 47.798914073150904
At time: 397.17713737487793 and batch: 950, loss is 3.849437141418457 and perplexity is 46.96662022520104
At time: 398.006635427475 and batch: 1000, loss is 3.831171545982361 and perplexity is 46.1165342390455
At time: 398.83124685287476 and batch: 1050, loss is 3.8332993936538697 and perplexity is 46.214767674873364
At time: 399.6655161380768 and batch: 1100, loss is 3.783827929496765 and perplexity is 43.98408788336912
At time: 400.4899055957794 and batch: 1150, loss is 3.782933917045593 and perplexity is 43.94478313323384
At time: 401.3176124095917 and batch: 1200, loss is 3.7953424310684203 and perplexity is 44.49346974357779
At time: 402.14742708206177 and batch: 1250, loss is 3.83007031917572 and perplexity is 46.06577742782206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.342221893533303 and perplexity of 76.87816480952704
Finished 18 epochs...
Completing Train Step...
At time: 404.590350151062 and batch: 50, loss is 3.904325180053711 and perplexity is 49.61658635025043
At time: 405.44347763061523 and batch: 100, loss is 3.920786375999451 and perplexity is 50.440094065781516
At time: 406.270024061203 and batch: 150, loss is 3.847941727638245 and perplexity is 46.89643818278175
At time: 407.0969753265381 and batch: 200, loss is 3.8992226552963256 and perplexity is 49.36406129558286
At time: 407.9218256473541 and batch: 250, loss is 3.9156650495529175 and perplexity is 50.18243422132257
At time: 408.784859418869 and batch: 300, loss is 3.9178963708877563 and perplexity is 50.29453237442559
At time: 409.60936164855957 and batch: 350, loss is 3.891245036125183 and perplexity is 48.97182026885442
At time: 410.4397087097168 and batch: 400, loss is 3.9048648357391356 and perplexity is 49.64336944934154
At time: 411.26512908935547 and batch: 450, loss is 3.8243407821655273 and perplexity is 45.802596523477895
At time: 412.0902826786041 and batch: 500, loss is 3.8617571115493776 and perplexity is 47.548826602110154
At time: 412.91769647598267 and batch: 550, loss is 3.8448173713684084 and perplexity is 46.75014565610296
At time: 413.7424433231354 and batch: 600, loss is 3.8757138299942016 and perplexity is 48.217104825087546
At time: 414.568071603775 and batch: 650, loss is 3.890965633392334 and perplexity is 48.95813931977501
At time: 415.39457964897156 and batch: 700, loss is 3.858789310455322 and perplexity is 47.40792033680687
At time: 416.22111892700195 and batch: 750, loss is 3.849633741378784 and perplexity is 46.975854768599696
At time: 417.05434226989746 and batch: 800, loss is 3.8728808164596558 and perplexity is 48.08069842634113
At time: 417.87907218933105 and batch: 850, loss is 3.918533434867859 and perplexity is 50.32658341759602
At time: 418.70522379875183 and batch: 900, loss is 3.865883822441101 and perplexity is 47.745452292386254
At time: 419.52974128723145 and batch: 950, loss is 3.849072365760803 and perplexity is 46.949491069760015
At time: 420.35619473457336 and batch: 1000, loss is 3.831902551651001 and perplexity is 46.15025801162498
At time: 421.1815445423126 and batch: 1050, loss is 3.8348905181884767 and perplexity is 46.28835965706029
At time: 422.007848739624 and batch: 1100, loss is 3.786073579788208 and perplexity is 44.08297175088073
At time: 422.83252477645874 and batch: 1150, loss is 3.785754179954529 and perplexity is 44.06889390538584
At time: 423.6623315811157 and batch: 1200, loss is 3.7986057662963866 and perplexity is 44.638904022143656
At time: 424.4889853000641 and batch: 1250, loss is 3.833076629638672 and perplexity is 46.20447383425588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.341767471202099 and perplexity of 76.84323759111064
Finished 19 epochs...
Completing Train Step...
At time: 426.94412994384766 and batch: 50, loss is 3.9019594192504883 and perplexity is 49.49934411329963
At time: 427.7996904850006 and batch: 100, loss is 3.9177950859069823 and perplexity is 50.28943855164923
At time: 428.6249649524689 and batch: 150, loss is 3.844686689376831 and perplexity is 46.74403665313912
At time: 429.4782729148865 and batch: 200, loss is 3.8955985355377196 and perplexity is 49.18548381422246
At time: 430.30831837654114 and batch: 250, loss is 3.911983361244202 and perplexity is 49.99801783009381
At time: 431.1401560306549 and batch: 300, loss is 3.9144913959503174 and perplexity is 50.123571975311734
At time: 431.9660270214081 and batch: 350, loss is 3.8879583835601808 and perplexity is 48.81113111950553
At time: 432.7924425601959 and batch: 400, loss is 3.901601724624634 and perplexity is 49.48164163015727
At time: 433.616094827652 and batch: 450, loss is 3.8213584804534912 and perplexity is 45.666202846090854
At time: 434.4428131580353 and batch: 500, loss is 3.8589608764648435 and perplexity is 47.41605462228226
At time: 435.27452206611633 and batch: 550, loss is 3.842398076057434 and perplexity is 46.63717995172814
At time: 436.10043454170227 and batch: 600, loss is 3.873409824371338 and perplexity is 48.10614022507095
At time: 436.9287393093109 and batch: 650, loss is 3.888969368934631 and perplexity is 48.86050341230545
At time: 437.7552480697632 and batch: 700, loss is 3.8569372034072877 and perplexity is 47.32019705494915
At time: 438.58262729644775 and batch: 750, loss is 3.8481556034088134 and perplexity is 46.90646926729974
At time: 439.404913187027 and batch: 800, loss is 3.871734104156494 and perplexity is 48.02559529766699
At time: 440.2275969982147 and batch: 850, loss is 3.917781229019165 and perplexity is 50.288741701368934
At time: 441.0607271194458 and batch: 900, loss is 3.8653619289398193 and perplexity is 47.72054075226998
At time: 441.9236271381378 and batch: 950, loss is 3.848936233520508 and perplexity is 46.94310016537391
At time: 442.79423546791077 and batch: 1000, loss is 3.832184147834778 and perplexity is 46.16325557810806
At time: 443.6193218231201 and batch: 1050, loss is 3.8355074977874755 and perplexity is 46.316927442604026
At time: 444.46555852890015 and batch: 1100, loss is 3.786921224594116 and perplexity is 44.12035429423542
At time: 445.3012070655823 and batch: 1150, loss is 3.786885166168213 and perplexity is 44.11876341239179
At time: 446.1269874572754 and batch: 1200, loss is 3.7998869132995607 and perplexity is 44.6961296696628
At time: 446.9525923728943 and batch: 1250, loss is 3.8340961360931396 and perplexity is 46.25160361403032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.341633817575274 and perplexity of 76.83296790001566
Finished 20 epochs...
Completing Train Step...
At time: 449.3896117210388 and batch: 50, loss is 3.8997682237625124 and perplexity is 49.391000118606634
At time: 450.2148334980011 and batch: 100, loss is 3.91531334400177 and perplexity is 50.16478788397614
At time: 451.07946586608887 and batch: 150, loss is 3.842159972190857 and perplexity is 46.626076780761814
At time: 451.9052040576935 and batch: 200, loss is 3.8928819274902344 and perplexity is 49.052047462269954
At time: 452.7319190502167 and batch: 250, loss is 3.909372162818909 and perplexity is 49.867633388576245
At time: 453.5554311275482 and batch: 300, loss is 3.912060647010803 and perplexity is 50.001882114555514
At time: 454.38320446014404 and batch: 350, loss is 3.885472660064697 and perplexity is 48.68995081679941
At time: 455.2131812572479 and batch: 400, loss is 3.89929349899292 and perplexity is 49.36755855204177
At time: 456.03918528556824 and batch: 450, loss is 3.819198017120361 and perplexity is 45.5676491883946
At time: 456.8691122531891 and batch: 500, loss is 3.8569017696380614 and perplexity is 47.3185203517131
At time: 457.6951401233673 and batch: 550, loss is 3.8406071424484254 and perplexity is 46.553730607146065
At time: 458.5288863182068 and batch: 600, loss is 3.871723103523254 and perplexity is 48.02506698861286
At time: 459.35931062698364 and batch: 650, loss is 3.8874625635147093 and perplexity is 48.786935581066516
At time: 460.1868395805359 and batch: 700, loss is 3.8555501317977905 and perplexity is 47.254606053282785
At time: 461.0148069858551 and batch: 750, loss is 3.846982145309448 and perplexity is 46.85145877359257
At time: 461.8403034210205 and batch: 800, loss is 3.8708151531219483 and perplexity is 47.981482399084754
At time: 462.6669635772705 and batch: 850, loss is 3.9170921182632448 and perplexity is 50.2540991262173
At time: 463.4958050251007 and batch: 900, loss is 3.8648553037643434 and perplexity is 47.69637044809683
At time: 464.32274627685547 and batch: 950, loss is 3.8486403226852417 and perplexity is 46.92921124843584
At time: 465.149062871933 and batch: 1000, loss is 3.832137846946716 and perplexity is 46.16111822785995
At time: 465.97529006004333 and batch: 1050, loss is 3.8356845140457154 and perplexity is 46.325127017500705
At time: 466.807888507843 and batch: 1100, loss is 3.7872602796554564 and perplexity is 44.13531605995487
At time: 467.6336371898651 and batch: 1150, loss is 3.7874292373657226 and perplexity is 44.1427736918927
At time: 468.46112275123596 and batch: 1200, loss is 3.8005282163619993 and perplexity is 44.724802627544264
At time: 469.288779258728 and batch: 1250, loss is 3.8345334672927858 and perplexity is 46.27183530697722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.341594612511405 and perplexity of 76.82995571764867
Finished 21 epochs...
Completing Train Step...
At time: 471.7497048377991 and batch: 50, loss is 3.89776575088501 and perplexity is 49.29219494083727
At time: 472.60546374320984 and batch: 100, loss is 3.913134469985962 and perplexity is 50.05560412317266
At time: 473.4294366836548 and batch: 150, loss is 3.83998929977417 and perplexity is 46.52497660936931
At time: 474.257887840271 and batch: 200, loss is 3.89058988571167 and perplexity is 48.93974686815289
At time: 475.08263325691223 and batch: 250, loss is 3.9071739292144776 and perplexity is 49.758133078812556
At time: 475.90997648239136 and batch: 300, loss is 3.9100189638137817 and perplexity is 49.89989825681353
At time: 476.7404808998108 and batch: 350, loss is 3.8833980894088747 and perplexity is 48.589044778145144
At time: 477.56648421287537 and batch: 400, loss is 3.8973377180099487 and perplexity is 49.27110077573879
At time: 478.3928873538971 and batch: 450, loss is 3.817340998649597 and perplexity is 45.483107743989564
At time: 479.21921706199646 and batch: 500, loss is 3.8551482486724855 and perplexity is 47.2356190400527
At time: 480.0485894680023 and batch: 550, loss is 3.839069375991821 and perplexity is 46.48219685698583
At time: 480.8735661506653 and batch: 600, loss is 3.870279474258423 and perplexity is 47.95578661608131
At time: 481.7007005214691 and batch: 650, loss is 3.8861474800109863 and perplexity is 48.7228188555512
At time: 482.5260009765625 and batch: 700, loss is 3.854334411621094 and perplexity is 47.19719258169118
At time: 483.35213232040405 and batch: 750, loss is 3.8459094285964968 and perplexity is 46.80122737758585
At time: 484.1788401603699 and batch: 800, loss is 3.86994731426239 and perplexity is 47.93986026738356
At time: 485.00519847869873 and batch: 850, loss is 3.9163841915130617 and perplexity is 50.21853549484775
At time: 485.8320994377136 and batch: 900, loss is 3.864305534362793 and perplexity is 47.67015564976673
At time: 486.6608419418335 and batch: 950, loss is 3.8482368421554565 and perplexity is 46.91028004486177
At time: 487.4889941215515 and batch: 1000, loss is 3.831923732757568 and perplexity is 46.151235535510516
At time: 488.3168988227844 and batch: 1050, loss is 3.8356534814834595 and perplexity is 46.32368945241831
At time: 489.14617896080017 and batch: 1100, loss is 3.7873452520370483 and perplexity is 44.13906650221254
At time: 489.9762086868286 and batch: 1150, loss is 3.7876772594451906 and perplexity is 44.153723432249805
At time: 490.8090069293976 and batch: 1200, loss is 3.8008524560928345 and perplexity is 44.73930653675462
At time: 491.63754963874817 and batch: 1250, loss is 3.8346792888641357 and perplexity is 46.278583230695396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.341590602902601 and perplexity of 76.8296476601994
Finished 22 epochs...
Completing Train Step...
At time: 494.1058416366577 and batch: 50, loss is 3.8959015369415284 and perplexity is 49.20038934294921
At time: 494.9331605434418 and batch: 100, loss is 3.9111486434936524 and perplexity is 49.95630101042587
At time: 495.7644000053406 and batch: 150, loss is 3.8380418014526367 and perplexity is 46.43445746706175
At time: 496.5912826061249 and batch: 200, loss is 3.8885544681549074 and perplexity is 48.84023535624886
At time: 497.4266221523285 and batch: 250, loss is 3.905223021507263 and perplexity is 49.66115418268923
At time: 498.2559473514557 and batch: 300, loss is 3.9082140159606933 and perplexity is 49.809912776565085
At time: 499.08486247062683 and batch: 350, loss is 3.8815337419509888 and perplexity is 48.49854230627256
At time: 499.90967631340027 and batch: 400, loss is 3.895594048500061 and perplexity is 49.185263117599455
At time: 500.74231362342834 and batch: 450, loss is 3.8156720542907716 and perplexity is 45.40726227643584
At time: 501.57263946533203 and batch: 500, loss is 3.853575015068054 and perplexity is 47.16136480179954
At time: 502.4068989753723 and batch: 550, loss is 3.837678689956665 and perplexity is 46.41759964256641
At time: 503.2417347431183 and batch: 600, loss is 3.868978533744812 and perplexity is 47.89343955411305
At time: 504.0670039653778 and batch: 650, loss is 3.8849440383911134 and perplexity is 48.664219055294545
At time: 504.9020342826843 and batch: 700, loss is 3.8532131814956667 and perplexity is 47.144303323588595
At time: 505.7373032569885 and batch: 750, loss is 3.8449029445648195 and perplexity is 46.75414638667465
At time: 506.5647530555725 and batch: 800, loss is 3.869108214378357 and perplexity is 47.89965080842806
At time: 507.39242577552795 and batch: 850, loss is 3.9156695556640626 and perplexity is 50.18266034945819
At time: 508.21836042404175 and batch: 900, loss is 3.8637244844436647 and perplexity is 47.642464955298564
At time: 509.050537109375 and batch: 950, loss is 3.847761745452881 and perplexity is 46.88799841887819
At time: 509.8825099468231 and batch: 1000, loss is 3.83161648273468 and perplexity is 46.1370577455106
At time: 510.71350932121277 and batch: 1050, loss is 3.8354756736755373 and perplexity is 46.315453470974504
At time: 511.5386209487915 and batch: 1100, loss is 3.7872569942474366 and perplexity is 44.13517105767173
At time: 512.3696718215942 and batch: 1150, loss is 3.7877458906173707 and perplexity is 44.156753858034726
At time: 513.1953980922699 and batch: 1200, loss is 3.800999150276184 and perplexity is 44.74587001419086
At time: 514.0254361629486 and batch: 1250, loss is 3.8347050285339357 and perplexity is 46.27977444147719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.341604859289462 and perplexity of 76.8307429811865
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 516.5034613609314 and batch: 50, loss is 3.896433415412903 and perplexity is 49.22656493132285
At time: 517.3905603885651 and batch: 100, loss is 3.9142461681365965 and perplexity is 50.11128178834978
At time: 518.2232413291931 and batch: 150, loss is 3.84368914604187 and perplexity is 46.69743070052008
At time: 519.0696983337402 and batch: 200, loss is 3.8946278572082518 and perplexity is 49.137763695149225
At time: 519.9250612258911 and batch: 250, loss is 3.9107984352111815 and perplexity is 49.938808963159104
At time: 520.7697131633759 and batch: 300, loss is 3.9141826152801515 and perplexity is 50.10809717444874
At time: 521.5951504707336 and batch: 350, loss is 3.886315493583679 and perplexity is 48.73100563814482
At time: 522.4200780391693 and batch: 400, loss is 3.8994932699203493 and perplexity is 49.37742174015495
At time: 523.2506387233734 and batch: 450, loss is 3.8187240600585937 and perplexity is 45.546057196516315
At time: 524.0760564804077 and batch: 500, loss is 3.857040810585022 and perplexity is 47.32510002100268
At time: 524.9009804725647 and batch: 550, loss is 3.8406618309020994 and perplexity is 46.556276628304076
At time: 525.7259948253632 and batch: 600, loss is 3.8719666242599486 and perplexity is 48.036763512420976
At time: 526.5512487888336 and batch: 650, loss is 3.887701640129089 and perplexity is 48.79860079083507
At time: 527.3760981559753 and batch: 700, loss is 3.8555445194244387 and perplexity is 47.25434084353525
At time: 528.2062003612518 and batch: 750, loss is 3.845800004005432 and perplexity is 46.79610645260141
At time: 529.0327055454254 and batch: 800, loss is 3.8681190538406374 and perplexity is 47.85229378978615
At time: 529.8607623577118 and batch: 850, loss is 3.9129109382629395 and perplexity is 50.04441635819284
At time: 530.6941330432892 and batch: 900, loss is 3.862414765357971 and perplexity is 47.58010755391029
At time: 531.5202231407166 and batch: 950, loss is 3.846813397407532 and perplexity is 46.84355335525294
At time: 532.3495092391968 and batch: 1000, loss is 3.8287216567993165 and perplexity is 46.003692122433655
At time: 533.177725315094 and batch: 1050, loss is 3.8287792778015137 and perplexity is 46.006342977650256
At time: 534.0026340484619 and batch: 1100, loss is 3.779611759185791 and perplexity is 43.79903386164419
At time: 534.8292071819305 and batch: 1150, loss is 3.780329523086548 and perplexity is 43.83048251204093
At time: 535.6533482074738 and batch: 1200, loss is 3.7945693349838256 and perplexity is 44.459085309277334
At time: 536.5321137905121 and batch: 1250, loss is 3.8296128702163696 and perplexity is 46.04470950499311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340904514284899 and perplexity of 76.77695379183636
Finished 24 epochs...
Completing Train Step...
At time: 538.9570486545563 and batch: 50, loss is 3.895727596282959 and perplexity is 49.19183213906943
At time: 539.8197238445282 and batch: 100, loss is 3.9125146865844727 and perplexity is 50.02459010256597
At time: 540.6467468738556 and batch: 150, loss is 3.841218156814575 and perplexity is 46.58218429726637
At time: 541.4708919525146 and batch: 200, loss is 3.891643362045288 and perplexity is 48.99133089975928
At time: 542.2956130504608 and batch: 250, loss is 3.9075481128692626 and perplexity is 49.77675524274066
At time: 543.1257357597351 and batch: 300, loss is 3.910870370864868 and perplexity is 49.942401473239435
At time: 543.9534997940063 and batch: 350, loss is 3.8830373287200928 and perplexity is 48.5715189223937
At time: 544.7858145236969 and batch: 400, loss is 3.8961996364593507 and perplexity is 49.21505814156128
At time: 545.6146230697632 and batch: 450, loss is 3.815639066696167 and perplexity is 45.40576442478115
At time: 546.4454379081726 and batch: 500, loss is 3.8544158267974855 and perplexity is 47.201035305876324
At time: 547.2761149406433 and batch: 550, loss is 3.838200488090515 and perplexity is 46.44182657967329
At time: 548.0995864868164 and batch: 600, loss is 3.869994626045227 and perplexity is 47.94212844129701
At time: 548.9270308017731 and batch: 650, loss is 3.8858840322494506 and perplexity is 48.7099846286364
At time: 549.7565240859985 and batch: 700, loss is 3.8539173650741576 and perplexity is 47.17751325938184
At time: 550.5843122005463 and batch: 750, loss is 3.844348611831665 and perplexity is 46.72823621501344
At time: 551.4075915813446 and batch: 800, loss is 3.8673553323745726 and perplexity is 47.815761917681584
At time: 552.2369866371155 and batch: 850, loss is 3.9125605058670043 and perplexity is 50.02688224590519
At time: 553.0628976821899 and batch: 900, loss is 3.862064037322998 and perplexity is 47.5634228023602
At time: 553.8944756984711 and batch: 950, loss is 3.846707592010498 and perplexity is 46.83859731668415
At time: 554.7199800014496 and batch: 1000, loss is 3.8291558122634886 and perplexity is 46.02366921300842
At time: 555.5463094711304 and batch: 1050, loss is 3.8296112394332886 and perplexity is 46.0446344161211
At time: 556.3719472885132 and batch: 1100, loss is 3.7809713459014893 and perplexity is 43.8586229453271
At time: 557.2511115074158 and batch: 1150, loss is 3.782224416732788 and perplexity is 43.91361535393671
At time: 558.0766019821167 and batch: 1200, loss is 3.79649010181427 and perplexity is 44.544562910642306
At time: 558.9031517505646 and batch: 1250, loss is 3.8311748552322387 and perplexity is 46.1166868504333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340692896042427 and perplexity of 76.76070810681482
Finished 25 epochs...
Completing Train Step...
At time: 561.3489444255829 and batch: 50, loss is 3.8952284479141235 and perplexity is 49.16728424332792
At time: 562.1793699264526 and batch: 100, loss is 3.911607027053833 and perplexity is 49.979205406634776
At time: 563.006016254425 and batch: 150, loss is 3.8400865507125856 and perplexity is 46.529501427022154
At time: 563.8302807807922 and batch: 200, loss is 3.8901191473007204 and perplexity is 48.91671447102223
At time: 564.6555194854736 and batch: 250, loss is 3.905842261314392 and perplexity is 49.69191586967471
At time: 565.4810092449188 and batch: 300, loss is 3.9092192459106445 and perplexity is 49.86000836726821
At time: 566.31187915802 and batch: 350, loss is 3.881531639099121 and perplexity is 48.49844032112952
At time: 567.142463684082 and batch: 400, loss is 3.8946725511550904 and perplexity is 49.13995990482585
At time: 567.9681355953217 and batch: 450, loss is 3.814342803955078 and perplexity is 45.34694475523589
At time: 568.8045344352722 and batch: 500, loss is 3.8532990074157714 and perplexity is 47.14834970043856
At time: 569.6332142353058 and batch: 550, loss is 3.837109489440918 and perplexity is 46.39118623888475
At time: 570.4590165615082 and batch: 600, loss is 3.869088134765625 and perplexity is 47.89868901164611
At time: 571.2853944301605 and batch: 650, loss is 3.88506040096283 and perplexity is 48.669882078449966
At time: 572.1159954071045 and batch: 700, loss is 3.853194990158081 and perplexity is 47.14344571345216
At time: 572.9415545463562 and batch: 750, loss is 3.843698987960815 and perplexity is 46.69789029510962
At time: 573.7686324119568 and batch: 800, loss is 3.8670174217224123 and perplexity is 47.7996071919691
At time: 574.5955865383148 and batch: 850, loss is 3.9124202871322633 and perplexity is 50.01986803154726
At time: 575.4282999038696 and batch: 900, loss is 3.8620290517807008 and perplexity is 47.56175879932813
At time: 576.254597902298 and batch: 950, loss is 3.8467875051498415 and perplexity is 46.842340485600374
At time: 577.0825088024139 and batch: 1000, loss is 3.8295026063919066 and perplexity is 46.03963271912485
At time: 577.9085149765015 and batch: 1050, loss is 3.830123085975647 and perplexity is 46.06820823561548
At time: 578.7867708206177 and batch: 1100, loss is 3.7816933822631835 and perplexity is 43.89030190117019
At time: 579.6125679016113 and batch: 1150, loss is 3.7831480932235717 and perplexity is 43.954196066904515
At time: 580.4363071918488 and batch: 1200, loss is 3.797404046058655 and perplexity is 44.585292767106964
At time: 581.2622671127319 and batch: 1250, loss is 3.8318534231185915 and perplexity is 46.14799077287206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3406127038663325 and perplexity of 76.7545527454024
Finished 26 epochs...
Completing Train Step...
At time: 583.7088930606842 and batch: 50, loss is 3.894699239730835 and perplexity is 49.14127139786872
At time: 584.5805830955505 and batch: 100, loss is 3.9108622646331788 and perplexity is 49.941996630202844
At time: 585.4065933227539 and batch: 150, loss is 3.8392558097839355 and perplexity is 46.49086351706574
At time: 586.2320425510406 and batch: 200, loss is 3.889093680381775 and perplexity is 48.86657770973733
At time: 587.0559737682343 and batch: 250, loss is 3.9047298574447633 and perplexity is 49.636669124215786
At time: 587.8870050907135 and batch: 300, loss is 3.9081667184829714 and perplexity is 49.807556949037995
At time: 588.7164855003357 and batch: 350, loss is 3.8805767774581907 and perplexity is 48.45215312327368
At time: 589.545031785965 and batch: 400, loss is 3.8937032985687257 and perplexity is 49.09235394642035
At time: 590.3748908042908 and batch: 450, loss is 3.813521499633789 and perplexity is 45.3097164035469
At time: 591.2071380615234 and batch: 500, loss is 3.8525864410400392 and perplexity is 47.11476533873525
At time: 592.0415089130402 and batch: 550, loss is 3.8364298915863038 and perplexity is 46.35966959877731
At time: 592.8839569091797 and batch: 600, loss is 3.8684995555877686 and perplexity is 47.870505135682144
At time: 593.7163667678833 and batch: 650, loss is 3.884544949531555 and perplexity is 48.64480158251673
At time: 594.5560743808746 and batch: 700, loss is 3.852744164466858 and perplexity is 47.1221970270385
At time: 595.3818438053131 and batch: 750, loss is 3.8432917404174805 and perplexity is 46.67887656591807
At time: 596.2076411247253 and batch: 800, loss is 3.86678503036499 and perplexity is 47.78850026699619
At time: 597.032972574234 and batch: 850, loss is 3.912291169166565 and perplexity is 50.01340998487642
At time: 597.8600640296936 and batch: 900, loss is 3.861997060775757 and perplexity is 47.560237275204926
At time: 598.689783334732 and batch: 950, loss is 3.846829662322998 and perplexity is 46.8443152678846
At time: 599.524017572403 and batch: 1000, loss is 3.82969407081604 and perplexity is 46.04844851481943
At time: 600.3864948749542 and batch: 1050, loss is 3.830418519973755 and perplexity is 46.08182036120353
At time: 601.2115867137909 and batch: 1100, loss is 3.7820966386795045 and perplexity is 43.90800451613274
At time: 602.0414526462555 and batch: 1150, loss is 3.7836575508117676 and perplexity is 43.97659457068325
At time: 602.8689720630646 and batch: 1200, loss is 3.797899694442749 and perplexity is 44.60739687290099
At time: 603.695196390152 and batch: 1250, loss is 3.8321988105773928 and perplexity is 46.16393246300534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340577508411268 and perplexity of 76.75185138152844
Finished 27 epochs...
Completing Train Step...
At time: 606.1454854011536 and batch: 50, loss is 3.894171814918518 and perplexity is 49.115359905807274
At time: 606.9702117443085 and batch: 100, loss is 3.9102104234695436 and perplexity is 49.9094529888
At time: 607.7940309047699 and batch: 150, loss is 3.8385673999786376 and perplexity is 46.45886976443384
At time: 608.6232056617737 and batch: 200, loss is 3.8882949924468995 and perplexity is 48.8275641456073
At time: 609.4467511177063 and batch: 250, loss is 3.9038981866836546 and perplexity is 49.59540491932115
At time: 610.2715983390808 and batch: 300, loss is 3.907387580871582 and perplexity is 49.768765122135655
At time: 611.0988347530365 and batch: 350, loss is 3.8798539781570436 and perplexity is 48.41714459445134
At time: 611.9234538078308 and batch: 400, loss is 3.892978172302246 and perplexity is 49.05676869455016
At time: 612.7516701221466 and batch: 450, loss is 3.8128912591934205 and perplexity is 45.28116938461555
At time: 613.5805706977844 and batch: 500, loss is 3.8520352697372435 and perplexity is 47.08880418732064
At time: 614.4117240905762 and batch: 550, loss is 3.8359234952926635 and perplexity is 46.33619917708693
At time: 615.2422587871552 and batch: 600, loss is 3.8680492496490477 and perplexity is 47.84895361568288
At time: 616.0722682476044 and batch: 650, loss is 3.8841539859771728 and perplexity is 48.625786955243136
At time: 616.9063239097595 and batch: 700, loss is 3.852403326034546 and perplexity is 47.1061387080783
At time: 617.733564376831 and batch: 750, loss is 3.842980327606201 and perplexity is 46.664342428915
At time: 618.5584661960602 and batch: 800, loss is 3.8665880298614503 and perplexity is 47.77908683563598
At time: 619.3831810951233 and batch: 850, loss is 3.9121530628204346 and perplexity is 50.0065032925059
At time: 620.2168321609497 and batch: 900, loss is 3.861939034461975 and perplexity is 47.55747761002062
At time: 621.094279050827 and batch: 950, loss is 3.846827907562256 and perplexity is 46.8442330673913
At time: 621.9192383289337 and batch: 1000, loss is 3.829787645339966 and perplexity is 46.05275767807753
At time: 622.7440304756165 and batch: 1050, loss is 3.8305864906311036 and perplexity is 46.08956140497733
At time: 623.5677251815796 and batch: 1100, loss is 3.7823321342468263 and perplexity is 43.918345874190436
At time: 624.3961992263794 and batch: 1150, loss is 3.783963656425476 and perplexity is 43.99005811368104
At time: 625.2197999954224 and batch: 1200, loss is 3.7981967878341676 and perplexity is 44.62065140453969
At time: 626.0516176223755 and batch: 1250, loss is 3.832394070625305 and perplexity is 46.17294731476144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340563252024407 and perplexity of 76.75075718524249
Finished 28 epochs...
Completing Train Step...
At time: 628.4713599681854 and batch: 50, loss is 3.893659443855286 and perplexity is 49.090201062513344
At time: 629.3270864486694 and batch: 100, loss is 3.9096177434921264 and perplexity is 49.87988141943385
At time: 630.1553621292114 and batch: 150, loss is 3.837959427833557 and perplexity is 46.43063265028435
At time: 630.9803822040558 and batch: 200, loss is 3.8876164674758913 and perplexity is 48.79444466153016
At time: 631.8041214942932 and batch: 250, loss is 3.9032159519195555 and perplexity is 49.56158074926989
At time: 632.6349267959595 and batch: 300, loss is 3.9067505311965944 and perplexity is 49.73707004323183
At time: 633.4580974578857 and batch: 350, loss is 3.8792475032806397 and perplexity is 48.38778971506577
At time: 634.2861604690552 and batch: 400, loss is 3.892379584312439 and perplexity is 49.027412688943585
At time: 635.1117112636566 and batch: 450, loss is 3.8123569297790527 and perplexity is 45.25698078681153
At time: 635.9390072822571 and batch: 500, loss is 3.8515633821487425 and perplexity is 47.06658880706064
At time: 636.7652108669281 and batch: 550, loss is 3.8355016469955445 and perplexity is 46.31665645269143
At time: 637.5898857116699 and batch: 600, loss is 3.867668242454529 and perplexity is 47.83072629269621
At time: 638.4163336753845 and batch: 650, loss is 3.8838207817077635 and perplexity is 48.60958733446755
At time: 639.241142988205 and batch: 700, loss is 3.852112822532654 and perplexity is 47.09245619732787
At time: 640.0718250274658 and batch: 750, loss is 3.8427115869522095 and perplexity is 46.65180350794694
At time: 640.9020569324493 and batch: 800, loss is 3.866404013633728 and perplexity is 47.77029551720991
At time: 641.7345957756042 and batch: 850, loss is 3.912006096839905 and perplexity is 49.99915457773541
At time: 642.5901131629944 and batch: 900, loss is 3.86185884475708 and perplexity is 47.55366414282798
At time: 643.4181225299835 and batch: 950, loss is 3.846793131828308 and perplexity is 46.842604053130415
At time: 644.2505640983582 and batch: 1000, loss is 3.8298219299316405 and perplexity is 46.05433660513629
At time: 645.0815811157227 and batch: 1050, loss is 3.8306767654418947 and perplexity is 46.09372231922266
At time: 645.9132361412048 and batch: 1100, loss is 3.782472224235535 and perplexity is 43.92449882574144
At time: 646.7393395900726 and batch: 1150, loss is 3.7841587448120118 and perplexity is 43.99864090031582
At time: 647.5636475086212 and batch: 1200, loss is 3.79838837146759 and perplexity is 44.62920080999839
At time: 648.3906621932983 and batch: 1250, loss is 3.8325125932693482 and perplexity is 46.1784201788832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340557905879334 and perplexity of 76.75034686565691
Finished 29 epochs...
Completing Train Step...
At time: 650.8632135391235 and batch: 50, loss is 3.893164401054382 and perplexity is 49.065905326093315
At time: 651.7181432247162 and batch: 100, loss is 3.9090655422210694 and perplexity is 49.8523452889567
At time: 652.547709941864 and batch: 150, loss is 3.837403082847595 and perplexity is 46.4048083848807
At time: 653.3742575645447 and batch: 200, loss is 3.8870095205307007 and perplexity is 48.76483800814338
At time: 654.2015357017517 and batch: 250, loss is 3.902620768547058 and perplexity is 49.532091297179555
At time: 655.0281388759613 and batch: 300, loss is 3.906195559501648 and perplexity is 49.709475035100745
At time: 655.8545715808868 and batch: 350, loss is 3.8787073850631715 and perplexity is 48.36166164509612
At time: 656.6818633079529 and batch: 400, loss is 3.89185350894928 and perplexity is 49.00162735811682
At time: 657.5091736316681 and batch: 450, loss is 3.811877074241638 and perplexity is 45.23526918360744
At time: 658.3364458084106 and batch: 500, loss is 3.851136074066162 and perplexity is 47.046481169627846
At time: 659.1631002426147 and batch: 550, loss is 3.8351249742507934 and perplexity is 46.29921351592337
At time: 659.9955730438232 and batch: 600, loss is 3.867325224876404 and perplexity is 47.81432232638856
At time: 660.824079990387 and batch: 650, loss is 3.883516263961792 and perplexity is 48.594787106080325
At time: 661.6496427059174 and batch: 700, loss is 3.851847920417786 and perplexity is 47.0799829582535
At time: 662.4759590625763 and batch: 750, loss is 3.8424627494812014 and perplexity is 46.640196235366126
At time: 663.3019256591797 and batch: 800, loss is 3.8662241649627687 and perplexity is 47.76170486558159
At time: 664.1582272052765 and batch: 850, loss is 3.911851887702942 and perplexity is 49.99144484572993
At time: 664.9859910011292 and batch: 900, loss is 3.86176251411438 and perplexity is 47.54908348843059
At time: 665.8154952526093 and batch: 950, loss is 3.8467355394363403 and perplexity is 46.839906353201236
At time: 666.6465535163879 and batch: 1000, loss is 3.8298186922073363 and perplexity is 46.05418749413274
At time: 667.4729776382446 and batch: 1050, loss is 3.8307179260253905 and perplexity is 46.095619602775194
At time: 668.3250670433044 and batch: 1100, loss is 3.7825545978546145 and perplexity is 43.928117194703
At time: 669.1876232624054 and batch: 1150, loss is 3.784287576675415 and perplexity is 44.004309692362895
At time: 670.0314676761627 and batch: 1200, loss is 3.798518195152283 and perplexity is 44.63499511340321
At time: 670.8579967021942 and batch: 1250, loss is 3.8325868320465086 and perplexity is 46.18184853558542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340558796903513 and perplexity of 76.75041525210217
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 673.3543171882629 and batch: 50, loss is 3.8932740926742553 and perplexity is 49.07128773992652
At time: 674.1889009475708 and batch: 100, loss is 3.9098456287384034 and perplexity is 49.89124960376693
At time: 675.0179853439331 and batch: 150, loss is 3.838876872062683 and perplexity is 46.47324971266374
At time: 675.8453168869019 and batch: 200, loss is 3.888680839538574 and perplexity is 48.84640775436848
At time: 676.6754846572876 and batch: 250, loss is 3.9039708042144774 and perplexity is 49.59900654593558
At time: 677.5054426193237 and batch: 300, loss is 3.9075679636001586 and perplexity is 49.77774335752123
At time: 678.3333003520966 and batch: 350, loss is 3.8797228813171385 and perplexity is 48.41079767583735
At time: 679.1612043380737 and batch: 400, loss is 3.8922818756103514 and perplexity is 49.02262251810754
At time: 679.9842545986176 and batch: 450, loss is 3.811931304931641 and perplexity is 45.23772238998666
At time: 680.8123128414154 and batch: 500, loss is 3.851468563079834 and perplexity is 47.06212620850632
At time: 681.6391804218292 and batch: 550, loss is 3.835110845565796 and perplexity is 46.29855937354107
At time: 682.4659860134125 and batch: 600, loss is 3.867482843399048 and perplexity is 47.82185934320599
At time: 683.29083776474 and batch: 650, loss is 3.883789577484131 and perplexity is 48.60807053369909
At time: 684.1268963813782 and batch: 700, loss is 3.8522453117370605 and perplexity is 47.09869585271736
At time: 684.9584274291992 and batch: 750, loss is 3.8421114778518675 and perplexity is 46.62381573481307
At time: 685.8468060493469 and batch: 800, loss is 3.865185866355896 and perplexity is 47.71213969013696
At time: 686.672679901123 and batch: 850, loss is 3.909813766479492 and perplexity is 49.889659981179264
At time: 687.5018982887268 and batch: 900, loss is 3.860104160308838 and perplexity is 47.47029563200473
At time: 688.3319134712219 and batch: 950, loss is 3.845493140220642 and perplexity is 46.781748625323196
At time: 689.1573877334595 and batch: 1000, loss is 3.828289370536804 and perplexity is 45.98380965607244
At time: 689.9866049289703 and batch: 1050, loss is 3.8284200382232667 and perplexity is 45.98981864667683
At time: 690.8184585571289 and batch: 1100, loss is 3.779755778312683 and perplexity is 43.805342214510475
At time: 691.6438162326813 and batch: 1150, loss is 3.7812247323989867 and perplexity is 43.86973753626444
At time: 692.4755945205688 and batch: 1200, loss is 3.7959779787063597 and perplexity is 44.52175645099996
At time: 693.3007197380066 and batch: 1250, loss is 3.8304450607299803 and perplexity is 46.083043423794585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340502216868157 and perplexity of 76.74607283374189
Finished 31 epochs...
Completing Train Step...
At time: 695.7627551555634 and batch: 50, loss is 3.8930643081665037 and perplexity is 49.060994423710156
At time: 696.6189982891083 and batch: 100, loss is 3.909507074356079 and perplexity is 49.87436156149561
At time: 697.4540045261383 and batch: 150, loss is 3.838373274803162 and perplexity is 46.44985180352336
At time: 698.2885165214539 and batch: 200, loss is 3.8882171154022216 and perplexity is 48.82376174727451
At time: 699.1144759654999 and batch: 250, loss is 3.9035654067993164 and perplexity is 49.57890331206215
At time: 699.9501235485077 and batch: 300, loss is 3.907144947052002 and perplexity is 49.75669100141254
At time: 700.7731080055237 and batch: 350, loss is 3.879327530860901 and perplexity is 48.3916622277427
At time: 701.6045715808868 and batch: 400, loss is 3.8919407081604005 and perplexity is 49.00590044766839
At time: 702.4344544410706 and batch: 450, loss is 3.811539258956909 and perplexity is 45.219990599083374
At time: 703.2620315551758 and batch: 500, loss is 3.851131000518799 and perplexity is 47.046242477682874
At time: 704.0883622169495 and batch: 550, loss is 3.8347888708114626 and perplexity is 46.28365480583681
At time: 704.9134795665741 and batch: 600, loss is 3.867221431732178 and perplexity is 47.809359785078605
At time: 705.7391917705536 and batch: 650, loss is 3.883539233207703 and perplexity is 48.59590330451441
At time: 706.6160731315613 and batch: 700, loss is 3.8520367622375487 and perplexity is 47.08887446742771
At time: 707.4474020004272 and batch: 750, loss is 3.842003860473633 and perplexity is 46.61879847197757
At time: 708.2782037258148 and batch: 800, loss is 3.8651712274551393 and perplexity is 47.71144124197143
At time: 709.1050314903259 and batch: 850, loss is 3.9098839950561524 and perplexity is 49.893163784021915
At time: 709.931421995163 and batch: 900, loss is 3.8601957178115844 and perplexity is 47.47464209269994
At time: 710.7566833496094 and batch: 950, loss is 3.8456082916259766 and perplexity is 46.787135919592714
At time: 711.5901913642883 and batch: 1000, loss is 3.828483781814575 and perplexity is 45.99275029631694
At time: 712.4157235622406 and batch: 1050, loss is 3.828610157966614 and perplexity is 45.99856305041008
At time: 713.2447164058685 and batch: 1100, loss is 3.780012230873108 and perplexity is 43.816577647297855
At time: 714.0749270915985 and batch: 1150, loss is 3.781552038192749 and perplexity is 43.884098705649656
At time: 714.9071273803711 and batch: 1200, loss is 3.7963043546676634 and perplexity is 44.53628965357556
At time: 715.7432708740234 and batch: 1250, loss is 3.8306641721725465 and perplexity is 46.09314185221723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340466130388914 and perplexity of 76.74330338814767
Finished 32 epochs...
Completing Train Step...
At time: 718.2091281414032 and batch: 50, loss is 3.8928916358947756 and perplexity is 49.05252368170195
At time: 719.0348370075226 and batch: 100, loss is 3.909271674156189 and perplexity is 49.862622508556555
At time: 719.8622794151306 and batch: 150, loss is 3.8380528402328493 and perplexity is 46.43497004966115
At time: 720.6925802230835 and batch: 200, loss is 3.8878722476959227 and perplexity is 48.806926911610276
At time: 721.5214767456055 and batch: 250, loss is 3.903247637748718 and perplexity is 49.563151173930514
At time: 722.3461618423462 and batch: 300, loss is 3.9068208646774294 and perplexity is 49.740568347517026
At time: 723.1757280826569 and batch: 350, loss is 3.879029755592346 and perplexity is 48.3772545327611
At time: 724.0080940723419 and batch: 400, loss is 3.891668429374695 and perplexity is 48.992558996981515
At time: 724.8407926559448 and batch: 450, loss is 3.811256518363953 and perplexity is 45.20720687945131
At time: 725.6741020679474 and batch: 500, loss is 3.850889687538147 and perplexity is 47.034890978369255
At time: 726.5025329589844 and batch: 550, loss is 3.834561653137207 and perplexity is 46.27313953610858
At time: 727.330837726593 and batch: 600, loss is 3.8670419120788573 and perplexity is 47.800777835721846
At time: 728.2102828025818 and batch: 650, loss is 3.883374171257019 and perplexity is 48.58788263189187
At time: 729.0429847240448 and batch: 700, loss is 3.8519017028808595 and perplexity is 47.08251510379037
At time: 729.8698732852936 and batch: 750, loss is 3.8419297981262206 and perplexity is 46.61534590218349
At time: 730.7011168003082 and batch: 800, loss is 3.8651606941223147 and perplexity is 47.71093868412809
At time: 731.5326836109161 and batch: 850, loss is 3.90993688583374 and perplexity is 49.89580274203843
At time: 732.3574686050415 and batch: 900, loss is 3.8602627086639405 and perplexity is 47.47782256596915
At time: 733.1869745254517 and batch: 950, loss is 3.8457019329071045 and perplexity is 46.791517332077866
At time: 734.0114741325378 and batch: 1000, loss is 3.82863862991333 and perplexity is 45.99987273769086
At time: 734.8418776988983 and batch: 1050, loss is 3.8287558555603027 and perplexity is 46.005265418607266
At time: 735.6677668094635 and batch: 1100, loss is 3.7802039909362795 and perplexity is 43.824980722656846
At time: 736.4976511001587 and batch: 1150, loss is 3.78180410861969 and perplexity is 43.8951619834503
At time: 737.3266172409058 and batch: 1200, loss is 3.7965562534332276 and perplexity is 44.54750970306108
At time: 738.1590132713318 and batch: 1250, loss is 3.830822439193726 and perplexity is 46.100437453786306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.34043850863937 and perplexity of 76.74118363311814
Finished 33 epochs...
Completing Train Step...
At time: 740.6581237316132 and batch: 50, loss is 3.892734169960022 and perplexity is 49.0448001883179
At time: 741.5125820636749 and batch: 100, loss is 3.909067211151123 and perplexity is 49.85242848910343
At time: 742.3430564403534 and batch: 150, loss is 3.837792978286743 and perplexity is 46.422904935676556
At time: 743.1723439693451 and batch: 200, loss is 3.887580547332764 and perplexity is 48.7926919895724
At time: 744.0036928653717 and batch: 250, loss is 3.9029685640335083 and perplexity is 49.54932133105748
At time: 744.8458440303802 and batch: 300, loss is 3.9065469455718995 and perplexity is 49.726945321415464
At time: 745.7068161964417 and batch: 350, loss is 3.8787777328491213 and perplexity is 48.365063900586875
At time: 746.5829575061798 and batch: 400, loss is 3.8914328145980837 and perplexity is 48.98101698592539
At time: 747.4248263835907 and batch: 450, loss is 3.8110277700424193 and perplexity is 45.19686698941775
At time: 748.2518315315247 and batch: 500, loss is 3.850694007873535 and perplexity is 47.0256881071143
At time: 749.0760545730591 and batch: 550, loss is 3.834381275177002 and perplexity is 46.26479363431778
At time: 749.9360229969025 and batch: 600, loss is 3.8668990993499754 and perplexity is 47.793951763633046
At time: 750.7610063552856 and batch: 650, loss is 3.8832455492019653 and perplexity is 48.58163356046986
At time: 751.5866298675537 and batch: 700, loss is 3.8517972469329833 and perplexity is 47.077597311897584
At time: 752.4117996692657 and batch: 750, loss is 3.841866374015808 and perplexity is 46.61238945909395
At time: 753.2386567592621 and batch: 800, loss is 3.8651474618911745 and perplexity is 47.71030736613639
At time: 754.0630750656128 and batch: 850, loss is 3.909972381591797 and perplexity is 49.897573862814056
At time: 754.8867101669312 and batch: 900, loss is 3.8603116130828856 and perplexity is 47.48014449807042
At time: 755.7119309902191 and batch: 950, loss is 3.8457743024826048 and perplexity is 46.79490373685906
At time: 756.5437090396881 and batch: 1000, loss is 3.8287598991394045 and perplexity is 46.00545144491319
At time: 757.3794231414795 and batch: 1050, loss is 3.8288695573806764 and perplexity is 46.010496598423835
At time: 758.2051661014557 and batch: 1100, loss is 3.7803538751602175 and perplexity is 43.8315498881764
At time: 759.0325350761414 and batch: 1150, loss is 3.782002191543579 and perplexity is 43.903857726691264
At time: 759.8598504066467 and batch: 1200, loss is 3.796753716468811 and perplexity is 44.55630705810237
At time: 760.6873342990875 and batch: 1250, loss is 3.830942006111145 and perplexity is 46.105949870529116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340417124059078 and perplexity of 76.73954257266176
Finished 34 epochs...
Completing Train Step...
At time: 763.1337673664093 and batch: 50, loss is 3.892586841583252 and perplexity is 49.03757502976566
At time: 763.9869484901428 and batch: 100, loss is 3.9088805103302002 and perplexity is 49.843121868583395
At time: 764.8150546550751 and batch: 150, loss is 3.837567925453186 and perplexity is 46.41245850492235
At time: 765.6411664485931 and batch: 200, loss is 3.8873242568969726 and perplexity is 48.78018849161087
At time: 766.4733145236969 and batch: 250, loss is 3.902718324661255 and perplexity is 49.536923691245505
At time: 767.2988953590393 and batch: 300, loss is 3.906306643486023 and perplexity is 49.71499726835899
At time: 768.1270651817322 and batch: 350, loss is 3.878556041717529 and perplexity is 48.35434298325128
At time: 768.9524731636047 and batch: 400, loss is 3.8912230730056763 and perplexity is 48.970744706724794
At time: 769.7786464691162 and batch: 450, loss is 3.8108326816558837 and perplexity is 45.18805046558899
At time: 770.6128482818604 and batch: 500, loss is 3.8505264377593993 and perplexity is 47.01780866738863
At time: 771.4671301841736 and batch: 550, loss is 3.834228444099426 and perplexity is 46.257723476336515
At time: 772.292313337326 and batch: 600, loss is 3.8667769765853883 and perplexity is 47.788115390497396
At time: 773.1171090602875 and batch: 650, loss is 3.883137354850769 and perplexity is 48.5763775864852
At time: 773.9425795078278 and batch: 700, loss is 3.8517093372344973 and perplexity is 47.073458916417664
At time: 774.7735118865967 and batch: 750, loss is 3.841807498931885 and perplexity is 46.60964523153679
At time: 775.5995445251465 and batch: 800, loss is 3.865129909515381 and perplexity is 47.70946994424166
At time: 776.4226517677307 and batch: 850, loss is 3.909993190765381 and perplexity is 49.898612200893425
At time: 777.2479493618011 and batch: 900, loss is 3.8603461170196534 and perplexity is 47.4817827782373
At time: 778.075332403183 and batch: 950, loss is 3.845829100608826 and perplexity is 46.7974680801605
At time: 778.9003756046295 and batch: 1000, loss is 3.828854956626892 and perplexity is 46.009824815395795
At time: 779.7262244224548 and batch: 1050, loss is 3.8289595651626587 and perplexity is 46.01463808755089
At time: 780.5510139465332 and batch: 1100, loss is 3.7804738807678224 and perplexity is 43.83681023558228
At time: 781.3861351013184 and batch: 1150, loss is 3.782161316871643 and perplexity is 43.91084449832672
At time: 782.2123293876648 and batch: 1200, loss is 3.7969115018844604 and perplexity is 44.563337948202694
At time: 783.0357527732849 and batch: 1250, loss is 3.831034822463989 and perplexity is 46.110229455245154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340405986256843 and perplexity of 76.73868786757275
Finished 35 epochs...
Completing Train Step...
At time: 785.4882137775421 and batch: 50, loss is 3.8924464702606203 and perplexity is 49.030692043598314
At time: 786.314505815506 and batch: 100, loss is 3.908706474304199 and perplexity is 49.83444812452376
At time: 787.1417310237885 and batch: 150, loss is 3.8373661756515505 and perplexity is 46.40309574512479
At time: 787.9679353237152 and batch: 200, loss is 3.8870940828323364 and perplexity is 48.76896184944253
At time: 788.7937672138214 and batch: 250, loss is 3.902491216659546 and perplexity is 49.52567473690729
At time: 789.6175961494446 and batch: 300, loss is 3.9060910892486573 and perplexity is 49.704282144923795
At time: 790.4483745098114 and batch: 350, loss is 3.8783568000793456 and perplexity is 48.34470974444502
At time: 791.2713706493378 and batch: 400, loss is 3.891033329963684 and perplexity is 48.96145373013247
At time: 792.1350772380829 and batch: 450, loss is 3.8106605291366575 and perplexity is 45.18027189843159
At time: 792.959636926651 and batch: 500, loss is 3.8503780794143676 and perplexity is 47.01083370051778
At time: 793.7836389541626 and batch: 550, loss is 3.8340936517715454 and perplexity is 46.25148871031543
At time: 794.6092619895935 and batch: 600, loss is 3.8666677284240722 and perplexity is 47.78289491192726
At time: 795.4346013069153 and batch: 650, loss is 3.883041749000549 and perplexity is 48.57173362260404
At time: 796.2598860263824 and batch: 700, loss is 3.8516317796707153 and perplexity is 47.06980815519922
At time: 797.0855569839478 and batch: 750, loss is 3.8417511940002442 and perplexity is 46.607020952528856
At time: 797.9170963764191 and batch: 800, loss is 3.865108275413513 and perplexity is 47.708437803873586
At time: 798.7427959442139 and batch: 850, loss is 3.9100017499923707 and perplexity is 49.89903929626953
At time: 799.5676925182343 and batch: 900, loss is 3.8603687572479246 and perplexity is 47.48285778880732
At time: 800.3907070159912 and batch: 950, loss is 3.8458698081970213 and perplexity is 46.79937313099445
At time: 801.2170622348785 and batch: 1000, loss is 3.8289289617538453 and perplexity is 46.01322990431782
At time: 802.0441415309906 and batch: 1050, loss is 3.8290315771102907 and perplexity is 46.01795181057155
At time: 802.8694710731506 and batch: 1100, loss is 3.7805713510513304 and perplexity is 43.841083230145635
At time: 803.6937072277069 and batch: 1150, loss is 3.782291359901428 and perplexity is 43.91655516889411
At time: 804.5182700157166 and batch: 1200, loss is 3.797039895057678 and perplexity is 44.569059943895766
At time: 805.3433170318604 and batch: 1250, loss is 3.831108341217041 and perplexity is 46.11361954643374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340396184990876 and perplexity of 76.73793573496894
Finished 36 epochs...
Completing Train Step...
At time: 807.8129501342773 and batch: 50, loss is 3.8923114347457886 and perplexity is 49.02407160586282
At time: 808.6664712429047 and batch: 100, loss is 3.9085419511795045 and perplexity is 49.8262498798198
At time: 809.4930963516235 and batch: 150, loss is 3.837181601524353 and perplexity is 46.394531724600995
At time: 810.318788766861 and batch: 200, loss is 3.8868841171264648 and perplexity is 48.75872311487237
At time: 811.144279718399 and batch: 250, loss is 3.9022828197479247 and perplexity is 49.515354814603484
At time: 811.9717900753021 and batch: 300, loss is 3.9058950614929198 and perplexity is 49.69453968097225
At time: 812.7972164154053 and batch: 350, loss is 3.8781746101379393 and perplexity is 48.33590262691785
At time: 813.6807689666748 and batch: 400, loss is 3.8908590126037597 and perplexity is 48.95291964262158
At time: 814.5061650276184 and batch: 450, loss is 3.8105044603347777 and perplexity is 45.173221217737755
At time: 815.333340883255 and batch: 500, loss is 3.8502429914474487 and perplexity is 47.00448353149538
At time: 816.1583073139191 and batch: 550, loss is 3.8339715957641602 and perplexity is 46.2458437827735
At time: 816.9898910522461 and batch: 600, loss is 3.866567120552063 and perplexity is 47.778087818371496
At time: 817.8198654651642 and batch: 650, loss is 3.882954454421997 and perplexity is 48.567493758649185
At time: 818.6626198291779 and batch: 700, loss is 3.85156081199646 and perplexity is 47.066467838915436
At time: 819.4865460395813 and batch: 750, loss is 3.841696472167969 and perplexity is 46.60447060072604
At time: 820.3164687156677 and batch: 800, loss is 3.865083236694336 and perplexity is 47.707243260652014
At time: 821.1422355175018 and batch: 850, loss is 3.910001029968262 and perplexity is 49.899003367771165
At time: 821.9736189842224 and batch: 900, loss is 3.8603822231292724 and perplexity is 47.48349719164141
At time: 822.8009305000305 and batch: 950, loss is 3.845899348258972 and perplexity is 46.80075560779512
At time: 823.6269288063049 and batch: 1000, loss is 3.8289866495132445 and perplexity is 46.01588438101839
At time: 824.452056646347 and batch: 1050, loss is 3.829089889526367 and perplexity is 46.02063530676432
At time: 825.2785601615906 and batch: 1100, loss is 3.780651807785034 and perplexity is 43.844610682406106
At time: 826.1038224697113 and batch: 1150, loss is 3.782399158477783 and perplexity is 43.921289566195796
At time: 826.9278225898743 and batch: 1200, loss is 3.7971455335617064 and perplexity is 44.573768401407015
At time: 827.7558448314667 and batch: 1250, loss is 3.8311674070358275 and perplexity is 46.11634336557098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340388611285356 and perplexity of 76.73735454664231
Finished 37 epochs...
Completing Train Step...
At time: 830.2614374160767 and batch: 50, loss is 3.89218062877655 and perplexity is 49.01765938404799
At time: 831.0861277580261 and batch: 100, loss is 3.908385272026062 and perplexity is 49.81844375671376
At time: 831.9117133617401 and batch: 150, loss is 3.8370100927352904 and perplexity is 46.38657533696048
At time: 832.7377822399139 and batch: 200, loss is 3.886690196990967 and perplexity is 48.749268733406325
At time: 833.5607197284698 and batch: 250, loss is 3.902090153694153 and perplexity is 49.50581580554136
At time: 834.3863573074341 and batch: 300, loss is 3.905714612007141 and perplexity is 49.68557313586937
At time: 835.2603540420532 and batch: 350, loss is 3.8780060815811157 and perplexity is 48.327757333381925
At time: 836.0858483314514 and batch: 400, loss is 3.8906971836090087 and perplexity is 48.94499828181591
At time: 836.9115512371063 and batch: 450, loss is 3.8103604412078855 and perplexity is 45.166715878316964
At time: 837.7357888221741 and batch: 500, loss is 3.8501179313659666 and perplexity is 46.99860551451521
At time: 838.5681300163269 and batch: 550, loss is 3.8338586616516115 and perplexity is 46.24062134434809
At time: 839.3972494602203 and batch: 600, loss is 3.866472911834717 and perplexity is 47.77358691801624
At time: 840.2231540679932 and batch: 650, loss is 3.882872881889343 and perplexity is 48.56353214676019
At time: 841.0533726215363 and batch: 700, loss is 3.851494836807251 and perplexity is 47.06336272222582
At time: 841.8796420097351 and batch: 750, loss is 3.8416423845291137 and perplexity is 46.60194994311997
At time: 842.706113576889 and batch: 800, loss is 3.8650551986694337 and perplexity is 47.70590566252935
At time: 843.5320744514465 and batch: 850, loss is 3.909992775917053 and perplexity is 49.89859150054189
At time: 844.3586103916168 and batch: 900, loss is 3.8603882598876953 and perplexity is 47.48378383890824
At time: 845.1915752887726 and batch: 950, loss is 3.845920023918152 and perplexity is 46.801723254270755
At time: 846.0171937942505 and batch: 1000, loss is 3.8290312623977663 and perplexity is 46.017937328148044
At time: 846.8419246673584 and batch: 1050, loss is 3.8291372156143186 and perplexity is 46.022813334936814
At time: 847.6687560081482 and batch: 1100, loss is 3.780718502998352 and perplexity is 43.84753500558651
At time: 848.4935483932495 and batch: 1150, loss is 3.7824897956848145 and perplexity is 43.9252706496257
At time: 849.3204262256622 and batch: 1200, loss is 3.7972337436676025 and perplexity is 44.577700431657746
At time: 850.1463055610657 and batch: 1250, loss is 3.8312156105041506 and perplexity is 46.118566386845814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340385047188641 and perplexity of 76.73708104777647
Finished 38 epochs...
Completing Train Step...
At time: 852.6133291721344 and batch: 50, loss is 3.892052855491638 and perplexity is 49.0113966368042
At time: 853.4664444923401 and batch: 100, loss is 3.9082348489761354 and perplexity is 49.81095047805632
At time: 854.2910010814667 and batch: 150, loss is 3.83684850692749 and perplexity is 46.37908053025707
At time: 855.1152167320251 and batch: 200, loss is 3.8865088701248167 and perplexity is 48.74042998265549
At time: 855.94105052948 and batch: 250, loss is 3.9019103145599363 and perplexity is 49.496913523001595
At time: 856.7974672317505 and batch: 300, loss is 3.905546989440918 and perplexity is 49.6772454105729
At time: 857.621345281601 and batch: 350, loss is 3.8778482913970946 and perplexity is 48.320132289253344
At time: 858.4446003437042 and batch: 400, loss is 3.8905452203750612 and perplexity is 48.937561006701955
At time: 859.2683482170105 and batch: 450, loss is 3.810225419998169 and perplexity is 45.16061782539266
At time: 860.0972075462341 and batch: 500, loss is 3.850000653266907 and perplexity is 46.99309393060237
At time: 860.9266846179962 and batch: 550, loss is 3.833752841949463 and perplexity is 46.235728434457975
At time: 861.7491528987885 and batch: 600, loss is 3.8663832473754884 and perplexity is 47.76930351721711
At time: 862.5728056430817 and batch: 650, loss is 3.882795648574829 and perplexity is 48.559781569044624
At time: 863.4001219272614 and batch: 700, loss is 3.8514322519302366 and perplexity is 47.06041735962651
At time: 864.2269313335419 and batch: 750, loss is 3.841589107513428 and perplexity is 46.599467196439115
At time: 865.0510492324829 and batch: 800, loss is 3.8650248193740846 and perplexity is 47.70445641274503
At time: 865.8782534599304 and batch: 850, loss is 3.909978594779968 and perplexity is 49.89788388679287
At time: 866.703982591629 and batch: 900, loss is 3.8603881072998045 and perplexity is 47.48377659345837
At time: 867.5278627872467 and batch: 950, loss is 3.845933346748352 and perplexity is 46.802346789836356
At time: 868.3632869720459 and batch: 1000, loss is 3.8290655183792115 and perplexity is 46.019513744756004
At time: 869.1881759166718 and batch: 1050, loss is 3.8291757583618162 and perplexity is 46.02458721479519
At time: 870.0147881507874 and batch: 1100, loss is 3.780774450302124 and perplexity is 43.849988225572005
At time: 870.8411133289337 and batch: 1150, loss is 3.7825665140151976 and perplexity is 43.928640652320404
At time: 871.6675832271576 and batch: 1200, loss is 3.79730815410614 and perplexity is 44.58101760131035
At time: 872.491457939148 and batch: 1250, loss is 3.831255307197571 and perplexity is 46.1203971777746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340381928604015 and perplexity of 76.73684173706842
Finished 39 epochs...
Completing Train Step...
At time: 874.9444813728333 and batch: 50, loss is 3.8919279909133913 and perplexity is 49.00527723149029
At time: 875.8017842769623 and batch: 100, loss is 3.9080895805358886 and perplexity is 49.80371504452596
At time: 876.6317858695984 and batch: 150, loss is 3.8366952323913575 and perplexity is 46.37197234296851
At time: 877.4853346347809 and batch: 200, loss is 3.886338233947754 and perplexity is 48.732113811554854
At time: 878.3102924823761 and batch: 250, loss is 3.9017411422729493 and perplexity is 49.48854072518476
At time: 879.1402616500854 and batch: 300, loss is 3.9053898906707762 and perplexity is 49.66944178940057
At time: 879.9658434391022 and batch: 350, loss is 3.8776992750167847 and perplexity is 48.312932334512425
At time: 880.7906732559204 and batch: 400, loss is 3.890401563644409 and perplexity is 48.930531301625955
At time: 881.6139404773712 and batch: 450, loss is 3.8100975227355955 and perplexity is 45.15484227534306
At time: 882.4475102424622 and batch: 500, loss is 3.8498892831802367 and perplexity is 46.98786059708223
At time: 883.2733523845673 and batch: 550, loss is 3.8336525678634645 and perplexity is 46.231092421488604
At time: 884.0989227294922 and batch: 600, loss is 3.866297483444214 and perplexity is 47.76520680963061
At time: 884.9272866249084 and batch: 650, loss is 3.882721700668335 and perplexity is 48.55619080762406
At time: 885.7512803077698 and batch: 700, loss is 3.8513721323013304 and perplexity is 47.05758818984385
At time: 886.5766661167145 and batch: 750, loss is 3.8415363025665283 and perplexity is 46.597006579015215
At time: 887.4034669399261 and batch: 800, loss is 3.8649925708770754 and perplexity is 47.70291804053031
At time: 888.2290518283844 and batch: 850, loss is 3.9099596977233886 and perplexity is 49.89694097256704
At time: 889.0566644668579 and batch: 900, loss is 3.8603830623626707 and perplexity is 47.48353704139485
At time: 889.8835768699646 and batch: 950, loss is 3.845940728187561 and perplexity is 46.80269225978906
At time: 890.7096886634827 and batch: 1000, loss is 3.8290915966033934 and perplexity is 46.02071386760065
At time: 891.5415053367615 and batch: 1050, loss is 3.8292071199417115 and perplexity is 46.02603064119822
At time: 892.3651130199432 and batch: 1100, loss is 3.7808215761184694 and perplexity is 43.852054740756586
At time: 893.1885371208191 and batch: 1150, loss is 3.7826320505142212 and perplexity is 43.93151967597516
At time: 894.0195028781891 and batch: 1200, loss is 3.7973715829849244 and perplexity is 44.58384541495345
At time: 894.8837783336639 and batch: 1250, loss is 3.8312881183624268 and perplexity is 46.12191046655586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340379255531478 and perplexity of 76.73663661419837
Finished 40 epochs...
Completing Train Step...
At time: 897.3742883205414 and batch: 50, loss is 3.891805286407471 and perplexity is 48.99926443206645
At time: 898.2149739265442 and batch: 100, loss is 3.9079487657547 and perplexity is 49.796702439040416
At time: 899.0691914558411 and batch: 150, loss is 3.836548581123352 and perplexity is 46.36517233305174
At time: 899.8979704380035 and batch: 200, loss is 3.886176137924194 and perplexity is 48.724215169872835
At time: 900.7243704795837 and batch: 250, loss is 3.901581115722656 and perplexity is 49.48062187836323
At time: 901.5544466972351 and batch: 300, loss is 3.9052414464950562 and perplexity is 49.66206919727839
At time: 902.3840124607086 and batch: 350, loss is 3.8775576305389405 and perplexity is 48.3060895590709
At time: 903.2093870639801 and batch: 400, loss is 3.890264797210693 and perplexity is 48.92383970496334
At time: 904.0360081195831 and batch: 450, loss is 3.8099751710891723 and perplexity is 45.149317844015215
At time: 904.8595416545868 and batch: 500, loss is 3.8497825717926024 and perplexity is 46.982846724799344
At time: 905.6878907680511 and batch: 550, loss is 3.8335565900802613 and perplexity is 46.22665547665039
At time: 906.5142874717712 and batch: 600, loss is 3.8662142896652223 and perplexity is 47.76123320686361
At time: 907.3412699699402 and batch: 650, loss is 3.8826499700546266 and perplexity is 48.55270796717272
At time: 908.1677799224854 and batch: 700, loss is 3.8513139152526854 and perplexity is 47.05484871568591
At time: 908.9941499233246 and batch: 750, loss is 3.8414839458465577 and perplexity is 46.59456697645564
At time: 909.8210775852203 and batch: 800, loss is 3.864958882331848 and perplexity is 47.70131102568758
At time: 910.6536209583282 and batch: 850, loss is 3.9099372482299803 and perplexity is 49.895820824093015
At time: 911.4806606769562 and batch: 900, loss is 3.8603740692138673 and perplexity is 47.48311001680067
At time: 912.3061475753784 and batch: 950, loss is 3.845943422317505 and perplexity is 46.802818352493595
At time: 913.1348226070404 and batch: 1000, loss is 3.8291108274459837 and perplexity is 46.0215988932148
At time: 913.9644477367401 and batch: 1050, loss is 3.829232292175293 and perplexity is 46.02718923377447
At time: 914.7908957004547 and batch: 1100, loss is 3.780861382484436 and perplexity is 43.85380036643926
At time: 915.6210799217224 and batch: 1150, loss is 3.7826884746551515 and perplexity is 43.93399854416598
At time: 916.4457049369812 and batch: 1200, loss is 3.797425875663757 and perplexity is 44.58626605706467
At time: 917.2727942466736 and batch: 1250, loss is 3.831315541267395 and perplexity is 46.12317528066589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340376136946852 and perplexity of 76.73639730487633
Finished 41 epochs...
Completing Train Step...
At time: 919.7091574668884 and batch: 50, loss is 3.8916846179962157 and perplexity is 48.99335212539626
At time: 920.5668823719025 and batch: 100, loss is 3.90781174659729 and perplexity is 49.789879804256934
At time: 921.3954999446869 and batch: 150, loss is 3.836407632827759 and perplexity is 46.35863770156991
At time: 922.2220323085785 and batch: 200, loss is 3.886021237373352 and perplexity is 48.7166683466223
At time: 923.0467598438263 and batch: 250, loss is 3.9014285612106323 and perplexity is 49.473073961986934
At time: 923.8724410533905 and batch: 300, loss is 3.905100440979004 and perplexity is 49.65506706526423
At time: 924.6980154514313 and batch: 350, loss is 3.877421932220459 and perplexity is 48.29953494868018
At time: 925.5241601467133 and batch: 400, loss is 3.8901337337493898 and perplexity is 48.91742799737083
At time: 926.3557364940643 and batch: 450, loss is 3.809857268333435 and perplexity is 45.1439949288211
At time: 927.1840767860413 and batch: 500, loss is 3.8496798133850096 and perplexity is 46.978019090329994
At time: 928.0116257667542 and batch: 550, loss is 3.8334640979766847 and perplexity is 46.2223800737676
At time: 928.8359062671661 and batch: 600, loss is 3.8661335849761964 and perplexity is 47.757378806926326
At time: 929.6629304885864 and batch: 650, loss is 3.8825801849365233 and perplexity is 48.54931982893519
At time: 930.4874911308289 and batch: 700, loss is 3.8512573051452637 and perplexity is 47.05218501104242
At time: 931.313572883606 and batch: 750, loss is 3.8414319276809694 and perplexity is 46.592143275593926
At time: 932.1404497623444 and batch: 800, loss is 3.8649236583709716 and perplexity is 47.699630826166064
At time: 932.964613199234 and batch: 850, loss is 3.9099118185043333 and perplexity is 49.89455200319148
At time: 933.7932131290436 and batch: 900, loss is 3.860361862182617 and perplexity is 47.482530392530585
At time: 934.637393951416 and batch: 950, loss is 3.845942077636719 and perplexity is 46.802755417685326
At time: 935.4888050556183 and batch: 1000, loss is 3.8291245746612548 and perplexity is 46.02223156639064
At time: 936.3128979206085 and batch: 1050, loss is 3.8292526054382323 and perplexity is 46.028124205667865
At time: 937.1325318813324 and batch: 1100, loss is 3.780895209312439 and perplexity is 43.85528382649177
At time: 937.9527206420898 and batch: 1150, loss is 3.7827372550964355 and perplexity is 43.936141716274356
At time: 938.7770810127258 and batch: 1200, loss is 3.7974727821350096 and perplexity is 44.58835749052223
At time: 939.6026649475098 and batch: 1250, loss is 3.831338491439819 and perplexity is 46.12423382763821
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.34037791899521 and perplexity of 76.73653405296898
Annealing...
Finished 42 epochs...
Completing Train Step...
At time: 942.036972284317 and batch: 50, loss is 3.891702961921692 and perplexity is 48.99425086403966
At time: 942.8615674972534 and batch: 100, loss is 3.9079774904251097 and perplexity is 49.79813285344947
At time: 943.6919920444489 and batch: 150, loss is 3.836717619895935 and perplexity is 46.37301050733253
At time: 944.5191857814789 and batch: 200, loss is 3.886393585205078 and perplexity is 48.73481126997956
At time: 945.3464620113373 and batch: 250, loss is 3.9017243242263793 and perplexity is 49.48770843160096
At time: 946.1741721630096 and batch: 300, loss is 3.9053942489624025 and perplexity is 49.669658263784534
At time: 946.9993081092834 and batch: 350, loss is 3.8776592350006105 and perplexity is 48.31099792264753
At time: 947.8301050662994 and batch: 400, loss is 3.8902048683166504 and perplexity is 48.92090784121005
At time: 948.662011384964 and batch: 450, loss is 3.809805750846863 and perplexity is 45.14166928357475
At time: 949.4900267124176 and batch: 500, loss is 3.8497111654281615 and perplexity is 46.97949197030048
At time: 950.3159644603729 and batch: 550, loss is 3.83334520816803 and perplexity is 46.21688503050385
At time: 951.1451644897461 and batch: 600, loss is 3.866055045127869 and perplexity is 47.75362809693033
At time: 951.9768438339233 and batch: 650, loss is 3.8825507259368894 and perplexity is 48.54788963560625
At time: 952.8015174865723 and batch: 700, loss is 3.8513102388381957 and perplexity is 47.054675722876276
At time: 953.6301488876343 and batch: 750, loss is 3.84129949092865 and perplexity is 46.58597317203816
At time: 954.4551486968994 and batch: 800, loss is 3.864592661857605 and perplexity is 47.68384502734005
At time: 955.2845675945282 and batch: 850, loss is 3.909336633682251 and perplexity is 49.86586166608884
At time: 956.110155582428 and batch: 900, loss is 3.8597950172424316 and perplexity is 47.45562278737078
At time: 956.9438767433167 and batch: 950, loss is 3.8455082464218138 and perplexity is 46.78245532516686
At time: 957.7723009586334 and batch: 1000, loss is 3.828624224662781 and perplexity is 45.999210102771556
At time: 958.6013693809509 and batch: 1050, loss is 3.828658494949341 and perplexity is 46.00078653589557
At time: 959.4302921295166 and batch: 1100, loss is 3.780148539543152 and perplexity is 43.822550633798514
At time: 960.2568991184235 and batch: 1150, loss is 3.781932291984558 and perplexity is 43.90078897365034
At time: 961.084130525589 and batch: 1200, loss is 3.7968064594268798 and perplexity is 44.55865715151212
At time: 961.9099624156952 and batch: 1250, loss is 3.8307828760147093 and perplexity is 46.09861361000535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3403805920677465 and perplexity of 76.73673917556486
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 964.34401679039 and batch: 50, loss is 3.8917011070251464 and perplexity is 48.994159984857255
At time: 965.2027649879456 and batch: 100, loss is 3.907997975349426 and perplexity is 49.799152974880585
At time: 966.0278165340424 and batch: 150, loss is 3.8367519664764402 and perplexity is 46.37460328902433
At time: 966.8542249202728 and batch: 200, loss is 3.886446647644043 and perplexity is 48.73739732653867
At time: 967.6830260753632 and batch: 250, loss is 3.901767764091492 and perplexity is 49.489858217672825
At time: 968.5114586353302 and batch: 300, loss is 3.9054433584213255 and perplexity is 49.67209757372286
At time: 969.3380358219147 and batch: 350, loss is 3.877692451477051 and perplexity is 48.31260267042373
At time: 970.1641225814819 and batch: 400, loss is 3.8902113676071166 and perplexity is 48.921225793433216
At time: 970.9893815517426 and batch: 450, loss is 3.8097774171829224 and perplexity is 45.140390272807174
At time: 971.8142647743225 and batch: 500, loss is 3.8497059631347654 and perplexity is 46.97924756983537
At time: 972.6485223770142 and batch: 550, loss is 3.8333093166351317 and perplexity is 46.21522626542232
At time: 973.4722907543182 and batch: 600, loss is 3.8660264444351196 and perplexity is 47.75226232961651
At time: 974.2985501289368 and batch: 650, loss is 3.8825250101089477 and perplexity is 48.546641202481766
At time: 975.1266543865204 and batch: 700, loss is 3.851302809715271 and perplexity is 47.054326149204655
At time: 975.9525845050812 and batch: 750, loss is 3.841259546279907 and perplexity is 46.58411234886867
At time: 976.7792313098907 and batch: 800, loss is 3.8645180463790894 and perplexity is 47.68028720716173
At time: 977.603718996048 and batch: 850, loss is 3.9092089128494263 and perplexity is 49.85949316341122
At time: 978.4327356815338 and batch: 900, loss is 3.859665284156799 and perplexity is 47.44946662233374
At time: 979.260924577713 and batch: 950, loss is 3.845409698486328 and perplexity is 46.77784523793863
At time: 980.1087367534637 and batch: 1000, loss is 3.828510112762451 and perplexity is 45.993961344971616
At time: 980.963531255722 and batch: 1050, loss is 3.8285242080688477 and perplexity is 45.994609648518185
At time: 981.8200936317444 and batch: 1100, loss is 3.7799788522720337 and perplexity is 43.81511513564056
At time: 982.6475179195404 and batch: 1150, loss is 3.7817477703094484 and perplexity is 43.89268907385681
At time: 983.4738259315491 and batch: 1200, loss is 3.7966531991958616 and perplexity is 44.551828604708916
At time: 984.3522853851318 and batch: 1250, loss is 3.830654344558716 and perplexity is 46.09268886884473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340381483091925 and perplexity of 76.73680754988534
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 986.7663309574127 and batch: 50, loss is 3.8917010974884034 and perplexity is 48.994159517612545
At time: 987.6217443943024 and batch: 100, loss is 3.9080025720596314 and perplexity is 49.799381887681406
At time: 988.4547572135925 and batch: 150, loss is 3.8367602252960205 and perplexity is 46.37498629008756
At time: 989.2806096076965 and batch: 200, loss is 3.8864587259292604 and perplexity is 48.73798599427937
At time: 990.1061964035034 and batch: 250, loss is 3.9017775011062623 and perplexity is 49.49034010349934
At time: 990.9330942630768 and batch: 300, loss is 3.905454044342041 and perplexity is 49.67262836865531
At time: 991.7581524848938 and batch: 350, loss is 3.877699589729309 and perplexity is 48.31294753919972
At time: 992.5835649967194 and batch: 400, loss is 3.890212755203247 and perplexity is 48.92129367638392
At time: 993.4089136123657 and batch: 450, loss is 3.809770188331604 and perplexity is 45.14006396081687
At time: 994.2369046211243 and batch: 500, loss is 3.8497041273117065 and perplexity is 46.97916132432856
At time: 995.0644025802612 and batch: 550, loss is 3.8333005046844484 and perplexity is 46.21481902092196
At time: 995.8913445472717 and batch: 600, loss is 3.8660194444656373 and perplexity is 47.75192806640741
At time: 996.7177848815918 and batch: 650, loss is 3.8825183296203614 and perplexity is 48.5463168882826
At time: 997.5410592556 and batch: 700, loss is 3.8513004970550537 and perplexity is 47.05421732866236
At time: 998.3731420040131 and batch: 750, loss is 3.841249694824219 and perplexity is 46.58365342981061
At time: 999.1997740268707 and batch: 800, loss is 3.8645007371902467 and perplexity is 47.67946190720905
At time: 1000.0292346477509 and batch: 850, loss is 3.9091790294647217 and perplexity is 49.85800321525831
At time: 1000.8541855812073 and batch: 900, loss is 3.8596344804763794 and perplexity is 47.4480050266392
At time: 1001.6811790466309 and batch: 950, loss is 3.8453864908218383 and perplexity is 46.77675964599787
At time: 1002.5061912536621 and batch: 1000, loss is 3.8284828758239744 and perplexity is 45.99270862733634
At time: 1003.333888053894 and batch: 1050, loss is 3.8284921169281008 and perplexity is 45.99313365270967
At time: 1004.1627855300903 and batch: 1100, loss is 3.779938607215881 and perplexity is 43.81335182935402
At time: 1005.0308260917664 and batch: 1150, loss is 3.781704139709473 and perplexity is 43.89077405127507
At time: 1005.8560471534729 and batch: 1200, loss is 3.7966168260574342 and perplexity is 44.55020814435068
At time: 1006.6816518306732 and batch: 1250, loss is 3.8306241607666016 and perplexity is 46.091297637702354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340382819628194 and perplexity of 76.73691011148028
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1009.1300344467163 and batch: 50, loss is 3.891701455116272 and perplexity is 48.99417703929252
At time: 1009.9541971683502 and batch: 100, loss is 3.908003931045532 and perplexity is 49.79944956438524
At time: 1010.7821762561798 and batch: 150, loss is 3.836762571334839 and perplexity is 46.37509508773322
At time: 1011.6070787906647 and batch: 200, loss is 3.8864619970321654 and perplexity is 48.73814542150769
At time: 1012.4329478740692 and batch: 250, loss is 3.901780114173889 and perplexity is 49.490469425273865
At time: 1013.2567653656006 and batch: 300, loss is 3.9054569721221926 and perplexity is 49.67277379940363
At time: 1014.0845489501953 and batch: 350, loss is 3.877701859474182 and perplexity is 48.31305719738914
At time: 1014.9120042324066 and batch: 400, loss is 3.8902135133743285 and perplexity is 48.92133076710811
At time: 1015.7373139858246 and batch: 450, loss is 3.809769015312195 and perplexity is 45.140011010676766
At time: 1016.5628442764282 and batch: 500, loss is 3.8497040700912475 and perplexity is 46.97915863615946
At time: 1017.3936622142792 and batch: 550, loss is 3.833298931121826 and perplexity is 46.214746299067365
At time: 1018.219146490097 and batch: 600, loss is 3.8660182905197145 and perplexity is 47.7518729632965
At time: 1019.0447664260864 and batch: 650, loss is 3.8825171518325807 and perplexity is 48.546259711057445
At time: 1019.8739540576935 and batch: 700, loss is 3.8513001823425292 and perplexity is 47.054202520113165
At time: 1020.7015919685364 and batch: 750, loss is 3.8412477779388428 and perplexity is 46.58356413437216
At time: 1021.5264647006989 and batch: 800, loss is 3.864496989250183 and perplexity is 47.67928320777843
At time: 1022.3527753353119 and batch: 850, loss is 3.909172396659851 and perplexity is 49.857672517948465
At time: 1023.1780672073364 and batch: 900, loss is 3.859627532958984 and perplexity is 47.447675381944016
At time: 1024.0061876773834 and batch: 950, loss is 3.84538122177124 and perplexity is 46.77651317753381
At time: 1024.8323729038239 and batch: 1000, loss is 3.8284767484664917 and perplexity is 45.99242681443237
At time: 1025.6635329723358 and batch: 1050, loss is 3.8284850454330446 and perplexity is 45.99280841364239
At time: 1026.5408930778503 and batch: 1100, loss is 3.7799295568466187 and perplexity is 43.812955304135684
At time: 1027.3672482967377 and batch: 1150, loss is 3.78169424533844 and perplexity is 43.89033978182011
At time: 1028.1927716732025 and batch: 1200, loss is 3.796608581542969 and perplexity is 44.54984085102928
At time: 1029.0160751342773 and batch: 1250, loss is 3.8306174755096434 and perplexity is 46.09098950656408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.340382819628194 and perplexity of 76.73691011148028
Annealing...
Model not improving. Stopping early with 76.73639730487633loss at 45 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f15b303a898>
SETTINGS FOR THIS RUN
{'batch_size': 50, 'wordvec_dim': 200, 'anneal': 8.0, 'lr': 0.0, 'dropout': 1.0, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.439087152481079 and batch: 50, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 2.2581253051757812 and batch: 100, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 3.0784695148468018 and batch: 150, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 3.8971993923187256 and batch: 200, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 4.714249134063721 and batch: 250, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 5.532688140869141 and batch: 300, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 6.355245351791382 and batch: 350, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 7.175479173660278 and batch: 400, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 7.994020700454712 and batch: 450, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 8.848932027816772 and batch: 500, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 9.666534662246704 and batch: 550, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 10.483929634094238 and batch: 600, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 11.306811094284058 and batch: 650, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 12.134740114212036 and batch: 700, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 12.955949306488037 and batch: 750, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 13.778001308441162 and batch: 800, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 14.598103046417236 and batch: 850, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 15.41835641860962 and batch: 900, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 16.238614082336426 and batch: 950, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 17.058988571166992 and batch: 1000, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 17.879024267196655 and batch: 1050, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 18.700397491455078 and batch: 1100, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 19.527699947357178 and batch: 1150, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 20.349204063415527 and batch: 1200, loss is 9.926758766174316 and perplexity is 20470.882020515448
At time: 21.168395519256592 and batch: 1250, loss is 9.926758766174316 and perplexity is 20470.882020515448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 1 epochs...
Completing Train Step...
At time: 23.6036639213562 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 24.42107582092285 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 25.247920274734497 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 26.06582498550415 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 26.88367533683777 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 27.70244264602661 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 28.520787715911865 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 29.33756947517395 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 30.15605926513672 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 30.977340698242188 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 31.796654224395752 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 32.62295603752136 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 33.44158220291138 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 34.261834144592285 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 35.082255840301514 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 35.90313506126404 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 36.7491455078125 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 37.56938600540161 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 38.38897705078125 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 39.21396017074585 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 40.03382730484009 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 40.85313057899475 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 41.67736005783081 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 42.500736236572266 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 43.320900201797485 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 2 epochs...
Completing Train Step...
At time: 45.740212202072144 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 46.56083536148071 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 47.38418245315552 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 48.212650537490845 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 49.03139042854309 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 49.85285472869873 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 50.674399852752686 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 51.4954047203064 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 52.31820774078369 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 53.13743758201599 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 53.95819878578186 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 54.78504753112793 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 55.604371309280396 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 56.42872405052185 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 57.250662326812744 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 58.10217881202698 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 58.923932790756226 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 59.74525475502014 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 60.56653904914856 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 61.3865647315979 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 62.21488094329834 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 63.03754472732544 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 63.86012816429138 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 64.68452644348145 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 65.50524926185608 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 3 epochs...
Completing Train Step...
At time: 67.88539171218872 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 68.77564883232117 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 69.6052405834198 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 70.4396014213562 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 71.27824640274048 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 72.09967803955078 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 72.92334485054016 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 73.74584460258484 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 74.56728315353394 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 75.3899154663086 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 76.21529841423035 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 77.04240012168884 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 77.86438989639282 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 78.69347095489502 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 79.55866384506226 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 80.38377976417542 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 81.21021246910095 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 82.03604316711426 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 82.85865998268127 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 83.67768239974976 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 84.50187611579895 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 85.32162356376648 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 86.14551663398743 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 86.9730339050293 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 87.79639911651611 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 4 epochs...
Completing Train Step...
At time: 90.22479677200317 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 91.04805874824524 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 91.87381863594055 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 92.69769191741943 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 93.52275156974792 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 94.34665989875793 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 95.17662000656128 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 96.00154256820679 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 96.82348299026489 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 97.65054392814636 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 98.47317123413086 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 99.29818654060364 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 100.15312242507935 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 100.97523212432861 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 101.80326676368713 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 102.62557768821716 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 103.44967937469482 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 104.27720499038696 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 105.11002802848816 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 105.93547558784485 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 106.75784587860107 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 107.58252573013306 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 108.40444874763489 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 109.23291969299316 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 110.0584647655487 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 5 epochs...
Completing Train Step...
At time: 112.49430465698242 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 113.34604334831238 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 114.16869115829468 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 114.9925377368927 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 115.81435441970825 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 116.63919115066528 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 117.46507477760315 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 118.28694248199463 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 119.11952424049377 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 119.9408974647522 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 120.7642605304718 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 121.61716842651367 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 122.4423999786377 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 123.26621389389038 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 124.09170913696289 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 124.91805481910706 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 125.74111938476562 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 126.56610083580017 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 127.38919758796692 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 128.22330117225647 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 129.04865789413452 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 129.87089800834656 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 130.69519424438477 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 131.51683354377747 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 132.3426549434662 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 6 epochs...
Completing Train Step...
At time: 134.79518389701843 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 135.6222207546234 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 136.448814868927 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 137.27128624916077 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 138.09847974777222 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 138.91889142990112 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 139.743754863739 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 140.5676293373108 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 141.39244174957275 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 142.216383934021 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 143.0718162059784 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 143.91650104522705 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 145.00868248939514 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 145.86227679252625 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 146.72133421897888 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 147.5467050075531 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 148.36993789672852 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 149.19183540344238 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 150.01469469070435 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 150.83837032318115 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 151.66666984558105 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 152.48867416381836 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 153.31330966949463 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 154.13718223571777 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 154.96232199668884 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 7 epochs...
Completing Train Step...
At time: 157.3928518295288 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 158.21487736701965 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 159.0400788784027 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 159.86251187324524 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 160.68349170684814 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 161.51148557662964 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 162.3414945602417 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 163.16352677345276 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 163.98690366744995 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 164.84077429771423 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 165.66328620910645 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 166.48648166656494 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 167.3095510005951 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 168.13656520843506 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 168.95881748199463 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 169.78430223464966 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 170.6081931591034 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 171.4384639263153 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 172.26306819915771 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 173.08684992790222 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 173.9103660583496 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 174.73469471931458 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 175.5579388141632 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 176.38221383094788 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 177.2066628932953 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 8 epochs...
Completing Train Step...
At time: 179.62888836860657 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 180.4512836933136 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 181.30611610412598 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 182.12952709197998 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 182.95025610923767 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 183.7750949859619 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 184.5988895893097 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 185.42280983924866 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 186.2466766834259 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 187.06763458251953 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 187.89021968841553 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 188.71966457366943 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 189.5440318584442 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 190.37112617492676 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 191.19788002967834 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 192.02292680740356 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 192.84491276741028 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 193.66861128807068 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 194.4949598312378 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 195.31804704666138 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 196.14306592941284 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 196.966943025589 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 197.79717254638672 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 198.62195372581482 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 199.44561052322388 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 9 epochs...
Completing Train Step...
At time: 201.88995718955994 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 202.71471667289734 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 203.53785181045532 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 204.3614799976349 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 205.19246435165405 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 206.01888728141785 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 206.84455370903015 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 207.66720533370972 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 208.49157047271729 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 209.31288647651672 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 210.1374146938324 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 210.96267676353455 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 211.7862093448639 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 212.61072325706482 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 213.43277311325073 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 214.256897687912 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 215.07937598228455 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 215.90138411521912 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 216.73754167556763 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 217.57492208480835 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 218.39757633209229 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 219.22643899917603 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 220.05793929100037 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 220.88224959373474 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 221.70650029182434 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 10 epochs...
Completing Train Step...
At time: 224.10686445236206 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 224.95919132232666 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 225.78001284599304 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 226.6038851737976 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 227.42714285850525 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 228.25413298606873 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 229.0758993625641 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 229.90694093704224 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 230.7302930355072 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 231.55402612686157 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 232.37702107429504 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 233.198233127594 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 234.02026796340942 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 234.84475708007812 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 235.66938138008118 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 236.4932188987732 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 237.31560921669006 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 238.14020895957947 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 238.96401405334473 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 239.79531908035278 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 240.6216983795166 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 241.4457025527954 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 242.2715916633606 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 243.09665513038635 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 243.94741106033325 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 11 epochs...
Completing Train Step...
At time: 246.3980689048767 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 247.22459173202515 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 248.05480670928955 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 248.87912154197693 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 249.70565724372864 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 250.53224110603333 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 251.36149168014526 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 252.1838436126709 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 253.01239943504333 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 253.8364748954773 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 254.65969252586365 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 255.4883086681366 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 256.3113384246826 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 257.1330192089081 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 257.95500016212463 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 258.7864933013916 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 259.6153042316437 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 260.43929076194763 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 261.2625026702881 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 262.0907175540924 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 262.9226493835449 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 263.74736189842224 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 264.5704560279846 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 265.44613242149353 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 266.2731423377991 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 12 epochs...
Completing Train Step...
At time: 268.6843276023865 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 269.53616189956665 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 270.35826325416565 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 271.1834628582001 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 272.0067729949951 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 272.8299663066864 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 273.65473985671997 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 274.4775631427765 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 275.3002996444702 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 276.1250915527344 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 276.95749711990356 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 277.78082728385925 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 278.6046748161316 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 279.4267497062683 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 280.25030755996704 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 281.0743589401245 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 281.9000577926636 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 282.7228133678436 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 283.5479848384857 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 284.3728439807892 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 285.2027962207794 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 286.02818989753723 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 286.8859646320343 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 287.70931816101074 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 288.5317871570587 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 13 epochs...
Completing Train Step...
At time: 290.95285391807556 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 291.77693819999695 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 292.6038498878479 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 293.4561471939087 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 294.312203168869 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 295.1699116230011 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 295.9939260482788 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 296.8454167842865 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 297.6718153953552 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 298.4996507167816 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 299.32624888420105 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 300.15542101860046 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 300.97991037368774 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 301.80529260635376 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 302.62959027290344 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 303.454971075058 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 304.2805817127228 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 305.1056098937988 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 305.930566072464 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 306.75251507759094 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 307.6123855113983 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 308.4379768371582 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 309.26335191726685 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 310.08865094184875 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 310.90988779067993 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 14 epochs...
Completing Train Step...
At time: 313.351197719574 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 314.17434883117676 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 315.0062806606293 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 315.83141040802 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 316.6546115875244 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 317.4818832874298 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 318.30709528923035 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 319.137410402298 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 319.96435832977295 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 320.78805589675903 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 321.6212487220764 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 322.44278860092163 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 323.26984190940857 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 324.095232963562 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 324.9232099056244 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 325.74745750427246 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 326.5738093852997 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 327.39952087402344 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 328.22371530532837 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 329.1032636165619 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 329.93439388275146 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 330.7601854801178 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 331.586923122406 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 332.41224360466003 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 333.24057483673096 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 15 epochs...
Completing Train Step...
At time: 335.6527268886566 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 336.50661993026733 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 337.3286302089691 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 338.15413212776184 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 338.98680233955383 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 339.80992817878723 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 340.635635137558 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 341.4590280056 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 342.28302526474 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 343.10659289360046 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 343.93209743499756 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 344.7575423717499 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 345.5807240009308 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 346.40595531463623 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 347.23913741111755 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 348.0660390853882 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 348.88823556900024 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 349.7115240097046 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 350.5652332305908 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 351.39053893089294 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 352.22327852249146 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 353.04576873779297 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 353.8733694553375 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 354.69625759124756 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 355.5214812755585 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 16 epochs...
Completing Train Step...
At time: 357.9474399089813 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 358.7713212966919 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 359.5995104312897 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 360.4226825237274 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 361.2532501220703 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 362.0782790184021 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 362.9049050807953 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 363.7314393520355 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 364.5543518066406 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 365.38136315345764 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 366.2046072483063 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 367.0303382873535 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 367.8543200492859 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 368.6926124095917 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 369.5344400405884 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 370.4036455154419 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 371.28595066070557 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 372.1538543701172 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 372.97199869155884 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 373.7902121543884 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 374.61064553260803 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 375.4265606403351 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 376.2537131309509 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 377.0965588092804 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 377.9160192012787 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 17 epochs...
Completing Train Step...
At time: 380.29082131385803 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 381.1571207046509 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 381.9817011356354 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 382.8090980052948 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 383.63288855552673 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 384.45810890197754 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 385.2808520793915 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 386.11184096336365 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 386.9385347366333 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 387.7622928619385 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 388.5843093395233 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 389.4113738536835 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 390.2342848777771 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 391.0553970336914 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 391.8816385269165 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 392.7536416053772 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 393.5803747177124 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 394.40168261528015 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 395.22651863098145 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 396.04937529563904 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 396.8715205192566 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 397.6967399120331 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 398.52838706970215 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 399.3517165184021 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 400.1752986907959 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 18 epochs...
Completing Train Step...
At time: 402.662965297699 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 403.48584508895874 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 404.3209993839264 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 405.15466594696045 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 405.9833846092224 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 406.80543065071106 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 407.6296708583832 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 408.451988697052 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 409.27954292297363 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 410.10167264938354 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 410.92976355552673 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 411.75363183021545 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 412.57978200912476 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 413.40576553344727 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 414.2786829471588 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 415.10858821868896 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 415.9345259666443 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 416.7568938732147 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 417.58387875556946 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 418.40748476982117 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 419.2306697368622 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 420.0561282634735 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 420.8833944797516 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 421.706520318985 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 422.53545594215393 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 19 epochs...
Completing Train Step...
At time: 425.00968623161316 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 425.85954689979553 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 426.68095088005066 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 427.50364661216736 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 428.32810521125793 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 429.1533832550049 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 429.9881377220154 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 430.8232135772705 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 431.64648485183716 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 432.4668471813202 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 433.2916142940521 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 434.11598110198975 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 434.9395031929016 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 435.81506395339966 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 436.66019773483276 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 437.4898250102997 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 438.32447624206543 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 439.15223026275635 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 439.981080532074 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 440.80677247047424 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 441.63496685028076 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 442.464017868042 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 443.28527426719666 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 444.1129822731018 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 444.93826031684875 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 20 epochs...
Completing Train Step...
At time: 447.3761899471283 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 448.23255133628845 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 449.05563473701477 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 449.8814318180084 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 450.72017455101013 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 451.56001353263855 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 452.378497838974 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 453.1966197490692 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 454.0353193283081 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 454.86723041534424 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 455.6897397041321 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 456.5138533115387 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 457.3683032989502 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 458.19451665878296 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 459.0179913043976 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 459.8425705432892 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 460.6642541885376 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 461.4910535812378 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 462.31758880615234 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 463.13940954208374 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 463.9641981124878 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 464.78738498687744 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 465.6094582080841 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 466.43372321128845 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 467.2586727142334 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 21 epochs...
Completing Train Step...
At time: 469.7089056968689 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 470.531361579895 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 471.3547275066376 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 472.1796522140503 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 473.00358057022095 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 473.8364636898041 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 474.6582112312317 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 475.4810357093811 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 476.3066129684448 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 477.1298952102661 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 477.9904944896698 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 478.81669211387634 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 479.6419560909271 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 480.46444272994995 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 481.2858910560608 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 482.11121249198914 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 482.9384722709656 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 483.77240657806396 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 484.5953516960144 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 485.4180598258972 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 486.24111437797546 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 487.0666058063507 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 487.8901162147522 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 488.7206959724426 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 489.54604148864746 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 22 epochs...
Completing Train Step...
At time: 491.9431838989258 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 492.7921097278595 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 493.6169958114624 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 494.4402883052826 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 495.2668082714081 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 496.0933256149292 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 496.91558289527893 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 497.7462124824524 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 498.57452178001404 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 499.44238662719727 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 500.2662513256073 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 501.0899167060852 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 501.91671109199524 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 502.739737033844 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 503.5636966228485 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 504.39339566230774 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 505.22353982925415 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 506.0451602935791 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 506.87007570266724 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 507.6948027610779 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 508.5208172798157 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 509.3405547142029 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 510.16591811180115 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 510.9956157207489 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 511.82691264152527 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 23 epochs...
Completing Train Step...
At time: 514.502067565918 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 515.3298602104187 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 516.1711857318878 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 516.9970726966858 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 517.819132566452 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 518.6487894058228 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 519.4791440963745 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 520.3019831180573 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 521.1935675144196 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 522.0190482139587 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 522.8450996875763 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 523.6701424121857 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 524.49706864357 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 525.3212654590607 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 526.1415319442749 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 526.9749066829681 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 527.8335807323456 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 528.6993753910065 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 529.5582375526428 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 530.3827996253967 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 531.226884841919 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 532.0607073307037 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 532.8845872879028 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 533.7145557403564 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 534.5481262207031 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 24 epochs...
Completing Train Step...
At time: 537.090815782547 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 537.9492266178131 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 538.7798256874084 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 539.6093592643738 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 540.437714099884 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 541.2615652084351 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 542.09095454216 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 542.9772226810455 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 543.8057341575623 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 544.6331689357758 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 545.4604957103729 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 546.2913954257965 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 547.1144461631775 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 547.9401602745056 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 548.7678668498993 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 549.5972578525543 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 550.4281141757965 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 551.2527129650116 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 552.0792367458344 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 552.903181552887 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 553.7326979637146 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 554.5576379299164 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 555.3822312355042 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 556.2139739990234 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 557.0375416278839 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 25 epochs...
Completing Train Step...
At time: 559.5412287712097 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 560.3967685699463 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 561.2238922119141 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 562.0488867759705 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 562.8734037876129 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 563.7318542003632 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 564.5566034317017 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 565.3904316425323 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 566.2174289226532 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 567.0527174472809 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 567.8803470134735 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 568.7075917720795 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 569.5331568717957 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 570.3580620288849 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 571.1865167617798 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 572.0152268409729 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 572.8461263179779 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 573.6704804897308 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 574.4989030361176 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 575.3247380256653 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 576.1492855548859 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 576.9776637554169 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 577.8014588356018 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 578.6320507526398 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 579.4566788673401 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 26 epochs...
Completing Train Step...
At time: 581.9679002761841 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 582.793160200119 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 583.6176629066467 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 584.4442186355591 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 585.320300579071 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 586.1516003608704 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 586.9810264110565 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 587.8035171031952 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 588.6296570301056 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 589.4523587226868 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 590.2883658409119 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 591.1142365932465 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 591.9402215480804 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 592.7677648067474 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 593.5986211299896 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 594.4246978759766 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 595.2462632656097 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 596.0739412307739 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 596.8971836566925 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 597.7231578826904 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 598.5615880489349 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 599.3883121013641 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 600.2165968418121 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 601.0412023067474 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 601.8705170154572 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 27 epochs...
Completing Train Step...
At time: 604.284013748169 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 605.1749575138092 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 606.0509312152863 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 606.9464647769928 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 607.8012778759003 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 608.6362836360931 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 609.4605679512024 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 610.2849695682526 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 611.108761548996 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 611.9335346221924 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 612.7593514919281 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 613.5913467407227 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 614.4150438308716 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 615.238331079483 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 616.0631442070007 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 616.8885941505432 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 617.7121126651764 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 618.5366957187653 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 619.3630814552307 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 620.190437078476 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 621.0148885250092 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 621.837806224823 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 622.6688899993896 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 623.4941236972809 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 624.3248627185822 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 28 epochs...
Completing Train Step...
At time: 626.9055666923523 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 627.7342495918274 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 628.6222450733185 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 629.4472095966339 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 630.2742528915405 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 631.0984470844269 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 631.9269609451294 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 632.7534742355347 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 633.5877795219421 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 634.4121661186218 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 635.2399971485138 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 636.0628478527069 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 636.8854205608368 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 637.7076201438904 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 638.5318984985352 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 639.3567559719086 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 640.1811029911041 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 641.0038497447968 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 641.834388256073 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 642.6648969650269 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 643.4895117282867 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 644.3138484954834 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 645.1352081298828 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 645.961464881897 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 646.7875308990479 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 29 epochs...
Completing Train Step...
At time: 649.312558889389 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 650.1696753501892 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 650.9923677444458 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 651.8179204463959 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 652.6416630744934 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 653.4688439369202 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 654.2915086746216 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 655.1174504756927 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 655.9395940303802 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 656.7623860836029 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 657.5837321281433 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 658.4170365333557 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 659.2407355308533 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 660.0660545825958 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 660.8896634578705 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 661.7192275524139 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 662.5469317436218 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 663.371200799942 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 664.1953046321869 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 665.0197730064392 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 665.8433840274811 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 666.6732106208801 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 667.4979274272919 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 668.3232831954956 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 669.1482231616974 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 30 epochs...
Completing Train Step...
At time: 671.5977170467377 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 672.4511978626251 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 673.2743318080902 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 674.1026048660278 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 674.9345262050629 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 675.7635893821716 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 676.5906820297241 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 677.4160556793213 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 678.2467217445374 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 679.0965557098389 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 679.9209837913513 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 680.7658200263977 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 681.5950012207031 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 682.4226331710815 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 683.2502081394196 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 684.0731208324432 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 684.895560503006 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 685.7198193073273 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 686.5459253787994 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 687.3685436248779 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 688.1914772987366 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 689.0126223564148 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 689.8422303199768 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 690.6662378311157 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 691.5444786548615 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 31 epochs...
Completing Train Step...
At time: 694.0333960056305 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 694.8590292930603 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 695.6840872764587 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 696.5120005607605 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 697.3383820056915 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 698.1696479320526 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 698.9932272434235 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 699.8157279491425 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 700.6411426067352 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 701.4700303077698 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 702.2951679229736 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 703.1208288669586 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 703.9418222904205 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 704.7674074172974 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 705.592277765274 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 706.418333530426 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 707.250378370285 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 708.0817024707794 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 708.9075858592987 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 709.7362701892853 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 710.558632850647 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 711.3865773677826 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 712.2110636234283 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 713.0918962955475 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 713.9164726734161 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 32 epochs...
Completing Train Step...
At time: 716.3204357624054 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 717.181539773941 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 718.0063986778259 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 718.8314518928528 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 719.6630461215973 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 720.4863357543945 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 721.3121645450592 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 722.141587972641 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 722.96892786026 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 723.795083284378 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 724.6231603622437 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 725.4551813602448 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 726.2808566093445 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 727.1065113544464 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 727.9320838451385 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 728.7585067749023 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 729.5826597213745 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 730.4113383293152 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 731.2375054359436 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 732.0638692378998 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 732.888879776001 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 733.7120926380157 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 734.5671381950378 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 735.3970694541931 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 736.222086429596 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 33 epochs...
Completing Train Step...
At time: 738.6711323261261 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 739.497524023056 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 740.3269302845001 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 741.1517763137817 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 741.982842206955 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 742.8083338737488 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 743.6331896781921 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 744.45893907547 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 745.2830274105072 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 746.1108045578003 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 746.933146238327 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 747.7602667808533 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 748.5849523544312 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 749.4087755680084 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 750.2402229309082 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 751.070928812027 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 751.8973803520203 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 752.7192375659943 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 753.5450735092163 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 754.3838999271393 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 755.2364201545715 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 756.1414568424225 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 756.9768950939178 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 757.8148508071899 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 758.6555685997009 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 34 epochs...
Completing Train Step...
At time: 761.1382639408112 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 762.0305452346802 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 762.8537337779999 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 763.6783578395844 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 764.5065596103668 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 765.3295638561249 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 766.1581435203552 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 766.9823672771454 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 767.808887720108 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 768.634523153305 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 769.472291469574 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 770.3064653873444 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 771.1373147964478 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 771.9681584835052 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 772.78959441185 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 773.6159098148346 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 774.4418668746948 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 775.2716610431671 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 776.1068906784058 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 776.9862430095673 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 777.8219351768494 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 778.64550948143 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 779.4720706939697 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 780.2963593006134 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 781.119678735733 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 35 epochs...
Completing Train Step...
At time: 783.574051618576 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 784.4018156528473 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 785.2282769680023 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 786.0592455863953 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 786.885094165802 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 787.7128913402557 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 788.5381479263306 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 789.3684976100922 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 790.192663192749 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 791.0173888206482 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 791.8416576385498 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 792.6698439121246 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 793.4948048591614 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 794.3257308006287 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 795.1514809131622 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 795.974244594574 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 796.8008422851562 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 797.6249825954437 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 798.508118391037 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 799.3389596939087 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 800.1620573997498 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 800.9874691963196 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 801.8112807273865 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 802.6423799991608 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 803.4739534854889 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 36 epochs...
Completing Train Step...
At time: 805.9347486495972 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 806.762041091919 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 807.5848531723022 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 808.410472869873 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 809.2379138469696 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 810.0663466453552 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 810.8906157016754 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 811.7229363918304 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 812.5502779483795 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 813.3739759922028 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 814.2028617858887 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 815.0277886390686 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 815.8592855930328 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 816.6845703125 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 817.5090615749359 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 818.3375375270844 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 819.1619684696198 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 820.0179874897003 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 820.8470883369446 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 821.6740787029266 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 822.499858379364 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 823.3233456611633 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 824.1544938087463 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 824.9790732860565 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 825.8061172962189 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 37 epochs...
Completing Train Step...
At time: 828.2356069087982 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 829.0973272323608 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 829.9221746921539 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 830.75119805336 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 831.5846538543701 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 832.4442474842072 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 833.3109567165375 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 834.1574375629425 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 834.97918176651 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 835.801899433136 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 836.6265199184418 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 837.4502005577087 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 838.2800259590149 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 839.1048398017883 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 839.928384065628 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 840.7508647441864 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 841.6038343906403 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 842.4262046813965 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 843.2502989768982 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 844.073812007904 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 844.8957829475403 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 845.7259492874146 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 846.5528764724731 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 847.3750379085541 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 848.1970858573914 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 38 epochs...
Completing Train Step...
At time: 850.642968416214 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 851.4640290737152 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 852.2876007556915 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 853.1177225112915 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 853.9425137042999 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 854.7662014961243 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 855.5879230499268 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 856.4089608192444 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 857.2319827079773 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 858.0586771965027 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 858.8831915855408 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 859.708544254303 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 860.5330970287323 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 861.3582897186279 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 862.1819891929626 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 863.0626885890961 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 863.887149810791 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 864.7117376327515 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 865.5373318195343 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 866.3616790771484 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 867.1855282783508 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 868.0096807479858 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 868.8345065116882 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 869.6597521305084 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 870.4905304908752 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 39 epochs...
Completing Train Step...
At time: 872.9769902229309 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 873.8324289321899 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 874.658549785614 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 875.4859478473663 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 876.3106598854065 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 877.1365795135498 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 877.9645903110504 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 878.7889611721039 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 879.6226179599762 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 880.4458904266357 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 881.275717496872 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 882.1016385555267 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 882.9282143115997 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 883.753148317337 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 884.633814573288 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 885.458025932312 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 886.2828996181488 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 887.1101815700531 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 887.938716173172 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 888.7658321857452 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 889.5942883491516 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 890.4189507961273 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 891.2453322410583 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 892.0718154907227 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 892.901125907898 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 40 epochs...
Completing Train Step...
At time: 895.3495237827301 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 896.1739547252655 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 897.0042049884796 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 897.831116437912 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 898.6632351875305 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 899.4880332946777 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 900.3134233951569 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 901.1428642272949 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 901.9656331539154 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 902.7990486621857 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 903.6272089481354 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 904.4715709686279 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 905.3569183349609 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 906.17462682724 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 906.9968998432159 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 907.8307929039001 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 908.6540098190308 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 909.4768888950348 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 910.3051767349243 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 911.1282422542572 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 911.9555354118347 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 912.7867214679718 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 913.6108329296112 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 914.4379937648773 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 915.2631795406342 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 41 epochs...
Completing Train Step...
At time: 917.6755094528198 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 918.5509572029114 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 919.3743484020233 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 920.2053821086884 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 921.0280380249023 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 921.8508703708649 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 922.6728000640869 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 923.4981045722961 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 924.32053565979 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 925.1503074169159 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 925.9744486808777 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 926.8281960487366 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 927.6511659622192 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 928.4804546833038 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 929.3054718971252 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 930.1300327777863 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 930.9596660137177 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 931.7854437828064 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 932.6084356307983 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 933.4333505630493 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 934.2575051784515 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 935.0829815864563 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 935.9074494838715 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 936.7295851707458 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 937.5598254203796 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 42 epochs...
Completing Train Step...
At time: 940.0078237056732 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 940.8345620632172 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 941.6613318920135 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 942.4870820045471 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 943.3128244876862 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 944.1444146633148 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 944.9720017910004 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 945.7969763278961 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 946.6220345497131 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 947.4526579380035 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 948.3219408988953 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 949.14710688591 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 949.9735109806061 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 950.7977991104126 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 951.6232256889343 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 952.4486513137817 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 953.2749643325806 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 954.1018290519714 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 954.925389289856 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 955.7560126781464 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 956.5799071788788 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 957.4073739051819 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 958.2402303218842 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 959.0657713413239 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 959.8944051265717 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 43 epochs...
Completing Train Step...
At time: 962.3751854896545 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 963.2019636631012 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 964.0381953716278 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 964.8614423274994 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 965.692498922348 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 966.5166375637054 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 967.3426880836487 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 968.1683478355408 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 969.0002129077911 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 969.8739953041077 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 970.7037038803101 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 971.5285696983337 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 972.3534350395203 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 973.1841261386871 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 974.0105013847351 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 974.835455417633 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 975.6583163738251 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 976.4894857406616 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 977.3182063102722 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 978.14452958107 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 978.972354888916 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 979.7971565723419 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 980.6252868175507 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 981.4743142127991 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 982.3424189090729 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 44 epochs...
Completing Train Step...
At time: 984.8503668308258 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 985.7439768314362 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 986.5675535202026 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 987.3948521614075 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 988.2274098396301 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 989.0575304031372 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 989.8859264850616 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 990.7089774608612 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 991.5668680667877 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 992.3951590061188 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 993.2219157218933 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 994.0472328662872 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 994.8759124279022 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 995.7036485671997 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 996.5313441753387 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 997.3639574050903 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 998.1899402141571 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 999.0180644989014 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 999.8494937419891 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 1000.6792893409729 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 1001.5089724063873 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 1002.33105301857 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 1003.159255027771 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 1003.9842264652252 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 1004.812130689621 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 45 epochs...
Completing Train Step...
At time: 1007.2854664325714 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 1008.1091833114624 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 1008.9356422424316 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 1009.7617473602295 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 1010.5954041481018 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 1011.422176361084 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 1012.2977528572083 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 1013.1244764328003 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 1013.9555361270905 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 1014.7838122844696 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 1015.6088054180145 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 1016.4389338493347 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 1017.2661533355713 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 1018.0905349254608 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 1018.9198229312897 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 1019.7532429695129 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 1020.5852856636047 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 1021.4265031814575 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 1022.2614362239838 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 1023.0884847640991 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 1023.9266495704651 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 1024.7708253860474 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 1025.6142060756683 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 1026.4540960788727 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 1027.2780892848969 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 46 epochs...
Completing Train Step...
At time: 1029.6725506782532 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 1030.5492475032806 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 1031.3809719085693 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 1032.2083938121796 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 1033.038334608078 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 1033.9152233600616 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 1034.7396907806396 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 1035.5664737224579 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 1036.395051240921 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 1037.2235763072968 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 1038.0542764663696 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 1038.8911700248718 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 1039.720031261444 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 1040.5513982772827 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 1041.379635810852 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 1042.2106761932373 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 1043.0395958423615 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 1043.8668415546417 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 1044.6978824138641 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 1045.5241584777832 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 1046.353153705597 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 1047.1799824237823 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 1048.009612083435 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 1048.8396456241608 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 1049.6695311069489 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 47 epochs...
Completing Train Step...
At time: 1052.1374056339264 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 1052.9634668827057 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 1053.7899947166443 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 1054.614930152893 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 1055.501306295395 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 1056.3308057785034 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 1057.1566607952118 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 1057.983500957489 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 1058.8174912929535 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 1059.6460056304932 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 1060.4749829769135 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 1061.3053007125854 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 1062.1345028877258 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 1062.9577839374542 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 1063.7899360656738 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 1064.6175360679626 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 1065.4452366828918 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 1066.275143146515 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 1067.1020691394806 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 1067.9386970996857 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 1068.789375782013 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 1069.6477212905884 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 1070.5082921981812 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 1071.349272966385 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 1072.1728343963623 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 48 epochs...
Completing Train Step...
At time: 1074.6180477142334 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 1075.4754436016083 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 1076.2992236614227 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 1077.184889793396 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 1078.0088198184967 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 1078.8350689411163 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 1079.6696548461914 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 1080.4967947006226 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 1081.3230283260345 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 1082.1468844413757 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 1082.9695410728455 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 1083.7943544387817 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 1084.6191141605377 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 1085.4481348991394 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 1086.2723677158356 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 1087.1002840995789 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 1087.9309966564178 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 1088.7535283565521 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 1089.578691959381 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 1090.404721736908 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 1091.2308294773102 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 1092.0562183856964 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 1092.8824632167816 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 1093.7060289382935 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 1094.5305502414703 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished 49 epochs...
Completing Train Step...
At time: 1097.0011613368988 and batch: 50, loss is 9.934270458221436 and perplexity is 20625.231971062163
At time: 1097.8280534744263 and batch: 100, loss is 9.932829151153564 and perplexity is 20595.52609123556
At time: 1098.6928219795227 and batch: 150, loss is 9.932551403045654 and perplexity is 20589.806517169505
At time: 1099.5164959430695 and batch: 200, loss is 9.933651638031005 and perplexity is 20612.472609368644
At time: 1100.3478116989136 and batch: 250, loss is 9.934074974060058 and perplexity is 20621.200458948308
At time: 1101.1746351718903 and batch: 300, loss is 9.933834590911864 and perplexity is 20616.244065603034
At time: 1101.99760055542 and batch: 350, loss is 9.933879146575928 and perplexity is 20617.162656511926
At time: 1102.8261785507202 and batch: 400, loss is 9.933810424804687 and perplexity is 20615.745857259248
At time: 1103.64901804924 and batch: 450, loss is 9.933546600341797 and perplexity is 20610.307636580757
At time: 1104.47421169281 and batch: 500, loss is 9.933644886016845 and perplexity is 20612.333434131568
At time: 1105.299500465393 and batch: 550, loss is 9.933195743560791 and perplexity is 20603.07763880888
At time: 1106.1271634101868 and batch: 600, loss is 9.933513851165772 and perplexity is 20609.632677040277
At time: 1106.9570252895355 and batch: 650, loss is 9.932873783111573 and perplexity is 20596.445330404797
At time: 1107.7796139717102 and batch: 700, loss is 9.932901058197022 and perplexity is 20597.007107872356
At time: 1108.6053755283356 and batch: 750, loss is 9.933616256713867 and perplexity is 20611.74332583983
At time: 1109.4308726787567 and batch: 800, loss is 9.93391969680786 and perplexity is 20617.9987041903
At time: 1110.259460926056 and batch: 850, loss is 9.933841514587403 and perplexity is 20616.386806281913
At time: 1111.0863423347473 and batch: 900, loss is 9.93371597290039 and perplexity is 20613.798752759856
At time: 1111.9130222797394 and batch: 950, loss is 9.933839168548584 and perplexity is 20616.338439494888
At time: 1112.7368099689484 and batch: 1000, loss is 9.934235134124755 and perplexity is 20624.5034162418
At time: 1113.5591492652893 and batch: 1050, loss is 9.934032821655274 and perplexity is 20620.331244079298
At time: 1114.3822150230408 and batch: 1100, loss is 9.933367576599121 and perplexity is 20606.61823242527
At time: 1115.205703496933 and batch: 1150, loss is 9.932895584106445 and perplexity is 20596.894358298443
At time: 1116.0342664718628 and batch: 1200, loss is 9.933137893676758 and perplexity is 20601.885787631305
At time: 1116.8569703102112 and batch: 1250, loss is 9.933752346038819 and perplexity is 20614.54855495167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 10.009967887488594 and perplexity of 22247.1210339194
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f15b303a898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'batch_size': 50, 'wordvec_dim': 200, 'anneal': 7.732698473565949, 'lr': 6.371976747761653, 'dropout': 0.35353854866768, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}, 'best_accuracy': -75.67835575245712}, {'params': {'batch_size': 50, 'wordvec_dim': 200, 'anneal': 5.400371833329866, 'lr': 15.300871416778174, 'dropout': 0.12688207441098331, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}, 'best_accuracy': -124.95230359747548}, {'params': {'batch_size': 50, 'wordvec_dim': 200, 'anneal': 3.8987434387266164, 'lr': 7.751153807508898, 'dropout': 0.7574826006389305, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}, 'best_accuracy': -78.9073985756669}, {'params': {'batch_size': 50, 'wordvec_dim': 200, 'anneal': 7.428288555337209, 'lr': 21.9743528860023, 'dropout': 0.2134645447692738, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}, 'best_accuracy': -153.27568240719728}, {'params': {'batch_size': 50, 'wordvec_dim': 200, 'anneal': 4.321431559402063, 'lr': 4.99481944811914, 'dropout': 0.2336529334756795, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}, 'best_accuracy': -76.73639730487633}, {'params': {'batch_size': 50, 'wordvec_dim': 200, 'anneal': 8.0, 'lr': 0.0, 'dropout': 1.0, 'wordvec_source': 'glove', 'num_layers': 1, 'data': 'wikitext', 'seq_len': 35, 'tune_wordvecs': True}, 'best_accuracy': -22247.1210339194}]
