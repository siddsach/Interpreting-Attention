Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [2, 8], 'name': 'anneal', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 11.995311358945631, 'anneal': 5.454218864592506, 'dropout': 0.10651906940627132, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.3819024562835693 and batch: 50, loss is 6.943992605209351 and perplexity is 1036.9019013083664
At time: 3.9918768405914307 and batch: 100, loss is 5.880865173339844 and perplexity is 358.1189426787218
At time: 5.603405714035034 and batch: 150, loss is 5.678937320709228 and perplexity is 292.6382839835824
At time: 7.244460344314575 and batch: 200, loss is 5.629536294937134 and perplexity is 278.532930552931
At time: 8.857052326202393 and batch: 250, loss is 5.6397631645202635 and perplexity is 281.3960660104679
At time: 10.470600605010986 and batch: 300, loss is 5.554057750701904 and perplexity is 258.2834823981593
At time: 12.084678888320923 and batch: 350, loss is 5.505181636810303 and perplexity is 245.96312757961394
At time: 13.700217723846436 and batch: 400, loss is 5.522049322128296 and perplexity is 250.1471443524493
At time: 15.313655138015747 and batch: 450, loss is 5.502688827514649 and perplexity is 245.3507519936854
At time: 16.927603721618652 and batch: 500, loss is 5.49820255279541 and perplexity is 244.25250647522313
At time: 18.540647745132446 and batch: 550, loss is 5.456813583374023 and perplexity is 234.34949825843105
At time: 20.153131008148193 and batch: 600, loss is 5.4957029914855955 and perplexity is 243.64274474609826
At time: 21.75339412689209 and batch: 650, loss is 5.484617128372192 and perplexity is 240.95667086196758
At time: 23.355512142181396 and batch: 700, loss is 5.433256454467774 and perplexity is 228.89341407161518
At time: 24.961054801940918 and batch: 750, loss is 5.433656921386719 and perplexity is 228.98509666861358
At time: 26.574711799621582 and batch: 800, loss is 5.441643114089966 and perplexity is 230.82113750631564
At time: 28.18009090423584 and batch: 850, loss is 5.4262081909179685 and perplexity is 227.28578511602137
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0735321044921875 and perplexity of 159.7375417805252
Finished 1 epochs...
Completing Train Step...
At time: 32.379655599594116 and batch: 50, loss is 5.3289806938171385 and perplexity is 206.2276578594338
At time: 33.97256135940552 and batch: 100, loss is 5.234197568893433 and perplexity is 187.57852709247786
At time: 35.56152701377869 and batch: 150, loss is 5.221097106933594 and perplexity is 185.13718798442451
At time: 37.15152072906494 and batch: 200, loss is 5.229876031875611 and perplexity is 186.7696486019318
At time: 38.743958711624146 and batch: 250, loss is 5.253786296844482 and perplexity is 191.28917662838393
At time: 40.34152054786682 and batch: 300, loss is 5.203657350540161 and perplexity is 181.93643185431696
At time: 41.93017792701721 and batch: 350, loss is 5.165594673156738 and perplexity is 175.14157960575747
At time: 43.52367544174194 and batch: 400, loss is 5.1882550907135006 and perplexity is 179.15566964305165
At time: 45.12421441078186 and batch: 450, loss is 5.200030307769776 and perplexity is 181.2777359157803
At time: 46.72629928588867 and batch: 500, loss is 5.200245952606201 and perplexity is 181.31683173874404
At time: 48.31779885292053 and batch: 550, loss is 5.179087181091308 and perplexity is 177.5206927672882
At time: 49.91691565513611 and batch: 600, loss is 5.211233749389648 and perplexity is 183.32008978075822
At time: 51.5166654586792 and batch: 650, loss is 5.203616371154785 and perplexity is 181.92897636392388
At time: 53.16064786911011 and batch: 700, loss is 5.160340814590454 and perplexity is 174.22382350886878
At time: 54.7558479309082 and batch: 750, loss is 5.150758876800537 and perplexity is 172.56239423823
At time: 56.348491191864014 and batch: 800, loss is 5.152063236236573 and perplexity is 172.78762448412482
At time: 57.950035572052 and batch: 850, loss is 5.147227516174317 and perplexity is 171.9540888985697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.0214643478393555 and perplexity of 151.63318502562
Finished 2 epochs...
Completing Train Step...
At time: 62.176992893218994 and batch: 50, loss is 5.135555686950684 and perplexity is 169.95873749151343
At time: 63.77999424934387 and batch: 100, loss is 5.056269283294678 and perplexity is 157.00368608085068
At time: 65.37952542304993 and batch: 150, loss is 5.047745962142944 and perplexity is 155.67117999788985
At time: 66.98916554450989 and batch: 200, loss is 5.060626945495605 and perplexity is 157.68934796551522
At time: 68.59894037246704 and batch: 250, loss is 5.084995412826538 and perplexity is 161.57919804409076
At time: 70.20561480522156 and batch: 300, loss is 5.046149520874024 and perplexity is 155.422858369947
At time: 71.81224322319031 and batch: 350, loss is 5.012801589965821 and perplexity is 150.32529659755892
At time: 73.41873836517334 and batch: 400, loss is 5.037750215530395 and perplexity is 154.1228814193833
At time: 75.02645707130432 and batch: 450, loss is 5.049789972305298 and perplexity is 155.98969888881913
At time: 76.63376522064209 and batch: 500, loss is 5.048491621017456 and perplexity is 155.78730088266488
At time: 78.23991537094116 and batch: 550, loss is 5.037947750091552 and perplexity is 154.15332902225668
At time: 79.8488621711731 and batch: 600, loss is 5.0783562755584715 and perplexity is 160.5100047564483
At time: 81.45687341690063 and batch: 650, loss is 5.062758836746216 and perplexity is 158.02588310745813
At time: 83.06641817092896 and batch: 700, loss is 5.038183040618897 and perplexity is 154.18960410775887
At time: 84.6708254814148 and batch: 750, loss is 5.034349899291993 and perplexity is 153.59970486992898
At time: 86.26888465881348 and batch: 800, loss is 5.030212478637695 and perplexity is 152.96551114630194
At time: 87.86779880523682 and batch: 850, loss is 5.031805200576782 and perplexity is 153.2093367935131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.9979244867960615 and perplexity of 148.10544507394874
Finished 3 epochs...
Completing Train Step...
At time: 92.04476189613342 and batch: 50, loss is 5.021131277084351 and perplexity is 151.5826888560652
At time: 93.68404459953308 and batch: 100, loss is 4.956226978302002 and perplexity is 142.05680009651545
At time: 95.28731656074524 and batch: 150, loss is 4.961152076721191 and perplexity is 142.75816955718125
At time: 96.89074373245239 and batch: 200, loss is 4.984865245819091 and perplexity is 146.18387481053165
At time: 98.48580288887024 and batch: 250, loss is 4.9968178176879885 and perplexity is 147.9416320132936
At time: 100.07518172264099 and batch: 300, loss is 4.9700308036804195 and perplexity is 144.03132399746184
At time: 101.6666579246521 and batch: 350, loss is 4.926551475524902 and perplexity is 137.90312908656705
At time: 103.26376533508301 and batch: 400, loss is 4.9456689453125 and perplexity is 140.56484960517366
At time: 104.85476112365723 and batch: 450, loss is 4.956683206558227 and perplexity is 142.12162520910803
At time: 106.44848990440369 and batch: 500, loss is 4.953038187026977 and perplexity is 141.60453208918398
At time: 108.03879284858704 and batch: 550, loss is 4.939152011871338 and perplexity is 139.6517762866999
At time: 109.63838505744934 and batch: 600, loss is 4.976056642532349 and perplexity is 144.90185374736535
At time: 111.23781728744507 and batch: 650, loss is 4.963122749328614 and perplexity is 143.03977655787824
At time: 112.83488011360168 and batch: 700, loss is 4.9456714916229245 and perplexity is 140.56520752737126
At time: 114.43853807449341 and batch: 750, loss is 4.936341075897217 and perplexity is 139.2597752878523
At time: 116.02806568145752 and batch: 800, loss is 4.917897071838379 and perplexity is 136.7148092545433
At time: 117.62381267547607 and batch: 850, loss is 4.918407669067383 and perplexity is 136.78463328177295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.980594952901204 and perplexity of 145.56095781083985
Finished 4 epochs...
Completing Train Step...
At time: 121.8040292263031 and batch: 50, loss is 4.914486150741578 and perplexity is 136.24928022015928
At time: 123.44162034988403 and batch: 100, loss is 4.850261507034301 and perplexity is 127.77379922474137
At time: 125.03386378288269 and batch: 150, loss is 4.866382064819336 and perplexity is 129.85027615366232
At time: 126.63760185241699 and batch: 200, loss is 4.884554681777954 and perplexity is 132.23156709468142
At time: 128.22935795783997 and batch: 250, loss is 4.899387054443359 and perplexity is 134.2074925825085
At time: 129.87430119514465 and batch: 300, loss is 4.868619546890259 and perplexity is 130.14113909742517
At time: 131.47568225860596 and batch: 350, loss is 4.8319073581695555 and perplexity is 125.45001070805169
At time: 133.06943225860596 and batch: 400, loss is 4.85819242477417 and perplexity is 128.7911918113791
At time: 134.66626477241516 and batch: 450, loss is 4.87652042388916 and perplexity is 131.17344090471727
At time: 136.2582290172577 and batch: 500, loss is 4.874820461273194 and perplexity is 130.9506403889008
At time: 137.85030937194824 and batch: 550, loss is 4.860067081451416 and perplexity is 129.03285752842925
At time: 139.44883489608765 and batch: 600, loss is 4.899120674133301 and perplexity is 134.17174711017864
At time: 141.05069780349731 and batch: 650, loss is 4.872871713638306 and perplexity is 130.6956991269273
At time: 142.65564727783203 and batch: 700, loss is 4.861681079864502 and perplexity is 129.2412845108796
At time: 144.25964903831482 and batch: 750, loss is 4.855634145736694 and perplexity is 128.46212910174305
At time: 145.86294221878052 and batch: 800, loss is 4.829896240234375 and perplexity is 125.19796946882687
At time: 147.4683427810669 and batch: 850, loss is 4.846848773956299 and perplexity is 127.33848458246442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.981218973795573 and perplexity of 145.65181923668385
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 151.74771690368652 and batch: 50, loss is 4.837738008499145 and perplexity is 126.18360243446058
At time: 153.35734367370605 and batch: 100, loss is 4.74765941619873 and perplexity is 115.31406618084624
At time: 154.96769666671753 and batch: 150, loss is 4.734725837707519 and perplexity is 113.8322459302184
At time: 156.5796296596527 and batch: 200, loss is 4.734614696502685 and perplexity is 113.81959518027958
At time: 158.18980169296265 and batch: 250, loss is 4.723064155578613 and perplexity is 112.51248075984765
At time: 159.8000466823578 and batch: 300, loss is 4.680959997177124 and perplexity is 107.87358121289985
At time: 161.4112527370453 and batch: 350, loss is 4.617813148498535 and perplexity is 101.27232226462617
At time: 163.02508687973022 and batch: 400, loss is 4.630148191452026 and perplexity is 102.52925694420333
At time: 164.6322853565216 and batch: 450, loss is 4.635937604904175 and perplexity is 103.12456277663901
At time: 166.24012422561646 and batch: 500, loss is 4.608426742553711 and perplexity is 100.3261864906692
At time: 167.84906935691833 and batch: 550, loss is 4.571436500549316 and perplexity is 96.68289509829901
At time: 169.45771622657776 and batch: 600, loss is 4.586759347915649 and perplexity is 98.17576060900483
At time: 171.11763381958008 and batch: 650, loss is 4.533850469589233 and perplexity is 93.11641360870486
At time: 172.7284812927246 and batch: 700, loss is 4.4969181632995605 and perplexity is 89.74014024144923
At time: 174.3378005027771 and batch: 750, loss is 4.462739343643189 and perplexity is 86.72475290238712
At time: 175.94791269302368 and batch: 800, loss is 4.393998260498047 and perplexity is 80.9634858045119
At time: 177.5589199066162 and batch: 850, loss is 4.397314739227295 and perplexity is 81.23244523562012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.798523585001628 and perplexity of 121.33115008497967
Finished 6 epochs...
Completing Train Step...
At time: 181.80991792678833 and batch: 50, loss is 4.6463633728027345 and perplexity is 104.20533970778881
At time: 183.41514086723328 and batch: 100, loss is 4.580290060043335 and perplexity is 97.5426833390386
At time: 185.0177538394928 and batch: 150, loss is 4.580834608078003 and perplexity is 97.59581448046352
At time: 186.61566376686096 and batch: 200, loss is 4.595870141983032 and perplexity is 99.07430676539012
At time: 188.21248698234558 and batch: 250, loss is 4.589885559082031 and perplexity is 98.48315901389634
At time: 189.8145089149475 and batch: 300, loss is 4.560363149642944 and perplexity is 95.61819724006389
At time: 191.417222738266 and batch: 350, loss is 4.503952932357788 and perplexity is 90.37366714838048
At time: 193.02095556259155 and batch: 400, loss is 4.5218069458007815 and perplexity is 92.00168994068119
At time: 194.62438893318176 and batch: 450, loss is 4.535418767929077 and perplexity is 93.26256249815474
At time: 196.2282280921936 and batch: 500, loss is 4.512028455734253 and perplexity is 91.1064365761399
At time: 197.83261489868164 and batch: 550, loss is 4.491731367111206 and perplexity is 89.27588147273185
At time: 199.43675327301025 and batch: 600, loss is 4.517936010360717 and perplexity is 91.64624573316934
At time: 201.04125833511353 and batch: 650, loss is 4.467661762237549 and perplexity is 87.15270084401263
At time: 202.6447458267212 and batch: 700, loss is 4.443155221939087 and perplexity is 85.04284785915887
At time: 204.24811244010925 and batch: 750, loss is 4.420057106018066 and perplexity is 83.1010307918118
At time: 205.85209250450134 and batch: 800, loss is 4.3647137641906735 and perplexity is 78.62689088873373
At time: 207.45796966552734 and batch: 850, loss is 4.38452039718628 and perplexity is 80.1997499619574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.784342447916667 and perplexity of 119.6226790754372
Finished 7 epochs...
Completing Train Step...
At time: 211.72183084487915 and batch: 50, loss is 4.567013339996338 and perplexity is 96.25619550662819
At time: 213.32739853858948 and batch: 100, loss is 4.505816555023193 and perplexity is 90.54224659828488
At time: 214.93311142921448 and batch: 150, loss is 4.509138145446777 and perplexity is 90.8434908856175
At time: 216.5372998714447 and batch: 200, loss is 4.527508497238159 and perplexity is 92.52774053540927
At time: 218.14030957221985 and batch: 250, loss is 4.521557826995849 and perplexity is 91.97877344421504
At time: 219.74358582496643 and batch: 300, loss is 4.497355899810791 and perplexity is 89.77943137631061
At time: 221.34745955467224 and batch: 350, loss is 4.441541652679444 and perplexity is 84.90573598360338
At time: 222.9519567489624 and batch: 400, loss is 4.461238203048706 and perplexity is 86.59466452016325
At time: 224.55920505523682 and batch: 450, loss is 4.480432090759277 and perplexity is 88.27280630036257
At time: 226.166277885437 and batch: 500, loss is 4.459884948730469 and perplexity is 86.47755917099603
At time: 227.77126574516296 and batch: 550, loss is 4.444204530715942 and perplexity is 85.1321309003736
At time: 229.37561321258545 and batch: 600, loss is 4.473870820999146 and perplexity is 87.69552054056518
At time: 230.98115921020508 and batch: 650, loss is 4.427612571716309 and perplexity is 83.73127567813012
At time: 232.58847570419312 and batch: 700, loss is 4.407537803649903 and perplexity is 82.06714910253724
At time: 234.19481801986694 and batch: 750, loss is 4.389269323348999 and perplexity is 80.58151843044986
At time: 235.80484008789062 and batch: 800, loss is 4.33853850364685 and perplexity is 76.59551343196071
At time: 237.4109342098236 and batch: 850, loss is 4.364643878936768 and perplexity is 78.62139622050054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.779257456461589 and perplexity of 119.01594270650241
Finished 8 epochs...
Completing Train Step...
At time: 241.6620659828186 and batch: 50, loss is 4.511479711532592 and perplexity is 91.05645616182025
At time: 243.29738330841064 and batch: 100, loss is 4.454735355377197 and perplexity is 86.03337956083999
At time: 244.90183234214783 and batch: 150, loss is 4.461134376525879 and perplexity is 86.58567416397759
At time: 246.50312042236328 and batch: 200, loss is 4.480065698623657 and perplexity is 88.24046976263335
At time: 248.10225200653076 and batch: 250, loss is 4.473533849716187 and perplexity is 87.66597464683731
At time: 249.7020719051361 and batch: 300, loss is 4.453135089874268 and perplexity is 85.89581341196671
At time: 251.30922484397888 and batch: 350, loss is 4.398746728897095 and perplexity is 81.34885258521912
At time: 252.9301836490631 and batch: 400, loss is 4.419991102218628 and perplexity is 83.09554598905318
At time: 254.52768421173096 and batch: 450, loss is 4.4391944789886475 and perplexity is 84.70668117339821
At time: 256.1332848072052 and batch: 500, loss is 4.422846097946167 and perplexity is 83.33312239626946
At time: 257.73820519447327 and batch: 550, loss is 4.409900035858154 and perplexity is 82.26123991892104
At time: 259.3453531265259 and batch: 600, loss is 4.440799293518066 and perplexity is 84.84272882250765
At time: 260.95173597335815 and batch: 650, loss is 4.3963285827636716 and perplexity is 81.15237682117619
At time: 262.55912733078003 and batch: 700, loss is 4.379220342636108 and perplexity is 79.77581135351383
At time: 264.16546297073364 and batch: 750, loss is 4.364755115509033 and perplexity is 78.63014228155473
At time: 265.77307176589966 and batch: 800, loss is 4.315805540084839 and perplexity is 74.87391309612474
At time: 267.37639689445496 and batch: 850, loss is 4.342358598709106 and perplexity is 76.8886751709568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.777539889017741 and perplexity of 118.81170024831955
Finished 9 epochs...
Completing Train Step...
At time: 271.58715057373047 and batch: 50, loss is 4.467003049850464 and perplexity is 87.09531118411383
At time: 273.22667813301086 and batch: 100, loss is 4.4120872020721436 and perplexity is 82.44135582350148
At time: 274.83693051338196 and batch: 150, loss is 4.418876323699951 and perplexity is 83.00296447291743
At time: 276.44615936279297 and batch: 200, loss is 4.43802887916565 and perplexity is 84.60800460068562
At time: 278.05429005622864 and batch: 250, loss is 4.430134201049805 and perplexity is 83.94268135036275
At time: 279.6417009830475 and batch: 300, loss is 4.415405530929565 and perplexity is 82.71537774956306
At time: 281.22888112068176 and batch: 350, loss is 4.360755805969238 and perplexity is 78.31630398995593
At time: 282.83266854286194 and batch: 400, loss is 4.383056869506836 and perplexity is 80.08246125657222
At time: 284.44564867019653 and batch: 450, loss is 4.404061393737793 and perplexity is 81.78234538600132
At time: 286.05643033981323 and batch: 500, loss is 4.388225460052491 and perplexity is 80.49744622856043
At time: 287.6666944026947 and batch: 550, loss is 4.377872838973999 and perplexity is 79.66838555015396
At time: 289.27782702445984 and batch: 600, loss is 4.411273107528687 and perplexity is 82.37426807715971
At time: 290.88826513290405 and batch: 650, loss is 4.367028417587281 and perplexity is 78.80909567791241
At time: 292.5477771759033 and batch: 700, loss is 4.350739498138427 and perplexity is 77.53577929442446
At time: 294.15606355667114 and batch: 750, loss is 4.338729705810547 and perplexity is 76.61016006004722
At time: 295.7632358074188 and batch: 800, loss is 4.291489906311035 and perplexity is 73.0752627280228
At time: 297.362380027771 and batch: 850, loss is 4.321243238449097 and perplexity is 75.28216381708344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.778355598449707 and perplexity of 118.90865561116595
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 301.5981206893921 and batch: 50, loss is 4.441948652267456 and perplexity is 84.94029961638363
At time: 303.2260720729828 and batch: 100, loss is 4.395019273757935 and perplexity is 81.04619281237598
At time: 304.8215937614441 and batch: 150, loss is 4.396564655303955 and perplexity is 81.17153693042124
At time: 306.42893719673157 and batch: 200, loss is 4.408685626983643 and perplexity is 82.16140177357008
At time: 308.03405499458313 and batch: 250, loss is 4.393414001464844 and perplexity is 80.91619597267199
At time: 309.6394920349121 and batch: 300, loss is 4.368678588867187 and perplexity is 78.93925154439664
At time: 311.2429859638214 and batch: 350, loss is 4.3096527671813964 and perplexity is 74.41464524670143
At time: 312.8478651046753 and batch: 400, loss is 4.324564237594604 and perplexity is 75.53259142356093
At time: 314.45122838020325 and batch: 450, loss is 4.346456928253174 and perplexity is 77.2044369058686
At time: 316.0546224117279 and batch: 500, loss is 4.323032388687134 and perplexity is 75.41697548159586
At time: 317.65768337249756 and batch: 550, loss is 4.302963056564331 and perplexity is 73.91849420808542
At time: 319.26188135147095 and batch: 600, loss is 4.321557817459106 and perplexity is 75.30584973099872
At time: 320.86568427085876 and batch: 650, loss is 4.270936489105225 and perplexity is 71.5886462242178
At time: 322.46921014785767 and batch: 700, loss is 4.24670476436615 and perplexity is 69.87477869860118
At time: 324.06875467300415 and batch: 750, loss is 4.221898727416992 and perplexity is 68.16278404496333
At time: 325.65578722953796 and batch: 800, loss is 4.162795605659485 and perplexity is 64.25089191724776
At time: 327.2506675720215 and batch: 850, loss is 4.193626098632812 and perplexity is 66.26263069090918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.746262868245442 and perplexity of 115.15313695658111
Finished 11 epochs...
Completing Train Step...
At time: 331.4564790725708 and batch: 50, loss is 4.404172620773315 and perplexity is 81.79144229973866
At time: 333.063955783844 and batch: 100, loss is 4.357771987915039 and perplexity is 78.08297067337107
At time: 334.700838804245 and batch: 150, loss is 4.360770902633667 and perplexity is 78.31748631384113
At time: 336.30942606925964 and batch: 200, loss is 4.377005481719971 and perplexity is 79.59931455697406
At time: 337.91549134254456 and batch: 250, loss is 4.363300514221192 and perplexity is 78.51584992038894
At time: 339.5217969417572 and batch: 300, loss is 4.34272575378418 and perplexity is 76.91691042130203
At time: 341.1277253627777 and batch: 350, loss is 4.284890193939209 and perplexity is 72.59457495777757
At time: 342.7327950000763 and batch: 400, loss is 4.301673555374146 and perplexity is 73.82323765174827
At time: 344.33828926086426 and batch: 450, loss is 4.326579303741455 and perplexity is 75.68494804430325
At time: 345.9439308643341 and batch: 500, loss is 4.304988260269165 and perplexity is 74.06834590530015
At time: 347.5489685535431 and batch: 550, loss is 4.289046087265015 and perplexity is 72.89689804345163
At time: 349.15555238723755 and batch: 600, loss is 4.311197814941406 and perplexity is 74.52970829371365
At time: 350.7614150047302 and batch: 650, loss is 4.262988653182983 and perplexity is 71.02192649107286
At time: 352.3620545864105 and batch: 700, loss is 4.243139839172363 and perplexity is 69.62612382096692
At time: 353.95163130760193 and batch: 750, loss is 4.222334280014038 and perplexity is 68.19247898896153
At time: 355.5500292778015 and batch: 800, loss is 4.1679340171813966 and perplexity is 64.58188911218473
At time: 357.15944170951843 and batch: 850, loss is 4.20199517250061 and perplexity is 66.81951459268414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.744154612223308 and perplexity of 114.9106203954519
Finished 12 epochs...
Completing Train Step...
At time: 361.46309447288513 and batch: 50, loss is 4.388940544128418 and perplexity is 80.55502925641068
At time: 363.0728554725647 and batch: 100, loss is 4.342022256851196 and perplexity is 76.86281863965822
At time: 364.6823308467865 and batch: 150, loss is 4.344579753875732 and perplexity is 77.05964665587524
At time: 366.29176473617554 and batch: 200, loss is 4.361404333114624 and perplexity is 78.36711071200821
At time: 367.9020278453827 and batch: 250, loss is 4.348581371307373 and perplexity is 77.36862768082938
At time: 369.5153625011444 and batch: 300, loss is 4.329531221389771 and perplexity is 75.90869385527785
At time: 371.12538719177246 and batch: 350, loss is 4.271884765625 and perplexity is 71.65656425396168
At time: 372.7322909832001 and batch: 400, loss is 4.289859819412231 and perplexity is 72.95624073407495
At time: 374.3984160423279 and batch: 450, loss is 4.316069602966309 and perplexity is 74.89368712804281
At time: 376.0091178417206 and batch: 500, loss is 4.295030345916748 and perplexity is 73.33443981206379
At time: 377.6205356121063 and batch: 550, loss is 4.281354713439941 and perplexity is 72.33837142183489
At time: 379.2285714149475 and batch: 600, loss is 4.305015335083008 and perplexity is 74.07035131912514
At time: 380.84197974205017 and batch: 650, loss is 4.25844521522522 and perplexity is 70.6999747122189
At time: 382.45225977897644 and batch: 700, loss is 4.240374107360839 and perplexity is 69.43382268466753
At time: 384.06172728538513 and batch: 750, loss is 4.222010135650635 and perplexity is 68.1703783633609
At time: 385.67107009887695 and batch: 800, loss is 4.169276633262634 and perplexity is 64.66865602935866
At time: 387.27897691726685 and batch: 850, loss is 4.204200105667114 and perplexity is 66.96700970525526
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.743592580159505 and perplexity of 114.8460550879043
Finished 13 epochs...
Completing Train Step...
At time: 391.55849146842957 and batch: 50, loss is 4.377200899124145 and perplexity is 79.61487116836562
At time: 393.15701508522034 and batch: 100, loss is 4.330618009567261 and perplexity is 75.99123537077588
At time: 394.7609808444977 and batch: 150, loss is 4.332891397476196 and perplexity is 76.16418944781685
At time: 396.36876106262207 and batch: 200, loss is 4.350193977355957 and perplexity is 77.49349345038635
At time: 397.97576665878296 and batch: 250, loss is 4.337804450988769 and perplexity is 76.53930892278731
At time: 399.5842320919037 and batch: 300, loss is 4.319315938949585 and perplexity is 75.13721226782913
At time: 401.1941421031952 and batch: 350, loss is 4.26218623161316 and perplexity is 70.96495982402384
At time: 402.80114340782166 and batch: 400, loss is 4.281003379821778 and perplexity is 72.3129609841031
At time: 404.4063866138458 and batch: 450, loss is 4.308357553482056 and perplexity is 74.31832477001223
At time: 406.00962114334106 and batch: 500, loss is 4.287813510894775 and perplexity is 72.8071024008216
At time: 407.61581349372864 and batch: 550, loss is 4.275263032913208 and perplexity is 71.8990486388824
At time: 409.21977829933167 and batch: 600, loss is 4.299751091003418 and perplexity is 73.68145144076904
At time: 410.82495379447937 and batch: 650, loss is 4.254509563446045 and perplexity is 70.4222710618421
At time: 412.4280152320862 and batch: 700, loss is 4.237216858863831 and perplexity is 69.21494855417995
At time: 414.03220200538635 and batch: 750, loss is 4.220351905822754 and perplexity is 68.0574298817329
At time: 415.68693590164185 and batch: 800, loss is 4.168811736106872 and perplexity is 64.63859874241861
At time: 417.2927722930908 and batch: 850, loss is 4.204316558837891 and perplexity is 66.97480867997173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.743470509847005 and perplexity of 114.8320366497053
Finished 14 epochs...
Completing Train Step...
At time: 421.53194761276245 and batch: 50, loss is 4.3670890998840335 and perplexity is 78.81387813994708
At time: 423.178603887558 and batch: 100, loss is 4.321112632751465 and perplexity is 75.27233217960676
At time: 424.78923988342285 and batch: 150, loss is 4.323200416564942 and perplexity is 75.42964870063395
At time: 426.3970093727112 and batch: 200, loss is 4.340846109390259 and perplexity is 76.77246977288492
At time: 428.00446486473083 and batch: 250, loss is 4.328721532821655 and perplexity is 75.84725632963024
At time: 429.61081862449646 and batch: 300, loss is 4.31092788696289 and perplexity is 74.50959335512906
At time: 431.216365814209 and batch: 350, loss is 4.25423674583435 and perplexity is 70.403061246548
At time: 432.8224787712097 and batch: 400, loss is 4.273671569824219 and perplexity is 71.78471495988678
At time: 434.4276933670044 and batch: 450, loss is 4.301733331680298 and perplexity is 73.82765066409875
At time: 436.03250217437744 and batch: 500, loss is 4.281549167633057 and perplexity is 72.3524392892146
At time: 437.6387779712677 and batch: 550, loss is 4.2698258113861085 and perplexity is 71.50917844962751
At time: 439.24601888656616 and batch: 600, loss is 4.294852533340454 and perplexity is 73.32140118563994
At time: 440.85289454460144 and batch: 650, loss is 4.250381917953491 and perplexity is 70.132191975785
At time: 442.4569766521454 and batch: 700, loss is 4.233851590156555 and perplexity is 68.98241314525826
At time: 444.0575661659241 and batch: 750, loss is 4.2179492092132564 and perplexity is 67.89410481456983
At time: 445.6635546684265 and batch: 800, loss is 4.167242813110351 and perplexity is 64.53726527138144
At time: 447.2715890407562 and batch: 850, loss is 4.203124170303345 and perplexity is 66.89499627915049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.743562380472819 and perplexity of 114.84258682539405
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 451.510085105896 and batch: 50, loss is 4.362341651916504 and perplexity is 78.44060011444643
At time: 453.1445937156677 and batch: 100, loss is 4.319517011642456 and perplexity is 75.15232182844375
At time: 454.7476215362549 and batch: 150, loss is 4.318922052383423 and perplexity is 75.10762255717525
At time: 456.38061571121216 and batch: 200, loss is 4.337636022567749 and perplexity is 76.52641861341719
At time: 457.98535203933716 and batch: 250, loss is 4.32180814743042 and perplexity is 75.32470340192262
At time: 459.5917053222656 and batch: 300, loss is 4.302625198364257 and perplexity is 73.89352445703521
At time: 461.1980285644531 and batch: 350, loss is 4.244292182922363 and perplexity is 69.70640329548056
At time: 462.80568861961365 and batch: 400, loss is 4.261623210906983 and perplexity is 70.92501632779278
At time: 464.411828994751 and batch: 450, loss is 4.289404220581055 and perplexity is 72.92300952669393
At time: 466.01784110069275 and batch: 500, loss is 4.267166814804077 and perplexity is 71.31928835897813
At time: 467.6237452030182 and batch: 550, loss is 4.252941780090332 and perplexity is 70.31195069921075
At time: 469.22888135910034 and batch: 600, loss is 4.272232236862183 and perplexity is 71.68146717526918
At time: 470.83730936050415 and batch: 650, loss is 4.22599494934082 and perplexity is 68.44256656980696
At time: 472.4365723133087 and batch: 700, loss is 4.207326264381408 and perplexity is 67.17668677735647
At time: 474.0334985256195 and batch: 750, loss is 4.188647937774658 and perplexity is 65.93358435876857
At time: 475.62443590164185 and batch: 800, loss is 4.134663710594177 and perplexity is 62.46858001610982
At time: 477.22932386398315 and batch: 850, loss is 4.1718459320068355 and perplexity is 64.83502275747722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738781929016113 and perplexity of 114.29489755790622
Finished 16 epochs...
Completing Train Step...
At time: 481.4885199069977 and batch: 50, loss is 4.3559240627288816 and perplexity is 77.93881242310742
At time: 483.12508845329285 and batch: 100, loss is 4.312612619400024 and perplexity is 74.63522788449959
At time: 484.7233917713165 and batch: 150, loss is 4.312663230895996 and perplexity is 74.63900538062659
At time: 486.3224527835846 and batch: 200, loss is 4.331707992553711 and perplexity is 76.074109682039
At time: 487.91422295570374 and batch: 250, loss is 4.316546802520752 and perplexity is 74.92943489090081
At time: 489.52065205574036 and batch: 300, loss is 4.297557039260864 and perplexity is 73.51996774041864
At time: 491.120991230011 and batch: 350, loss is 4.239888277053833 and perplexity is 69.40009782221948
At time: 492.71865463256836 and batch: 400, loss is 4.257527933120728 and perplexity is 70.63515262524267
At time: 494.31850481033325 and batch: 450, loss is 4.285981178283691 and perplexity is 72.67381772099806
At time: 495.92352056503296 and batch: 500, loss is 4.264250717163086 and perplexity is 71.11161729215443
At time: 497.5486137866974 and batch: 550, loss is 4.25080002784729 and perplexity is 70.16152107008887
At time: 499.1468393802643 and batch: 600, loss is 4.270988531112671 and perplexity is 71.59237193802366
At time: 500.7445158958435 and batch: 650, loss is 4.225406608581543 and perplexity is 68.4023108614226
At time: 502.34615182876587 and batch: 700, loss is 4.207582745552063 and perplexity is 67.19391854233899
At time: 503.942852973938 and batch: 750, loss is 4.189892473220826 and perplexity is 66.01569212401019
At time: 505.5410490036011 and batch: 800, loss is 4.1367711210250855 and perplexity is 62.60036576780002
At time: 507.14063835144043 and batch: 850, loss is 4.174484853744507 and perplexity is 65.00634325967134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738371849060059 and perplexity of 114.2480371202582
Finished 17 epochs...
Completing Train Step...
At time: 511.3336455821991 and batch: 50, loss is 4.352579956054687 and perplexity is 77.67861203172338
At time: 512.928542137146 and batch: 100, loss is 4.309002380371094 and perplexity is 74.3662626783373
At time: 514.5324449539185 and batch: 150, loss is 4.309081888198852 and perplexity is 74.37217561340054
At time: 516.1255195140839 and batch: 200, loss is 4.3285465335845945 and perplexity is 75.83398427897266
At time: 517.7175626754761 and batch: 250, loss is 4.313433780670166 and perplexity is 74.69654061340039
At time: 519.3116853237152 and batch: 300, loss is 4.294577922821045 and perplexity is 73.30126912194129
At time: 520.9128460884094 and batch: 350, loss is 4.23727972984314 and perplexity is 69.21930030257627
At time: 522.515664100647 and batch: 400, loss is 4.255158348083496 and perplexity is 70.46797477377137
At time: 524.1183390617371 and batch: 450, loss is 4.2838508415222165 and perplexity is 72.5191628075237
At time: 525.7200212478638 and batch: 500, loss is 4.262405862808228 and perplexity is 70.98054765468223
At time: 527.323956489563 and batch: 550, loss is 4.24949954032898 and perplexity is 70.0703361929204
At time: 528.9288363456726 and batch: 600, loss is 4.2702845764160156 and perplexity is 71.5419918862707
At time: 530.5346851348877 and batch: 650, loss is 4.225098905563354 and perplexity is 68.38126650179198
At time: 532.1409454345703 and batch: 700, loss is 4.207840147018433 and perplexity is 67.21121658167776
At time: 533.7439677715302 and batch: 750, loss is 4.190773096084595 and perplexity is 66.07385265685102
At time: 535.34432721138 and batch: 800, loss is 4.1380195140838625 and perplexity is 62.67856443107971
At time: 536.994199514389 and batch: 850, loss is 4.175891132354736 and perplexity is 65.09782459877557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738284428914388 and perplexity of 114.23804997675566
Finished 18 epochs...
Completing Train Step...
At time: 541.25488114357 and batch: 50, loss is 4.349931240081787 and perplexity is 77.4731356956389
At time: 542.8601644039154 and batch: 100, loss is 4.306243991851806 and perplexity is 74.16141428875198
At time: 544.463894367218 and batch: 150, loss is 4.306284875869751 and perplexity is 74.16444636732591
At time: 546.0479719638824 and batch: 200, loss is 4.326105518341064 and perplexity is 75.64909811414792
At time: 547.6331510543823 and batch: 250, loss is 4.3109667873382564 and perplexity is 74.51249186265511
At time: 549.2278907299042 and batch: 300, loss is 4.292241697311401 and perplexity is 73.13022070876029
At time: 550.823798418045 and batch: 350, loss is 4.235225572586059 and perplexity is 69.07725891215222
At time: 552.4237787723541 and batch: 400, loss is 4.25330943107605 and perplexity is 70.33780570971193
At time: 554.0193197727203 and batch: 450, loss is 4.282180137634278 and perplexity is 72.39810591354117
At time: 555.6164011955261 and batch: 500, loss is 4.260934114456177 and perplexity is 70.87615898641155
At time: 557.2179238796234 and batch: 550, loss is 4.248443117141724 and perplexity is 69.9963513515591
At time: 558.8210217952728 and batch: 600, loss is 4.269595785140991 and perplexity is 71.49273135352172
At time: 560.4221405982971 and batch: 650, loss is 4.224746751785278 and perplexity is 68.3571900200046
At time: 562.016996383667 and batch: 700, loss is 4.207904663085937 and perplexity is 67.21555292494419
At time: 563.6115946769714 and batch: 750, loss is 4.191178665161133 and perplexity is 66.10065560311209
At time: 565.2054855823517 and batch: 800, loss is 4.138778100013733 and perplexity is 62.72612954699448
At time: 566.7989017963409 and batch: 850, loss is 4.176740322113037 and perplexity is 65.15312848313523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738274574279785 and perplexity of 114.23692420806246
Finished 19 epochs...
Completing Train Step...
At time: 571.0487847328186 and batch: 50, loss is 4.347603273391724 and perplexity is 77.29299058363172
At time: 572.6394684314728 and batch: 100, loss is 4.303890171051026 and perplexity is 73.98705689271844
At time: 574.2486050128937 and batch: 150, loss is 4.303865556716919 and perplexity is 73.98523577299342
At time: 575.8551986217499 and batch: 200, loss is 4.324010581970215 and perplexity is 75.49078395403284
At time: 577.4571409225464 and batch: 250, loss is 4.308810224533081 and perplexity is 74.3519741396695
At time: 579.1079697608948 and batch: 300, loss is 4.290209875106812 and perplexity is 72.98178395211274
At time: 580.7147643566132 and batch: 350, loss is 4.233433589935303 and perplexity is 68.9535845069094
At time: 582.3176274299622 and batch: 400, loss is 4.251706762313843 and perplexity is 70.22516779044572
At time: 583.9154922962189 and batch: 450, loss is 4.280700826644898 and perplexity is 72.29108577740128
At time: 585.5143961906433 and batch: 500, loss is 4.259623994827271 and perplexity is 70.78336353914341
At time: 587.1217470169067 and batch: 550, loss is 4.247463426589966 and perplexity is 69.92781016753975
At time: 588.7316381931305 and batch: 600, loss is 4.268919038772583 and perplexity is 71.44436527484059
At time: 590.3452522754669 and batch: 650, loss is 4.2243295001983645 and perplexity is 68.3286738236196
At time: 591.9554035663605 and batch: 700, loss is 4.207809557914734 and perplexity is 67.20916068224824
At time: 593.5693805217743 and batch: 750, loss is 4.191380596160888 and perplexity is 66.11400472233763
At time: 595.1815629005432 and batch: 800, loss is 4.139216437339782 and perplexity is 62.75363077786121
At time: 596.7922263145447 and batch: 850, loss is 4.177262897491455 and perplexity is 65.18718480162916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738283475240071 and perplexity of 114.23794103091339
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 601.047810792923 and batch: 50, loss is 4.346395778656006 and perplexity is 77.19971602999352
At time: 602.6823117733002 and batch: 100, loss is 4.303355722427368 and perplexity is 73.94752517677023
At time: 604.2899081707001 and batch: 150, loss is 4.303030090332031 and perplexity is 73.92344940933597
At time: 605.9003710746765 and batch: 200, loss is 4.323217248916626 and perplexity is 75.43091836969401
At time: 607.506108045578 and batch: 250, loss is 4.307010793685913 and perplexity is 74.21830320572403
At time: 609.1122629642487 and batch: 300, loss is 4.288299646377563 and perplexity is 72.8425051212597
At time: 610.7180852890015 and batch: 350, loss is 4.23140793800354 and perplexity is 68.81404991721884
At time: 612.323187828064 and batch: 400, loss is 4.248713817596435 and perplexity is 70.01530196055162
At time: 613.9300663471222 and batch: 450, loss is 4.27780707359314 and perplexity is 72.08219561136288
At time: 615.5338709354401 and batch: 500, loss is 4.256314811706543 and perplexity is 70.54951556357783
At time: 617.1409232616425 and batch: 550, loss is 4.243348436355591 and perplexity is 69.64064914919366
At time: 618.7973456382751 and batch: 600, loss is 4.264054183959961 and perplexity is 71.09764287149241
At time: 620.3993339538574 and batch: 650, loss is 4.218849058151245 and perplexity is 67.95522674881238
At time: 622.0040299892426 and batch: 700, loss is 4.2019016599655155 and perplexity is 66.81326642262643
At time: 623.6040458679199 and batch: 750, loss is 4.184819593429565 and perplexity is 65.68165044652093
At time: 625.2069458961487 and batch: 800, loss is 4.13218563079834 and perplexity is 62.31396953776889
At time: 626.814799785614 and batch: 850, loss is 4.170415959358215 and perplexity is 64.74237670470914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738273938496907 and perplexity of 114.23685157820512
Finished 21 epochs...
Completing Train Step...
At time: 631.0556464195251 and batch: 50, loss is 4.345556411743164 and perplexity is 77.13494433009092
At time: 632.693391084671 and batch: 100, loss is 4.3020405578613286 and perplexity is 73.85033593584302
At time: 634.2955174446106 and batch: 150, loss is 4.302004261016846 and perplexity is 73.84765545033142
At time: 635.9005298614502 and batch: 200, loss is 4.3221958637237545 and perplexity is 75.35391367900641
At time: 637.5030674934387 and batch: 250, loss is 4.30622784614563 and perplexity is 74.16021691001353
At time: 639.1082065105438 and batch: 300, loss is 4.287539091110229 and perplexity is 72.78712543263038
At time: 640.7121572494507 and batch: 350, loss is 4.2307944583892825 and perplexity is 68.77184684710535
At time: 642.3164584636688 and batch: 400, loss is 4.248112382888794 and perplexity is 69.97320498844559
At time: 643.9152092933655 and batch: 450, loss is 4.277424297332764 and perplexity is 72.05460953807088
At time: 645.522465467453 and batch: 500, loss is 4.256011066436767 and perplexity is 70.52808973610205
At time: 647.1293489933014 and batch: 550, loss is 4.243228063583374 and perplexity is 69.6322668157091
At time: 648.7376656532288 and batch: 600, loss is 4.2639350318908695 and perplexity is 71.089171944911
At time: 650.3429498672485 and batch: 650, loss is 4.218808145523071 and perplexity is 67.95244657876033
At time: 651.9534220695496 and batch: 700, loss is 4.201906008720398 and perplexity is 66.81355697777677
At time: 653.5598919391632 and batch: 750, loss is 4.185073251724243 and perplexity is 65.6983132552053
At time: 655.1709337234497 and batch: 800, loss is 4.132605390548706 and perplexity is 62.34013192464076
At time: 656.7773280143738 and batch: 850, loss is 4.170821685791015 and perplexity is 64.76864972773008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738222440083821 and perplexity of 114.23096871311331
Finished 22 epochs...
Completing Train Step...
At time: 661.013105392456 and batch: 50, loss is 4.344909343719483 and perplexity is 77.08504891870999
At time: 662.6552791595459 and batch: 100, loss is 4.301328458786011 and perplexity is 73.79776589966973
At time: 664.2614805698395 and batch: 150, loss is 4.3013386535644536 and perplexity is 73.79851825537769
At time: 665.8680307865143 and batch: 200, loss is 4.32156231880188 and perplexity is 75.30618870920416
At time: 667.4771900177002 and batch: 250, loss is 4.305632019042969 and perplexity is 74.11604340403223
At time: 669.0845823287964 and batch: 300, loss is 4.286949224472046 and perplexity is 72.74420339603166
At time: 670.6917762756348 and batch: 350, loss is 4.230318470001221 and perplexity is 68.73912003598949
At time: 672.2981555461884 and batch: 400, loss is 4.247699480056763 and perplexity is 69.94431881794077
At time: 673.9021956920624 and batch: 450, loss is 4.277152519226075 and perplexity is 72.03502933356837
At time: 675.508088350296 and batch: 500, loss is 4.255791130065918 and perplexity is 70.51257974967018
At time: 677.1173257827759 and batch: 550, loss is 4.243116426467895 and perplexity is 69.62449370418948
At time: 678.7209203243256 and batch: 600, loss is 4.263875703811646 and perplexity is 71.08495448599402
At time: 680.3259513378143 and batch: 650, loss is 4.21881817817688 and perplexity is 67.95312832555219
At time: 681.9319934844971 and batch: 700, loss is 4.201922488212586 and perplexity is 66.81465804033947
At time: 683.5370671749115 and batch: 750, loss is 4.1852533149719235 and perplexity is 65.71014417198184
At time: 685.1417112350464 and batch: 800, loss is 4.132913002967834 and perplexity is 62.359311473213296
At time: 686.7479875087738 and batch: 850, loss is 4.171101779937744 and perplexity is 64.78679358828629
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7382001876831055 and perplexity of 114.22842682810497
Finished 23 epochs...
Completing Train Step...
At time: 691.0388352870941 and batch: 50, loss is 4.344329700469971 and perplexity is 77.04038003772433
At time: 692.648140668869 and batch: 100, loss is 4.300723628997803 and perplexity is 73.75314430814457
At time: 694.2500689029694 and batch: 150, loss is 4.300760107040405 and perplexity is 73.75583472755503
At time: 695.8590214252472 and batch: 200, loss is 4.321027374267578 and perplexity is 75.26591484825634
At time: 697.4686095714569 and batch: 250, loss is 4.305100049972534 and perplexity is 74.07662644654893
At time: 699.0774068832397 and batch: 300, loss is 4.286428213119507 and perplexity is 72.70631271182634
At time: 700.71568775177 and batch: 350, loss is 4.229890127182006 and perplexity is 68.70968243266607
At time: 702.3238072395325 and batch: 400, loss is 4.247330560684204 and perplexity is 69.91851976290657
At time: 703.9327776432037 and batch: 450, loss is 4.276909351348877 and perplexity is 72.01751485796669
At time: 705.5446767807007 and batch: 500, loss is 4.255585041046142 and perplexity is 70.4980493785541
At time: 707.1523146629333 and batch: 550, loss is 4.242991800308228 and perplexity is 69.61581721159052
At time: 708.7632508277893 and batch: 600, loss is 4.2638242816925045 and perplexity is 71.0812992409763
At time: 710.3734650611877 and batch: 650, loss is 4.218821411132812 and perplexity is 67.95334801537665
At time: 711.9826600551605 and batch: 700, loss is 4.201925978660584 and perplexity is 66.8148912538359
At time: 713.5824856758118 and batch: 750, loss is 4.185381689071655 and perplexity is 65.71858019405495
At time: 715.1942613124847 and batch: 800, loss is 4.133149418830872 and perplexity is 62.37405594649867
At time: 716.8062546253204 and batch: 850, loss is 4.171306848526001 and perplexity is 64.80008068692186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738190650939941 and perplexity of 114.22733746613078
Finished 24 epochs...
Completing Train Step...
At time: 721.068681716919 and batch: 50, loss is 4.343803424835205 and perplexity is 76.99984622973064
At time: 722.6717374324799 and batch: 100, loss is 4.300176830291748 and perplexity is 73.71282720794302
At time: 724.273800611496 and batch: 150, loss is 4.300235214233399 and perplexity is 73.71713097898001
At time: 725.8757336139679 and batch: 200, loss is 4.320548610687256 and perplexity is 75.22988889403295
At time: 727.4717946052551 and batch: 250, loss is 4.304604568481445 and perplexity is 74.03993194069733
At time: 729.0696637630463 and batch: 300, loss is 4.285955057144165 and perplexity is 72.67191942285845
At time: 730.6731956005096 and batch: 350, loss is 4.229493150711059 and perplexity is 68.68241171868809
At time: 732.2768838405609 and batch: 400, loss is 4.246985282897949 and perplexity is 69.89438261843432
At time: 733.8829085826874 and batch: 450, loss is 4.2766832256317135 and perplexity is 72.00123168686235
At time: 735.4877307415009 and batch: 500, loss is 4.25539303779602 and perplexity is 70.48451482332702
At time: 737.0964143276215 and batch: 550, loss is 4.242859086990356 and perplexity is 69.60657887855054
At time: 738.7021124362946 and batch: 600, loss is 4.263775491714478 and perplexity is 71.07783127055002
At time: 740.3120684623718 and batch: 650, loss is 4.218811645507812 and perplexity is 67.95268441170268
At time: 741.9635429382324 and batch: 700, loss is 4.201927266120911 and perplexity is 66.81497727541299
At time: 743.568523645401 and batch: 750, loss is 4.1854843044281 and perplexity is 65.72532427560387
At time: 745.1731526851654 and batch: 800, loss is 4.133352947235108 and perplexity is 62.386752130544394
At time: 746.7755682468414 and batch: 850, loss is 4.171485900878906 and perplexity is 64.81168433263633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7381900151570635 and perplexity of 114.22726484236851
Finished 25 epochs...
Completing Train Step...
At time: 751.0245079994202 and batch: 50, loss is 4.343308181762695 and perplexity is 76.96172203045307
At time: 752.6298260688782 and batch: 100, loss is 4.299668560028076 and perplexity is 73.67537068964563
At time: 754.2360441684723 and batch: 150, loss is 4.299747128486633 and perplexity is 73.68115947735942
At time: 755.8424000740051 and batch: 200, loss is 4.320098657608032 and perplexity is 75.19604658817123
At time: 757.4487826824188 and batch: 250, loss is 4.304133157730103 and perplexity is 74.00503694633579
At time: 759.0542166233063 and batch: 300, loss is 4.285513324737549 and perplexity is 72.63982497010015
At time: 760.6620662212372 and batch: 350, loss is 4.2291264152526855 and perplexity is 68.65722806109657
At time: 762.270188331604 and batch: 400, loss is 4.24667179107666 and perplexity is 69.87247473527992
At time: 763.8774089813232 and batch: 450, loss is 4.276468839645386 and perplexity is 71.98579728630905
At time: 765.4826300144196 and batch: 500, loss is 4.255212030410767 and perplexity is 70.47175776019412
At time: 767.0858838558197 and batch: 550, loss is 4.242730655670166 and perplexity is 69.59763978777312
At time: 768.692019701004 and batch: 600, loss is 4.263721199035644 and perplexity is 71.07397236944065
At time: 770.2972838878632 and batch: 650, loss is 4.218794536590576 and perplexity is 67.95152182479445
At time: 771.89985704422 and batch: 700, loss is 4.2019228887557984 and perplexity is 66.81468480250263
At time: 773.498937368393 and batch: 750, loss is 4.18556926727295 and perplexity is 65.73090872336489
At time: 775.0943710803986 and batch: 800, loss is 4.133529267311096 and perplexity is 62.397753137241395
At time: 776.692994594574 and batch: 850, loss is 4.171642112731933 and perplexity is 64.82180947675693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738193829854329 and perplexity of 114.22770058563448
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 780.9406645298004 and batch: 50, loss is 4.343035020828247 and perplexity is 76.94070196560749
At time: 782.5700452327728 and batch: 100, loss is 4.299430618286133 and perplexity is 73.65784232905244
At time: 784.181113243103 and batch: 150, loss is 4.299660682678223 and perplexity is 73.67479032526101
At time: 785.7950553894043 and batch: 200, loss is 4.319695587158203 and perplexity is 75.16574339141945
At time: 787.4056913852692 and batch: 250, loss is 4.30376259803772 and perplexity is 73.9776187429644
At time: 789.0160284042358 and batch: 300, loss is 4.285088214874268 and perplexity is 72.60895162678467
At time: 790.6237771511078 and batch: 350, loss is 4.2287447643280025 and perplexity is 68.63102996610246
At time: 792.231353521347 and batch: 400, loss is 4.245983896255493 and perplexity is 69.82442634977986
At time: 793.8413197994232 and batch: 450, loss is 4.2759740447998045 and perplexity is 71.95018789525481
At time: 795.4495437145233 and batch: 500, loss is 4.254844923019409 and perplexity is 70.44589180512051
At time: 797.0559737682343 and batch: 550, loss is 4.241976184844971 and perplexity is 69.54515020247104
At time: 798.6648621559143 and batch: 600, loss is 4.262708225250244 and perplexity is 71.00201275136276
At time: 800.2746367454529 and batch: 650, loss is 4.217660617828369 and perplexity is 67.8745139878436
At time: 801.8857295513153 and batch: 700, loss is 4.2007142162323 and perplexity is 66.73397651358674
At time: 803.4959895610809 and batch: 750, loss is 4.184210233688354 and perplexity is 65.64163888495801
At time: 805.1077709197998 and batch: 800, loss is 4.13216275215149 and perplexity is 62.31254389477448
At time: 806.7202734947205 and batch: 850, loss is 4.1702671813964844 and perplexity is 64.73274518236231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738144874572754 and perplexity of 114.22210867326656
Finished 27 epochs...
Completing Train Step...
At time: 810.9498991966248 and batch: 50, loss is 4.34287769317627 and perplexity is 76.92859801779358
At time: 812.5847280025482 and batch: 100, loss is 4.299238338470459 and perplexity is 73.64368077424068
At time: 814.1897192001343 and batch: 150, loss is 4.2994628477096555 and perplexity is 73.66021631710458
At time: 815.7951250076294 and batch: 200, loss is 4.319591369628906 and perplexity is 75.15791021153983
At time: 817.4043734073639 and batch: 250, loss is 4.303642625808716 and perplexity is 73.96874401551847
At time: 819.0092575550079 and batch: 300, loss is 4.284982948303223 and perplexity is 72.6013087336974
At time: 820.615754365921 and batch: 350, loss is 4.228668727874756 and perplexity is 68.62581170439276
At time: 822.2464525699615 and batch: 400, loss is 4.245872964859009 and perplexity is 69.81668105826213
At time: 823.8431310653687 and batch: 450, loss is 4.275895738601685 and perplexity is 71.94455397017525
At time: 825.4478838443756 and batch: 500, loss is 4.254775171279907 and perplexity is 70.44097825299276
At time: 827.0519592761993 and batch: 550, loss is 4.241948051452637 and perplexity is 69.54319368899726
At time: 828.6529824733734 and batch: 600, loss is 4.2626951217651365 and perplexity is 71.0010823836416
At time: 830.2550718784332 and batch: 650, loss is 4.217644090652466 and perplexity is 67.87339222308138
At time: 831.8564524650574 and batch: 700, loss is 4.200730547904969 and perplexity is 66.73506639994685
At time: 833.4597337245941 and batch: 750, loss is 4.184250650405883 and perplexity is 65.64429195814888
At time: 835.0629224777222 and batch: 800, loss is 4.132255630493164 and perplexity is 62.3183316492913
At time: 836.6657001972198 and batch: 850, loss is 4.170337967872619 and perplexity is 64.73732754746713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738111813863118 and perplexity of 114.21833247172002
Finished 28 epochs...
Completing Train Step...
At time: 840.8339869976044 and batch: 50, loss is 4.342756252288819 and perplexity is 76.91925630782418
At time: 842.4648115634918 and batch: 100, loss is 4.299105777740478 and perplexity is 73.63391916117637
At time: 844.0682384967804 and batch: 150, loss is 4.299331622123718 and perplexity is 73.65055084625052
At time: 845.6722769737244 and batch: 200, loss is 4.319483919143677 and perplexity is 75.14983489147507
At time: 847.2745792865753 and batch: 250, loss is 4.303546514511108 and perplexity is 73.96163512517654
At time: 848.87961602211 and batch: 300, loss is 4.284888963699341 and perplexity is 72.59448564909223
At time: 850.4850282669067 and batch: 350, loss is 4.228597078323364 and perplexity is 68.62089487191678
At time: 852.0876853466034 and batch: 400, loss is 4.245792064666748 and perplexity is 69.8110331038049
At time: 853.6946477890015 and batch: 450, loss is 4.275851173400879 and perplexity is 71.94134781812261
At time: 855.2984066009521 and batch: 500, loss is 4.254751901626587 and perplexity is 70.43933913492023
At time: 856.8988106250763 and batch: 550, loss is 4.241934700012207 and perplexity is 69.54226519338783
At time: 858.4980936050415 and batch: 600, loss is 4.262680549621582 and perplexity is 71.00004775321496
At time: 860.1092298030853 and batch: 650, loss is 4.217630424499512 and perplexity is 67.87246466125985
At time: 861.7256906032562 and batch: 700, loss is 4.200736236572266 and perplexity is 66.73544603461646
At time: 863.372750043869 and batch: 750, loss is 4.184286460876465 and perplexity is 65.64664275322619
At time: 864.9758789539337 and batch: 800, loss is 4.13232424736023 and perplexity is 62.322607884679094
At time: 866.5807061195374 and batch: 850, loss is 4.170391607284546 and perplexity is 64.74080011277881
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738094965616862 and perplexity of 114.21640810933872
Finished 29 epochs...
Completing Train Step...
At time: 870.8035743236542 and batch: 50, loss is 4.342644233703613 and perplexity is 76.91064040413717
At time: 872.4011681079865 and batch: 100, loss is 4.2989883136749265 and perplexity is 73.6252703296426
At time: 873.9971060752869 and batch: 150, loss is 4.29921890258789 and perplexity is 73.64224945821876
At time: 875.6014523506165 and batch: 200, loss is 4.319382123947143 and perplexity is 75.14218538861077
At time: 877.2058672904968 and batch: 250, loss is 4.303454875946045 and perplexity is 73.95485769760555
At time: 878.8107371330261 and batch: 300, loss is 4.284798336029053 and perplexity is 72.58790687809594
At time: 880.4134066104889 and batch: 350, loss is 4.228526964187622 and perplexity is 68.61608374584489
At time: 882.0156457424164 and batch: 400, loss is 4.245720529556275 and perplexity is 69.80603934245632
At time: 883.6082916259766 and batch: 450, loss is 4.27581407546997 and perplexity is 71.93867899247604
At time: 885.199371099472 and batch: 500, loss is 4.254736680984497 and perplexity is 70.43826701110939
At time: 886.7955341339111 and batch: 550, loss is 4.241923246383667 and perplexity is 69.54146868667594
At time: 888.3938372135162 and batch: 600, loss is 4.26266583442688 and perplexity is 70.99900298137545
At time: 889.9889118671417 and batch: 650, loss is 4.217618455886841 and perplexity is 67.8716523268806
At time: 891.5851619243622 and batch: 700, loss is 4.200738801956176 and perplexity is 66.73561723687558
At time: 893.1800920963287 and batch: 750, loss is 4.18431791305542 and perplexity is 65.6487075156523
At time: 894.7831737995148 and batch: 800, loss is 4.132383017539978 and perplexity is 62.32627070317807
At time: 896.3789231777191 and batch: 850, loss is 4.1704363489151 and perplexity is 64.74369678653972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738087336222331 and perplexity of 114.21553671062343
Finished 30 epochs...
Completing Train Step...
At time: 900.5774121284485 and batch: 50, loss is 4.342537117004395 and perplexity is 76.90240243142242
At time: 902.1652612686157 and batch: 100, loss is 4.29887752532959 and perplexity is 73.6171139595915
At time: 903.7941014766693 and batch: 150, loss is 4.299114937782288 and perplexity is 73.63459365404367
At time: 905.3973281383514 and batch: 200, loss is 4.319285192489624 and perplexity is 75.13490210005546
At time: 907.0034425258636 and batch: 250, loss is 4.303365468978882 and perplexity is 73.948245913646
At time: 908.6112711429596 and batch: 300, loss is 4.28471037864685 and perplexity is 72.58152251660735
At time: 910.2207117080688 and batch: 350, loss is 4.228458147048951 and perplexity is 68.61136194576693
At time: 911.8290324211121 and batch: 400, loss is 4.245653800964355 and perplexity is 69.8013814391529
At time: 913.4404149055481 and batch: 450, loss is 4.275779485702515 and perplexity is 71.93619069333371
At time: 915.0487122535706 and batch: 500, loss is 4.254723281860351 and perplexity is 70.43732320634821
At time: 916.6587054729462 and batch: 550, loss is 4.241911354064942 and perplexity is 69.5406416822832
At time: 918.2694301605225 and batch: 600, loss is 4.26265097618103 and perplexity is 70.99794806857118
At time: 919.8805918693542 and batch: 650, loss is 4.217607641220093 and perplexity is 67.87091832154806
At time: 921.4906899929047 and batch: 700, loss is 4.20073968410492 and perplexity is 66.73567610764243
At time: 923.1004791259766 and batch: 750, loss is 4.184346094131469 and perplexity is 65.65055759293983
At time: 924.7111852169037 and batch: 800, loss is 4.132436232566834 and perplexity is 62.32958748559788
At time: 926.3215260505676 and batch: 850, loss is 4.170475401878357 and perplexity is 64.74622526912349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.73808224995931 and perplexity of 114.21495578183996
Finished 31 epochs...
Completing Train Step...
At time: 930.5901753902435 and batch: 50, loss is 4.342433443069458 and perplexity is 76.89443007002646
At time: 932.1933665275574 and batch: 100, loss is 4.298771181106567 and perplexity is 73.60928562106301
At time: 933.7977504730225 and batch: 150, loss is 4.29901617527008 and perplexity is 73.62732167569436
At time: 935.4022200107574 and batch: 200, loss is 4.319191789627075 and perplexity is 75.12788461285376
At time: 937.0051162242889 and batch: 250, loss is 4.303277521133423 and perplexity is 73.94174261072237
At time: 938.6019675731659 and batch: 300, loss is 4.28462435722351 and perplexity is 72.5752792192648
At time: 940.1952295303345 and batch: 350, loss is 4.22839056968689 and perplexity is 68.60672552757946
At time: 941.7882766723633 and batch: 400, loss is 4.245589866638183 and perplexity is 69.79691887752168
At time: 943.386314868927 and batch: 450, loss is 4.275745782852173 and perplexity is 71.93376627951969
At time: 945.0073170661926 and batch: 500, loss is 4.254710998535156 and perplexity is 70.43645800711516
At time: 946.60276222229 and batch: 550, loss is 4.241899003982544 and perplexity is 69.53978285493174
At time: 948.1982796192169 and batch: 600, loss is 4.262636032104492 and perplexity is 70.99688707772899
At time: 949.7962603569031 and batch: 650, loss is 4.217596864700317 and perplexity is 67.87018691319558
At time: 951.389074087143 and batch: 700, loss is 4.200739326477051 and perplexity is 66.7356522411091
At time: 952.9776132106781 and batch: 750, loss is 4.184371786117554 and perplexity is 65.65224430781937
At time: 954.5688667297363 and batch: 800, loss is 4.132485699653626 and perplexity is 62.33267082497302
At time: 956.1604821681976 and batch: 850, loss is 4.170510883331299 and perplexity is 64.74852260002461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7380797068278 and perplexity of 114.21466531855637
Finished 32 epochs...
Completing Train Step...
At time: 960.3203840255737 and batch: 50, loss is 4.342332105636597 and perplexity is 76.88663818069355
At time: 961.9363515377045 and batch: 100, loss is 4.298667821884155 and perplexity is 73.60167781571411
At time: 963.5288894176483 and batch: 150, loss is 4.2989208078384396 and perplexity is 73.62030036193522
At time: 965.1282079219818 and batch: 200, loss is 4.3191008949279786 and perplexity is 75.12105619672624
At time: 966.73100233078 and batch: 250, loss is 4.3031908130645755 and perplexity is 73.93533154296304
At time: 968.3318955898285 and batch: 300, loss is 4.284539995193481 and perplexity is 72.56915687963004
At time: 969.9240491390228 and batch: 350, loss is 4.228324012756348 and perplexity is 68.60215942646832
At time: 971.5150029659271 and batch: 400, loss is 4.2455274105072025 and perplexity is 69.79255976814218
At time: 973.1200733184814 and batch: 450, loss is 4.27571286201477 and perplexity is 71.93139819867605
At time: 974.7203252315521 and batch: 500, loss is 4.254698610305786 and perplexity is 70.43558542952219
At time: 976.3107850551605 and batch: 550, loss is 4.241885976791382 and perplexity is 69.5388769527878
At time: 977.9024713039398 and batch: 600, loss is 4.262620859146118 and perplexity is 70.99580985308901
At time: 979.4942281246185 and batch: 650, loss is 4.217586469650269 and perplexity is 67.86948140287274
At time: 981.0953681468964 and batch: 700, loss is 4.200738205909729 and perplexity is 66.73557745935985
At time: 982.6922130584717 and batch: 750, loss is 4.1843950557708744 and perplexity is 65.65377203055886
At time: 984.3365428447723 and batch: 800, loss is 4.132532291412353 and perplexity is 62.33557508138963
At time: 985.9318935871124 and batch: 850, loss is 4.170543580055237 and perplexity is 64.75063969920438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738077163696289 and perplexity of 114.21437485601135
Finished 33 epochs...
Completing Train Step...
At time: 990.0714819431305 and batch: 50, loss is 4.342232875823974 and perplexity is 76.87900911251545
At time: 991.704553604126 and batch: 100, loss is 4.298566989898681 and perplexity is 73.59425678655056
At time: 993.3128280639648 and batch: 150, loss is 4.298827977180481 and perplexity is 73.6134664582163
At time: 994.9113569259644 and batch: 200, loss is 4.319011869430542 and perplexity is 75.11436880500901
At time: 996.5064668655396 and batch: 250, loss is 4.303105354309082 and perplexity is 73.9290133915169
At time: 998.1066670417786 and batch: 300, loss is 4.284456901550293 and perplexity is 72.5631270945237
At time: 999.7072265148163 and batch: 350, loss is 4.2282586193084715 and perplexity is 68.59767344141028
At time: 1001.3036372661591 and batch: 400, loss is 4.2454663848876955 and perplexity is 69.78830076390085
At time: 1002.9013078212738 and batch: 450, loss is 4.275680160522461 and perplexity is 71.92904597307188
At time: 1004.5066950321198 and batch: 500, loss is 4.254686193466187 and perplexity is 70.43471084758562
At time: 1006.1040754318237 and batch: 550, loss is 4.2418720912933345 and perplexity is 69.53791137755142
At time: 1007.7121682167053 and batch: 600, loss is 4.262605676651001 and perplexity is 70.9947319677351
At time: 1009.3230805397034 and batch: 650, loss is 4.217576065063477 and perplexity is 67.86877525263655
At time: 1010.9386973381042 and batch: 700, loss is 4.200736422538757 and perplexity is 66.73545844517434
At time: 1012.5490515232086 and batch: 750, loss is 4.184416551589965 and perplexity is 65.6551833273335
At time: 1014.1595249176025 and batch: 800, loss is 4.132576670646667 and perplexity is 62.338341547868644
At time: 1015.77135014534 and batch: 850, loss is 4.170574159622192 and perplexity is 64.75261977600124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076527913411 and perplexity of 114.2143022404905
Finished 34 epochs...
Completing Train Step...
At time: 1019.9334070682526 and batch: 50, loss is 4.342135429382324 and perplexity is 76.87151789164218
At time: 1021.5526533126831 and batch: 100, loss is 4.298468093872071 and perplexity is 73.58697896685267
At time: 1023.1518540382385 and batch: 150, loss is 4.298737092018127 and perplexity is 73.6067763903844
At time: 1024.7736382484436 and batch: 200, loss is 4.31892430305481 and perplexity is 75.10779159994252
At time: 1026.3662543296814 and batch: 250, loss is 4.303020725250244 and perplexity is 73.92275711342795
At time: 1027.9693388938904 and batch: 300, loss is 4.284375143051148 and perplexity is 72.55719468467494
At time: 1029.570033788681 and batch: 350, loss is 4.228194017410278 and perplexity is 68.59324204463417
At time: 1031.1622786521912 and batch: 400, loss is 4.2454063415527346 and perplexity is 69.78411056737968
At time: 1032.7550477981567 and batch: 450, loss is 4.275647573471069 and perplexity is 71.9267020557451
At time: 1034.3473403453827 and batch: 500, loss is 4.254673957824707 and perplexity is 70.43384903898838
At time: 1035.940128326416 and batch: 550, loss is 4.241857595443726 and perplexity is 69.53690337375194
At time: 1037.5376467704773 and batch: 600, loss is 4.262590160369873 and perplexity is 70.99363040206144
At time: 1039.1410162448883 and batch: 650, loss is 4.217565383911133 and perplexity is 67.86805033978014
At time: 1040.7347791194916 and batch: 700, loss is 4.200734105110168 and perplexity is 66.73530379069426
At time: 1042.3269102573395 and batch: 750, loss is 4.184436492919922 and perplexity is 65.65649259206177
At time: 1043.9194691181183 and batch: 800, loss is 4.132619075775146 and perplexity is 62.34098506930018
At time: 1045.518733739853 and batch: 850, loss is 4.170603303909302 and perplexity is 64.75450697244348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076210021973 and perplexity of 114.21426593274742
Finished 35 epochs...
Completing Train Step...
At time: 1049.7545976638794 and batch: 50, loss is 4.342039356231689 and perplexity is 76.86413295747687
At time: 1051.3602612018585 and batch: 100, loss is 4.298370962142944 and perplexity is 73.57983168346404
At time: 1052.9546165466309 and batch: 150, loss is 4.298647928237915 and perplexity is 73.60021362453703
At time: 1054.552014350891 and batch: 200, loss is 4.318838405609131 and perplexity is 75.10134030957214
At time: 1056.1465826034546 and batch: 250, loss is 4.302937068939209 and perplexity is 73.91657326692882
At time: 1057.7405292987823 and batch: 300, loss is 4.284294309616089 and perplexity is 72.55132987442994
At time: 1059.3337082862854 and batch: 350, loss is 4.228130102157593 and perplexity is 68.58885803034057
At time: 1060.935198545456 and batch: 400, loss is 4.245347061157227 and perplexity is 69.7799738603191
At time: 1062.5415098667145 and batch: 450, loss is 4.275615148544311 and perplexity is 71.9243698755096
At time: 1064.1452572345734 and batch: 500, loss is 4.254661436080933 and perplexity is 70.43296708989945
At time: 1065.779619216919 and batch: 550, loss is 4.241842670440674 and perplexity is 69.53586554300169
At time: 1067.3835968971252 and batch: 600, loss is 4.2625743103027345 and perplexity is 70.99250515717077
At time: 1068.9882740974426 and batch: 650, loss is 4.217554779052734 and perplexity is 67.86733061253281
At time: 1070.5928027629852 and batch: 700, loss is 4.200731334686279 and perplexity is 66.73511890587048
At time: 1072.1983890533447 and batch: 750, loss is 4.184455156326294 and perplexity is 65.65771797729889
At time: 1073.801702260971 and batch: 800, loss is 4.132659769058227 and perplexity is 62.34352198027042
At time: 1075.4078183174133 and batch: 850, loss is 4.170630965232849 and perplexity is 64.75629819258563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076527913411 and perplexity of 114.2143022404905
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1079.649968624115 and batch: 50, loss is 4.3419810390472415 and perplexity is 76.85965058835862
At time: 1081.2537972927094 and batch: 100, loss is 4.298309984207154 and perplexity is 73.57534507400565
At time: 1082.8608739376068 and batch: 150, loss is 4.298620352745056 and perplexity is 73.59818409035464
At time: 1084.468059539795 and batch: 200, loss is 4.318747444152832 and perplexity is 75.0945092929719
At time: 1086.0743246078491 and batch: 250, loss is 4.302870540618897 and perplexity is 73.91165588504052
At time: 1087.679445028305 and batch: 300, loss is 4.2842205619812015 and perplexity is 72.54597958273185
At time: 1089.2853407859802 and batch: 350, loss is 4.2280810546875 and perplexity is 68.58549400287684
At time: 1090.8896114826202 and batch: 400, loss is 4.245201082229614 and perplexity is 69.76978819803026
At time: 1092.4949836730957 and batch: 450, loss is 4.2754905319213865 and perplexity is 71.91540746187418
At time: 1094.0940806865692 and batch: 500, loss is 4.254598455429077 and perplexity is 70.42853131540546
At time: 1095.6878485679626 and batch: 550, loss is 4.241703109741211 and perplexity is 69.52616174611886
At time: 1097.2810597419739 and batch: 600, loss is 4.262365379333496 and perplexity is 70.97767417363946
At time: 1098.8863315582275 and batch: 650, loss is 4.217316656112671 and perplexity is 67.85117176820525
At time: 1100.4980268478394 and batch: 700, loss is 4.200464940071106 and perplexity is 66.71734339730601
At time: 1102.1070806980133 and batch: 750, loss is 4.184198570251465 and perplexity is 65.64087328231037
At time: 1103.714715719223 and batch: 800, loss is 4.132358956336975 and perplexity is 62.32477107616772
At time: 1105.3266847133636 and batch: 850, loss is 4.170359940528869 and perplexity is 64.7387500141395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076210021973 and perplexity of 114.21426593274742
Finished 37 epochs...
Completing Train Step...
At time: 1109.6055090427399 and batch: 50, loss is 4.341962900161743 and perplexity is 76.85825645260121
At time: 1111.2152762413025 and batch: 100, loss is 4.298286809921264 and perplexity is 73.57364003768107
At time: 1112.82466173172 and batch: 150, loss is 4.298594179153443 and perplexity is 73.59625778675004
At time: 1114.4320697784424 and batch: 200, loss is 4.3187354469299315 and perplexity is 75.0936083728096
At time: 1116.0439944267273 and batch: 250, loss is 4.302850017547607 and perplexity is 73.91013900642322
At time: 1117.657352924347 and batch: 300, loss is 4.284205017089843 and perplexity is 72.54485187212586
At time: 1119.2657122612 and batch: 350, loss is 4.228068284988403 and perplexity is 68.58461819234793
At time: 1120.8732252120972 and batch: 400, loss is 4.245186815261841 and perplexity is 69.76879280181113
At time: 1122.4828045368195 and batch: 450, loss is 4.2754793548583985 and perplexity is 71.91460366332724
At time: 1124.092413187027 and batch: 500, loss is 4.254585247039795 and perplexity is 70.42760107409082
At time: 1125.6880979537964 and batch: 550, loss is 4.241698188781738 and perplexity is 69.52581961153642
At time: 1127.3009266853333 and batch: 600, loss is 4.262364368438721 and perplexity is 70.97760242271576
At time: 1128.9120917320251 and batch: 650, loss is 4.217315254211425 and perplexity is 67.8510766476297
At time: 1130.5236163139343 and batch: 700, loss is 4.2004672622680665 and perplexity is 66.71749832829798
At time: 1132.1350061893463 and batch: 750, loss is 4.184203577041626 and perplexity is 65.64120193321163
At time: 1133.7480866909027 and batch: 800, loss is 4.1323742055892945 and perplexity is 62.32572148957415
At time: 1135.3600630760193 and batch: 850, loss is 4.170369038581848 and perplexity is 64.73933901339626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7380781173706055 and perplexity of 114.21448377937915
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1139.6412541866302 and batch: 50, loss is 4.341951723098755 and perplexity is 76.8573974078285
At time: 1141.27015042305 and batch: 100, loss is 4.298271551132202 and perplexity is 73.57251740159224
At time: 1142.8781015872955 and batch: 150, loss is 4.298581581115723 and perplexity is 73.59533062415865
At time: 1144.482521057129 and batch: 200, loss is 4.318721599578858 and perplexity is 75.0925685324506
At time: 1146.0860214233398 and batch: 250, loss is 4.302834253311158 and perplexity is 73.9089738786996
At time: 1147.730714082718 and batch: 300, loss is 4.284191989898682 and perplexity is 72.54390682262843
At time: 1149.3374388217926 and batch: 350, loss is 4.228059768676758 and perplexity is 68.58403410685246
At time: 1150.9476335048676 and batch: 400, loss is 4.2451581954956055 and perplexity is 69.76679606384387
At time: 1152.5548791885376 and batch: 450, loss is 4.275451698303223 and perplexity is 71.91261478062603
At time: 1154.1589856147766 and batch: 500, loss is 4.254564123153687 and perplexity is 70.42611338517975
At time: 1155.7677257061005 and batch: 550, loss is 4.2416700172424315 and perplexity is 69.52386098976523
At time: 1157.368878364563 and batch: 600, loss is 4.262326745986939 and perplexity is 70.97493212152294
At time: 1158.9715077877045 and batch: 650, loss is 4.217272605895996 and perplexity is 67.84818297521616
At time: 1160.5739130973816 and batch: 700, loss is 4.2004196119308475 and perplexity is 66.71431929274588
At time: 1162.1740999221802 and batch: 750, loss is 4.184156293869019 and perplexity is 65.63809828230627
At time: 1163.7758166790009 and batch: 800, loss is 4.132318015098572 and perplexity is 62.32221947508989
At time: 1165.3775355815887 and batch: 850, loss is 4.170318646430969 and perplexity is 64.73607674105406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738077163696289 and perplexity of 114.21437485601135
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 1169.5989451408386 and batch: 50, loss is 4.341950569152832 and perplexity is 76.85730871859928
At time: 1171.2300465106964 and batch: 100, loss is 4.298269481658935 and perplexity is 73.57236514539187
At time: 1172.8362934589386 and batch: 150, loss is 4.2985799646377565 and perplexity is 73.5952116590244
At time: 1174.4411916732788 and batch: 200, loss is 4.318719854354859 and perplexity is 75.09243747921221
At time: 1176.0467221736908 and batch: 250, loss is 4.302832069396973 and perplexity is 73.90881246801943
At time: 1177.6534991264343 and batch: 300, loss is 4.284190492630005 and perplexity is 72.54379820499035
At time: 1179.251263141632 and batch: 350, loss is 4.228059206008911 and perplexity is 68.58399551683254
At time: 1180.8414254188538 and batch: 400, loss is 4.245153837203979 and perplexity is 69.76649200046339
At time: 1182.4384305477142 and batch: 450, loss is 4.275447483062744 and perplexity is 71.91231165230018
At time: 1184.0445737838745 and batch: 500, loss is 4.254560918807983 and perplexity is 70.42588771592747
At time: 1185.6481127738953 and batch: 550, loss is 4.241665573120117 and perplexity is 69.52355201790976
At time: 1187.2992374897003 and batch: 600, loss is 4.262320985794068 and perplexity is 70.97452329340237
At time: 1188.9008495807648 and batch: 650, loss is 4.217265911102295 and perplexity is 67.84772874714865
At time: 1190.5055322647095 and batch: 700, loss is 4.200412020683289 and perplexity is 66.7138128497547
At time: 1192.1116392612457 and batch: 750, loss is 4.18414852142334 and perplexity is 65.63758811573551
At time: 1193.7186028957367 and batch: 800, loss is 4.132308616638183 and perplexity is 62.32163374493131
At time: 1195.3259258270264 and batch: 850, loss is 4.170310173034668 and perplexity is 64.73552820894483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076527913411 and perplexity of 114.2143022404905
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1199.505978345871 and batch: 50, loss is 4.341950616836548 and perplexity is 76.85731238344142
At time: 1201.1332850456238 and batch: 100, loss is 4.29826922416687 and perplexity is 73.57234620109408
At time: 1202.7375586032867 and batch: 150, loss is 4.29857976436615 and perplexity is 73.5951969199946
At time: 1204.3357832431793 and batch: 200, loss is 4.3187196254730225 and perplexity is 75.0924202919192
At time: 1205.9350049495697 and batch: 250, loss is 4.302831945419311 and perplexity is 73.90880330497824
At time: 1207.5459039211273 and batch: 300, loss is 4.284190444946289 and perplexity is 72.54379474583259
At time: 1209.1534960269928 and batch: 350, loss is 4.228059225082397 and perplexity is 68.58399682496842
At time: 1210.7551078796387 and batch: 400, loss is 4.245153436660766 and perplexity is 69.76646405597413
At time: 1212.3569235801697 and batch: 450, loss is 4.275446977615356 and perplexity is 71.91227530441927
At time: 1213.9702818393707 and batch: 500, loss is 4.254560670852661 and perplexity is 70.42587025345597
At time: 1215.5811138153076 and batch: 550, loss is 4.241665163040161 and perplexity is 69.52352350770047
At time: 1217.1940805912018 and batch: 600, loss is 4.262320537567138 and perplexity is 70.97449148071685
At time: 1218.8082513809204 and batch: 650, loss is 4.217265157699585 and perplexity is 67.8476776305052
At time: 1220.4172472953796 and batch: 700, loss is 4.200411128997803 and perplexity is 66.71375336204257
At time: 1222.0287754535675 and batch: 750, loss is 4.184147748947144 and perplexity is 65.63753741228072
At time: 1223.6400225162506 and batch: 800, loss is 4.132307381629944 and perplexity is 62.32155677724765
At time: 1225.2536625862122 and batch: 850, loss is 4.170309314727783 and perplexity is 64.7354726460191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076527913411 and perplexity of 114.2143022404905
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1229.538825750351 and batch: 50, loss is 4.341950626373291 and perplexity is 76.85731311640987
At time: 1231.1446723937988 and batch: 100, loss is 4.2982692050933835 and perplexity is 73.57234479781292
At time: 1232.7499332427979 and batch: 150, loss is 4.29857973575592 and perplexity is 73.59519481441914
At time: 1234.3533482551575 and batch: 200, loss is 4.31871958732605 and perplexity is 75.09241742737075
At time: 1235.9598169326782 and batch: 250, loss is 4.302831964492798 and perplexity is 73.90880471467683
At time: 1237.563682794571 and batch: 300, loss is 4.284190340042114 and perplexity is 72.54378713568603
At time: 1239.1623785495758 and batch: 350, loss is 4.228059253692627 and perplexity is 68.58399878717236
At time: 1240.7649006843567 and batch: 400, loss is 4.245153398513794 and perplexity is 69.76646139459478
At time: 1242.3719210624695 and batch: 450, loss is 4.275446910858154 and perplexity is 71.91227050375711
At time: 1243.97762799263 and batch: 500, loss is 4.25456051826477 and perplexity is 70.42585950732179
At time: 1245.583668231964 and batch: 550, loss is 4.241665029525757 and perplexity is 69.52351422530928
At time: 1247.1893079280853 and batch: 600, loss is 4.262320442199707 and perplexity is 70.97448471206224
At time: 1248.7958724498749 and batch: 650, loss is 4.217265205383301 and perplexity is 67.84768086573465
At time: 1250.4036691188812 and batch: 700, loss is 4.200411114692688 and perplexity is 66.7137524076947
At time: 1252.011276960373 and batch: 750, loss is 4.184147748947144 and perplexity is 65.63753741228072
At time: 1253.6172978878021 and batch: 800, loss is 4.1323073196411135 and perplexity is 62.32155291400738
At time: 1255.2229492664337 and batch: 850, loss is 4.170309257507324 and perplexity is 64.73546894182577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.738076527913411 and perplexity of 114.2143022404905
Annealing...
Model not improving. Stopping early with 114.21426593274742loss at 41 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -114.21426593274742
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f07aaaf1860>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 3.5659892080942015, 'anneal': 6.055312748419992, 'dropout': 0.6749421451680976, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1890878677368164 and batch: 50, loss is 7.544729328155517 and perplexity is 1890.7508951179016
At time: 3.802824020385742 and batch: 100, loss is 6.577054595947265 and perplexity is 718.4201722075494
At time: 5.41774582862854 and batch: 150, loss is 6.409057073593139 and perplexity is 607.3207522143567
At time: 7.032434940338135 and batch: 200, loss is 6.365645341873169 and perplexity is 581.5199864487167
At time: 8.632418632507324 and batch: 250, loss is 6.317378158569336 and perplexity is 554.1182760132658
At time: 10.276119470596313 and batch: 300, loss is 6.220212364196778 and perplexity is 502.8099995247209
At time: 11.890054702758789 and batch: 350, loss is 6.170511779785156 and perplexity is 478.43089470548955
At time: 13.504764556884766 and batch: 400, loss is 6.164252424240113 and perplexity is 475.44557845663746
At time: 15.12778615951538 and batch: 450, loss is 6.147826261520386 and perplexity is 467.6996243381536
At time: 16.741546154022217 and batch: 500, loss is 6.1317266178131105 and perplexity is 460.2301165708977
At time: 18.352150678634644 and batch: 550, loss is 6.063698825836181 and perplexity is 429.9628569781696
At time: 19.954566478729248 and batch: 600, loss is 6.074843978881836 and perplexity is 434.7816621031185
At time: 21.56395959854126 and batch: 650, loss is 6.0871923828125 and perplexity is 440.1838069774474
At time: 23.184633255004883 and batch: 700, loss is 6.0278082370758055 and perplexity is 414.80487837552226
At time: 24.802290201187134 and batch: 750, loss is 6.0078480052948 and perplexity is 406.60736119391896
At time: 26.413700342178345 and batch: 800, loss is 6.0222522735595705 and perplexity is 412.50662801449295
At time: 28.02983331680298 and batch: 850, loss is 6.0167592334747315 and perplexity is 410.2469245742614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.378002166748047 and perplexity of 216.5891339230045
Finished 1 epochs...
Completing Train Step...
At time: 32.299046993255615 and batch: 50, loss is 5.676296167373657 and perplexity is 291.8664011828415
At time: 33.90213656425476 and batch: 100, loss is 5.515877542495727 and perplexity is 248.60804567618445
At time: 35.50851488113403 and batch: 150, loss is 5.439979724884033 and perplexity is 230.43751126601353
At time: 37.11790204048157 and batch: 200, loss is 5.4264397716522215 and perplexity is 227.33842622012133
At time: 38.7246675491333 and batch: 250, loss is 5.437567262649536 and perplexity is 229.8822595039221
At time: 40.332916021347046 and batch: 300, loss is 5.363138313293457 and perplexity is 213.39359263865944
At time: 41.93480443954468 and batch: 350, loss is 5.309282102584839 and perplexity is 202.20501381305118
At time: 43.54230523109436 and batch: 400, loss is 5.309478092193603 and perplexity is 202.24464777839444
At time: 45.151028633117676 and batch: 450, loss is 5.2889590549469 and perplexity is 198.13706822376557
At time: 46.766584634780884 and batch: 500, loss is 5.266378154754639 and perplexity is 193.71309152937133
At time: 48.37131857872009 and batch: 550, loss is 5.2323271179199216 and perplexity is 187.22799857926353
At time: 49.97374963760376 and batch: 600, loss is 5.263869438171387 and perplexity is 193.2277293568827
At time: 51.57558083534241 and batch: 650, loss is 5.252532567977905 and perplexity is 191.0495021405872
At time: 53.182124614715576 and batch: 700, loss is 5.188095827102661 and perplexity is 179.12713893621387
At time: 54.78860831260681 and batch: 750, loss is 5.16866888999939 and perplexity is 175.68083126319706
At time: 56.3932409286499 and batch: 800, loss is 5.159191751480103 and perplexity is 174.02374431415575
At time: 57.99994492530823 and batch: 850, loss is 5.140770015716552 and perplexity is 170.84727276974195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.889046986897786 and perplexity of 132.82692791052685
Finished 2 epochs...
Completing Train Step...
At time: 62.31349754333496 and batch: 50, loss is 5.085344772338868 and perplexity is 161.63565713560806
At time: 63.92441272735596 and batch: 100, loss is 4.9993015384674075 and perplexity is 148.30953441315538
At time: 65.5400938987732 and batch: 150, loss is 4.977621917724609 and perplexity is 145.12884262803536
At time: 67.15200185775757 and batch: 200, loss is 5.00137885093689 and perplexity is 148.6179398746871
At time: 68.76105999946594 and batch: 250, loss is 5.01684723854065 and perplexity is 150.93469178771298
At time: 70.37153697013855 and batch: 300, loss is 4.977071132659912 and perplexity is 145.04892983846474
At time: 71.982421875 and batch: 350, loss is 4.935012845993042 and perplexity is 139.07492907617473
At time: 73.59220623970032 and batch: 400, loss is 4.948348627090454 and perplexity is 140.94202379899102
At time: 75.1986312866211 and batch: 450, loss is 4.942259244918823 and perplexity is 140.08638176169268
At time: 76.80171942710876 and batch: 500, loss is 4.941351280212403 and perplexity is 139.95924599733064
At time: 78.39933848381042 and batch: 550, loss is 4.924256229400635 and perplexity is 137.5869704337658
At time: 80.00651860237122 and batch: 600, loss is 4.962268142700196 and perplexity is 142.917586036556
At time: 81.61581540107727 and batch: 650, loss is 4.954443197250367 and perplexity is 141.80362773740603
At time: 83.22559714317322 and batch: 700, loss is 4.909463891983032 and perplexity is 135.56671651968566
At time: 84.84026646614075 and batch: 750, loss is 4.8936692810058595 and perplexity is 133.44231418931196
At time: 86.4509756565094 and batch: 800, loss is 4.876019649505615 and perplexity is 131.10776905047425
At time: 88.06343746185303 and batch: 850, loss is 4.870063800811767 and perplexity is 130.32923174199118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.747718811035156 and perplexity of 115.3209154443481
Finished 3 epochs...
Completing Train Step...
At time: 92.30683588981628 and batch: 50, loss is 4.850618085861206 and perplexity is 127.81936878026457
At time: 93.96076560020447 and batch: 100, loss is 4.774451484680176 and perplexity is 118.4453277223807
At time: 95.56500554084778 and batch: 150, loss is 4.76185733795166 and perplexity is 116.96296403205149
At time: 97.19835066795349 and batch: 200, loss is 4.79513876914978 and perplexity is 120.9211607454933
At time: 98.8027606010437 and batch: 250, loss is 4.8051283645629885 and perplexity is 122.13516784243201
At time: 100.40540313720703 and batch: 300, loss is 4.778809614181519 and perplexity is 118.96265426865445
At time: 102.00654578208923 and batch: 350, loss is 4.734769639968872 and perplexity is 113.83723214920799
At time: 103.60937070846558 and batch: 400, loss is 4.752926139831543 and perplexity is 115.92299562237795
At time: 105.21301507949829 and batch: 450, loss is 4.7583205509185795 and perplexity is 116.5500216126626
At time: 106.81976819038391 and batch: 500, loss is 4.759847660064697 and perplexity is 116.72814218681073
At time: 108.42456483840942 and batch: 550, loss is 4.752029008865357 and perplexity is 115.81904414932681
At time: 110.03106117248535 and batch: 600, loss is 4.791495714187622 and perplexity is 120.4814397609926
At time: 111.64253759384155 and batch: 650, loss is 4.779634094238281 and perplexity is 119.06077704917787
At time: 113.24871158599854 and batch: 700, loss is 4.744250001907349 and perplexity is 114.92158220603794
At time: 114.85585260391235 and batch: 750, loss is 4.7277536201477055 and perplexity is 113.04134312342217
At time: 116.4624547958374 and batch: 800, loss is 4.702743072509765 and perplexity is 110.24917954270764
At time: 118.06917977333069 and batch: 850, loss is 4.706631908416748 and perplexity is 110.67875524416074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.686036745707194 and perplexity of 108.42262074636179
Finished 4 epochs...
Completing Train Step...
At time: 122.33324074745178 and batch: 50, loss is 4.697244052886963 and perplexity is 109.64458101413874
At time: 123.93922972679138 and batch: 100, loss is 4.629712257385254 and perplexity is 102.48457068909867
At time: 125.54589867591858 and batch: 150, loss is 4.619905185699463 and perplexity is 101.48440950006923
At time: 127.15255165100098 and batch: 200, loss is 4.6537245082855225 and perplexity is 104.97523952333326
At time: 128.76147866249084 and batch: 250, loss is 4.663421964645385 and perplexity is 105.99818429208283
At time: 130.3714427947998 and batch: 300, loss is 4.640323810577392 and perplexity is 103.57788176732643
At time: 131.98071312904358 and batch: 350, loss is 4.597380104064942 and perplexity is 99.22401821275672
At time: 133.58807587623596 and batch: 400, loss is 4.621317548751831 and perplexity is 101.6278435971049
At time: 135.19604468345642 and batch: 450, loss is 4.636837577819824 and perplexity is 103.21741386554164
At time: 136.80539083480835 and batch: 500, loss is 4.632674264907837 and perplexity is 102.78858077624191
At time: 138.466965675354 and batch: 550, loss is 4.630395784378051 and perplexity is 102.55464560583017
At time: 140.069917678833 and batch: 600, loss is 4.670113744735718 and perplexity is 106.7098794292122
At time: 141.6819326877594 and batch: 650, loss is 4.654978179931641 and perplexity is 105.10692653355598
At time: 143.28873014450073 and batch: 700, loss is 4.623687744140625 and perplexity is 101.86900713282076
At time: 144.89479780197144 and batch: 750, loss is 4.606063947677613 and perplexity is 100.08941612135176
At time: 146.48963046073914 and batch: 800, loss is 4.576445922851563 and perplexity is 97.16843567278299
At time: 148.0962107181549 and batch: 850, loss is 4.5880195426940915 and perplexity is 98.29955917866583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.64805793762207 and perplexity of 104.3820721104092
Finished 5 epochs...
Completing Train Step...
At time: 152.3754584789276 and batch: 50, loss is 4.580886611938476 and perplexity is 97.60088997155393
At time: 153.98604702949524 and batch: 100, loss is 4.519385175704956 and perplexity is 91.77915257515055
At time: 155.59652662277222 and batch: 150, loss is 4.510394353866577 and perplexity is 90.95768095199684
At time: 157.20562887191772 and batch: 200, loss is 4.544503784179687 and perplexity is 94.11371490642846
At time: 158.81583213806152 and batch: 250, loss is 4.553607997894287 and perplexity is 94.97445853202974
At time: 160.42801523208618 and batch: 300, loss is 4.533660593032837 and perplexity is 93.09873466320664
At time: 162.04066371917725 and batch: 350, loss is 4.494712715148926 and perplexity is 89.54244110284988
At time: 163.65255117416382 and batch: 400, loss is 4.518677759170532 and perplexity is 91.71424944458398
At time: 165.2679374217987 and batch: 450, loss is 4.54433653831482 and perplexity is 94.09797609294547
At time: 166.87909960746765 and batch: 500, loss is 4.5327052402496335 and perplexity is 93.00983499996529
At time: 168.47913122177124 and batch: 550, loss is 4.533881168365479 and perplexity is 93.1192722125286
At time: 170.08366584777832 and batch: 600, loss is 4.576837024688721 and perplexity is 97.20644585893069
At time: 171.69031429290771 and batch: 650, loss is 4.557844200134277 and perplexity is 95.37764292838428
At time: 173.29886317253113 and batch: 700, loss is 4.530252447128296 and perplexity is 92.78198067046134
At time: 174.90901470184326 and batch: 750, loss is 4.512151536941528 and perplexity is 91.11765075645748
At time: 176.52028155326843 and batch: 800, loss is 4.477843265533448 and perplexity is 88.04457898044423
At time: 178.1323266029358 and batch: 850, loss is 4.49685338973999 and perplexity is 89.73432764138288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.627629280090332 and perplexity of 102.2713198307258
Finished 6 epochs...
Completing Train Step...
At time: 182.44239735603333 and batch: 50, loss is 4.48704683303833 and perplexity is 88.85864360959857
At time: 184.04876041412354 and batch: 100, loss is 4.431281690597534 and perplexity is 84.03905998598374
At time: 185.65600728988647 and batch: 150, loss is 4.423948621749878 and perplexity is 83.42504981412017
At time: 187.2643814086914 and batch: 200, loss is 4.457686586380005 and perplexity is 86.28765897195751
At time: 188.872394323349 and batch: 250, loss is 4.464930028915405 and perplexity is 86.91494779381703
At time: 190.48199820518494 and batch: 300, loss is 4.4461595249176025 and perplexity is 85.29872651632967
At time: 192.09141087532043 and batch: 350, loss is 4.406006526947022 and perplexity is 81.94157775582029
At time: 193.70054125785828 and batch: 400, loss is 4.434255380630493 and perplexity is 84.289338041292
At time: 195.30999326705933 and batch: 450, loss is 4.459032135009766 and perplexity is 86.40384136025017
At time: 196.92081999778748 and batch: 500, loss is 4.451891784667969 and perplexity is 85.78908506165759
At time: 198.52990293502808 and batch: 550, loss is 4.452482385635376 and perplexity is 85.83976714325799
At time: 200.14116311073303 and batch: 600, loss is 4.49711196899414 and perplexity is 89.75753407711817
At time: 201.75408387184143 and batch: 650, loss is 4.474120826721191 and perplexity is 87.71744766333694
At time: 203.36260557174683 and batch: 700, loss is 4.451282920837403 and perplexity is 85.73686708913675
At time: 204.97023344039917 and batch: 750, loss is 4.433817195892334 and perplexity is 84.25241183061459
At time: 206.58109068870544 and batch: 800, loss is 4.396534738540649 and perplexity is 81.16910857708814
At time: 208.1887743473053 and batch: 850, loss is 4.419725542068481 and perplexity is 83.07348205316455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.612202008565267 and perplexity of 100.705660389344
Finished 7 epochs...
Completing Train Step...
At time: 212.42085361480713 and batch: 50, loss is 4.408028764724731 and perplexity is 82.10745077073439
At time: 214.052099943161 and batch: 100, loss is 4.354161291122437 and perplexity is 77.80154511852639
At time: 215.6580171585083 and batch: 150, loss is 4.348619737625122 and perplexity is 77.37159608712587
At time: 217.2652986049652 and batch: 200, loss is 4.382702121734619 and perplexity is 80.05405722027996
At time: 218.87125945091248 and batch: 250, loss is 4.388328638076782 and perplexity is 80.50575222451356
At time: 220.50626635551453 and batch: 300, loss is 4.370755519866943 and perplexity is 79.10337329880961
At time: 222.11441016197205 and batch: 350, loss is 4.332741355895996 and perplexity is 76.15276250975678
At time: 223.72204399108887 and batch: 400, loss is 4.362794599533081 and perplexity is 78.47613764502347
At time: 225.3253755569458 and batch: 450, loss is 4.390504980087281 and perplexity is 80.68115106987356
At time: 226.9261257648468 and batch: 500, loss is 4.3779274845123295 and perplexity is 79.67273919092273
At time: 228.52830004692078 and batch: 550, loss is 4.383539266586304 and perplexity is 80.12110212136982
At time: 230.13446831703186 and batch: 600, loss is 4.427088623046875 and perplexity is 83.68741627868891
At time: 231.73815178871155 and batch: 650, loss is 4.403092794418335 and perplexity is 81.70316941300365
At time: 233.34099769592285 and batch: 700, loss is 4.382224044799805 and perplexity is 80.01579436900674
At time: 234.93608474731445 and batch: 750, loss is 4.366776723861694 and perplexity is 78.78926241906922
At time: 236.54113054275513 and batch: 800, loss is 4.3283565235137935 and perplexity is 75.81957642711251
At time: 238.13691473007202 and batch: 850, loss is 4.351883134841919 and perplexity is 77.62450278150737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.605140686035156 and perplexity of 99.99705004821843
Finished 8 epochs...
Completing Train Step...
At time: 242.33253359794617 and batch: 50, loss is 4.339933614730835 and perplexity is 76.70244725667115
At time: 243.9618480205536 and batch: 100, loss is 4.288008871078492 and perplexity is 72.8213273991764
At time: 245.56108951568604 and batch: 150, loss is 4.283345117568969 and perplexity is 72.4824974018909
At time: 247.16665077209473 and batch: 200, loss is 4.319088687896729 and perplexity is 75.12013919724266
At time: 248.76843881607056 and batch: 250, loss is 4.322704515457153 and perplexity is 75.39225232750103
At time: 250.37854981422424 and batch: 300, loss is 4.305829801559448 and perplexity is 74.1307037113366
At time: 251.98767566680908 and batch: 350, loss is 4.2690884876251225 and perplexity is 71.45647246630273
At time: 253.59777545928955 and batch: 400, loss is 4.3001001262664795 and perplexity is 73.70717335422172
At time: 255.20709323883057 and batch: 450, loss is 4.327751541137696 and perplexity is 75.77372079193508
At time: 256.8145432472229 and batch: 500, loss is 4.3137142372131345 and perplexity is 74.71749268488799
At time: 258.4207627773285 and batch: 550, loss is 4.321522550582886 and perplexity is 75.30319397574796
At time: 260.05976915359497 and batch: 600, loss is 4.365718364715576 and perplexity is 78.705919193884
At time: 261.6676573753357 and batch: 650, loss is 4.340593795776368 and perplexity is 76.7531014771343
At time: 263.27511501312256 and batch: 700, loss is 4.321853294372558 and perplexity is 75.32810415871502
At time: 264.8841667175293 and batch: 750, loss is 4.305484037399292 and perplexity is 74.10507640158403
At time: 266.4957916736603 and batch: 800, loss is 4.26644944190979 and perplexity is 71.2681441815862
At time: 268.10554003715515 and batch: 850, loss is 4.290182056427002 and perplexity is 72.97975372347233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.603034655253093 and perplexity of 99.78667478884493
Finished 9 epochs...
Completing Train Step...
At time: 272.3685393333435 and batch: 50, loss is 4.280372819900513 and perplexity is 72.26737770212422
At time: 274.0155849456787 and batch: 100, loss is 4.229154500961304 and perplexity is 68.65915637507734
At time: 275.62947368621826 and batch: 150, loss is 4.2294719028472905 and perplexity is 68.68095237966455
At time: 277.2418591976166 and batch: 200, loss is 4.263922309875488 and perplexity is 71.08826755312491
At time: 278.85495376586914 and batch: 250, loss is 4.263898229598999 and perplexity is 71.08655574859753
At time: 280.4672763347626 and batch: 300, loss is 4.248612518310547 and perplexity is 70.00820981968222
At time: 282.07838201522827 and batch: 350, loss is 4.21190233707428 and perplexity is 67.48479660814463
At time: 283.69351410865784 and batch: 400, loss is 4.242381267547607 and perplexity is 69.5733274465421
At time: 285.3030490875244 and batch: 450, loss is 4.2707373237609865 and perplexity is 71.57438966660224
At time: 286.91309547424316 and batch: 500, loss is 4.2600206279754635 and perplexity is 70.81144413594323
At time: 288.52305269241333 and batch: 550, loss is 4.2657013034820555 and perplexity is 71.21484568406365
At time: 290.1327188014984 and batch: 600, loss is 4.311450929641723 and perplexity is 74.54857524614394
At time: 291.7456429004669 and batch: 650, loss is 4.28465316772461 and perplexity is 72.5773701795472
At time: 293.35793566703796 and batch: 700, loss is 4.267191390991211 and perplexity is 71.32104113669334
At time: 294.96977162361145 and batch: 750, loss is 4.251374158859253 and perplexity is 70.2018145409405
At time: 296.5800623893738 and batch: 800, loss is 4.211196451187134 and perplexity is 67.43717685165637
At time: 298.19916820526123 and batch: 850, loss is 4.237049999237061 and perplexity is 69.20340033718969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.600705782572429 and perplexity of 99.5545547219875
Finished 10 epochs...
Completing Train Step...
At time: 302.4660544395447 and batch: 50, loss is 4.225220546722412 and perplexity is 68.3895849842318
At time: 304.07166504859924 and batch: 100, loss is 4.176359248161316 and perplexity is 65.12830505307886
At time: 305.674697637558 and batch: 150, loss is 4.174946084022522 and perplexity is 65.03633306901813
At time: 307.2816336154938 and batch: 200, loss is 4.210366163253784 and perplexity is 67.38120781588012
At time: 308.88840675354004 and batch: 250, loss is 4.213132972717285 and perplexity is 67.56789692683094
At time: 310.4926369190216 and batch: 300, loss is 4.198178877830506 and perplexity is 66.56499760010715
At time: 312.099169254303 and batch: 350, loss is 4.160547437667847 and perplexity is 64.10660736732956
At time: 313.7068886756897 and batch: 400, loss is 4.191967844963074 and perplexity is 66.15284149472643
At time: 315.3127763271332 and batch: 450, loss is 4.220328617095947 and perplexity is 68.05584492929707
At time: 316.91936349868774 and batch: 500, loss is 4.209163122177124 and perplexity is 67.3001941962097
At time: 318.52621269226074 and batch: 550, loss is 4.21553482055664 and perplexity is 67.73037978551405
At time: 320.1317069530487 and batch: 600, loss is 4.260192966461181 and perplexity is 70.82364872462708
At time: 321.72689294815063 and batch: 650, loss is 4.235286741256714 and perplexity is 69.08148440548494
At time: 323.322381734848 and batch: 700, loss is 4.218097105026245 and perplexity is 67.9041468109648
At time: 324.91918325424194 and batch: 750, loss is 4.203099117279053 and perplexity is 66.89332037817701
At time: 326.52684783935547 and batch: 800, loss is 4.163603258132935 and perplexity is 64.30280527017807
At time: 328.13538885116577 and batch: 850, loss is 4.190168504714966 and perplexity is 66.03391704935484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.605767567952474 and perplexity of 100.0597560432524
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 332.40000891685486 and batch: 50, loss is 4.207258567810059 and perplexity is 67.17213929991307
At time: 333.99830317497253 and batch: 100, loss is 4.17724123954773 and perplexity is 65.18577299653755
At time: 335.5942757129669 and batch: 150, loss is 4.178741641044617 and perplexity is 65.28365123788524
At time: 337.1969792842865 and batch: 200, loss is 4.206946935653686 and perplexity is 67.15120956265383
At time: 338.79613280296326 and batch: 250, loss is 4.19488712310791 and perplexity is 66.34624219699278
At time: 340.38896775245667 and batch: 300, loss is 4.176228952407837 and perplexity is 65.11981966431622
At time: 342.0101089477539 and batch: 350, loss is 4.131334948539734 and perplexity is 62.2609826901008
At time: 343.61026334762573 and batch: 400, loss is 4.141147952079773 and perplexity is 62.87495747512871
At time: 345.2044141292572 and batch: 450, loss is 4.162117700576783 and perplexity is 64.20735067113237
At time: 346.8020589351654 and batch: 500, loss is 4.132924489974975 and perplexity is 62.36002779918371
At time: 348.40656065940857 and batch: 550, loss is 4.120018548965454 and perplexity is 61.560384135276905
At time: 350.0037717819214 and batch: 600, loss is 4.148240489959717 and perplexity is 63.32248567202519
At time: 351.60429859161377 and batch: 650, loss is 4.102918124198913 and perplexity is 60.51662521316833
At time: 353.2030351161957 and batch: 700, loss is 4.07660638332367 and perplexity is 58.94509299050163
At time: 354.80633521080017 and batch: 750, loss is 4.042655162811279 and perplexity is 56.977426484710094
At time: 356.41506791114807 and batch: 800, loss is 3.9781797170639037 and perplexity is 53.41970669199641
At time: 358.040917634964 and batch: 850, loss is 3.992433500289917 and perplexity is 54.18659213663773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.546332995096843 and perplexity of 94.28602629007509
Finished 12 epochs...
Completing Train Step...
At time: 362.28660225868225 and batch: 50, loss is 4.149843225479126 and perplexity is 63.42405624262708
At time: 363.88545870780945 and batch: 100, loss is 4.110813083648682 and perplexity is 60.996292500670634
At time: 365.489951133728 and batch: 150, loss is 4.107920122146607 and perplexity is 60.82008757462219
At time: 367.1011526584625 and batch: 200, loss is 4.140099272727967 and perplexity is 62.80905636608442
At time: 368.7107300758362 and batch: 250, loss is 4.130833415985108 and perplexity is 62.22976460949494
At time: 370.3213539123535 and batch: 300, loss is 4.118961400985718 and perplexity is 61.4953400862066
At time: 371.9287691116333 and batch: 350, loss is 4.074381227493286 and perplexity is 58.81407679294782
At time: 373.53826570510864 and batch: 400, loss is 4.089109892845154 and perplexity is 59.68674045777123
At time: 375.1472818851471 and batch: 450, loss is 4.116425776481629 and perplexity is 61.339608516826665
At time: 376.75788474082947 and batch: 500, loss is 4.092057409286499 and perplexity is 59.86292763634629
At time: 378.3665986061096 and batch: 550, loss is 4.087576479911804 and perplexity is 59.595286174514285
At time: 379.97826313972473 and batch: 600, loss is 4.120856804847717 and perplexity is 61.61200912384487
At time: 381.58976578712463 and batch: 650, loss is 4.082075862884522 and perplexity is 59.26837525878416
At time: 383.24946999549866 and batch: 700, loss is 4.061768174171448 and perplexity is 58.076910438454426
At time: 384.86295914649963 and batch: 750, loss is 4.036407346725464 and perplexity is 56.622551753401204
At time: 386.48006987571716 and batch: 800, loss is 3.980165038108826 and perplexity is 53.525867306487314
At time: 388.08882904052734 and batch: 850, loss is 4.00280647277832 and perplexity is 54.75159347166177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5441592534383135 and perplexity of 94.08129542353323
Finished 13 epochs...
Completing Train Step...
At time: 392.3319549560547 and batch: 50, loss is 4.124792103767395 and perplexity is 61.85494850280201
At time: 393.969277381897 and batch: 100, loss is 4.083903069496155 and perplexity is 59.37676982539422
At time: 395.5808918476105 and batch: 150, loss is 4.079555277824402 and perplexity is 59.119172395817756
At time: 397.1927967071533 and batch: 200, loss is 4.11307116985321 and perplexity is 61.13418301303419
At time: 398.8002028465271 and batch: 250, loss is 4.103503499031067 and perplexity is 60.55206049294362
At time: 400.4109218120575 and batch: 300, loss is 4.09345205783844 and perplexity is 59.94647362679481
At time: 402.02148723602295 and batch: 350, loss is 4.049061036109924 and perplexity is 57.34358820030633
At time: 403.6328363418579 and batch: 400, loss is 4.0660388374328615 and perplexity is 58.32546774069261
At time: 405.2414960861206 and batch: 450, loss is 4.096010789871216 and perplexity is 60.100056994663284
At time: 406.84382033348083 and batch: 500, loss is 4.0725662755966185 and perplexity is 58.70742888241075
At time: 408.44035816192627 and batch: 550, loss is 4.071577715873718 and perplexity is 58.64942175925931
At time: 410.04334712028503 and batch: 600, loss is 4.107020659446716 and perplexity is 60.76540676980347
At time: 411.653489112854 and batch: 650, loss is 4.070988874435425 and perplexity is 58.614896715281404
At time: 413.2474858760834 and batch: 700, loss is 4.05307819366455 and perplexity is 57.5744097416961
At time: 414.84470868110657 and batch: 750, loss is 4.031481566429139 and perplexity is 56.34432730245948
At time: 416.440066576004 and batch: 800, loss is 3.978318462371826 and perplexity is 53.427118939845926
At time: 418.05181789398193 and batch: 850, loss is 4.004203820228577 and perplexity is 54.828153949533636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.544159889221191 and perplexity of 94.08135523882902
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 422.25958228111267 and batch: 50, loss is 4.122165975570678 and perplexity is 61.69272258469448
At time: 423.9077236652374 and batch: 100, loss is 4.097992467880249 and perplexity is 60.219274041837316
At time: 425.50960302352905 and batch: 150, loss is 4.09664101600647 and perplexity is 60.13794555923931
At time: 427.1186227798462 and batch: 200, loss is 4.13328405380249 and perplexity is 62.382454241090144
At time: 428.7236964702606 and batch: 250, loss is 4.120591034889221 and perplexity is 61.59563667848562
At time: 430.330908536911 and batch: 300, loss is 4.102720518112182 and perplexity is 60.50466794113157
At time: 431.9376540184021 and batch: 350, loss is 4.059032597541809 and perplexity is 57.91825370730101
At time: 433.5437505245209 and batch: 400, loss is 4.07297610282898 and perplexity is 58.731493716384
At time: 435.14540815353394 and batch: 450, loss is 4.108201751708984 and perplexity is 60.83721872147506
At time: 436.75020146369934 and batch: 500, loss is 4.081748328208923 and perplexity is 59.2489659894984
At time: 438.35434460639954 and batch: 550, loss is 4.071476192474365 and perplexity is 58.6434677728318
At time: 439.9592487812042 and batch: 600, loss is 4.096078271865845 and perplexity is 60.10411280323207
At time: 441.5612931251526 and batch: 650, loss is 4.047519264221191 and perplexity is 57.25524558759809
At time: 443.1665666103363 and batch: 700, loss is 4.018184871673584 and perplexity is 55.60009287526539
At time: 444.7707769870758 and batch: 750, loss is 3.992491068840027 and perplexity is 54.18971166997506
At time: 446.37563467025757 and batch: 800, loss is 3.932971477508545 and perplexity is 51.05847157727353
At time: 447.97194385528564 and batch: 850, loss is 3.9608217525482177 and perplexity is 52.50045060684234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.530416170756022 and perplexity of 92.79717251652248
Finished 15 epochs...
Completing Train Step...
At time: 452.1565029621124 and batch: 50, loss is 4.1148273229599 and perplexity is 61.241638324830596
At time: 453.77987003326416 and batch: 100, loss is 4.0809783935546875 and perplexity is 59.20336571422419
At time: 455.37680411338806 and batch: 150, loss is 4.077284955978394 and perplexity is 58.98510509275905
At time: 456.9851567745209 and batch: 200, loss is 4.112532534599304 and perplexity is 61.10126285361976
At time: 458.59256958961487 and batch: 250, loss is 4.099155917167663 and perplexity is 60.28937688595423
At time: 460.20497155189514 and batch: 300, loss is 4.083367094993592 and perplexity is 59.34495391774252
At time: 461.81365990638733 and batch: 350, loss is 4.04062126159668 and perplexity is 56.86165779872252
At time: 463.4141173362732 and batch: 400, loss is 4.055705943107605 and perplexity is 57.72589981663832
At time: 465.0680675506592 and batch: 450, loss is 4.092515115737915 and perplexity is 59.890333555980725
At time: 466.6664834022522 and batch: 500, loss is 4.067833180427551 and perplexity is 58.43021758561538
At time: 468.27293610572815 and batch: 550, loss is 4.060097608566284 and perplexity is 57.97997014452717
At time: 469.87566089630127 and batch: 600, loss is 4.0875919151306155 and perplexity is 59.59620604789573
At time: 471.4819526672363 and batch: 650, loss is 4.043323945999146 and perplexity is 57.01554477464594
At time: 473.0872235298157 and batch: 700, loss is 4.018403429985046 and perplexity is 55.61224606572336
At time: 474.69246530532837 and batch: 750, loss is 3.9965200328826906 and perplexity is 54.40848047967844
At time: 476.2965290546417 and batch: 800, loss is 3.940490975379944 and perplexity is 51.443852766253364
At time: 477.90310525894165 and batch: 850, loss is 3.9706032180786135 and perplexity is 53.01650170829904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5292409261067705 and perplexity of 92.6881771966853
Finished 16 epochs...
Completing Train Step...
At time: 482.16296648979187 and batch: 50, loss is 4.110946154594421 and perplexity is 61.00440987508164
At time: 483.76048612594604 and batch: 100, loss is 4.074199228286743 and perplexity is 58.80337365164911
At time: 485.36608028411865 and batch: 150, loss is 4.068651647567749 and perplexity is 58.47806037490848
At time: 486.98184084892273 and batch: 200, loss is 4.103547701835632 and perplexity is 60.55473712299644
At time: 488.5935616493225 and batch: 250, loss is 4.090323557853699 and perplexity is 59.75922414269771
At time: 490.2047770023346 and batch: 300, loss is 4.075135636329651 and perplexity is 58.858463392899495
At time: 491.81584453582764 and batch: 350, loss is 4.032645516395569 and perplexity is 56.40994746221467
At time: 493.42551493644714 and batch: 400, loss is 4.048435502052307 and perplexity is 57.30772904963037
At time: 495.03496837615967 and batch: 450, loss is 4.085930118560791 and perplexity is 59.497251521029895
At time: 496.64445304870605 and batch: 500, loss is 4.062068810462952 and perplexity is 58.09437309025257
At time: 498.2529411315918 and batch: 550, loss is 4.055627875328064 and perplexity is 57.72139345972054
At time: 499.8616592884064 and batch: 600, loss is 4.0844854068756105 and perplexity is 59.411357207720236
At time: 501.4724876880646 and batch: 650, loss is 4.041893134117126 and perplexity is 56.93402458965472
At time: 503.0826618671417 and batch: 700, loss is 4.018596949577332 and perplexity is 55.623009166309934
At time: 504.7434163093567 and batch: 750, loss is 3.9983698654174806 and perplexity is 54.50922020412154
At time: 506.355019569397 and batch: 800, loss is 3.943876528739929 and perplexity is 51.61831383182115
At time: 507.96639108657837 and batch: 850, loss is 3.9749157571792604 and perplexity is 53.24563115463158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528978029886882 and perplexity of 92.66381302803616
Finished 17 epochs...
Completing Train Step...
At time: 512.2191803455353 and batch: 50, loss is 4.10667426109314 and perplexity is 60.74436137819895
At time: 513.8270134925842 and batch: 100, loss is 4.06884135723114 and perplexity is 58.489155280430076
At time: 515.4346153736115 and batch: 150, loss is 4.062531094551087 and perplexity is 58.121235403073875
At time: 517.0430471897125 and batch: 200, loss is 4.0974230337142945 and perplexity is 60.184992891104486
At time: 518.6500799655914 and batch: 250, loss is 4.0843721294403075 and perplexity is 59.40462762271014
At time: 520.257319688797 and batch: 300, loss is 4.069473810195923 and perplexity is 58.52615862029743
At time: 521.8640072345734 and batch: 350, loss is 4.027253689765931 and perplexity is 56.10661330278268
At time: 523.4705662727356 and batch: 400, loss is 4.0435269165039065 and perplexity is 57.02711842306296
At time: 525.0773048400879 and batch: 450, loss is 4.081535167694092 and perplexity is 59.23633779536886
At time: 526.6836404800415 and batch: 500, loss is 4.058298921585083 and perplexity is 57.87577606141953
At time: 528.2888333797455 and batch: 550, loss is 4.052727837562561 and perplexity is 57.55424172912407
At time: 529.8955681324005 and batch: 600, loss is 4.0822404813766475 and perplexity is 59.278132732458275
At time: 531.5041837692261 and batch: 650, loss is 4.040634226799011 and perplexity is 56.862395026399916
At time: 533.111976146698 and batch: 700, loss is 4.018314790725708 and perplexity is 55.60731685588597
At time: 534.7191255092621 and batch: 750, loss is 3.9991272497177124 and perplexity is 54.55052026975842
At time: 536.3261182308197 and batch: 800, loss is 3.9455486965179443 and perplexity is 51.704700519354724
At time: 537.935320854187 and batch: 850, loss is 3.977206492424011 and perplexity is 53.36774260765311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528971354166667 and perplexity of 92.66319443241113
Finished 18 epochs...
Completing Train Step...
At time: 542.178076505661 and batch: 50, loss is 4.102614774703979 and perplexity is 60.49827030959093
At time: 543.7738473415375 and batch: 100, loss is 4.064258227348327 and perplexity is 58.221705232348285
At time: 545.4082272052765 and batch: 150, loss is 4.05758505821228 and perplexity is 57.83447540798253
At time: 547.0149536132812 and batch: 200, loss is 4.092552871704101 and perplexity is 59.89259481607712
At time: 548.6225295066833 and batch: 250, loss is 4.079635429382324 and perplexity is 59.12391107949226
At time: 550.2312867641449 and batch: 300, loss is 4.064992847442627 and perplexity is 58.26449178094135
At time: 551.8384108543396 and batch: 350, loss is 4.022981963157654 and perplexity is 55.86745236857698
At time: 553.4454319477081 and batch: 400, loss is 4.039603748321533 and perplexity is 56.803829732467285
At time: 555.0535416603088 and batch: 450, loss is 4.077998423576355 and perplexity is 59.027204070347466
At time: 556.6613836288452 and batch: 500, loss is 4.055254344940185 and perplexity is 57.699836791518614
At time: 558.2692947387695 and batch: 550, loss is 4.050221662521363 and perplexity is 57.410181320690064
At time: 559.8759713172913 and batch: 600, loss is 4.080252366065979 and perplexity is 59.16039804303422
At time: 561.4729166030884 and batch: 650, loss is 4.0394023990631105 and perplexity is 56.79239347485471
At time: 563.0679228305817 and batch: 700, loss is 4.017722015380859 and perplexity is 55.57436397725252
At time: 564.6757473945618 and batch: 750, loss is 3.999309558868408 and perplexity is 54.56046623537154
At time: 566.2870354652405 and batch: 800, loss is 3.946376333236694 and perplexity is 51.74751094133139
At time: 567.8947088718414 and batch: 850, loss is 3.9784752702713013 and perplexity is 53.4354973910284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.52907117207845 and perplexity of 92.67244434062412
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 572.121164560318 and batch: 50, loss is 4.102921838760376 and perplexity is 60.51685000630972
At time: 573.7578542232513 and batch: 100, loss is 4.069493708610534 and perplexity is 58.52732320965396
At time: 575.3652427196503 and batch: 150, loss is 4.064791188240052 and perplexity is 58.252743394615244
At time: 576.9752757549286 and batch: 200, loss is 4.1006740283966066 and perplexity is 60.38097237446923
At time: 578.5840060710907 and batch: 250, loss is 4.085557656288147 and perplexity is 59.47509516597164
At time: 580.1929049491882 and batch: 300, loss is 4.069012508392334 and perplexity is 58.49916662397571
At time: 581.8019936084747 and batch: 350, loss is 4.025212936401367 and perplexity is 55.99223029636068
At time: 583.4124600887299 and batch: 400, loss is 4.038452425003052 and perplexity is 56.73846779230019
At time: 585.0211431980133 and batch: 450, loss is 4.077376527786255 and perplexity is 58.99050671278354
At time: 586.6578221321106 and batch: 500, loss is 4.054739880561828 and perplexity is 57.670159915354745
At time: 588.2664403915405 and batch: 550, loss is 4.049764866828919 and perplexity is 57.383962585918795
At time: 589.8759582042694 and batch: 600, loss is 4.077584133148194 and perplexity is 59.0027547296136
At time: 591.487667798996 and batch: 650, loss is 4.034364476203918 and perplexity is 56.50699728316706
At time: 593.0992228984833 and batch: 700, loss is 4.0078467798233035 and perplexity is 55.028254957671415
At time: 594.711799621582 and batch: 750, loss is 3.986459240913391 and perplexity is 53.863832465467574
At time: 596.3248391151428 and batch: 800, loss is 3.9313756084442137 and perplexity is 50.97705392524693
At time: 597.9434833526611 and batch: 850, loss is 3.964869794845581 and perplexity is 52.71340538570447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528631846110026 and perplexity of 92.63173987118789
Finished 20 epochs...
Completing Train Step...
At time: 602.1796689033508 and batch: 50, loss is 4.1013659000396725 and perplexity is 60.42276271214363
At time: 603.8164167404175 and batch: 100, loss is 4.06580246925354 and perplexity is 58.31168308526578
At time: 605.4249200820923 and batch: 150, loss is 4.0612193298339845 and perplexity is 58.04504400067981
At time: 607.0349426269531 and batch: 200, loss is 4.097121243476868 and perplexity is 60.1668323882794
At time: 608.6466512680054 and batch: 250, loss is 4.0819199705123905 and perplexity is 59.2591364913181
At time: 610.2549870014191 and batch: 300, loss is 4.065823578834534 and perplexity is 58.31291403345514
At time: 611.864636182785 and batch: 350, loss is 4.022157692909241 and perplexity is 55.82142146329401
At time: 613.4741156101227 and batch: 400, loss is 4.0354816198349 and perplexity is 56.57015898906771
At time: 615.0820519924164 and batch: 450, loss is 4.0750417137146 and perplexity is 58.85293551170036
At time: 616.6902854442596 and batch: 500, loss is 4.0529669618606565 and perplexity is 57.56800599239931
At time: 618.300089597702 and batch: 550, loss is 4.048407354354858 and perplexity is 57.30611599171366
At time: 619.9119820594788 and batch: 600, loss is 4.076731119155884 and perplexity is 58.952446014313985
At time: 621.5167300701141 and batch: 650, loss is 4.034054169654846 and perplexity is 56.48946551209406
At time: 623.1139142513275 and batch: 700, loss is 4.008389940261841 and perplexity is 55.05815224754368
At time: 624.7234058380127 and batch: 750, loss is 3.987714800834656 and perplexity is 53.93150420879132
At time: 626.3333637714386 and batch: 800, loss is 3.9334101390838625 and perplexity is 51.0808738800044
At time: 627.9696810245514 and batch: 850, loss is 3.9673698282241823 and perplexity is 52.84535552982491
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.52852217356364 and perplexity of 92.62158126947003
Finished 21 epochs...
Completing Train Step...
At time: 632.1749756336212 and batch: 50, loss is 4.1004527759552 and perplexity is 60.367614414711994
At time: 633.8082158565521 and batch: 100, loss is 4.0639897012710575 and perplexity is 58.206073285116936
At time: 635.4138433933258 and batch: 150, loss is 4.059170217514038 and perplexity is 57.926224964257244
At time: 637.0199663639069 and batch: 200, loss is 4.0950083875656125 and perplexity is 60.03984274345903
At time: 638.6177303791046 and batch: 250, loss is 4.079751687049866 and perplexity is 59.13078508706061
At time: 640.2151122093201 and batch: 300, loss is 4.06381067276001 and perplexity is 58.19565367121457
At time: 641.821617603302 and batch: 350, loss is 4.020284805297852 and perplexity is 55.71697205611712
At time: 643.4310626983643 and batch: 400, loss is 4.033731689453125 and perplexity is 56.47125171481297
At time: 645.047705411911 and batch: 450, loss is 4.0736164331436155 and perplexity is 58.769113315433884
At time: 646.6584928035736 and batch: 500, loss is 4.05191143989563 and perplexity is 57.507273755335255
At time: 648.272709608078 and batch: 550, loss is 4.047738738059998 and perplexity is 57.267812995197175
At time: 649.8813080787659 and batch: 600, loss is 4.076386423110962 and perplexity is 58.932128841160136
At time: 651.4917833805084 and batch: 650, loss is 4.034117212295532 and perplexity is 56.49302686942841
At time: 653.1015272140503 and batch: 700, loss is 4.008927178382874 and perplexity is 55.08773953280182
At time: 654.7094435691833 and batch: 750, loss is 3.988656826019287 and perplexity is 53.98233298123407
At time: 656.3208754062653 and batch: 800, loss is 3.934773144721985 and perplexity is 51.15054486929253
At time: 657.9268274307251 and batch: 850, loss is 3.968897566795349 and perplexity is 52.92615111932943
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5284684499104815 and perplexity of 92.61660543342418
Finished 22 epochs...
Completing Train Step...
At time: 662.1543350219727 and batch: 50, loss is 4.099614682197571 and perplexity is 60.317041889127246
At time: 663.7481701374054 and batch: 100, loss is 4.062599539756775 and perplexity is 58.12521365913061
At time: 665.3482897281647 and batch: 150, loss is 4.057586255073548 and perplexity is 57.834544627867515
At time: 666.9548320770264 and batch: 200, loss is 4.093390092849732 and perplexity is 59.94275915931809
At time: 668.6113197803497 and batch: 250, loss is 4.078135066032409 and perplexity is 59.03527024356395
At time: 670.2187602519989 and batch: 300, loss is 4.0622931528091435 and perplexity is 58.10740758025153
At time: 671.826623916626 and batch: 350, loss is 4.0188977241516115 and perplexity is 55.63974166944169
At time: 673.4344964027405 and batch: 400, loss is 4.032477927207947 and perplexity is 56.400494557070175
At time: 675.0420145988464 and batch: 450, loss is 4.072580332756043 and perplexity is 58.70825414789836
At time: 676.649082660675 and batch: 500, loss is 4.05111409664154 and perplexity is 57.46143899399255
At time: 678.2567443847656 and batch: 550, loss is 4.047235655784607 and perplexity is 57.239009819319996
At time: 679.8640937805176 and batch: 600, loss is 4.07614800453186 and perplexity is 58.918080001557136
At time: 681.4714710712433 and batch: 650, loss is 4.034196243286133 and perplexity is 56.497491745733406
At time: 683.0791001319885 and batch: 700, loss is 4.009321851730347 and perplexity is 55.109485486359816
At time: 684.6850669384003 and batch: 750, loss is 3.9893365001678465 and perplexity is 54.01903584902263
At time: 686.2983825206757 and batch: 800, loss is 3.935729956626892 and perplexity is 51.19950974091644
At time: 687.9070270061493 and batch: 850, loss is 3.969930200576782 and perplexity is 52.980832679050316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528450647989909 and perplexity of 92.61495669464598
Finished 23 epochs...
Completing Train Step...
At time: 692.1977429389954 and batch: 50, loss is 4.09881000995636 and perplexity is 60.268525962174735
At time: 693.8099267482758 and batch: 100, loss is 4.061417484283448 and perplexity is 58.05654702406782
At time: 695.4206728935242 and batch: 150, loss is 4.0562519311904905 and perplexity is 57.75742607570975
At time: 697.0310349464417 and batch: 200, loss is 4.092045078277588 and perplexity is 59.8621894706033
At time: 698.6409122943878 and batch: 250, loss is 4.076816110610962 and perplexity is 58.957456681410086
At time: 700.2524960041046 and batch: 300, loss is 4.061051850318909 and perplexity is 58.03532345887655
At time: 701.8655438423157 and batch: 350, loss is 4.017773175239563 and perplexity is 55.57720722659066
At time: 703.4768989086151 and batch: 400, loss is 4.031479172706604 and perplexity is 56.344192429934935
At time: 705.0883088111877 and batch: 450, loss is 4.0717433834075925 and perplexity is 58.65913886920792
At time: 706.7011616230011 and batch: 500, loss is 4.050448894500732 and perplexity is 57.42322823210915
At time: 708.3702194690704 and batch: 550, loss is 4.046819972991943 and perplexity is 57.21522149241062
At time: 709.9812216758728 and batch: 600, loss is 4.075938272476196 and perplexity is 58.9057242872626
At time: 711.5928213596344 and batch: 650, loss is 4.034236059188843 and perplexity is 56.499741289151615
At time: 713.2032868862152 and batch: 700, loss is 4.009596018791199 and perplexity is 55.124596763433246
At time: 714.8126440048218 and batch: 750, loss is 3.9898328590393066 and perplexity is 54.04585533218758
At time: 716.4194467067719 and batch: 800, loss is 3.9364339971542357 and perplexity is 51.23556896284151
At time: 718.0210289955139 and batch: 850, loss is 3.9706805086135866 and perplexity is 53.02059954043825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528451919555664 and perplexity of 92.61507446072818
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 722.304416179657 and batch: 50, loss is 4.098799014091492 and perplexity is 60.26786326125098
At time: 723.9123978614807 and batch: 100, loss is 4.061999568939209 and perplexity is 58.09035068659919
At time: 725.5240466594696 and batch: 150, loss is 4.057407975196838 and perplexity is 57.82423481142543
At time: 727.1305539608002 and batch: 200, loss is 4.093361072540283 and perplexity is 59.94101962713908
At time: 728.7338750362396 and batch: 250, loss is 4.0777474212646485 and perplexity is 59.01238996493735
At time: 730.3382387161255 and batch: 300, loss is 4.061791024208069 and perplexity is 58.078237513146576
At time: 731.9421892166138 and batch: 350, loss is 4.018012509346009 and perplexity is 55.5905103397018
At time: 733.5455620288849 and batch: 400, loss is 4.030315685272217 and perplexity is 56.27867479190726
At time: 735.1490931510925 and batch: 450, loss is 4.070512027740478 and perplexity is 58.58695305845776
At time: 736.7522804737091 and batch: 500, loss is 4.049060091972351 and perplexity is 57.34353406009572
At time: 738.3557884693146 and batch: 550, loss is 4.045332407951355 and perplexity is 57.13017340209516
At time: 739.9596431255341 and batch: 600, loss is 4.074057579040527 and perplexity is 58.795044787998954
At time: 741.5645401477814 and batch: 650, loss is 4.0322243309021 and perplexity is 56.38619341343779
At time: 743.1689038276672 and batch: 700, loss is 4.007165503501892 and perplexity is 54.99077827799693
At time: 744.7729125022888 and batch: 750, loss is 3.987122678756714 and perplexity is 53.89957962701027
At time: 746.378075838089 and batch: 800, loss is 3.9333567333221438 and perplexity is 51.07814593987008
At time: 747.970899105072 and batch: 850, loss is 3.9675407123565676 and perplexity is 52.854386734178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.52833875020345 and perplexity of 92.60459386579848
Finished 25 epochs...
Completing Train Step...
At time: 752.1791496276855 and batch: 50, loss is 4.098554697036743 and perplexity is 60.25314059297568
At time: 753.7969071865082 and batch: 100, loss is 4.061640377044678 and perplexity is 58.06948885040763
At time: 755.3948159217834 and batch: 150, loss is 4.056952781677246 and perplexity is 57.797919584178736
At time: 756.9971644878387 and batch: 200, loss is 4.092951617240906 and perplexity is 59.91648148298208
At time: 758.5945129394531 and batch: 250, loss is 4.077387776374817 and perplexity is 58.99117027645468
At time: 760.1898050308228 and batch: 300, loss is 4.061461482048035 and perplexity is 58.059101438550385
At time: 761.7861716747284 and batch: 350, loss is 4.017735266685486 and perplexity is 55.5751004149584
At time: 763.386830329895 and batch: 400, loss is 4.0301023912429805 and perplexity is 56.26667216669047
At time: 764.9770941734314 and batch: 450, loss is 4.070320091247559 and perplexity is 58.57570916324829
At time: 766.5687716007233 and batch: 500, loss is 4.048974170684814 and perplexity is 57.33860724147967
At time: 768.1651909351349 and batch: 550, loss is 4.0452155494689945 and perplexity is 57.12349764680133
At time: 769.7599756717682 and batch: 600, loss is 4.074098081588745 and perplexity is 58.79742618536143
At time: 771.3465993404388 and batch: 650, loss is 4.032318277359009 and perplexity is 56.391490945365845
At time: 772.9530491828918 and batch: 700, loss is 4.0073473834991455 and perplexity is 55.00078091021088
At time: 774.5598402023315 and batch: 750, loss is 3.9873714733123777 and perplexity is 53.91299121727018
At time: 776.1686291694641 and batch: 800, loss is 3.933648343086243 and perplexity is 51.09304299791645
At time: 777.7768650054932 and batch: 850, loss is 3.9678083944320677 and perplexity is 52.86853679989328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528311093648274 and perplexity of 92.6020327771543
Finished 26 epochs...
Completing Train Step...
At time: 782.0142986774445 and batch: 50, loss is 4.0983502435684205 and perplexity is 60.24082288864572
At time: 783.6553883552551 and batch: 100, loss is 4.061375503540039 and perplexity is 58.05410981822222
At time: 785.2665379047394 and batch: 150, loss is 4.056614770889282 and perplexity is 57.77838656520892
At time: 786.8779454231262 and batch: 200, loss is 4.092599925994873 and perplexity is 59.89541308595397
At time: 788.4919385910034 and batch: 250, loss is 4.07707832813263 and perplexity is 58.972918386661334
At time: 790.1276640892029 and batch: 300, loss is 4.061170163154602 and perplexity is 58.042190188768515
At time: 791.7388525009155 and batch: 350, loss is 4.017492442131043 and perplexity is 55.56160705428793
At time: 793.3476293087006 and batch: 400, loss is 4.029915018081665 and perplexity is 56.256130290113106
At time: 794.9627270698547 and batch: 450, loss is 4.070170903205872 and perplexity is 58.56697101973729
At time: 796.5796027183533 and batch: 500, loss is 4.04891598701477 and perplexity is 57.33527116792861
At time: 798.1905324459076 and batch: 550, loss is 4.0452096652984615 and perplexity is 57.123161523388646
At time: 799.8077454566956 and batch: 600, loss is 4.074126024246215 and perplexity is 58.799069164655975
At time: 801.4212212562561 and batch: 650, loss is 4.032397131919861 and perplexity is 56.39593784694709
At time: 803.0327069759369 and batch: 700, loss is 4.007496061325074 and perplexity is 55.008958914669854
At time: 804.648323059082 and batch: 750, loss is 3.9875723457336427 and perplexity is 53.923821938113775
At time: 806.2624900341034 and batch: 800, loss is 3.9338917541503906 and perplexity is 51.10548112361042
At time: 807.8794298171997 and batch: 850, loss is 3.9680436563491823 and perplexity is 52.88097621641898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5283158620198565 and perplexity of 92.60247433910861
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 812.1607091426849 and batch: 50, loss is 4.098290982246399 and perplexity is 60.2372530436196
At time: 813.7683570384979 and batch: 100, loss is 4.061397104263306 and perplexity is 58.05536384252679
At time: 815.3781404495239 and batch: 150, loss is 4.0567147827148435 and perplexity is 57.78416537609721
At time: 816.9859447479248 and batch: 200, loss is 4.0927044916152955 and perplexity is 59.90167641444249
At time: 818.5930223464966 and batch: 250, loss is 4.07719729423523 and perplexity is 58.9799345822572
At time: 820.2009541988373 and batch: 300, loss is 4.061266894340515 and perplexity is 58.04780495021534
At time: 821.8099579811096 and batch: 350, loss is 4.017563085556031 and perplexity is 55.56553225515127
At time: 823.4167020320892 and batch: 400, loss is 4.029691357612609 and perplexity is 56.243549424598754
At time: 825.0251290798187 and batch: 450, loss is 4.069932317733764 and perplexity is 58.55299945807872
At time: 826.6333713531494 and batch: 500, loss is 4.048721551895142 and perplexity is 57.3241242613303
At time: 828.2424311637878 and batch: 550, loss is 4.0449464321136475 and perplexity is 57.108126790561315
At time: 829.8518280982971 and batch: 600, loss is 4.073774743080139 and perplexity is 58.77841778650807
At time: 831.5108997821808 and batch: 650, loss is 4.032016096115112 and perplexity is 56.37445306887978
At time: 833.1199419498444 and batch: 700, loss is 4.007031183242798 and perplexity is 54.983392398458584
At time: 834.7292168140411 and batch: 750, loss is 3.9870937395095827 and perplexity is 53.89801983632487
At time: 836.3422205448151 and batch: 800, loss is 3.933312454223633 and perplexity is 51.075884295686414
At time: 837.9434947967529 and batch: 850, loss is 3.9675126123428344 and perplexity is 52.85290154605191
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528318723042806 and perplexity of 92.60273927729185
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 842.2120616436005 and batch: 50, loss is 4.098279962539673 and perplexity is 60.236589250414454
At time: 843.8213889598846 and batch: 100, loss is 4.061386561393737 and perplexity is 58.05475177562452
At time: 845.4311530590057 and batch: 150, loss is 4.0567082500457765 and perplexity is 57.78378789250048
At time: 847.0420925617218 and batch: 200, loss is 4.092720255851746 and perplexity is 59.90262072607642
At time: 848.643707036972 and batch: 250, loss is 4.077204828262329 and perplexity is 58.980378940356545
At time: 850.2536735534668 and batch: 300, loss is 4.061278128623963 and perplexity is 58.04845707937275
At time: 851.8592128753662 and batch: 350, loss is 4.017572193145752 and perplexity is 55.56603832552624
At time: 853.4657974243164 and batch: 400, loss is 4.029654850959778 and perplexity is 56.24149619834434
At time: 855.0737552642822 and batch: 450, loss is 4.069885659217834 and perplexity is 58.5502675257552
At time: 856.6822698116302 and batch: 500, loss is 4.048671536445617 and perplexity is 57.32125724118504
At time: 858.288736820221 and batch: 550, loss is 4.044897813796997 and perplexity is 57.10535035706298
At time: 859.8973231315613 and batch: 600, loss is 4.073718385696411 and perplexity is 58.77510528200489
At time: 861.5055432319641 and batch: 650, loss is 4.03195430278778 and perplexity is 56.37096961147648
At time: 863.1131558418274 and batch: 700, loss is 4.006956667900085 and perplexity is 54.97929544477533
At time: 864.7195677757263 and batch: 750, loss is 3.9870157480239867 and perplexity is 53.89381641360488
At time: 866.3287837505341 and batch: 800, loss is 3.9332168245315553 and perplexity is 51.071000158136634
At time: 867.9365522861481 and batch: 850, loss is 3.9674243450164797 and perplexity is 52.848236567627964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.528319358825684 and perplexity of 92.60279815254664
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 872.1932368278503 and batch: 50, loss is 4.098278841972351 and perplexity is 60.236521751298795
At time: 873.8265237808228 and batch: 100, loss is 4.061385912895203 and perplexity is 58.054714127215256
At time: 875.4274842739105 and batch: 150, loss is 4.056708283424378 and perplexity is 57.78378982124252
At time: 877.0251262187958 and batch: 200, loss is 4.0927238941192625 and perplexity is 59.902838668232036
At time: 878.6330161094666 and batch: 250, loss is 4.0772069597244265 and perplexity is 58.98050465493273
At time: 880.2422981262207 and batch: 300, loss is 4.061281099319458 and perplexity is 58.04862952391882
At time: 881.851381778717 and batch: 350, loss is 4.017574496269226 and perplexity is 55.566166301120845
At time: 883.459879398346 and batch: 400, loss is 4.029649391174316 and perplexity is 56.2411891326793
At time: 885.0677285194397 and batch: 450, loss is 4.069878129959107 and perplexity is 58.54982668730203
At time: 886.6785020828247 and batch: 500, loss is 4.048663034439087 and perplexity is 57.32076989755332
At time: 888.2863762378693 and batch: 550, loss is 4.044889316558838 and perplexity is 57.104865121362465
At time: 889.8968167304993 and batch: 600, loss is 4.073709044456482 and perplexity is 58.774556252208896
At time: 891.5039663314819 and batch: 650, loss is 4.03194375038147 and perplexity is 56.37037476523956
At time: 893.1128723621368 and batch: 700, loss is 4.0069437599182125 and perplexity is 54.978585777606554
At time: 894.7193903923035 and batch: 750, loss is 3.987001910209656 and perplexity is 53.89307064613968
At time: 896.3329038619995 and batch: 800, loss is 3.933199853897095 and perplexity is 51.070133458215665
At time: 897.9402091503143 and batch: 850, loss is 3.9674089336395264 and perplexity is 52.84742210980888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5283199946085615 and perplexity of 92.60285702783887
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 902.2189249992371 and batch: 50, loss is 4.098278999328613 and perplexity is 60.23653122989345
At time: 903.8650226593018 and batch: 100, loss is 4.061385760307312 and perplexity is 58.05470526876956
At time: 905.4756510257721 and batch: 150, loss is 4.056708178520203 and perplexity is 57.783783759482034
At time: 907.0885796546936 and batch: 200, loss is 4.092724289894104 and perplexity is 59.9028623762732
At time: 908.7021563053131 and batch: 250, loss is 4.077207288742065 and perplexity is 58.98052406056231
At time: 910.3133335113525 and batch: 300, loss is 4.06128158569336 and perplexity is 58.04865775726414
At time: 911.928130865097 and batch: 350, loss is 4.017575030326843 and perplexity is 55.56619597666312
At time: 913.5397980213165 and batch: 400, loss is 4.029648885726929 and perplexity is 56.24116070572439
At time: 915.1796796321869 and batch: 450, loss is 4.069877486228943 and perplexity is 58.54978899702466
At time: 916.7914819717407 and batch: 500, loss is 4.048662185668945 and perplexity is 57.320721245416
At time: 918.4035074710846 and batch: 550, loss is 4.044888663291931 and perplexity is 57.10482781665602
At time: 920.0153226852417 and batch: 600, loss is 4.073708114624023 and perplexity is 58.774501601744134
At time: 921.6214869022369 and batch: 650, loss is 4.031942644119263 and perplexity is 56.37031240485884
At time: 923.229475736618 and batch: 700, loss is 4.006942572593689 and perplexity is 54.97852050022217
At time: 924.8382437229156 and batch: 750, loss is 3.987000527381897 and perplexity is 53.8929961213571
At time: 926.4441225528717 and batch: 800, loss is 3.9331982946395874 and perplexity is 51.070053826788744
At time: 928.0555922985077 and batch: 850, loss is 3.9674074602127076 and perplexity is 52.84734424305721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5283199946085615 and perplexity of 92.60285702783887
Annealing...
Model not improving. Stopping early with 92.6020327771543loss at 30 epochs.
Finished Training.
Improved accuracyfrom -114.21426593274742 to -92.6020327771543
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f079a7ddba8>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 1.3304114881382434, 'anneal': 5.461146488664952, 'dropout': 0.20786476639081441, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2021431922912598 and batch: 50, loss is 7.924656391143799 and perplexity is 2764.6142463344045
At time: 3.819782018661499 and batch: 100, loss is 6.460144958496094 and perplexity is 639.1537006138336
At time: 5.438023090362549 and batch: 150, loss is 6.24817081451416 and perplexity is 517.0661492144382
At time: 7.056656122207642 and batch: 200, loss is 6.218847885131836 and perplexity is 502.1243936605221
At time: 8.672796249389648 and batch: 250, loss is 6.177898817062378 and perplexity is 481.97816734615714
At time: 10.289670705795288 and batch: 300, loss is 6.049819717407226 and perplexity is 424.0365767398858
At time: 11.905908584594727 and batch: 350, loss is 5.994677839279174 and perplexity is 401.28738413143935
At time: 13.5249342918396 and batch: 400, loss is 5.9773460865020756 and perplexity is 394.39229498756964
At time: 15.142670392990112 and batch: 450, loss is 5.9505859470367435 and perplexity is 383.97826408361493
At time: 16.787765741348267 and batch: 500, loss is 5.924691009521484 and perplexity is 374.16280475273015
At time: 18.40486216545105 and batch: 550, loss is 5.844616584777832 and perplexity is 345.370096239705
At time: 20.02287793159485 and batch: 600, loss is 5.84989028930664 and perplexity is 347.19628724374974
At time: 21.640844106674194 and batch: 650, loss is 5.851591939926148 and perplexity is 347.78759697963415
At time: 23.260061025619507 and batch: 700, loss is 5.781854629516602 and perplexity is 324.3602009217001
At time: 24.878963947296143 and batch: 750, loss is 5.756780185699463 and perplexity is 316.3281694734831
At time: 26.499929666519165 and batch: 800, loss is 5.766229867935181 and perplexity is 319.33153824833465
At time: 28.119619131088257 and batch: 850, loss is 5.754516124725342 and perplexity is 315.6127933431354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.3029327392578125 and perplexity of 200.92520798943582
Finished 1 epochs...
Completing Train Step...
At time: 32.4335834980011 and batch: 50, loss is 5.63207763671875 and perplexity is 279.2416781303678
At time: 34.04061484336853 and batch: 100, loss is 5.542309646606445 and perplexity is 255.26689544476122
At time: 35.64525079727173 and batch: 150, loss is 5.5091746807098385 and perplexity is 246.9472326254514
At time: 37.24962091445923 and batch: 200, loss is 5.518920106887817 and perplexity is 249.36560353842904
At time: 38.88341808319092 and batch: 250, loss is 5.553823947906494 and perplexity is 258.22310205678764
At time: 40.48638844490051 and batch: 300, loss is 5.493712949752807 and perplexity is 243.1583676413441
At time: 42.08177947998047 and batch: 350, loss is 5.453391618728638 and perplexity is 233.5489330947776
At time: 43.67901015281677 and batch: 400, loss is 5.4647025299072265 and perplexity is 236.20558056112773
At time: 45.26736545562744 and batch: 450, loss is 5.466366729736328 and perplexity is 236.5990011223456
At time: 46.869051456451416 and batch: 500, loss is 5.449023904800415 and perplexity is 232.53108262473634
At time: 48.46256637573242 and batch: 550, loss is 5.397822523117066 and perplexity is 220.92483335920684
At time: 50.05912685394287 and batch: 600, loss is 5.423039512634277 and perplexity is 226.56672941426714
At time: 51.66155648231506 and batch: 650, loss is 5.429474258422852 and perplexity is 228.0293294050702
At time: 53.26489758491516 and batch: 700, loss is 5.365346927642822 and perplexity is 213.86541763735897
At time: 54.85853624343872 and batch: 750, loss is 5.3531467056274415 and perplexity is 211.27206397332486
At time: 56.45322608947754 and batch: 800, loss is 5.358373498916626 and perplexity is 212.37923032372856
At time: 58.05150103569031 and batch: 850, loss is 5.35310471534729 and perplexity is 211.2631927864235
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.050594011942546 and perplexity of 156.11517122533442
Finished 2 epochs...
Completing Train Step...
At time: 62.2669358253479 and batch: 50, loss is 5.33950888633728 and perplexity is 208.4103319899711
At time: 63.86776113510132 and batch: 100, loss is 5.266768379211426 and perplexity is 193.78869786604727
At time: 65.47748708724976 and batch: 150, loss is 5.244864292144776 and perplexity is 189.59008461988068
At time: 67.09422302246094 and batch: 200, loss is 5.267850151062012 and perplexity is 193.99844645396507
At time: 68.69799518585205 and batch: 250, loss is 5.3054776382446285 and perplexity is 201.43719354711413
At time: 70.3316605091095 and batch: 300, loss is 5.259056053161621 and perplexity is 192.2998847290469
At time: 71.94506597518921 and batch: 350, loss is 5.226883172988892 and perplexity is 186.21150903244353
At time: 73.55877208709717 and batch: 400, loss is 5.243841075897217 and perplexity is 189.3961921787863
At time: 75.16978359222412 and batch: 450, loss is 5.249851360321045 and perplexity is 190.53794485472443
At time: 76.78224205970764 and batch: 500, loss is 5.2396821117401124 and perplexity is 188.61013592694417
At time: 78.39539384841919 and batch: 550, loss is 5.206120481491089 and perplexity is 182.38511746958335
At time: 80.00881171226501 and batch: 600, loss is 5.238320598602295 and perplexity is 188.3535154846477
At time: 81.62603402137756 and batch: 650, loss is 5.243310976028442 and perplexity is 189.29581988818532
At time: 83.24246406555176 and batch: 700, loss is 5.186099834442139 and perplexity is 178.76995906438157
At time: 84.86093592643738 and batch: 750, loss is 5.1785609149932865 and perplexity is 177.4272942233867
At time: 86.48028779029846 and batch: 800, loss is 5.178964996337891 and perplexity is 177.49900377027535
At time: 88.094970703125 and batch: 850, loss is 5.181588144302368 and perplexity is 177.96522113194055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.929412841796875 and perplexity of 138.29828552297536
Finished 3 epochs...
Completing Train Step...
At time: 92.34893417358398 and batch: 50, loss is 5.178722677230835 and perplexity is 177.45599758099945
At time: 94.01821494102478 and batch: 100, loss is 5.11201961517334 and perplexity is 166.00528333566368
At time: 95.62792325019836 and batch: 150, loss is 5.0931019115447995 and perplexity is 162.89436309563519
At time: 97.24068140983582 and batch: 200, loss is 5.125285549163818 and perplexity is 168.22217049434155
At time: 98.85060691833496 and batch: 250, loss is 5.153312692642212 and perplexity is 173.00365001749103
At time: 100.46042776107788 and batch: 300, loss is 5.116799106597901 and perplexity is 166.8006032621897
At time: 102.06950616836548 and batch: 350, loss is 5.085421848297119 and perplexity is 161.64811583889647
At time: 103.67898654937744 and batch: 400, loss is 5.102378406524658 and perplexity is 164.41248236343282
At time: 105.29075884819031 and batch: 450, loss is 5.112573804855347 and perplexity is 166.09730724784305
At time: 106.90037107467651 and batch: 500, loss is 5.104875650405884 and perplexity is 164.82357351280254
At time: 108.50945472717285 and batch: 550, loss is 5.080723237991333 and perplexity is 160.89037589231373
At time: 110.11764144897461 and batch: 600, loss is 5.116424913406372 and perplexity is 166.73819928840538
At time: 111.75795316696167 and batch: 650, loss is 5.118250856399536 and perplexity is 167.0429318625765
At time: 113.36663174629211 and batch: 700, loss is 5.066115312576294 and perplexity is 158.55718431566683
At time: 114.97390389442444 and batch: 750, loss is 5.05976902961731 and perplexity is 157.55412178469544
At time: 116.57647228240967 and batch: 800, loss is 5.053387908935547 and perplexity is 156.55195080724303
At time: 118.17521905899048 and batch: 850, loss is 5.060231132507324 and perplexity is 157.62694482427847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858631769816081 and perplexity of 128.84778801466348
Finished 4 epochs...
Completing Train Step...
At time: 122.44376611709595 and batch: 50, loss is 5.061335754394531 and perplexity is 157.80115920033688
At time: 124.05341005325317 and batch: 100, loss is 4.998209056854248 and perplexity is 148.14759744643823
At time: 125.66365027427673 and batch: 150, loss is 4.984194116592407 and perplexity is 146.08579945397668
At time: 127.26260614395142 and batch: 200, loss is 5.020452814102173 and perplexity is 151.47988049271547
At time: 128.84593987464905 and batch: 250, loss is 5.04203429222107 and perplexity is 154.7845720177886
At time: 130.42823123931885 and batch: 300, loss is 5.010066833496094 and perplexity is 149.91475514144295
At time: 132.01153588294983 and batch: 350, loss is 4.977691926956177 and perplexity is 145.1390033424538
At time: 133.59481859207153 and batch: 400, loss is 4.997547550201416 and perplexity is 148.04962923201168
At time: 135.17811226844788 and batch: 450, loss is 5.008999547958374 and perplexity is 149.7548386448518
At time: 136.76177382469177 and batch: 500, loss is 5.0034854698181155 and perplexity is 148.93135123615593
At time: 138.34508204460144 and batch: 550, loss is 4.984747867584229 and perplexity is 146.16671701233676
At time: 139.92921352386475 and batch: 600, loss is 5.02179895401001 and perplexity is 151.6839309144759
At time: 141.51236772537231 and batch: 650, loss is 5.022463417053222 and perplexity is 151.7847527733078
At time: 143.0976951122284 and batch: 700, loss is 4.972808313369751 and perplexity is 144.43192847930644
At time: 144.68333339691162 and batch: 750, loss is 4.96736665725708 and perplexity is 143.6481141510143
At time: 146.27020740509033 and batch: 800, loss is 4.956316356658935 and perplexity is 142.0694974673259
At time: 147.85892581939697 and batch: 850, loss is 4.964152812957764 and perplexity is 143.18719254011867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.804224967956543 and perplexity of 122.02488117007891
Finished 5 epochs...
Completing Train Step...
At time: 152.06569933891296 and batch: 50, loss is 4.96764139175415 and perplexity is 143.68758466511835
At time: 153.6777195930481 and batch: 100, loss is 4.906690673828125 and perplexity is 135.19128126278187
At time: 155.28686237335205 and batch: 150, loss is 4.895794296264649 and perplexity is 133.72618264871446
At time: 156.89555168151855 and batch: 200, loss is 4.935220336914062 and perplexity is 139.10378885526706
At time: 158.5039713382721 and batch: 250, loss is 4.9525173377990725 and perplexity is 141.53079668215938
At time: 160.10665893554688 and batch: 300, loss is 4.923716888427735 and perplexity is 137.51278415092293
At time: 161.6919767856598 and batch: 350, loss is 4.891278057098389 and perplexity is 133.12360494180402
At time: 163.27509903907776 and batch: 400, loss is 4.913322772979736 and perplexity is 136.09086300488437
At time: 164.85874009132385 and batch: 450, loss is 4.925057621002197 and perplexity is 137.697275669297
At time: 166.44227623939514 and batch: 500, loss is 4.919517183303833 and perplexity is 136.93648200330844
At time: 168.02426862716675 and batch: 550, loss is 4.90667426109314 and perplexity is 135.18906242231887
At time: 169.60673093795776 and batch: 600, loss is 4.945123701095581 and perplexity is 140.4882283243778
At time: 171.18931651115417 and batch: 650, loss is 4.944770107269287 and perplexity is 140.43856133566302
At time: 172.77465415000916 and batch: 700, loss is 4.896767101287842 and perplexity is 133.8563354473115
At time: 174.36073517799377 and batch: 750, loss is 4.891791954040527 and perplexity is 133.19203433663245
At time: 175.94485354423523 and batch: 800, loss is 4.877124795913696 and perplexity is 131.25274242415824
At time: 177.5297737121582 and batch: 850, loss is 4.887861042022705 and perplexity is 132.66949586741288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.76392141977946 and perplexity of 117.20463448863273
Finished 6 epochs...
Completing Train Step...
At time: 181.72754859924316 and batch: 50, loss is 4.8900783729553225 and perplexity is 132.96399442413426
At time: 183.31614565849304 and batch: 100, loss is 4.831848697662354 and perplexity is 125.44265196263105
At time: 184.90263724327087 and batch: 150, loss is 4.822426633834839 and perplexity is 124.26627394628585
At time: 186.49185514450073 and batch: 200, loss is 4.864997425079346 and perplexity is 129.67060471986082
At time: 188.1073718070984 and batch: 250, loss is 4.8788563251495365 and perplexity is 131.4802072591239
At time: 189.72051215171814 and batch: 300, loss is 4.852756700515747 and perplexity is 128.0930176659324
At time: 191.33413815498352 and batch: 350, loss is 4.819107208251953 and perplexity is 123.85446515989821
At time: 192.98231720924377 and batch: 400, loss is 4.842695999145508 and perplexity is 126.8107730232702
At time: 194.59455394744873 and batch: 450, loss is 4.855094528198242 and perplexity is 128.39282738374573
At time: 196.20478701591492 and batch: 500, loss is 4.850124769210815 and perplexity is 127.75632890799058
At time: 197.81422090530396 and batch: 550, loss is 4.8420881748199465 and perplexity is 126.73371777108413
At time: 199.42495346069336 and batch: 600, loss is 4.880488386154175 and perplexity is 131.694966180434
At time: 201.03385829925537 and batch: 650, loss is 4.880020179748535 and perplexity is 131.6333201863286
At time: 202.64163780212402 and batch: 700, loss is 4.832634830474854 and perplexity is 125.54130531966575
At time: 204.2458770275116 and batch: 750, loss is 4.827614936828613 and perplexity is 124.9126804538125
At time: 205.8566975593567 and batch: 800, loss is 4.80972885131836 and perplexity is 122.69834351228494
At time: 207.4687614440918 and batch: 850, loss is 4.823681650161743 and perplexity is 124.42232805371533
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.731257438659668 and perplexity of 113.43811417503163
Finished 7 epochs...
Completing Train Step...
At time: 211.72811579704285 and batch: 50, loss is 4.823980865478515 and perplexity is 124.45956269032649
At time: 213.36379981040955 and batch: 100, loss is 4.76837420463562 and perplexity is 117.72768516199122
At time: 214.97124886512756 and batch: 150, loss is 4.761094465255737 and perplexity is 116.87377020644996
At time: 216.5751438140869 and batch: 200, loss is 4.803820676803589 and perplexity is 121.97555756140277
At time: 218.17982578277588 and batch: 250, loss is 4.816396541595459 and perplexity is 123.5191916036474
At time: 219.78467869758606 and batch: 300, loss is 4.791960210800171 and perplexity is 120.5374159810124
At time: 221.3905267715454 and batch: 350, loss is 4.756990184783936 and perplexity is 116.39507050462412
At time: 222.9954228401184 and batch: 400, loss is 4.782213134765625 and perplexity is 119.3682359224315
At time: 224.5996298789978 and batch: 450, loss is 4.795668458938598 and perplexity is 120.98522841608973
At time: 226.2021403312683 and batch: 500, loss is 4.79102861404419 and perplexity is 120.42517600462659
At time: 227.80481624603271 and batch: 550, loss is 4.785989894866943 and perplexity is 119.81991351529291
At time: 229.40961122512817 and batch: 600, loss is 4.824923000335693 and perplexity is 124.57687563627655
At time: 231.0014820098877 and batch: 650, loss is 4.825250177383423 and perplexity is 124.6176409990445
At time: 232.64483451843262 and batch: 700, loss is 4.778856582641602 and perplexity is 118.96824189255285
At time: 234.24434995651245 and batch: 750, loss is 4.773047962188721 and perplexity is 118.27920364762251
At time: 235.83891701698303 and batch: 800, loss is 4.752172737121582 and perplexity is 115.83569181492142
At time: 237.43490481376648 and batch: 850, loss is 4.768271017074585 and perplexity is 117.7155377560344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.704710006713867 and perplexity of 110.46624583249663
Finished 8 epochs...
Completing Train Step...
At time: 241.6368420124054 and batch: 50, loss is 4.765687160491943 and perplexity is 117.41177030369383
At time: 243.26538133621216 and batch: 100, loss is 4.713048515319824 and perplexity is 111.39122066985131
At time: 244.8677523136139 and batch: 150, loss is 4.706606464385986 and perplexity is 110.675939166334
At time: 246.46880459785461 and batch: 200, loss is 4.749951848983764 and perplexity is 115.57871916041692
At time: 248.07721829414368 and batch: 250, loss is 4.761429824829102 and perplexity is 116.91297151706391
At time: 249.68270444869995 and batch: 300, loss is 4.738420133590698 and perplexity is 114.25355366628789
At time: 251.2808473110199 and batch: 350, loss is 4.702402839660644 and perplexity is 110.2116755306488
At time: 252.88252902030945 and batch: 400, loss is 4.7291629600524905 and perplexity is 113.20076911549457
At time: 254.48765015602112 and batch: 450, loss is 4.744013328552246 and perplexity is 114.89438654797422
At time: 256.08758783340454 and batch: 500, loss is 4.7387517642974855 and perplexity is 114.29144993647931
At time: 257.6844449043274 and batch: 550, loss is 4.736638879776001 and perplexity is 114.05022023596374
At time: 259.29033398628235 and batch: 600, loss is 4.776039066314698 and perplexity is 118.63351869410629
At time: 260.89407896995544 and batch: 650, loss is 4.775697135925293 and perplexity is 118.59296122317387
At time: 262.49573135375977 and batch: 700, loss is 4.7302003097534175 and perplexity is 113.31825882790442
At time: 264.0939722061157 and batch: 750, loss is 4.72253643989563 and perplexity is 112.45312182291786
At time: 265.7003335952759 and batch: 800, loss is 4.700564708709717 and perplexity is 110.00927811216907
At time: 267.3112254142761 and batch: 850, loss is 4.71882887840271 and perplexity is 112.03696689538654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.682279904683431 and perplexity of 108.01605837012484
Finished 9 epochs...
Completing Train Step...
At time: 271.60705733299255 and batch: 50, loss is 4.71378942489624 and perplexity is 111.47378207346478
At time: 273.2471969127655 and batch: 100, loss is 4.664293060302734 and perplexity is 106.09055907790268
At time: 274.8598909378052 and batch: 150, loss is 4.6581181621551515 and perplexity is 105.43747910745226
At time: 276.4736113548279 and batch: 200, loss is 4.701847648620605 and perplexity is 110.1505039784066
At time: 278.08543610572815 and batch: 250, loss is 4.712256212234497 and perplexity is 111.30300001539958
At time: 279.69831466674805 and batch: 300, loss is 4.690423040390015 and perplexity is 108.89923884078489
At time: 281.31029748916626 and batch: 350, loss is 4.65411117553711 and perplexity is 105.0158378592021
At time: 282.9205975532532 and batch: 400, loss is 4.681881113052368 and perplexity is 107.97299105803906
At time: 284.5316119194031 and batch: 450, loss is 4.698390150070191 and perplexity is 109.7703163983169
At time: 286.14205503463745 and batch: 500, loss is 4.691986570358276 and perplexity is 109.06963924258693
At time: 287.7552044391632 and batch: 550, loss is 4.692604360580444 and perplexity is 109.13704221755204
At time: 289.36756777763367 and batch: 600, loss is 4.7313563251495365 and perplexity is 113.44933222660744
At time: 290.9818205833435 and batch: 650, loss is 4.730514373779297 and perplexity is 113.35385360570456
At time: 292.5936391353607 and batch: 700, loss is 4.686209592819214 and perplexity is 108.44136290295256
At time: 294.2056188583374 and batch: 750, loss is 4.677506322860718 and perplexity is 107.50166360686991
At time: 295.81854796409607 and batch: 800, loss is 4.654412126541137 and perplexity is 105.04744723724309
At time: 297.4321711063385 and batch: 850, loss is 4.67490888595581 and perplexity is 107.22279714428439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.665512720743815 and perplexity of 106.22003247672347
Finished 10 epochs...
Completing Train Step...
At time: 301.72675609588623 and batch: 50, loss is 4.667791128158569 and perplexity is 106.46232089736814
At time: 303.3358085155487 and batch: 100, loss is 4.621104679107666 and perplexity is 101.60621241659365
At time: 304.94627237319946 and batch: 150, loss is 4.6148154926300045 and perplexity is 100.96919725269392
At time: 306.55508637428284 and batch: 200, loss is 4.659338254928588 and perplexity is 105.56620112419363
At time: 308.15393018722534 and batch: 250, loss is 4.668013219833374 and perplexity is 106.48596791832587
At time: 309.7511672973633 and batch: 300, loss is 4.647314977645874 and perplexity is 104.30454921037071
At time: 311.34108781814575 and batch: 350, loss is 4.610887317657471 and perplexity is 100.57335056558487
At time: 312.9438133239746 and batch: 400, loss is 4.6396032905578615 and perplexity is 103.50327870965828
At time: 314.57742524147034 and batch: 450, loss is 4.65625075340271 and perplexity is 105.24076796335224
At time: 316.17565751075745 and batch: 500, loss is 4.649479484558105 and perplexity is 104.5305616426089
At time: 317.7769479751587 and batch: 550, loss is 4.651761894226074 and perplexity is 104.7694156847974
At time: 319.3834660053253 and batch: 600, loss is 4.690677213668823 and perplexity is 108.92692163534505
At time: 320.9840340614319 and batch: 650, loss is 4.689953050613403 and perplexity is 108.84806933736257
At time: 322.5810730457306 and batch: 700, loss is 4.646192359924316 and perplexity is 104.18752077637562
At time: 324.18441820144653 and batch: 750, loss is 4.637591047286987 and perplexity is 103.29521434182989
At time: 325.79117131233215 and batch: 800, loss is 4.612655668258667 and perplexity is 100.75135685289804
At time: 327.39591455459595 and batch: 850, loss is 4.6346178150177 and perplexity is 102.9885497956604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.652743975321452 and perplexity of 104.87235828803364
Finished 11 epochs...
Completing Train Step...
At time: 331.63782596588135 and batch: 50, loss is 4.6264315032958985 and perplexity is 102.14889495057332
At time: 333.2469789981842 and batch: 100, loss is 4.581428899765014 and perplexity is 97.65383209968316
At time: 334.85664677619934 and batch: 150, loss is 4.574450311660766 and perplexity is 96.9747186114192
At time: 336.46573638916016 and batch: 200, loss is 4.619417133331299 and perplexity is 101.43489187826022
At time: 338.0746500492096 and batch: 250, loss is 4.627667045593261 and perplexity is 102.27518223148675
At time: 339.68257427215576 and batch: 300, loss is 4.607897052764892 and perplexity is 100.27305880597224
At time: 341.2953345775604 and batch: 350, loss is 4.5718734836578365 and perplexity is 96.72515312268881
At time: 342.90680718421936 and batch: 400, loss is 4.599975709915161 and perplexity is 99.48189918881477
At time: 344.5198485851288 and batch: 450, loss is 4.618130741119384 and perplexity is 101.3044907148345
At time: 346.13365602493286 and batch: 500, loss is 4.610770597457885 and perplexity is 100.56161230909302
At time: 347.7430727481842 and batch: 550, loss is 4.614538583755493 and perplexity is 100.94124185664964
At time: 349.3519444465637 and batch: 600, loss is 4.6535813522338865 and perplexity is 104.9602127581354
At time: 350.96033668518066 and batch: 650, loss is 4.652426633834839 and perplexity is 104.83908321800922
At time: 352.5689866542816 and batch: 700, loss is 4.609913721084594 and perplexity is 100.4754803469371
At time: 354.18280029296875 and batch: 750, loss is 4.601382131576538 and perplexity is 99.62191112157751
At time: 355.82182908058167 and batch: 800, loss is 4.5738303661346436 and perplexity is 96.91461819988456
At time: 357.4317181110382 and batch: 850, loss is 4.598537254333496 and perplexity is 99.338901768028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.641725540161133 and perplexity of 103.72317175326727
Finished 12 epochs...
Completing Train Step...
At time: 361.70440554618835 and batch: 50, loss is 4.589149293899536 and perplexity is 98.41067597949386
At time: 363.3036117553711 and batch: 100, loss is 4.544424533843994 and perplexity is 94.10625665846703
At time: 364.90780568122864 and batch: 150, loss is 4.538722171783447 and perplexity is 93.57115583015434
At time: 366.50689721107483 and batch: 200, loss is 4.582879180908203 and perplexity is 97.79556035900684
At time: 368.1169214248657 and batch: 250, loss is 4.590409517288208 and perplexity is 98.53477359399247
At time: 369.7264060974121 and batch: 300, loss is 4.571503915786743 and perplexity is 96.68941321833505
At time: 371.33692955970764 and batch: 350, loss is 4.535301132202148 and perplexity is 93.2515921340861
At time: 372.94624853134155 and batch: 400, loss is 4.564135503768921 and perplexity is 95.97958415208096
At time: 374.556036233902 and batch: 450, loss is 4.583266258239746 and perplexity is 97.8334221307914
At time: 376.1650359630585 and batch: 500, loss is 4.575481367111206 and perplexity is 97.07475648703453
At time: 377.7738902568817 and batch: 550, loss is 4.58017331123352 and perplexity is 97.53129601159398
At time: 379.3824462890625 and batch: 600, loss is 4.619255504608154 and perplexity is 101.41849841106685
At time: 380.9930729866028 and batch: 650, loss is 4.617924146652221 and perplexity is 101.28356392930753
At time: 382.60262060165405 and batch: 700, loss is 4.5756312847137455 and perplexity is 97.0893107927402
At time: 384.2122621536255 and batch: 750, loss is 4.567131443023682 and perplexity is 96.26756432605087
At time: 385.82030057907104 and batch: 800, loss is 4.5384949111938475 and perplexity is 93.54989321028016
At time: 387.4341640472412 and batch: 850, loss is 4.564730062484741 and perplexity is 96.03666661813665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.6328080495198565 and perplexity of 102.80233322655373
Finished 13 epochs...
Completing Train Step...
At time: 391.67723274230957 and batch: 50, loss is 4.5531850433349605 and perplexity is 94.93429714559355
At time: 393.3196096420288 and batch: 100, loss is 4.5100691032409665 and perplexity is 90.92810171995576
At time: 394.9324064254761 and batch: 150, loss is 4.505055351257324 and perplexity is 90.47335172404954
At time: 396.57445001602173 and batch: 200, loss is 4.549333772659302 and perplexity is 94.56938261424224
At time: 398.18748807907104 and batch: 250, loss is 4.55626335144043 and perplexity is 95.22698442166615
At time: 399.80147647857666 and batch: 300, loss is 4.538113527297973 and perplexity is 93.5142215902721
At time: 401.4151294231415 and batch: 350, loss is 4.501656694412231 and perplexity is 90.16638577934559
At time: 403.02856492996216 and batch: 400, loss is 4.5307102394104 and perplexity is 92.82446526894925
At time: 404.63971281051636 and batch: 450, loss is 4.550666284561157 and perplexity is 94.69548143756
At time: 406.24897956848145 and batch: 500, loss is 4.543377923965454 and perplexity is 94.00781564427076
At time: 407.85963773727417 and batch: 550, loss is 4.549559907913208 and perplexity is 94.59077050377826
At time: 409.47076416015625 and batch: 600, loss is 4.5882924365997315 and perplexity is 98.32638818986312
At time: 411.080863237381 and batch: 650, loss is 4.585839900970459 and perplexity is 98.08553469114975
At time: 412.6942882537842 and batch: 700, loss is 4.544165964126587 and perplexity is 94.0819267758976
At time: 414.29524421691895 and batch: 750, loss is 4.5351314353942875 and perplexity is 93.2357689791804
At time: 415.8985683917999 and batch: 800, loss is 4.505551795959473 and perplexity is 90.51827789095468
At time: 417.51086235046387 and batch: 850, loss is 4.534014282226562 and perplexity is 93.13166850343484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.625304222106934 and perplexity of 102.03380930195262
Finished 14 epochs...
Completing Train Step...
At time: 421.7762725353241 and batch: 50, loss is 4.5204012966156 and perplexity is 91.87245868836149
At time: 423.4183084964752 and batch: 100, loss is 4.478309965133667 and perplexity is 88.08567894017723
At time: 425.01917934417725 and batch: 150, loss is 4.473832769393921 and perplexity is 87.69218364872502
At time: 426.6057574748993 and batch: 200, loss is 4.5181006240844725 and perplexity is 91.66133320471602
At time: 428.1901800632477 and batch: 250, loss is 4.523909168243408 and perplexity is 92.19530139390226
At time: 429.7736556529999 and batch: 300, loss is 4.506638240814209 and perplexity is 90.61667444976777
At time: 431.3573625087738 and batch: 350, loss is 4.47042160987854 and perplexity is 87.39356123595391
At time: 432.96482491493225 and batch: 400, loss is 4.499072818756104 and perplexity is 89.93370778508638
At time: 434.5770492553711 and batch: 450, loss is 4.520713758468628 and perplexity is 91.9011698123774
At time: 436.21596121788025 and batch: 500, loss is 4.513335638046264 and perplexity is 91.22560717041995
At time: 437.81904578208923 and batch: 550, loss is 4.5198773002624515 and perplexity is 91.8243304656618
At time: 439.42264437675476 and batch: 600, loss is 4.559187717437744 and perplexity is 95.50587056075591
At time: 441.0308394432068 and batch: 650, loss is 4.556041393280029 and perplexity is 95.20585036090881
At time: 442.6401798725128 and batch: 700, loss is 4.514358367919922 and perplexity is 91.31895405030045
At time: 444.2522118091583 and batch: 750, loss is 4.505399169921875 and perplexity is 90.50446349911478
At time: 445.86248111724854 and batch: 800, loss is 4.474535055160523 and perplexity is 87.75379025133176
At time: 447.47466492652893 and batch: 850, loss is 4.504713115692138 and perplexity is 90.44239382313752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.619301160176595 and perplexity of 101.42312883596372
Finished 15 epochs...
Completing Train Step...
At time: 451.7454037666321 and batch: 50, loss is 4.4894428730010985 and perplexity is 89.07180774363505
At time: 453.38589119911194 and batch: 100, loss is 4.448279685974121 and perplexity is 85.47976540244288
At time: 454.9932150840759 and batch: 150, loss is 4.444549970626831 and perplexity is 85.1615440160251
At time: 456.60392928123474 and batch: 200, loss is 4.4879593181610105 and perplexity is 88.93976280431453
At time: 458.21354246139526 and batch: 250, loss is 4.493743352890014 and perplexity is 89.45568409615151
At time: 459.82268953323364 and batch: 300, loss is 4.477386255264282 and perplexity is 88.00435089672962
At time: 461.4306402206421 and batch: 350, loss is 4.440513973236084 and perplexity is 84.81852492429162
At time: 463.0321969985962 and batch: 400, loss is 4.469569597244263 and perplexity is 87.31913252922814
At time: 464.63389468193054 and batch: 450, loss is 4.492377843856811 and perplexity is 89.33361491370019
At time: 466.23688769340515 and batch: 500, loss is 4.483594036102295 and perplexity is 88.5523618258912
At time: 467.83495116233826 and batch: 550, loss is 4.492270402908325 and perplexity is 89.32401734097753
At time: 469.4450159072876 and batch: 600, loss is 4.5336333274841305 and perplexity is 93.09619630972709
At time: 471.0575876235962 and batch: 650, loss is 4.527869939804077 and perplexity is 92.56119004404093
At time: 472.6682095527649 and batch: 700, loss is 4.486621704101562 and perplexity is 88.82087525769667
At time: 474.2810323238373 and batch: 750, loss is 4.4774706840515135 and perplexity is 88.01178131101297
At time: 475.8892285823822 and batch: 800, loss is 4.445684070587158 and perplexity is 85.25818050707775
At time: 477.5264003276825 and batch: 850, loss is 4.477361602783203 and perplexity is 88.00218139787617
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.614584287007649 and perplexity of 100.94585530510317
Finished 16 epochs...
Completing Train Step...
At time: 481.83872294425964 and batch: 50, loss is 4.460144271850586 and perplexity is 86.49998770945396
At time: 483.4525601863861 and batch: 100, loss is 4.420241527557373 and perplexity is 83.11635782510247
At time: 485.06228733062744 and batch: 150, loss is 4.417169961929321 and perplexity is 82.86145215743093
At time: 486.6620891094208 and batch: 200, loss is 4.460230951309204 and perplexity is 86.50748580651987
At time: 488.2785391807556 and batch: 250, loss is 4.465715408325195 and perplexity is 86.98323581671073
At time: 489.896431684494 and batch: 300, loss is 4.449041824340821 and perplexity is 85.54493764321047
At time: 491.50752878189087 and batch: 350, loss is 4.412870569229126 and perplexity is 82.50596297628758
At time: 493.0954978466034 and batch: 400, loss is 4.442793378829956 and perplexity is 85.0120812573573
At time: 494.6833894252777 and batch: 450, loss is 4.465690126419068 and perplexity is 86.98103674250665
At time: 496.27691411972046 and batch: 500, loss is 4.456566247940064 and perplexity is 86.1910417228336
At time: 497.86414098739624 and batch: 550, loss is 4.466497430801391 and perplexity is 87.05128526680338
At time: 499.45238876342773 and batch: 600, loss is 4.507344923019409 and perplexity is 90.68073427339857
At time: 501.04081559181213 and batch: 650, loss is 4.501512994766236 and perplexity is 90.15342983253315
At time: 502.64406061172485 and batch: 700, loss is 4.460228567123413 and perplexity is 86.50727955684727
At time: 504.25996232032776 and batch: 750, loss is 4.452165031433106 and perplexity is 85.8125298545958
At time: 505.87323784828186 and batch: 800, loss is 4.418920049667358 and perplexity is 83.00659393718699
At time: 507.49120569229126 and batch: 850, loss is 4.451653976440429 and perplexity is 85.76868613700904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.611021041870117 and perplexity of 100.58680055697717
Finished 17 epochs...
Completing Train Step...
At time: 511.82664585113525 and batch: 50, loss is 4.433608112335205 and perplexity is 84.23479787811043
At time: 513.4363670349121 and batch: 100, loss is 4.39562066078186 and perplexity is 81.09494759985076
At time: 515.0463383197784 and batch: 150, loss is 4.392260799407959 and perplexity is 80.82293703257696
At time: 516.6568953990936 and batch: 200, loss is 4.434495420455932 and perplexity is 84.30957326781481
At time: 518.3165175914764 and batch: 250, loss is 4.439751749038696 and perplexity is 84.75389882512478
At time: 519.9242806434631 and batch: 300, loss is 4.422916584014892 and perplexity is 83.33899642747805
At time: 521.5365445613861 and batch: 350, loss is 4.387076930999756 and perplexity is 80.40504564543038
At time: 523.1400599479675 and batch: 400, loss is 4.416963081359864 and perplexity is 82.8443115061175
At time: 524.7489998340607 and batch: 450, loss is 4.440563068389893 and perplexity is 84.82268920504079
At time: 526.3573286533356 and batch: 500, loss is 4.43079306602478 and perplexity is 83.99800646689934
At time: 527.9666900634766 and batch: 550, loss is 4.441082534790039 and perplexity is 84.8667631885385
At time: 529.5738554000854 and batch: 600, loss is 4.483298597335815 and perplexity is 88.52620388956899
At time: 531.1820816993713 and batch: 650, loss is 4.475992879867554 and perplexity is 87.88181318972367
At time: 532.789457321167 and batch: 700, loss is 4.434792356491089 and perplexity is 84.33461153542468
At time: 534.3956894874573 and batch: 750, loss is 4.427435846328735 and perplexity is 83.71647954344868
At time: 536.0054621696472 and batch: 800, loss is 4.393018808364868 and perplexity is 80.88422476816308
At time: 537.6150336265564 and batch: 850, loss is 4.427340393066406 and perplexity is 83.70848891373747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.607908248901367 and perplexity of 100.2741814831082
Finished 18 epochs...
Completing Train Step...
At time: 541.8974225521088 and batch: 50, loss is 4.408775911331177 and perplexity is 82.16881999699139
At time: 543.4949896335602 and batch: 100, loss is 4.3703400421142575 and perplexity is 79.07051443357798
At time: 545.1061339378357 and batch: 150, loss is 4.367711687088013 and perplexity is 78.8629619298573
At time: 546.700201511383 and batch: 200, loss is 4.410325164794922 and perplexity is 82.29621898716519
At time: 548.3067014217377 and batch: 250, loss is 4.415489549636841 and perplexity is 82.72232768063144
At time: 549.9184851646423 and batch: 300, loss is 4.3977380466461184 and perplexity is 81.26683881135182
At time: 551.52965259552 and batch: 350, loss is 4.363014516830444 and perplexity is 78.49339780295593
At time: 553.1352627277374 and batch: 400, loss is 4.391513738632202 and perplexity is 80.76257993455582
At time: 554.7448923587799 and batch: 450, loss is 4.417552795410156 and perplexity is 82.89318036851061
At time: 556.3520498275757 and batch: 500, loss is 4.407456798553467 and perplexity is 82.06050151445787
At time: 557.9581255912781 and batch: 550, loss is 4.417324810028076 and perplexity is 82.87428408923208
At time: 559.6225690841675 and batch: 600, loss is 4.460169715881348 and perplexity is 86.50218864580233
At time: 561.2260982990265 and batch: 650, loss is 4.452377758026123 and perplexity is 85.83078640346783
At time: 562.8100805282593 and batch: 700, loss is 4.411075401306152 and perplexity is 82.35798378159045
At time: 564.3954131603241 and batch: 750, loss is 4.403549966812133 and perplexity is 81.74053038609716
At time: 565.9794845581055 and batch: 800, loss is 4.3686439990997314 and perplexity is 78.93652110126557
At time: 567.565461397171 and batch: 850, loss is 4.404628248214721 and perplexity is 81.82871721641521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.606560389200847 and perplexity of 100.13911699931968
Finished 19 epochs...
Completing Train Step...
At time: 571.7311291694641 and batch: 50, loss is 4.383413791656494 and perplexity is 80.11104956239011
At time: 573.3688395023346 and batch: 100, loss is 4.3472428798675535 and perplexity is 77.26513970930046
At time: 574.9611234664917 and batch: 150, loss is 4.345523710250855 and perplexity is 77.13242194354522
At time: 576.5451283454895 and batch: 200, loss is 4.386302623748779 and perplexity is 80.34281153284371
At time: 578.1289207935333 and batch: 250, loss is 4.391760931015015 and perplexity is 80.7825462967966
At time: 579.7121272087097 and batch: 300, loss is 4.373863658905029 and perplexity is 79.34962006776227
At time: 581.2966823577881 and batch: 350, loss is 4.339500074386597 and perplexity is 76.6692008586317
At time: 582.880330324173 and batch: 400, loss is 4.367987127304077 and perplexity is 78.88468695296588
At time: 584.4640800952911 and batch: 450, loss is 4.394329919815063 and perplexity is 80.99034255231622
At time: 586.0466866493225 and batch: 500, loss is 4.384419765472412 and perplexity is 80.19167972973443
At time: 587.6295878887177 and batch: 550, loss is 4.395201797485352 and perplexity is 81.06098701568652
At time: 589.2137098312378 and batch: 600, loss is 4.437785415649414 and perplexity is 84.58740814572829
At time: 590.7952966690063 and batch: 650, loss is 4.430386438369751 and perplexity is 83.96385749793069
At time: 592.3775930404663 and batch: 700, loss is 4.3886543655395505 and perplexity is 80.5319794301526
At time: 593.9608130455017 and batch: 750, loss is 4.381066942214966 and perplexity is 79.92326143188907
At time: 595.5442774295807 and batch: 800, loss is 4.34581995010376 and perplexity is 77.15527502570572
At time: 597.1272330284119 and batch: 850, loss is 4.38284291267395 and perplexity is 80.0653288996498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.602959950764974 and perplexity of 99.77922055481956
Finished 20 epochs...
Completing Train Step...
At time: 601.3232133388519 and batch: 50, loss is 4.359925861358643 and perplexity is 78.25133276052796
At time: 602.9445893764496 and batch: 100, loss is 4.32461049079895 and perplexity is 75.53608512874371
At time: 604.5629086494446 and batch: 150, loss is 4.322259616851807 and perplexity is 75.35871787985423
At time: 606.1608486175537 and batch: 200, loss is 4.36346848487854 and perplexity is 78.52903938700302
At time: 607.7754328250885 and batch: 250, loss is 4.3681465435028075 and perplexity is 78.8972634523202
At time: 609.3928935527802 and batch: 300, loss is 4.351258592605591 and perplexity is 77.57603813663053
At time: 611.0103075504303 and batch: 350, loss is 4.316390333175659 and perplexity is 74.91771164848319
At time: 612.6269011497498 and batch: 400, loss is 4.346045742034912 and perplexity is 77.17269803116535
At time: 614.2435519695282 and batch: 450, loss is 4.3723354911804195 and perplexity is 79.22845314468529
At time: 615.8697924613953 and batch: 500, loss is 4.3626611709594725 and perplexity is 78.46566738444669
At time: 617.4802641868591 and batch: 550, loss is 4.373261871337891 and perplexity is 79.30188281823064
At time: 619.0801060199738 and batch: 600, loss is 4.417496557235718 and perplexity is 82.88851873845532
At time: 620.6687581539154 and batch: 650, loss is 4.409577674865723 and perplexity is 82.23472637767748
At time: 622.2568662166595 and batch: 700, loss is 4.36721604347229 and perplexity is 78.82388369149943
At time: 623.845861196518 and batch: 750, loss is 4.3612415218353275 and perplexity is 78.35435270106063
At time: 625.4367129802704 and batch: 800, loss is 4.323884792327881 and perplexity is 75.48128859254138
At time: 627.0274963378906 and batch: 850, loss is 4.361302452087402 and perplexity is 78.35912699696992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.603930791219075 and perplexity of 99.87613729634732
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 631.1158730983734 and batch: 50, loss is 4.360136394500732 and perplexity is 78.26780899382246
At time: 632.7427458763123 and batch: 100, loss is 4.334815378189087 and perplexity is 76.31086893833582
At time: 634.3233270645142 and batch: 150, loss is 4.332032203674316 and perplexity is 76.09877775300346
At time: 635.9045314788818 and batch: 200, loss is 4.374127464294434 and perplexity is 79.37055568652646
At time: 637.4844846725464 and batch: 250, loss is 4.367665319442749 and perplexity is 78.85930532478883
At time: 639.0919494628906 and batch: 300, loss is 4.344163265228271 and perplexity is 77.02755887042875
At time: 640.6725673675537 and batch: 350, loss is 4.3013875198364255 and perplexity is 73.80212460195553
At time: 642.2594699859619 and batch: 400, loss is 4.327200040817261 and perplexity is 75.73194308190693
At time: 643.8446471691132 and batch: 450, loss is 4.353809413909912 and perplexity is 77.77417334373428
At time: 645.4294497966766 and batch: 500, loss is 4.335982942581177 and perplexity is 76.40001882562717
At time: 647.0190756320953 and batch: 550, loss is 4.339019317626953 and perplexity is 76.6323504808063
At time: 648.6027891635895 and batch: 600, loss is 4.37277193069458 and perplexity is 79.2630391190774
At time: 650.1914331912994 and batch: 650, loss is 4.355907669067383 and perplexity is 77.937534731072
At time: 651.7739019393921 and batch: 700, loss is 4.3087160110473635 and perplexity is 74.34496951098622
At time: 653.360386133194 and batch: 750, loss is 4.293496799468994 and perplexity is 73.22206423100195
At time: 654.9464778900146 and batch: 800, loss is 4.2433146429061885 and perplexity is 69.63829579120457
At time: 656.5367307662964 and batch: 850, loss is 4.275254554748535 and perplexity is 71.89843906949227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.576637903849284 and perplexity of 97.18709195677918
Finished 22 epochs...
Completing Train Step...
At time: 660.6863243579865 and batch: 50, loss is 4.338259744644165 and perplexity is 76.57416471873749
At time: 662.2687587738037 and batch: 100, loss is 4.308096733093262 and perplexity is 74.29894356326798
At time: 663.8518741130829 and batch: 150, loss is 4.303355436325074 and perplexity is 73.94750402021661
At time: 665.4400773048401 and batch: 200, loss is 4.34563157081604 and perplexity is 77.1407419388618
At time: 667.0246307849884 and batch: 250, loss is 4.342477407455444 and perplexity is 76.89781076074469
At time: 668.6077218055725 and batch: 300, loss is 4.32135178565979 and perplexity is 75.29033592950267
At time: 670.1912910938263 and batch: 350, loss is 4.279542865753174 and perplexity is 72.20742397514671
At time: 671.7771611213684 and batch: 400, loss is 4.306685838699341 and perplexity is 74.19418951614641
At time: 673.364307641983 and batch: 450, loss is 4.33527982711792 and perplexity is 76.3463196715628
At time: 674.9479143619537 and batch: 500, loss is 4.319187231063843 and perplexity is 75.12754213842187
At time: 676.5349612236023 and batch: 550, loss is 4.3252525806427 and perplexity is 75.58460165615942
At time: 678.1204307079315 and batch: 600, loss is 4.362858972549438 and perplexity is 78.48118955331731
At time: 679.7530632019043 and batch: 650, loss is 4.3490976715087895 and perplexity is 77.4085834325781
At time: 681.338873386383 and batch: 700, loss is 4.304616708755493 and perplexity is 74.04083081121779
At time: 682.9212172031403 and batch: 750, loss is 4.292570600509643 and perplexity is 73.15427742818866
At time: 684.5036528110504 and batch: 800, loss is 4.246295404434204 and perplexity is 69.84618061780193
At time: 686.0879302024841 and batch: 850, loss is 4.281981029510498 and perplexity is 72.38369229748918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.57504940032959 and perplexity of 97.03283247244137
Finished 23 epochs...
Completing Train Step...
At time: 690.1986391544342 and batch: 50, loss is 4.327668190002441 and perplexity is 75.76740522949294
At time: 691.7843329906464 and batch: 100, loss is 4.296323976516724 and perplexity is 73.42936887576904
At time: 693.3723847866058 and batch: 150, loss is 4.290530047416687 and perplexity is 73.00515443955133
At time: 694.9653553962708 and batch: 200, loss is 4.333632125854492 and perplexity is 76.22062732433861
At time: 696.559736251831 and batch: 250, loss is 4.331098155975342 and perplexity is 76.0277310504184
At time: 698.1479256153107 and batch: 300, loss is 4.310982961654663 and perplexity is 74.5136970610213
At time: 699.7368550300598 and batch: 350, loss is 4.269201030731201 and perplexity is 71.46451485221257
At time: 701.345965385437 and batch: 400, loss is 4.297429943084717 and perplexity is 73.51062422742335
At time: 702.9583086967468 and batch: 450, loss is 4.326888837814331 and perplexity is 75.70837874063919
At time: 704.5541536808014 and batch: 500, loss is 4.311390552520752 and perplexity is 74.5440743536749
At time: 706.1437537670135 and batch: 550, loss is 4.318465023040772 and perplexity is 75.07330401269813
At time: 707.7317957878113 and batch: 600, loss is 4.357722053527832 and perplexity is 78.07907174542525
At time: 709.3200731277466 and batch: 650, loss is 4.345005950927734 and perplexity is 77.09249624981112
At time: 710.9084837436676 and batch: 700, loss is 4.301595697402954 and perplexity is 73.81749014798528
At time: 712.4980871677399 and batch: 750, loss is 4.290809030532837 and perplexity is 73.02552448634934
At time: 714.0864434242249 and batch: 800, loss is 4.2462148761749265 and perplexity is 69.84055625292278
At time: 715.6739103794098 and batch: 850, loss is 4.283191604614258 and perplexity is 72.47137125357565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.574588775634766 and perplexity of 96.98814704599137
Finished 24 epochs...
Completing Train Step...
At time: 719.7909119129181 and batch: 50, loss is 4.319108114242554 and perplexity is 75.12159852121978
At time: 721.3787045478821 and batch: 100, loss is 4.287580051422119 and perplexity is 72.7901068770497
At time: 722.9627079963684 and batch: 150, loss is 4.281350221633911 and perplexity is 72.33804649263169
At time: 724.5473611354828 and batch: 200, loss is 4.325298738479614 and perplexity is 75.58809055839538
At time: 726.1561686992645 and batch: 250, loss is 4.322608642578125 and perplexity is 75.38502460169092
At time: 727.7532179355621 and batch: 300, loss is 4.303373584747314 and perplexity is 73.94884606292115
At time: 729.3377494812012 and batch: 350, loss is 4.261623964309693 and perplexity is 70.92506976291241
At time: 730.9211046695709 and batch: 400, loss is 4.290794324874878 and perplexity is 73.02445060586005
At time: 732.505809545517 and batch: 450, loss is 4.320569972991944 and perplexity is 75.23149599500677
At time: 734.0883095264435 and batch: 500, loss is 4.305517501831055 and perplexity is 74.10755632735095
At time: 735.6709575653076 and batch: 550, loss is 4.313413181304932 and perplexity is 74.69500192792661
At time: 737.254861831665 and batch: 600, loss is 4.353440008163452 and perplexity is 77.745448423077
At time: 738.8365468978882 and batch: 650, loss is 4.341373996734619 and perplexity is 76.81300768685784
At time: 740.419769525528 and batch: 700, loss is 4.298508729934692 and perplexity is 73.58996931269563
At time: 742.0032167434692 and batch: 750, loss is 4.288439502716065 and perplexity is 72.85269331973869
At time: 743.5876982212067 and batch: 800, loss is 4.244767765998841 and perplexity is 69.73956236553128
At time: 745.1708564758301 and batch: 850, loss is 4.282260856628418 and perplexity is 72.40395005169147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.574329694112142 and perplexity of 96.96302246397623
Finished 25 epochs...
Completing Train Step...
At time: 749.2604653835297 and batch: 50, loss is 4.31179573059082 and perplexity is 74.5742840976061
At time: 750.8662066459656 and batch: 100, loss is 4.280443334579468 and perplexity is 72.27247379273432
At time: 752.4479427337646 and batch: 150, loss is 4.273738503456116 and perplexity is 71.78951993237905
At time: 754.0304737091064 and batch: 200, loss is 4.318415670394898 and perplexity is 75.06959903793651
At time: 755.6139183044434 and batch: 250, loss is 4.315666751861572 and perplexity is 74.86352219984079
At time: 757.1963720321655 and batch: 300, loss is 4.2970826530456545 and perplexity is 73.48509915242258
At time: 758.7783353328705 and batch: 350, loss is 4.255332279205322 and perplexity is 70.48023241363822
At time: 760.4056398868561 and batch: 400, loss is 4.28507285118103 and perplexity is 72.60783609369496
At time: 761.9876911640167 and batch: 450, loss is 4.315274753570557 and perplexity is 74.83418157819152
At time: 763.5701916217804 and batch: 500, loss is 4.300375633239746 and perplexity is 73.72748299205551
At time: 765.1515717506409 and batch: 550, loss is 4.308879356384278 and perplexity is 74.35711440695793
At time: 766.7346615791321 and batch: 600, loss is 4.349499521255493 and perplexity is 77.43969630315405
At time: 768.3167765140533 and batch: 650, loss is 4.337827844619751 and perplexity is 76.54109947607954
At time: 769.9002285003662 and batch: 700, loss is 4.295402250289917 and perplexity is 73.36171828311069
At time: 771.4870734214783 and batch: 750, loss is 4.28565224647522 and perplexity is 72.6499169217886
At time: 773.0707056522369 and batch: 800, loss is 4.242707505226135 and perplexity is 69.59602859015857
At time: 774.6550045013428 and batch: 850, loss is 4.280572605133057 and perplexity is 72.28181709932493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.574198404947917 and perplexity of 96.95029310542795
Finished 26 epochs...
Completing Train Step...
At time: 778.7407546043396 and batch: 50, loss is 4.305201005935669 and perplexity is 74.08410530122877
At time: 780.3488435745239 and batch: 100, loss is 4.2742006206512455 and perplexity is 71.82270277055945
At time: 781.9325213432312 and batch: 150, loss is 4.267135334014893 and perplexity is 71.31704320683629
At time: 783.516185760498 and batch: 200, loss is 4.3123266315460205 and perplexity is 74.61388616772517
At time: 785.0985989570618 and batch: 250, loss is 4.309563798904419 and perplexity is 74.40802499843234
At time: 786.6815407276154 and batch: 300, loss is 4.2914605236053465 and perplexity is 73.07311561062915
At time: 788.2637021541595 and batch: 350, loss is 4.249666557312012 and perplexity is 70.08204010642034
At time: 789.8455839157104 and batch: 400, loss is 4.280005540847778 and perplexity is 72.2408402817158
At time: 791.4282166957855 and batch: 450, loss is 4.310744905471802 and perplexity is 74.49596072593401
At time: 793.0172345638275 and batch: 500, loss is 4.295758991241455 and perplexity is 73.38789408100942
At time: 794.6323790550232 and batch: 550, loss is 4.304640474319458 and perplexity is 74.04259045422788
At time: 796.2284729480743 and batch: 600, loss is 4.345691537857055 and perplexity is 77.14536797960122
At time: 797.8086931705475 and batch: 650, loss is 4.334245586395264 and perplexity is 76.26740001672871
At time: 799.4034624099731 and batch: 700, loss is 4.292079620361328 and perplexity is 73.11836894610433
At time: 801.0669686794281 and batch: 750, loss is 4.282817811965942 and perplexity is 72.44428705004115
At time: 802.6507332324982 and batch: 800, loss is 4.240157198905945 and perplexity is 69.4187635347591
At time: 804.2346193790436 and batch: 850, loss is 4.278281421661377 and perplexity is 72.1163957723543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.574036280314128 and perplexity of 96.93457634873363
Finished 27 epochs...
Completing Train Step...
At time: 808.378340959549 and batch: 50, loss is 4.299127893447876 and perplexity is 73.63554764539457
At time: 809.9976813793182 and batch: 100, loss is 4.268472547531128 and perplexity is 71.41247311180963
At time: 811.5841221809387 and batch: 150, loss is 4.261227979660034 and perplexity is 70.89699008393816
At time: 813.1708626747131 and batch: 200, loss is 4.307027063369751 and perplexity is 74.21951072387509
At time: 814.761313199997 and batch: 250, loss is 4.304096937179565 and perplexity is 74.00235649169912
At time: 816.3495779037476 and batch: 300, loss is 4.286220350265503 and perplexity is 72.69120134076273
At time: 817.9374260902405 and batch: 350, loss is 4.244498872756958 and perplexity is 69.72081238950325
At time: 819.5312466621399 and batch: 400, loss is 4.275397806167603 and perplexity is 71.90873936066575
At time: 821.1203544139862 and batch: 450, loss is 4.305966501235962 and perplexity is 74.1408380472181
At time: 822.7085621356964 and batch: 500, loss is 4.291306200027466 and perplexity is 73.06183957608263
At time: 824.2971978187561 and batch: 550, loss is 4.300595121383667 and perplexity is 73.74366707649578
At time: 825.8907651901245 and batch: 600, loss is 4.342005395889283 and perplexity is 76.86152266952622
At time: 827.4783990383148 and batch: 650, loss is 4.330700216293335 and perplexity is 75.99748261822536
At time: 829.0657505989075 and batch: 700, loss is 4.288742570877075 and perplexity is 72.87477599763706
At time: 830.6522982120514 and batch: 750, loss is 4.279633321762085 and perplexity is 72.21395586595315
At time: 832.2488377094269 and batch: 800, loss is 4.237443451881409 and perplexity is 69.2306339552684
At time: 833.8459348678589 and batch: 850, loss is 4.275714235305786 and perplexity is 71.93149698148679
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.573952992757161 and perplexity of 96.92650324088332
Finished 28 epochs...
Completing Train Step...
At time: 837.960841178894 and batch: 50, loss is 4.2935083293914795 and perplexity is 73.2229084805938
At time: 839.5500593185425 and batch: 100, loss is 4.263148727416993 and perplexity is 71.03329618152675
At time: 841.1593561172485 and batch: 150, loss is 4.2558774518966676 and perplexity is 70.51866678736336
At time: 842.7406523227692 and batch: 200, loss is 4.30201057434082 and perplexity is 73.84812167597677
At time: 844.322705745697 and batch: 250, loss is 4.2989046764373775 and perplexity is 73.61911277292253
At time: 845.9054825305939 and batch: 300, loss is 4.281368150711059 and perplexity is 72.33934345867468
At time: 847.4882595539093 and batch: 350, loss is 4.239654951095581 and perplexity is 69.38390686685086
At time: 849.0688672065735 and batch: 400, loss is 4.270745496749878 and perplexity is 71.57497464568443
At time: 850.6508991718292 and batch: 450, loss is 4.301640396118164 and perplexity is 73.82078976869877
At time: 852.2316796779633 and batch: 500, loss is 4.287122211456299 and perplexity is 72.75678828487861
At time: 853.812343120575 and batch: 550, loss is 4.296641359329223 and perplexity is 73.45267779411229
At time: 855.3946318626404 and batch: 600, loss is 4.338408718109131 and perplexity is 76.5855730871335
At time: 856.975245475769 and batch: 650, loss is 4.327218427658081 and perplexity is 75.733335565891
At time: 858.5559768676758 and batch: 700, loss is 4.285359897613525 and perplexity is 72.62868090558817
At time: 860.1363530158997 and batch: 750, loss is 4.276654443740845 and perplexity is 71.99915938509206
At time: 861.7169573307037 and batch: 800, loss is 4.234563531875611 and perplexity is 69.03154208945486
At time: 863.2992472648621 and batch: 850, loss is 4.273004999160767 and perplexity is 71.73688131883227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.573837598164876 and perplexity of 96.91531909186777
Finished 29 epochs...
Completing Train Step...
At time: 867.4288175106049 and batch: 50, loss is 4.288148746490479 and perplexity is 72.83151402476042
At time: 869.0121896266937 and batch: 100, loss is 4.258122835159302 and perplexity is 70.67718612320157
At time: 870.5956614017487 and batch: 150, loss is 4.2506594514846805 and perplexity is 70.15165871188516
At time: 872.178986787796 and batch: 200, loss is 4.297069263458252 and perplexity is 73.48411522385192
At time: 873.7616667747498 and batch: 250, loss is 4.294208812713623 and perplexity is 73.27421787537291
At time: 875.3448143005371 and batch: 300, loss is 4.276806917190552 and perplexity is 72.01013818226576
At time: 876.9280979633331 and batch: 350, loss is 4.23497049331665 and perplexity is 69.05964098249596
At time: 878.5185952186584 and batch: 400, loss is 4.2662403106689455 and perplexity is 71.2532413445395
At time: 880.1401474475861 and batch: 450, loss is 4.297475967407227 and perplexity is 73.51400758195837
At time: 881.7227363586426 and batch: 500, loss is 4.2830760955810545 and perplexity is 72.46300063899734
At time: 883.304666519165 and batch: 550, loss is 4.292884016036988 and perplexity is 73.17720870794817
At time: 884.8871331214905 and batch: 600, loss is 4.334854764938354 and perplexity is 76.31387463458898
At time: 886.4705369472504 and batch: 650, loss is 4.3236918449401855 and perplexity is 75.4667260800321
At time: 888.0583944320679 and batch: 700, loss is 4.281889820098877 and perplexity is 72.37709052458027
At time: 889.6411955356598 and batch: 750, loss is 4.273496732711792 and perplexity is 71.77216542470103
At time: 891.2240693569183 and batch: 800, loss is 4.231480588912964 and perplexity is 68.8190495021365
At time: 892.8077237606049 and batch: 850, loss is 4.270047273635864 and perplexity is 71.52501678689731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.573673566182454 and perplexity of 96.8994231837044
Finished 30 epochs...
Completing Train Step...
At time: 896.9057030677795 and batch: 50, loss is 4.282973346710205 and perplexity is 72.45555552999812
At time: 898.4934074878693 and batch: 100, loss is 4.253276109695435 and perplexity is 70.33546199596431
At time: 900.081169128418 and batch: 150, loss is 4.245685448646546 and perplexity is 69.8035905260452
At time: 901.6697306632996 and batch: 200, loss is 4.292514963150024 and perplexity is 73.15020743057073
At time: 903.2657408714294 and batch: 250, loss is 4.289617900848389 and perplexity is 72.93859339978816
At time: 904.8533425331116 and batch: 300, loss is 4.272252950668335 and perplexity is 71.68295198666294
At time: 906.4428114891052 and batch: 350, loss is 4.230382709503174 and perplexity is 68.74353594466169
At time: 908.0377147197723 and batch: 400, loss is 4.262072658538818 and perplexity is 70.95690057303172
At time: 909.6240568161011 and batch: 450, loss is 4.293355388641357 and perplexity is 73.21171057037488
At time: 911.21009516716 and batch: 500, loss is 4.279194316864014 and perplexity is 72.18226054332888
At time: 912.8246388435364 and batch: 550, loss is 4.289068784713745 and perplexity is 72.898552635835
At time: 914.4218277931213 and batch: 600, loss is 4.3315700435638425 and perplexity is 76.06361605926219
At time: 916.0090909004211 and batch: 650, loss is 4.320293960571289 and perplexity is 75.21073403309917
At time: 917.6146726608276 and batch: 700, loss is 4.278551292419434 and perplexity is 72.13586050511267
At time: 919.2504777908325 and batch: 750, loss is 4.270285596847534 and perplexity is 71.54206489001139
At time: 920.9373824596405 and batch: 800, loss is 4.228442258834839 and perplexity is 68.61027184241777
At time: 922.5492634773254 and batch: 850, loss is 4.266980562210083 and perplexity is 71.30600619347676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.573713938395183 and perplexity of 96.90333530680044
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 926.9959721565247 and batch: 50, loss is 4.288099164962769 and perplexity is 72.82790301655002
At time: 928.6361134052277 and batch: 100, loss is 4.2701571083068846 and perplexity is 71.53287314502819
At time: 930.2449448108673 and batch: 150, loss is 4.262908411026001 and perplexity is 71.01622776714021
At time: 931.8546743392944 and batch: 200, loss is 4.31408392906189 and perplexity is 74.74512023941922
At time: 933.462651014328 and batch: 250, loss is 4.308159704208374 and perplexity is 74.30362239791002
At time: 935.0714163780212 and batch: 300, loss is 4.287035007476806 and perplexity is 72.75044388003776
At time: 936.673677444458 and batch: 350, loss is 4.241951398849487 and perplexity is 69.5434264780544
At time: 938.2576494216919 and batch: 400, loss is 4.268547391891479 and perplexity is 71.41781813270062
At time: 939.8421332836151 and batch: 450, loss is 4.299784078598022 and perplexity is 73.68388205470886
At time: 941.4263367652893 and batch: 500, loss is 4.281289978027344 and perplexity is 72.33368871908425
At time: 943.0098972320557 and batch: 550, loss is 4.288300771713256 and perplexity is 72.84258709357681
At time: 944.5936963558197 and batch: 600, loss is 4.326615476608277 and perplexity is 75.68768583536585
At time: 946.1810483932495 and batch: 650, loss is 4.312441072463989 and perplexity is 74.62242553796878
At time: 947.77676653862 and batch: 700, loss is 4.268625497817993 and perplexity is 71.42339650540463
At time: 949.3665060997009 and batch: 750, loss is 4.258998260498047 and perplexity is 70.73908581313037
At time: 950.9554047584534 and batch: 800, loss is 4.214788537025452 and perplexity is 67.6798525746767
At time: 952.5464835166931 and batch: 850, loss is 4.250034475326538 and perplexity is 70.10782929533293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.563740094502767 and perplexity of 95.94164043728816
Finished 32 epochs...
Completing Train Step...
At time: 956.7044954299927 and batch: 50, loss is 4.284801378250122 and perplexity is 72.58812770689148
At time: 958.4375200271606 and batch: 100, loss is 4.262015371322632 and perplexity is 70.95283576616055
At time: 960.0206167697906 and batch: 150, loss is 4.252491784095764 and perplexity is 70.28031772091644
At time: 961.6300649642944 and batch: 200, loss is 4.302507228851319 and perplexity is 73.8848077881064
At time: 963.2127075195312 and batch: 250, loss is 4.296821546554566 and perplexity is 73.46591422079966
At time: 964.7991900444031 and batch: 300, loss is 4.276786851882934 and perplexity is 72.00869329118771
At time: 966.3865416049957 and batch: 350, loss is 4.233036222457886 and perplexity is 68.92619003817097
At time: 968.0054976940155 and batch: 400, loss is 4.260806102752685 and perplexity is 70.86708658926166
At time: 969.6188015937805 and batch: 450, loss is 4.2938501739501955 and perplexity is 73.24794361224923
At time: 971.2307970523834 and batch: 500, loss is 4.276379070281982 and perplexity is 71.97933545715408
At time: 972.8434875011444 and batch: 550, loss is 4.284885692596435 and perplexity is 72.59424818544767
At time: 974.4565081596375 and batch: 600, loss is 4.325036096572876 and perplexity is 75.56824056499855
At time: 976.0759744644165 and batch: 650, loss is 4.312017230987549 and perplexity is 74.59080416065223
At time: 977.68639087677 and batch: 700, loss is 4.269539375305175 and perplexity is 71.48869857402933
At time: 979.27294921875 and batch: 750, loss is 4.260669956207275 and perplexity is 70.85743893700143
At time: 980.861513376236 and batch: 800, loss is 4.217819800376892 and perplexity is 67.88531928594456
At time: 982.4471261501312 and batch: 850, loss is 4.253710489273072 and perplexity is 70.36602092084497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.56284777323405 and perplexity of 95.85606785576564
Finished 33 epochs...
Completing Train Step...
At time: 986.5778019428253 and batch: 50, loss is 4.282720127105713 and perplexity is 72.43721068561872
At time: 988.1954956054688 and batch: 100, loss is 4.259170064926147 and perplexity is 70.75124014536696
At time: 989.8086512088776 and batch: 150, loss is 4.248913979530334 and perplexity is 70.02931776146266
At time: 991.4210133552551 and batch: 200, loss is 4.298570718765259 and perplexity is 73.59453121022662
At time: 993.0342628955841 and batch: 250, loss is 4.292919692993164 and perplexity is 73.17981949458851
At time: 994.6491360664368 and batch: 300, loss is 4.273153505325317 and perplexity is 71.7475354790184
At time: 996.2631914615631 and batch: 350, loss is 4.229701662063599 and perplexity is 68.69673427440708
At time: 997.8775148391724 and batch: 400, loss is 4.257893295288086 and perplexity is 70.66096475279778
At time: 999.4908313751221 and batch: 450, loss is 4.291492300033569 and perplexity is 73.07543765013523
At time: 1001.1149067878723 and batch: 500, loss is 4.274411249160766 and perplexity is 71.83783227269008
At time: 1002.7869319915771 and batch: 550, loss is 4.283578844070434 and perplexity is 72.49944046237007
At time: 1004.3966324329376 and batch: 600, loss is 4.324457530975342 and perplexity is 75.52453202608918
At time: 1006.0063307285309 and batch: 650, loss is 4.311947317123413 and perplexity is 74.58558941159808
At time: 1007.6139073371887 and batch: 700, loss is 4.269948310852051 and perplexity is 71.51793882235711
At time: 1009.2225360870361 and batch: 750, loss is 4.261437578201294 and perplexity is 70.91185154705556
At time: 1010.8297646045685 and batch: 800, loss is 4.219008464813232 and perplexity is 67.96606012810643
At time: 1012.4430396556854 and batch: 850, loss is 4.255103254318238 and perplexity is 70.46409253465552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5625565846761065 and perplexity of 95.82815972905739
Finished 34 epochs...
Completing Train Step...
At time: 1016.89377784729 and batch: 50, loss is 4.280894165039062 and perplexity is 72.30506377103578
At time: 1018.5029819011688 and batch: 100, loss is 4.257099132537842 and perplexity is 70.60487072353729
At time: 1020.112478017807 and batch: 150, loss is 4.246502633094788 and perplexity is 69.86065624808917
At time: 1021.7210824489594 and batch: 200, loss is 4.296071901321411 and perplexity is 73.41086148599294
At time: 1023.3324840068817 and batch: 250, loss is 4.2905466270446775 and perplexity is 73.00636484788735
At time: 1024.9444563388824 and batch: 300, loss is 4.270841245651245 and perplexity is 71.58182819897698
At time: 1026.5563263893127 and batch: 350, loss is 4.227574882507324 and perplexity is 68.55078671852085
At time: 1028.1677551269531 and batch: 400, loss is 4.256055212020874 and perplexity is 70.5312033085441
At time: 1029.7681667804718 and batch: 450, loss is 4.28995945930481 and perplexity is 72.96351044823538
At time: 1031.3772792816162 and batch: 500, loss is 4.27309434890747 and perplexity is 71.74329127736723
At time: 1032.9786438941956 and batch: 550, loss is 4.282648830413819 and perplexity is 72.43204633622949
At time: 1034.5870361328125 and batch: 600, loss is 4.323958616256714 and perplexity is 75.4868611235092
At time: 1036.1976385116577 and batch: 650, loss is 4.311726427078247 and perplexity is 74.56911601685565
At time: 1037.8072724342346 and batch: 700, loss is 4.269981832504272 and perplexity is 71.52033626201273
At time: 1039.4164669513702 and batch: 750, loss is 4.261656427383423 and perplexity is 70.9273722460541
At time: 1041.0261883735657 and batch: 800, loss is 4.219492230415344 and perplexity is 67.99894772441004
At time: 1042.6400949954987 and batch: 850, loss is 4.25571159362793 and perplexity is 70.50697165327047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562442461649577 and perplexity of 95.81722415345473
Finished 35 epochs...
Completing Train Step...
At time: 1046.9430258274078 and batch: 50, loss is 4.2792614078521725 and perplexity is 72.18710348497332
At time: 1048.547373533249 and batch: 100, loss is 4.255378837585449 and perplexity is 70.48351393548096
At time: 1050.1522006988525 and batch: 150, loss is 4.244585032463074 and perplexity is 69.72681977300235
At time: 1051.7582862377167 and batch: 200, loss is 4.294185228347779 and perplexity is 73.27248976978981
At time: 1053.3639588356018 and batch: 250, loss is 4.288727130889892 and perplexity is 72.87365082071607
At time: 1054.9747734069824 and batch: 300, loss is 4.269143915176391 and perplexity is 71.46043323336063
At time: 1056.5795702934265 and batch: 350, loss is 4.225971765518189 and perplexity is 68.4409798278766
At time: 1058.184399843216 and batch: 400, loss is 4.254644021987915 and perplexity is 70.43174057433842
At time: 1059.788239479065 and batch: 450, loss is 4.2887797355651855 and perplexity is 72.87748441628655
At time: 1061.3865852355957 and batch: 500, loss is 4.2720292377471925 and perplexity is 71.66691737771934
At time: 1062.9709763526917 and batch: 550, loss is 4.281826791763305 and perplexity is 72.3725288607895
At time: 1064.5525386333466 and batch: 600, loss is 4.323415803909302 and perplexity is 75.44589704213433
At time: 1066.1349840164185 and batch: 650, loss is 4.311376733779907 and perplexity is 74.54304425554723
At time: 1067.7176835536957 and batch: 700, loss is 4.269784326553345 and perplexity is 71.50621196485095
At time: 1069.2995553016663 and batch: 750, loss is 4.261588735580444 and perplexity is 70.92257120684354
At time: 1070.881447315216 and batch: 800, loss is 4.219615669250488 and perplexity is 68.0073419533844
At time: 1072.4635736942291 and batch: 850, loss is 4.2559393119812015 and perplexity is 70.52302921298066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562384923299153 and perplexity of 95.81171114704092
Finished 36 epochs...
Completing Train Step...
At time: 1076.6914422512054 and batch: 50, loss is 4.277764520645142 and perplexity is 72.07912836670205
At time: 1078.3204565048218 and batch: 100, loss is 4.25386791229248 and perplexity is 70.37709902427346
At time: 1079.9267461299896 and batch: 150, loss is 4.242955131530762 and perplexity is 69.6132645314832
At time: 1081.5347216129303 and batch: 200, loss is 4.292605600357056 and perplexity is 73.15683786154334
At time: 1083.1425850391388 and batch: 250, loss is 4.287211589813232 and perplexity is 72.76329145768838
At time: 1084.7797527313232 and batch: 300, loss is 4.267723808288574 and perplexity is 71.35902380307748
At time: 1086.3914341926575 and batch: 350, loss is 4.224613885879517 and perplexity is 68.34810828337724
At time: 1087.9977197647095 and batch: 400, loss is 4.253436355590821 and perplexity is 70.34673386816074
At time: 1089.604819059372 and batch: 450, loss is 4.287756109237671 and perplexity is 72.80292327244045
At time: 1091.2114989757538 and batch: 500, loss is 4.271076612472534 and perplexity is 71.59867816922639
At time: 1092.81969332695 and batch: 550, loss is 4.281046800613403 and perplexity is 72.31610093828296
At time: 1094.4256477355957 and batch: 600, loss is 4.322848749160767 and perplexity is 75.40312721552182
At time: 1096.0340449810028 and batch: 650, loss is 4.310949935913086 and perplexity is 74.51123623155381
At time: 1097.6316921710968 and batch: 700, loss is 4.269459190368653 and perplexity is 71.48296648708866
At time: 1099.2177209854126 and batch: 750, loss is 4.261357021331787 and perplexity is 70.90613934036585
At time: 1100.8071854114532 and batch: 800, loss is 4.2195210647583 and perplexity is 68.00090845765746
At time: 1102.3913650512695 and batch: 850, loss is 4.255927515029907 and perplexity is 70.52219726114718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562338193257649 and perplexity of 95.8072339664127
Finished 37 epochs...
Completing Train Step...
At time: 1106.6397457122803 and batch: 50, loss is 4.276355628967285 and perplexity is 71.9776481866759
At time: 1108.2869141101837 and batch: 100, loss is 4.252491426467896 and perplexity is 70.2802925867207
At time: 1109.8887794017792 and batch: 150, loss is 4.241497054100036 and perplexity is 69.51183696418359
At time: 1111.496473312378 and batch: 200, loss is 4.291208639144897 and perplexity is 73.05471194622605
At time: 1113.103012561798 and batch: 250, loss is 4.2858590888977055 and perplexity is 72.66494556082458
At time: 1114.6906487941742 and batch: 300, loss is 4.266462059020996 and perplexity is 71.26904338535941
At time: 1116.278615951538 and batch: 350, loss is 4.223392276763916 and perplexity is 68.2646645894299
At time: 1117.8679163455963 and batch: 400, loss is 4.2523470211029055 and perplexity is 70.27014446815713
At time: 1119.4574584960938 and batch: 450, loss is 4.286826658248901 and perplexity is 72.73528796014254
At time: 1121.0502452850342 and batch: 500, loss is 4.2701866149902346 and perplexity is 71.53498387400535
At time: 1122.660793542862 and batch: 550, loss is 4.280290517807007 and perplexity is 72.26143019039685
At time: 1124.3295097351074 and batch: 600, loss is 4.322262945175171 and perplexity is 75.35896869845307
At time: 1125.9495944976807 and batch: 650, loss is 4.310469875335693 and perplexity is 74.47547490895364
At time: 1127.561811208725 and batch: 700, loss is 4.269052543640137 and perplexity is 71.45390408208851
At time: 1129.1746830940247 and batch: 750, loss is 4.261027383804321 and perplexity is 70.8827698678507
At time: 1130.7871496677399 and batch: 800, loss is 4.219303479194641 and perplexity is 67.98611405124437
At time: 1132.4004216194153 and batch: 850, loss is 4.2557972717285155 and perplexity is 70.51301281547373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562315940856934 and perplexity of 95.80510204917123
Finished 38 epochs...
Completing Train Step...
At time: 1136.7187044620514 and batch: 50, loss is 4.275030641555786 and perplexity is 71.88234186270229
At time: 1138.366531610489 and batch: 100, loss is 4.251213083267212 and perplexity is 70.19050765275725
At time: 1139.973718881607 and batch: 150, loss is 4.240172033309936 and perplexity is 69.41979332838012
At time: 1141.580106496811 and batch: 200, loss is 4.289939489364624 and perplexity is 72.96205338584475
At time: 1143.1880478858948 and batch: 250, loss is 4.284637088775635 and perplexity is 72.5762032210971
At time: 1144.795933008194 and batch: 300, loss is 4.265306310653687 and perplexity is 71.18672188546009
At time: 1146.401995897293 and batch: 350, loss is 4.2222722721099855 and perplexity is 68.18825064736394
At time: 1148.0003974437714 and batch: 400, loss is 4.251263570785523 and perplexity is 70.19405148675655
At time: 1149.5828702449799 and batch: 450, loss is 4.285950050354004 and perplexity is 72.67155557071816
At time: 1151.1659603118896 and batch: 500, loss is 4.2693359375 and perplexity is 71.47415654934554
At time: 1152.74938082695 and batch: 550, loss is 4.279547853469849 and perplexity is 72.20778412621745
At time: 1154.3333613872528 and batch: 600, loss is 4.321660003662109 and perplexity is 75.31354534303264
At time: 1155.918372631073 and batch: 650, loss is 4.3099532794952395 and perplexity is 74.43701112437084
At time: 1157.5038254261017 and batch: 700, loss is 4.268591661453247 and perplexity is 71.42097983819488
At time: 1159.0903346538544 and batch: 750, loss is 4.2606198024749755 and perplexity is 70.85388526109334
At time: 1160.6756763458252 and batch: 800, loss is 4.2190214538574216 and perplexity is 67.9669429479983
At time: 1162.27539396286 and batch: 850, loss is 4.255579061508179 and perplexity is 70.49762783405181
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5622866948445635 and perplexity of 95.80230017294363
Finished 39 epochs...
Completing Train Step...
At time: 1166.6948461532593 and batch: 50, loss is 4.27375168800354 and perplexity is 71.79046645094888
At time: 1168.3050525188446 and batch: 100, loss is 4.250001630783081 and perplexity is 70.10552667350142
At time: 1169.9120242595673 and batch: 150, loss is 4.238923544883728 and perplexity is 69.33317760047761
At time: 1171.520145893097 and batch: 200, loss is 4.288748645782471 and perplexity is 72.87521870635167
At time: 1173.1286976337433 and batch: 250, loss is 4.283482465744019 and perplexity is 72.49245342433717
At time: 1174.737987279892 and batch: 300, loss is 4.264213771820068 and perplexity is 71.10899009759264
At time: 1176.3480525016785 and batch: 350, loss is 4.22122022151947 and perplexity is 68.11655088050838
At time: 1177.9594552516937 and batch: 400, loss is 4.250255870819092 and perplexity is 70.12335257105984
At time: 1179.568405866623 and batch: 450, loss is 4.28512731552124 and perplexity is 72.6117907392745
At time: 1181.1736924648285 and batch: 500, loss is 4.268514490127563 and perplexity is 71.41546839916442
At time: 1182.7846608161926 and batch: 550, loss is 4.278809804916381 and perplexity is 72.15451093711232
At time: 1184.3891327381134 and batch: 600, loss is 4.3210447406768795 and perplexity is 75.26722195828995
At time: 1185.9925136566162 and batch: 650, loss is 4.309408884048462 and perplexity is 74.39649898275627
At time: 1187.5748763084412 and batch: 700, loss is 4.2680967426300045 and perplexity is 71.38564099654886
At time: 1189.1639697551727 and batch: 750, loss is 4.260179138183593 and perplexity is 70.82266936232419
At time: 1190.748253583908 and batch: 800, loss is 4.218660378456116 and perplexity is 67.94240618687488
At time: 1192.334813117981 and batch: 850, loss is 4.255291919708252 and perplexity is 70.47738792430611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562257130940755 and perplexity of 95.79946792482303
Finished 40 epochs...
Completing Train Step...
At time: 1196.7011740207672 and batch: 50, loss is 4.27251615524292 and perplexity is 71.70182175074078
At time: 1198.304626941681 and batch: 100, loss is 4.248838863372803 and perplexity is 70.02405762576095
At time: 1199.9131755828857 and batch: 150, loss is 4.23772023677826 and perplexity is 69.24979860126658
At time: 1201.5214624404907 and batch: 200, loss is 4.287603197097778 and perplexity is 72.79179167275242
At time: 1203.1307661533356 and batch: 250, loss is 4.282364072799683 and perplexity is 72.41142369589411
At time: 1204.7377846240997 and batch: 300, loss is 4.263172035217285 and perplexity is 71.03495183070297
At time: 1206.397842168808 and batch: 350, loss is 4.220208983421326 and perplexity is 68.0477036454843
At time: 1208.012135028839 and batch: 400, loss is 4.249331665039063 and perplexity is 70.05857410222723
At time: 1209.6210179328918 and batch: 450, loss is 4.28428427696228 and perplexity is 72.55060199570521
At time: 1211.2292544841766 and batch: 500, loss is 4.267703752517701 and perplexity is 71.35759265719774
At time: 1212.8373355865479 and batch: 550, loss is 4.2780841445922855 and perplexity is 72.10217026439229
At time: 1214.4456734657288 and batch: 600, loss is 4.320440454483032 and perplexity is 75.2217527548009
At time: 1216.0547845363617 and batch: 650, loss is 4.308860578536987 and perplexity is 74.35571815352799
At time: 1217.6638808250427 and batch: 700, loss is 4.267582292556763 and perplexity is 71.34892609311191
At time: 1219.2717518806458 and batch: 750, loss is 4.259701595306397 and perplexity is 70.78885657519656
At time: 1220.8806335926056 and batch: 800, loss is 4.21827844619751 and perplexity is 67.91646174504979
At time: 1222.4900772571564 and batch: 850, loss is 4.254969596862793 and perplexity is 70.45467511271546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562230110168457 and perplexity of 95.79687938418633
Finished 41 epochs...
Completing Train Step...
At time: 1226.8120865821838 and batch: 50, loss is 4.271331844329834 and perplexity is 71.61695476512128
At time: 1228.4213070869446 and batch: 100, loss is 4.247726583480835 and perplexity is 69.94621457416575
At time: 1230.016242980957 and batch: 150, loss is 4.236586365699768 and perplexity is 69.17132275659435
At time: 1231.604008436203 and batch: 200, loss is 4.286515760421753 and perplexity is 72.71267823199851
At time: 1233.1972737312317 and batch: 250, loss is 4.281305780410767 and perplexity is 72.33483177279926
At time: 1234.799483537674 and batch: 300, loss is 4.262166547775268 and perplexity is 70.96356297500643
At time: 1236.4118037223816 and batch: 350, loss is 4.219235677719116 and perplexity is 67.98150464866036
At time: 1238.024770975113 and batch: 400, loss is 4.248418455123901 and perplexity is 69.9946251215808
At time: 1239.6366860866547 and batch: 450, loss is 4.2834873962402344 and perplexity is 72.49281084898558
At time: 1241.251128911972 and batch: 500, loss is 4.266913299560547 and perplexity is 71.30121012387231
At time: 1242.863951921463 and batch: 550, loss is 4.27737042427063 and perplexity is 72.0507278401746
At time: 1244.4743173122406 and batch: 600, loss is 4.3198221397399905 and perplexity is 75.17525641224357
At time: 1246.0927753448486 and batch: 650, loss is 4.308290920257568 and perplexity is 74.3133728653768
At time: 1247.7351615428925 and batch: 700, loss is 4.267046928405762 and perplexity is 71.31073865887298
At time: 1249.348825931549 and batch: 750, loss is 4.25921085357666 and perplexity is 70.75412605183034
At time: 1250.963411808014 and batch: 800, loss is 4.217865395545959 and perplexity is 67.8884145991197
At time: 1252.575548171997 and batch: 850, loss is 4.254617748260498 and perplexity is 70.42989009429463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562215487162272 and perplexity of 95.79547855606882
Finished 42 epochs...
Completing Train Step...
At time: 1256.8582553863525 and batch: 50, loss is 4.270187768936157 and perplexity is 71.53506642155595
At time: 1258.4889647960663 and batch: 100, loss is 4.246654329299926 and perplexity is 69.87125464837872
At time: 1260.0935318470001 and batch: 150, loss is 4.2355022668838505 and perplexity is 69.09637484031245
At time: 1261.6998496055603 and batch: 200, loss is 4.28547101020813 and perplexity is 72.63675131512004
At time: 1263.3059928417206 and batch: 250, loss is 4.280298299789429 and perplexity is 72.26199252976444
At time: 1264.9106087684631 and batch: 300, loss is 4.261190929412842 and perplexity is 70.89436338159067
At time: 1266.5131223201752 and batch: 350, loss is 4.218298974037171 and perplexity is 67.91785593759667
At time: 1268.1092746257782 and batch: 400, loss is 4.247534446716308 and perplexity is 69.93277662580986
At time: 1269.7105340957642 and batch: 450, loss is 4.28270601272583 and perplexity is 72.43618828652475
At time: 1271.3167276382446 and batch: 500, loss is 4.266131267547608 and perplexity is 71.24547209229732
At time: 1272.9222526550293 and batch: 550, loss is 4.276658954620362 and perplexity is 71.99948416535788
At time: 1274.5286457538605 and batch: 600, loss is 4.319203863143921 and perplexity is 75.12879167610994
At time: 1276.1344735622406 and batch: 650, loss is 4.307716131210327 and perplexity is 74.2706706261556
At time: 1277.7399654388428 and batch: 700, loss is 4.266498785018921 and perplexity is 71.27166086016332
At time: 1279.3415656089783 and batch: 750, loss is 4.258691759109497 and perplexity is 70.71740750748896
At time: 1280.9242463111877 and batch: 800, loss is 4.2174235391616826 and perplexity is 67.85842429590205
At time: 1282.519073009491 and batch: 850, loss is 4.254229669570923 and perplexity is 70.40256305770318
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562184969584147 and perplexity of 95.79255515467575
Finished 43 epochs...
Completing Train Step...
At time: 1286.7522497177124 and batch: 50, loss is 4.269057540893555 and perplexity is 71.45426115624713
At time: 1288.37633395195 and batch: 100, loss is 4.24560962677002 and perplexity is 69.79829808746713
At time: 1289.9583704471588 and batch: 150, loss is 4.23443727016449 and perplexity is 69.02282659905727
At time: 1291.547729253769 and batch: 200, loss is 4.284445791244507 and perplexity is 72.56232090047145
At time: 1293.1299314498901 and batch: 250, loss is 4.279299755096435 and perplexity is 72.18987171453992
At time: 1294.716799736023 and batch: 300, loss is 4.260240859985352 and perplexity is 70.82704079998769
At time: 1296.2989521026611 and batch: 350, loss is 4.217375502586365 and perplexity is 67.85516468788327
At time: 1297.8830199241638 and batch: 400, loss is 4.246662063598633 and perplexity is 69.871795055623
At time: 1299.4658477306366 and batch: 450, loss is 4.281934146881103 and perplexity is 72.38029883921672
At time: 1301.0482070446014 and batch: 500, loss is 4.265358076095581 and perplexity is 71.1904069929553
At time: 1302.6327378749847 and batch: 550, loss is 4.275952472686767 and perplexity is 71.9486357944096
At time: 1304.2270045280457 and batch: 600, loss is 4.31857702255249 and perplexity is 75.08171265696387
At time: 1305.8251078128815 and batch: 650, loss is 4.307127885818481 and perplexity is 74.22699409392933
At time: 1307.4325950145721 and batch: 700, loss is 4.2659376525878905 and perplexity is 71.23167923838265
At time: 1309.0387499332428 and batch: 750, loss is 4.258169441223145 and perplexity is 70.6804801854113
At time: 1310.6438264846802 and batch: 800, loss is 4.216962537765503 and perplexity is 67.82714867716494
At time: 1312.2471332550049 and batch: 850, loss is 4.253827056884766 and perplexity is 70.37422379793372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562169710795085 and perplexity of 95.79109348743455
Finished 44 epochs...
Completing Train Step...
At time: 1316.540162563324 and batch: 50, loss is 4.2679691410064695 and perplexity is 71.37653265399263
At time: 1318.1529920101166 and batch: 100, loss is 4.244599113464355 and perplexity is 69.72780160335348
At time: 1319.761565208435 and batch: 150, loss is 4.233411684036255 and perplexity is 68.95207403319236
At time: 1321.3716564178467 and batch: 200, loss is 4.283459415435791 and perplexity is 72.49078247019965
At time: 1322.980664730072 and batch: 250, loss is 4.278339290618897 and perplexity is 72.12056919375236
At time: 1324.5905592441559 and batch: 300, loss is 4.259309749603272 and perplexity is 70.76112369977733
At time: 1326.198184967041 and batch: 350, loss is 4.216471877098083 and perplexity is 67.79387672641214
At time: 1327.8101439476013 and batch: 400, loss is 4.245814304351807 and perplexity is 69.81258569645928
At time: 1329.4545650482178 and batch: 450, loss is 4.2811807441711425 and perplexity is 72.32578786286057
At time: 1331.066767692566 and batch: 500, loss is 4.264598722457886 and perplexity is 71.13636881807513
At time: 1332.6788167953491 and batch: 550, loss is 4.275256109237671 and perplexity is 71.89855083492154
At time: 1334.290026664734 and batch: 600, loss is 4.3179527759552006 and perplexity is 75.0348577793396
At time: 1335.8941617012024 and batch: 650, loss is 4.30653678894043 and perplexity is 74.18313171418825
At time: 1337.5067675113678 and batch: 700, loss is 4.265368356704712 and perplexity is 71.19113887746553
At time: 1339.1193301677704 and batch: 750, loss is 4.257621355056763 and perplexity is 70.6417518062024
At time: 1340.7326791286469 and batch: 800, loss is 4.216502995491028 and perplexity is 67.79598639573192
At time: 1342.3453810214996 and batch: 850, loss is 4.253405656814575 and perplexity is 70.3445743426657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562140782674153 and perplexity of 95.78832247117832
Finished 45 epochs...
Completing Train Step...
At time: 1346.6001896858215 and batch: 50, loss is 4.26689603805542 and perplexity is 71.2999793682906
At time: 1348.2087006568909 and batch: 100, loss is 4.2436071872711185 and perplexity is 69.65867106241168
At time: 1349.8165555000305 and batch: 150, loss is 4.232408146858216 and perplexity is 68.88291277214982
At time: 1351.4284613132477 and batch: 200, loss is 4.282504358291626 and perplexity is 72.42158268065201
At time: 1353.0359761714935 and batch: 250, loss is 4.2774050998687745 and perplexity is 72.05322628557653
At time: 1354.641633272171 and batch: 300, loss is 4.258393001556397 and perplexity is 70.6962833035253
At time: 1356.2486507892609 and batch: 350, loss is 4.215584106445313 and perplexity is 67.73371801973514
At time: 1357.8478469848633 and batch: 400, loss is 4.244975061416626 and perplexity is 69.75402055575451
At time: 1359.45312333107 and batch: 450, loss is 4.2804254627227785 and perplexity is 72.27118216098205
At time: 1361.0615146160126 and batch: 500, loss is 4.26383755683899 and perplexity is 71.08224286189942
At time: 1362.6719739437103 and batch: 550, loss is 4.274552583694458 and perplexity is 71.84798615674609
At time: 1364.28373837471 and batch: 600, loss is 4.31732216835022 and perplexity is 74.98755514365376
At time: 1365.8970565795898 and batch: 650, loss is 4.305941619873047 and perplexity is 74.13899334506928
At time: 1367.5044150352478 and batch: 700, loss is 4.2647965621948245 and perplexity is 71.1504438108194
At time: 1369.1118688583374 and batch: 750, loss is 4.257085437774658 and perplexity is 70.60390381317393
At time: 1370.7601792812347 and batch: 800, loss is 4.216007242202759 and perplexity is 67.76238464232286
At time: 1372.374094247818 and batch: 850, loss is 4.252966775894165 and perplexity is 70.31370822490284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562114715576172 and perplexity of 95.78582558013447
Finished 46 epochs...
Completing Train Step...
At time: 1376.6481232643127 and batch: 50, loss is 4.265844411849976 and perplexity is 71.22503785367618
At time: 1378.2569587230682 and batch: 100, loss is 4.24263370513916 and perplexity is 69.59089258671658
At time: 1379.8657026290894 and batch: 150, loss is 4.2313951396942135 and perplexity is 68.81316921935775
At time: 1381.4759702682495 and batch: 200, loss is 4.2815592575073245 and perplexity is 72.35316931991294
At time: 1383.0828866958618 and batch: 250, loss is 4.276477708816528 and perplexity is 71.9864357434963
At time: 1384.6916069984436 and batch: 300, loss is 4.257505178451538 and perplexity is 70.63354536399792
At time: 1386.2993865013123 and batch: 350, loss is 4.214714331626892 and perplexity is 67.67483055057487
At time: 1387.905327796936 and batch: 400, loss is 4.244150695800781 and perplexity is 69.696541434802
At time: 1389.5110683441162 and batch: 450, loss is 4.279690227508545 and perplexity is 72.21806537194267
At time: 1391.115549325943 and batch: 500, loss is 4.263089981079101 and perplexity is 71.029123358078
At time: 1392.7204003334045 and batch: 550, loss is 4.273859844207764 and perplexity is 71.79823145520879
At time: 1394.3260860443115 and batch: 600, loss is 4.316690816879272 and perplexity is 74.94022658246152
At time: 1395.9317259788513 and batch: 650, loss is 4.305342092514038 and perplexity is 74.09455831152961
At time: 1397.5378801822662 and batch: 700, loss is 4.264215641021728 and perplexity is 71.10912301475919
At time: 1399.1421954631805 and batch: 750, loss is 4.256519794464111 and perplexity is 70.56397848009681
At time: 1400.751876592636 and batch: 800, loss is 4.215517187118531 and perplexity is 67.72918547658387
At time: 1402.3576261997223 and batch: 850, loss is 4.252514419555664 and perplexity is 70.28190856623462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562085787455241 and perplexity of 95.78305471626678
Finished 47 epochs...
Completing Train Step...
At time: 1406.6040847301483 and batch: 50, loss is 4.264818515777588 and perplexity is 71.1520058351222
At time: 1408.2436854839325 and batch: 100, loss is 4.241681299209595 and perplexity is 69.52464536010658
At time: 1409.8503262996674 and batch: 150, loss is 4.230450301170349 and perplexity is 68.74818259189927
At time: 1411.4903931617737 and batch: 200, loss is 4.2806417179107665 and perplexity is 72.28681286911635
At time: 1413.096886396408 and batch: 250, loss is 4.275575227737427 and perplexity is 71.92149865394066
At time: 1414.7050187587738 and batch: 300, loss is 4.25661904335022 and perplexity is 70.57098222391149
At time: 1416.3124120235443 and batch: 350, loss is 4.213861408233643 and perplexity is 67.61713371345293
At time: 1417.920404434204 and batch: 400, loss is 4.243343887329101 and perplexity is 69.64033235275649
At time: 1419.5318188667297 and batch: 450, loss is 4.278958425521851 and perplexity is 72.16523538113209
At time: 1421.1401498317719 and batch: 500, loss is 4.262345762252807 and perplexity is 70.97628181253516
At time: 1422.7433207035065 and batch: 550, loss is 4.273168087005615 and perplexity is 71.74858168627063
At time: 1424.3270761966705 and batch: 600, loss is 4.316063499450683 and perplexity is 74.89323001464818
At time: 1425.9095213413239 and batch: 650, loss is 4.3047391796112064 and perplexity is 74.04989921042096
At time: 1427.4928295612335 and batch: 700, loss is 4.263633422851562 and perplexity is 71.06773404115083
At time: 1429.0821180343628 and batch: 750, loss is 4.255962562561035 and perplexity is 70.52466893336366
At time: 1430.6717383861542 and batch: 800, loss is 4.215011396408081 and perplexity is 67.69493734566686
At time: 1432.2608363628387 and batch: 850, loss is 4.2520521545410155 and perplexity is 70.2494272068182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562065442403157 and perplexity of 95.781106024853
Finished 48 epochs...
Completing Train Step...
At time: 1436.4082791805267 and batch: 50, loss is 4.263795957565308 and perplexity is 71.07928595372775
At time: 1438.0469317436218 and batch: 100, loss is 4.2407379245758055 and perplexity is 69.45908850045178
At time: 1439.6558396816254 and batch: 150, loss is 4.229516658782959 and perplexity is 68.68402632873912
At time: 1441.261899471283 and batch: 200, loss is 4.279725732803345 and perplexity is 72.22062954116413
At time: 1442.8671820163727 and batch: 250, loss is 4.274683475494385 and perplexity is 71.8573910844758
At time: 1444.4790601730347 and batch: 300, loss is 4.2557518005371096 and perplexity is 70.50980657766766
At time: 1446.091881275177 and batch: 350, loss is 4.213017997741699 and perplexity is 67.56012875611253
At time: 1447.705088376999 and batch: 400, loss is 4.242534341812134 and perplexity is 69.58397814762499
At time: 1449.3195395469666 and batch: 450, loss is 4.278233909606934 and perplexity is 72.11296945562846
At time: 1450.9697980880737 and batch: 500, loss is 4.261606168746948 and perplexity is 70.9238076226136
At time: 1452.580857038498 and batch: 550, loss is 4.272485513687133 and perplexity is 71.69962472902982
At time: 1454.1764891147614 and batch: 600, loss is 4.3154379081726075 and perplexity is 74.84639211538229
At time: 1455.763843536377 and batch: 650, loss is 4.304142627716065 and perplexity is 74.00573777631519
At time: 1457.3567759990692 and batch: 700, loss is 4.2630491542816165 and perplexity is 71.02622352563897
At time: 1458.9435279369354 and batch: 750, loss is 4.255386667251587 and perplexity is 70.48406580002374
At time: 1460.5320522785187 and batch: 800, loss is 4.214509153366089 and perplexity is 67.66094657093647
At time: 1462.1280963420868 and batch: 850, loss is 4.251582260131836 and perplexity is 70.21642514807435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562035242716472 and perplexity of 95.7782135091374
Finished 49 epochs...
Completing Train Step...
At time: 1466.3801655769348 and batch: 50, loss is 4.262805738449097 and perplexity is 71.00893672233491
At time: 1468.01678109169 and batch: 100, loss is 4.239822168350219 and perplexity is 69.39551002337019
At time: 1469.6206200122833 and batch: 150, loss is 4.2285875749588016 and perplexity is 68.62024274563488
At time: 1471.225543498993 and batch: 200, loss is 4.278842649459839 and perplexity is 72.15688085800173
At time: 1472.8324825763702 and batch: 250, loss is 4.2738077926635745 and perplexity is 71.79449434365353
At time: 1474.4382345676422 and batch: 300, loss is 4.254892864227295 and perplexity is 70.44926914722056
At time: 1476.047117948532 and batch: 350, loss is 4.212189297676087 and perplexity is 67.50416486482251
At time: 1477.653018951416 and batch: 400, loss is 4.24174313545227 and perplexity is 69.52894463587316
At time: 1479.2602858543396 and batch: 450, loss is 4.277509727478027 and perplexity is 72.06076543677655
At time: 1480.8674778938293 and batch: 500, loss is 4.260868978500366 and perplexity is 70.87154255040143
At time: 1482.4755227565765 and batch: 550, loss is 4.271799745559693 and perplexity is 71.65047226716335
At time: 1484.0808293819427 and batch: 600, loss is 4.314807929992676 and perplexity is 74.79925537062226
At time: 1485.6864898204803 and batch: 650, loss is 4.3035305213928225 and perplexity is 73.96045225745628
At time: 1487.2925000190735 and batch: 700, loss is 4.262459211349487 and perplexity is 70.98433446436688
At time: 1488.893883228302 and batch: 750, loss is 4.254821109771728 and perplexity is 70.44421427962463
At time: 1490.4995028972626 and batch: 800, loss is 4.213986592292786 and perplexity is 67.62559883055658
At time: 1492.1308665275574 and batch: 850, loss is 4.251104097366333 and perplexity is 70.18285829389131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.562020301818848 and perplexity of 95.776782507345
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f079a7ddba8>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 7.8107127518982145, 'anneal': 5.76722681168048, 'dropout': 0.6158263652314355, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.179077386856079 and batch: 50, loss is 7.2706662368774415 and perplexity is 1437.5078548174931
At time: 3.792236328125 and batch: 100, loss is 6.408569736480713 and perplexity is 607.0248543795103
At time: 5.437124252319336 and batch: 150, loss is 6.210036401748657 and perplexity is 497.71936881241555
At time: 7.044113636016846 and batch: 200, loss is 6.161752071380615 and perplexity is 474.2582816939939
At time: 8.659649848937988 and batch: 250, loss is 6.160987939834595 and perplexity is 473.89602440372846
At time: 10.275214910507202 and batch: 300, loss is 6.074874448776245 and perplexity is 434.7949100562848
At time: 11.891333103179932 and batch: 350, loss is 6.028992109298706 and perplexity is 415.2962451493022
At time: 13.507150888442993 and batch: 400, loss is 6.034866046905518 and perplexity is 417.74284794311245
At time: 15.121460676193237 and batch: 450, loss is 6.024580450057983 and perplexity is 413.4681350960578
At time: 16.73940420150757 and batch: 500, loss is 6.01291428565979 and perplexity is 408.67257514328423
At time: 18.35597252845764 and batch: 550, loss is 5.958746175765992 and perplexity is 387.1244338195039
At time: 19.971699237823486 and batch: 600, loss is 5.97472749710083 and perplexity is 393.3608945005346
At time: 21.58661389350891 and batch: 650, loss is 5.990675840377808 and perplexity is 399.6846416868899
At time: 23.179758548736572 and batch: 700, loss is 5.936227264404297 and perplexity is 378.50423601659514
At time: 24.772908926010132 and batch: 750, loss is 5.920913677215577 and perplexity is 372.75213346709467
At time: 26.36554479598999 and batch: 800, loss is 5.936194705963135 and perplexity is 378.4919127093119
At time: 27.957571029663086 and batch: 850, loss is 5.9249896049499515 and perplexity is 374.2745447374259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.306636810302734 and perplexity of 201.67082929919403
Finished 1 epochs...
Completing Train Step...
At time: 32.16051459312439 and batch: 50, loss is 5.580932865142822 and perplexity is 265.31899722109904
At time: 33.747482776641846 and batch: 100, loss is 5.421116485595703 and perplexity is 226.13145412477607
At time: 35.33450388908386 and batch: 150, loss is 5.364225187301636 and perplexity is 213.6256506740897
At time: 36.9218692779541 and batch: 200, loss is 5.352138195037842 and perplexity is 211.05910126514462
At time: 38.508177042007446 and batch: 250, loss is 5.3746020698547365 and perplexity is 215.8539604204565
At time: 40.09453296661377 and batch: 300, loss is 5.305203742980957 and perplexity is 201.38202840895426
At time: 41.68171787261963 and batch: 350, loss is 5.260733737945556 and perplexity is 192.62277409716484
At time: 43.275837898254395 and batch: 400, loss is 5.256779508590698 and perplexity is 191.8626034045471
At time: 44.88342356681824 and batch: 450, loss is 5.244634523391723 and perplexity is 189.54652774674233
At time: 46.49312710762024 and batch: 500, loss is 5.243407325744629 and perplexity is 189.314059365377
At time: 48.10037064552307 and batch: 550, loss is 5.213736314773559 and perplexity is 183.77943482238578
At time: 49.70881199836731 and batch: 600, loss is 5.240559301376343 and perplexity is 188.77565536882338
At time: 51.369866371154785 and batch: 650, loss is 5.2303625583648685 and perplexity is 186.86053909188865
At time: 52.97745323181152 and batch: 700, loss is 5.177816047668457 and perplexity is 177.29518363792414
At time: 54.585402488708496 and batch: 750, loss is 5.1672818565368654 and perplexity is 175.43732498622748
At time: 56.193495750427246 and batch: 800, loss is 5.147999811172485 and perplexity is 172.08693947465528
At time: 57.80223822593689 and batch: 850, loss is 5.136697654724121 and perplexity is 170.152935755518
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.904417673746745 and perplexity of 134.884340439879
Finished 2 epochs...
Completing Train Step...
At time: 62.045509576797485 and batch: 50, loss is 5.083110990524292 and perplexity is 161.27500130729305
At time: 63.64487314224243 and batch: 100, loss is 4.9935034656524655 and perplexity is 147.45211303131978
At time: 65.24276280403137 and batch: 150, loss is 4.9831945419311525 and perplexity is 145.93984874693982
At time: 66.83760809898376 and batch: 200, loss is 4.995468511581421 and perplexity is 147.7421480785136
At time: 68.42542386054993 and batch: 250, loss is 5.007551555633545 and perplexity is 149.5381517062603
At time: 70.00901007652283 and batch: 300, loss is 4.965125961303711 and perplexity is 143.32660274210164
At time: 71.59393095970154 and batch: 350, loss is 4.917066526412964 and perplexity is 136.60130853540647
At time: 73.17829918861389 and batch: 400, loss is 4.929723320007324 and perplexity is 138.34123079361308
At time: 74.76304221153259 and batch: 450, loss is 4.937113723754883 and perplexity is 139.36741563357376
At time: 76.34835815429688 and batch: 500, loss is 4.9418850517272945 and perplexity is 140.03397219767416
At time: 77.93340277671814 and batch: 550, loss is 4.929266738891601 and perplexity is 138.2780812176559
At time: 79.52458119392395 and batch: 600, loss is 4.961908369064331 and perplexity is 142.86617730530628
At time: 81.10739040374756 and batch: 650, loss is 4.945387582778931 and perplexity is 140.52530548633564
At time: 82.69022464752197 and batch: 700, loss is 4.906851072311401 and perplexity is 135.2129674784201
At time: 84.27249455451965 and batch: 750, loss is 4.897454147338867 and perplexity is 133.948332513477
At time: 85.8550374507904 and batch: 800, loss is 4.8747735023498535 and perplexity is 130.94449123219746
At time: 87.47756505012512 and batch: 850, loss is 4.873822841644287 and perplexity is 130.82006660194682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.813773155212402 and perplexity of 123.19557770609106
Finished 3 epochs...
Completing Train Step...
At time: 91.71628069877625 and batch: 50, loss is 4.850382862091064 and perplexity is 127.7893061623027
At time: 93.35308766365051 and batch: 100, loss is 4.77448353767395 and perplexity is 118.44912431057841
At time: 94.9608063697815 and batch: 150, loss is 4.765869131088257 and perplexity is 117.43313773761365
At time: 96.569580078125 and batch: 200, loss is 4.783451137542724 and perplexity is 119.516105642677
At time: 98.17870235443115 and batch: 250, loss is 4.804315547943116 and perplexity is 122.0359346827828
At time: 99.78540086746216 and batch: 300, loss is 4.772025709152222 and perplexity is 118.15835415244472
At time: 101.39308762550354 and batch: 350, loss is 4.7260909748077395 and perplexity is 112.85355161964958
At time: 103.0003297328949 and batch: 400, loss is 4.742618160247803 and perplexity is 114.7342013101707
At time: 104.60754203796387 and batch: 450, loss is 4.756395654678345 and perplexity is 116.32589069784336
At time: 106.21312475204468 and batch: 500, loss is 4.755572328567505 and perplexity is 116.2301559705234
At time: 107.81842994689941 and batch: 550, loss is 4.755869512557983 and perplexity is 116.26470284522345
At time: 109.42449450492859 and batch: 600, loss is 4.784028720855713 and perplexity is 119.58515609020314
At time: 111.03549265861511 and batch: 650, loss is 4.766021270751953 and perplexity is 117.45100533484643
At time: 112.64090013504028 and batch: 700, loss is 4.73083083152771 and perplexity is 113.3897309875289
At time: 114.25219798088074 and batch: 750, loss is 4.720419902801513 and perplexity is 112.21536232139083
At time: 115.85858154296875 and batch: 800, loss is 4.694662885665894 and perplexity is 109.36193495095955
At time: 117.46739363670349 and batch: 850, loss is 4.699402570724487 and perplexity is 109.8815064099019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.806944529215495 and perplexity of 122.3571869677125
Finished 4 epochs...
Completing Train Step...
At time: 121.7361855506897 and batch: 50, loss is 4.684966068267823 and perplexity is 108.30659721538328
At time: 123.34296441078186 and batch: 100, loss is 4.62295729637146 and perplexity is 101.79462431350305
At time: 124.95162343978882 and batch: 150, loss is 4.618716945648194 and perplexity is 101.36389327540418
At time: 126.55616760253906 and batch: 200, loss is 4.6415424728393555 and perplexity is 103.70418516797282
At time: 128.1897144317627 and batch: 250, loss is 4.651272659301758 and perplexity is 104.718171363922
At time: 129.78986525535583 and batch: 300, loss is 4.623858766555786 and perplexity is 101.88643050630192
At time: 131.39174151420593 and batch: 350, loss is 4.582594146728516 and perplexity is 97.76768925398032
At time: 132.99755263328552 and batch: 400, loss is 4.592163543701172 and perplexity is 98.70775785460245
At time: 134.60389113426208 and batch: 450, loss is 4.618970127105713 and perplexity is 101.38955998267353
At time: 136.2115442752838 and batch: 500, loss is 4.616598320007324 and perplexity is 101.14936846118114
At time: 137.81944751739502 and batch: 550, loss is 4.623468980789185 and perplexity is 101.84672436482788
At time: 139.42388010025024 and batch: 600, loss is 4.653578786849976 and perplexity is 104.95994349523971
At time: 141.0161473751068 and batch: 650, loss is 4.6244128322601314 and perplexity is 101.94289792506912
At time: 142.5991108417511 and batch: 700, loss is 4.605924320220947 and perplexity is 100.07544186635718
At time: 144.18237590789795 and batch: 750, loss is 4.597754411697387 and perplexity is 99.26116547191329
At time: 145.76652359962463 and batch: 800, loss is 4.571521520614624 and perplexity is 96.69111543379621
At time: 147.34983229637146 and batch: 850, loss is 4.580810785293579 and perplexity is 97.59348950410825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.76537545522054 and perplexity of 117.37517813924696
Finished 5 epochs...
Completing Train Step...
At time: 151.5198004245758 and batch: 50, loss is 4.567473602294922 and perplexity is 96.30050880151222
At time: 153.10627961158752 and batch: 100, loss is 4.514807929992676 and perplexity is 91.36001681800495
At time: 154.6927580833435 and batch: 150, loss is 4.506606874465942 and perplexity is 90.6138321801742
At time: 156.2786729335785 and batch: 200, loss is 4.52984619140625 and perplexity is 92.7442951154157
At time: 157.86559104919434 and batch: 250, loss is 4.561197395324707 and perplexity is 95.6979995909644
At time: 159.4522361755371 and batch: 300, loss is 4.540809278488159 and perplexity is 93.76665275736327
At time: 161.04010772705078 and batch: 350, loss is 4.499800081253052 and perplexity is 89.99913698718856
At time: 162.6270740032196 and batch: 400, loss is 4.5120954513549805 and perplexity is 91.11254051287682
At time: 164.21924710273743 and batch: 450, loss is 4.5304125213623045 and perplexity is 92.796833863722
At time: 165.83059191703796 and batch: 500, loss is 4.512094240188599 and perplexity is 91.11243016049765
At time: 167.49898147583008 and batch: 550, loss is 4.522298307418823 and perplexity is 92.04690714800331
At time: 169.10411548614502 and batch: 600, loss is 4.553910245895386 and perplexity is 95.00316871085505
At time: 170.72172594070435 and batch: 650, loss is 4.521944379806518 and perplexity is 92.01433497037293
At time: 172.3368730545044 and batch: 700, loss is 4.49595498085022 and perplexity is 89.65374572688654
At time: 173.94725918769836 and batch: 750, loss is 4.490436029434204 and perplexity is 89.16031392547102
At time: 175.55804824829102 and batch: 800, loss is 4.46897442817688 and perplexity is 87.2671783448514
At time: 177.16767382621765 and batch: 850, loss is 4.478529624938965 and perplexity is 88.10502994850432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.753653844197591 and perplexity of 116.00738399358676
Finished 6 epochs...
Completing Train Step...
At time: 181.41795921325684 and batch: 50, loss is 4.46653974533081 and perplexity is 87.05496887890936
At time: 183.01970601081848 and batch: 100, loss is 4.412613306045532 and perplexity is 82.48473995965432
At time: 184.62568020820618 and batch: 150, loss is 4.416020221710205 and perplexity is 82.76623775967113
At time: 186.22931504249573 and batch: 200, loss is 4.444025535583496 and perplexity is 85.11689402703138
At time: 187.83266234397888 and batch: 250, loss is 4.456048631668091 and perplexity is 86.14643938158426
At time: 189.4376139640808 and batch: 300, loss is 4.4288004207611085 and perplexity is 83.83079488916245
At time: 191.0425319671631 and batch: 350, loss is 4.390542593002319 and perplexity is 80.68418578022577
At time: 192.64648866653442 and batch: 400, loss is 4.408432369232178 and perplexity is 82.14059639637239
At time: 194.2498459815979 and batch: 450, loss is 4.4390089797973635 and perplexity is 84.6909696098304
At time: 195.8530969619751 and batch: 500, loss is 4.427739477157592 and perplexity is 83.74190230689354
At time: 197.45658159255981 and batch: 550, loss is 4.439317855834961 and perplexity is 84.71713266130561
At time: 199.0601646900177 and batch: 600, loss is 4.470805625915528 and perplexity is 87.42712820971363
At time: 200.66261386871338 and batch: 650, loss is 4.438728876113892 and perplexity is 84.66725067932089
At time: 202.2647054195404 and batch: 700, loss is 4.415635843276977 and perplexity is 82.73443031631611
At time: 203.86974263191223 and batch: 750, loss is 4.406785097122192 and perplexity is 82.00539986613927
At time: 205.47198629379272 and batch: 800, loss is 4.3811421298980715 and perplexity is 79.92927090265862
At time: 207.06977128982544 and batch: 850, loss is 4.399656295776367 and perplexity is 81.42287846783944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.7506914138793945 and perplexity of 115.66422873983394
Finished 7 epochs...
Completing Train Step...
At time: 211.292475938797 and batch: 50, loss is 4.390605068206787 and perplexity is 80.68922669869426
At time: 212.93026185035706 and batch: 100, loss is 4.329092321395874 and perplexity is 75.875384840206
At time: 214.53389811515808 and batch: 150, loss is 4.329570789337158 and perplexity is 75.91169746590549
At time: 216.1288378238678 and batch: 200, loss is 4.360098552703858 and perplexity is 78.26484725533179
At time: 217.71218252182007 and batch: 250, loss is 4.370960350036621 and perplexity is 79.11957771570458
At time: 219.29591393470764 and batch: 300, loss is 4.345991811752319 and perplexity is 77.16853619797753
At time: 220.87983536720276 and batch: 350, loss is 4.309718103408813 and perplexity is 74.41950737772133
At time: 222.46269631385803 and batch: 400, loss is 4.330638332366943 and perplexity is 75.99277974112285
At time: 224.05228900909424 and batch: 450, loss is 4.3637282752990725 and perplexity is 78.54944312940303
At time: 225.66355180740356 and batch: 500, loss is 4.349827060699463 and perplexity is 77.46506501262208
At time: 227.2752571105957 and batch: 550, loss is 4.362092342376709 and perplexity is 78.42104656207552
At time: 228.88253259658813 and batch: 600, loss is 4.396793661117553 and perplexity is 81.19012781290587
At time: 230.4895956516266 and batch: 650, loss is 4.36429084777832 and perplexity is 78.59364531666911
At time: 232.09694981575012 and batch: 700, loss is 4.3415172290802 and perplexity is 76.82401058208636
At time: 233.70435118675232 and batch: 750, loss is 4.337796106338501 and perplexity is 76.53867023168739
At time: 235.31233382225037 and batch: 800, loss is 4.310409994125366 and perplexity is 74.4710153608993
At time: 236.92016577720642 and batch: 850, loss is 4.333685083389282 and perplexity is 76.22466388774414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.760867118835449 and perplexity of 116.84720239333103
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 241.1746244430542 and batch: 50, loss is 4.340850706100464 and perplexity is 76.77282267449131
At time: 242.81403517723083 and batch: 100, loss is 4.286386489868164 and perplexity is 72.70327923135063
At time: 244.42369985580444 and batch: 150, loss is 4.281854462623596 and perplexity is 72.37453149863175
At time: 246.02570033073425 and batch: 200, loss is 4.300515241622925 and perplexity is 73.73777668527812
At time: 247.6323835849762 and batch: 250, loss is 4.282826089859009 and perplexity is 72.44488673858474
At time: 249.2728168964386 and batch: 300, loss is 4.254975004196167 and perplexity is 70.45505608566158
At time: 250.88416242599487 and batch: 350, loss is 4.187728104591369 and perplexity is 65.87296434440675
At time: 252.49362993240356 and batch: 400, loss is 4.198289880752563 and perplexity is 66.57238691945795
At time: 254.1040802001953 and batch: 450, loss is 4.213718395233155 and perplexity is 67.60746427572029
At time: 255.71207451820374 and batch: 500, loss is 4.175739336013794 and perplexity is 65.08794373715646
At time: 257.3232636451721 and batch: 550, loss is 4.174150967597962 and perplexity is 64.98464216526838
At time: 258.93440866470337 and batch: 600, loss is 4.182713289260864 and perplexity is 65.54345050902434
At time: 260.55568289756775 and batch: 650, loss is 4.134749526977539 and perplexity is 62.47394107375057
At time: 262.1670219898224 and batch: 700, loss is 4.080642166137696 and perplexity is 59.18346326555483
At time: 263.78259897232056 and batch: 750, loss is 4.044777040481567 and perplexity is 57.09845397102969
At time: 265.39418029785156 and batch: 800, loss is 3.983289947509766 and perplexity is 53.6933924064824
At time: 267.00594449043274 and batch: 850, loss is 3.9747594499588015 and perplexity is 53.23730912843755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.633679707845052 and perplexity of 102.89198080151158
Finished 9 epochs...
Completing Train Step...
At time: 271.22359442710876 and batch: 50, loss is 4.210952453613281 and perplexity is 67.42072435138377
At time: 272.86006569862366 and batch: 100, loss is 4.157166709899903 and perplexity is 63.890246314675025
At time: 274.4674699306488 and batch: 150, loss is 4.157708258628845 and perplexity is 63.92485536675681
At time: 276.07510709762573 and batch: 200, loss is 4.184321789741516 and perplexity is 65.6489620155773
At time: 277.6824777126312 and batch: 250, loss is 4.17050223827362 and perplexity is 64.74796284773169
At time: 279.290433883667 and batch: 300, loss is 4.152505521774292 and perplexity is 63.593134841383204
At time: 280.8988263607025 and batch: 350, loss is 4.092135057449341 and perplexity is 59.867576063168144
At time: 282.50549602508545 and batch: 400, loss is 4.110008263587952 and perplexity is 60.94722121026417
At time: 284.11409735679626 and batch: 450, loss is 4.132321338653565 and perplexity is 62.32242660675781
At time: 285.7086818218231 and batch: 500, loss is 4.102421841621399 and perplexity is 60.486599317705746
At time: 287.29920053482056 and batch: 550, loss is 4.107577719688416 and perplexity is 60.799266191978965
At time: 288.90925550460815 and batch: 600, loss is 4.1256416273117065 and perplexity is 61.90751806426583
At time: 290.56826281547546 and batch: 650, loss is 4.086146645545959 and perplexity is 59.51013567636087
At time: 292.1755769252777 and batch: 700, loss is 4.044817361831665 and perplexity is 57.10075630419858
At time: 293.78132367134094 and batch: 750, loss is 4.022044043540955 and perplexity is 55.815077754493956
At time: 295.382360458374 and batch: 800, loss is 3.9723782682418824 and perplexity is 53.110692230031475
At time: 296.9846920967102 and batch: 850, loss is 3.977210087776184 and perplexity is 53.36793448382739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.627928415934245 and perplexity of 102.30191742448183
Finished 10 epochs...
Completing Train Step...
At time: 301.23391366004944 and batch: 50, loss is 4.161327657699585 and perplexity is 64.15664414387368
At time: 302.8454415798187 and batch: 100, loss is 4.105765709877014 and perplexity is 60.68919707838652
At time: 304.45668840408325 and batch: 150, loss is 4.107779016494751 and perplexity is 60.81150612197904
At time: 306.0641508102417 and batch: 200, loss is 4.133175611495972 and perplexity is 62.375689710653226
At time: 307.67224884033203 and batch: 250, loss is 4.120266981124878 and perplexity is 61.57567961430836
At time: 309.2823703289032 and batch: 300, loss is 4.108407139778137 and perplexity is 60.84971524363525
At time: 310.88921642303467 and batch: 350, loss is 4.0491787672042845 and perplexity is 57.35033972112388
At time: 312.500235080719 and batch: 400, loss is 4.067462711334229 and perplexity is 58.40857500508492
At time: 314.0979723930359 and batch: 450, loss is 4.093161702156067 and perplexity is 59.92907035423086
At time: 315.7010657787323 and batch: 500, loss is 4.06581591129303 and perplexity is 58.31246691848071
At time: 317.3077528476715 and batch: 550, loss is 4.073616328239441 and perplexity is 58.769107150308905
At time: 318.92029452323914 and batch: 600, loss is 4.096273217201233 and perplexity is 60.115830961823775
At time: 320.5279223918915 and batch: 650, loss is 4.059296064376831 and perplexity is 57.93351525666303
At time: 322.13428115844727 and batch: 700, loss is 4.021981129646301 and perplexity is 55.81156632103214
At time: 323.7433440685272 and batch: 750, loss is 4.005340166091919 and perplexity is 54.890493108183286
At time: 325.35554814338684 and batch: 800, loss is 3.9610686254501344 and perplexity is 52.51341314541951
At time: 326.9652338027954 and batch: 850, loss is 3.970705676078796 and perplexity is 53.02193395132438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.627672513326009 and perplexity of 102.2757414463787
Finished 11 epochs...
Completing Train Step...
At time: 331.2205617427826 and batch: 50, loss is 4.124789867401123 and perplexity is 61.854810172636114
At time: 332.8265423774719 and batch: 100, loss is 4.070714592933655 and perplexity is 58.59882193799202
At time: 334.43121218681335 and batch: 150, loss is 4.073182983398437 and perplexity is 58.74364537817716
At time: 336.038925409317 and batch: 200, loss is 4.10037823677063 and perplexity is 60.36311482965884
At time: 337.64483857154846 and batch: 250, loss is 4.086294345855713 and perplexity is 59.518925990983846
At time: 339.25536727905273 and batch: 300, loss is 4.0780102825164795 and perplexity is 59.02790407457688
At time: 340.8631579875946 and batch: 350, loss is 4.019065499305725 and perplexity is 55.64907741880662
At time: 342.4699447154999 and batch: 400, loss is 4.037879672050476 and perplexity is 56.70597997198315
At time: 344.08149933815 and batch: 450, loss is 4.06499041557312 and perplexity is 58.2643500894727
At time: 345.6883280277252 and batch: 500, loss is 4.039035148620606 and perplexity is 56.77154027262842
At time: 347.2952284812927 and batch: 550, loss is 4.048741750717163 and perplexity is 57.32528215280776
At time: 348.90296840667725 and batch: 600, loss is 4.074354987144471 and perplexity is 58.81253351130572
At time: 350.51131105422974 and batch: 650, loss is 4.038240566253662 and perplexity is 56.726448524709866
At time: 352.10644340515137 and batch: 700, loss is 4.004371447563171 and perplexity is 54.83734541719001
At time: 353.6932284832001 and batch: 750, loss is 3.9903189373016357 and perplexity is 54.07213223343141
At time: 355.2768120765686 and batch: 800, loss is 3.947989501953125 and perplexity is 51.831055774973656
At time: 356.8615062236786 and batch: 850, loss is 3.960943064689636 and perplexity is 52.506819935261525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.628270149230957 and perplexity of 102.33688337015691
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 361.10231733322144 and batch: 50, loss is 4.116644105911255 and perplexity is 61.35300222063428
At time: 362.68992829322815 and batch: 100, loss is 4.082627468109131 and perplexity is 59.301077022633706
At time: 364.2824692726135 and batch: 150, loss is 4.086123080253601 and perplexity is 59.50873331913891
At time: 365.86994647979736 and batch: 200, loss is 4.113287596702576 and perplexity is 61.14741552353587
At time: 367.4572992324829 and batch: 250, loss is 4.09853600025177 and perplexity is 60.25201406349336
At time: 369.04520320892334 and batch: 300, loss is 4.082988672256469 and perplexity is 59.32250068652114
At time: 370.6325902938843 and batch: 350, loss is 4.018677606582641 and perplexity is 55.62749573258965
At time: 372.259624004364 and batch: 400, loss is 4.02941303730011 and perplexity is 56.22789788051915
At time: 373.84692573547363 and batch: 450, loss is 4.060092678070069 and perplexity is 57.97968427520853
At time: 375.440153837204 and batch: 500, loss is 4.023131189346313 and perplexity is 55.875789877635924
At time: 377.05335307121277 and batch: 550, loss is 4.017183132171631 and perplexity is 55.544423953454235
At time: 378.66551208496094 and batch: 600, loss is 4.026577115058899 and perplexity is 56.06866582592388
At time: 380.2755494117737 and batch: 650, loss is 3.9875889158248903 and perplexity is 53.924715468166625
At time: 381.8896174430847 and batch: 700, loss is 3.945036292076111 and perplexity is 51.67821358773518
At time: 383.5041694641113 and batch: 750, loss is 3.923751225471497 and perplexity is 50.589863263896305
At time: 385.11622977256775 and batch: 800, loss is 3.8719914960861206 and perplexity is 48.037958289311
At time: 386.7271056175232 and batch: 850, loss is 3.8823513984680176 and perplexity is 48.538213672020824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.604787508646647 and perplexity of 99.96173958701908
Finished 13 epochs...
Completing Train Step...
At time: 390.9256896972656 and batch: 50, loss is 4.09837809085846 and perplexity is 60.24250045567064
At time: 392.55789041519165 and batch: 100, loss is 4.052351245880127 and perplexity is 57.53257136109639
At time: 394.1518518924713 and batch: 150, loss is 4.053630709648132 and perplexity is 57.60622931292506
At time: 395.7567174434662 and batch: 200, loss is 4.081576552391052 and perplexity is 59.23878932398508
At time: 397.36527967453003 and batch: 250, loss is 4.067184071540833 and perplexity is 58.39230231902718
At time: 398.9742841720581 and batch: 300, loss is 4.053683333396911 and perplexity is 57.60926084842921
At time: 400.57969403266907 and batch: 350, loss is 3.9928808641433715 and perplexity is 54.21083868241123
At time: 402.1858811378479 and batch: 400, loss is 4.005469107627869 and perplexity is 54.89757122899577
At time: 403.79168248176575 and batch: 450, loss is 4.037401776313782 and perplexity is 56.67888690025687
At time: 405.3966782093048 and batch: 500, loss is 4.0031844329833985 and perplexity is 54.772291306391224
At time: 407.0008981227875 and batch: 550, loss is 4.002996978759765 and perplexity is 54.76202497131176
At time: 408.60651445388794 and batch: 600, loss is 4.017891187667846 and perplexity is 55.58376641479791
At time: 410.2014102935791 and batch: 650, loss is 3.982811994552612 and perplexity is 53.667735622658384
At time: 411.8616647720337 and batch: 700, loss is 3.9433327198028563 and perplexity is 51.59025096255391
At time: 413.46576023101807 and batch: 750, loss is 3.9266587257385255 and perplexity is 50.73716734440145
At time: 415.07119584083557 and batch: 800, loss is 3.8785074615478514 and perplexity is 48.35199397812249
At time: 416.67921018600464 and batch: 850, loss is 3.891930594444275 and perplexity is 49.00540481840912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.602165857950847 and perplexity of 99.70001804401211
Finished 14 epochs...
Completing Train Step...
At time: 420.9116961956024 and batch: 50, loss is 4.088377933502198 and perplexity is 59.6430681755599
At time: 422.54741954803467 and batch: 100, loss is 4.041081576347351 and perplexity is 56.88783808367828
At time: 424.1570918560028 and batch: 150, loss is 4.040689239501953 and perplexity is 56.86552326649173
At time: 425.7655940055847 and batch: 200, loss is 4.068260049819946 and perplexity is 58.45516498135464
At time: 427.37271094322205 and batch: 250, loss is 4.054196238517761 and perplexity is 57.63881651230449
At time: 428.97717332839966 and batch: 300, loss is 4.041042056083679 and perplexity is 56.88558990574206
At time: 430.581903219223 and batch: 350, loss is 3.981436986923218 and perplexity is 53.593992786824316
At time: 432.17298340797424 and batch: 400, loss is 3.994462776184082 and perplexity is 54.29666332648124
At time: 433.7607283592224 and batch: 450, loss is 4.028212485313415 and perplexity is 56.160433871139425
At time: 435.34600257873535 and batch: 500, loss is 3.9956153869628905 and perplexity is 54.359282326619436
At time: 436.92706632614136 and batch: 550, loss is 3.9968511724472044 and perplexity is 54.426500263577374
At time: 438.5128083229065 and batch: 600, loss is 4.013448147773743 and perplexity is 55.33735334007491
At time: 440.09819865226746 and batch: 650, loss is 3.980120577812195 and perplexity is 53.52348758345142
At time: 441.6813201904297 and batch: 700, loss is 3.9419893836975097 and perplexity is 51.52099444355802
At time: 443.2694525718689 and batch: 750, loss is 3.927935338020325 and perplexity is 50.801980397144234
At time: 444.85344314575195 and batch: 800, loss is 3.8810822200775146 and perplexity is 48.47664909659314
At time: 446.4370050430298 and batch: 850, loss is 3.895829577445984 and perplexity is 49.1968490351324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.601552327473958 and perplexity of 99.63886780508157
Finished 15 epochs...
Completing Train Step...
At time: 450.5761811733246 and batch: 50, loss is 4.080209393501281 and perplexity is 59.157855823625006
At time: 452.2050995826721 and batch: 100, loss is 4.032613177299499 and perplexity is 56.40812324500134
At time: 453.7920596599579 and batch: 150, loss is 4.03147677898407 and perplexity is 56.34405755773328
At time: 455.3813931941986 and batch: 200, loss is 4.058704977035522 and perplexity is 57.899281607692316
At time: 456.9720141887665 and batch: 250, loss is 4.0452845001220705 and perplexity is 57.127436485061246
At time: 458.56014370918274 and batch: 300, loss is 4.032347249984741 and perplexity is 56.39312477859559
At time: 460.1491494178772 and batch: 350, loss is 3.973386764526367 and perplexity is 53.16428118340011
At time: 461.7363135814667 and batch: 400, loss is 3.9864855098724363 and perplexity is 53.86524743086139
At time: 463.32375264167786 and batch: 450, loss is 4.021848978996277 and perplexity is 55.804191273583356
At time: 464.9236273765564 and batch: 500, loss is 3.9903114652633667 and perplexity is 54.07172820589953
At time: 466.5317692756653 and batch: 550, loss is 3.9922053146362306 and perplexity is 54.17422894429519
At time: 468.13738346099854 and batch: 600, loss is 4.009684209823608 and perplexity is 55.1294584729094
At time: 469.73822951316833 and batch: 650, loss is 3.9773882675170897 and perplexity is 53.37744441577984
At time: 471.3479278087616 and batch: 700, loss is 3.9404669284820555 and perplexity is 51.44261571605258
At time: 472.95892453193665 and batch: 750, loss is 3.9280256748199465 and perplexity is 50.806569892764806
At time: 474.57062673568726 and batch: 800, loss is 3.881981067657471 and perplexity is 48.5202418039828
At time: 476.18157935142517 and batch: 850, loss is 3.8974411249160767 and perplexity is 49.27619601126827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.601581891377767 and perplexity of 99.6418135625288
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 480.4750769138336 and batch: 50, loss is 4.080048189163208 and perplexity is 59.148320089256664
At time: 482.0838222503662 and batch: 100, loss is 4.042509522438049 and perplexity is 56.96912887529932
At time: 483.6923644542694 and batch: 150, loss is 4.042410674095154 and perplexity is 56.96349784962719
At time: 485.299654006958 and batch: 200, loss is 4.069009203910827 and perplexity is 58.498973314880814
At time: 486.9105887413025 and batch: 250, loss is 4.055187821388245 and perplexity is 57.69599852109794
At time: 488.5183925628662 and batch: 300, loss is 4.041650404930115 and perplexity is 56.9202067172211
At time: 490.1264183521271 and batch: 350, loss is 3.980432639122009 and perplexity is 53.54019279948277
At time: 491.73393273353577 and batch: 400, loss is 3.9899115228652953 and perplexity is 54.050106953169035
At time: 493.3719336986542 and batch: 450, loss is 4.02798210144043 and perplexity is 56.14749690316728
At time: 494.9815933704376 and batch: 500, loss is 3.9961207580566405 and perplexity is 54.386760879432615
At time: 496.5893530845642 and batch: 550, loss is 3.996773509979248 and perplexity is 54.422273531376014
At time: 498.19715881347656 and batch: 600, loss is 4.006422462463379 and perplexity is 54.949933049716286
At time: 499.79470038414 and batch: 650, loss is 3.969182333946228 and perplexity is 52.94122489474685
At time: 501.3882884979248 and batch: 700, loss is 3.9297082138061525 and perplexity is 50.89212588282554
At time: 502.98135566711426 and batch: 750, loss is 3.908371868133545 and perplexity is 49.81777600012355
At time: 504.5783796310425 and batch: 800, loss is 3.859420518875122 and perplexity is 47.43785406150434
At time: 506.18725299835205 and batch: 850, loss is 3.8773274707794187 and perplexity is 48.29497272048807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.597735404968262 and perplexity of 99.25927885975767
Finished 17 epochs...
Completing Train Step...
At time: 510.4255087375641 and batch: 50, loss is 4.077375602722168 and perplexity is 58.990452142809566
At time: 512.0272481441498 and batch: 100, loss is 4.034934568405151 and perplexity is 56.53922066591016
At time: 513.6337049007416 and batch: 150, loss is 4.034025049209594 and perplexity is 56.487820537657626
At time: 515.2406418323517 and batch: 200, loss is 4.05999210357666 and perplexity is 57.973853291063634
At time: 516.8481991291046 and batch: 250, loss is 4.045478324890137 and perplexity is 57.13851027034046
At time: 518.4548065662384 and batch: 300, loss is 4.0327212572097775 and perplexity is 56.41422015937168
At time: 520.0622563362122 and batch: 350, loss is 3.972105236053467 and perplexity is 53.096193280933626
At time: 521.6708524227142 and batch: 400, loss is 3.982807545661926 and perplexity is 53.667496861300336
At time: 523.2780816555023 and batch: 450, loss is 4.0207370042800905 and perplexity is 55.742172911645504
At time: 524.885181427002 and batch: 500, loss is 3.989255971908569 and perplexity is 54.0146859652445
At time: 526.4931154251099 and batch: 550, loss is 3.9916541051864622 and perplexity is 54.144375825783555
At time: 528.1001250743866 and batch: 600, loss is 4.003366346359253 and perplexity is 54.782256025136384
At time: 529.7080583572388 and batch: 650, loss is 3.9682959938049316 and perplexity is 52.89432175113329
At time: 531.3137149810791 and batch: 700, loss is 3.931134705543518 and perplexity is 50.9647748841751
At time: 532.9184930324554 and batch: 750, loss is 3.9117628049850466 and perplexity is 49.986991670304995
At time: 534.5729994773865 and batch: 800, loss is 3.8644770431518554 and perplexity is 47.67833220159184
At time: 536.1778881549835 and batch: 850, loss is 3.882611770629883 and perplexity is 48.55085331708213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.597057024637858 and perplexity of 99.19196615175919
Finished 18 epochs...
Completing Train Step...
At time: 540.4041154384613 and batch: 50, loss is 4.0758542776107785 and perplexity is 58.90077671666694
At time: 542.0073637962341 and batch: 100, loss is 4.031931223869324 and perplexity is 56.369668645478015
At time: 543.6124908924103 and batch: 150, loss is 4.030062007904053 and perplexity is 56.26439997647764
At time: 545.2146081924438 and batch: 200, loss is 4.0554900646209715 and perplexity is 57.71343938176465
At time: 546.816624879837 and batch: 250, loss is 4.040516557693482 and perplexity is 56.85570447288234
At time: 548.4223275184631 and batch: 300, loss is 4.028276152610779 and perplexity is 56.16400956800906
At time: 550.0278413295746 and batch: 350, loss is 3.968068633079529 and perplexity is 52.8822970267971
At time: 551.6327702999115 and batch: 400, loss is 3.979533762931824 and perplexity is 53.492088418137854
At time: 553.2390685081482 and batch: 450, loss is 4.017183899879456 and perplexity is 55.544466595359474
At time: 554.8481252193451 and batch: 500, loss is 3.986103744506836 and perplexity is 53.84468746977304
At time: 556.4517307281494 and batch: 550, loss is 3.989084072113037 and perplexity is 54.00540164978017
At time: 558.0675480365753 and batch: 600, loss is 4.001857905387879 and perplexity is 54.69968251993023
At time: 559.6862585544586 and batch: 650, loss is 3.9680901908874513 and perplexity is 52.88343706548722
At time: 561.3046975135803 and batch: 700, loss is 3.932301697731018 and perplexity is 51.02428509552473
At time: 562.9228353500366 and batch: 750, loss is 3.913923153877258 and perplexity is 50.09509774375772
At time: 564.541437625885 and batch: 800, loss is 3.8674202251434324 and perplexity is 47.818864915547536
At time: 566.1582896709442 and batch: 850, loss is 3.8854943561553954 and perplexity is 48.69100720984818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5967973073323565 and perplexity of 99.16620762669497
Finished 19 epochs...
Completing Train Step...
At time: 570.3512263298035 and batch: 50, loss is 4.074298677444458 and perplexity is 58.80922188842582
At time: 571.985493183136 and batch: 100, loss is 4.029684257507324 and perplexity is 56.2431500908939
At time: 573.5761778354645 and batch: 150, loss is 4.027409453392028 and perplexity is 56.115353352992074
At time: 575.2080686092377 and batch: 200, loss is 4.052742505073548 and perplexity is 57.555085912788016
At time: 576.8044481277466 and batch: 250, loss is 4.037456393241882 and perplexity is 56.68198261148585
At time: 578.4043502807617 and batch: 300, loss is 4.02527596950531 and perplexity is 55.9957597716687
At time: 579.9989824295044 and batch: 350, loss is 3.9654650497436523 and perplexity is 52.74479263923574
At time: 581.5874471664429 and batch: 400, loss is 3.977380871772766 and perplexity is 53.377049651308084
At time: 583.175347328186 and batch: 450, loss is 4.0150637483596805 and perplexity is 55.42682865928233
At time: 584.7628662586212 and batch: 500, loss is 3.9842577505111696 and perplexity is 53.74538218668362
At time: 586.3499178886414 and batch: 550, loss is 3.9876567363739013 and perplexity is 53.92837279599453
At time: 587.9373769760132 and batch: 600, loss is 4.001171383857727 and perplexity is 54.66214289754175
At time: 589.5257680416107 and batch: 650, loss is 3.9681099367141726 and perplexity is 52.88448130298157
At time: 591.1152424812317 and batch: 700, loss is 3.933036861419678 and perplexity is 51.061810088983
At time: 592.7027795314789 and batch: 750, loss is 3.915105323791504 and perplexity is 50.154353679552884
At time: 594.2903332710266 and batch: 800, loss is 3.8690843534469606 and perplexity is 47.89850789178179
At time: 595.8786027431488 and batch: 850, loss is 3.8871725511550905 and perplexity is 48.77278881822727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596696535746257 and perplexity of 99.15621499416011
Finished 20 epochs...
Completing Train Step...
At time: 600.0356342792511 and batch: 50, loss is 4.072739291191101 and perplexity is 58.71758706185576
At time: 601.6573667526245 and batch: 100, loss is 4.027752485275268 and perplexity is 56.13460601028028
At time: 603.2390072345734 and batch: 150, loss is 4.025233225822449 and perplexity is 55.993366357823454
At time: 604.8369555473328 and batch: 200, loss is 4.050603494644165 and perplexity is 57.43210655770774
At time: 606.4221332073212 and batch: 250, loss is 4.035168523788452 and perplexity is 56.552449868413824
At time: 608.0042843818665 and batch: 300, loss is 4.022972497940064 and perplexity is 55.86692357348669
At time: 609.5877842903137 and batch: 350, loss is 3.9634397506713865 and perplexity is 52.63807676189791
At time: 611.1695473194122 and batch: 400, loss is 3.9756605434417724 and perplexity is 53.28530254077221
At time: 612.751455783844 and batch: 450, loss is 4.01344717502594 and perplexity is 55.3372995108122
At time: 614.3728935718536 and batch: 500, loss is 3.9828837633132936 and perplexity is 53.67158742775059
At time: 615.9550561904907 and batch: 550, loss is 3.986660761833191 and perplexity is 53.8746882483269
At time: 617.5368027687073 and batch: 600, loss is 4.000748958587646 and perplexity is 54.63905710341976
At time: 619.1190812587738 and batch: 650, loss is 3.9681313180923463 and perplexity is 52.88561205816434
At time: 620.72279047966 and batch: 700, loss is 3.933514943122864 and perplexity is 51.08622764244581
At time: 622.3279454708099 and batch: 750, loss is 3.915817618370056 and perplexity is 50.19009108003694
At time: 623.9370529651642 and batch: 800, loss is 3.8701033401489258 and perplexity is 47.947340710138356
At time: 625.544145822525 and batch: 850, loss is 3.888239541053772 and perplexity is 48.82485666421991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596657752990723 and perplexity of 99.15236951748386
Finished 21 epochs...
Completing Train Step...
At time: 629.7793867588043 and batch: 50, loss is 4.071219511032105 and perplexity is 58.62841701465253
At time: 631.4160304069519 and batch: 100, loss is 4.026017918586731 and perplexity is 56.0373211905164
At time: 633.0258719921112 and batch: 150, loss is 4.023321228027344 and perplexity is 55.88640944807844
At time: 634.6354792118073 and batch: 200, loss is 4.048775267601013 and perplexity is 57.32720354983081
At time: 636.2425458431244 and batch: 250, loss is 4.033256726264954 and perplexity is 56.44443631772582
At time: 637.8495309352875 and batch: 300, loss is 4.021041979789734 and perplexity is 55.75917550179289
At time: 639.457188129425 and batch: 350, loss is 3.9617179822921753 and perplexity is 52.547524163457275
At time: 641.0622589588165 and batch: 400, loss is 3.97416738986969 and perplexity is 53.20579877138377
At time: 642.668470621109 and batch: 450, loss is 4.012099847793579 and perplexity is 55.26279226430814
At time: 644.2733447551727 and batch: 500, loss is 3.9817142724990844 and perplexity is 53.608855688516094
At time: 645.8798878192902 and batch: 550, loss is 3.985814299583435 and perplexity is 53.829104653625095
At time: 647.4855980873108 and batch: 600, loss is 4.000365443229676 and perplexity is 54.618106203628265
At time: 649.0914294719696 and batch: 650, loss is 3.968111724853516 and perplexity is 52.88457586788778
At time: 650.6974186897278 and batch: 700, loss is 3.9338408088684083 and perplexity is 51.10287760678275
At time: 652.305392742157 and batch: 750, loss is 3.9162871503829955 and perplexity is 50.2136624678589
At time: 653.9169759750366 and batch: 800, loss is 3.8707971715927125 and perplexity is 47.98061962641322
At time: 655.5751974582672 and batch: 850, loss is 3.8889903593063355 and perplexity is 48.86152902319767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596648534138997 and perplexity of 99.15145545070438
Finished 22 epochs...
Completing Train Step...
At time: 659.8493916988373 and batch: 50, loss is 4.069764938354492 and perplexity is 58.54319971353536
At time: 661.4594666957855 and batch: 100, loss is 4.024433856010437 and perplexity is 55.9486248359775
At time: 663.0647218227386 and batch: 150, loss is 4.02159704208374 and perplexity is 55.790133908786146
At time: 664.6777334213257 and batch: 200, loss is 4.047149133682251 and perplexity is 57.23405759409523
At time: 666.292530298233 and batch: 250, loss is 4.031574153900147 and perplexity is 56.349544322741714
At time: 667.9071371555328 and batch: 300, loss is 4.019340438842773 and perplexity is 55.66437965388763
At time: 669.5170397758484 and batch: 350, loss is 3.960174808502197 and perplexity is 52.46649673721999
At time: 671.126740694046 and batch: 400, loss is 3.9728142118453977 and perplexity is 53.133850544080104
At time: 672.7375521659851 and batch: 450, loss is 4.010877509117126 and perplexity is 55.195283683534754
At time: 674.3482110500336 and batch: 500, loss is 3.9806623363494875 and perplexity is 53.55249224584776
At time: 675.9589705467224 and batch: 550, loss is 3.9850460720062255 and perplexity is 53.787767531165414
At time: 677.5675966739655 and batch: 600, loss is 3.999984097480774 and perplexity is 54.59728179191727
At time: 679.1595907211304 and batch: 650, loss is 3.9680448007583617 and perplexity is 52.881036733928205
At time: 680.7474608421326 and batch: 700, loss is 3.934042615890503 and perplexity is 51.113191567012834
At time: 682.336816072464 and batch: 750, loss is 3.91660493850708 and perplexity is 50.22962230924769
At time: 683.9257037639618 and batch: 800, loss is 3.871294083595276 and perplexity is 48.00446769690153
At time: 685.5169415473938 and batch: 850, loss is 3.889541230201721 and perplexity is 48.888452832531215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596657435099284 and perplexity of 99.1523379977995
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 689.6914494037628 and batch: 50, loss is 4.070199270248413 and perplexity is 58.56863241506001
At time: 691.2754123210907 and batch: 100, loss is 4.027556357383728 and perplexity is 56.123597527931516
At time: 692.8599050045013 and batch: 150, loss is 4.025397429466247 and perplexity is 56.00256142751927
At time: 694.4444358348846 and batch: 200, loss is 4.0518286991119385 and perplexity is 57.50251575527984
At time: 696.0717356204987 and batch: 250, loss is 4.036037907600403 and perplexity is 56.60163703101896
At time: 697.6566815376282 and batch: 300, loss is 4.023116216659546 and perplexity is 55.87495327319936
At time: 699.243846654892 and batch: 350, loss is 3.9623607540130616 and perplexity is 52.58131108346722
At time: 700.8508288860321 and batch: 400, loss is 3.9734412097930907 and perplexity is 53.167175805667824
At time: 702.4581813812256 and batch: 450, loss is 4.010837988853455 and perplexity is 55.193102394473
At time: 704.0662820339203 and batch: 500, loss is 3.9813659811019897 and perplexity is 53.590187436456205
At time: 705.6751141548157 and batch: 550, loss is 3.984483828544617 and perplexity is 53.75753421059634
At time: 707.2832732200623 and batch: 600, loss is 3.9971537828445434 and perplexity is 54.44297278069968
At time: 708.8915302753448 and batch: 650, loss is 3.962923626899719 and perplexity is 52.61091600894431
At time: 710.5001983642578 and batch: 700, loss is 3.9295371389389038 and perplexity is 50.883420263823666
At time: 712.1075177192688 and batch: 750, loss is 3.9107463121414185 and perplexity is 49.93620606697169
At time: 713.715829372406 and batch: 800, loss is 3.8640392780303956 and perplexity is 47.65746485853534
At time: 715.3235976696014 and batch: 850, loss is 3.8834881019592284 and perplexity is 48.5934185988313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596409797668457 and perplexity of 99.12778720753
Finished 24 epochs...
Completing Train Step...
At time: 719.6180849075317 and batch: 50, loss is 4.068905515670776 and perplexity is 58.49290797375072
At time: 721.2279829978943 and batch: 100, loss is 4.025862736701965 and perplexity is 56.028625888091014
At time: 722.833190202713 and batch: 150, loss is 4.023684911727905 and perplexity is 55.90673812065696
At time: 724.4412462711334 and batch: 200, loss is 4.0500208854675295 and perplexity is 57.398655830690146
At time: 726.0479311943054 and batch: 250, loss is 4.03440676689148 and perplexity is 56.50938705346637
At time: 727.6543552875519 and batch: 300, loss is 4.021652746200561 and perplexity is 55.79324173548145
At time: 729.2628061771393 and batch: 350, loss is 3.960946955680847 and perplexity is 52.50702423923388
At time: 730.8701553344727 and batch: 400, loss is 3.9723255491256713 and perplexity is 53.10789235507986
At time: 732.4774539470673 and batch: 450, loss is 4.010024781227112 and perplexity is 55.148237187521
At time: 734.0854132175446 and batch: 500, loss is 3.980578970909119 and perplexity is 53.54802800483314
At time: 735.6924469470978 and batch: 550, loss is 3.9839514923095702 and perplexity is 53.72892474283307
At time: 737.3300123214722 and batch: 600, loss is 3.997052812576294 and perplexity is 54.43747593664729
At time: 738.9373114109039 and batch: 650, loss is 3.963341269493103 and perplexity is 52.632893157323764
At time: 740.5440044403076 and batch: 700, loss is 3.9303347206115724 and perplexity is 50.9240201359735
At time: 742.1482455730438 and batch: 750, loss is 3.9117251682281493 and perplexity is 49.98511035745495
At time: 743.7461407184601 and batch: 800, loss is 3.8652692079544066 and perplexity is 47.71611626183173
At time: 745.3542215824127 and batch: 850, loss is 3.8844633150100707 and perplexity is 48.640830649501645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596288681030273 and perplexity of 99.11578191022815
Finished 25 epochs...
Completing Train Step...
At time: 749.5710670948029 and batch: 50, loss is 4.067926955223084 and perplexity is 58.43569712423967
At time: 751.2021367549896 and batch: 100, loss is 4.024668602943421 and perplexity is 55.96176014573892
At time: 752.8073451519012 and batch: 150, loss is 4.0224201250076295 and perplexity is 55.83607271845938
At time: 754.4144992828369 and batch: 200, loss is 4.048642745018006 and perplexity is 57.31960690411445
At time: 756.0220408439636 and batch: 250, loss is 4.033155856132507 and perplexity is 56.43874304710388
At time: 757.6301662921906 and batch: 300, loss is 4.020539984703064 and perplexity is 55.731191694108865
At time: 759.2381589412689 and batch: 350, loss is 3.959954118728638 and perplexity is 52.45491919550512
At time: 760.8463761806488 and batch: 400, loss is 3.9715306997299193 and perplexity is 53.065696350887556
At time: 762.4525372982025 and batch: 450, loss is 4.009462523460388 and perplexity is 55.11723837831845
At time: 764.0593450069427 and batch: 500, loss is 3.9800359296798704 and perplexity is 53.518957111942676
At time: 765.6666359901428 and batch: 550, loss is 3.983662543296814 and perplexity is 53.71340206581128
At time: 767.2741539478302 and batch: 600, loss is 3.9971139860153198 and perplexity is 54.44080616612197
At time: 768.8851833343506 and batch: 650, loss is 3.9637474393844605 and perplexity is 52.654275395936104
At time: 770.4924557209015 and batch: 700, loss is 3.9310434579849245 and perplexity is 50.96012468505554
At time: 772.1072020530701 and batch: 750, loss is 3.9125676202774047 and perplexity is 50.027238158942595
At time: 773.7149062156677 and batch: 800, loss is 3.866240406036377 and perplexity is 47.762480573245114
At time: 775.317676782608 and batch: 850, loss is 3.8852180671691894 and perplexity is 48.677556279086396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596241633097331 and perplexity of 99.11111882726237
Finished 26 epochs...
Completing Train Step...
At time: 779.5301291942596 and batch: 50, loss is 4.067199873924255 and perplexity is 58.3932250638681
At time: 781.1688747406006 and batch: 100, loss is 4.023848328590393 and perplexity is 55.91587497092996
At time: 782.7808401584625 and batch: 150, loss is 4.021507630348205 and perplexity is 55.785145839086965
At time: 784.3962972164154 and batch: 200, loss is 4.047638001441956 and perplexity is 57.26204431995668
At time: 786.0083863735199 and batch: 250, loss is 4.0322434616088865 and perplexity is 56.387272131489084
At time: 787.620180606842 and batch: 300, loss is 4.019691886901856 and perplexity is 55.68394623019326
At time: 789.2329828739166 and batch: 350, loss is 3.959242653846741 and perplexity is 52.41761263534215
At time: 790.8451964855194 and batch: 400, loss is 3.9709669589996337 and perplexity is 53.03578948712424
At time: 792.4582464694977 and batch: 450, loss is 4.009055962562561 and perplexity is 55.09483441899314
At time: 794.0697338581085 and batch: 500, loss is 3.979655175209045 and perplexity is 53.498583408683835
At time: 795.6834893226624 and batch: 550, loss is 3.9834766721725465 and perplexity is 53.703419223170904
At time: 797.2946779727936 and batch: 600, loss is 3.99715106010437 and perplexity is 54.442824546832334
At time: 798.9109768867493 and batch: 650, loss is 3.964039783477783 and perplexity is 52.66967081260621
At time: 800.5224587917328 and batch: 700, loss is 3.9315731000900267 and perplexity is 50.98712246171851
At time: 802.1322686672211 and batch: 750, loss is 3.9131991243362427 and perplexity is 50.05884054035825
At time: 803.7423212528229 and batch: 800, loss is 3.8669363403320314 and perplexity is 47.79573169047596
At time: 805.3541753292084 and batch: 850, loss is 3.885761709213257 and perplexity is 48.70402663983075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5962216059366865 and perplexity of 99.10913393283992
Finished 27 epochs...
Completing Train Step...
At time: 809.6049892902374 and batch: 50, loss is 4.0666674137115475 and perplexity is 58.362141271005775
At time: 811.2110140323639 and batch: 100, loss is 4.023256750106811 and perplexity is 55.88280612477984
At time: 812.8164174556732 and batch: 150, loss is 4.02082549571991 and perplexity is 55.747105835042646
At time: 814.4231317043304 and batch: 200, loss is 4.046914343833923 and perplexity is 57.22062119582021
At time: 816.0295226573944 and batch: 250, loss is 4.031545686721802 and perplexity is 56.34794023304586
At time: 817.6347448825836 and batch: 300, loss is 4.019041423797607 and perplexity is 55.647737655119286
At time: 819.294851064682 and batch: 350, loss is 3.958697724342346 and perplexity is 52.38905651291068
At time: 820.8977382183075 and batch: 400, loss is 3.97052827835083 and perplexity is 53.01252881495863
At time: 822.4991879463196 and batch: 450, loss is 4.008721318244934 and perplexity is 55.07640033032774
At time: 824.0930671691895 and batch: 500, loss is 3.979362826347351 and perplexity is 53.48294544470392
At time: 825.6902697086334 and batch: 550, loss is 3.9833203887939455 and perplexity is 53.69502692717755
At time: 827.2974193096161 and batch: 600, loss is 3.9971644496917724 and perplexity is 54.443553518670335
At time: 828.9000117778778 and batch: 650, loss is 3.96422935962677 and perplexity is 52.6796566724778
At time: 830.5035243034363 and batch: 700, loss is 3.931953639984131 and perplexity is 51.00652878810729
At time: 832.1084485054016 and batch: 750, loss is 3.9136606311798094 and perplexity is 50.081948369649176
At time: 833.7183678150177 and batch: 800, loss is 3.867438135147095 and perplexity is 47.81972135926273
At time: 835.3401501178741 and batch: 850, loss is 3.886161732673645 and perplexity is 48.72351329040088
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596210797627767 and perplexity of 99.10806273649251
Finished 28 epochs...
Completing Train Step...
At time: 839.6056854724884 and batch: 50, loss is 4.066251072883606 and perplexity is 58.337847786324254
At time: 841.2175829410553 and batch: 100, loss is 4.022793884277344 and perplexity is 55.85694586874606
At time: 842.8239562511444 and batch: 150, loss is 4.020278668403625 and perplexity is 55.71663012800218
At time: 844.4356081485748 and batch: 200, loss is 4.0463611078262325 and perplexity is 57.18897344293768
At time: 846.0459752082825 and batch: 250, loss is 4.030995044708252 and perplexity is 56.31692123074328
At time: 847.6525464057922 and batch: 300, loss is 4.018510026931763 and perplexity is 55.6181744773237
At time: 849.2600793838501 and batch: 350, loss is 3.958252577781677 and perplexity is 52.36574089440597
At time: 850.8659427165985 and batch: 400, loss is 3.9701635122299193 and perplexity is 52.9931951668074
At time: 852.4718453884125 and batch: 450, loss is 4.008430776596069 and perplexity is 55.06040066655912
At time: 854.0793986320496 and batch: 500, loss is 3.9791213607788087 and perplexity is 53.47003271392748
At time: 855.689178943634 and batch: 550, loss is 3.98317946434021 and perplexity is 53.68746051799809
At time: 857.3005146980286 and batch: 600, loss is 3.9971532154083254 and perplexity is 54.442941887793864
At time: 858.9394474029541 and batch: 650, loss is 3.964347229003906 and perplexity is 52.68586635675614
At time: 860.5474395751953 and batch: 700, loss is 3.932230291366577 and perplexity is 51.020641766907836
At time: 862.1542329788208 and batch: 750, loss is 3.9140057373046875 and perplexity is 50.09923493945636
At time: 863.7606298923492 and batch: 800, loss is 3.8678180170059204 and perplexity is 47.83789065477559
At time: 865.3652226924896 and batch: 850, loss is 3.8864730644226073 and perplexity is 48.73868482857739
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596205075581868 and perplexity of 99.10749563723111
Finished 29 epochs...
Completing Train Step...
At time: 869.6264953613281 and batch: 50, loss is 4.065899724960327 and perplexity is 58.317354505002704
At time: 871.2387864589691 and batch: 100, loss is 4.022400512695312 and perplexity is 55.83497765470105
At time: 872.8510851860046 and batch: 150, loss is 4.0198121786117555 and perplexity is 55.69064495019141
At time: 874.4629046916962 and batch: 200, loss is 4.045907583236694 and perplexity is 57.163042717786055
At time: 876.0744903087616 and batch: 250, loss is 4.030531735420227 and perplexity is 56.290835121499065
At time: 877.6862242221832 and batch: 300, loss is 4.018052325248719 and perplexity is 55.59272377011766
At time: 879.2959630489349 and batch: 350, loss is 3.9578670692443847 and perplexity is 52.345557344944815
At time: 880.9070227146149 and batch: 400, loss is 3.96984299659729 and perplexity is 52.976212741045316
At time: 882.5163667201996 and batch: 450, loss is 4.008168797492981 and perplexity is 55.04597788149349
At time: 884.1268138885498 and batch: 500, loss is 3.97891218662262 and perplexity is 53.45884933463089
At time: 885.7387108802795 and batch: 550, loss is 3.9830487298965456 and perplexity is 53.68044217649499
At time: 887.350611448288 and batch: 600, loss is 3.9971244287490846 and perplexity is 54.44137467993512
At time: 888.9617416858673 and batch: 650, loss is 3.9644180488586427 and perplexity is 52.68959769428302
At time: 890.5738792419434 and batch: 700, loss is 3.9324359846115113 and perplexity is 51.031137447679804
At time: 892.1847655773163 and batch: 750, loss is 3.914273238182068 and perplexity is 50.11263832138704
At time: 893.7959554195404 and batch: 800, loss is 3.8681201410293578 and perplexity is 47.85234581428848
At time: 895.4076969623566 and batch: 850, loss is 3.886728229522705 and perplexity is 48.75112282677435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596200942993164 and perplexity of 99.10708606756045
Finished 30 epochs...
Completing Train Step...
At time: 899.6579570770264 and batch: 50, loss is 4.065585722923279 and perplexity is 58.29904561155822
At time: 901.2931616306305 and batch: 100, loss is 4.022046656608581 and perplexity is 55.815223603257245
At time: 902.9039061069489 and batch: 150, loss is 4.019396553039551 and perplexity is 55.667503303480736
At time: 904.516717672348 and batch: 200, loss is 4.045513758659363 and perplexity is 57.14053493899788
At time: 906.1229898929596 and batch: 250, loss is 4.030121111869812 and perplexity is 56.2677255239229
At time: 907.729425907135 and batch: 300, loss is 4.017643218040466 and perplexity is 55.56998503770374
At time: 909.3351287841797 and batch: 350, loss is 3.957519040107727 and perplexity is 52.32734273560542
At time: 910.9395034313202 and batch: 400, loss is 3.9695504808425905 and perplexity is 52.96071863044046
At time: 912.5431995391846 and batch: 450, loss is 4.007926321029663 and perplexity is 55.032632145535686
At time: 914.1475028991699 and batch: 500, loss is 3.9787245178222657 and perplexity is 53.44881771784813
At time: 915.7539215087891 and batch: 550, loss is 3.9829252910614015 and perplexity is 53.67381633419425
At time: 917.3612196445465 and batch: 600, loss is 3.9970840787887574 and perplexity is 54.43917801694456
At time: 918.9697258472443 and batch: 650, loss is 3.9644576692581177 and perplexity is 52.69168531854783
At time: 920.5783607959747 and batch: 700, loss is 3.9325933027267457 and perplexity is 51.03916620155899
At time: 922.1876883506775 and batch: 750, loss is 3.9144876194000244 and perplexity is 50.123382681478745
At time: 923.7966859340668 and batch: 800, loss is 3.8683708238601686 and perplexity is 47.86434307948953
At time: 925.4050250053406 and batch: 850, loss is 3.8869454717636107 and perplexity is 48.76171478041212
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596198081970215 and perplexity of 99.1068025203184
Finished 31 epochs...
Completing Train Step...
At time: 929.626122713089 and batch: 50, loss is 4.065294814109802 and perplexity is 58.282088371997354
At time: 931.2624185085297 and batch: 100, loss is 4.021718373298645 and perplexity is 55.79690340418028
At time: 932.8667526245117 and batch: 150, loss is 4.019015121459961 and perplexity is 55.64627400878296
At time: 934.4724369049072 and batch: 200, loss is 4.0451568460464475 and perplexity is 57.1201444004061
At time: 936.0782272815704 and batch: 250, loss is 4.029746379852295 and perplexity is 56.24664415579469
At time: 937.6867189407349 and batch: 300, loss is 4.017268257141113 and perplexity is 55.54915237209857
At time: 939.292875289917 and batch: 350, loss is 3.957195963859558 and perplexity is 52.31043974466222
At time: 940.94908452034 and batch: 400, loss is 3.969276394844055 and perplexity is 52.94620482809734
At time: 942.5555539131165 and batch: 450, loss is 4.007697439193725 and perplexity is 55.02003761703813
At time: 944.1612050533295 and batch: 500, loss is 3.9785511302948 and perplexity is 53.439551162873805
At time: 945.7674458026886 and batch: 550, loss is 3.9828067874908446 and perplexity is 53.667456172171626
At time: 947.3785429000854 and batch: 600, loss is 3.9970357990264893 and perplexity is 54.436549769817915
At time: 948.9850888252258 and batch: 650, loss is 3.964476161003113 and perplexity is 52.69265968876499
At time: 950.592759847641 and batch: 700, loss is 3.9327165985107424 and perplexity is 51.045459503531184
At time: 952.1982622146606 and batch: 750, loss is 3.9146644639968873 and perplexity is 50.132247514708276
At time: 953.8026146888733 and batch: 800, loss is 3.868585147857666 and perplexity is 47.874602656233385
At time: 955.4066061973572 and batch: 850, loss is 3.8871355295181274 and perplexity is 48.77098320316958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596198717753093 and perplexity of 99.10686553074655
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 959.6582088470459 and batch: 50, loss is 4.065360856056214 and perplexity is 58.28593756165694
At time: 961.3002512454987 and batch: 100, loss is 4.022238821983337 and perplexity is 55.825950387243275
At time: 962.9055171012878 and batch: 150, loss is 4.019719781875611 and perplexity is 55.68549955407678
At time: 964.5180292129517 and batch: 200, loss is 4.045996541976929 and perplexity is 57.16812809624528
At time: 966.1192092895508 and batch: 250, loss is 4.030745224952698 and perplexity is 56.302853908468514
At time: 967.7118554115295 and batch: 300, loss is 4.018040552139282 and perplexity is 55.59206927474956
At time: 969.31343126297 and batch: 350, loss is 3.957673497200012 and perplexity is 52.33542568903004
At time: 970.9205870628357 and batch: 400, loss is 3.9692146396636963 and perplexity is 52.94293522662733
At time: 972.5284509658813 and batch: 450, loss is 4.007360520362854 and perplexity is 55.00150345271953
At time: 974.1353042125702 and batch: 500, loss is 3.9785502195358275 and perplexity is 53.43950249234527
At time: 975.7455253601074 and batch: 550, loss is 3.9824132442474367 and perplexity is 53.646339862767164
At time: 977.3531875610352 and batch: 600, loss is 3.996035499572754 and perplexity is 54.38212414431984
At time: 978.9667749404907 and batch: 650, loss is 3.9627802848815916 and perplexity is 52.603375194538735
At time: 980.5765852928162 and batch: 700, loss is 3.9311221981048585 and perplexity is 50.96413744936579
At time: 982.211835861206 and batch: 750, loss is 3.9131523895263673 and perplexity is 50.05650110462998
At time: 983.8194153308868 and batch: 800, loss is 3.8669835472106935 and perplexity is 47.797988031039424
At time: 985.427613735199 and batch: 850, loss is 3.885941972732544 and perplexity is 48.71280699044098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596203168233235 and perplexity of 99.10730660486509
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 989.6796407699585 and batch: 50, loss is 4.065263371467591 and perplexity is 58.28025585795502
At time: 991.2906305789948 and batch: 100, loss is 4.022194390296936 and perplexity is 55.823470001226895
At time: 992.9017145633698 and batch: 150, loss is 4.019705581665039 and perplexity is 55.684708813871666
At time: 994.5121059417725 and batch: 200, loss is 4.046041464805603 and perplexity is 57.17069630795459
At time: 996.1213366985321 and batch: 250, loss is 4.0308226156234745 and perplexity is 56.307211392711274
At time: 997.730667591095 and batch: 300, loss is 4.0181229162216185 and perplexity is 55.596648253089654
At time: 999.3428344726562 and batch: 350, loss is 3.9577116632461546 and perplexity is 52.33742316341939
At time: 1000.9487247467041 and batch: 400, loss is 3.9691773796081544 and perplexity is 52.940962606670425
At time: 1002.5544259548187 and batch: 450, loss is 4.00727831363678 and perplexity is 54.99698214503501
At time: 1004.1644184589386 and batch: 500, loss is 3.978504514694214 and perplexity is 53.43706010416286
At time: 1005.7769551277161 and batch: 550, loss is 3.982346940040588 and perplexity is 53.64278300267093
At time: 1007.3879475593567 and batch: 600, loss is 3.995871911048889 and perplexity is 54.37322858053234
At time: 1008.9977684020996 and batch: 650, loss is 3.962512254714966 and perplexity is 52.589277792469176
At time: 1010.6081438064575 and batch: 700, loss is 3.930861053466797 and perplexity is 50.950830175774655
At time: 1012.2182331085205 and batch: 750, loss is 3.9129022789001464 and perplexity is 50.04398300731211
At time: 1013.8279039859772 and batch: 800, loss is 3.866706018447876 and perplexity is 47.78472455513916
At time: 1015.4383082389832 and batch: 850, loss is 3.8857346200942993 and perplexity is 48.70270730852923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5962066650390625 and perplexity of 99.10765316447826
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1019.6916270256042 and batch: 50, loss is 4.065246906280517 and perplexity is 58.27929627053955
At time: 1021.2941353321075 and batch: 100, loss is 4.022187714576721 and perplexity is 55.82309734060363
At time: 1022.9349675178528 and batch: 150, loss is 4.019703269004822 and perplexity is 55.6845800342098
At time: 1024.5421900749207 and batch: 200, loss is 4.046047859191894 and perplexity is 57.17106188064015
At time: 1026.1483478546143 and batch: 250, loss is 4.030835366249084 and perplexity is 56.30792934946005
At time: 1027.754725933075 and batch: 300, loss is 4.0181374168396 and perplexity is 55.597454444692126
At time: 1029.3602797985077 and batch: 350, loss is 3.957717943191528 and perplexity is 52.337751840609876
At time: 1030.9662878513336 and batch: 400, loss is 3.9691708755493162 and perplexity is 52.940618276654455
At time: 1032.5717747211456 and batch: 450, loss is 4.007263879776001 and perplexity is 54.9961883319804
At time: 1034.176654100418 and batch: 500, loss is 3.978495693206787 and perplexity is 53.43658871188822
At time: 1035.7807064056396 and batch: 550, loss is 3.982336368560791 and perplexity is 53.64221592207159
At time: 1037.383724451065 and batch: 600, loss is 3.9958438873291016 and perplexity is 54.371704861760904
At time: 1038.9723041057587 and batch: 650, loss is 3.962466320991516 and perplexity is 52.58686222660503
At time: 1040.556440114975 and batch: 700, loss is 3.930815968513489 and perplexity is 50.94853311175708
At time: 1042.1370832920074 and batch: 750, loss is 3.912858695983887 and perplexity is 50.041801992119254
At time: 1043.7186045646667 and batch: 800, loss is 3.8666575241088865 and perplexity is 47.78240732269485
At time: 1045.3002412319183 and batch: 850, loss is 3.885698370933533 and perplexity is 48.70094190825957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.596206347147624 and perplexity of 99.10762165900883
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1049.468632221222 and batch: 50, loss is 4.065244364738464 and perplexity is 58.27914815144547
At time: 1051.0516273975372 and batch: 100, loss is 4.022186880111694 and perplexity is 55.823050758200644
At time: 1052.633977174759 and batch: 150, loss is 4.0197034311294555 and perplexity is 55.68458906205267
At time: 1054.2169017791748 and batch: 200, loss is 4.0460493803024296 and perplexity is 57.17114884421083
At time: 1055.800410747528 and batch: 250, loss is 4.030838150978088 and perplexity is 56.30808615200238
At time: 1057.3843715190887 and batch: 300, loss is 4.018140463829041 and perplexity is 55.597623849806865
At time: 1058.9671256542206 and batch: 350, loss is 3.9577197265625 and perplexity is 52.337845178320464
At time: 1060.551168680191 and batch: 400, loss is 3.969170484542847 and perplexity is 52.94059757653426
At time: 1062.133316040039 and batch: 450, loss is 4.007262048721313 and perplexity is 54.996087631044105
At time: 1063.7751441001892 and batch: 500, loss is 3.978494825363159 and perplexity is 53.436542337305326
At time: 1065.3590383529663 and batch: 550, loss is 3.9823350620269777 and perplexity is 53.64214583674846
At time: 1066.9429552555084 and batch: 600, loss is 3.9958398008346556 and perplexity is 54.37148267254496
At time: 1068.5246994495392 and batch: 650, loss is 3.9624590635299684 and perplexity is 52.586480580859394
At time: 1070.1091680526733 and batch: 700, loss is 3.9308089542388918 and perplexity is 50.94817574600884
At time: 1071.6938462257385 and batch: 750, loss is 3.912852029800415 and perplexity is 50.04146840539779
At time: 1073.2802600860596 and batch: 800, loss is 3.8666499757766726 and perplexity is 47.78204664657165
At time: 1074.8639600276947 and batch: 850, loss is 3.885692949295044 and perplexity is 48.700677870074244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5962066650390625 and perplexity of 99.10765316447826
Annealing...
Model not improving. Stopping early with 99.1068025203184loss at 35 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f079a7ddba8>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 13.533260850088455, 'anneal': 4.875188484560672, 'dropout': 0.375251703046402, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1893444061279297 and batch: 50, loss is 6.9510430908203125 and perplexity is 1044.2383957768582
At time: 3.8115928173065186 and batch: 100, loss is 6.131372175216675 and perplexity is 460.0670203192593
At time: 5.4293928146362305 and batch: 150, loss is 5.9231673431396485 and perplexity is 373.59313956585146
At time: 7.043766260147095 and batch: 200, loss is 5.8985646629333495 and perplexity is 364.5138919008688
At time: 8.660170793533325 and batch: 250, loss is 5.922087917327881 and perplexity is 373.19009105752093
At time: 10.27796483039856 and batch: 300, loss is 5.84120701789856 and perplexity is 344.1945390078116
At time: 11.90165662765503 and batch: 350, loss is 5.810823097229004 and perplexity is 333.89383969511806
At time: 13.520731925964355 and batch: 400, loss is 5.822303352355957 and perplexity is 337.74911358157345
At time: 15.134993553161621 and batch: 450, loss is 5.811527376174927 and perplexity is 334.1290769231789
At time: 16.736992120742798 and batch: 500, loss is 5.811952905654907 and perplexity is 334.2712889510877
At time: 18.398141145706177 and batch: 550, loss is 5.769358949661255 and perplexity is 320.33231767296627
At time: 19.99614906311035 and batch: 600, loss is 5.7937220764160156 and perplexity is 328.232459849917
At time: 21.59177589416504 and batch: 650, loss is 5.801579761505127 and perplexity is 330.8217668121317
At time: 23.188183784484863 and batch: 700, loss is 5.754996843338013 and perplexity is 315.76455076067776
At time: 24.785202503204346 and batch: 750, loss is 5.745570964813233 and perplexity is 312.8021758888095
At time: 26.40162944793701 and batch: 800, loss is 5.763237972259521 and perplexity is 318.37755941414565
At time: 28.02474093437195 and batch: 850, loss is 5.741992454528809 and perplexity is 311.68481052950636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.229138692220052 and perplexity of 186.6319866915925
Finished 1 epochs...
Completing Train Step...
At time: 32.319384813308716 and batch: 50, loss is 5.5350416564941405 and perplexity is 253.4183439369776
At time: 33.92820620536804 and batch: 100, loss is 5.4416522026062015 and perplexity is 230.82323533750446
At time: 35.53702139854431 and batch: 150, loss is 5.4291042232513425 and perplexity is 227.94496614270435
At time: 37.14336133003235 and batch: 200, loss is 5.433553190231323 and perplexity is 228.96134501188237
At time: 38.749834299087524 and batch: 250, loss is 5.473373241424561 and perplexity is 238.26255584143846
At time: 40.3520393371582 and batch: 300, loss is 5.4273306751251225 and perplexity is 227.54105306064042
At time: 41.97576570510864 and batch: 350, loss is 5.399432249069214 and perplexity is 221.28074818274894
At time: 43.5825297832489 and batch: 400, loss is 5.40152156829834 and perplexity is 221.74355761492873
At time: 45.19017934799194 and batch: 450, loss is 5.412704162597656 and perplexity is 224.23714224559947
At time: 46.79752016067505 and batch: 500, loss is 5.418716840744018 and perplexity is 225.58946949047592
At time: 48.39329719543457 and batch: 550, loss is 5.394041938781738 and perplexity is 220.09118522639173
At time: 50.004658937454224 and batch: 600, loss is 5.427450199127197 and perplexity is 227.56825130332746
At time: 51.61525630950928 and batch: 650, loss is 5.440069389343262 and perplexity is 230.45817424720093
At time: 53.23038911819458 and batch: 700, loss is 5.392019519805908 and perplexity is 219.64651843981434
At time: 54.843347787857056 and batch: 750, loss is 5.39938889503479 and perplexity is 221.27115497752848
At time: 56.45539665222168 and batch: 800, loss is 5.402229080200195 and perplexity is 221.90049933362124
At time: 58.067636251449585 and batch: 850, loss is 5.387398900985718 and perplexity is 218.63395673528183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.1678892771403 and perplexity of 175.54392160324875
Finished 2 epochs...
Completing Train Step...
At time: 62.33756494522095 and batch: 50, loss is 5.3857325839996335 and perplexity is 218.26994662178276
At time: 63.944151163101196 and batch: 100, loss is 5.310810317993164 and perplexity is 202.51426287020192
At time: 65.55307173728943 and batch: 150, loss is 5.282967901229858 and perplexity is 196.95354745850713
At time: 67.14245271682739 and batch: 200, loss is 5.323627538681031 and perplexity is 205.12663880571426
At time: 68.72453165054321 and batch: 250, loss is 5.363779783248901 and perplexity is 213.53052213040357
At time: 70.30823016166687 and batch: 300, loss is 5.3193387603759765 and perplexity is 204.2487799440192
At time: 71.91623616218567 and batch: 350, loss is 5.287353277206421 and perplexity is 197.81915944378053
At time: 73.55030345916748 and batch: 400, loss is 5.304656352996826 and perplexity is 201.27182406874695
At time: 75.1562922000885 and batch: 450, loss is 5.320015726089477 and perplexity is 204.38709617745712
At time: 76.76048636436462 and batch: 500, loss is 5.322221088409424 and perplexity is 204.83834117451653
At time: 78.37123918533325 and batch: 550, loss is 5.297048816680908 and perplexity is 199.74645088042658
At time: 79.98243713378906 and batch: 600, loss is 5.313399238586426 and perplexity is 203.03923547875485
At time: 81.59638285636902 and batch: 650, loss is 5.31638352394104 and perplexity is 203.6460675252323
At time: 83.20607948303223 and batch: 700, loss is 5.290448379516602 and perplexity is 198.43237847946438
At time: 84.81594848632812 and batch: 750, loss is 5.299416036605835 and perplexity is 200.21985476336727
At time: 86.42738151550293 and batch: 800, loss is 5.300188255310059 and perplexity is 200.37452799326692
At time: 88.04232621192932 and batch: 850, loss is 5.287063541412354 and perplexity is 197.7618524548813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 5.177177747090657 and perplexity of 177.18205212956164
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 92.32501173019409 and batch: 50, loss is 5.301492176055908 and perplexity is 200.6359709106367
At time: 93.96579837799072 and batch: 100, loss is 5.1980975723266605 and perplexity is 180.92771237094212
At time: 95.5759494304657 and batch: 150, loss is 5.170364160537719 and perplexity is 175.97891039158046
At time: 97.18648195266724 and batch: 200, loss is 5.155753021240234 and perplexity is 173.4263513274659
At time: 98.80136561393738 and batch: 250, loss is 5.158204755783081 and perplexity is 173.85206836298724
At time: 100.4152364730835 and batch: 300, loss is 5.115020923614502 and perplexity is 166.50426481792124
At time: 102.0254955291748 and batch: 350, loss is 5.06093936920166 and perplexity is 157.73862155272946
At time: 103.63403606414795 and batch: 400, loss is 5.061055793762207 and perplexity is 157.75698727151465
At time: 105.22905969619751 and batch: 450, loss is 5.085371780395508 and perplexity is 161.64002265954292
At time: 106.81387615203857 and batch: 500, loss is 5.076176853179931 and perplexity is 160.16056658510362
At time: 108.40312051773071 and batch: 550, loss is 5.051773166656494 and perplexity is 156.29936373979757
At time: 109.98640561103821 and batch: 600, loss is 5.069877824783325 and perplexity is 159.15488137329683
At time: 111.57079124450684 and batch: 650, loss is 5.036060752868653 and perplexity is 153.8627163973325
At time: 113.15447211265564 and batch: 700, loss is 4.966547079086304 and perplexity is 143.53043152402643
At time: 114.77785658836365 and batch: 750, loss is 4.94727520942688 and perplexity is 140.79081531055527
At time: 116.36223292350769 and batch: 800, loss is 4.917412195205689 and perplexity is 136.64853550678734
At time: 117.94633793830872 and batch: 850, loss is 4.928906078338623 and perplexity is 138.2282187606703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.943442662556966 and perplexity of 140.2522605893392
Finished 4 epochs...
Completing Train Step...
At time: 122.0982186794281 and batch: 50, loss is 5.098962688446045 and perplexity is 163.85185369489085
At time: 123.68757963180542 and batch: 100, loss is 5.032910709381103 and perplexity is 153.37880472112596
At time: 125.27384996414185 and batch: 150, loss is 5.0231122875213625 and perplexity is 151.88327337699477
At time: 126.8617491722107 and batch: 200, loss is 5.025023984909057 and perplexity is 152.17390594619397
At time: 128.44955396652222 and batch: 250, loss is 5.032856855392456 and perplexity is 153.3705448831324
At time: 130.0484104156494 and batch: 300, loss is 5.00165979385376 and perplexity is 148.6596988978909
At time: 131.63695907592773 and batch: 350, loss is 4.949843063354492 and perplexity is 141.15281013462524
At time: 133.22545337677002 and batch: 400, loss is 4.95867847442627 and perplexity is 142.40547900925435
At time: 134.81434321403503 and batch: 450, loss is 4.991970252990723 and perplexity is 147.22621080694793
At time: 136.4006428718567 and batch: 500, loss is 4.982600984573364 and perplexity is 145.8532507788945
At time: 137.98761582374573 and batch: 550, loss is 4.965540924072266 and perplexity is 143.38609028768775
At time: 139.58553051948547 and batch: 600, loss is 4.989194593429565 and perplexity is 146.81812757932403
At time: 141.19500255584717 and batch: 650, loss is 4.959771308898926 and perplexity is 142.5611896933155
At time: 142.8125398159027 and batch: 700, loss is 4.904336729049683 and perplexity is 134.87342270967576
At time: 144.4357099533081 and batch: 750, loss is 4.899549341201782 and perplexity is 134.2292744488422
At time: 146.05267643928528 and batch: 800, loss is 4.881178865432739 and perplexity is 131.7859302264532
At time: 147.66984486579895 and batch: 850, loss is 4.902746200561523 and perplexity is 134.65907319825075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.928386052449544 and perplexity of 138.15635519539018
Finished 5 epochs...
Completing Train Step...
At time: 152.00896906852722 and batch: 50, loss is 5.020808916091919 and perplexity is 151.53383238518347
At time: 153.62159872055054 and batch: 100, loss is 4.962506494522095 and perplexity is 142.9516547635795
At time: 155.25721836090088 and batch: 150, loss is 4.95425085067749 and perplexity is 141.77635491859
At time: 156.8559787273407 and batch: 200, loss is 4.957215156555176 and perplexity is 142.19724691890522
At time: 158.45378494262695 and batch: 250, loss is 4.966291522979736 and perplexity is 143.4937561322819
At time: 160.0616261959076 and batch: 300, loss is 4.941002321243286 and perplexity is 139.91041448372198
At time: 161.6678876876831 and batch: 350, loss is 4.88739483833313 and perplexity is 132.60765927428926
At time: 163.27361178398132 and batch: 400, loss is 4.902490520477295 and perplexity is 134.62464795618527
At time: 164.88084387779236 and batch: 450, loss is 4.939338264465332 and perplexity is 139.67778921470165
At time: 166.48818349838257 and batch: 500, loss is 4.932675046920776 and perplexity is 138.75017958344097
At time: 168.09508347511292 and batch: 550, loss is 4.915056352615356 and perplexity is 136.32699196862856
At time: 169.70227575302124 and batch: 600, loss is 4.941351022720337 and perplexity is 139.9592099589399
At time: 171.31123328208923 and batch: 650, loss is 4.9144649410247805 and perplexity is 136.24639044215778
At time: 172.919819355011 and batch: 700, loss is 4.865813941955566 and perplexity is 129.77652619441523
At time: 174.52619624137878 and batch: 750, loss is 4.866027450561523 and perplexity is 129.80423755781095
At time: 176.13446426391602 and batch: 800, loss is 4.848101434707641 and perplexity is 127.49809645312786
At time: 177.7419090270996 and batch: 850, loss is 4.87277304649353 and perplexity is 130.68280439161384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.919427553812663 and perplexity of 136.9242090061233
Finished 6 epochs...
Completing Train Step...
At time: 181.99095273017883 and batch: 50, loss is 4.965616436004638 and perplexity is 143.3969180572485
At time: 183.59768271446228 and batch: 100, loss is 4.907824783325196 and perplexity is 135.34468995346674
At time: 185.206294298172 and batch: 150, loss is 4.899205741882324 and perplexity is 134.18316128416993
At time: 186.81599140167236 and batch: 200, loss is 4.906893157958985 and perplexity is 135.21865812346456
At time: 188.4253215789795 and batch: 250, loss is 4.914996252059937 and perplexity is 136.31879888689943
At time: 190.03467059135437 and batch: 300, loss is 4.894466676712036 and perplexity is 133.54876295292195
At time: 191.6453492641449 and batch: 350, loss is 4.83880202293396 and perplexity is 126.31793506214176
At time: 193.2551600933075 and batch: 400, loss is 4.857746562957764 and perplexity is 128.73378153613675
At time: 194.86135268211365 and batch: 450, loss is 4.897435054779053 and perplexity is 133.94577512134015
At time: 196.49949741363525 and batch: 500, loss is 4.891127901077271 and perplexity is 133.10361713165162
At time: 198.1039490699768 and batch: 550, loss is 4.873600511550904 and perplexity is 130.79098459734496
At time: 199.70942616462708 and batch: 600, loss is 4.902499189376831 and perplexity is 134.625815008792
At time: 201.31321454048157 and batch: 650, loss is 4.87663857460022 and perplexity is 131.18894005563206
At time: 202.91464114189148 and batch: 700, loss is 4.833994598388672 and perplexity is 125.71212847209848
At time: 204.51758289337158 and batch: 750, loss is 4.835018129348755 and perplexity is 125.84086459911462
At time: 206.11335039138794 and batch: 800, loss is 4.817746267318726 and perplexity is 123.68602119564085
At time: 207.7133069038391 and batch: 850, loss is 4.844564380645752 and perplexity is 127.04792540218877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.916262944539388 and perplexity of 136.491582292875
Finished 7 epochs...
Completing Train Step...
At time: 211.93718361854553 and batch: 50, loss is 4.922402448654175 and perplexity is 137.33215062001827
At time: 213.57574701309204 and batch: 100, loss is 4.870902700424194 and perplexity is 130.4386107565793
At time: 215.19017696380615 and batch: 150, loss is 4.858684196472168 and perplexity is 128.8545432504089
At time: 216.80076098442078 and batch: 200, loss is 4.8668256282806395 and perplexity is 129.9078857674195
At time: 218.40987086296082 and batch: 250, loss is 4.874303789138794 and perplexity is 130.8829993176631
At time: 220.01966786384583 and batch: 300, loss is 4.855443048477173 and perplexity is 128.4375826863705
At time: 221.62833166122437 and batch: 350, loss is 4.798298902511597 and perplexity is 121.3038921623639
At time: 223.23738408088684 and batch: 400, loss is 4.82055401802063 and perplexity is 124.03378870223129
At time: 224.84371542930603 and batch: 450, loss is 4.861440286636353 and perplexity is 129.21016783126535
At time: 226.45000743865967 and batch: 500, loss is 4.854921226501465 and perplexity is 128.37057861683633
At time: 228.05608224868774 and batch: 550, loss is 4.839369831085205 and perplexity is 126.38967978200925
At time: 229.66544437408447 and batch: 600, loss is 4.86834225654602 and perplexity is 130.10505721897465
At time: 231.27235531806946 and batch: 650, loss is 4.84251651763916 and perplexity is 126.78801487710113
At time: 232.8782217502594 and batch: 700, loss is 4.804945945739746 and perplexity is 122.11289012083733
At time: 234.48649549484253 and batch: 750, loss is 4.80708535194397 and perplexity is 122.37441885392192
At time: 236.12013173103333 and batch: 800, loss is 4.79058876991272 and perplexity is 120.37221934487151
At time: 237.72404670715332 and batch: 850, loss is 4.817699403762817 and perplexity is 123.68022496468846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.913113594055176 and perplexity of 136.06239864169473
Finished 8 epochs...
Completing Train Step...
At time: 241.89475440979004 and batch: 50, loss is 4.8839182281494145 and perplexity is 132.14743461004912
At time: 243.52388620376587 and batch: 100, loss is 4.8342484092712406 and perplexity is 125.7440396279036
At time: 245.11882376670837 and batch: 150, loss is 4.823452291488647 and perplexity is 124.3937939860425
At time: 246.70618867874146 and batch: 200, loss is 4.833545503616333 and perplexity is 125.65568448767989
At time: 248.2937662601471 and batch: 250, loss is 4.839587955474854 and perplexity is 126.41725146068812
At time: 249.88166213035583 and batch: 300, loss is 4.822593879699707 and perplexity is 124.28705870477995
At time: 251.46930313110352 and batch: 350, loss is 4.764594354629517 and perplexity is 117.28353211527222
At time: 253.05623817443848 and batch: 400, loss is 4.79000108718872 and perplexity is 120.30149945357762
At time: 254.64163303375244 and batch: 450, loss is 4.829864225387573 and perplexity is 125.19396133917458
At time: 256.22519731521606 and batch: 500, loss is 4.825540428161621 and perplexity is 124.65381661606818
At time: 257.81294536590576 and batch: 550, loss is 4.809024934768677 and perplexity is 122.6120045089597
At time: 259.4007270336151 and batch: 600, loss is 4.838207683563232 and perplexity is 126.24288164591474
At time: 260.9821891784668 and batch: 650, loss is 4.814240427017212 and perplexity is 123.25315697756768
At time: 262.5633943080902 and batch: 700, loss is 4.777550487518311 and perplexity is 118.81295948086529
At time: 264.1458206176758 and batch: 750, loss is 4.7827918910980225 and perplexity is 119.4373410404421
At time: 265.7310559749603 and batch: 800, loss is 4.766172676086426 and perplexity is 117.46878938985981
At time: 267.3149027824402 and batch: 850, loss is 4.794160995483399 and perplexity is 120.80298500278776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.912242889404297 and perplexity of 135.9439800396828
Finished 9 epochs...
Completing Train Step...
At time: 271.4571087360382 and batch: 50, loss is 4.849714565277099 and perplexity is 127.70393350645006
At time: 273.08075428009033 and batch: 100, loss is 4.799644842147827 and perplexity is 121.46726980241094
At time: 274.6641585826874 and batch: 150, loss is 4.794052505493164 and perplexity is 120.78987979902912
At time: 276.2888300418854 and batch: 200, loss is 4.8042662239074705 and perplexity is 122.02991552643627
At time: 277.88060331344604 and batch: 250, loss is 4.808509893417359 and perplexity is 122.5488705161737
At time: 279.4714250564575 and batch: 300, loss is 4.792793416976929 and perplexity is 120.63789035265908
At time: 281.07075238227844 and batch: 350, loss is 4.734621915817261 and perplexity is 113.8204168827081
At time: 282.65296697616577 and batch: 400, loss is 4.761857271194458 and perplexity is 116.96295622393147
At time: 284.24185705184937 and batch: 450, loss is 4.801869297027588 and perplexity is 121.73776900855341
At time: 285.8240375518799 and batch: 500, loss is 4.798871898651123 and perplexity is 121.37341874161093
At time: 287.407511472702 and batch: 550, loss is 4.781824054718018 and perplexity is 119.32180115752209
At time: 288.9909255504608 and batch: 600, loss is 4.8120459461212155 and perplexity is 122.98297684021072
At time: 290.5753381252289 and batch: 650, loss is 4.786972064971923 and perplexity is 119.9376548638803
At time: 292.1669158935547 and batch: 700, loss is 4.754277458190918 and perplexity is 116.07975038357594
At time: 293.75692772865295 and batch: 750, loss is 4.759723005294799 and perplexity is 116.71359237397651
At time: 295.36908292770386 and batch: 800, loss is 4.743299522399902 and perplexity is 114.81240349147262
At time: 296.97742676734924 and batch: 850, loss is 4.770586385726928 and perplexity is 117.9884083982106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.913344383239746 and perplexity of 136.09380399559655
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 301.16576957702637 and batch: 50, loss is 4.827148733139038 and perplexity is 124.85445927382911
At time: 302.756671667099 and batch: 100, loss is 4.779877319335937 and perplexity is 119.08973914031364
At time: 304.34799361228943 and batch: 150, loss is 4.767534828186035 and perplexity is 117.62890877668728
At time: 305.9406752586365 and batch: 200, loss is 4.770478792190552 and perplexity is 117.9757142910138
At time: 307.53988432884216 and batch: 250, loss is 4.76786714553833 and perplexity is 117.66800540008867
At time: 309.13275599479675 and batch: 300, loss is 4.737192077636719 and perplexity is 114.1133300283043
At time: 310.725736618042 and batch: 350, loss is 4.675339803695679 and perplexity is 107.26901130622818
At time: 312.31843066215515 and batch: 400, loss is 4.697843885421753 and perplexity is 109.71036913004804
At time: 313.9115490913391 and batch: 450, loss is 4.73852123260498 and perplexity is 114.26510517185484
At time: 315.5029881000519 and batch: 500, loss is 4.723955087661743 and perplexity is 112.61276640592534
At time: 317.14259362220764 and batch: 550, loss is 4.696078300476074 and perplexity is 109.51683705286284
At time: 318.7339165210724 and batch: 600, loss is 4.7138487911224365 and perplexity is 111.48040004766644
At time: 320.3290660381317 and batch: 650, loss is 4.6806843471527095 and perplexity is 107.84384995550404
At time: 321.92111587524414 and batch: 700, loss is 4.64463770866394 and perplexity is 104.02567135818455
At time: 323.5133304595947 and batch: 750, loss is 4.634284076690673 and perplexity is 102.9541843042085
At time: 325.1058142185211 and batch: 800, loss is 4.606876029968261 and perplexity is 100.17072997596722
At time: 326.6993315219879 and batch: 850, loss is 4.652214574813843 and perplexity is 104.8168535017491
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8714243570963545 and perplexity of 130.50667267912232
Finished 11 epochs...
Completing Train Step...
At time: 330.8847403526306 and batch: 50, loss is 4.781314563751221 and perplexity is 119.26102326189951
At time: 332.4825711250305 and batch: 100, loss is 4.737902278900147 and perplexity is 114.19440224485204
At time: 334.0792315006256 and batch: 150, loss is 4.726943073272705 and perplexity is 112.94975493928015
At time: 335.6735911369324 and batch: 200, loss is 4.734854068756103 and perplexity is 113.84684369440001
At time: 337.2673976421356 and batch: 250, loss is 4.734503211975098 and perplexity is 113.80690676377492
At time: 338.852956533432 and batch: 300, loss is 4.708920497894287 and perplexity is 110.93234354787766
At time: 340.44007539749146 and batch: 350, loss is 4.647218189239502 and perplexity is 104.29445422782179
At time: 342.0274667739868 and batch: 400, loss is 4.672841644287109 and perplexity is 107.00137066007565
At time: 343.61280512809753 and batch: 450, loss is 4.717129402160644 and perplexity is 111.84672443397494
At time: 345.19863533973694 and batch: 500, loss is 4.704312648773193 and perplexity is 110.42235991232552
At time: 346.7884850502014 and batch: 550, loss is 4.68107458114624 and perplexity is 107.88594250418743
At time: 348.37638878822327 and batch: 600, loss is 4.7029805183410645 and perplexity is 110.27536085899341
At time: 349.9654657840729 and batch: 650, loss is 4.672238864898682 and perplexity is 106.93689187450207
At time: 351.55358052253723 and batch: 700, loss is 4.640086812973022 and perplexity is 103.5533369661263
At time: 353.1416702270508 and batch: 750, loss is 4.634652080535889 and perplexity is 102.99207881214814
At time: 354.74054884910583 and batch: 800, loss is 4.612470493316651 and perplexity is 100.73270195349798
At time: 356.32817125320435 and batch: 850, loss is 4.657907876968384 and perplexity is 105.41530949851771
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.868948300679524 and perplexity of 130.1839305235791
Finished 12 epochs...
Completing Train Step...
At time: 360.53385043144226 and batch: 50, loss is 4.7648303413391115 and perplexity is 117.31121273610664
At time: 362.1139416694641 and batch: 100, loss is 4.721208839416504 and perplexity is 112.30392806127439
At time: 363.69416904449463 and batch: 150, loss is 4.709553413391113 and perplexity is 111.00257657064812
At time: 365.27574348449707 and batch: 200, loss is 4.718511934280396 and perplexity is 112.0014630639097
At time: 366.85672187805176 and batch: 250, loss is 4.719102869033813 and perplexity is 112.06766818039328
At time: 368.43925857543945 and batch: 300, loss is 4.694327983856201 and perplexity is 109.32531557332321
At time: 370.0225787162781 and batch: 350, loss is 4.633464460372925 and perplexity is 102.86983594613947
At time: 371.60757756233215 and batch: 400, loss is 4.661385622024536 and perplexity is 105.78255529344855
At time: 373.1930510997772 and batch: 450, loss is 4.706956844329834 and perplexity is 110.71472459010494
At time: 374.77642798423767 and batch: 500, loss is 4.694814205169678 and perplexity is 109.37848479681486
At time: 376.35960149765015 and batch: 550, loss is 4.6731053161621094 and perplexity is 107.02958763195267
At time: 377.94655323028564 and batch: 600, loss is 4.696407642364502 and perplexity is 109.5529114748755
At time: 379.53495359420776 and batch: 650, loss is 4.667388925552368 and perplexity is 106.41951008432838
At time: 381.1299731731415 and batch: 700, loss is 4.637125730514526 and perplexity is 103.24716052707174
At time: 382.7202408313751 and batch: 750, loss is 4.633425436019897 and perplexity is 102.86582159567489
At time: 384.31254982948303 and batch: 800, loss is 4.612716770172119 and perplexity is 100.75751314166318
At time: 385.9036750793457 and batch: 850, loss is 4.657936801910401 and perplexity is 105.41835867433103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867904980977376 and perplexity of 130.04817789297186
Finished 13 epochs...
Completing Train Step...
At time: 390.06356167793274 and batch: 50, loss is 4.753098306655883 and perplexity is 115.9429554345617
At time: 391.68326473236084 and batch: 100, loss is 4.709401350021363 and perplexity is 110.98569842810996
At time: 393.27312183380127 and batch: 150, loss is 4.697796106338501 and perplexity is 109.70512739441146
At time: 394.86264061927795 and batch: 200, loss is 4.707316131591797 and perplexity is 110.75451012715217
At time: 396.4505319595337 and batch: 250, loss is 4.708113718032837 and perplexity is 110.84288165999706
At time: 398.06550455093384 and batch: 300, loss is 4.6837978935241695 and perplexity is 108.1801500545146
At time: 399.6628019809723 and batch: 350, loss is 4.623544979095459 and perplexity is 101.85446483750682
At time: 401.2629323005676 and batch: 400, loss is 4.652934732437134 and perplexity is 104.89236534479899
At time: 402.85268235206604 and batch: 450, loss is 4.699623508453369 and perplexity is 109.9057860624211
At time: 404.44445061683655 and batch: 500, loss is 4.687893352508545 and perplexity is 108.62410590293935
At time: 406.0444493293762 and batch: 550, loss is 4.6672716808319095 and perplexity is 106.40703369002718
At time: 407.6642291545868 and batch: 600, loss is 4.6912710475921635 and perplexity is 108.99162534630501
At time: 409.2624797821045 and batch: 650, loss is 4.662982873916626 and perplexity is 105.95165168885977
At time: 410.8540816307068 and batch: 700, loss is 4.633714561462402 and perplexity is 102.89556702173488
At time: 412.45666313171387 and batch: 750, loss is 4.63100944519043 and perplexity is 102.61759868690972
At time: 414.06503081321716 and batch: 800, loss is 4.610738983154297 and perplexity is 100.55843317400549
At time: 415.67445039749146 and batch: 850, loss is 4.655779638290405 and perplexity is 105.19119912436554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867636362711589 and perplexity of 130.01324926840078
Finished 14 epochs...
Completing Train Step...
At time: 419.8963487148285 and batch: 50, loss is 4.742813520431518 and perplexity is 114.75661799440958
At time: 421.53351283073425 and batch: 100, loss is 4.6998076820373536 and perplexity is 109.92602966905241
At time: 423.139351606369 and batch: 150, loss is 4.6881107902526855 and perplexity is 108.64772745150123
At time: 424.7456419467926 and batch: 200, loss is 4.698075799942017 and perplexity is 109.7358155082511
At time: 426.3499655723572 and batch: 250, loss is 4.699079399108887 and perplexity is 109.8460015633391
At time: 427.9593770503998 and batch: 300, loss is 4.674688749313354 and perplexity is 107.19919607555161
At time: 429.56886434555054 and batch: 350, loss is 4.61509991645813 and perplexity is 100.99791938273472
At time: 431.17732286453247 and batch: 400, loss is 4.645452280044555 and perplexity is 104.11044221419148
At time: 432.78599405288696 and batch: 450, loss is 4.693286876678467 and perplexity is 109.21155543109751
At time: 434.39590287208557 and batch: 500, loss is 4.68164891242981 and perplexity is 107.94792257286748
At time: 436.0065677165985 and batch: 550, loss is 4.661349906921386 and perplexity is 105.77877732604047
At time: 437.66604804992676 and batch: 600, loss is 4.6860417556762695 and perplexity is 108.42316394169947
At time: 439.2749054431915 and batch: 650, loss is 4.658289661407471 and perplexity is 105.4555631069375
At time: 440.8845088481903 and batch: 700, loss is 4.6298404312133785 and perplexity is 102.49770737071898
At time: 442.47627353668213 and batch: 750, loss is 4.627532072067261 and perplexity is 102.26137872109383
At time: 444.0634460449219 and batch: 800, loss is 4.608019704818726 and perplexity is 100.28535825683997
At time: 445.6518609523773 and batch: 850, loss is 4.653058423995971 and perplexity is 104.9053404474174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867348353068034 and perplexity of 129.97580959057453
Finished 15 epochs...
Completing Train Step...
At time: 449.80103182792664 and batch: 50, loss is 4.7339558506011965 and perplexity is 113.74463030434828
At time: 451.42670035362244 and batch: 100, loss is 4.6914198684692385 and perplexity is 109.00784678259714
At time: 453.01286458969116 and batch: 150, loss is 4.679753704071045 and perplexity is 107.74353250975169
At time: 454.6000597476959 and batch: 200, loss is 4.690079441070557 and perplexity is 108.86182756404237
At time: 456.2064161300659 and batch: 250, loss is 4.690966863632202 and perplexity is 108.95847688397538
At time: 457.81377124786377 and batch: 300, loss is 4.666945161819458 and perplexity is 106.3722954421267
At time: 459.4238564968109 and batch: 350, loss is 4.6078869915008545 and perplexity is 100.27204993732695
At time: 461.031378030777 and batch: 400, loss is 4.639052505493164 and perplexity is 103.44628634630985
At time: 462.63386368751526 and batch: 450, loss is 4.6872677230834965 and perplexity is 108.5561687199832
At time: 464.22981429100037 and batch: 500, loss is 4.675718297958374 and perplexity is 107.30961969610908
At time: 465.83252453804016 and batch: 550, loss is 4.655686664581299 and perplexity is 105.18141956304567
At time: 467.4411823749542 and batch: 600, loss is 4.681002511978149 and perplexity is 107.8781675342336
At time: 469.05107617378235 and batch: 650, loss is 4.653664951324463 and perplexity is 104.96898770325228
At time: 470.6610939502716 and batch: 700, loss is 4.625672121047973 and perplexity is 102.07135433831112
At time: 472.27076268196106 and batch: 750, loss is 4.623965377807617 and perplexity is 101.89729332524188
At time: 473.8810987472534 and batch: 800, loss is 4.6046850299835205 and perplexity is 99.95149616645735
At time: 475.490914106369 and batch: 850, loss is 4.649730434417725 and perplexity is 104.55679686410397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867078463236491 and perplexity of 129.9407351745465
Finished 16 epochs...
Completing Train Step...
At time: 479.8087272644043 and batch: 50, loss is 4.725668725967407 and perplexity is 112.80590939749514
At time: 481.4167597293854 and batch: 100, loss is 4.683971424102783 and perplexity is 108.1989242474492
At time: 483.0201756954193 and batch: 150, loss is 4.672466716766357 and perplexity is 106.96126042114311
At time: 484.6266975402832 and batch: 200, loss is 4.682904367446899 and perplexity is 108.08353144145131
At time: 486.2346816062927 and batch: 250, loss is 4.683871803283691 and perplexity is 108.18814591887269
At time: 487.8393075466156 and batch: 300, loss is 4.65969217300415 and perplexity is 105.60356952322576
At time: 489.43273878097534 and batch: 350, loss is 4.601262445449829 and perplexity is 99.60998847440199
At time: 491.0181589126587 and batch: 400, loss is 4.632568426132202 and perplexity is 102.77770233439401
At time: 492.6030445098877 and batch: 450, loss is 4.681508378982544 and perplexity is 107.93275334510001
At time: 494.1859505176544 and batch: 500, loss is 4.670268144607544 and perplexity is 106.72635669292956
At time: 495.7706458568573 and batch: 550, loss is 4.650360441207885 and perplexity is 104.6226891101876
At time: 497.3770697116852 and batch: 600, loss is 4.6760694026947025 and perplexity is 107.34730322688391
At time: 498.98134660720825 and batch: 650, loss is 4.648939189910888 and perplexity is 104.47409959412764
At time: 500.59240913391113 and batch: 700, loss is 4.621546087265014 and perplexity is 101.6510721275844
At time: 502.20147919654846 and batch: 750, loss is 4.620265588760376 and perplexity is 101.52099138360346
At time: 503.8056561946869 and batch: 800, loss is 4.601186532974243 and perplexity is 99.60242712058792
At time: 505.41737842559814 and batch: 850, loss is 4.646029415130616 and perplexity is 104.17054534536315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.867019335428874 and perplexity of 129.93305229089373
Finished 17 epochs...
Completing Train Step...
At time: 509.72576236724854 and batch: 50, loss is 4.718216505050659 and perplexity is 111.96837944512198
At time: 511.3404641151428 and batch: 100, loss is 4.677269735336304 and perplexity is 107.47623306280012
At time: 512.9510259628296 and batch: 150, loss is 4.665360269546508 and perplexity is 106.2038403398811
At time: 514.560005903244 and batch: 200, loss is 4.676207895278931 and perplexity is 107.36217106183642
At time: 516.1703872680664 and batch: 250, loss is 4.677125797271729 and perplexity is 107.46076425512707
At time: 517.779287815094 and batch: 300, loss is 4.653205041885376 and perplexity is 104.92072257464102
At time: 519.4389276504517 and batch: 350, loss is 4.595280599594116 and perplexity is 99.0159154756609
At time: 521.0480532646179 and batch: 400, loss is 4.6269008159637455 and perplexity is 102.196845972111
At time: 522.652753829956 and batch: 450, loss is 4.676439781188964 and perplexity is 107.3870697232891
At time: 524.2573018074036 and batch: 500, loss is 4.665653009414672 and perplexity is 106.23493498920041
At time: 525.8629765510559 and batch: 550, loss is 4.645526361465454 and perplexity is 104.11815514937021
At time: 527.4673132896423 and batch: 600, loss is 4.671360244750977 and perplexity is 106.84297623089822
At time: 529.0725803375244 and batch: 650, loss is 4.644273138046264 and perplexity is 103.98775356719955
At time: 530.6780476570129 and batch: 700, loss is 4.61723554611206 and perplexity is 101.21384401981499
At time: 532.2851629257202 and batch: 750, loss is 4.616505422592163 and perplexity is 101.13997238274835
At time: 533.8933684825897 and batch: 800, loss is 4.59753903388977 and perplexity is 99.23978912179078
At time: 535.4906792640686 and batch: 850, loss is 4.642196807861328 and perplexity is 103.77206465379389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.866739273071289 and perplexity of 129.89666802910983
Finished 18 epochs...
Completing Train Step...
At time: 539.7880256175995 and batch: 50, loss is 4.711035413742065 and perplexity is 111.16720438719203
At time: 541.3829393386841 and batch: 100, loss is 4.670350866317749 and perplexity is 106.73518564484715
At time: 542.9797561168671 and batch: 150, loss is 4.658762226104736 and perplexity is 105.50540946007783
At time: 544.5765817165375 and batch: 200, loss is 4.670088787078857 and perplexity is 106.7072162338914
At time: 546.1731007099152 and batch: 250, loss is 4.671083192825318 and perplexity is 106.81337927872592
At time: 547.776597738266 and batch: 300, loss is 4.646941137313843 and perplexity is 104.26556325077763
At time: 549.3836393356323 and batch: 350, loss is 4.589316635131836 and perplexity is 98.42714552126206
At time: 550.9936120510101 and batch: 400, loss is 4.62136022567749 and perplexity is 101.63218085358072
At time: 552.5905368328094 and batch: 450, loss is 4.671377897262573 and perplexity is 106.84486229442196
At time: 554.1941094398499 and batch: 500, loss is 4.660691156387329 and perplexity is 105.70911844641611
At time: 555.8045213222504 and batch: 550, loss is 4.640716562271118 and perplexity is 103.61857014551477
At time: 557.4221494197845 and batch: 600, loss is 4.66679202079773 and perplexity is 106.35600672738612
At time: 559.033673286438 and batch: 650, loss is 4.639757337570191 and perplexity is 103.51922430866419
At time: 560.7118742465973 and batch: 700, loss is 4.613212919235229 and perplexity is 100.80751629089183
At time: 562.31476354599 and batch: 750, loss is 4.612560205459594 and perplexity is 100.74173930542878
At time: 563.9306526184082 and batch: 800, loss is 4.593969497680664 and perplexity is 98.88618058583408
At time: 565.542810678482 and batch: 850, loss is 4.63847578048706 and perplexity is 103.38664348660284
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.866756121317546 and perplexity of 129.89885657859728
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 569.9157676696777 and batch: 50, loss is 4.70834002494812 and perplexity is 110.86796900923986
At time: 571.5535848140717 and batch: 100, loss is 4.668544645309448 and perplexity is 106.54257231369884
At time: 573.161993265152 and batch: 150, loss is 4.655591268539428 and perplexity is 105.17138615052257
At time: 574.7832667827606 and batch: 200, loss is 4.666710147857666 and perplexity is 106.3472994048738
At time: 576.3960447311401 and batch: 250, loss is 4.663257617950439 and perplexity is 105.9807652722425
At time: 578.0052890777588 and batch: 300, loss is 4.6357073402404785 and perplexity is 103.10081956758833
At time: 579.6136798858643 and batch: 350, loss is 4.577374744415283 and perplexity is 97.25872973819489
At time: 581.2179758548737 and batch: 400, loss is 4.606619806289673 and perplexity is 100.14506715089797
At time: 582.8111324310303 and batch: 450, loss is 4.654498014450073 and perplexity is 105.05646993028994
At time: 584.4137380123138 and batch: 500, loss is 4.641889362335205 and perplexity is 103.74016530068751
At time: 586.0224997997284 and batch: 550, loss is 4.617603397369384 and perplexity is 101.25108250828876
At time: 587.6385953426361 and batch: 600, loss is 4.638656396865844 and perplexity is 103.40531849421936
At time: 589.247095823288 and batch: 650, loss is 4.608149194717408 and perplexity is 100.2983450385303
At time: 590.857274055481 and batch: 700, loss is 4.580598011016845 and perplexity is 97.57272632897822
At time: 592.4653887748718 and batch: 750, loss is 4.575364894866944 and perplexity is 97.06345063070725
At time: 594.0748422145844 and batch: 800, loss is 4.552509756088257 and perplexity is 94.87021086621525
At time: 595.6848375797272 and batch: 850, loss is 4.6039924144744875 and perplexity is 99.88229217870553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8594160079956055 and perplexity of 128.9488750023285
Finished 20 epochs...
Completing Train Step...
At time: 599.9312431812286 and batch: 50, loss is 4.700039577484131 and perplexity is 109.95152397070699
At time: 601.5675723552704 and batch: 100, loss is 4.659762830734253 and perplexity is 105.61103149535887
At time: 603.1706638336182 and batch: 150, loss is 4.646592855453491 and perplexity is 104.2292557694236
At time: 604.7813019752502 and batch: 200, loss is 4.6586761569976805 and perplexity is 105.49632909447114
At time: 606.3909242153168 and batch: 250, loss is 4.65609489440918 and perplexity is 105.22436652136847
At time: 608.000070810318 and batch: 300, loss is 4.629612417221069 and perplexity is 102.47433912350374
At time: 609.609343290329 and batch: 350, loss is 4.572059745788574 and perplexity is 96.74317103378054
At time: 611.2198581695557 and batch: 400, loss is 4.601660938262939 and perplexity is 99.64969024883352
At time: 612.8302211761475 and batch: 450, loss is 4.650462827682495 and perplexity is 104.63340160688779
At time: 614.4405999183655 and batch: 500, loss is 4.638350992202759 and perplexity is 103.37374284968337
At time: 616.0293431282043 and batch: 550, loss is 4.615038805007934 and perplexity is 100.99174744200457
At time: 617.6139945983887 and batch: 600, loss is 4.637269916534424 and perplexity is 103.26204839749948
At time: 619.197523355484 and batch: 650, loss is 4.607627115249634 and perplexity is 100.24599499856357
At time: 620.8075470924377 and batch: 700, loss is 4.581171026229859 and perplexity is 97.62865300742841
At time: 622.4143505096436 and batch: 750, loss is 4.577006731033325 and perplexity is 97.22294380939326
At time: 624.0215742588043 and batch: 800, loss is 4.554935445785523 and perplexity is 95.10061589186466
At time: 625.6284167766571 and batch: 850, loss is 4.606724500656128 and perplexity is 100.15555232411667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858900388081868 and perplexity of 128.88240353300262
Finished 21 epochs...
Completing Train Step...
At time: 629.8772714138031 and batch: 50, loss is 4.695979824066162 and perplexity is 109.50605275893295
At time: 631.5188624858856 and batch: 100, loss is 4.655398769378662 and perplexity is 105.15114269543854
At time: 633.1226851940155 and batch: 150, loss is 4.642034244537354 and perplexity is 103.75519649313735
At time: 634.7373518943787 and batch: 200, loss is 4.654831924438477 and perplexity is 105.09155519227977
At time: 636.3501999378204 and batch: 250, loss is 4.652217302322388 and perplexity is 104.81713939100258
At time: 637.9612047672272 and batch: 300, loss is 4.6261297130584715 and perplexity is 102.11807206257058
At time: 639.5713155269623 and batch: 350, loss is 4.5691586780548095 and perplexity is 96.46291925308593
At time: 641.1791615486145 and batch: 400, loss is 4.599075622558594 and perplexity is 99.3923970750507
At time: 642.8163349628448 and batch: 450, loss is 4.648152961730957 and perplexity is 104.39199139507333
At time: 644.4270837306976 and batch: 500, loss is 4.636378583908081 and perplexity is 103.17004857201373
At time: 646.0430135726929 and batch: 550, loss is 4.613715696334839 and perplexity is 100.85821274499153
At time: 647.6595122814178 and batch: 600, loss is 4.636375484466552 and perplexity is 103.16972880297625
At time: 649.2723417282104 and batch: 650, loss is 4.607328567504883 and perplexity is 100.21607124989255
At time: 650.8852124214172 and batch: 700, loss is 4.581754417419433 and perplexity is 97.68562532039985
At time: 652.497848033905 and batch: 750, loss is 4.577863130569458 and perplexity is 97.30624115618667
At time: 654.1116392612457 and batch: 800, loss is 4.5561481285095216 and perplexity is 95.21601272152738
At time: 655.726425409317 and batch: 850, loss is 4.607981576919555 and perplexity is 100.28153465970543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858721733093262 and perplexity of 128.85938010535418
Finished 22 epochs...
Completing Train Step...
At time: 660.0111665725708 and batch: 50, loss is 4.692932424545288 and perplexity is 109.17285202196528
At time: 661.6204755306244 and batch: 100, loss is 4.65217806816101 and perplexity is 104.81302705911307
At time: 663.2269601821899 and batch: 150, loss is 4.638702688217163 and perplexity is 103.41010537694075
At time: 664.8339354991913 and batch: 200, loss is 4.651925268173218 and perplexity is 104.7865336760566
At time: 666.4464333057404 and batch: 250, loss is 4.649372663497925 and perplexity is 104.51939617355721
At time: 668.0560688972473 and batch: 300, loss is 4.6235221481323245 and perplexity is 101.85213942852083
At time: 669.6644506454468 and batch: 350, loss is 4.567013740539551 and perplexity is 96.25623406140171
At time: 671.2731194496155 and batch: 400, loss is 4.597167224884033 and perplexity is 99.20289773316807
At time: 672.8786880970001 and batch: 450, loss is 4.646415672302246 and perplexity is 104.21078973741763
At time: 674.4807674884796 and batch: 500, loss is 4.634988498687744 and perplexity is 103.02673304578764
At time: 676.0874993801117 and batch: 550, loss is 4.612662849426269 and perplexity is 100.75208036787555
At time: 677.6947729587555 and batch: 600, loss is 4.6356853485107425 and perplexity is 103.09855222716031
At time: 679.3022217750549 and batch: 650, loss is 4.6070763301849365 and perplexity is 100.19079620445378
At time: 680.9080274105072 and batch: 700, loss is 4.582032251358032 and perplexity is 97.7127694730359
At time: 682.5420632362366 and batch: 750, loss is 4.5782654666900635 and perplexity is 97.34539884851293
At time: 684.147171497345 and batch: 800, loss is 4.556765718460083 and perplexity is 95.27483533637495
At time: 685.7535018920898 and batch: 850, loss is 4.608590459823608 and perplexity is 100.34261296463272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858645121256511 and perplexity of 128.84950832971398
Finished 23 epochs...
Completing Train Step...
At time: 689.9763989448547 and batch: 50, loss is 4.690389947891235 and perplexity is 108.89563515248432
At time: 691.5824038982391 and batch: 100, loss is 4.649575786590576 and perplexity is 104.54062863287847
At time: 693.1874039173126 and batch: 150, loss is 4.6360327243804935 and perplexity is 103.1343723975815
At time: 694.7946019172668 and batch: 200, loss is 4.6496232318878175 and perplexity is 104.54558871174301
At time: 696.4018592834473 and batch: 250, loss is 4.647026948928833 and perplexity is 104.27451083104566
At time: 698.0115923881531 and batch: 300, loss is 4.621374292373657 and perplexity is 101.63361049264469
At time: 699.6255700588226 and batch: 350, loss is 4.56527379989624 and perplexity is 96.0888995458642
At time: 701.2344431877136 and batch: 400, loss is 4.595599031448364 and perplexity is 99.0474503178085
At time: 702.8391315937042 and batch: 450, loss is 4.6449551677703855 and perplexity is 104.05870049728439
At time: 704.4419448375702 and batch: 500, loss is 4.633804817199707 and perplexity is 102.90485435611308
At time: 706.045220375061 and batch: 550, loss is 4.611701717376709 and perplexity is 100.6552908355859
At time: 707.6390669345856 and batch: 600, loss is 4.635029354095459 and perplexity is 103.03094233095717
At time: 709.240617275238 and batch: 650, loss is 4.606758813858033 and perplexity is 100.1589890407675
At time: 710.8393561840057 and batch: 700, loss is 4.582078227996826 and perplexity is 97.71726208102024
At time: 712.436023235321 and batch: 750, loss is 4.57835244178772 and perplexity is 97.35386584228765
At time: 714.02876496315 and batch: 800, loss is 4.557015161514283 and perplexity is 95.29860394662374
At time: 715.6191759109497 and batch: 850, loss is 4.608845291137695 and perplexity is 100.36818666290458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858608563741048 and perplexity of 128.84479799792038
Finished 24 epochs...
Completing Train Step...
At time: 719.8195512294769 and batch: 50, loss is 4.688163614273071 and perplexity is 108.65346681285769
At time: 721.402426481247 and batch: 100, loss is 4.64732629776001 and perplexity is 104.30572995645575
At time: 722.984521150589 and batch: 150, loss is 4.633718509674072 and perplexity is 102.89597327601533
At time: 724.6062281131744 and batch: 200, loss is 4.647632026672364 and perplexity is 104.33762410906144
At time: 726.1906845569611 and batch: 250, loss is 4.644966402053833 and perplexity is 104.05986952878756
At time: 727.7834670543671 and batch: 300, loss is 4.619464225769043 and perplexity is 101.43966880706878
At time: 729.39138007164 and batch: 350, loss is 4.563733425140381 and perplexity is 95.94100056985394
At time: 730.9844901561737 and batch: 400, loss is 4.594216213226319 and perplexity is 98.91058035361225
At time: 732.571617603302 and batch: 450, loss is 4.643625221252441 and perplexity is 103.92039997742864
At time: 734.1774768829346 and batch: 500, loss is 4.632705841064453 and perplexity is 102.7918264958103
At time: 735.7855844497681 and batch: 550, loss is 4.6107635593414305 and perplexity is 100.56090454724536
At time: 737.3945257663727 and batch: 600, loss is 4.634381723403931 and perplexity is 102.9642379327658
At time: 739.0030527114868 and batch: 650, loss is 4.606386890411377 and perplexity is 100.12174449083993
At time: 740.6127562522888 and batch: 700, loss is 4.582018213272095 and perplexity is 97.71139778240885
At time: 742.2190098762512 and batch: 750, loss is 4.578290014266968 and perplexity is 97.34778847150707
At time: 743.8209569454193 and batch: 800, loss is 4.557022686004639 and perplexity is 95.2993210227479
At time: 745.416177034378 and batch: 850, loss is 4.608878517150879 and perplexity is 100.37152155300018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858599662780762 and perplexity of 128.8436511605943
Finished 25 epochs...
Completing Train Step...
At time: 749.6527607440948 and batch: 50, loss is 4.686138896942139 and perplexity is 108.43369681667446
At time: 751.3108050823212 and batch: 100, loss is 4.645303497314453 and perplexity is 104.09495353061958
At time: 752.9234178066254 and batch: 150, loss is 4.631637592315673 and perplexity is 102.6820778856158
At time: 754.5371193885803 and batch: 200, loss is 4.645843467712402 and perplexity is 104.15117690221412
At time: 756.1498565673828 and batch: 250, loss is 4.64305926322937 and perplexity is 103.86160203345415
At time: 757.7642891407013 and batch: 300, loss is 4.6177504348754885 and perplexity is 101.26597130953039
At time: 759.3765845298767 and batch: 350, loss is 4.562328233718872 and perplexity is 95.80627977530249
At time: 760.9912333488464 and batch: 400, loss is 4.592966918945312 and perplexity is 98.78708908577424
At time: 762.6005969047546 and batch: 450, loss is 4.642391242980957 and perplexity is 103.7922435492784
At time: 764.2404534816742 and batch: 500, loss is 4.631697788238525 and perplexity is 102.68825911409496
At time: 765.850227355957 and batch: 550, loss is 4.609834928512573 and perplexity is 100.46756393729669
At time: 767.4623758792877 and batch: 600, loss is 4.633713598251343 and perplexity is 102.8954679116345
At time: 769.0744323730469 and batch: 650, loss is 4.605926141738892 and perplexity is 100.07562415573636
At time: 770.6862576007843 and batch: 700, loss is 4.581826467514038 and perplexity is 97.69266383250536
At time: 772.2765641212463 and batch: 750, loss is 4.5781121349334715 and perplexity is 97.33047385177856
At time: 773.8658154010773 and batch: 800, loss is 4.556863403320312 and perplexity is 95.28414269993499
At time: 775.4561324119568 and batch: 850, loss is 4.6087631988525395 and perplexity is 100.35994754729306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858608563741048 and perplexity of 128.84479799792038
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 779.5990154743195 and batch: 50, loss is 4.685369567871094 and perplexity is 108.35030770238036
At time: 781.2230844497681 and batch: 100, loss is 4.644948968887329 and perplexity is 104.05805545156826
At time: 782.8047606945038 and batch: 150, loss is 4.630578231811524 and perplexity is 102.57335814468703
At time: 784.3857407569885 and batch: 200, loss is 4.645224733352661 and perplexity is 104.08675492255863
At time: 785.9651823043823 and batch: 250, loss is 4.640615205764771 and perplexity is 103.60806826147827
At time: 787.5466389656067 and batch: 300, loss is 4.615081996917724 and perplexity is 100.99610956265312
At time: 789.1312739849091 and batch: 350, loss is 4.559514579772949 and perplexity is 95.53709293506424
At time: 790.7155566215515 and batch: 400, loss is 4.588537302017212 and perplexity is 98.3504678699767
At time: 792.298999786377 and batch: 450, loss is 4.637595090866089 and perplexity is 103.29563202504438
At time: 793.8818514347076 and batch: 500, loss is 4.626948080062866 and perplexity is 102.20167632811916
At time: 795.4647486209869 and batch: 550, loss is 4.604046325683594 and perplexity is 99.88767709899771
At time: 797.0490076541901 and batch: 600, loss is 4.626568107604981 and perplexity is 102.1628498829218
At time: 798.6309959888458 and batch: 650, loss is 4.598008136749268 and perplexity is 99.28635371158096
At time: 800.2139999866486 and batch: 700, loss is 4.573964233398438 and perplexity is 96.92759276305988
At time: 801.7962985038757 and batch: 750, loss is 4.568824291229248 and perplexity is 96.43066871611035
At time: 803.3776619434357 and batch: 800, loss is 4.5468205547332765 and perplexity is 94.33200755916842
At time: 804.9983170032501 and batch: 850, loss is 4.6000339221954345 and perplexity is 99.48769042557139
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858815828959147 and perplexity of 128.87150581078313
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 809.1217768192291 and batch: 50, loss is 4.684434661865234 and perplexity is 108.24905768597166
At time: 810.7656478881836 and batch: 100, loss is 4.6435004997253415 and perplexity is 103.9074396746778
At time: 812.3506507873535 and batch: 150, loss is 4.629622917175293 and perplexity is 102.47541510502258
At time: 813.9352147579193 and batch: 200, loss is 4.643963794708252 and perplexity is 103.9555906233514
At time: 815.5199315547943 and batch: 250, loss is 4.639510869979858 and perplexity is 103.49371331884099
At time: 817.1062867641449 and batch: 300, loss is 4.613945341110229 and perplexity is 100.88137696627304
At time: 818.6898415088654 and batch: 350, loss is 4.558708648681641 and perplexity is 95.46012764002272
At time: 820.2915830612183 and batch: 400, loss is 4.587233467102051 and perplexity is 98.2223186569089
At time: 821.9000675678253 and batch: 450, loss is 4.6366388130187985 and perplexity is 103.19689991560544
At time: 823.5066726207733 and batch: 500, loss is 4.626205654144287 and perplexity is 102.12582731431176
At time: 825.1166970729828 and batch: 550, loss is 4.603018140792846 and perplexity is 99.78502687937822
At time: 826.725291967392 and batch: 600, loss is 4.6251139640808105 and perplexity is 102.01439839739575
At time: 828.3338437080383 and batch: 650, loss is 4.596320343017578 and perplexity is 99.11892016253809
At time: 829.942432641983 and batch: 700, loss is 4.572219409942627 and perplexity is 96.75861868352918
At time: 831.5539357662201 and batch: 750, loss is 4.5668752574920655 and perplexity is 96.24290512770641
At time: 833.1591103076935 and batch: 800, loss is 4.544896202087402 and perplexity is 94.15065406083816
At time: 834.7649319171906 and batch: 850, loss is 4.598196249008179 and perplexity is 99.30503244865132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858308792114258 and perplexity of 128.80617977183454
Finished 28 epochs...
Completing Train Step...
At time: 839.0793974399567 and batch: 50, loss is 4.684103479385376 and perplexity is 108.21321343042727
At time: 840.6845548152924 and batch: 100, loss is 4.643134088516235 and perplexity is 103.86937379837921
At time: 842.2890706062317 and batch: 150, loss is 4.629217557907104 and perplexity is 102.43388416379287
At time: 843.899448633194 and batch: 200, loss is 4.643628406524658 and perplexity is 103.92073099271862
At time: 845.5397372245789 and batch: 250, loss is 4.639246854782105 and perplexity is 103.4663930122996
At time: 847.15056848526 and batch: 300, loss is 4.613718242645263 and perplexity is 100.85846956163704
At time: 848.7628228664398 and batch: 350, loss is 4.55846360206604 and perplexity is 95.43673832467304
At time: 850.3742501735687 and batch: 400, loss is 4.587081823348999 and perplexity is 98.20742498516901
At time: 851.9877202510834 and batch: 450, loss is 4.636576948165893 and perplexity is 103.19051585204848
At time: 853.6024217605591 and batch: 500, loss is 4.626184511184692 and perplexity is 102.12366809489751
At time: 855.208368062973 and batch: 550, loss is 4.602987394332886 and perplexity is 99.78195889020978
At time: 856.8221113681793 and batch: 600, loss is 4.62508786201477 and perplexity is 102.01173564558354
At time: 858.4330596923828 and batch: 650, loss is 4.5962639427185055 and perplexity is 99.11332998344251
At time: 860.0426414012909 and batch: 700, loss is 4.5722090435028075 and perplexity is 96.75761564633055
At time: 861.6524932384491 and batch: 750, loss is 4.566975917816162 and perplexity is 96.25259345733556
At time: 863.2619609832764 and batch: 800, loss is 4.545101079940796 and perplexity is 94.1699454208565
At time: 864.8716762065887 and batch: 850, loss is 4.598360271453857 and perplexity is 99.32132203883437
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8582407633463545 and perplexity of 128.79741754417145
Finished 29 epochs...
Completing Train Step...
At time: 869.1234090328217 and batch: 50, loss is 4.683834381103516 and perplexity is 108.18409735833676
At time: 870.7286803722382 and batch: 100, loss is 4.642856950759888 and perplexity is 103.84059166166415
At time: 872.3347129821777 and batch: 150, loss is 4.628932828903198 and perplexity is 102.40472241778316
At time: 873.9414038658142 and batch: 200, loss is 4.643362045288086 and perplexity is 103.89305422447866
At time: 875.5476417541504 and batch: 250, loss is 4.639018297195435 and perplexity is 103.4427476854736
At time: 877.1543855667114 and batch: 300, loss is 4.61351619720459 and perplexity is 100.83809362621056
At time: 878.7620244026184 and batch: 350, loss is 4.558269367218018 and perplexity is 95.41820298447156
At time: 880.369580745697 and batch: 400, loss is 4.586962766647339 and perplexity is 98.19573342906462
At time: 881.9739844799042 and batch: 450, loss is 4.636549596786499 and perplexity is 103.18769348769747
At time: 883.5782999992371 and batch: 500, loss is 4.626194829940796 and perplexity is 102.12472188955792
At time: 885.2312626838684 and batch: 550, loss is 4.602982378005981 and perplexity is 99.7814583525402
At time: 886.8181791305542 and batch: 600, loss is 4.625077781677246 and perplexity is 102.01070733803961
At time: 888.4045691490173 and batch: 650, loss is 4.596237554550171 and perplexity is 99.11071459871444
At time: 889.991616487503 and batch: 700, loss is 4.572207670211792 and perplexity is 96.75748277005758
At time: 891.5835039615631 and batch: 750, loss is 4.5670667552948 and perplexity is 96.26133719736121
At time: 893.1779112815857 and batch: 800, loss is 4.545275440216065 and perplexity is 94.18636634999936
At time: 894.7818865776062 and batch: 850, loss is 4.598489379882812 and perplexity is 99.33414608651304
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858218193054199 and perplexity of 128.79451058163426
Finished 30 epochs...
Completing Train Step...
At time: 899.0115814208984 and batch: 50, loss is 4.683589382171631 and perplexity is 108.15759561661878
At time: 900.6179106235504 and batch: 100, loss is 4.642612714767456 and perplexity is 103.81523314856179
At time: 902.2163772583008 and batch: 150, loss is 4.628678646087646 and perplexity is 102.37869620496134
At time: 903.8267202377319 and batch: 200, loss is 4.643126516342163 and perplexity is 103.86858728437791
At time: 905.4340307712555 and batch: 250, loss is 4.638806753158569 and perplexity is 103.42086730345753
At time: 907.0404517650604 and batch: 300, loss is 4.613332815170288 and perplexity is 100.81960342690327
At time: 908.6481046676636 and batch: 350, loss is 4.558100471496582 and perplexity is 95.40208861910236
At time: 910.2570145130157 and batch: 400, loss is 4.586860427856445 and perplexity is 98.18568471063016
At time: 911.8652999401093 and batch: 450, loss is 4.636533756256103 and perplexity is 103.1860589528483
At time: 913.4740746021271 and batch: 500, loss is 4.626214084625244 and perplexity is 102.12668828778335
At time: 915.0794222354889 and batch: 550, loss is 4.60298583984375 and perplexity is 99.78180378035931
At time: 916.695095539093 and batch: 600, loss is 4.625069799423218 and perplexity is 102.00989306590998
At time: 918.2992050647736 and batch: 650, loss is 4.596221656799316 and perplexity is 99.10913897379122
At time: 919.9030945301056 and batch: 700, loss is 4.572208890914917 and perplexity is 96.75760088229119
At time: 921.5118522644043 and batch: 750, loss is 4.567147798538208 and perplexity is 96.26913884447366
At time: 923.1224658489227 and batch: 800, loss is 4.545429878234863 and perplexity is 94.20091342909839
At time: 924.7324185371399 and batch: 850, loss is 4.598598403930664 and perplexity is 99.34497648758565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858206748962402 and perplexity of 128.79303665386612
Finished 31 epochs...
Completing Train Step...
At time: 929.009045124054 and batch: 50, loss is 4.683361740112304 and perplexity is 108.13297720102055
At time: 930.6412630081177 and batch: 100, loss is 4.642387781143189 and perplexity is 103.79188423799154
At time: 932.250403881073 and batch: 150, loss is 4.628442621231079 and perplexity is 102.35453513929218
At time: 933.8606188297272 and batch: 200, loss is 4.642910671234131 and perplexity is 103.84617017733282
At time: 935.4702184200287 and batch: 250, loss is 4.63860704421997 and perplexity is 103.40021529408347
At time: 937.07941365242 and batch: 300, loss is 4.613162546157837 and perplexity is 100.80243843396673
At time: 938.688987493515 and batch: 350, loss is 4.557947626113892 and perplexity is 95.38750796467919
At time: 940.297857761383 and batch: 400, loss is 4.586767511367798 and perplexity is 98.17656206540006
At time: 941.9033238887787 and batch: 450, loss is 4.636522588729858 and perplexity is 103.18490662626114
At time: 943.5101103782654 and batch: 500, loss is 4.626236686706543 and perplexity is 102.12899658958094
At time: 945.1143906116486 and batch: 550, loss is 4.6029927444458005 and perplexity is 99.78249273638474
At time: 946.7181894779205 and batch: 600, loss is 4.625062332153321 and perplexity is 102.00913133335027
At time: 948.3224396705627 and batch: 650, loss is 4.596210098266601 and perplexity is 99.10799342418649
At time: 949.9271280765533 and batch: 700, loss is 4.57221097946167 and perplexity is 96.7578029652754
At time: 951.5341627597809 and batch: 750, loss is 4.5672212600708 and perplexity is 96.27621118272373
At time: 953.1347942352295 and batch: 800, loss is 4.545569133758545 and perplexity is 94.2140323400485
At time: 954.7293479442596 and batch: 850, loss is 4.598693170547485 and perplexity is 99.35439152101398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858202616373698 and perplexity of 128.79250440631748
Finished 32 epochs...
Completing Train Step...
At time: 958.9945704936981 and batch: 50, loss is 4.683147773742676 and perplexity is 108.10984285552597
At time: 960.6245713233948 and batch: 100, loss is 4.64217625617981 and perplexity is 103.76993198528567
At time: 962.2369117736816 and batch: 150, loss is 4.628220510482788 and perplexity is 102.33180362145214
At time: 963.8415284156799 and batch: 200, loss is 4.642709159851075 and perplexity is 103.82524610024593
At time: 965.4386613368988 and batch: 250, loss is 4.6384171199798585 and perplexity is 103.38057895153409
At time: 967.0711722373962 and batch: 300, loss is 4.613002185821533 and perplexity is 100.78627501705931
At time: 968.6764049530029 and batch: 350, loss is 4.557806158065796 and perplexity is 95.37401463457476
At time: 970.2811241149902 and batch: 400, loss is 4.586680898666382 and perplexity is 98.1680590963817
At time: 971.8796503543854 and batch: 450, loss is 4.636513595581055 and perplexity is 103.1839786732142
At time: 973.4808869361877 and batch: 500, loss is 4.626260757446289 and perplexity is 102.13145493966543
At time: 975.075430393219 and batch: 550, loss is 4.6030010318756105 and perplexity is 99.78331968021617
At time: 976.6757671833038 and batch: 600, loss is 4.625054130554199 and perplexity is 102.00829469877921
At time: 978.2709846496582 and batch: 650, loss is 4.59620002746582 and perplexity is 99.10699533235473
At time: 979.8636286258698 and batch: 700, loss is 4.572213230133056 and perplexity is 96.75802073553899
At time: 981.4751954078674 and batch: 750, loss is 4.5672885513305665 and perplexity is 96.28268994823942
At time: 983.0855712890625 and batch: 800, loss is 4.545696859359741 and perplexity is 94.22606665249877
At time: 984.6943669319153 and batch: 850, loss is 4.598777112960815 and perplexity is 99.3627319184648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.8582000732421875 and perplexity of 128.79217687045767
Finished 33 epochs...
Completing Train Step...
At time: 988.8817720413208 and batch: 50, loss is 4.682944631576538 and perplexity is 108.08788341838675
At time: 990.4858860969543 and batch: 100, loss is 4.641974802017212 and perplexity is 103.74902920608136
At time: 992.0923662185669 and batch: 150, loss is 4.628009386062622 and perplexity is 102.31020115923194
At time: 993.6934776306152 and batch: 200, loss is 4.642518129348755 and perplexity is 103.80541420563863
At time: 995.2986476421356 and batch: 250, loss is 4.638234720230103 and perplexity is 103.361724079418
At time: 996.9010002613068 and batch: 300, loss is 4.6128494167327885 and perplexity is 100.77087916570201
At time: 998.4964654445648 and batch: 350, loss is 4.557673549652099 and perplexity is 95.36136807632471
At time: 1000.0957071781158 and batch: 400, loss is 4.58659854888916 and perplexity is 98.15997531143837
At time: 1001.6990222930908 and batch: 450, loss is 4.636505975723266 and perplexity is 103.18319242896622
At time: 1003.2992503643036 and batch: 500, loss is 4.626284704208374 and perplexity is 102.13390068660205
At time: 1004.8952968120575 and batch: 550, loss is 4.603009757995605 and perplexity is 99.78419040523617
At time: 1006.4901175498962 and batch: 600, loss is 4.625044956207275 and perplexity is 102.00735884358745
At time: 1008.1517624855042 and batch: 650, loss is 4.5961900806427005 and perplexity is 99.10600953750499
At time: 1009.7577826976776 and batch: 700, loss is 4.572214727401733 and perplexity is 96.75816560840114
At time: 1011.3636019229889 and batch: 750, loss is 4.567350873947143 and perplexity is 96.28869072439814
At time: 1012.9712066650391 and batch: 800, loss is 4.545814628601074 and perplexity is 94.23716423834655
At time: 1014.581093788147 and batch: 850, loss is 4.598852033615112 and perplexity is 99.37017651822656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858199119567871 and perplexity of 128.79205404472498
Finished 34 epochs...
Completing Train Step...
At time: 1018.8389782905579 and batch: 50, loss is 4.682749576568604 and perplexity is 108.06680239147579
At time: 1020.4489209651947 and batch: 100, loss is 4.641781034469605 and perplexity is 103.72892795867308
At time: 1022.0592231750488 and batch: 150, loss is 4.627806997299194 and perplexity is 102.28949681936675
At time: 1023.6686491966248 and batch: 200, loss is 4.64233551979065 and perplexity is 103.78646007547697
At time: 1025.2743034362793 and batch: 250, loss is 4.6380589580535885 and perplexity is 103.34355859427488
At time: 1026.8802485466003 and batch: 300, loss is 4.612702627182006 and perplexity is 100.756088139228
At time: 1028.4856326580048 and batch: 350, loss is 4.557547616958618 and perplexity is 95.34935971852708
At time: 1030.0925779342651 and batch: 400, loss is 4.586518678665161 and perplexity is 98.15213556530782
At time: 1031.698340177536 and batch: 450, loss is 4.636498641967774 and perplexity is 103.18243571143678
At time: 1033.3034386634827 and batch: 500, loss is 4.626307458877563 and perplexity is 102.13622473616653
At time: 1034.9083137512207 and batch: 550, loss is 4.6030179882049564 and perplexity is 99.78501165339269
At time: 1036.513282775879 and batch: 600, loss is 4.62503415107727 and perplexity is 102.00625664676839
At time: 1038.1128284931183 and batch: 650, loss is 4.596179027557373 and perplexity is 99.10491411637896
At time: 1039.7085800170898 and batch: 700, loss is 4.572214126586914 and perplexity is 96.75810747467884
At time: 1041.3038456439972 and batch: 750, loss is 4.567409257888794 and perplexity is 96.29431260181113
At time: 1042.9069011211395 and batch: 800, loss is 4.545923805236816 and perplexity is 94.2474532965521
At time: 1044.5061221122742 and batch: 850, loss is 4.59891830444336 and perplexity is 99.37676208034043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858197530110677 and perplexity of 128.7918493354308
Finished 35 epochs...
Completing Train Step...
At time: 1048.6823761463165 and batch: 50, loss is 4.682560520172119 and perplexity is 108.04637360239352
At time: 1050.3140802383423 and batch: 100, loss is 4.64159273147583 and perplexity is 103.70939732989308
At time: 1051.9223802089691 and batch: 150, loss is 4.627611618041993 and perplexity is 102.26951352568278
At time: 1053.5273349285126 and batch: 200, loss is 4.642159061431885 and perplexity is 103.76814770280316
At time: 1055.1368527412415 and batch: 250, loss is 4.6378884029388425 and perplexity is 103.32593432477796
At time: 1056.7488820552826 and batch: 300, loss is 4.612559881210327 and perplexity is 100.74170663999891
At time: 1058.3623650074005 and batch: 350, loss is 4.557425746917724 and perplexity is 95.33774019620923
At time: 1059.97492313385 and batch: 400, loss is 4.586439752578736 and perplexity is 98.14438910707617
At time: 1061.5891873836517 and batch: 450, loss is 4.636489334106446 and perplexity is 103.18147530810337
At time: 1063.2026808261871 and batch: 500, loss is 4.626326837539673 and perplexity is 102.13820401873271
At time: 1064.8137640953064 and batch: 550, loss is 4.603023786544799 and perplexity is 99.7855902424789
At time: 1066.4235441684723 and batch: 600, loss is 4.62501971244812 and perplexity is 102.00478382689046
At time: 1068.0335948467255 and batch: 650, loss is 4.5961644744873045 and perplexity is 99.10347184611449
At time: 1069.6428015232086 and batch: 700, loss is 4.572209491729736 and perplexity is 96.75765901570914
At time: 1071.2526185512543 and batch: 750, loss is 4.567464857101441 and perplexity is 96.29966663861285
At time: 1072.862676858902 and batch: 800, loss is 4.546025180816651 and perplexity is 94.25700817108527
At time: 1074.4759147167206 and batch: 850, loss is 4.59897518157959 and perplexity is 99.38241450672076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858189582824707 and perplexity of 128.79082579384075
Finished 36 epochs...
Completing Train Step...
At time: 1078.671203136444 and batch: 50, loss is 4.6823739910125735 and perplexity is 108.02622168265225
At time: 1080.3041760921478 and batch: 100, loss is 4.64140682220459 and perplexity is 103.69011858351904
At time: 1081.9100906848907 and batch: 150, loss is 4.627420349121094 and perplexity is 102.24995441677433
At time: 1083.5199825763702 and batch: 200, loss is 4.641985807418823 and perplexity is 103.75017101209757
At time: 1085.1281516551971 and batch: 250, loss is 4.637722024917602 and perplexity is 103.30874459031887
At time: 1086.7338919639587 and batch: 300, loss is 4.612418308258056 and perplexity is 100.72744534870347
At time: 1088.3388612270355 and batch: 350, loss is 4.5573046684265135 and perplexity is 95.32619754526829
At time: 1089.943737745285 and batch: 400, loss is 4.586358194351196 and perplexity is 98.13638495106447
At time: 1091.5789902210236 and batch: 450, loss is 4.6364741706848145 and perplexity is 103.17991073575091
At time: 1093.184630393982 and batch: 500, loss is 4.626337203979492 and perplexity is 102.139262833766
At time: 1094.7911071777344 and batch: 550, loss is 4.60302472114563 and perplexity is 99.785683502218
At time: 1096.3980464935303 and batch: 600, loss is 4.6249973106384275 and perplexity is 102.00249876073033
At time: 1097.996842622757 and batch: 650, loss is 4.596140918731689 and perplexity is 99.10113741644578
At time: 1099.5941467285156 and batch: 700, loss is 4.572194547653198 and perplexity is 96.75621307265133
At time: 1101.2045159339905 and batch: 750, loss is 4.567523717880249 and perplexity is 96.30533507881294
At time: 1102.814002275467 and batch: 800, loss is 4.546118850708008 and perplexity is 94.265837628321
At time: 1104.4230480194092 and batch: 850, loss is 4.59901909828186 and perplexity is 99.38677915046924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858173370361328 and perplexity of 128.78873779421988
Finished 37 epochs...
Completing Train Step...
At time: 1108.6923716068268 and batch: 50, loss is 4.682184066772461 and perplexity is 108.00570683278222
At time: 1110.299847126007 and batch: 100, loss is 4.641217441558838 and perplexity is 103.67048354121077
At time: 1111.907390832901 and batch: 150, loss is 4.627228155136108 and perplexity is 102.23030447893079
At time: 1113.5169386863708 and batch: 200, loss is 4.641811618804931 and perplexity is 103.73210048750343
At time: 1115.125358581543 and batch: 250, loss is 4.6375601196289065 and perplexity is 103.29201971216081
At time: 1116.7336084842682 and batch: 300, loss is 4.612273960113526 and perplexity is 100.71290657821159
At time: 1118.343845129013 and batch: 350, loss is 4.557179384231567 and perplexity is 95.31425542744645
At time: 1119.9567279815674 and batch: 400, loss is 4.586269149780273 and perplexity is 98.12764682782156
At time: 1121.5621123313904 and batch: 450, loss is 4.636446466445923 and perplexity is 103.1770522544513
At time: 1123.1694695949554 and batch: 500, loss is 4.626331119537354 and perplexity is 102.13864137522182
At time: 1124.776035308838 and batch: 550, loss is 4.603018398284912 and perplexity is 99.78505257323424
At time: 1126.383490562439 and batch: 600, loss is 4.624961614608765 and perplexity is 101.99885774149425
At time: 1127.990297794342 and batch: 650, loss is 4.596103572845459 and perplexity is 99.09743646575066
At time: 1129.5939719676971 and batch: 700, loss is 4.572164258956909 and perplexity is 96.7532824974814
At time: 1131.2210838794708 and batch: 750, loss is 4.567610712051391 and perplexity is 96.31371344604398
At time: 1132.8212220668793 and batch: 800, loss is 4.546210823059082 and perplexity is 94.27450787773911
At time: 1134.423392534256 and batch: 850, loss is 4.599051752090454 and perplexity is 99.3900245603196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858146667480469 and perplexity of 128.78529880981415
Finished 38 epochs...
Completing Train Step...
At time: 1138.6357548236847 and batch: 50, loss is 4.681991500854492 and perplexity is 107.98491061708556
At time: 1140.2437152862549 and batch: 100, loss is 4.641025123596191 and perplexity is 103.65054776209519
At time: 1141.8531970977783 and batch: 150, loss is 4.627038059234619 and perplexity is 102.21087276404454
At time: 1143.4629697799683 and batch: 200, loss is 4.641642036437989 and perplexity is 103.71451084386366
At time: 1145.0714011192322 and batch: 250, loss is 4.637407512664795 and perplexity is 103.27625783333238
At time: 1146.6780099868774 and batch: 300, loss is 4.612132253646851 and perplexity is 100.69863591921806
At time: 1148.2871723175049 and batch: 350, loss is 4.557058057785034 and perplexity is 95.30269198902114
At time: 1149.8954985141754 and batch: 400, loss is 4.586183166503906 and perplexity is 98.11920985396962
At time: 1151.5022194385529 and batch: 450, loss is 4.6364205455780025 and perplexity is 103.17437785036897
At time: 1153.1111521720886 and batch: 500, loss is 4.6263282680511475 and perplexity is 102.13835012871006
At time: 1154.7186534404755 and batch: 550, loss is 4.60301739692688 and perplexity is 99.78495265272039
At time: 1156.3185305595398 and batch: 600, loss is 4.624929304122925 and perplexity is 101.9955621620867
At time: 1157.9231297969818 and batch: 650, loss is 4.596071443557739 and perplexity is 99.09425258685029
At time: 1159.532392501831 and batch: 700, loss is 4.572143459320069 and perplexity is 96.75127008527112
At time: 1161.138444185257 and batch: 750, loss is 4.567721109390259 and perplexity is 96.32434681064169
At time: 1162.7438549995422 and batch: 800, loss is 4.5463049030303955 and perplexity is 94.28337763796273
At time: 1164.3443431854248 and batch: 850, loss is 4.599086799621582 and perplexity is 99.39350799634174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858130137125651 and perplexity of 128.7831699607249
Finished 39 epochs...
Completing Train Step...
At time: 1168.5947132110596 and batch: 50, loss is 4.681813735961914 and perplexity is 107.9657163971294
At time: 1170.2335736751556 and batch: 100, loss is 4.640845756530762 and perplexity is 103.63195793476424
At time: 1171.84268784523 and batch: 150, loss is 4.626860618591309 and perplexity is 102.19273800999677
At time: 1173.4857485294342 and batch: 200, loss is 4.641485414505005 and perplexity is 103.6982681487112
At time: 1175.0972168445587 and batch: 250, loss is 4.6372599792480464 and perplexity is 103.26102225805114
At time: 1176.7101421356201 and batch: 300, loss is 4.612001276016235 and perplexity is 100.68544751419098
At time: 1178.322103023529 and batch: 350, loss is 4.556949520111084 and perplexity is 95.29234861784438
At time: 1179.9358716011047 and batch: 400, loss is 4.586109209060669 and perplexity is 98.11195347641124
At time: 1181.5498299598694 and batch: 450, loss is 4.636408996582031 and perplexity is 103.17318629677548
At time: 1183.146642446518 and batch: 500, loss is 4.6263423538208 and perplexity is 102.13978883611532
At time: 1184.743397474289 and batch: 550, loss is 4.603022832870483 and perplexity is 99.78549507956973
At time: 1186.3523745536804 and batch: 600, loss is 4.624910335540772 and perplexity is 101.99362746923583
At time: 1187.95991396904 and batch: 650, loss is 4.596054134368896 and perplexity is 99.09253736056363
At time: 1189.566651582718 and batch: 700, loss is 4.572142124176025 and perplexity is 96.75114090847542
At time: 1191.1656262874603 and batch: 750, loss is 4.567806243896484 and perplexity is 96.33254768542862
At time: 1192.759937763214 and batch: 800, loss is 4.546392698287963 and perplexity is 94.29165563476595
At time: 1194.3607621192932 and batch: 850, loss is 4.599124059677124 and perplexity is 99.39721147296568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858126640319824 and perplexity of 128.78271963177312
Finished 40 epochs...
Completing Train Step...
At time: 1198.6968133449554 and batch: 50, loss is 4.681649293899536 and perplexity is 107.94796375173978
At time: 1200.2807672023773 and batch: 100, loss is 4.640678510665894 and perplexity is 103.61462736760468
At time: 1201.8667075634003 and batch: 150, loss is 4.626691980361938 and perplexity is 102.1755058606447
At time: 1203.45978474617 and batch: 200, loss is 4.641335678100586 and perplexity is 103.68274190534508
At time: 1205.0517115592957 and batch: 250, loss is 4.637111749649048 and perplexity is 103.24571705249988
At time: 1206.6439237594604 and batch: 300, loss is 4.611876602172852 and perplexity is 100.67289545494961
At time: 1208.2381520271301 and batch: 350, loss is 4.556847677230835 and perplexity is 95.28264426476346
At time: 1209.8266298770905 and batch: 400, loss is 4.586040372848511 and perplexity is 98.1052000536092
At time: 1211.4188318252563 and batch: 450, loss is 4.636402578353882 and perplexity is 103.17252410985192
At time: 1213.0020291805267 and batch: 500, loss is 4.626362419128418 and perplexity is 102.14183832296
At time: 1214.7287452220917 and batch: 550, loss is 4.603029203414917 and perplexity is 99.78613076952486
At time: 1216.313866853714 and batch: 600, loss is 4.624896068572998 and perplexity is 101.9921723398197
At time: 1217.9093577861786 and batch: 650, loss is 4.596042728424072 and perplexity is 99.09140712299572
At time: 1219.518610715866 and batch: 700, loss is 4.572148132324219 and perplexity is 96.75172220541411
At time: 1221.11674284935 and batch: 750, loss is 4.567870845794678 and perplexity is 96.33877115188864
At time: 1222.7016379833221 and batch: 800, loss is 4.546473646163941 and perplexity is 94.2992886529461
At time: 1224.2859346866608 and batch: 850, loss is 4.599161739349365 and perplexity is 99.4009567978765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858128547668457 and perplexity of 128.78296526555158
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1228.432604789734 and batch: 50, loss is 4.681559295654297 and perplexity is 107.93824906158406
At time: 1230.0146865844727 and batch: 100, loss is 4.640572366714477 and perplexity is 103.60362988529975
At time: 1231.597017288208 and batch: 150, loss is 4.626644268035888 and perplexity is 102.17063094589243
At time: 1233.1805860996246 and batch: 200, loss is 4.641139326095581 and perplexity is 103.6623855896546
At time: 1234.7760214805603 and batch: 250, loss is 4.6369389724731445 and perplexity is 103.22788009003783
At time: 1236.36048579216 and batch: 300, loss is 4.611724843978882 and perplexity is 100.65761867737108
At time: 1237.9481709003448 and batch: 350, loss is 4.556784877777099 and perplexity is 95.27666075463567
At time: 1239.5384635925293 and batch: 400, loss is 4.585767602920532 and perplexity is 98.07844355460627
At time: 1241.1394577026367 and batch: 450, loss is 4.63619969367981 and perplexity is 103.15159410918473
At time: 1242.7225284576416 and batch: 500, loss is 4.626324062347412 and perplexity is 102.13792056597269
At time: 1244.3048543930054 and batch: 550, loss is 4.60285062789917 and perplexity is 99.76831300071427
At time: 1245.8870627880096 and batch: 600, loss is 4.624539976119995 and perplexity is 101.95586016261399
At time: 1247.4696099758148 and batch: 650, loss is 4.595595664978028 and perplexity is 99.04711687806638
At time: 1249.0531508922577 and batch: 700, loss is 4.57171627998352 and perplexity is 96.70994876834062
At time: 1250.6471762657166 and batch: 750, loss is 4.56743483543396 and perplexity is 96.29677560543954
At time: 1252.2319829463959 and batch: 800, loss is 4.545985641479493 and perplexity is 94.25328138513748
At time: 1253.8144297599792 and batch: 850, loss is 4.598736848831177 and perplexity is 99.35873124508797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858090082804362 and perplexity of 128.77801174156386
Finished 42 epochs...
Completing Train Step...
At time: 1257.937623500824 and batch: 50, loss is 4.6815094470977785 and perplexity is 107.93286862977973
At time: 1259.551617383957 and batch: 100, loss is 4.640505456924439 and perplexity is 103.59669802008433
At time: 1261.1410439014435 and batch: 150, loss is 4.626562881469726 and perplexity is 102.16231596744551
At time: 1262.7302889823914 and batch: 200, loss is 4.6411300373077395 and perplexity is 103.66142269621983
At time: 1264.3192377090454 and batch: 250, loss is 4.636893587112427 and perplexity is 103.22319516177821
At time: 1265.9070818424225 and batch: 300, loss is 4.611697988510132 and perplexity is 100.65491550613585
At time: 1267.4937608242035 and batch: 350, loss is 4.556769905090332 and perplexity is 95.27523421771748
At time: 1269.0816533565521 and batch: 400, loss is 4.585737648010254 and perplexity is 98.07550566763172
At time: 1270.6694152355194 and batch: 450, loss is 4.636176490783692 and perplexity is 103.14920072122905
At time: 1272.2570033073425 and batch: 500, loss is 4.6262806224823 and perplexity is 102.13348380484732
At time: 1273.8439271450043 and batch: 550, loss is 4.602833223342896 and perplexity is 99.766576592607
At time: 1275.4341168403625 and batch: 600, loss is 4.624534196853638 and perplexity is 101.95527093424411
At time: 1277.0242447853088 and batch: 650, loss is 4.595590620040894 and perplexity is 99.04661719284888
At time: 1278.6150896549225 and batch: 700, loss is 4.57172137260437 and perplexity is 96.71044127669614
At time: 1280.2065844535828 and batch: 750, loss is 4.567450551986695 and perplexity is 96.29828907068473
At time: 1281.797515153885 and batch: 800, loss is 4.546034421920776 and perplexity is 94.25787921393703
At time: 1283.388160943985 and batch: 850, loss is 4.598759689331055 and perplexity is 99.3610006740942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858057339986165 and perplexity of 128.7737952555678
Finished 43 epochs...
Completing Train Step...
At time: 1287.4988825321198 and batch: 50, loss is 4.681468706130982 and perplexity is 107.92847142993632
At time: 1289.1105949878693 and batch: 100, loss is 4.640456237792969 and perplexity is 103.59159920606525
At time: 1290.6946332454681 and batch: 150, loss is 4.62650881767273 and perplexity is 102.15679283403647
At time: 1292.2848000526428 and batch: 200, loss is 4.6411059284210205 and perplexity is 103.65892356484868
At time: 1293.870703458786 and batch: 250, loss is 4.636860179901123 and perplexity is 103.21974682028602
At time: 1295.4898369312286 and batch: 300, loss is 4.611675519943237 and perplexity is 100.65265395984044
At time: 1297.0760502815247 and batch: 350, loss is 4.556753454208374 and perplexity is 95.27366686897803
At time: 1298.6625723838806 and batch: 400, loss is 4.585715866088867 and perplexity is 98.07336941794313
At time: 1300.2515325546265 and batch: 450, loss is 4.636168327331543 and perplexity is 103.14835867110179
At time: 1301.8395509719849 and batch: 500, loss is 4.626273212432861 and perplexity is 102.13272699348697
At time: 1303.427977323532 and batch: 550, loss is 4.602831764221191 and perplexity is 99.76643102113592
At time: 1305.0183024406433 and batch: 600, loss is 4.624529914855957 and perplexity is 101.9548343629451
At time: 1306.6116094589233 and batch: 650, loss is 4.595585527420044 and perplexity is 99.04611278726547
At time: 1308.203855752945 and batch: 700, loss is 4.571723461151123 and perplexity is 96.71064326118521
At time: 1309.7954802513123 and batch: 750, loss is 4.567469110488892 and perplexity is 96.30007623927757
At time: 1311.4091045856476 and batch: 800, loss is 4.546066608428955 and perplexity is 94.260913094762
At time: 1313.0176248550415 and batch: 850, loss is 4.598775234222412 and perplexity is 99.36254524205987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858035087585449 and perplexity of 128.77092976135626
Finished 44 epochs...
Completing Train Step...
At time: 1317.3325591087341 and batch: 50, loss is 4.681429758071899 and perplexity is 107.9242679073144
At time: 1318.925588607788 and batch: 100, loss is 4.640413122177124 and perplexity is 103.58713288675389
At time: 1320.5107808113098 and batch: 150, loss is 4.62646369934082 and perplexity is 102.15218379392743
At time: 1322.0949840545654 and batch: 200, loss is 4.641076402664185 and perplexity is 103.65586300186057
At time: 1323.6798853874207 and batch: 250, loss is 4.636829299926758 and perplexity is 103.2165594463635
At time: 1325.2637610435486 and batch: 300, loss is 4.6116533279418945 and perplexity is 100.65042030079343
At time: 1326.8484785556793 and batch: 350, loss is 4.556735973358155 and perplexity is 95.27200141883446
At time: 1328.4326055049896 and batch: 400, loss is 4.5856963157653805 and perplexity is 98.07145207058805
At time: 1330.014680147171 and batch: 450, loss is 4.636164360046386 and perplexity is 103.14794945296127
At time: 1331.5961322784424 and batch: 500, loss is 4.626276683807373 and perplexity is 102.1330815350477
At time: 1333.1784434318542 and batch: 550, loss is 4.602834787368774 and perplexity is 99.76673263023662
At time: 1334.7619392871857 and batch: 600, loss is 4.6245254611969 and perplexity is 101.95438029188473
At time: 1336.3761882781982 and batch: 650, loss is 4.5955797290802005 and perplexity is 99.04553848590832
At time: 1337.9597744941711 and batch: 700, loss is 4.571724004745484 and perplexity is 96.7106958325598
At time: 1339.5441110134125 and batch: 750, loss is 4.56748706817627 and perplexity is 96.3018055814686
At time: 1341.1303024291992 and batch: 800, loss is 4.546092224121094 and perplexity is 94.26332768421814
At time: 1342.7153227329254 and batch: 850, loss is 4.598787870407104 and perplexity is 99.36380081346584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858019510904948 and perplexity of 128.7689239533475
Finished 45 epochs...
Completing Train Step...
At time: 1346.8071954250336 and batch: 50, loss is 4.681391410827636 and perplexity is 107.920129388402
At time: 1348.3908247947693 and batch: 100, loss is 4.640372819900513 and perplexity is 103.58295817359672
At time: 1349.9735252857208 and batch: 150, loss is 4.626422481536865 and perplexity is 102.14797339201462
At time: 1351.5544755458832 and batch: 200, loss is 4.6410453987121585 and perplexity is 103.65264931027565
At time: 1353.1363434791565 and batch: 250, loss is 4.636799221038818 and perplexity is 103.21345485373
At time: 1354.7178423404694 and batch: 300, loss is 4.611631393432617 and perplexity is 100.648212607428
At time: 1356.301164150238 and batch: 350, loss is 4.5567186832427975 and perplexity is 95.27035416918022
At time: 1357.884933233261 and batch: 400, loss is 4.58567795753479 and perplexity is 98.06965166878271
At time: 1359.4692528247833 and batch: 450, loss is 4.6361620998382564 and perplexity is 103.14771631739079
At time: 1361.0561940670013 and batch: 500, loss is 4.626283664703369 and perplexity is 102.13379451795622
At time: 1362.6411645412445 and batch: 550, loss is 4.602838830947876 and perplexity is 99.76713604572736
At time: 1364.2275669574738 and batch: 600, loss is 4.624520778656006 and perplexity is 101.95390288744746
At time: 1365.8126754760742 and batch: 650, loss is 4.595574064254761 and perplexity is 99.04497741181139
At time: 1367.4001197814941 and batch: 700, loss is 4.5717241573333744 and perplexity is 96.71071058944202
At time: 1368.9828879833221 and batch: 750, loss is 4.567503471374511 and perplexity is 96.30338525203236
At time: 1370.5659573078156 and batch: 800, loss is 4.546114330291748 and perplexity is 94.26541150845894
At time: 1372.1493821144104 and batch: 850, loss is 4.598799371719361 and perplexity is 99.36494363413794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858010292053223 and perplexity of 128.76773685720255
Finished 46 epochs...
Completing Train Step...
At time: 1376.2414650917053 and batch: 50, loss is 4.681354312896729 and perplexity is 107.91612584916037
At time: 1377.863941669464 and batch: 100, loss is 4.640333995819092 and perplexity is 103.57893673845955
At time: 1379.4521470069885 and batch: 150, loss is 4.626383419036865 and perplexity is 102.1439833147357
At time: 1381.0395810604095 and batch: 200, loss is 4.641014051437378 and perplexity is 103.6494001331227
At time: 1382.6270685195923 and batch: 250, loss is 4.636769609451294 and perplexity is 103.21039858472864
At time: 1384.215223312378 and batch: 300, loss is 4.6116096305847165 and perplexity is 100.64602223951995
At time: 1385.8069851398468 and batch: 350, loss is 4.556701555252075 and perplexity is 95.26872239341243
At time: 1387.394689321518 and batch: 400, loss is 4.5856601428985595 and perplexity is 98.06790460917466
At time: 1388.9815187454224 and batch: 450, loss is 4.636160020828247 and perplexity is 103.14750187247903
At time: 1390.5674095153809 and batch: 500, loss is 4.626291437149048 and perplexity is 102.13458835041111
At time: 1392.1579809188843 and batch: 550, loss is 4.602843551635742 and perplexity is 99.76760701634761
At time: 1393.7424669265747 and batch: 600, loss is 4.624516382217407 and perplexity is 101.95345465435881
At time: 1395.334053993225 and batch: 650, loss is 4.595567893981934 and perplexity is 99.04436627916405
At time: 1396.9207780361176 and batch: 700, loss is 4.571723880767823 and perplexity is 96.71068384259468
At time: 1398.510296344757 and batch: 750, loss is 4.567518863677979 and perplexity is 96.30486759437139
At time: 1400.0993626117706 and batch: 800, loss is 4.546134786605835 and perplexity is 94.26733985104764
At time: 1401.687345981598 and batch: 850, loss is 4.598810176849366 and perplexity is 99.36601729107234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858003616333008 and perplexity of 128.76687724268788
Finished 47 epochs...
Completing Train Step...
At time: 1405.7919619083405 and batch: 50, loss is 4.681317873001099 and perplexity is 107.91219346844586
At time: 1407.3735978603363 and batch: 100, loss is 4.640296173095703 and perplexity is 103.57501917507331
At time: 1408.955650806427 and batch: 150, loss is 4.6263457298278805 and perplexity is 102.14013366134766
At time: 1410.5384645462036 and batch: 200, loss is 4.640982704162598 and perplexity is 103.646151057821
At time: 1412.12144947052 and batch: 250, loss is 4.636739826202392 and perplexity is 103.20732468951385
At time: 1413.702939748764 and batch: 300, loss is 4.611588125228882 and perplexity is 100.64385783427157
At time: 1415.2853555679321 and batch: 350, loss is 4.556684417724609 and perplexity is 95.2670897370557
At time: 1416.8681952953339 and batch: 400, loss is 4.585642576217651 and perplexity is 98.06618189671828
At time: 1418.4778044223785 and batch: 450, loss is 4.6361579418182375 and perplexity is 103.1472874280131
At time: 1420.0703210830688 and batch: 500, loss is 4.626299657821655 and perplexity is 102.13542796887495
At time: 1421.66144657135 and batch: 550, loss is 4.602848167419434 and perplexity is 99.7680675231038
At time: 1423.2457480430603 and batch: 600, loss is 4.624511985778809 and perplexity is 101.95300642324086
At time: 1424.830960035324 and batch: 650, loss is 4.595561819076538 and perplexity is 99.04376459583654
At time: 1426.4168388843536 and batch: 700, loss is 4.571723489761353 and perplexity is 96.71064602809898
At time: 1428.0018780231476 and batch: 750, loss is 4.567533187866211 and perplexity is 96.30624709330255
At time: 1429.5897243022919 and batch: 800, loss is 4.546154003143311 and perplexity is 94.26915136032206
At time: 1431.176316022873 and batch: 850, loss is 4.598820552825928 and perplexity is 99.36704831588777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.858001073201497 and perplexity of 128.7665497720012
Finished 48 epochs...
Completing Train Step...
At time: 1435.3587501049042 and batch: 50, loss is 4.681282205581665 and perplexity is 107.90834458761961
At time: 1436.9408390522003 and batch: 100, loss is 4.640259132385254 and perplexity is 103.57118275383061
At time: 1438.5294961929321 and batch: 150, loss is 4.626308660507203 and perplexity is 102.13634746615516
At time: 1440.1128504276276 and batch: 200, loss is 4.640951681137085 and perplexity is 103.64293569050788
At time: 1441.6980929374695 and batch: 250, loss is 4.6367103862762455 and perplexity is 103.20428631822213
At time: 1443.2819221019745 and batch: 300, loss is 4.611566686630249 and perplexity is 100.64170019412713
At time: 1444.8714241981506 and batch: 350, loss is 4.556667613983154 and perplexity is 95.26548890696057
At time: 1446.456327676773 and batch: 400, loss is 4.585625314712525 and perplexity is 98.06448914142652
At time: 1448.0389609336853 and batch: 450, loss is 4.636156196594238 and perplexity is 103.14710741304872
At time: 1449.6269900798798 and batch: 500, loss is 4.626307935714721 and perplexity is 102.1362734385253
At time: 1451.2214481830597 and batch: 550, loss is 4.602852783203125 and perplexity is 99.76852803198558
At time: 1452.804433107376 and batch: 600, loss is 4.624507169723511 and perplexity is 101.95251541310654
At time: 1454.3849847316742 and batch: 650, loss is 4.595555515289306 and perplexity is 99.04314024698577
At time: 1455.966052532196 and batch: 700, loss is 4.571723203659058 and perplexity is 96.71061835896518
At time: 1457.5468928813934 and batch: 750, loss is 4.5675467681884765 and perplexity is 96.30755497205497
At time: 1459.1552667617798 and batch: 800, loss is 4.546172361373902 and perplexity is 94.27088199102593
At time: 1460.7389554977417 and batch: 850, loss is 4.598830461502075 and perplexity is 99.36803291666727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.857998847961426 and perplexity of 128.76626323583363
Finished 49 epochs...
Completing Train Step...
At time: 1464.8239653110504 and batch: 50, loss is 4.681246967315674 and perplexity is 107.90454215166636
At time: 1466.4586260318756 and batch: 100, loss is 4.640222730636597 and perplexity is 103.56741265028748
At time: 1468.052763223648 and batch: 150, loss is 4.626272411346435 and perplexity is 102.13264517637853
At time: 1469.6413249969482 and batch: 200, loss is 4.640920934677124 and perplexity is 103.63974908612407
At time: 1471.2274289131165 and batch: 250, loss is 4.636680707931519 and perplexity is 103.20122343128644
At time: 1472.8390746116638 and batch: 300, loss is 4.611545515060425 and perplexity is 100.63956947389964
At time: 1474.4538671970367 and batch: 350, loss is 4.5566511154174805 and perplexity is 95.2639171760011
At time: 1476.0559012889862 and batch: 400, loss is 4.585608291625976 and perplexity is 98.0628197953493
At time: 1477.644181728363 and batch: 450, loss is 4.636154260635376 and perplexity is 103.14690772468528
At time: 1479.2278151512146 and batch: 500, loss is 4.626316184997559 and perplexity is 102.13711599300805
At time: 1480.8217170238495 and batch: 550, loss is 4.602857446670532 and perplexity is 99.76899330034917
At time: 1482.4107401371002 and batch: 600, loss is 4.624502897262573 and perplexity is 101.95207982589743
At time: 1483.9972138404846 and batch: 650, loss is 4.59554913520813 and perplexity is 99.04250834572683
At time: 1485.5822372436523 and batch: 700, loss is 4.571722717285156 and perplexity is 96.7105713214558
At time: 1487.168039560318 and batch: 750, loss is 4.567559814453125 and perplexity is 96.3088114341008
At time: 1488.754460811615 and batch: 800, loss is 4.546190233230591 and perplexity is 94.2725668017741
At time: 1490.3396575450897 and batch: 850, loss is 4.598840618133545 and perplexity is 99.3690421662828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.857996940612793 and perplexity of 128.76601763391173
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f079a7ddba8>
SETTINGS FOR THIS RUN
{'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 4.033060124244714, 'anneal': 3.5785500742123912, 'dropout': 0.0, 'tune_wordvecs': True}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 880 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.288452625274658 and batch: 50, loss is 7.298686885833741 and perplexity is 1478.3574004383981
At time: 3.8693010807037354 and batch: 100, loss is 6.341238784790039 and perplexity is 567.4988851101632
At time: 5.481122016906738 and batch: 150, loss is 6.021509819030761 and perplexity is 412.20047426704167
At time: 7.062371492385864 and batch: 200, loss is 5.890142574310302 and perplexity is 361.4568151548407
At time: 8.642977952957153 and batch: 250, loss is 5.799003162384033 and perplexity is 329.9704689361752
At time: 10.224640607833862 and batch: 300, loss is 5.678650856018066 and perplexity is 292.5544654540367
At time: 11.8052499294281 and batch: 350, loss is 5.556024026870728 and perplexity is 258.7918386750301
At time: 13.385971307754517 and batch: 400, loss is 5.535183277130127 and perplexity is 253.45423574546695
At time: 14.970778703689575 and batch: 450, loss is 5.491712388992309 and perplexity is 242.67240081761304
At time: 16.551220893859863 and batch: 500, loss is 5.447926244735718 and perplexity is 232.27598257372077
At time: 18.130945205688477 and batch: 550, loss is 5.378709468841553 and perplexity is 216.7423820608344
At time: 19.711997032165527 and batch: 600, loss is 5.398841352462768 and perplexity is 221.1500327630283
At time: 21.29339575767517 and batch: 650, loss is 5.385713891983032 and perplexity is 218.26586675444756
At time: 22.878103733062744 and batch: 700, loss is 5.314483127593994 and perplexity is 203.25942678410607
At time: 24.45898175239563 and batch: 750, loss is 5.287502555847168 and perplexity is 197.84869182323797
At time: 26.040156364440918 and batch: 800, loss is 5.275595073699951 and perplexity is 195.50678284833305
At time: 27.621716737747192 and batch: 850, loss is 5.256893692016601 and perplexity is 191.88451218469243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.974844932556152 and perplexity of 144.72638105808406
Finished 1 epochs...
Completing Train Step...
At time: 31.71281671524048 and batch: 50, loss is 5.234858150482178 and perplexity is 187.702478949569
At time: 33.29447817802429 and batch: 100, loss is 5.144492664337158 and perplexity is 171.48446241477544
At time: 34.87467288970947 and batch: 150, loss is 5.11185438156128 and perplexity is 165.97785594910252
At time: 36.45459866523743 and batch: 200, loss is 5.118335781097412 and perplexity is 167.05711853548814
At time: 38.03554630279541 and batch: 250, loss is 5.1284412670135495 and perplexity is 168.75387070717017
At time: 39.61618399620056 and batch: 300, loss is 5.075528755187988 and perplexity is 160.05680047244797
At time: 41.198225021362305 and batch: 350, loss is 5.0212616539001464 and perplexity is 151.60245301273395
At time: 42.77875566482544 and batch: 400, loss is 5.0260379600524905 and perplexity is 152.32828475921508
At time: 44.35926151275635 and batch: 450, loss is 5.021777143478394 and perplexity is 151.68062264338272
At time: 45.941457986831665 and batch: 500, loss is 5.005683727264405 and perplexity is 149.25910079496782
At time: 47.52187752723694 and batch: 550, loss is 4.9840788078308105 and perplexity is 146.0689554525039
At time: 49.10180592536926 and batch: 600, loss is 5.020098323822022 and perplexity is 151.42619186405304
At time: 50.71015000343323 and batch: 650, loss is 5.008860321044922 and perplexity is 149.73399019225926
At time: 52.28936719894409 and batch: 700, loss is 4.962722063064575 and perplexity is 142.98247396514793
At time: 53.86831879615784 and batch: 750, loss is 4.9388409614562985 and perplexity is 139.6083442988441
At time: 55.449459075927734 and batch: 800, loss is 4.925900897979736 and perplexity is 137.81344158487596
At time: 57.0305449962616 and batch: 850, loss is 4.923432350158691 and perplexity is 137.47366206747822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.796777089436849 and perplexity of 121.11943070678055
Finished 2 epochs...
Completing Train Step...
At time: 61.08255577087402 and batch: 50, loss is 4.92202166557312 and perplexity is 137.2798668156031
At time: 62.66469955444336 and batch: 100, loss is 4.846494474411011 and perplexity is 127.29337660661872
At time: 64.2472038269043 and batch: 150, loss is 4.840686302185059 and perplexity is 126.55617771357325
At time: 65.82921266555786 and batch: 200, loss is 4.857896862030029 and perplexity is 128.7531315581796
At time: 67.41106224060059 and batch: 250, loss is 4.867515211105347 and perplexity is 129.9974989085291
At time: 68.99353122711182 and batch: 300, loss is 4.826704587936401 and perplexity is 124.79901807759691
At time: 70.57586550712585 and batch: 350, loss is 4.78208176612854 and perplexity is 119.35255570993375
At time: 72.15647268295288 and batch: 400, loss is 4.789843406677246 and perplexity is 120.28253174707078
At time: 73.7368495464325 and batch: 450, loss is 4.799928255081177 and perplexity is 121.50170007641368
At time: 75.32443499565125 and batch: 500, loss is 4.78772873878479 and perplexity is 120.02844289062187
At time: 76.90507888793945 and batch: 550, loss is 4.784372358322144 and perplexity is 119.62625709178226
At time: 78.49673104286194 and batch: 600, loss is 4.823535585403443 and perplexity is 124.40415566364624
At time: 80.10551190376282 and batch: 650, loss is 4.8119828987121585 and perplexity is 122.97522332658487
At time: 81.68548822402954 and batch: 700, loss is 4.772391395568848 and perplexity is 118.20157095898713
At time: 83.2713553905487 and batch: 750, loss is 4.749450330734253 and perplexity is 115.52076885628497
At time: 84.85248804092407 and batch: 800, loss is 4.728709783554077 and perplexity is 113.1494808095343
At time: 86.48033809661865 and batch: 850, loss is 4.736931104660034 and perplexity is 114.08355341850164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.714468955993652 and perplexity of 111.54955771796742
Finished 3 epochs...
Completing Train Step...
At time: 90.54368615150452 and batch: 50, loss is 4.7368769931793215 and perplexity is 114.07738035551954
At time: 92.12990736961365 and batch: 100, loss is 4.669068279266358 and perplexity is 106.59837623155684
At time: 93.71546959877014 and batch: 150, loss is 4.675382680892945 and perplexity is 107.27361079939242
At time: 95.300936460495 and batch: 200, loss is 4.69643162727356 and perplexity is 109.5555391230061
At time: 96.88587737083435 and batch: 250, loss is 4.700880517959595 and perplexity is 110.04402554626108
At time: 98.47925209999084 and batch: 300, loss is 4.666877670288086 and perplexity is 106.36511645527483
At time: 100.06409502029419 and batch: 350, loss is 4.62421685218811 and perplexity is 101.92292110618483
At time: 101.64739632606506 and batch: 400, loss is 4.636049356460571 and perplexity is 103.13608775098689
At time: 103.23105430603027 and batch: 450, loss is 4.65321985244751 and perplexity is 104.92227652102923
At time: 104.81667304039001 and batch: 500, loss is 4.642031650543213 and perplexity is 103.75492735311468
At time: 106.39916658401489 and batch: 550, loss is 4.645922574996948 and perplexity is 104.1594163448968
At time: 107.98181653022766 and batch: 600, loss is 4.688380327224731 and perplexity is 108.67701597797121
At time: 109.56533932685852 and batch: 650, loss is 4.672171850204467 and perplexity is 106.92972577151265
At time: 111.15073204040527 and batch: 700, loss is 4.641242742538452 and perplexity is 103.67310653918355
At time: 112.73512530326843 and batch: 750, loss is 4.616660737991333 and perplexity is 101.15568219788757
At time: 114.3203182220459 and batch: 800, loss is 4.592070169448853 and perplexity is 98.69854152180544
At time: 115.92063355445862 and batch: 850, loss is 4.60624852180481 and perplexity is 100.10789174298246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.667593955993652 and perplexity of 106.44133156039922
Finished 4 epochs...
Completing Train Step...
At time: 119.97774076461792 and batch: 50, loss is 4.606143131256103 and perplexity is 100.09734187327985
At time: 121.55778074264526 and batch: 100, loss is 4.545928926467895 and perplexity is 94.24793596077497
At time: 123.13894081115723 and batch: 150, loss is 4.556536798477173 and perplexity is 95.2530275189158
At time: 124.71924662590027 and batch: 200, loss is 4.5750631332397464 and perplexity is 97.03416502476183
At time: 126.32464694976807 and batch: 250, loss is 4.57825101852417 and perplexity is 97.34399239620171
At time: 127.90914940834045 and batch: 300, loss is 4.5493976020812985 and perplexity is 94.57541911592426
At time: 129.4993269443512 and batch: 350, loss is 4.509844942092895 and perplexity is 90.90772145659925
At time: 131.08540749549866 and batch: 400, loss is 4.521358451843262 and perplexity is 91.96043699020218
At time: 132.6639244556427 and batch: 450, loss is 4.544936056137085 and perplexity is 94.15440642045563
At time: 134.24132204055786 and batch: 500, loss is 4.532267179489136 and perplexity is 92.9690999637733
At time: 135.82176971435547 and batch: 550, loss is 4.545236320495605 and perplexity is 94.18268187774488
At time: 137.40032601356506 and batch: 600, loss is 4.586172895431519 and perplexity is 98.11820206963814
At time: 138.98899793624878 and batch: 650, loss is 4.565186147689819 and perplexity is 96.0804775109168
At time: 140.57842302322388 and batch: 700, loss is 4.541523971557617 and perplexity is 93.83369108729663
At time: 142.1606855392456 and batch: 750, loss is 4.513185329437256 and perplexity is 91.21189620676398
At time: 143.75013875961304 and batch: 800, loss is 4.484771928787231 and perplexity is 88.65672845940009
At time: 145.34160566329956 and batch: 850, loss is 4.504807977676392 and perplexity is 90.45097377502543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.643377621968587 and perplexity of 103.8946725459946
Finished 5 epochs...
Completing Train Step...
At time: 149.438214302063 and batch: 50, loss is 4.500666913986206 and perplexity is 90.07718500748693
At time: 151.02144503593445 and batch: 100, loss is 4.445479650497436 and perplexity is 85.2407538034138
At time: 152.60533928871155 and batch: 150, loss is 4.456973018646241 and perplexity is 86.22610884539691
At time: 154.1882302761078 and batch: 200, loss is 4.480423202514649 and perplexity is 88.27202171355296
At time: 155.76955461502075 and batch: 250, loss is 4.4811412525177 and perplexity is 88.33542820080358
At time: 157.35148882865906 and batch: 300, loss is 4.45489182472229 and perplexity is 86.04684220061375
At time: 158.93279027938843 and batch: 350, loss is 4.4128999996185305 and perplexity is 82.50839119463771
At time: 160.51317501068115 and batch: 400, loss is 4.429293413162231 and perplexity is 83.87213302287853
At time: 162.0926296710968 and batch: 450, loss is 4.457268905639649 and perplexity is 86.25162580437842
At time: 163.6718306541443 and batch: 500, loss is 4.440948400497437 and perplexity is 84.8553804087204
At time: 165.2519416809082 and batch: 550, loss is 4.461555795669556 and perplexity is 86.62217071427078
At time: 166.86086463928223 and batch: 600, loss is 4.502815561294556 and perplexity is 90.27093718660502
At time: 168.44605827331543 and batch: 650, loss is 4.475705251693726 and perplexity is 87.85653953916446
At time: 170.03160214424133 and batch: 700, loss is 4.455803356170654 and perplexity is 86.12531236188856
At time: 171.62614631652832 and batch: 750, loss is 4.431643409729004 and perplexity is 84.06946402030033
At time: 173.20968508720398 and batch: 800, loss is 4.3994588279724125 and perplexity is 81.40680165821512
At time: 174.7919807434082 and batch: 850, loss is 4.4227274131774905 and perplexity is 83.32323261080985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.627553304036458 and perplexity of 102.26354995458658
Finished 6 epochs...
Completing Train Step...
At time: 178.85871243476868 and batch: 50, loss is 4.415375156402588 and perplexity is 82.71286534724689
At time: 180.46968412399292 and batch: 100, loss is 4.361482677459716 and perplexity is 78.37325057248225
At time: 182.0561273097992 and batch: 150, loss is 4.376442718505859 and perplexity is 79.55453159315712
At time: 183.64432549476624 and batch: 200, loss is 4.398251180648804 and perplexity is 81.30855029051055
At time: 185.23692870140076 and batch: 250, loss is 4.398846769332886 and perplexity is 81.35699116696972
At time: 186.82274389266968 and batch: 300, loss is 4.376151351928711 and perplexity is 79.53135543813265
At time: 188.40960693359375 and batch: 350, loss is 4.336685857772827 and perplexity is 76.4537404381589
At time: 189.99518394470215 and batch: 400, loss is 4.35184024810791 and perplexity is 77.62117379148931
At time: 191.58657693862915 and batch: 450, loss is 4.381151838302612 and perplexity is 79.930046892122
At time: 193.1720006465912 and batch: 500, loss is 4.365659427642822 and perplexity is 78.70128063409123
At time: 194.75812315940857 and batch: 550, loss is 4.388862190246582 and perplexity is 80.54871770443754
At time: 196.344575881958 and batch: 600, loss is 4.430535202026367 and perplexity is 83.97634919752812
At time: 197.93042135238647 and batch: 650, loss is 4.400010414123535 and perplexity is 81.45171690879332
At time: 199.5243103504181 and batch: 700, loss is 4.383204984664917 and perplexity is 80.09432356145263
At time: 201.11000609397888 and batch: 750, loss is 4.358317089080811 and perplexity is 78.12554539442286
At time: 202.70474100112915 and batch: 800, loss is 4.324186668395996 and perplexity is 75.50407802677731
At time: 204.30286622047424 and batch: 850, loss is 4.350309276580811 and perplexity is 77.50242890522802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.621405283610026 and perplexity of 101.63676029269838
Finished 7 epochs...
Completing Train Step...
At time: 208.35979223251343 and batch: 50, loss is 4.340743036270141 and perplexity is 76.76455700269028
At time: 209.96577072143555 and batch: 100, loss is 4.289821157455444 and perplexity is 72.95342015757319
At time: 211.54995918273926 and batch: 150, loss is 4.306044311523437 and perplexity is 74.1466071915867
At time: 213.13522720336914 and batch: 200, loss is 4.33107364654541 and perplexity is 76.02586767690654
At time: 214.71736407279968 and batch: 250, loss is 4.328401794433594 and perplexity is 75.82300892677188
At time: 216.30006885528564 and batch: 300, loss is 4.306619625091553 and perplexity is 74.1892770138207
At time: 217.8837490081787 and batch: 350, loss is 4.267480945587158 and perplexity is 71.34169546207526
At time: 219.47499990463257 and batch: 400, loss is 4.285438537597656 and perplexity is 72.63439264848476
At time: 221.0579514503479 and batch: 450, loss is 4.317431621551513 and perplexity is 74.99576322081339
At time: 222.63952898979187 and batch: 500, loss is 4.301261339187622 and perplexity is 73.79281278948781
At time: 224.2213807106018 and batch: 550, loss is 4.326006212234497 and perplexity is 75.64158606975099
At time: 225.80399870872498 and batch: 600, loss is 4.370846328735351 and perplexity is 79.11055691278912
At time: 227.38596510887146 and batch: 650, loss is 4.34418776512146 and perplexity is 77.02944606051156
At time: 228.96874332427979 and batch: 700, loss is 4.326852264404297 and perplexity is 75.7056098776942
At time: 230.55073285102844 and batch: 750, loss is 4.300971021652222 and perplexity is 73.77139255143432
At time: 232.13298678398132 and batch: 800, loss is 4.264250869750977 and perplexity is 71.11162814292696
At time: 233.7154541015625 and batch: 850, loss is 4.292184553146362 and perplexity is 73.12604186275824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.615074475606282 and perplexity of 100.99534994231533
Finished 8 epochs...
Completing Train Step...
At time: 237.77688598632812 and batch: 50, loss is 4.2778747940063475 and perplexity is 72.08707721272476
At time: 239.38278532028198 and batch: 100, loss is 4.230474348068237 and perplexity is 68.74983579230313
At time: 240.96267700195312 and batch: 150, loss is 4.246339492797851 and perplexity is 69.84926008949631
At time: 242.54668164253235 and batch: 200, loss is 4.271212015151978 and perplexity is 71.60837347846575
At time: 244.12824249267578 and batch: 250, loss is 4.271540508270264 and perplexity is 71.63190020033666
At time: 245.74494862556458 and batch: 300, loss is 4.250636053085327 and perplexity is 70.15001729456267
At time: 247.33159112930298 and batch: 350, loss is 4.210212960243225 and perplexity is 67.37088560270485
At time: 248.92099142074585 and batch: 400, loss is 4.227809228897095 and perplexity is 68.5668532303953
At time: 250.51746320724487 and batch: 450, loss is 4.26260401725769 and perplexity is 70.99461415964952
At time: 252.10482716560364 and batch: 500, loss is 4.24632001876831 and perplexity is 69.84789985618657
At time: 253.68742561340332 and batch: 550, loss is 4.27488543510437 and perplexity is 71.87190484070068
At time: 255.2706663608551 and batch: 600, loss is 4.319926462173462 and perplexity is 75.1830992870161
At time: 256.855770111084 and batch: 650, loss is 4.287080392837525 and perplexity is 72.75374576010363
At time: 258.4386076927185 and batch: 700, loss is 4.265819883346557 and perplexity is 71.2232908315177
At time: 260.03109860420227 and batch: 750, loss is 4.242729196548462 and perplexity is 69.59753823642043
At time: 261.62048864364624 and batch: 800, loss is 4.202487936019898 and perplexity is 66.85244892560438
At time: 263.22251057624817 and batch: 850, loss is 4.234042682647705 and perplexity is 68.9955964260046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.616306304931641 and perplexity of 101.11983563291999
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 267.3288240432739 and batch: 50, loss is 4.238477840423584 and perplexity is 69.30228237956541
At time: 268.91169357299805 and batch: 100, loss is 4.194016027450561 and perplexity is 66.28847343824395
At time: 270.4955427646637 and batch: 150, loss is 4.2072123479843135 and perplexity is 67.16903468708756
At time: 272.08201789855957 and batch: 200, loss is 4.226109008789063 and perplexity is 68.45037353640691
At time: 273.663911819458 and batch: 250, loss is 4.209333848953247 and perplexity is 67.31168512227377
At time: 275.24700379371643 and batch: 300, loss is 4.18435453414917 and perplexity is 65.65111168714625
At time: 276.83045864105225 and batch: 350, loss is 4.135344185829163 and perplexity is 62.5111028039867
At time: 278.4118914604187 and batch: 400, loss is 4.1385293245315555 and perplexity is 62.710526764746625
At time: 279.99683833122253 and batch: 450, loss is 4.166917667388916 and perplexity is 64.51628466676709
At time: 281.5880455970764 and batch: 500, loss is 4.134133253097534 and perplexity is 62.435451876847374
At time: 283.173898935318 and batch: 550, loss is 4.153673529624939 and perplexity is 63.667455517242345
At time: 284.7542402744293 and batch: 600, loss is 4.182899017333984 and perplexity is 65.55562489832096
At time: 286.36098980903625 and batch: 650, loss is 4.135883650779724 and perplexity is 62.544834450672894
At time: 287.94187092781067 and batch: 700, loss is 4.104640417098999 and perplexity is 60.62094237367629
At time: 289.52275252342224 and batch: 750, loss is 4.0746314907073975 and perplexity is 58.828797634806776
At time: 291.1044571399689 and batch: 800, loss is 4.02065896987915 and perplexity is 55.737823274288125
At time: 292.68474888801575 and batch: 850, loss is 4.04266143321991 and perplexity is 56.977783757576994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.560579299926758 and perplexity of 95.63886737438233
Finished 10 epochs...
Completing Train Step...
At time: 296.7511818408966 and batch: 50, loss is 4.166196088790894 and perplexity is 64.46974788851846
At time: 298.3360421657562 and batch: 100, loss is 4.118659291267395 and perplexity is 61.476764552400915
At time: 299.92090249061584 and batch: 150, loss is 4.131479997634887 and perplexity is 62.270014244296924
At time: 301.50699639320374 and batch: 200, loss is 4.156621751785278 and perplexity is 63.85543829181917
At time: 303.09301018714905 and batch: 250, loss is 4.142949042320251 and perplexity is 62.98830298951713
At time: 304.6859304904938 and batch: 300, loss is 4.124539399147034 and perplexity is 61.83931944637373
At time: 306.2769627571106 and batch: 350, loss is 4.078057246208191 and perplexity is 59.030676307962864
At time: 307.8631429672241 and batch: 400, loss is 4.085265502929688 and perplexity is 59.45772185513334
At time: 309.4475474357605 and batch: 450, loss is 4.120754055976867 and perplexity is 61.60567888469464
At time: 311.03313755989075 and batch: 500, loss is 4.0923848152160645 and perplexity is 59.88253032265831
At time: 312.6212594509125 and batch: 550, loss is 4.1199203300476075 and perplexity is 61.55433803789033
At time: 314.2068073749542 and batch: 600, loss is 4.151702222824096 and perplexity is 63.54207105541421
At time: 315.79216742515564 and batch: 650, loss is 4.110635542869568 and perplexity is 60.985464132643905
At time: 317.3770401477814 and batch: 700, loss is 4.08501139163971 and perplexity is 59.44261489623928
At time: 318.96600556373596 and batch: 750, loss is 4.062231993675232 and perplexity is 58.10385389020147
At time: 320.55163407325745 and batch: 800, loss is 4.01303870677948 and perplexity is 55.31470059690079
At time: 322.1371006965637 and batch: 850, loss is 4.040565671920777 and perplexity is 56.858496965449824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.559789975484212 and perplexity of 95.56340706395791
Finished 11 epochs...
Completing Train Step...
At time: 326.22230315208435 and batch: 50, loss is 4.132097272872925 and perplexity is 62.30846384793544
At time: 327.8028938770294 and batch: 100, loss is 4.084532246589661 and perplexity is 59.41414008387726
At time: 329.3826947212219 and batch: 150, loss is 4.097155728340149 and perplexity is 60.16890726904417
At time: 330.96243238449097 and batch: 200, loss is 4.124047765731811 and perplexity is 61.808924642722026
At time: 332.54241251945496 and batch: 250, loss is 4.110259504318237 and perplexity is 60.962535558343326
At time: 334.1232976913452 and batch: 300, loss is 4.094103631973266 and perplexity is 59.985545926351634
At time: 335.7032549381256 and batch: 350, loss is 4.049235920906067 and perplexity is 57.35361759900798
At time: 337.28370928764343 and batch: 400, loss is 4.057641530036927 and perplexity is 57.83774151855705
At time: 338.8642346858978 and batch: 450, loss is 4.0953904724121095 and perplexity is 60.062787440688794
At time: 340.4450581073761 and batch: 500, loss is 4.0688823890686034 and perplexity is 58.49155524718012
At time: 342.02406883239746 and batch: 550, loss is 4.09876003742218 and perplexity is 60.26551426645282
At time: 343.60350346565247 and batch: 600, loss is 4.1310755443573 and perplexity is 62.24483402539468
At time: 345.182208776474 and batch: 650, loss is 4.092793669700622 and perplexity is 59.90701856945104
At time: 346.7621924877167 and batch: 700, loss is 4.068902759552002 and perplexity is 58.49274676057106
At time: 348.34787607192993 and batch: 750, loss is 4.050575456619263 and perplexity is 57.43049629744823
At time: 349.9364869594574 and batch: 800, loss is 4.002256803512573 and perplexity is 54.72150647318642
At time: 351.5223557949066 and batch: 850, loss is 4.031379446983338 and perplexity is 56.3385737447614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.560696919759114 and perplexity of 95.65011706350997
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 355.6260724067688 and batch: 50, loss is 4.1239485168457035 and perplexity is 61.802790480209154
At time: 357.2329695224762 and batch: 100, loss is 4.08861349105835 and perplexity is 59.65711920578896
At time: 358.81544399261475 and batch: 150, loss is 4.098753833770752 and perplexity is 60.26514040136884
At time: 360.3985707759857 and batch: 200, loss is 4.129184513092041 and perplexity is 62.12723832177145
At time: 361.9812824726105 and batch: 250, loss is 4.10988862991333 and perplexity is 60.9399303063603
At time: 363.57244539260864 and batch: 300, loss is 4.087705249786377 and perplexity is 59.602960746156334
At time: 365.1623933315277 and batch: 350, loss is 4.039634189605713 and perplexity is 56.805558940310185
At time: 366.77670526504517 and batch: 400, loss is 4.042874145507812 and perplexity is 56.98990492143424
At time: 368.36055397987366 and batch: 450, loss is 4.079819860458374 and perplexity is 59.13481637163943
At time: 369.9480378627777 and batch: 500, loss is 4.047700138092041 and perplexity is 57.265602502113346
At time: 371.53174924850464 and batch: 550, loss is 4.070503106117249 and perplexity is 58.58643037006802
At time: 373.1162314414978 and batch: 600, loss is 4.095746335983276 and perplexity is 60.08416540231493
At time: 374.7002980709076 and batch: 650, loss is 4.053488483428955 and perplexity is 57.59803677933918
At time: 376.2864067554474 and batch: 700, loss is 4.019917569160461 and perplexity is 55.696514527114026
At time: 377.8711757659912 and batch: 750, loss is 4.001216669082641 and perplexity is 54.66461834102723
At time: 379.458664894104 and batch: 800, loss is 3.941937918663025 and perplexity is 51.518342982031655
At time: 381.04351115226746 and batch: 850, loss is 3.9716323804855347 and perplexity is 53.07109238532155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.541971524556478 and perplexity of 93.87569603615614
Finished 13 epochs...
Completing Train Step...
At time: 385.1537914276123 and batch: 50, loss is 4.107381143569946 and perplexity is 60.787315682856004
At time: 386.77440643310547 and batch: 100, loss is 4.062687287330627 and perplexity is 58.130314229386904
At time: 388.3603014945984 and batch: 150, loss is 4.070882358551025 and perplexity is 58.60865363021854
At time: 389.94838070869446 and batch: 200, loss is 4.102010197639466 and perplexity is 60.46170549715623
At time: 391.54750990867615 and batch: 250, loss is 4.0835956192016605 and perplexity is 59.35851722605258
At time: 393.1408360004425 and batch: 300, loss is 4.064229612350464 and perplexity is 58.2200392422137
At time: 394.7289843559265 and batch: 350, loss is 4.017724704742432 and perplexity is 55.57451343701239
At time: 396.3184621334076 and batch: 400, loss is 4.023067541122437 and perplexity is 55.87223359602923
At time: 397.90994215011597 and batch: 450, loss is 4.062523999214172 and perplexity is 58.12082301478983
At time: 399.50016808509827 and batch: 500, loss is 4.0328367233276365 and perplexity is 56.42073446644917
At time: 401.08825731277466 and batch: 550, loss is 4.058976287841797 and perplexity is 57.914992439630886
At time: 402.68573117256165 and batch: 600, loss is 4.087422108650207 and perplexity is 59.58608708505809
At time: 404.2825548648834 and batch: 650, loss is 4.048150410652161 and perplexity is 57.291393437598316
At time: 405.8985822200775 and batch: 700, loss is 4.017710094451904 and perplexity is 55.573701483156604
At time: 407.4879860877991 and batch: 750, loss is 4.0012277364730835 and perplexity is 54.66522333904966
At time: 409.0828104019165 and batch: 800, loss is 3.9454149866104125 and perplexity is 51.69778755080581
At time: 410.677481174469 and batch: 850, loss is 3.976945915222168 and perplexity is 53.353838002305984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.541481653849284 and perplexity of 93.82972034454234
Finished 14 epochs...
Completing Train Step...
At time: 414.76712679862976 and batch: 50, loss is 4.095507750511169 and perplexity is 60.06983190329658
At time: 416.3911406993866 and batch: 100, loss is 4.049948935508728 and perplexity is 57.39452614833488
At time: 417.9885218143463 and batch: 150, loss is 4.057291646003723 and perplexity is 57.817508556084796
At time: 419.5735228061676 and batch: 200, loss is 4.089304618835449 and perplexity is 59.69836414909502
At time: 421.1569037437439 and batch: 250, loss is 4.071157412528992 and perplexity is 58.62477639075585
At time: 422.7417435646057 and batch: 300, loss is 4.052609882354736 and perplexity is 57.54745330695249
At time: 424.34214901924133 and batch: 350, loss is 4.006572623252868 and perplexity is 54.958184994589175
At time: 425.934317111969 and batch: 400, loss is 4.012806835174561 and perplexity is 55.30187617536482
At time: 427.51860213279724 and batch: 450, loss is 4.0535216760635375 and perplexity is 57.599948641656354
At time: 429.10093808174133 and batch: 500, loss is 4.024885311126709 and perplexity is 55.97388883125894
At time: 430.68191957473755 and batch: 550, loss is 4.0522034549713135 and perplexity is 57.52406919837522
At time: 432.26432371139526 and batch: 600, loss is 4.081933150291443 and perplexity is 59.25991751879075
At time: 433.846364736557 and batch: 650, loss is 4.043971710205078 and perplexity is 57.052489368133486
At time: 435.4299545288086 and batch: 700, loss is 4.01459969997406 and perplexity is 55.40111389583159
At time: 437.01226019859314 and batch: 750, loss is 3.999105248451233 and perplexity is 54.549320102428034
At time: 438.5954661369324 and batch: 800, loss is 3.944831233024597 and perplexity is 51.66761758871284
At time: 440.1838674545288 and batch: 850, loss is 3.9772604084014893 and perplexity is 53.37062005923122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.541844367980957 and perplexity of 93.86375988301984
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 444.2685911655426 and batch: 50, loss is 4.094607920646667 and perplexity is 60.01580358638646
At time: 445.8485345840454 and batch: 100, loss is 4.0587679958343506 and perplexity is 57.90293046584437
At time: 447.45360827445984 and batch: 150, loss is 4.0655190372467045 and perplexity is 58.295158029882415
At time: 449.03475999832153 and batch: 200, loss is 4.098053121566773 and perplexity is 60.22292667357942
At time: 450.6167345046997 and batch: 250, loss is 4.080048804283142 and perplexity is 59.148356472578584
At time: 452.19646525382996 and batch: 300, loss is 4.057721157073974 and perplexity is 57.84234714990765
At time: 453.7768449783325 and batch: 350, loss is 4.009340257644653 and perplexity is 55.110499836162134
At time: 455.3576467037201 and batch: 400, loss is 4.012981395721436 and perplexity is 55.3115305437246
At time: 456.93831968307495 and batch: 450, loss is 4.053304901123047 and perplexity is 57.587463769469906
At time: 458.5204846858978 and batch: 500, loss is 4.023721795082093 and perplexity is 55.90880018670827
At time: 460.10202622413635 and batch: 550, loss is 4.047148866653442 and perplexity is 57.23404231095505
At time: 461.6864092350006 and batch: 600, loss is 4.070973925590515 and perplexity is 58.614020496830214
At time: 463.270715713501 and batch: 650, loss is 4.030907955169678 and perplexity is 56.312016829623474
At time: 464.85970973968506 and batch: 700, loss is 3.998949546813965 and perplexity is 54.54082734516148
At time: 466.45557856559753 and batch: 750, loss is 3.9790459442138673 and perplexity is 53.4660003397887
At time: 468.0383040904999 and batch: 800, loss is 3.922161226272583 and perplexity is 50.50948933600679
At time: 469.6229145526886 and batch: 850, loss is 3.9549672746658326 and perplexity is 52.19398585085436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5357669194539385 and perplexity of 93.29503765431053
Finished 16 epochs...
Completing Train Step...
At time: 473.6939480304718 and batch: 50, loss is 4.088265709877014 and perplexity is 59.636375189794734
At time: 475.2767415046692 and batch: 100, loss is 4.049361658096314 and perplexity is 57.36082953513013
At time: 476.8592629432678 and batch: 150, loss is 4.054304513931275 and perplexity is 57.64505771687518
At time: 478.44149231910706 and batch: 200, loss is 4.085279426574707 and perplexity is 59.4585497291096
At time: 480.02419328689575 and batch: 250, loss is 4.067289938926697 and perplexity is 58.39848448666747
At time: 481.6057870388031 and batch: 300, loss is 4.0467563819885255 and perplexity is 57.211583234746605
At time: 483.1880786418915 and batch: 350, loss is 3.9993355083465576 and perplexity is 54.56188206936793
At time: 484.76960158348083 and batch: 400, loss is 4.003942666053772 and perplexity is 54.81383721775134
At time: 486.37676787376404 and batch: 450, loss is 4.046436629295349 and perplexity is 57.1932926013219
At time: 487.9572560787201 and batch: 500, loss is 4.017678904533386 and perplexity is 55.57196817096666
At time: 489.538654088974 and batch: 550, loss is 4.0426722240448 and perplexity is 56.97839859818148
At time: 491.1205015182495 and batch: 600, loss is 4.068735718727112 and perplexity is 58.482976899907584
At time: 492.70203161239624 and batch: 650, loss is 4.030540933609009 and perplexity is 56.29135289760852
At time: 494.28345680236816 and batch: 700, loss is 3.999872260093689 and perplexity is 54.5911761160075
At time: 495.8662679195404 and batch: 750, loss is 3.981603422164917 and perplexity is 53.60291345830388
At time: 497.44817876815796 and batch: 800, loss is 3.926078372001648 and perplexity is 50.7077303824862
At time: 499.028391122818 and batch: 850, loss is 3.9592480754852293 and perplexity is 52.41789682545867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5353132883707685 and perplexity of 93.25272572305421
Finished 17 epochs...
Completing Train Step...
At time: 503.1119785308838 and batch: 50, loss is 4.084283118247986 and perplexity is 59.39934018130024
At time: 504.6960644721985 and batch: 100, loss is 4.044624066352844 and perplexity is 57.08972005283081
At time: 506.2795977592468 and batch: 150, loss is 4.048736033439636 and perplexity is 57.32495440919728
At time: 507.8629937171936 and batch: 200, loss is 4.079750370979309 and perplexity is 59.13070726682658
At time: 509.44797348976135 and batch: 250, loss is 4.061756262779236 and perplexity is 58.07621866571572
At time: 511.03362107276917 and batch: 300, loss is 4.041753311157226 and perplexity is 56.926064462334814
At time: 512.6292757987976 and batch: 350, loss is 3.994629044532776 and perplexity is 54.305691893593796
At time: 514.221654176712 and batch: 400, loss is 3.999728031158447 and perplexity is 54.58330305657782
At time: 515.8074412345886 and batch: 450, loss is 4.043006906509399 and perplexity is 56.99747146055149
At time: 517.3957417011261 and batch: 500, loss is 4.014738001823425 and perplexity is 55.408776502204404
At time: 518.9812164306641 and batch: 550, loss is 4.0403674221038814 and perplexity is 56.84722589611811
At time: 520.5664446353912 and batch: 600, loss is 4.067302951812744 and perplexity is 58.39924442443589
At time: 522.1517419815063 and batch: 650, loss is 4.02989628314972 and perplexity is 56.25507634521343
At time: 523.7366850376129 and batch: 700, loss is 3.9998992252349854 and perplexity is 54.59264819463232
At time: 525.3206648826599 and batch: 750, loss is 3.9822855043411254 and perplexity is 53.639487522007144
At time: 526.9311385154724 and batch: 800, loss is 3.92736177444458 and perplexity is 50.77285058631054
At time: 528.5164232254028 and batch: 850, loss is 3.9607712030410767 and perplexity is 52.497796802014314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535294850667317 and perplexity of 93.2510063728018
Finished 18 epochs...
Completing Train Step...
At time: 532.5606641769409 and batch: 50, loss is 4.080664916038513 and perplexity is 59.1848096987897
At time: 534.1660270690918 and batch: 100, loss is 4.040776476860047 and perplexity is 56.87048428089787
At time: 535.7478597164154 and batch: 150, loss is 4.044524564743042 and perplexity is 57.084039816383424
At time: 537.3416254520416 and batch: 200, loss is 4.075727005004882 and perplexity is 58.89328073835137
At time: 538.9222159385681 and batch: 250, loss is 4.057814979553223 and perplexity is 57.84777431691398
At time: 540.5056321620941 and batch: 300, loss is 4.038112626075745 and perplexity is 56.719191397042025
At time: 542.0896608829498 and batch: 350, loss is 3.9911997175216674 and perplexity is 54.11977887798834
At time: 543.6726801395416 and batch: 400, loss is 3.9966765308380126 and perplexity is 54.41699596193608
At time: 545.2525062561035 and batch: 450, loss is 4.040428738594056 and perplexity is 56.85071167535298
At time: 546.8320960998535 and batch: 500, loss is 4.012498831748962 and perplexity is 55.28484563092841
At time: 548.412534236908 and batch: 550, loss is 4.038487510681152 and perplexity is 56.740458534840144
At time: 549.9933502674103 and batch: 600, loss is 4.065925507545471 and perplexity is 58.31885809654377
At time: 551.5735898017883 and batch: 650, loss is 4.029011516571045 and perplexity is 56.20532574586164
At time: 553.1531555652618 and batch: 700, loss is 3.999430832862854 and perplexity is 54.56708340228776
At time: 554.7324731349945 and batch: 750, loss is 3.982242298126221 and perplexity is 53.63717001284766
At time: 556.313934803009 and batch: 800, loss is 3.927721185684204 and perplexity is 50.79110219919995
At time: 557.9038038253784 and batch: 850, loss is 3.9613634729385376 and perplexity is 52.52889887624516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535404523213704 and perplexity of 93.26123400895902
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 561.974960565567 and batch: 50, loss is 4.08040267944336 and perplexity is 59.169291310643054
At time: 563.5846509933472 and batch: 100, loss is 4.044742908477783 and perplexity is 57.09650511964197
At time: 565.16623878479 and batch: 150, loss is 4.0493193626403805 and perplexity is 57.358403483998075
At time: 566.7738244533539 and batch: 200, loss is 4.079393644332885 and perplexity is 59.10961752978184
At time: 568.3554832935333 and batch: 250, loss is 4.060062208175659 and perplexity is 57.97791766726509
At time: 569.9499313831329 and batch: 300, loss is 4.039037203788757 and perplexity is 56.771656947809774
At time: 571.5315952301025 and batch: 350, loss is 3.992213544845581 and perplexity is 54.17467481137559
At time: 573.1121497154236 and batch: 400, loss is 3.996412181854248 and perplexity is 54.40261278552681
At time: 574.6963591575623 and batch: 450, loss is 4.040028944015503 and perplexity is 56.82798761181967
At time: 576.2772376537323 and batch: 500, loss is 4.012249684333801 and perplexity is 55.271073270287154
At time: 577.8606288433075 and batch: 550, loss is 4.036218724250793 and perplexity is 56.611872474774955
At time: 579.4505488872528 and batch: 600, loss is 4.059989104270935 and perplexity is 57.97367941001429
At time: 581.0342237949371 and batch: 650, loss is 4.021104655265808 and perplexity is 55.76267034418227
At time: 582.6181361675262 and batch: 700, loss is 3.9916514587402343 and perplexity is 54.14423253579399
At time: 584.1992154121399 and batch: 750, loss is 3.9729948329925535 and perplexity is 53.1434485078897
At time: 585.780314207077 and batch: 800, loss is 3.917320809364319 and perplexity is 50.265593105715276
At time: 587.3623692989349 and batch: 850, loss is 3.951790051460266 and perplexity is 52.02841707158306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.5354413986206055 and perplexity of 93.26467311832012
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 591.4001722335815 and batch: 50, loss is 4.078327188491821 and perplexity is 59.04661333447155
At time: 593.0098831653595 and batch: 100, loss is 4.04314507484436 and perplexity is 57.005347250360025
At time: 594.5943675041199 and batch: 150, loss is 4.048542194366455 and perplexity is 57.313843670046936
At time: 596.1866810321808 and batch: 200, loss is 4.07848286151886 and perplexity is 59.05580601501336
At time: 597.7731845378876 and batch: 250, loss is 4.058449859619141 and perplexity is 57.88451237658236
At time: 599.359393119812 and batch: 300, loss is 4.037808709144592 and perplexity is 56.70195609363808
At time: 600.9451067447662 and batch: 350, loss is 3.9906875705718994 and perplexity is 54.09206869476245
At time: 602.5317871570587 and batch: 400, loss is 3.9943102693557737 and perplexity is 54.28838334596243
At time: 604.1189126968384 and batch: 450, loss is 4.038442912101746 and perplexity is 56.73792804742312
At time: 605.7045738697052 and batch: 500, loss is 4.011392331123352 and perplexity is 55.22370674599615
At time: 607.326425075531 and batch: 550, loss is 4.034918766021729 and perplexity is 56.53832721852611
At time: 608.913437128067 and batch: 600, loss is 4.058012318611145 and perplexity is 57.85919106864904
At time: 610.4987330436707 and batch: 650, loss is 4.018491020202637 and perplexity is 55.61711736779307
At time: 612.0862231254578 and batch: 700, loss is 3.989272508621216 and perplexity is 54.01557919797056
At time: 613.675455570221 and batch: 750, loss is 3.9702423334121706 and perplexity is 52.99737231772355
At time: 615.2801094055176 and batch: 800, loss is 3.9144156408309936 and perplexity is 50.119775001957706
At time: 616.869793176651 and batch: 850, loss is 3.9489963817596436 and perplexity is 51.88326970054642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535493850708008 and perplexity of 93.2695651734042
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 620.9499182701111 and batch: 50, loss is 4.077718577384949 and perplexity is 59.010687843208416
At time: 622.5308079719543 and batch: 100, loss is 4.042600846290588 and perplexity is 56.9743317531938
At time: 624.1154353618622 and batch: 150, loss is 4.048148460388184 and perplexity is 57.291281704366455
At time: 625.6974329948425 and batch: 200, loss is 4.078047914505005 and perplexity is 59.03012545378289
At time: 627.2787098884583 and batch: 250, loss is 4.057991452217102 and perplexity is 57.85798376856524
At time: 628.8658180236816 and batch: 300, loss is 4.037470979690552 and perplexity is 56.682809406345676
At time: 630.4529709815979 and batch: 350, loss is 3.9902448272705078 and perplexity is 54.06812509451487
At time: 632.0376417636871 and batch: 400, loss is 3.993603477478027 and perplexity is 54.25002631437349
At time: 633.6202948093414 and batch: 450, loss is 4.037895503044129 and perplexity is 56.70687769109804
At time: 635.201329946518 and batch: 500, loss is 4.011147861480713 and perplexity is 55.210207876241924
At time: 636.7835488319397 and batch: 550, loss is 4.034540472030639 and perplexity is 56.516943154059646
At time: 638.3798248767853 and batch: 600, loss is 4.057426152229309 and perplexity is 57.825285893971916
At time: 639.9668533802032 and batch: 650, loss is 4.01767427444458 and perplexity is 55.57171086841459
At time: 641.552259683609 and batch: 700, loss is 3.988522310256958 and perplexity is 53.97507199493031
At time: 643.134437084198 and batch: 750, loss is 3.9694425868988037 and perplexity is 52.95500479789114
At time: 644.7162263393402 and batch: 800, loss is 3.913579378128052 and perplexity is 50.07787922382367
At time: 646.3221642971039 and batch: 850, loss is 3.948231301307678 and perplexity is 51.84359000612928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.535451571146647 and perplexity of 93.26562186046175
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 650.3790559768677 and batch: 50, loss is 4.077532172203064 and perplexity is 58.99968897036315
At time: 651.9604473114014 and batch: 100, loss is 4.042403860092163 and perplexity is 56.96310970150487
At time: 653.5419647693634 and batch: 150, loss is 4.04796600818634 and perplexity is 57.28082973739424
At time: 655.1243488788605 and batch: 200, loss is 4.077929353713989 and perplexity is 59.023127210282155
At time: 656.7060713768005 and batch: 250, loss is 4.057842545509338 and perplexity is 57.849368968101096
At time: 658.2875516414642 and batch: 300, loss is 4.037360906600952 and perplexity is 56.6765704977614
At time: 659.8696582317352 and batch: 350, loss is 3.9901420497894287 and perplexity is 54.062568394367695
At time: 661.4509904384613 and batch: 400, loss is 3.99340473651886 and perplexity is 54.23924568342088
At time: 663.0344948768616 and batch: 450, loss is 4.0377185344696045 and perplexity is 56.696843243704436
At time: 664.6140084266663 and batch: 500, loss is 4.011042079925537 and perplexity is 55.20436796347417
At time: 666.1941986083984 and batch: 550, loss is 4.034420680999756 and perplexity is 56.510173336666654
At time: 667.7741990089417 and batch: 600, loss is 4.057257585525512 and perplexity is 57.81553929763158
At time: 669.3552646636963 and batch: 650, loss is 4.0174523448944095 and perplexity is 55.55937923204721
At time: 670.935263633728 and batch: 700, loss is 3.9883046913146973 and perplexity is 53.96332727483746
At time: 672.5163555145264 and batch: 750, loss is 3.9692178535461426 and perplexity is 52.94310537927094
At time: 674.0979528427124 and batch: 800, loss is 3.9133130359649657 and perplexity is 50.06454314920675
At time: 675.6790745258331 and batch: 850, loss is 3.9480065441131593 and perplexity is 51.83193909564774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 97 batches
Done Evaluating: Achieved loss of 4.53544553120931 and perplexity of 93.26505854365114
Annealing...
Model not improving. Stopping early with 93.2510063728018loss at 22 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f079a7ddba8>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 11.995311358945631, 'anneal': 5.454218864592506, 'dropout': 0.10651906940627132, 'tune_wordvecs': True}, 'best_accuracy': -114.21426593274742}, {'params': {'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 3.5659892080942015, 'anneal': 6.055312748419992, 'dropout': 0.6749421451680976, 'tune_wordvecs': True}, 'best_accuracy': -92.6020327771543}, {'params': {'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 1.3304114881382434, 'anneal': 5.461146488664952, 'dropout': 0.20786476639081441, 'tune_wordvecs': True}, 'best_accuracy': -95.776782507345}, {'params': {'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 7.8107127518982145, 'anneal': 5.76722681168048, 'dropout': 0.6158263652314355, 'tune_wordvecs': True}, 'best_accuracy': -99.1068025203184}, {'params': {'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 13.533260850088455, 'anneal': 4.875188484560672, 'dropout': 0.375251703046402, 'tune_wordvecs': True}, 'best_accuracy': -128.76601763391173}, {'params': {'wordvec_dim': 200, 'data': 'wikitext', 'seq_len': 50, 'num_layers': 1, 'batch_size': 50, 'wordvec_source': '', 'lr': 4.033060124244714, 'anneal': 3.5785500742123912, 'dropout': 0.0, 'tune_wordvecs': True}, 'best_accuracy': -93.2510063728018}]
