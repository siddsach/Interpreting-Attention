Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 2.505901894233711, 'data': 'wikitext', 'dropout': 0.9342705742519514, 'tune_wordvecs': True, 'lr': 26.084625863899376, 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.9514217376708984 and batch: 50, loss is 8.69259165763855 and perplexity is 5958.604912874769
At time: 3.1514687538146973 and batch: 100, loss is 7.8524391555786135 and perplexity is 2572.0001806086843
At time: 4.351780414581299 and batch: 150, loss is 7.612216444015503 and perplexity is 2022.7564593136224
At time: 5.5803351402282715 and batch: 200, loss is 7.484170913696289 and perplexity is 1779.64807612411
At time: 6.7795729637146 and batch: 250, loss is 7.462354564666748 and perplexity is 1741.243103919677
At time: 7.9799628257751465 and batch: 300, loss is 7.38446608543396 and perplexity is 1610.7675531071884
At time: 9.181807041168213 and batch: 350, loss is 7.344678945541382 and perplexity is 1547.9379131194842
At time: 10.382798433303833 and batch: 400, loss is 7.269206705093384 and perplexity is 1435.4112967822796
At time: 11.588303327560425 and batch: 450, loss is 7.229720535278321 and perplexity is 1379.836834482127
At time: 12.796783685684204 and batch: 500, loss is 7.1948326969146725 and perplexity is 1332.5273709966127
At time: 14.004894018173218 and batch: 550, loss is 7.176095628738404 and perplexity is 1307.7921710241978
At time: 15.213369369506836 and batch: 600, loss is 7.160164155960083 and perplexity is 1287.1222043458713
At time: 16.4223051071167 and batch: 650, loss is 7.126685495376587 and perplexity is 1244.7444109036815
At time: 17.630242347717285 and batch: 700, loss is 7.104790420532226 and perplexity is 1217.786834871033
At time: 18.839085340499878 and batch: 750, loss is 7.036511964797974 and perplexity is 1137.4133416952557
At time: 20.049432039260864 and batch: 800, loss is 7.035336170196533 and perplexity is 1136.0767631533852
At time: 21.257453680038452 and batch: 850, loss is 7.061205835342407 and perplexity is 1165.8501418226358
At time: 22.466676950454712 and batch: 900, loss is 7.050428123474121 and perplexity is 1153.3524143421698
At time: 23.676265001296997 and batch: 950, loss is 7.015839767456055 and perplexity is 1114.1418735566635
At time: 24.893651962280273 and batch: 1000, loss is 6.997666301727295 and perplexity is 1094.076931412262
At time: 26.103538751602173 and batch: 1050, loss is 6.970879926681518 and perplexity is 1065.1596006506256
At time: 27.313560962677002 and batch: 1100, loss is 6.955484685897827 and perplexity is 1048.8867954078755
At time: 28.523861408233643 and batch: 1150, loss is 6.993705453872681 and perplexity is 1089.7520299382572
At time: 29.7351233959198 and batch: 1200, loss is 6.995510654449463 and perplexity is 1091.7210276144156
At time: 30.946229934692383 and batch: 1250, loss is 6.974675798416138 and perplexity is 1069.2104933421408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.3863650134010035 and perplexity of 593.6945808049736
Finished 1 epochs...
Completing Train Step...
At time: 33.891719579696655 and batch: 50, loss is 6.626865482330322 and perplexity is 755.1115467157465
At time: 35.124295473098755 and batch: 100, loss is 6.505176639556884 and perplexity is 668.5937590298225
At time: 36.33146572113037 and batch: 150, loss is 6.232121601104736 and perplexity is 508.83388165136694
At time: 37.54266142845154 and batch: 200, loss is 6.17382287979126 and perplexity is 480.01765275147096
At time: 38.75071930885315 and batch: 250, loss is 6.135812940597535 and perplexity is 462.11461309057074
At time: 39.996376514434814 and batch: 300, loss is 6.100900382995605 and perplexity is 446.2593936175258
At time: 41.20498251914978 and batch: 350, loss is 6.1282783889770505 and perplexity is 458.6458708024462
At time: 42.413432598114014 and batch: 400, loss is 6.011877117156982 and perplexity is 408.2489325527364
At time: 43.621037006378174 and batch: 450, loss is 5.962762203216553 and perplexity is 388.6822622195321
At time: 44.829609632492065 and batch: 500, loss is 5.937262773513794 and perplexity is 378.896383602131
At time: 46.03868389129639 and batch: 550, loss is 5.937868204116821 and perplexity is 379.1258485236738
At time: 47.246089458465576 and batch: 600, loss is 5.941032342910766 and perplexity is 380.3273551932729
At time: 48.454286336898804 and batch: 650, loss is 5.930561122894287 and perplexity is 376.365641963112
At time: 49.661020278930664 and batch: 700, loss is 5.910583763122559 and perplexity is 368.92145530190004
At time: 50.86836552619934 and batch: 750, loss is 5.892774496078491 and perplexity is 362.4093942218209
At time: 52.07565116882324 and batch: 800, loss is 5.888385438919068 and perplexity is 360.8222442692809
At time: 53.28311634063721 and batch: 850, loss is 5.942691659927368 and perplexity is 380.9589627192632
At time: 54.49155068397522 and batch: 900, loss is 5.920117998123169 and perplexity is 372.4556603522157
At time: 55.69920086860657 and batch: 950, loss is 5.908882579803467 and perplexity is 368.2943858073968
At time: 56.90728044509888 and batch: 1000, loss is 5.9124432563781735 and perplexity is 369.60810046806125
At time: 58.11437654495239 and batch: 1050, loss is 5.877585067749023 and perplexity is 356.94619914544376
At time: 59.321767807006836 and batch: 1100, loss is 5.8917460441589355 and perplexity is 362.0368651816074
At time: 60.529130935668945 and batch: 1150, loss is 5.891806488037109 and perplexity is 362.0587487551382
At time: 61.73876404762268 and batch: 1200, loss is 5.90349326133728 and perplexity is 366.31486898746044
At time: 62.950820207595825 and batch: 1250, loss is 5.882122735977173 and perplexity is 358.5695829755653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.784142709996578 and perplexity of 325.1032128773281
Finished 2 epochs...
Completing Train Step...
At time: 65.96140313148499 and batch: 50, loss is 5.905402383804321 and perplexity is 367.0148769216802
At time: 67.2061858177185 and batch: 100, loss is 5.8957080078125 and perplexity is 363.47408731360736
At time: 68.41470074653625 and batch: 150, loss is 5.799042072296142 and perplexity is 329.983308307908
At time: 69.62308287620544 and batch: 200, loss is 5.866930465698243 and perplexity is 353.1632680394029
At time: 70.8292670249939 and batch: 250, loss is 5.866011438369751 and perplexity is 352.83885044180806
At time: 72.03452563285828 and batch: 300, loss is 5.898183650970459 and perplexity is 364.37503420231695
At time: 73.23982810974121 and batch: 350, loss is 5.901909008026123 and perplexity is 365.7349928999307
At time: 74.44527292251587 and batch: 400, loss is 5.864299793243408 and perplexity is 352.2354321094098
At time: 75.65099930763245 and batch: 450, loss is 5.8570299816131595 and perplexity is 349.6840322036443
At time: 76.86589193344116 and batch: 500, loss is 5.822160015106201 and perplexity is 337.700705021983
At time: 78.07494044303894 and batch: 550, loss is 5.8397770118713375 and perplexity is 343.7026904994749
At time: 79.27933979034424 and batch: 600, loss is 5.882926549911499 and perplexity is 358.8579220727534
At time: 80.48528337478638 and batch: 650, loss is 5.866848411560059 and perplexity is 353.1342907206766
At time: 81.68984818458557 and batch: 700, loss is 5.873223028182983 and perplexity is 355.39257664667656
At time: 82.89539408683777 and batch: 750, loss is 5.843310194015503 and perplexity is 344.91920252221325
At time: 84.10640668869019 and batch: 800, loss is 5.868690309524536 and perplexity is 353.7853274394872
At time: 85.31802272796631 and batch: 850, loss is 5.903568601608276 and perplexity is 366.34246828861626
At time: 86.52382206916809 and batch: 900, loss is 5.865944614410401 and perplexity is 352.8152731405821
At time: 87.72958827018738 and batch: 950, loss is 5.88013617515564 and perplexity is 357.85796975573095
At time: 88.9353597164154 and batch: 1000, loss is 5.856538734436035 and perplexity is 349.51229309656827
At time: 90.14185786247253 and batch: 1050, loss is 5.847729206085205 and perplexity is 346.4467773411818
At time: 91.34799265861511 and batch: 1100, loss is 5.836431570053101 and perplexity is 342.5547743595877
At time: 92.55418539047241 and batch: 1150, loss is 5.853130435943603 and perplexity is 348.3230786250748
At time: 93.76013827323914 and batch: 1200, loss is 5.8652518939971925 and perplexity is 352.5709554304885
At time: 94.96654677391052 and batch: 1250, loss is 5.853119049072266 and perplexity is 348.3191123375765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.660407073306342 and perplexity of 287.2655568981305
Finished 3 epochs...
Completing Train Step...
At time: 97.94366240501404 and batch: 50, loss is 5.818696403503418 and perplexity is 336.533064236198
At time: 99.1594569683075 and batch: 100, loss is 5.863000049591064 and perplexity is 351.77791373510496
At time: 100.3667802810669 and batch: 150, loss is 5.78638635635376 and perplexity is 325.833448405419
At time: 101.57226729393005 and batch: 200, loss is 5.8556592082977295 and perplexity is 349.2050230449595
At time: 102.77954983711243 and batch: 250, loss is 5.820466861724854 and perplexity is 337.1294097131646
At time: 103.98815631866455 and batch: 300, loss is 5.7828023147583005 and perplexity is 324.6677379982657
At time: 105.19482064247131 and batch: 350, loss is 5.812034740447998 and perplexity is 334.29864509218186
At time: 106.40399646759033 and batch: 400, loss is 5.79188235282898 and perplexity is 327.6291579760044
At time: 107.61322402954102 and batch: 450, loss is 5.815135011672973 and perplexity is 335.33666980976074
At time: 108.82054662704468 and batch: 500, loss is 5.826119232177734 and perplexity is 339.04038571170133
At time: 110.02818393707275 and batch: 550, loss is 5.813978862762451 and perplexity is 334.94919471691617
At time: 111.23627996444702 and batch: 600, loss is 5.796107940673828 and perplexity is 329.0165128942689
At time: 112.44297766685486 and batch: 650, loss is 5.826019840240479 and perplexity is 339.0066895055509
At time: 113.64928030967712 and batch: 700, loss is 5.856017026901245 and perplexity is 349.3299974563974
At time: 114.85542583465576 and batch: 750, loss is 5.811888418197632 and perplexity is 334.24973334066374
At time: 116.06190323829651 and batch: 800, loss is 5.802078475952149 and perplexity is 330.98679355381995
At time: 117.27636098861694 and batch: 850, loss is 5.813695955276489 and perplexity is 334.85444848515476
At time: 118.48585891723633 and batch: 900, loss is 5.808688812255859 and perplexity is 333.1819750216911
At time: 119.69966793060303 and batch: 950, loss is 5.782211418151856 and perplexity is 324.4759496027968
At time: 120.91019296646118 and batch: 1000, loss is 5.785533113479614 and perplexity is 325.5555519108702
At time: 122.11887335777283 and batch: 1050, loss is 5.839103546142578 and perplexity is 343.47129644327026
At time: 123.32786059379578 and batch: 1100, loss is 5.790451831817627 and perplexity is 327.16081265037064
At time: 124.53815412521362 and batch: 1150, loss is 5.878838710784912 and perplexity is 357.3939628714849
At time: 125.74800205230713 and batch: 1200, loss is 5.834401416778564 and perplexity is 341.8600411088187
At time: 126.95703196525574 and batch: 1250, loss is 5.862360963821411 and perplexity is 351.5531692994933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.744152208314325 and perplexity of 312.35870043512386
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 129.9548783302307 and batch: 50, loss is 5.765909194946289 and perplexity is 319.22915366637955
At time: 131.18725538253784 and batch: 100, loss is 5.719696063995361 and perplexity is 304.8122654545191
At time: 132.3955216407776 and batch: 150, loss is 5.639666814804077 and perplexity is 281.36895488546725
At time: 133.60353469848633 and batch: 200, loss is 5.704205961227417 and perplexity is 300.12707287629394
At time: 134.81250619888306 and batch: 250, loss is 5.70802134513855 and perplexity is 301.2743601603948
At time: 136.0212836265564 and batch: 300, loss is 5.698322982788086 and perplexity is 298.3666152220822
At time: 137.22948932647705 and batch: 350, loss is 5.707140893936157 and perplexity is 301.00921952656427
At time: 138.43821740150452 and batch: 400, loss is 5.657677040100098 and perplexity is 286.4823819220562
At time: 139.64717149734497 and batch: 450, loss is 5.635878992080689 and perplexity is 280.3051951031005
At time: 140.85244464874268 and batch: 500, loss is 5.621887292861938 and perplexity is 276.41055894935204
At time: 142.0558168888092 and batch: 550, loss is 5.64640209197998 and perplexity is 283.2704491424797
At time: 143.26097655296326 and batch: 600, loss is 5.664821844100953 and perplexity is 288.5365720449358
At time: 144.4652693271637 and batch: 650, loss is 5.6337834739685055 and perplexity is 279.7184254977092
At time: 145.67737483978271 and batch: 700, loss is 5.6570741653442385 and perplexity is 286.3097209776888
At time: 146.8822317123413 and batch: 750, loss is 5.605619869232178 and perplexity is 271.9504469129916
At time: 148.08719635009766 and batch: 800, loss is 5.626844072341919 and perplexity is 277.7840664105852
At time: 149.29244136810303 and batch: 850, loss is 5.635005693435669 and perplexity is 280.0605118123689
At time: 150.4965798854828 and batch: 900, loss is 5.629076948165894 and perplexity is 278.40501673124703
At time: 151.702330827713 and batch: 950, loss is 5.62254301071167 and perplexity is 276.59186572325706
At time: 152.90826964378357 and batch: 1000, loss is 5.5945455265045165 and perplexity is 268.9553892163408
At time: 154.11403608322144 and batch: 1050, loss is 5.59040488243103 and perplexity is 267.8440431101132
At time: 155.31949758529663 and batch: 1100, loss is 5.566004085540771 and perplexity is 261.3875274268307
At time: 156.524662733078 and batch: 1150, loss is 5.612731943130493 and perplexity is 273.89147276609447
At time: 157.73083114624023 and batch: 1200, loss is 5.597884149551391 and perplexity is 269.8548304895806
At time: 158.98094272613525 and batch: 1250, loss is 5.583891668319702 and perplexity is 266.10518642896767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.461175403455748 and perplexity of 235.37392115608324
Finished 5 epochs...
Completing Train Step...
At time: 161.9521999359131 and batch: 50, loss is 5.5912056827545165 and perplexity is 268.05861861107803
At time: 163.1570258140564 and batch: 100, loss is 5.595633296966553 and perplexity is 269.2481101220499
At time: 164.36292815208435 and batch: 150, loss is 5.52955325126648 and perplexity is 252.03129120885097
At time: 165.56800723075867 and batch: 200, loss is 5.596648778915405 and perplexity is 269.5216655894534
At time: 166.77387046813965 and batch: 250, loss is 5.586225862503052 and perplexity is 266.72705310372544
At time: 167.98259472846985 and batch: 300, loss is 5.583922681808471 and perplexity is 266.11343940715454
At time: 169.19102311134338 and batch: 350, loss is 5.601185293197632 and perplexity is 270.7471320461406
At time: 170.40412735939026 and batch: 400, loss is 5.572071943283081 and perplexity is 262.9784115067982
At time: 171.61207127571106 and batch: 450, loss is 5.551570777893066 and perplexity is 257.6419364847494
At time: 172.8201892375946 and batch: 500, loss is 5.53289490699768 and perplexity is 252.87490176055235
At time: 174.02787566184998 and batch: 550, loss is 5.547873792648315 and perplexity is 256.691196565731
At time: 175.23554158210754 and batch: 600, loss is 5.567596549987793 and perplexity is 261.8041093790913
At time: 176.44273853302002 and batch: 650, loss is 5.540885496139526 and perplexity is 254.90361572031964
At time: 177.647873878479 and batch: 700, loss is 5.587981681823731 and perplexity is 267.1957890042658
At time: 178.85255551338196 and batch: 750, loss is 5.541151685714722 and perplexity is 254.9714774371441
At time: 180.05735516548157 and batch: 800, loss is 5.574418077468872 and perplexity is 263.5961184765391
At time: 181.26341557502747 and batch: 850, loss is 5.588364715576172 and perplexity is 267.29815361326314
At time: 182.46957063674927 and batch: 900, loss is 5.5586469364166256 and perplexity is 259.4715172371228
At time: 183.67460656166077 and batch: 950, loss is 5.554913969039917 and perplexity is 258.50472415430244
At time: 184.8827018737793 and batch: 1000, loss is 5.532216138839722 and perplexity is 252.70331656916042
At time: 186.08818912506104 and batch: 1050, loss is 5.531664943695068 and perplexity is 252.56406610864394
At time: 187.29399704933167 and batch: 1100, loss is 5.514087247848511 and perplexity is 248.16336219875333
At time: 188.50107431411743 and batch: 1150, loss is 5.54260633468628 and perplexity is 255.3426413257104
At time: 189.73210501670837 and batch: 1200, loss is 5.538080902099609 and perplexity is 254.18971612613672
At time: 190.94512581825256 and batch: 1250, loss is 5.565509700775147 and perplexity is 261.2583333537688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.480981979927007 and perplexity of 240.0823477045573
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 193.94949340820312 and batch: 50, loss is 5.532799205780029 and perplexity is 252.85070248250915
At time: 195.18259167671204 and batch: 100, loss is 5.5003022289276124 and perplexity is 244.76589642099492
At time: 196.39112544059753 and batch: 150, loss is 5.417277431488037 and perplexity is 225.2649875072489
At time: 197.6081416606903 and batch: 200, loss is 5.457895193099976 and perplexity is 234.6031100848039
At time: 198.81801319122314 and batch: 250, loss is 5.45904619216919 and perplexity is 234.87329350675157
At time: 200.02943921089172 and batch: 300, loss is 5.447936525344849 and perplexity is 232.27837052458293
At time: 201.23676300048828 and batch: 350, loss is 5.4687936496734615 and perplexity is 237.17390529629688
At time: 202.44481921195984 and batch: 400, loss is 5.426861238479614 and perplexity is 227.4342620197514
At time: 203.65088272094727 and batch: 450, loss is 5.4138008403778075 and perplexity is 224.4831930315393
At time: 204.85940599441528 and batch: 500, loss is 5.383150091171265 and perplexity is 217.70699327434875
At time: 206.0710165500641 and batch: 550, loss is 5.380335350036621 and perplexity is 217.0950660574472
At time: 207.28097438812256 and batch: 600, loss is 5.408018503189087 and perplexity is 223.18890113856764
At time: 208.4895806312561 and batch: 650, loss is 5.388928279876709 and perplexity is 218.96858671635636
At time: 209.69992184638977 and batch: 700, loss is 5.410743131637573 and perplexity is 223.79783715348276
At time: 210.9082293510437 and batch: 750, loss is 5.377225160598755 and perplexity is 216.4209081987513
At time: 212.11594772338867 and batch: 800, loss is 5.399806547164917 and perplexity is 221.36358864795642
At time: 213.32297587394714 and batch: 850, loss is 5.406040859222412 and perplexity is 222.74794912156656
At time: 214.53033137321472 and batch: 900, loss is 5.391047801971435 and perplexity is 219.43318766601155
At time: 215.73811507225037 and batch: 950, loss is 5.387649812698364 and perplexity is 218.68882143861936
At time: 216.94674682617188 and batch: 1000, loss is 5.363248567581177 and perplexity is 213.4171214942746
At time: 218.1537094116211 and batch: 1050, loss is 5.351517744064331 and perplexity is 210.92819005648946
At time: 219.4073987007141 and batch: 1100, loss is 5.315763339996338 and perplexity is 203.51980865965905
At time: 220.61718225479126 and batch: 1150, loss is 5.343931188583374 and perplexity is 209.33402639178166
At time: 221.82740330696106 and batch: 1200, loss is 5.344154510498047 and perplexity is 209.38078048777444
At time: 223.03561687469482 and batch: 1250, loss is 5.391114587783814 and perplexity is 219.44784317909728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.316355907133896 and perplexity of 203.64044354871828
Finished 7 epochs...
Completing Train Step...
At time: 225.99164867401123 and batch: 50, loss is 5.419345140457153 and perplexity is 225.73125182567918
At time: 227.2341673374176 and batch: 100, loss is 5.431540584564209 and perplexity is 228.50099951372945
At time: 228.44107699394226 and batch: 150, loss is 5.369061594009399 and perplexity is 214.6613336772825
At time: 229.65323948860168 and batch: 200, loss is 5.401979570388794 and perplexity is 221.84513988853223
At time: 230.86449360847473 and batch: 250, loss is 5.40297905921936 and perplexity is 222.06698247409147
At time: 232.07327938079834 and batch: 300, loss is 5.391872234344483 and perplexity is 219.61417008327527
At time: 233.28792428970337 and batch: 350, loss is 5.419474964141846 and perplexity is 225.76055899088232
At time: 234.4969663619995 and batch: 400, loss is 5.375808515548706 and perplexity is 216.114533653654
At time: 235.70501923561096 and batch: 450, loss is 5.360001707077027 and perplexity is 212.7253095877258
At time: 236.92061924934387 and batch: 500, loss is 5.342046241760254 and perplexity is 208.93981453467777
At time: 238.13071131706238 and batch: 550, loss is 5.334762039184571 and perplexity is 207.4233842970961
At time: 239.3411054611206 and batch: 600, loss is 5.366010093688965 and perplexity is 214.0072929590288
At time: 240.55147290229797 and batch: 650, loss is 5.352662649154663 and perplexity is 211.16982111089732
At time: 241.75535440444946 and batch: 700, loss is 5.383129587173462 and perplexity is 217.70252945640024
At time: 242.95981621742249 and batch: 750, loss is 5.346744985580444 and perplexity is 209.92387932064966
At time: 244.16382956504822 and batch: 800, loss is 5.368476486206054 and perplexity is 214.5357703934823
At time: 245.37747359275818 and batch: 850, loss is 5.3761475563049315 and perplexity is 216.18781771101436
At time: 246.58203172683716 and batch: 900, loss is 5.359381475448608 and perplexity is 212.59341153045614
At time: 247.79699277877808 and batch: 950, loss is 5.3609060859680175 and perplexity is 212.91778088766827
At time: 249.00394916534424 and batch: 1000, loss is 5.333961000442505 and perplexity is 207.25729666046055
At time: 250.23871684074402 and batch: 1050, loss is 5.327645921707154 and perplexity is 205.95257456101396
At time: 251.448472738266 and batch: 1100, loss is 5.294944801330566 and perplexity is 199.32662309852398
At time: 252.65743112564087 and batch: 1150, loss is 5.326500692367554 and perplexity is 205.71684663709195
At time: 253.86534786224365 and batch: 1200, loss is 5.331153020858765 and perplexity is 206.67613872452333
At time: 255.0742073059082 and batch: 1250, loss is 5.3687080383300785 and perplexity is 214.58545235855635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3081829878535585 and perplexity of 201.98288939610143
Finished 8 epochs...
Completing Train Step...
At time: 258.07751727104187 and batch: 50, loss is 5.375007905960083 and perplexity is 215.94157952937354
At time: 259.28596091270447 and batch: 100, loss is 5.388382358551025 and perplexity is 218.8490797188903
At time: 260.494176864624 and batch: 150, loss is 5.328953914642334 and perplexity is 206.22213532687917
At time: 261.7017307281494 and batch: 200, loss is 5.364837875366211 and perplexity is 213.75657666484517
At time: 262.9094965457916 and batch: 250, loss is 5.36935510635376 and perplexity is 214.72434867596132
At time: 264.12006998062134 and batch: 300, loss is 5.360426731109619 and perplexity is 212.81574217329106
At time: 265.330482006073 and batch: 350, loss is 5.378442449569702 and perplexity is 216.68451539389875
At time: 266.5390477180481 and batch: 400, loss is 5.342202844619751 and perplexity is 208.97253766929805
At time: 267.74809217453003 and batch: 450, loss is 5.326391553878784 and perplexity is 205.69439623645914
At time: 268.9566557407379 and batch: 500, loss is 5.308259630203247 and perplexity is 201.99837043258566
At time: 270.16544699668884 and batch: 550, loss is 5.305149812698364 and perplexity is 201.37116811210498
At time: 271.3855061531067 and batch: 600, loss is 5.336645135879516 and perplexity is 207.8143505846501
At time: 272.5996673107147 and batch: 650, loss is 5.324701805114746 and perplexity is 205.34711787387155
At time: 273.80716276168823 and batch: 700, loss is 5.359117813110352 and perplexity is 212.53736604334085
At time: 275.01515555381775 and batch: 750, loss is 5.324362878799438 and perplexity is 205.27753212473849
At time: 276.2304308414459 and batch: 800, loss is 5.343888597488403 and perplexity is 209.3251108162464
At time: 277.4392681121826 and batch: 850, loss is 5.355512981414795 and perplexity is 211.77258389338792
At time: 278.64684295654297 and batch: 900, loss is 5.337771415710449 and perplexity is 208.04853955269184
At time: 279.8551802635193 and batch: 950, loss is 5.342161655426025 and perplexity is 208.96393043622427
At time: 281.1280941963196 and batch: 1000, loss is 5.315831861495972 and perplexity is 203.5337546199471
At time: 282.33907747268677 and batch: 1050, loss is 5.3098037147521975 and perplexity is 202.310513921235
At time: 283.54848432540894 and batch: 1100, loss is 5.275605764389038 and perplexity is 195.50887296173522
At time: 284.7573297023773 and batch: 1150, loss is 5.311156272888184 and perplexity is 202.5843357910892
At time: 285.9654269218445 and batch: 1200, loss is 5.317931499481201 and perplexity is 203.96155077378992
At time: 287.17200446128845 and batch: 1250, loss is 5.353130559921265 and perplexity is 211.2686528641941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.300681900804061 and perplexity of 200.47346639436233
Finished 9 epochs...
Completing Train Step...
At time: 290.1227071285248 and batch: 50, loss is 5.34854721069336 and perplexity is 210.30255053064994
At time: 291.3512682914734 and batch: 100, loss is 5.3607454586029055 and perplexity is 212.88358321215358
At time: 292.55714106559753 and batch: 150, loss is 5.3008131980896 and perplexity is 200.49978974437687
At time: 293.762850522995 and batch: 200, loss is 5.337551574707032 and perplexity is 208.00280698012853
At time: 294.96882247924805 and batch: 250, loss is 5.344355669021606 and perplexity is 209.42290345299375
At time: 296.1752061843872 and batch: 300, loss is 5.335255451202393 and perplexity is 207.52575474101238
At time: 297.3806486129761 and batch: 350, loss is 5.35242377281189 and perplexity is 211.11938366072297
At time: 298.5871753692627 and batch: 400, loss is 5.319463567733765 and perplexity is 204.27427328542066
At time: 299.7946798801422 and batch: 450, loss is 5.300670394897461 and perplexity is 200.4711597786521
At time: 301.00144505500793 and batch: 500, loss is 5.287048397064209 and perplexity is 197.7588575032162
At time: 302.2073950767517 and batch: 550, loss is 5.28033745765686 and perplexity is 196.4361530521423
At time: 303.4131622314453 and batch: 600, loss is 5.311882705688476 and perplexity is 202.73155316272317
At time: 304.6201651096344 and batch: 650, loss is 5.301475629806519 and perplexity is 200.6326511652903
At time: 305.8257944583893 and batch: 700, loss is 5.337957029342651 and perplexity is 208.08715978190216
At time: 307.0315914154053 and batch: 750, loss is 5.310552854537963 and perplexity is 202.462129559855
At time: 308.2368516921997 and batch: 800, loss is 5.322652187347412 and perplexity is 204.92666580281497
At time: 309.4427447319031 and batch: 850, loss is 5.3335373878479 and perplexity is 207.16951845256125
At time: 310.64881563186646 and batch: 900, loss is 5.316156463623047 and perplexity is 203.59983283361254
At time: 311.88119864463806 and batch: 950, loss is 5.32078950881958 and perplexity is 204.54530858585574
At time: 313.0860245227814 and batch: 1000, loss is 5.296184501647949 and perplexity is 199.57388160800167
At time: 314.2919936180115 and batch: 1050, loss is 5.293318185806275 and perplexity is 199.00265887303732
At time: 315.4986038208008 and batch: 1100, loss is 5.256550931930542 and perplexity is 191.81875310322067
At time: 316.70570969581604 and batch: 1150, loss is 5.294651908874512 and perplexity is 199.26825038320882
At time: 317.91144585609436 and batch: 1200, loss is 5.3020978450775145 and perplexity is 200.75752671041244
At time: 319.11708903312683 and batch: 1250, loss is 5.332176294326782 and perplexity is 206.88773317478265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.309735597485173 and perplexity of 202.2967335512823
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 322.099981546402 and batch: 50, loss is 5.32204086303711 and perplexity is 204.80142743471035
At time: 323.3069677352905 and batch: 100, loss is 5.326367797851563 and perplexity is 205.68950981282418
At time: 324.51834535598755 and batch: 150, loss is 5.256338024139405 and perplexity is 191.77791774343606
At time: 325.7264950275421 and batch: 200, loss is 5.290447435379028 and perplexity is 198.4321911320885
At time: 326.93627071380615 and batch: 250, loss is 5.295786161422729 and perplexity is 199.49439913463803
At time: 328.145800113678 and batch: 300, loss is 5.279899864196778 and perplexity is 196.35021268108642
At time: 329.3529827594757 and batch: 350, loss is 5.293538303375244 and perplexity is 199.0464676758936
At time: 330.5594766139984 and batch: 400, loss is 5.2524583053588865 and perplexity is 191.03531483099593
At time: 331.76605439186096 and batch: 450, loss is 5.234671926498413 and perplexity is 187.66752750067647
At time: 332.9735562801361 and batch: 500, loss is 5.21386908531189 and perplexity is 183.80383693678635
At time: 334.17935729026794 and batch: 550, loss is 5.213124828338623 and perplexity is 183.66709054297377
At time: 335.38584184646606 and batch: 600, loss is 5.234697103500366 and perplexity is 187.67225246586295
At time: 336.5929548740387 and batch: 650, loss is 5.231867847442627 and perplexity is 187.1420300299125
At time: 337.7992362976074 and batch: 700, loss is 5.263418912887573 and perplexity is 193.1406949863379
At time: 339.0064263343811 and batch: 750, loss is 5.23234037399292 and perplexity is 187.2304805037303
At time: 340.2208659648895 and batch: 800, loss is 5.237065105438233 and perplexity is 188.117187318771
At time: 341.4659357070923 and batch: 850, loss is 5.2512853145599365 and perplexity is 190.81136353650663
At time: 342.67351269721985 and batch: 900, loss is 5.234889640808105 and perplexity is 187.70838985487617
At time: 343.8805687427521 and batch: 950, loss is 5.226253452301026 and perplexity is 186.09428470605258
At time: 345.0881905555725 and batch: 1000, loss is 5.199473056793213 and perplexity is 181.17674686115015
At time: 346.29447317123413 and batch: 1050, loss is 5.195662326812744 and perplexity is 180.4876450245453
At time: 347.5016598701477 and batch: 1100, loss is 5.151443881988525 and perplexity is 172.68064086868884
At time: 348.7085635662079 and batch: 1150, loss is 5.190553970336914 and perplexity is 179.56800072958737
At time: 349.9152846336365 and batch: 1200, loss is 5.20063289642334 and perplexity is 181.38700474135774
At time: 351.1215190887451 and batch: 1250, loss is 5.252878751754761 and perplexity is 191.1156518281183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.230502163406706 and perplexity of 186.8866275862667
Finished 11 epochs...
Completing Train Step...
At time: 354.0687665939331 and batch: 50, loss is 5.2691674900054934 and perplexity is 194.25417656708987
At time: 355.30071210861206 and batch: 100, loss is 5.283685388565064 and perplexity is 197.0949098412278
At time: 356.5071666240692 and batch: 150, loss is 5.219056482315064 and perplexity is 184.75977768815503
At time: 357.71362686157227 and batch: 200, loss is 5.251997241973877 and perplexity is 190.94725574405388
At time: 358.9195222854614 and batch: 250, loss is 5.263083782196045 and perplexity is 193.07597845651824
At time: 360.12648701667786 and batch: 300, loss is 5.250467224121094 and perplexity is 190.6553264193143
At time: 361.3315691947937 and batch: 350, loss is 5.265940589904785 and perplexity is 193.62834803130167
At time: 362.53707480430603 and batch: 400, loss is 5.227696294784546 and perplexity is 186.36298324418522
At time: 363.74328541755676 and batch: 450, loss is 5.208714456558227 and perplexity is 182.85883405579833
At time: 364.95020961761475 and batch: 500, loss is 5.194224739074707 and perplexity is 180.22836461301466
At time: 366.1558201313019 and batch: 550, loss is 5.19267876625061 and perplexity is 179.94995172402292
At time: 367.3687126636505 and batch: 600, loss is 5.217417478561401 and perplexity is 184.4572037466458
At time: 368.5750296115875 and batch: 650, loss is 5.217346410751343 and perplexity is 184.44409524292794
At time: 369.78030586242676 and batch: 700, loss is 5.246046676635742 and perplexity is 189.8143855744295
At time: 370.98492217063904 and batch: 750, loss is 5.213544349670411 and perplexity is 183.74415897019816
At time: 372.21598505973816 and batch: 800, loss is 5.227768135070801 and perplexity is 186.37637209517248
At time: 373.4312903881073 and batch: 850, loss is 5.244917993545532 and perplexity is 189.60026614637297
At time: 374.6415066719055 and batch: 900, loss is 5.228582162857055 and perplexity is 186.528149407853
At time: 375.8494403362274 and batch: 950, loss is 5.221046867370606 and perplexity is 185.12788700664785
At time: 377.0554053783417 and batch: 1000, loss is 5.19535080909729 and perplexity is 180.43142868234733
At time: 378.26057481765747 and batch: 1050, loss is 5.195338001251221 and perplexity is 180.42911775918176
At time: 379.46580958366394 and batch: 1100, loss is 5.153532314300537 and perplexity is 173.0416495386104
At time: 380.67427492141724 and batch: 1150, loss is 5.192966203689576 and perplexity is 180.0016835117585
At time: 381.88999247550964 and batch: 1200, loss is 5.203598985671997 and perplexity is 181.9258134683309
At time: 383.0976197719574 and batch: 1250, loss is 5.250197257995605 and perplexity is 190.603862886555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.229622277030109 and perplexity of 186.72226091128877
Finished 12 epochs...
Completing Train Step...
At time: 386.12617778778076 and batch: 50, loss is 5.255200290679932 and perplexity is 191.55984966482904
At time: 387.3642909526825 and batch: 100, loss is 5.268230476379395 and perplexity is 194.0722430071436
At time: 388.57546043395996 and batch: 150, loss is 5.205309448242187 and perplexity is 182.23725704317656
At time: 389.78713488578796 and batch: 200, loss is 5.238196029663086 and perplexity is 188.33005394834714
At time: 390.992751121521 and batch: 250, loss is 5.25006139755249 and perplexity is 190.57796912029286
At time: 392.1999206542969 and batch: 300, loss is 5.236785736083984 and perplexity is 188.0646404819564
At time: 393.40479803085327 and batch: 350, loss is 5.251864833831787 and perplexity is 190.92197444644543
At time: 394.6090729236603 and batch: 400, loss is 5.215361080169678 and perplexity is 184.0782759962609
At time: 395.81374883651733 and batch: 450, loss is 5.196606283187866 and perplexity is 180.65809792509108
At time: 397.0197522640228 and batch: 500, loss is 5.184059448242188 and perplexity is 178.40557117936436
At time: 398.22513461112976 and batch: 550, loss is 5.183029327392578 and perplexity is 178.22188650575507
At time: 399.43144154548645 and batch: 600, loss is 5.209345932006836 and perplexity is 182.97434138624914
At time: 400.63628125190735 and batch: 650, loss is 5.210265283584595 and perplexity is 183.14263648502052
At time: 401.84078073501587 and batch: 700, loss is 5.239015092849732 and perplexity is 188.48437135170022
At time: 403.0710322856903 and batch: 750, loss is 5.207443571090698 and perplexity is 182.62658903066034
At time: 404.2829556465149 and batch: 800, loss is 5.222541379928589 and perplexity is 185.4047698094887
At time: 405.4877643585205 and batch: 850, loss is 5.241410074234008 and perplexity is 188.93632891125216
At time: 406.6930363178253 and batch: 900, loss is 5.223694810867309 and perplexity is 185.6187447861297
At time: 407.898277759552 and batch: 950, loss is 5.21513876914978 and perplexity is 184.03735791542184
At time: 409.10233974456787 and batch: 1000, loss is 5.191640796661377 and perplexity is 179.7632660503364
At time: 410.3059628009796 and batch: 1050, loss is 5.192645034790039 and perplexity is 179.94388185169493
At time: 411.51052165031433 and batch: 1100, loss is 5.152107658386231 and perplexity is 172.79530025232455
At time: 412.7156412601471 and batch: 1150, loss is 5.191172189712525 and perplexity is 179.67904746896414
At time: 413.9208188056946 and batch: 1200, loss is 5.201342353820801 and perplexity is 181.51573675321427
At time: 415.1263554096222 and batch: 1250, loss is 5.245088901519775 and perplexity is 189.63267311300166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.230058433365648 and perplexity of 186.80371877126092
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 418.09866976737976 and batch: 50, loss is 5.247224388122558 and perplexity is 190.03806384506427
At time: 419.3125412464142 and batch: 100, loss is 5.259823846817016 and perplexity is 192.4475880560578
At time: 420.5230770111084 and batch: 150, loss is 5.19491870880127 and perplexity is 180.35348105041254
At time: 421.7303144931793 and batch: 200, loss is 5.228284006118774 and perplexity is 186.47254307334157
At time: 422.9406373500824 and batch: 250, loss is 5.2394025516510006 and perplexity is 188.55741543015378
At time: 424.1497142314911 and batch: 300, loss is 5.22379771232605 and perplexity is 185.63784620850305
At time: 425.3632709980011 and batch: 350, loss is 5.232837896347046 and perplexity is 187.32365502943745
At time: 426.56993865966797 and batch: 400, loss is 5.193406143188477 and perplexity is 180.08089078413033
At time: 427.78449726104736 and batch: 450, loss is 5.172604551315308 and perplexity is 176.373613899425
At time: 428.9923326969147 and batch: 500, loss is 5.156584730148316 and perplexity is 173.5706515683407
At time: 430.200660943985 and batch: 550, loss is 5.157189168930054 and perplexity is 173.67559611460192
At time: 431.4082112312317 and batch: 600, loss is 5.1849648380279545 and perplexity is 178.56717090556575
At time: 432.6232988834381 and batch: 650, loss is 5.185816469192505 and perplexity is 178.71930904687065
At time: 433.86867594718933 and batch: 700, loss is 5.209146738052368 and perplexity is 182.93789763343037
At time: 435.0753755569458 and batch: 750, loss is 5.177886924743652 and perplexity is 177.30775024732367
At time: 436.2828574180603 and batch: 800, loss is 5.184728956222534 and perplexity is 178.52505512627198
At time: 437.49029207229614 and batch: 850, loss is 5.203930511474609 and perplexity is 181.98613656843642
At time: 438.69828844070435 and batch: 900, loss is 5.183064641952515 and perplexity is 178.2281804443813
At time: 439.9060547351837 and batch: 950, loss is 5.173719902038574 and perplexity is 176.5704420830766
At time: 441.1136531829834 and batch: 1000, loss is 5.150069599151611 and perplexity is 172.44349181993377
At time: 442.3210141658783 and batch: 1050, loss is 5.146437187194824 and perplexity is 171.81824228779854
At time: 443.52802634239197 and batch: 1100, loss is 5.104141292572021 and perplexity is 164.70257846265721
At time: 444.73995304107666 and batch: 1150, loss is 5.141175174713135 and perplexity is 170.91650710385582
At time: 445.95285964012146 and batch: 1200, loss is 5.15687334060669 and perplexity is 173.6207531032158
At time: 447.1692147254944 and batch: 1250, loss is 5.2074672222137455 and perplexity is 182.6309084056681
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.203392752765739 and perplexity of 181.88829824766538
Finished 14 epochs...
Completing Train Step...
At time: 450.12935400009155 and batch: 50, loss is 5.223968276977539 and perplexity is 185.66951216351427
At time: 451.36800384521484 and batch: 100, loss is 5.238440008163452 and perplexity is 188.37600803816036
At time: 452.5807087421417 and batch: 150, loss is 5.175626125335693 and perplexity is 176.90734577817742
At time: 453.7864851951599 and batch: 200, loss is 5.208042612075806 and perplexity is 182.73602261678926
At time: 454.9912676811218 and batch: 250, loss is 5.2218475437164305 and perplexity is 185.27617388370197
At time: 456.1966755390167 and batch: 300, loss is 5.208493843078613 and perplexity is 182.81849738170968
At time: 457.40141677856445 and batch: 350, loss is 5.218399877548218 and perplexity is 184.63850335641285
At time: 458.6067600250244 and batch: 400, loss is 5.180916452407837 and perplexity is 177.84572347269525
At time: 459.8140776157379 and batch: 450, loss is 5.1602780055999755 and perplexity is 174.21288103004346
At time: 461.02027559280396 and batch: 500, loss is 5.145980291366577 and perplexity is 171.73975718080484
At time: 462.226482629776 and batch: 550, loss is 5.146589479446411 and perplexity is 171.8444108673657
At time: 463.46445965766907 and batch: 600, loss is 5.175526132583618 and perplexity is 176.88965721018982
At time: 464.66964173316956 and batch: 650, loss is 5.178028717041015 and perplexity is 177.33289290304683
At time: 465.8748080730438 and batch: 700, loss is 5.201912050247192 and perplexity is 181.61917508120172
At time: 467.07953429222107 and batch: 750, loss is 5.169986133575439 and perplexity is 175.9123981911544
At time: 468.2856855392456 and batch: 800, loss is 5.181060094833374 and perplexity is 177.8712714986324
At time: 469.49250745773315 and batch: 850, loss is 5.201579008102417 and perplexity is 181.5586983128157
At time: 470.70345973968506 and batch: 900, loss is 5.180827875137329 and perplexity is 177.82997108160063
At time: 471.9090163707733 and batch: 950, loss is 5.17263014793396 and perplexity is 176.37812852533963
At time: 473.11366391181946 and batch: 1000, loss is 5.1512425422668455 and perplexity is 172.64587689631853
At time: 474.31945514678955 and batch: 1050, loss is 5.149516553878784 and perplexity is 172.34814912879537
At time: 475.5246317386627 and batch: 1100, loss is 5.108980007171631 and perplexity is 165.5014584510982
At time: 476.7401957511902 and batch: 1150, loss is 5.146543617248535 and perplexity is 171.8365298857116
At time: 477.9527826309204 and batch: 1200, loss is 5.16163890838623 and perplexity is 174.4501292244867
At time: 479.15801668167114 and batch: 1250, loss is 5.208048000335693 and perplexity is 182.7370072486226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.202651420648951 and perplexity of 181.75350857863486
Finished 15 epochs...
Completing Train Step...
At time: 482.14676308631897 and batch: 50, loss is 5.215728874206543 and perplexity is 184.14599134037536
At time: 483.35771107673645 and batch: 100, loss is 5.230268545150757 and perplexity is 186.84297255777415
At time: 484.5668179988861 and batch: 150, loss is 5.168413286209106 and perplexity is 175.6359323152611
At time: 485.77511405944824 and batch: 200, loss is 5.2007509708404545 and perplexity is 181.40842317067415
At time: 486.98241567611694 and batch: 250, loss is 5.215075159072876 and perplexity is 184.02565165725363
At time: 488.19101452827454 and batch: 300, loss is 5.201908302307129 and perplexity is 181.6184943846948
At time: 489.39976167678833 and batch: 350, loss is 5.21206958770752 and perplexity is 183.47337979014006
At time: 490.6081850528717 and batch: 400, loss is 5.175201377868652 and perplexity is 176.83222078686154
At time: 491.81614351272583 and batch: 450, loss is 5.154366607666016 and perplexity is 173.18607727794694
At time: 493.024521112442 and batch: 500, loss is 5.141218690872193 and perplexity is 170.9239448955954
At time: 494.25969982147217 and batch: 550, loss is 5.1423454761505125 and perplexity is 171.11664802755348
At time: 495.46760630607605 and batch: 600, loss is 5.17164041519165 and perplexity is 176.20364767545877
At time: 496.6747200489044 and batch: 650, loss is 5.174793252944946 and perplexity is 176.7600658753545
At time: 497.8829462528229 and batch: 700, loss is 5.198857707977295 and perplexity is 181.0652942591274
At time: 499.0901925563812 and batch: 750, loss is 5.166863994598389 and perplexity is 175.36403171982724
At time: 500.2969512939453 and batch: 800, loss is 5.178411960601807 and perplexity is 177.40086761697214
At time: 501.5057349205017 and batch: 850, loss is 5.199838752746582 and perplexity is 181.24301458050172
At time: 502.713259935379 and batch: 900, loss is 5.17917010307312 and perplexity is 177.53541374528297
At time: 503.9217760562897 and batch: 950, loss is 5.171519250869751 and perplexity is 176.18229937332438
At time: 505.13464856147766 and batch: 1000, loss is 5.150964908599853 and perplexity is 172.5979512416202
At time: 506.34289026260376 and batch: 1050, loss is 5.150208444595337 and perplexity is 172.46743647533776
At time: 507.55106592178345 and batch: 1100, loss is 5.110334320068359 and perplexity is 165.7257510576082
At time: 508.75969767570496 and batch: 1150, loss is 5.1478542137145995 and perplexity is 172.06188587764672
At time: 509.96847462654114 and batch: 1200, loss is 5.162615089416504 and perplexity is 174.6205072777464
At time: 511.1757791042328 and batch: 1250, loss is 5.2069336032867435 and perplexity is 182.53347909366164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.202646074503877 and perplexity of 181.7525369006078
Finished 16 epochs...
Completing Train Step...
At time: 514.1440320014954 and batch: 50, loss is 5.209855308532715 and perplexity is 183.06756796228842
At time: 515.3762528896332 and batch: 100, loss is 5.224631090164184 and perplexity is 185.79261715783414
At time: 516.5816547870636 and batch: 150, loss is 5.163372755050659 and perplexity is 174.7528613688711
At time: 517.7868683338165 and batch: 200, loss is 5.195813226699829 and perplexity is 180.51488264482487
At time: 518.9939239025116 and batch: 250, loss is 5.210212345123291 and perplexity is 183.13294145226826
At time: 520.2023358345032 and batch: 300, loss is 5.197198486328125 and perplexity is 180.76511590305913
At time: 521.4073419570923 and batch: 350, loss is 5.207295207977295 and perplexity is 182.59949599117516
At time: 522.6134870052338 and batch: 400, loss is 5.170582838058472 and perplexity is 176.01739723135964
At time: 523.8199498653412 and batch: 450, loss is 5.1498291873931885 and perplexity is 172.40203935986585
At time: 525.0522940158844 and batch: 500, loss is 5.13738561630249 and perplexity is 170.2700347129378
At time: 526.2581589221954 and batch: 550, loss is 5.138893833160401 and perplexity is 170.5270326052492
At time: 527.4638695716858 and batch: 600, loss is 5.168783416748047 and perplexity is 175.70095256979494
At time: 528.6693069934845 and batch: 650, loss is 5.172034158706665 and perplexity is 176.27304037962003
At time: 529.8745341300964 and batch: 700, loss is 5.196261615753174 and perplexity is 180.5958416913873
At time: 531.0849704742432 and batch: 750, loss is 5.164132242202759 and perplexity is 174.88563433517223
At time: 532.3050956726074 and batch: 800, loss is 5.176032915115356 and perplexity is 176.97932451750245
At time: 533.5111026763916 and batch: 850, loss is 5.198282299041748 and perplexity is 180.9611376400917
At time: 534.7163193225861 and batch: 900, loss is 5.177336254119873 and perplexity is 177.21013895619598
At time: 535.9220476150513 and batch: 950, loss is 5.170009632110595 and perplexity is 175.9165319233958
At time: 537.1284275054932 and batch: 1000, loss is 5.150178537368775 and perplexity is 172.46227852977086
At time: 538.3329999446869 and batch: 1050, loss is 5.149832611083984 and perplexity is 172.4026296121516
At time: 539.5397136211395 and batch: 1100, loss is 5.110556764602661 and perplexity is 165.7626199456133
At time: 540.7559740543365 and batch: 1150, loss is 5.1480100631713865 and perplexity is 172.0887037188132
At time: 541.9657044410706 and batch: 1200, loss is 5.162366466522217 and perplexity is 174.57709801831658
At time: 543.1721048355103 and batch: 1250, loss is 5.205284051895141 and perplexity is 182.23262894132068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.20256009067062 and perplexity of 181.73690979262986
Finished 17 epochs...
Completing Train Step...
At time: 546.1292176246643 and batch: 50, loss is 5.204805116653443 and perplexity is 182.14537220996309
At time: 547.3658955097198 and batch: 100, loss is 5.2198402690887455 and perplexity is 184.90464672401617
At time: 548.5728125572205 and batch: 150, loss is 5.159262809753418 and perplexity is 174.03611058030012
At time: 549.7795586585999 and batch: 200, loss is 5.1918770217895505 and perplexity is 179.8057356668978
At time: 550.9859204292297 and batch: 250, loss is 5.206280603408813 and perplexity is 182.41432366257033
At time: 552.2004082202911 and batch: 300, loss is 5.193307514190674 and perplexity is 180.06313046220467
At time: 553.4083275794983 and batch: 350, loss is 5.203171892166138 and perplexity is 181.84813072492884
At time: 554.6420476436615 and batch: 400, loss is 5.166450309753418 and perplexity is 175.2915012809578
At time: 555.8480405807495 and batch: 450, loss is 5.145904750823974 and perplexity is 171.72678435635424
At time: 557.0539286136627 and batch: 500, loss is 5.133939790725708 and perplexity is 169.68432358111076
At time: 558.2610523700714 and batch: 550, loss is 5.135976057052613 and perplexity is 170.030198082212
At time: 559.4677927494049 and batch: 600, loss is 5.165845727920532 and perplexity is 175.18555525357286
At time: 560.6817824840546 and batch: 650, loss is 5.169281034469605 and perplexity is 175.7884062349304
At time: 561.8877007961273 and batch: 700, loss is 5.193851165771484 and perplexity is 180.16104868201185
At time: 563.0933945178986 and batch: 750, loss is 5.161509571075439 and perplexity is 174.42756777295583
At time: 564.2988839149475 and batch: 800, loss is 5.173498296737671 and perplexity is 176.53131747239897
At time: 565.5098352432251 and batch: 850, loss is 5.196425046920776 and perplexity is 180.62535909262508
At time: 566.7182190418243 and batch: 900, loss is 5.175276927947998 and perplexity is 176.84558097984828
At time: 567.9286012649536 and batch: 950, loss is 5.168318309783936 and perplexity is 175.619251834417
At time: 569.1380841732025 and batch: 1000, loss is 5.148820285797119 and perplexity is 172.22819038019307
At time: 570.3477771282196 and batch: 1050, loss is 5.148899002075195 and perplexity is 172.24174807591828
At time: 571.557806968689 and batch: 1100, loss is 5.110160875320434 and perplexity is 165.6970092891178
At time: 572.7686858177185 and batch: 1150, loss is 5.147516822814941 and perplexity is 172.00384355519984
At time: 573.9783756732941 and batch: 1200, loss is 5.161712169647217 and perplexity is 174.46291012909984
At time: 575.1934559345245 and batch: 1250, loss is 5.20354395866394 and perplexity is 181.91580291055544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.202632263629106 and perplexity of 181.75002675641488
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 578.2227909564972 and batch: 50, loss is 5.2029331684112545 and perplexity is 181.80472443760578
At time: 579.4311366081238 and batch: 100, loss is 5.2215448665618895 and perplexity is 185.22010350462608
At time: 580.6520743370056 and batch: 150, loss is 5.158728504180909 and perplexity is 173.9431469543025
At time: 581.8704741001129 and batch: 200, loss is 5.191593885421753 and perplexity is 179.75483333048328
At time: 583.0797765254974 and batch: 250, loss is 5.204135675430297 and perplexity is 182.02347739446006
At time: 584.2881951332092 and batch: 300, loss is 5.189889297485352 and perplexity is 179.44868641137975
At time: 585.5246996879578 and batch: 350, loss is 5.1981019687652585 and perplexity is 180.9285078102688
At time: 586.7334830760956 and batch: 400, loss is 5.1587778663635255 and perplexity is 173.95173337960804
At time: 587.9425809383392 and batch: 450, loss is 5.138183546066284 and perplexity is 170.40595246071425
At time: 589.1513657569885 and batch: 500, loss is 5.122782726287841 and perplexity is 167.80166664062136
At time: 590.3600199222565 and batch: 550, loss is 5.125552673339843 and perplexity is 168.2671127053323
At time: 591.5735709667206 and batch: 600, loss is 5.155370283126831 and perplexity is 173.35998715380384
At time: 592.7811119556427 and batch: 650, loss is 5.1595830154418945 and perplexity is 174.09184685596824
At time: 593.9898824691772 and batch: 700, loss is 5.183194055557251 and perplexity is 178.25124708821465
At time: 595.2025818824768 and batch: 750, loss is 5.147410335540772 and perplexity is 171.98552830994058
At time: 596.4144067764282 and batch: 800, loss is 5.156498260498047 and perplexity is 173.55564362367792
At time: 597.6227622032166 and batch: 850, loss is 5.175316591262817 and perplexity is 176.85259540090775
At time: 598.8314940929413 and batch: 900, loss is 5.153942174911499 and perplexity is 173.11258703106117
At time: 600.038834810257 and batch: 950, loss is 5.147790193557739 and perplexity is 172.0508708013204
At time: 601.2441835403442 and batch: 1000, loss is 5.128018846511841 and perplexity is 168.68260066646374
At time: 602.458455324173 and batch: 1050, loss is 5.125747451782226 and perplexity is 168.29989070356942
At time: 603.6641089916229 and batch: 1100, loss is 5.084034109115601 and perplexity is 161.42394599554163
At time: 604.8768901824951 and batch: 1150, loss is 5.119280920028687 and perplexity is 167.2150853605016
At time: 606.0904426574707 and batch: 1200, loss is 5.139675140380859 and perplexity is 170.66031866900883
At time: 607.2963562011719 and batch: 1250, loss is 5.186532688140869 and perplexity is 178.84735705217867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.192599331375456 and perplexity of 179.93565798979176
Finished 19 epochs...
Completing Train Step...
At time: 610.2339210510254 and batch: 50, loss is 5.193758192062378 and perplexity is 180.14429921972138
At time: 611.4666831493378 and batch: 100, loss is 5.212389230728149 and perplexity is 183.53203514935
At time: 612.6718587875366 and batch: 150, loss is 5.150502595901489 and perplexity is 172.51817545914912
At time: 613.8770415782928 and batch: 200, loss is 5.1824843692779545 and perplexity is 178.1247895018033
At time: 615.08256483078 and batch: 250, loss is 5.196122808456421 and perplexity is 180.5707754105289
At time: 616.3233571052551 and batch: 300, loss is 5.1825265693664555 and perplexity is 178.13230654229326
At time: 617.5288968086243 and batch: 350, loss is 5.191982803344726 and perplexity is 179.8247568032711
At time: 618.7351369857788 and batch: 400, loss is 5.153361644744873 and perplexity is 173.0121191172159
At time: 619.9439134597778 and batch: 450, loss is 5.13252742767334 and perplexity is 169.444836872818
At time: 621.1601502895355 and batch: 500, loss is 5.117818393707275 and perplexity is 166.97070764480557
At time: 622.3767786026001 and batch: 550, loss is 5.120984477996826 and perplexity is 167.5001887276669
At time: 623.5924296379089 and batch: 600, loss is 5.151440782546997 and perplexity is 172.68010565596882
At time: 624.8065338134766 and batch: 650, loss is 5.156411514282227 and perplexity is 173.5405889813354
At time: 626.0238130092621 and batch: 700, loss is 5.180446548461914 and perplexity is 177.76217269742676
At time: 627.2381784915924 and batch: 750, loss is 5.145788621902466 and perplexity is 171.7068430679954
At time: 628.4447157382965 and batch: 800, loss is 5.155753126144409 and perplexity is 173.4263695206152
At time: 629.6504120826721 and batch: 850, loss is 5.175478677749634 and perplexity is 176.88126314004458
At time: 630.8561081886292 and batch: 900, loss is 5.154576683044434 and perplexity is 173.22246323043044
At time: 632.0721538066864 and batch: 950, loss is 5.14872989654541 and perplexity is 172.21262350649098
At time: 633.281263589859 and batch: 1000, loss is 5.129444103240967 and perplexity is 168.92318808684348
At time: 634.4882762432098 and batch: 1050, loss is 5.127709455490113 and perplexity is 168.63041985684896
At time: 635.6949343681335 and batch: 1100, loss is 5.086845951080322 and perplexity is 161.87848336514642
At time: 636.9024050235748 and batch: 1150, loss is 5.122651166915894 and perplexity is 167.77959221082727
At time: 638.1172747612 and batch: 1200, loss is 5.14310881614685 and perplexity is 171.24731807552416
At time: 639.3257250785828 and batch: 1250, loss is 5.187441368103027 and perplexity is 179.00994592125306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.192293710082117 and perplexity of 179.88067422381187
Finished 20 epochs...
Completing Train Step...
At time: 642.3914306163788 and batch: 50, loss is 5.1900541210174564 and perplexity is 179.47826621536254
At time: 643.6002011299133 and batch: 100, loss is 5.208738069534302 and perplexity is 182.8631519480509
At time: 644.8102176189423 and batch: 150, loss is 5.1470352268219 and perplexity is 171.92102713698418
At time: 646.0174288749695 and batch: 200, loss is 5.178501586914063 and perplexity is 177.41676811506858
At time: 647.2654936313629 and batch: 250, loss is 5.1925472640991215 and perplexity is 179.92628947406337
At time: 648.4745676517487 and batch: 300, loss is 5.178875217437744 and perplexity is 177.4830688202615
At time: 649.6837692260742 and batch: 350, loss is 5.189056119918823 and perplexity is 179.29923605950452
At time: 650.8930263519287 and batch: 400, loss is 5.150704002380371 and perplexity is 172.5529252367091
At time: 652.1017203330994 and batch: 450, loss is 5.129587326049805 and perplexity is 168.9473834729433
At time: 653.3102865219116 and batch: 500, loss is 5.115257110595703 and perplexity is 166.54359560211248
At time: 654.5185222625732 and batch: 550, loss is 5.118937702178955 and perplexity is 167.15770400617723
At time: 655.7249388694763 and batch: 600, loss is 5.1498394870758055 and perplexity is 172.4038150552983
At time: 656.9318008422852 and batch: 650, loss is 5.155044794082642 and perplexity is 173.303569559437
At time: 658.1394805908203 and batch: 700, loss is 5.179204139709473 and perplexity is 177.54145655643828
At time: 659.3551154136658 and batch: 750, loss is 5.145204467773437 and perplexity is 171.60656909721325
At time: 660.5625321865082 and batch: 800, loss is 5.155406484603882 and perplexity is 173.36626315499987
At time: 661.7700383663177 and batch: 850, loss is 5.175278768539429 and perplexity is 176.84590648060885
At time: 662.9772872924805 and batch: 900, loss is 5.154688339233399 and perplexity is 173.24180567034932
At time: 664.1835563182831 and batch: 950, loss is 5.148793802261353 and perplexity is 172.22362922915116
At time: 665.393440246582 and batch: 1000, loss is 5.129815263748169 and perplexity is 168.9858973398911
At time: 666.6009159088135 and batch: 1050, loss is 5.128537378311157 and perplexity is 168.77009064010704
At time: 667.8092563152313 and batch: 1100, loss is 5.087921733856201 and perplexity is 162.05272315461414
At time: 669.0171692371368 and batch: 1150, loss is 5.124001502990723 and perplexity is 168.00630408096904
At time: 670.2256276607513 and batch: 1200, loss is 5.144574909210205 and perplexity is 171.49856671261142
At time: 671.4330880641937 and batch: 1250, loss is 5.187468881607056 and perplexity is 179.01487117987662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.192308411981068 and perplexity of 179.8833188307478
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 674.4027674198151 and batch: 50, loss is 5.188781604766846 and perplexity is 179.250022457717
At time: 675.6342267990112 and batch: 100, loss is 5.208695945739746 and perplexity is 182.8554492204415
At time: 676.8392724990845 and batch: 150, loss is 5.146553907394409 and perplexity is 171.83829811776826
At time: 678.0713038444519 and batch: 200, loss is 5.177941570281982 and perplexity is 177.31743958952325
At time: 679.2773554325104 and batch: 250, loss is 5.191659440994263 and perplexity is 179.7666176477533
At time: 680.4830393791199 and batch: 300, loss is 5.177409133911133 and perplexity is 177.22305446476628
At time: 681.6883366107941 and batch: 350, loss is 5.18682147026062 and perplexity is 178.89901242927564
At time: 682.8932414054871 and batch: 400, loss is 5.146720199584961 and perplexity is 171.86687586084489
At time: 684.0989263057709 and batch: 450, loss is 5.125425224304199 and perplexity is 168.24566859063225
At time: 685.3105635643005 and batch: 500, loss is 5.110864839553833 and perplexity is 165.81369512374673
At time: 686.5171275138855 and batch: 550, loss is 5.11388671875 and perplexity is 166.3155219275902
At time: 687.7229492664337 and batch: 600, loss is 5.144594297409058 and perplexity is 171.50189179315933
At time: 688.9281163215637 and batch: 650, loss is 5.150258541107178 and perplexity is 172.4760767087323
At time: 690.1332168579102 and batch: 700, loss is 5.174466876983643 and perplexity is 176.70238505226348
At time: 691.337849855423 and batch: 750, loss is 5.139374599456787 and perplexity is 170.60903596579513
At time: 692.5434989929199 and batch: 800, loss is 5.147493915557861 and perplexity is 171.99990346406534
At time: 693.7487432956696 and batch: 850, loss is 5.164750623703003 and perplexity is 174.99381382075256
At time: 694.9537072181702 and batch: 900, loss is 5.144157667160034 and perplexity is 171.4270252251311
At time: 696.1611151695251 and batch: 950, loss is 5.1383700180053715 and perplexity is 170.43773135194525
At time: 697.3666882514954 and batch: 1000, loss is 5.119894218444824 and perplexity is 167.3176695616561
At time: 698.5720698833466 and batch: 1050, loss is 5.116942739486694 and perplexity is 166.82456303535326
At time: 699.7789072990417 and batch: 1100, loss is 5.07519790649414 and perplexity is 160.00385464808164
At time: 700.9843673706055 and batch: 1150, loss is 5.11004487991333 and perplexity is 165.67779031174743
At time: 702.188957452774 and batch: 1200, loss is 5.134010944366455 and perplexity is 169.69639766806387
At time: 703.3944630622864 and batch: 1250, loss is 5.17944899559021 and perplexity is 177.58493394878116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189269128507071 and perplexity of 179.33743240458185
Finished 22 epochs...
Completing Train Step...
At time: 706.3625218868256 and batch: 50, loss is 5.185178899765015 and perplexity is 178.6053993958343
At time: 707.5992074012756 and batch: 100, loss is 5.204396295547485 and perplexity is 182.07092255678285
At time: 708.816329240799 and batch: 150, loss is 5.142835359573365 and perplexity is 171.20049577292963
At time: 710.0265827178955 and batch: 200, loss is 5.173760824203491 and perplexity is 176.57766787567357
At time: 711.2366979122162 and batch: 250, loss is 5.187961578369141 and perplexity is 179.10309295877974
At time: 712.4484603404999 and batch: 300, loss is 5.174183177947998 and perplexity is 176.65226186631392
At time: 713.659435749054 and batch: 350, loss is 5.1838461208343505 and perplexity is 178.36751644052464
At time: 714.8715722560883 and batch: 400, loss is 5.1441669940948485 and perplexity is 171.42862412127727
At time: 716.0816156864166 and batch: 450, loss is 5.12302306175232 and perplexity is 167.84200017870933
At time: 717.2924072742462 and batch: 500, loss is 5.109005422592163 and perplexity is 165.50566479371608
At time: 718.503458738327 and batch: 550, loss is 5.112197771072387 and perplexity is 166.0348607907838
At time: 719.7138047218323 and batch: 600, loss is 5.143340950012207 and perplexity is 171.28707499168576
At time: 720.9250416755676 and batch: 650, loss is 5.149205360412598 and perplexity is 172.29452385522734
At time: 722.136358499527 and batch: 700, loss is 5.173381814956665 and perplexity is 176.5107559876788
At time: 723.346556186676 and batch: 750, loss is 5.138779811859131 and perplexity is 170.5075899995465
At time: 724.5572361946106 and batch: 800, loss is 5.147102003097534 and perplexity is 171.93250776619217
At time: 725.7687499523163 and batch: 850, loss is 5.165158195495605 and perplexity is 175.06515089964893
At time: 726.9767560958862 and batch: 900, loss is 5.145090360641479 and perplexity is 171.58698868094217
At time: 728.184629201889 and batch: 950, loss is 5.1393606567382815 and perplexity is 170.60665722861518
At time: 729.3966374397278 and batch: 1000, loss is 5.121090574264526 and perplexity is 167.5179608152894
At time: 730.6098523139954 and batch: 1050, loss is 5.118244075775147 and perplexity is 167.04179921103872
At time: 731.8163268566132 and batch: 1100, loss is 5.0767249011993405 and perplexity is 160.24836632344676
At time: 733.0237636566162 and batch: 1150, loss is 5.111833286285401 and perplexity is 165.97435463737216
At time: 734.2313256263733 and batch: 1200, loss is 5.135832071304321 and perplexity is 170.00571791934857
At time: 735.4390211105347 and batch: 1250, loss is 5.179860429763794 and perplexity is 177.65801349200117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18922457729813 and perplexity of 179.3294428831325
Finished 23 epochs...
Completing Train Step...
At time: 738.3962864875793 and batch: 50, loss is 5.183217573165893 and perplexity is 178.25543918057755
At time: 739.6026313304901 and batch: 100, loss is 5.202357339859009 and perplexity is 181.70006622182774
At time: 740.8114647865295 and batch: 150, loss is 5.1408124446868895 and perplexity is 170.85452179739377
At time: 742.0165700912476 and batch: 200, loss is 5.171550683975219 and perplexity is 176.1878374171608
At time: 743.2232460975647 and batch: 250, loss is 5.18590295791626 and perplexity is 178.7347669202766
At time: 744.4283459186554 and batch: 300, loss is 5.172356615066528 and perplexity is 176.32988990781627
At time: 745.651237487793 and batch: 350, loss is 5.182105932235718 and perplexity is 178.05739323674118
At time: 746.8578701019287 and batch: 400, loss is 5.142611703872681 and perplexity is 171.16221008765555
At time: 748.0611367225647 and batch: 450, loss is 5.121517839431763 and perplexity is 167.58955069767475
At time: 749.2665495872498 and batch: 500, loss is 5.107861528396606 and perplexity is 165.31645206473
At time: 750.4732365608215 and batch: 550, loss is 5.111214942932129 and perplexity is 165.8717572218526
At time: 751.6781585216522 and batch: 600, loss is 5.142645092010498 and perplexity is 171.16792497051915
At time: 752.8837292194366 and batch: 650, loss is 5.148530073165894 and perplexity is 172.17821483600952
At time: 754.0970904827118 and batch: 700, loss is 5.172791366577148 and perplexity is 176.4065662601935
At time: 755.3058354854584 and batch: 750, loss is 5.138520860671997 and perplexity is 170.46344257296187
At time: 756.5143582820892 and batch: 800, loss is 5.146888647079468 and perplexity is 171.8958288439326
At time: 757.7226243019104 and batch: 850, loss is 5.165333595275879 and perplexity is 175.09585998175464
At time: 758.9361910820007 and batch: 900, loss is 5.145704975128174 and perplexity is 171.69248094512992
At time: 760.1428596973419 and batch: 950, loss is 5.139972333908081 and perplexity is 170.71104534851787
At time: 761.3526484966278 and batch: 1000, loss is 5.121837568283081 and perplexity is 167.64314247917508
At time: 762.5574214458466 and batch: 1050, loss is 5.119003257751465 and perplexity is 167.16866248435352
At time: 763.7619354724884 and batch: 1100, loss is 5.077570190429688 and perplexity is 160.38387980765623
At time: 764.9670667648315 and batch: 1150, loss is 5.112907447814941 and perplexity is 166.15273369082095
At time: 766.1727068424225 and batch: 1200, loss is 5.136870927810669 and perplexity is 170.1824212343278
At time: 767.3771951198578 and batch: 1250, loss is 5.180009651184082 and perplexity is 177.6845258511567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18923927919708 and perplexity of 179.3320793858614
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 770.3334150314331 and batch: 50, loss is 5.182732057571411 and perplexity is 178.16891439132107
At time: 771.5652599334717 and batch: 100, loss is 5.202038555145264 and perplexity is 181.64215224976476
At time: 772.7714178562164 and batch: 150, loss is 5.1407884502410885 and perplexity is 170.8504222870136
At time: 773.9762141704559 and batch: 200, loss is 5.170693826675415 and perplexity is 176.0369342430092
At time: 775.1804151535034 and batch: 250, loss is 5.185629138946533 and perplexity is 178.6858326504156
At time: 776.389285326004 and batch: 300, loss is 5.171952981948852 and perplexity is 176.25873168648675
At time: 777.6106762886047 and batch: 350, loss is 5.180721349716187 and perplexity is 177.8110286779812
At time: 778.819091796875 and batch: 400, loss is 5.140810527801514 and perplexity is 170.85419428917348
At time: 780.0272433757782 and batch: 450, loss is 5.1192956638336184 and perplexity is 167.21755076527646
At time: 781.2367894649506 and batch: 500, loss is 5.105919094085693 and perplexity is 164.99564738795436
At time: 782.4461967945099 and batch: 550, loss is 5.108605756759643 and perplexity is 165.43953105098856
At time: 783.655017375946 and batch: 600, loss is 5.139635467529297 and perplexity is 170.65354822182107
At time: 784.8643245697021 and batch: 650, loss is 5.1458100318908695 and perplexity is 171.71051934886873
At time: 786.0699336528778 and batch: 700, loss is 5.17017599105835 and perplexity is 175.94579964694552
At time: 787.2755000591278 and batch: 750, loss is 5.135873756408691 and perplexity is 170.01280477315063
At time: 788.4804630279541 and batch: 800, loss is 5.143304147720337 and perplexity is 171.2807713507533
At time: 789.6858787536621 and batch: 850, loss is 5.160784425735474 and perplexity is 174.30112828406743
At time: 790.8926179409027 and batch: 900, loss is 5.140630912780762 and perplexity is 170.82350906536672
At time: 792.0984466075897 and batch: 950, loss is 5.135582818984985 and perplexity is 169.9633488803667
At time: 793.3034682273865 and batch: 1000, loss is 5.117205057144165 and perplexity is 166.8683298040845
At time: 794.5088186264038 and batch: 1050, loss is 5.11360071182251 and perplexity is 166.26796133782312
At time: 795.7151415348053 and batch: 1100, loss is 5.071866798400879 and perplexity is 159.47175125190572
At time: 796.9221813678741 and batch: 1150, loss is 5.106533746719361 and perplexity is 165.09709357105302
At time: 798.1281642913818 and batch: 1200, loss is 5.13183108329773 and perplexity is 169.32688598567
At time: 799.3664863109589 and batch: 1250, loss is 5.176960248947143 and perplexity is 177.1435195526933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189040580805201 and perplexity of 179.29644992995003
Finished 25 epochs...
Completing Train Step...
At time: 802.3298456668854 and batch: 50, loss is 5.181640100479126 and perplexity is 177.97446776463428
At time: 803.5446555614471 and batch: 100, loss is 5.200382843017578 and perplexity is 181.3416539733545
At time: 804.7588121891022 and batch: 150, loss is 5.139192476272583 and perplexity is 170.57796693419232
At time: 805.9676403999329 and batch: 200, loss is 5.16904995918274 and perplexity is 175.74779057135086
At time: 807.1726515293121 and batch: 250, loss is 5.184270553588867 and perplexity is 178.44323752496123
At time: 808.3773865699768 and batch: 300, loss is 5.170760183334351 and perplexity is 176.04861585338685
At time: 809.5835793018341 and batch: 350, loss is 5.17951868057251 and perplexity is 177.5973093829461
At time: 810.7901644706726 and batch: 400, loss is 5.139793777465821 and perplexity is 170.68056651278312
At time: 811.9955213069916 and batch: 450, loss is 5.118322696685791 and perplexity is 167.0549327056852
At time: 813.2019526958466 and batch: 500, loss is 5.105141563415527 and perplexity is 164.8674080731272
At time: 814.407719373703 and batch: 550, loss is 5.107977819442749 and perplexity is 165.33567800576597
At time: 815.612405538559 and batch: 600, loss is 5.138992757797241 and perplexity is 170.54390276444457
At time: 816.8175213336945 and batch: 650, loss is 5.145475025177002 and perplexity is 171.653004806462
At time: 818.0225079059601 and batch: 700, loss is 5.169886617660523 and perplexity is 175.8948929789413
At time: 819.2323081493378 and batch: 750, loss is 5.135801048278808 and perplexity is 170.0004439094325
At time: 820.4413866996765 and batch: 800, loss is 5.143276042938233 and perplexity is 171.27595760964073
At time: 821.653781414032 and batch: 850, loss is 5.161032857894898 and perplexity is 174.3444356690079
At time: 822.8589506149292 and batch: 900, loss is 5.140856657028198 and perplexity is 170.86207584281544
At time: 824.0639233589172 and batch: 950, loss is 5.135955801010132 and perplexity is 170.0267539781787
At time: 825.2809381484985 and batch: 1000, loss is 5.117657470703125 and perplexity is 166.94384037876478
At time: 826.4975221157074 and batch: 1050, loss is 5.114305114746093 and perplexity is 166.3851222352826
At time: 827.7132995128632 and batch: 1100, loss is 5.072640371322632 and perplexity is 159.5951620079141
At time: 828.9276797771454 and batch: 1150, loss is 5.107370281219483 and perplexity is 165.23526076845147
At time: 830.182977437973 and batch: 1200, loss is 5.132780447006225 and perplexity is 169.487715116688
At time: 831.3875939846039 and batch: 1250, loss is 5.177350368499756 and perplexity is 177.21264018506787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189066866018476 and perplexity of 179.30116283731556
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 834.3237357139587 and batch: 50, loss is 5.181341238021851 and perplexity is 177.92128582530427
At time: 835.5727410316467 and batch: 100, loss is 5.20005576133728 and perplexity is 181.28235013959247
At time: 836.7797434329987 and batch: 150, loss is 5.138985500335694 and perplexity is 170.54266505311946
At time: 837.9855003356934 and batch: 200, loss is 5.168484268188476 and perplexity is 175.64839974386143
At time: 839.189510345459 and batch: 250, loss is 5.184138507843017 and perplexity is 178.41967641017717
At time: 840.3950242996216 and batch: 300, loss is 5.170688190460205 and perplexity is 176.03594206375894
At time: 841.6014592647552 and batch: 350, loss is 5.178791418075561 and perplexity is 177.46819647545135
At time: 842.8065140247345 and batch: 400, loss is 5.139031677246094 and perplexity is 170.55054036831052
At time: 844.0154070854187 and batch: 450, loss is 5.1172480869293215 and perplexity is 166.87551026695115
At time: 845.2250289916992 and batch: 500, loss is 5.104342470169067 and perplexity is 164.73571626479955
At time: 846.4287085533142 and batch: 550, loss is 5.106684436798096 and perplexity is 165.12197393964746
At time: 847.6337852478027 and batch: 600, loss is 5.137377061843872 and perplexity is 170.26857815120204
At time: 848.8376889228821 and batch: 650, loss is 5.144258480072022 and perplexity is 171.44430815389407
At time: 850.0434925556183 and batch: 700, loss is 5.168850965499878 and perplexity is 175.7128213506927
At time: 851.2477781772614 and batch: 750, loss is 5.134633388519287 and perplexity is 169.80205707867876
At time: 852.4508228302002 and batch: 800, loss is 5.142237195968628 and perplexity is 171.09812048888227
At time: 853.6549513339996 and batch: 850, loss is 5.1592041015625 and perplexity is 174.02589353500852
At time: 854.8657891750336 and batch: 900, loss is 5.138580532073974 and perplexity is 170.4736146690546
At time: 856.0750052928925 and batch: 950, loss is 5.134112586975098 and perplexity is 169.7136469292152
At time: 857.2836346626282 and batch: 1000, loss is 5.115367670059204 and perplexity is 166.5620095905933
At time: 858.4915404319763 and batch: 1050, loss is 5.112273292541504 and perplexity is 166.04740046089648
At time: 859.7581050395966 and batch: 1100, loss is 5.070491456985474 and perplexity is 159.25257390421265
At time: 860.968718290329 and batch: 1150, loss is 5.10476469039917 and perplexity is 164.80528570260557
At time: 862.1779396533966 and batch: 1200, loss is 5.130823888778687 and perplexity is 169.15642673140198
At time: 863.3858411312103 and batch: 1250, loss is 5.176618480682373 and perplexity is 177.0829878638949
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189600143989507 and perplexity of 179.39680569748552
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 866.3835527896881 and batch: 50, loss is 5.181421546936035 and perplexity is 177.93557506434817
At time: 867.621194601059 and batch: 100, loss is 5.199544792175293 and perplexity is 181.18974411048598
At time: 868.831253528595 and batch: 150, loss is 5.138468494415283 and perplexity is 170.4545162742882
At time: 870.0397520065308 and batch: 200, loss is 5.168184061050415 and perplexity is 175.59567675477405
At time: 871.2473411560059 and batch: 250, loss is 5.183727016448975 and perplexity is 178.3462733522056
At time: 872.4550569057465 and batch: 300, loss is 5.170370292663574 and perplexity is 175.9799895197155
At time: 873.6617939472198 and batch: 350, loss is 5.178270425796509 and perplexity is 177.37576099648652
At time: 874.8704857826233 and batch: 400, loss is 5.138430242538452 and perplexity is 170.44799619382977
At time: 876.0781021118164 and batch: 450, loss is 5.1167540359497075 and perplexity is 166.79308562029544
At time: 877.2860479354858 and batch: 500, loss is 5.104062280654907 and perplexity is 164.6895655102752
At time: 878.4939115047455 and batch: 550, loss is 5.106110792160035 and perplexity is 165.02727976766963
At time: 879.7139656543732 and batch: 600, loss is 5.1367562389373775 and perplexity is 170.16290432339005
At time: 880.9214894771576 and batch: 650, loss is 5.143979797363281 and perplexity is 171.396536246611
At time: 882.128909111023 and batch: 700, loss is 5.168568172454834 and perplexity is 175.66313801227201
At time: 883.3362240791321 and batch: 750, loss is 5.134273452758789 and perplexity is 169.74095024405725
At time: 884.5419058799744 and batch: 800, loss is 5.141914691925049 and perplexity is 171.04294955007947
At time: 885.7485945224762 and batch: 850, loss is 5.158427267074585 and perplexity is 173.89075671538737
At time: 886.955718755722 and batch: 900, loss is 5.137771415710449 and perplexity is 170.33573746474963
At time: 888.1623966693878 and batch: 950, loss is 5.133382358551025 and perplexity is 169.58976243776792
At time: 889.3702816963196 and batch: 1000, loss is 5.114353475570678 and perplexity is 166.39316895156392
At time: 890.6216967105865 and batch: 1050, loss is 5.11150652885437 and perplexity is 165.92013014324462
At time: 891.8294050693512 and batch: 1100, loss is 5.069717264175415 and perplexity is 159.12932942016508
At time: 893.0367348194122 and batch: 1150, loss is 5.103812351226806 and perplexity is 164.64840988456825
At time: 894.2446084022522 and batch: 1200, loss is 5.12995641708374 and perplexity is 169.0097519465041
At time: 895.4525015354156 and batch: 1250, loss is 5.176217584609986 and perplexity is 177.01201021785394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189280711821396 and perplexity of 179.33950973846274
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 898.4163410663605 and batch: 50, loss is 5.181466293334961 and perplexity is 177.94353721871062
At time: 899.6214745044708 and batch: 100, loss is 5.19932822227478 and perplexity is 181.1505081144513
At time: 900.8263583183289 and batch: 150, loss is 5.138289546966552 and perplexity is 170.42401660247685
At time: 902.0310897827148 and batch: 200, loss is 5.168094787597656 and perplexity is 175.58000142212666
At time: 903.2365126609802 and batch: 250, loss is 5.1834453773498534 and perplexity is 178.29605114104817
At time: 904.4429318904877 and batch: 300, loss is 5.170055952072143 and perplexity is 175.92468055911098
At time: 905.6493194103241 and batch: 350, loss is 5.177973670959473 and perplexity is 177.3231316908246
At time: 906.854326248169 and batch: 400, loss is 5.138122539520264 and perplexity is 170.3955568992352
At time: 908.0604388713837 and batch: 450, loss is 5.116574640274048 and perplexity is 166.76316634577972
At time: 909.2649364471436 and batch: 500, loss is 5.10397611618042 and perplexity is 164.6753757317457
At time: 910.4714403152466 and batch: 550, loss is 5.105913143157959 and perplexity is 164.99466551370193
At time: 911.6773045063019 and batch: 600, loss is 5.136548290252685 and perplexity is 170.12752285014548
At time: 912.8814430236816 and batch: 650, loss is 5.1439792442321775 and perplexity is 171.39644144188196
At time: 914.086484670639 and batch: 700, loss is 5.168638858795166 and perplexity is 175.6755554354953
At time: 915.290771484375 and batch: 750, loss is 5.134266653060913 and perplexity is 169.73979606080246
At time: 916.4959707260132 and batch: 800, loss is 5.141695756912231 and perplexity is 171.0055063586897
At time: 917.7023658752441 and batch: 850, loss is 5.158037157058716 and perplexity is 173.82293341965917
At time: 918.9203534126282 and batch: 900, loss is 5.137377548217773 and perplexity is 170.26866096541477
At time: 920.1362588405609 and batch: 950, loss is 5.133077278137207 and perplexity is 169.53803181426522
At time: 921.386194229126 and batch: 1000, loss is 5.113979778289795 and perplexity is 166.33099989369657
At time: 922.591029882431 and batch: 1050, loss is 5.111178293228149 and perplexity is 165.86567818244993
At time: 923.7966449260712 and batch: 1100, loss is 5.0694770336151125 and perplexity is 159.09110628356456
At time: 925.0043568611145 and batch: 1150, loss is 5.103504905700683 and perplexity is 164.59779724827897
At time: 926.2095477581024 and batch: 1200, loss is 5.12955638885498 and perplexity is 168.94215679567546
At time: 927.4141595363617 and batch: 1250, loss is 5.175877027511596 and perplexity is 176.95173778495908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189068648066834 and perplexity of 179.30148236094303
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 930.353836774826 and batch: 50, loss is 5.181344881057739 and perplexity is 177.92193400011442
At time: 931.5879619121552 and batch: 100, loss is 5.199229755401611 and perplexity is 181.13267166850824
At time: 932.7983677387238 and batch: 150, loss is 5.1381424713134765 and perplexity is 170.3989532220869
At time: 934.0087785720825 and batch: 200, loss is 5.168003988265991 and perplexity is 175.5640595991082
At time: 935.218945980072 and batch: 250, loss is 5.1833246803283695 and perplexity is 178.27453263736885
At time: 936.4295990467072 and batch: 300, loss is 5.169930057525635 and perplexity is 175.90253399532736
At time: 937.639595746994 and batch: 350, loss is 5.177848033905029 and perplexity is 177.30085473430915
At time: 938.851006269455 and batch: 400, loss is 5.13801791191101 and perplexity is 170.37772975211152
At time: 940.0605819225311 and batch: 450, loss is 5.116531476974488 and perplexity is 166.75596845261865
At time: 941.2694761753082 and batch: 500, loss is 5.103959426879883 and perplexity is 164.67262743784266
At time: 942.4802258014679 and batch: 550, loss is 5.105824365615844 and perplexity is 164.98001834301505
At time: 943.695769071579 and batch: 600, loss is 5.1364496803283695 and perplexity is 170.11074741511896
At time: 944.9087088108063 and batch: 650, loss is 5.143960189819336 and perplexity is 171.3931756144415
At time: 946.1183059215546 and batch: 700, loss is 5.168706016540527 and perplexity is 175.68735380588498
At time: 947.3257985115051 and batch: 750, loss is 5.134322690963745 and perplexity is 169.7493081895182
At time: 948.5324976444244 and batch: 800, loss is 5.141610746383667 and perplexity is 170.9909697080997
At time: 949.7396275997162 and batch: 850, loss is 5.157867927551269 and perplexity is 173.79351993913826
At time: 950.9474360942841 and batch: 900, loss is 5.1371635055541995 and perplexity is 170.23222010778684
At time: 952.1867558956146 and batch: 950, loss is 5.132908039093017 and perplexity is 169.50934178761204
At time: 953.3939187526703 and batch: 1000, loss is 5.113799095153809 and perplexity is 166.30094940191435
At time: 954.6016707420349 and batch: 1050, loss is 5.111076803207397 and perplexity is 165.84884532552712
At time: 955.8089270591736 and batch: 1100, loss is 5.06935411453247 and perplexity is 159.07155215253619
At time: 957.0225973129272 and batch: 1150, loss is 5.103367586135864 and perplexity is 164.57519630220213
At time: 958.2299180030823 and batch: 1200, loss is 5.129398765563965 and perplexity is 168.91552967551246
At time: 959.437837600708 and batch: 1250, loss is 5.175740900039673 and perplexity is 176.9276514316864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.189020978273266 and perplexity of 179.29293530001246
Finished 30 epochs...
Completing Train Step...
At time: 962.4377827644348 and batch: 50, loss is 5.181268177032471 and perplexity is 177.90828719498225
At time: 963.6456341743469 and batch: 100, loss is 5.199173316955567 and perplexity is 181.1224491104667
At time: 964.8580710887909 and batch: 150, loss is 5.138031253814697 and perplexity is 170.38000293053648
At time: 966.0686612129211 and batch: 200, loss is 5.167897138595581 and perplexity is 175.54530163936292
At time: 967.2763624191284 and batch: 250, loss is 5.183264198303223 and perplexity is 178.26375055866717
At time: 968.4846823215485 and batch: 300, loss is 5.169909315109253 and perplexity is 175.8988853895652
At time: 969.69269323349 and batch: 350, loss is 5.177787160873413 and perplexity is 177.29006222226312
At time: 970.9037098884583 and batch: 400, loss is 5.1379859828948975 and perplexity is 170.3722898456789
At time: 972.1144304275513 and batch: 450, loss is 5.116494522094727 and perplexity is 166.74980611971992
At time: 973.322826385498 and batch: 500, loss is 5.103932781219482 and perplexity is 164.6682396853922
At time: 974.5309369564056 and batch: 550, loss is 5.105803651809692 and perplexity is 164.9766010142892
At time: 975.7395026683807 and batch: 600, loss is 5.136444578170776 and perplexity is 170.10987948549152
At time: 976.9447524547577 and batch: 650, loss is 5.143964624404907 and perplexity is 171.39393567383027
At time: 978.1524517536163 and batch: 700, loss is 5.168701114654541 and perplexity is 175.68649260861818
At time: 979.3614842891693 and batch: 750, loss is 5.134318866729736 and perplexity is 169.74865902968216
At time: 980.5693612098694 and batch: 800, loss is 5.141619749069214 and perplexity is 170.9925090929607
At time: 981.8050940036774 and batch: 850, loss is 5.157873201370239 and perplexity is 173.79443649711746
At time: 983.0127205848694 and batch: 900, loss is 5.137164354324341 and perplexity is 170.23236459587366
At time: 984.2209365367889 and batch: 950, loss is 5.132911911010742 and perplexity is 169.50999811510772
At time: 985.4399607181549 and batch: 1000, loss is 5.11380443572998 and perplexity is 166.30183754717362
At time: 986.6474206447601 and batch: 1050, loss is 5.11111047744751 and perplexity is 165.8544302534006
At time: 987.8559327125549 and batch: 1100, loss is 5.069415254592895 and perplexity is 159.081278094166
At time: 989.0712592601776 and batch: 1150, loss is 5.103423318862915 and perplexity is 164.58436878229833
At time: 990.2806785106659 and batch: 1200, loss is 5.129439249038696 and perplexity is 168.9223681015104
At time: 991.4886960983276 and batch: 1250, loss is 5.175753631591797 and perplexity is 176.9299040096421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188985782818203 and perplexity of 179.2866251146105
Finished 31 epochs...
Completing Train Step...
At time: 994.4593126773834 and batch: 50, loss is 5.181213598251343 and perplexity is 177.8985774424903
At time: 995.6960415840149 and batch: 100, loss is 5.199115161895752 and perplexity is 181.111916229878
At time: 996.90465092659 and batch: 150, loss is 5.137961692810059 and perplexity is 170.36815153856452
At time: 998.1129097938538 and batch: 200, loss is 5.167827205657959 and perplexity is 175.5330256699858
At time: 999.321551322937 and batch: 250, loss is 5.1832068729400635 and perplexity is 178.25353181732754
At time: 1000.5279331207275 and batch: 300, loss is 5.169863691329956 and perplexity is 175.8908604007062
At time: 1001.7377243041992 and batch: 350, loss is 5.177741003036499 and perplexity is 177.28187908534406
At time: 1002.9470989704132 and batch: 400, loss is 5.137953252792358 and perplexity is 170.36671363441786
At time: 1004.1563985347748 and batch: 450, loss is 5.116462440490722 and perplexity is 166.74445660428316
At time: 1005.365953207016 and batch: 500, loss is 5.1039036273956295 and perplexity is 164.66343904651708
At time: 1006.5754516124725 and batch: 550, loss is 5.105786380767822 and perplexity is 164.97375172111063
At time: 1007.8002505302429 and batch: 600, loss is 5.136447372436524 and perplexity is 170.1103548183652
At time: 1009.0085823535919 and batch: 650, loss is 5.143982639312744 and perplexity is 171.39702334759735
At time: 1010.215455532074 and batch: 700, loss is 5.16872272491455 and perplexity is 175.69028928042692
At time: 1011.4197533130646 and batch: 750, loss is 5.134341487884521 and perplexity is 169.75249898380454
At time: 1012.6792750358582 and batch: 800, loss is 5.14163197517395 and perplexity is 170.99459967806573
At time: 1013.8930385112762 and batch: 850, loss is 5.1578755378723145 and perplexity is 173.79484256865342
At time: 1015.1018896102905 and batch: 900, loss is 5.137161102294922 and perplexity is 170.23181099611608
At time: 1016.3081889152527 and batch: 950, loss is 5.132916498184204 and perplexity is 169.51077568865597
At time: 1017.5153670310974 and batch: 1000, loss is 5.113806943893433 and perplexity is 166.3022546598878
At time: 1018.7227458953857 and batch: 1050, loss is 5.111143751144409 and perplexity is 165.8599489352552
At time: 1019.9309952259064 and batch: 1100, loss is 5.069461479187011 and perplexity is 159.0886317316355
At time: 1021.1390657424927 and batch: 1150, loss is 5.103466901779175 and perplexity is 164.5915420053745
At time: 1022.3445072174072 and batch: 1200, loss is 5.129473266601562 and perplexity is 168.92811452652592
At time: 1023.5509266853333 and batch: 1250, loss is 5.1757646179199215 and perplexity is 176.93184783030028
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188969744382984 and perplexity of 179.28374966074693
Finished 32 epochs...
Completing Train Step...
At time: 1026.509070634842 and batch: 50, loss is 5.181166429519653 and perplexity is 177.89018639012204
At time: 1027.7444586753845 and batch: 100, loss is 5.199060716629028 and perplexity is 181.10205581172104
At time: 1028.9652788639069 and batch: 150, loss is 5.137902536392212 and perplexity is 170.35807346709854
At time: 1030.184544801712 and batch: 200, loss is 5.167766962051392 and perplexity is 175.5224512459717
At time: 1031.4098808765411 and batch: 250, loss is 5.183152465820313 and perplexity is 178.2438338198984
At time: 1032.6323573589325 and batch: 300, loss is 5.169817495346069 and perplexity is 175.88273513703194
At time: 1033.8450841903687 and batch: 350, loss is 5.177697324752808 and perplexity is 177.2741358862421
At time: 1035.057429075241 and batch: 400, loss is 5.137920207977295 and perplexity is 170.36108399088872
At time: 1036.270703792572 and batch: 450, loss is 5.116432466506958 and perplexity is 166.73945868355227
At time: 1037.4861104488373 and batch: 500, loss is 5.103876686096191 and perplexity is 164.65900285925775
At time: 1038.6996545791626 and batch: 550, loss is 5.105770807266236 and perplexity is 164.9711825221323
At time: 1039.9151384830475 and batch: 600, loss is 5.136450471878052 and perplexity is 170.1108820662804
At time: 1041.1332416534424 and batch: 650, loss is 5.144001398086548 and perplexity is 171.4002385757457
At time: 1042.3501987457275 and batch: 700, loss is 5.168747501373291 and perplexity is 175.6946423175566
At time: 1043.5925872325897 and batch: 750, loss is 5.134368200302124 and perplexity is 169.75703354401068
At time: 1044.8196890354156 and batch: 800, loss is 5.14164361000061 and perplexity is 170.99658918216656
At time: 1046.0448393821716 and batch: 850, loss is 5.157876806259155 and perplexity is 173.79506300788455
At time: 1047.2611072063446 and batch: 900, loss is 5.137156467437745 and perplexity is 170.23102199781354
At time: 1048.4815661907196 and batch: 950, loss is 5.132920732498169 and perplexity is 169.51149345202032
At time: 1049.6978569030762 and batch: 1000, loss is 5.113808689117431 and perplexity is 166.30254489482684
At time: 1050.9112722873688 and batch: 1050, loss is 5.111173496246338 and perplexity is 165.86488252971702
At time: 1052.1255173683167 and batch: 1100, loss is 5.069502239227295 and perplexity is 159.09511632282872
At time: 1053.350866317749 and batch: 1150, loss is 5.103507108688355 and perplexity is 164.59815985559644
At time: 1054.574098110199 and batch: 1200, loss is 5.129505348205567 and perplexity is 168.93353409833517
At time: 1055.797547340393 and batch: 1250, loss is 5.1757750320434575 and perplexity is 176.93369043001556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1889612796532845 and perplexity of 179.28223207868953
Finished 33 epochs...
Completing Train Step...
At time: 1059.0237181186676 and batch: 50, loss is 5.18112380027771 and perplexity is 177.88260322796057
At time: 1060.232139825821 and batch: 100, loss is 5.199009046554566 and perplexity is 181.0926984967606
At time: 1061.4437937736511 and batch: 150, loss is 5.137848491668701 and perplexity is 170.34886676090937
At time: 1062.6580169200897 and batch: 200, loss is 5.1677109909057615 and perplexity is 175.51262732822215
At time: 1063.8730006217957 and batch: 250, loss is 5.183100423812866 and perplexity is 178.23455789434243
At time: 1065.0880916118622 and batch: 300, loss is 5.169772787094116 and perplexity is 175.8748719031723
At time: 1066.3048400878906 and batch: 350, loss is 5.177654638290405 and perplexity is 177.2665688420119
At time: 1067.5292510986328 and batch: 400, loss is 5.1378873920440675 and perplexity is 170.35549352466055
At time: 1068.7409806251526 and batch: 450, loss is 5.116403579711914 and perplexity is 166.73464218455047
At time: 1069.9640891551971 and batch: 500, loss is 5.1038514995574955 and perplexity is 164.65485572113693
At time: 1071.1802067756653 and batch: 550, loss is 5.105756340026855 and perplexity is 164.96879586180808
At time: 1072.3977415561676 and batch: 600, loss is 5.136453657150269 and perplexity is 170.11142391660977
At time: 1073.6219699382782 and batch: 650, loss is 5.144020118713379 and perplexity is 171.40344732568565
At time: 1074.893862247467 and batch: 700, loss is 5.168772926330567 and perplexity is 175.69910940311857
At time: 1076.108163356781 and batch: 750, loss is 5.134395751953125 and perplexity is 169.76171069498528
At time: 1077.3214452266693 and batch: 800, loss is 5.141654205322266 and perplexity is 170.9984009556291
At time: 1078.5348012447357 and batch: 850, loss is 5.157877588272095 and perplexity is 173.79519891792583
At time: 1079.7513091564178 and batch: 900, loss is 5.137151651382446 and perplexity is 170.23020215777228
At time: 1080.967541217804 and batch: 950, loss is 5.132924947738648 and perplexity is 169.51220798523505
At time: 1082.1810421943665 and batch: 1000, loss is 5.113810014724732 and perplexity is 166.30276534684054
At time: 1083.392838716507 and batch: 1050, loss is 5.111200637817383 and perplexity is 165.869384424304
At time: 1084.6118023395538 and batch: 1100, loss is 5.069539623260498 and perplexity is 159.10106405111418
At time: 1085.8301270008087 and batch: 1150, loss is 5.10354549407959 and perplexity is 164.60447814162328
At time: 1087.0499436855316 and batch: 1200, loss is 5.129535856246949 and perplexity is 168.93868800820172
At time: 1088.262324333191 and batch: 1250, loss is 5.1757853317260745 and perplexity is 176.93551280025613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188956824532391 and perplexity of 179.28143335645075
Finished 34 epochs...
Completing Train Step...
At time: 1091.3076267242432 and batch: 50, loss is 5.181084232330322 and perplexity is 177.87556491772148
At time: 1092.5602006912231 and batch: 100, loss is 5.198959646224975 and perplexity is 181.0837526787334
At time: 1093.7744400501251 and batch: 150, loss is 5.137797946929932 and perplexity is 170.34025673953673
At time: 1094.9876737594604 and batch: 200, loss is 5.167658214569092 and perplexity is 175.5033646591395
At time: 1096.200358390808 and batch: 250, loss is 5.183050384521485 and perplexity is 178.22563938650546
At time: 1097.4195766448975 and batch: 300, loss is 5.16972993850708 and perplexity is 175.86733607486707
At time: 1098.6353673934937 and batch: 350, loss is 5.177612648010254 and perplexity is 177.25912552539944
At time: 1099.8487043380737 and batch: 400, loss is 5.137855291366577 and perplexity is 170.350025085675
At time: 1101.0621383190155 and batch: 450, loss is 5.1163757705688475 and perplexity is 166.7300055015032
At time: 1102.2819519042969 and batch: 500, loss is 5.103827791213989 and perplexity is 164.6509520735322
At time: 1103.4990181922913 and batch: 550, loss is 5.105742559432984 and perplexity is 164.96652250949495
At time: 1104.7584867477417 and batch: 600, loss is 5.1364569664001465 and perplexity is 170.11198685875002
At time: 1105.9818992614746 and batch: 650, loss is 5.144038543701172 and perplexity is 171.40660546120455
At time: 1107.1921153068542 and batch: 700, loss is 5.168798580169677 and perplexity is 175.7036168176191
At time: 1108.3997514247894 and batch: 750, loss is 5.134423236846924 and perplexity is 169.7663766416961
At time: 1109.6066536903381 and batch: 800, loss is 5.141664247512818 and perplexity is 171.00011816277785
At time: 1110.8180735111237 and batch: 850, loss is 5.157877883911133 and perplexity is 173.7952502985788
At time: 1112.0352945327759 and batch: 900, loss is 5.137146215438843 and perplexity is 170.22927679850895
At time: 1113.2534697055817 and batch: 950, loss is 5.132928838729859 and perplexity is 169.51286755702967
At time: 1114.4668536186218 and batch: 1000, loss is 5.113811197280884 and perplexity is 166.3029620093152
At time: 1115.6804373264313 and batch: 1050, loss is 5.111225595474243 and perplexity is 165.8735241871433
At time: 1116.8947925567627 and batch: 1100, loss is 5.069574766159057 and perplexity is 159.10665542191668
At time: 1118.1108119487762 and batch: 1150, loss is 5.103582601547242 and perplexity is 164.61058631029985
At time: 1119.331975221634 and batch: 1200, loss is 5.129565305709839 and perplexity is 168.9436632350835
At time: 1120.5480077266693 and batch: 1250, loss is 5.175795278549194 and perplexity is 176.93727275525856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188952814923586 and perplexity of 179.28071450947814
Finished 35 epochs...
Completing Train Step...
At time: 1123.5849900245667 and batch: 50, loss is 5.181046733856201 and perplexity is 177.8688949805106
At time: 1124.800668478012 and batch: 100, loss is 5.198912134170532 and perplexity is 181.075149222003
At time: 1126.0202550888062 and batch: 150, loss is 5.137750062942505 and perplexity is 170.3321003641062
At time: 1127.2316455841064 and batch: 200, loss is 5.167607412338257 and perplexity is 175.4944489231674
At time: 1128.443942308426 and batch: 250, loss is 5.183001976013184 and perplexity is 178.217011957984
At time: 1129.6555478572845 and batch: 300, loss is 5.16968825340271 and perplexity is 175.86000517940315
At time: 1130.872215270996 and batch: 350, loss is 5.177571687698364 and perplexity is 177.25186508502856
At time: 1132.0884454250336 and batch: 400, loss is 5.137823581695557 and perplexity is 170.344623428064
At time: 1133.2999258041382 and batch: 450, loss is 5.1163488864898685 and perplexity is 166.72552317911897
At time: 1134.511566400528 and batch: 500, loss is 5.1038054180145265 and perplexity is 164.64726834614825
At time: 1135.7545669078827 and batch: 550, loss is 5.105729351043701 and perplexity is 164.9643435818371
At time: 1136.9669823646545 and batch: 600, loss is 5.1364602375030515 and perplexity is 170.1125433134745
At time: 1138.1822094917297 and batch: 650, loss is 5.144056663513184 and perplexity is 171.409711344812
At time: 1139.3934252262115 and batch: 700, loss is 5.168823776245117 and perplexity is 175.70804391497592
At time: 1140.6041615009308 and batch: 750, loss is 5.134450168609619 and perplexity is 169.77094881103338
At time: 1141.8156642913818 and batch: 800, loss is 5.141673698425293 and perplexity is 171.00173427756465
At time: 1143.0312309265137 and batch: 850, loss is 5.157877759933472 and perplexity is 173.79522875185148
At time: 1144.2477605342865 and batch: 900, loss is 5.137140846252441 and perplexity is 170.2283628082444
At time: 1145.4594428539276 and batch: 950, loss is 5.1329323673248295 and perplexity is 169.51346570033695
At time: 1146.6719236373901 and batch: 1000, loss is 5.113812217712402 and perplexity is 166.30313171018574
At time: 1147.884259223938 and batch: 1050, loss is 5.111248779296875 and perplexity is 165.87736981408528
At time: 1149.0978209972382 and batch: 1100, loss is 5.069608421325683 and perplexity is 159.1120102730249
At time: 1150.3142211437225 and batch: 1150, loss is 5.10361870765686 and perplexity is 164.61652986547224
At time: 1151.5272817611694 and batch: 1200, loss is 5.129593982696533 and perplexity is 168.94850809973383
At time: 1152.738918542862 and batch: 1250, loss is 5.175804901123047 and perplexity is 176.93897535542456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188951923899407 and perplexity of 179.28055476609788
Finished 36 epochs...
Completing Train Step...
At time: 1155.7397255897522 and batch: 50, loss is 5.1810109996795655 and perplexity is 177.86253909556123
At time: 1156.9849438667297 and batch: 100, loss is 5.198865785598755 and perplexity is 181.06675684194101
At time: 1158.1985142230988 and batch: 150, loss is 5.137704000473023 and perplexity is 170.32425462762973
At time: 1159.4116659164429 and batch: 200, loss is 5.167558193206787 and perplexity is 175.48581145137976
At time: 1160.6242299079895 and batch: 250, loss is 5.18295503616333 and perplexity is 178.20864667453534
At time: 1161.8390221595764 and batch: 300, loss is 5.169647951126098 and perplexity is 175.85291776364988
At time: 1163.0572974681854 and batch: 350, loss is 5.177531099319458 and perplexity is 177.24467086516842
At time: 1164.2733585834503 and batch: 400, loss is 5.137792358398437 and perplexity is 170.33930479030718
At time: 1165.4918658733368 and batch: 450, loss is 5.1163227748870845 and perplexity is 166.7211697653213
At time: 1166.73064661026 and batch: 500, loss is 5.103783769607544 and perplexity is 164.6437040336555
At time: 1167.9488978385925 and batch: 550, loss is 5.105716505050659 and perplexity is 164.96222446463844
At time: 1169.166265487671 and batch: 600, loss is 5.1364638614654545 and perplexity is 170.11315979605283
At time: 1170.3812863826752 and batch: 650, loss is 5.144074192047119 and perplexity is 171.4127159320871
At time: 1171.595267534256 and batch: 700, loss is 5.168849248886108 and perplexity is 175.71251971990287
At time: 1172.80579829216 and batch: 750, loss is 5.134476919174194 and perplexity is 169.77549034050648
At time: 1174.013145685196 and batch: 800, loss is 5.141682586669922 and perplexity is 171.00325418956558
At time: 1175.2204723358154 and batch: 850, loss is 5.157877426147461 and perplexity is 173.79517074144513
At time: 1176.4306042194366 and batch: 900, loss is 5.137135324478149 and perplexity is 170.227422848242
At time: 1177.6488683223724 and batch: 950, loss is 5.1329360198974605 and perplexity is 169.51408486171312
At time: 1178.8682653903961 and batch: 1000, loss is 5.113813219070434 and perplexity is 166.3032982392458
At time: 1180.0827162265778 and batch: 1050, loss is 5.111270599365234 and perplexity is 165.88098930912255
At time: 1181.2973673343658 and batch: 1100, loss is 5.0696407985687255 and perplexity is 159.1171619646508
At time: 1182.5180790424347 and batch: 1150, loss is 5.103653554916382 and perplexity is 164.62226640036067
At time: 1183.73273563385 and batch: 1200, loss is 5.129621629714966 and perplexity is 168.9531790868206
At time: 1184.9484581947327 and batch: 1250, loss is 5.175814323425293 and perplexity is 176.94064253578378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188951032875228 and perplexity of 179.28039502285995
Finished 37 epochs...
Completing Train Step...
At time: 1187.9297077655792 and batch: 50, loss is 5.180976877212524 and perplexity is 177.85647009047838
At time: 1189.168663740158 and batch: 100, loss is 5.1988207054138185 and perplexity is 181.05859450303805
At time: 1190.3809823989868 and batch: 150, loss is 5.137659759521484 and perplexity is 170.316719487217
At time: 1191.6052837371826 and batch: 200, loss is 5.167510499954224 and perplexity is 175.47744216183384
At time: 1192.8229806423187 and batch: 250, loss is 5.182909183502197 and perplexity is 178.20047552118442
At time: 1194.0392577648163 and batch: 300, loss is 5.169608535766602 and perplexity is 175.84598659427593
At time: 1195.2523684501648 and batch: 350, loss is 5.177491054534912 and perplexity is 177.23757328262337
At time: 1196.5096600055695 and batch: 400, loss is 5.137761487960815 and perplexity is 170.33404642258853
At time: 1197.7234601974487 and batch: 450, loss is 5.116297111511231 and perplexity is 166.71689119218038
At time: 1198.9413433074951 and batch: 500, loss is 5.103762855529785 and perplexity is 164.6402606984341
At time: 1200.153697013855 and batch: 550, loss is 5.105703945159912 and perplexity is 164.9601525701332
At time: 1201.3649895191193 and batch: 600, loss is 5.136467323303223 and perplexity is 170.11374870123367
At time: 1202.5827023983002 and batch: 650, loss is 5.144091548919678 and perplexity is 171.4156911465726
At time: 1203.801326751709 and batch: 700, loss is 5.168874483108521 and perplexity is 175.71695374465057
At time: 1205.0226678848267 and batch: 750, loss is 5.1345030879974365 and perplexity is 169.77993322343636
At time: 1206.240800857544 and batch: 800, loss is 5.141690807342529 and perplexity is 171.00465995711127
At time: 1207.452400445938 and batch: 850, loss is 5.157876815795898 and perplexity is 173.7950646653234
At time: 1208.667870759964 and batch: 900, loss is 5.13712968826294 and perplexity is 170.22646341255614
At time: 1209.8854897022247 and batch: 950, loss is 5.1329394054412845 and perplexity is 169.51465876004767
At time: 1211.12087225914 and batch: 1000, loss is 5.113814029693604 and perplexity is 166.30343304860716
At time: 1212.3409614562988 and batch: 1050, loss is 5.111291561126709 and perplexity is 165.88446650329763
At time: 1213.5601301193237 and batch: 1100, loss is 5.069671974182129 and perplexity is 159.1221226171034
At time: 1214.77397274971 and batch: 1150, loss is 5.1036878681182865 and perplexity is 164.62791521433957
At time: 1216.0014226436615 and batch: 1200, loss is 5.129648818969726 and perplexity is 168.95777286029974
At time: 1217.2180480957031 and batch: 1250, loss is 5.175823345184326 and perplexity is 176.94223885882474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18894925082687 and perplexity of 179.2800755368111
Finished 38 epochs...
Completing Train Step...
At time: 1220.2878246307373 and batch: 50, loss is 5.1809438037872315 and perplexity is 177.85058786507523
At time: 1221.5050132274628 and batch: 100, loss is 5.1987764930725096 and perplexity is 181.0505896556188
At time: 1222.7219638824463 and batch: 150, loss is 5.137617120742798 and perplexity is 170.30945754512945
At time: 1223.9394755363464 and batch: 200, loss is 5.16746374130249 and perplexity is 175.46923726505514
At time: 1225.1652212142944 and batch: 250, loss is 5.182864103317261 and perplexity is 178.19244239186108
At time: 1226.381643295288 and batch: 300, loss is 5.169570140838623 and perplexity is 175.83923512989702
At time: 1227.639149904251 and batch: 350, loss is 5.1774517440795895 and perplexity is 177.23060612985915
At time: 1228.8534591197968 and batch: 400, loss is 5.137730932235717 and perplexity is 170.32884182180683
At time: 1230.0731098651886 and batch: 450, loss is 5.1162721538543705 and perplexity is 166.71273038113947
At time: 1231.2868933677673 and batch: 500, loss is 5.103742780685425 and perplexity is 164.63695560399992
At time: 1232.5013420581818 and batch: 550, loss is 5.105691881179809 and perplexity is 164.9581625061389
At time: 1233.720502614975 and batch: 600, loss is 5.13647081375122 and perplexity is 170.1143424754634
At time: 1234.944138288498 and batch: 650, loss is 5.1441085910797115 and perplexity is 171.4186124651061
At time: 1236.1619148254395 and batch: 700, loss is 5.168899154663086 and perplexity is 175.72128900854145
At time: 1237.3775517940521 and batch: 750, loss is 5.134528970718383 and perplexity is 169.78432764693994
At time: 1238.5916409492493 and batch: 800, loss is 5.141699056625367 and perplexity is 171.00607062873627
At time: 1239.808334827423 and batch: 850, loss is 5.157875967025757 and perplexity is 173.7949171533244
At time: 1241.0241191387177 and batch: 900, loss is 5.137124319076538 and perplexity is 170.22554943739715
At time: 1242.2412040233612 and batch: 950, loss is 5.13294282913208 and perplexity is 169.5152391268181
At time: 1243.4554934501648 and batch: 1000, loss is 5.1138150596618654 and perplexity is 166.30360433595322
At time: 1244.6680691242218 and batch: 1050, loss is 5.111311626434326 and perplexity is 165.88779505954108
At time: 1245.8806846141815 and batch: 1100, loss is 5.069702501296997 and perplexity is 159.12698023056265
At time: 1247.0974955558777 and batch: 1150, loss is 5.103721370697022 and perplexity is 164.63343076642315
At time: 1248.3158462047577 and batch: 1200, loss is 5.129675216674805 and perplexity is 168.96223301662715
At time: 1249.5341312885284 and batch: 1250, loss is 5.175832004547119 and perplexity is 176.9437710724984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1889483598026915 and perplexity of 179.27991579400015
Finished 39 epochs...
Completing Train Step...
At time: 1252.6018221378326 and batch: 50, loss is 5.180911588668823 and perplexity is 177.84485847961506
At time: 1253.838154554367 and batch: 100, loss is 5.198733205795288 and perplexity is 181.04275263817607
At time: 1255.0429141521454 and batch: 150, loss is 5.137575635910034 and perplexity is 170.30239243231364
At time: 1256.2464172840118 and batch: 200, loss is 5.1674181079864505 and perplexity is 175.46123020459154
At time: 1257.4521896839142 and batch: 250, loss is 5.182820129394531 and perplexity is 178.18460674345167
At time: 1258.7082130908966 and batch: 300, loss is 5.16953221321106 and perplexity is 175.83256609134725
At time: 1259.922434091568 and batch: 350, loss is 5.1774129390716555 and perplexity is 177.22372882821986
At time: 1261.284919977188 and batch: 400, loss is 5.13770097732544 and perplexity is 170.32373971304935
At time: 1262.4975550174713 and batch: 450, loss is 5.116247653961182 and perplexity is 166.70864598708573
At time: 1263.7166969776154 and batch: 500, loss is 5.10372314453125 and perplexity is 164.63372279909672
At time: 1264.9345381259918 and batch: 550, loss is 5.105679559707641 and perplexity is 164.95612999125254
At time: 1266.1484632492065 and batch: 600, loss is 5.136474237442017 and perplexity is 170.1149248953691
At time: 1267.361837387085 and batch: 650, loss is 5.144125328063965 and perplexity is 171.4214815197333
At time: 1268.5752012729645 and batch: 700, loss is 5.168923950195312 and perplexity is 175.72564616544474
At time: 1269.7871716022491 and batch: 750, loss is 5.134554290771485 and perplexity is 169.788626649557
At time: 1271.0049486160278 and batch: 800, loss is 5.141706628799438 and perplexity is 171.00736552137295
At time: 1272.2218787670135 and batch: 850, loss is 5.157874813079834 and perplexity is 173.79471660350407
At time: 1273.44002866745 and batch: 900, loss is 5.13711916923523 and perplexity is 170.22467280508823
At time: 1274.6620388031006 and batch: 950, loss is 5.132946214675903 and perplexity is 169.51581302906033
At time: 1275.8749823570251 and batch: 1000, loss is 5.11381576538086 and perplexity is 166.30372169960705
At time: 1277.088535308838 and batch: 1050, loss is 5.111330871582031 and perplexity is 165.89098762538
At time: 1278.302574634552 and batch: 1100, loss is 5.069732122421264 and perplexity is 159.1316938204289
At time: 1279.516860961914 and batch: 1150, loss is 5.103754272460938 and perplexity is 164.63884758580588
At time: 1280.7389981746674 and batch: 1200, loss is 5.129701251983643 and perplexity is 168.96663205781059
At time: 1281.9547901153564 and batch: 1250, loss is 5.175840482711792 and perplexity is 176.94527123728682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1889483598026915 and perplexity of 179.27991579400015
Finished 40 epochs...
Completing Train Step...
At time: 1284.9950301647186 and batch: 50, loss is 5.180880317687988 and perplexity is 177.83929718340798
At time: 1286.2458801269531 and batch: 100, loss is 5.198690576553345 and perplexity is 181.03503508736958
At time: 1287.4638772010803 and batch: 150, loss is 5.137535276412964 and perplexity is 170.2955192521053
At time: 1288.6789095401764 and batch: 200, loss is 5.167373218536377 and perplexity is 175.45335402323846
At time: 1289.9168527126312 and batch: 250, loss is 5.1827767467498775 and perplexity is 178.17687679164865
At time: 1291.1308019161224 and batch: 300, loss is 5.169495162963867 and perplexity is 175.82605157199202
At time: 1292.3451745510101 and batch: 350, loss is 5.177374610900879 and perplexity is 177.21693629704913
At time: 1293.571766614914 and batch: 400, loss is 5.137671070098877 and perplexity is 170.31864587854835
At time: 1294.7887299060822 and batch: 450, loss is 5.1162236785888675 and perplexity is 166.70464913314342
At time: 1296.0021958351135 and batch: 500, loss is 5.103704147338867 and perplexity is 164.63059525029942
At time: 1297.2176475524902 and batch: 550, loss is 5.105667791366577 and perplexity is 164.9541887426768
At time: 1298.4305589199066 and batch: 600, loss is 5.136477584838867 and perplexity is 170.115494338486
At time: 1299.6474800109863 and batch: 650, loss is 5.144141778945923 and perplexity is 171.424301577487
At time: 1300.8627576828003 and batch: 700, loss is 5.168948526382446 and perplexity is 175.7299648848778
At time: 1302.0758273601532 and batch: 750, loss is 5.134579019546509 and perplexity is 169.79282536622142
At time: 1303.2880971431732 and batch: 800, loss is 5.141713962554932 and perplexity is 171.00861965217803
At time: 1304.5008563995361 and batch: 850, loss is 5.157873420715332 and perplexity is 173.7944746180785
At time: 1305.7160115242004 and batch: 900, loss is 5.1371137809753415 and perplexity is 170.22375559278288
At time: 1306.938675403595 and batch: 950, loss is 5.132949485778808 and perplexity is 169.5163675336357
At time: 1308.1545045375824 and batch: 1000, loss is 5.113816480636597 and perplexity is 166.30384064934069
At time: 1309.3661103248596 and batch: 1050, loss is 5.111349725723267 and perplexity is 165.89411538697598
At time: 1310.583899974823 and batch: 1100, loss is 5.069761142730713 and perplexity is 159.13631193843597
At time: 1311.798523426056 and batch: 1150, loss is 5.103786325454712 and perplexity is 164.6441248383379
At time: 1313.0161666870117 and batch: 1200, loss is 5.12972659111023 and perplexity is 168.9709135789341
At time: 1314.232349872589 and batch: 1250, loss is 5.175848770141601 and perplexity is 176.9467376648787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18894925082687 and perplexity of 179.2800755368111
Annealing...
Finished 41 epochs...
Completing Train Step...
At time: 1317.2865478992462 and batch: 50, loss is 5.180869493484497 and perplexity is 177.8373722250847
At time: 1318.51229596138 and batch: 100, loss is 5.198670978546143 and perplexity is 181.03148719621404
At time: 1319.7303926944733 and batch: 150, loss is 5.137514972686768 and perplexity is 170.2920616536111
At time: 1320.9498465061188 and batch: 200, loss is 5.167370004653931 and perplexity is 175.45279013768996
At time: 1322.1729011535645 and batch: 250, loss is 5.182744560241699 and perplexity is 178.17114199243863
At time: 1323.3993504047394 and batch: 300, loss is 5.169455223083496 and perplexity is 175.8190292407626
At time: 1324.6181218624115 and batch: 350, loss is 5.177328605651855 and perplexity is 177.208783575299
At time: 1325.8341827392578 and batch: 400, loss is 5.137633047103882 and perplexity is 170.3121699766459
At time: 1327.049903869629 and batch: 450, loss is 5.116217117309571 and perplexity is 166.70355534096873
At time: 1328.2704467773438 and batch: 500, loss is 5.103719501495362 and perplexity is 164.63312303362866
At time: 1329.4873616695404 and batch: 550, loss is 5.105633125305176 and perplexity is 164.94847052975604
At time: 1330.6992223262787 and batch: 600, loss is 5.136420774459839 and perplexity is 170.10583028728578
At time: 1331.908029794693 and batch: 650, loss is 5.144101734161377 and perplexity is 171.41743706570924
At time: 1333.1167314052582 and batch: 700, loss is 5.168944673538208 and perplexity is 175.7292878259994
At time: 1334.3230226039886 and batch: 750, loss is 5.134584093093872 and perplexity is 169.79368682034823
At time: 1335.5302848815918 and batch: 800, loss is 5.1416688156127925 and perplexity is 171.00089931019747
At time: 1336.7473006248474 and batch: 850, loss is 5.157808523178101 and perplexity is 173.78319615066775
At time: 1337.9608602523804 and batch: 900, loss is 5.137022018432617 and perplexity is 170.20813614478868
At time: 1339.175912618637 and batch: 950, loss is 5.132874841690064 and perplexity is 169.50371461109265
At time: 1340.3965711593628 and batch: 1000, loss is 5.113737754821777 and perplexity is 166.29074875931946
At time: 1341.6168761253357 and batch: 1050, loss is 5.111307163238525 and perplexity is 165.8870546714829
At time: 1342.8508694171906 and batch: 1100, loss is 5.0696776008605955 and perplexity is 159.12301794864308
At time: 1344.056851387024 and batch: 1150, loss is 5.103698320388794 and perplexity is 164.62963595883534
At time: 1345.267163991928 and batch: 1200, loss is 5.129646844863892 and perplexity is 168.9574393201038
At time: 1346.4843134880066 and batch: 1250, loss is 5.17579327583313 and perplexity is 176.9369184004949
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188939895072993 and perplexity of 179.27839824439542
Finished 42 epochs...
Completing Train Step...
At time: 1349.486071586609 and batch: 50, loss is 5.180853042602539 and perplexity is 177.83444666753056
At time: 1350.7233781814575 and batch: 100, loss is 5.198654623031616 and perplexity is 181.0285263573085
At time: 1351.943870306015 and batch: 150, loss is 5.137491273880005 and perplexity is 170.28802598276917
At time: 1353.1579313278198 and batch: 200, loss is 5.167344484329224 and perplexity is 175.4483125826495
At time: 1354.3909018039703 and batch: 250, loss is 5.182728128433228 and perplexity is 178.16821434241166
At time: 1355.6090683937073 and batch: 300, loss is 5.16944727897644 and perplexity is 175.81763252111972
At time: 1356.8264615535736 and batch: 350, loss is 5.177311010360718 and perplexity is 177.2056655625912
At time: 1358.0444819927216 and batch: 400, loss is 5.13762321472168 and perplexity is 170.31049541052957
At time: 1359.2593848705292 and batch: 450, loss is 5.116206541061401 and perplexity is 166.70179225212013
At time: 1360.472605228424 and batch: 500, loss is 5.1037142372131346 and perplexity is 164.63225636068628
At time: 1361.6854000091553 and batch: 550, loss is 5.105627756118775 and perplexity is 164.9475848930487
At time: 1362.9018449783325 and batch: 600, loss is 5.136419982910156 and perplexity is 170.1056956401231
At time: 1364.1162486076355 and batch: 650, loss is 5.144105014801025 and perplexity is 171.41799942547206
At time: 1365.3306086063385 and batch: 700, loss is 5.1689458847045895 and perplexity is 175.7295006635339
At time: 1366.5500116348267 and batch: 750, loss is 5.134585294723511 and perplexity is 169.79389084959732
At time: 1367.7639348506927 and batch: 800, loss is 5.141671171188355 and perplexity is 171.0013021162114
At time: 1368.9807748794556 and batch: 850, loss is 5.157809019088745 and perplexity is 173.78328233162586
At time: 1370.1984210014343 and batch: 900, loss is 5.1370226097106935 and perplexity is 170.20823678515777
At time: 1371.4152414798737 and batch: 950, loss is 5.13287582397461 and perplexity is 169.5038811120538
At time: 1372.6318218708038 and batch: 1000, loss is 5.113738660812378 and perplexity is 166.29089941724308
At time: 1373.8901562690735 and batch: 1050, loss is 5.1113153839111325 and perplexity is 165.88841838025448
At time: 1375.1058802604675 and batch: 1100, loss is 5.069695539474488 and perplexity is 159.12587242062597
At time: 1376.323023557663 and batch: 1150, loss is 5.103716163635254 and perplexity is 164.63257351221205
At time: 1377.535660982132 and batch: 1200, loss is 5.1296592044830325 and perplexity is 168.9595275826098
At time: 1378.7485342025757 and batch: 1250, loss is 5.175796403884887 and perplexity is 176.93747186919904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188930984831204 and perplexity of 179.2768008376362
Finished 43 epochs...
Completing Train Step...
At time: 1381.7949438095093 and batch: 50, loss is 5.180837907791138 and perplexity is 177.83175519708706
At time: 1383.0101585388184 and batch: 100, loss is 5.198637475967407 and perplexity is 181.0254222761565
At time: 1384.224889755249 and batch: 150, loss is 5.137471265792847 and perplexity is 170.28461887918832
At time: 1385.4464361667633 and batch: 200, loss is 5.167322225570679 and perplexity is 175.44440736448544
At time: 1386.6662459373474 and batch: 250, loss is 5.1827113056182865 and perplexity is 178.1652170767247
At time: 1387.8866448402405 and batch: 300, loss is 5.169435644149781 and perplexity is 175.81558692534176
At time: 1389.104008436203 and batch: 350, loss is 5.177294998168946 and perplexity is 177.20282813420783
At time: 1390.319465637207 and batch: 400, loss is 5.13761308670044 and perplexity is 170.30877051094953
At time: 1391.5340044498444 and batch: 450, loss is 5.116196546554566 and perplexity is 166.70012615824388
At time: 1392.750597000122 and batch: 500, loss is 5.103707904815674 and perplexity is 164.631213847105
At time: 1393.9762756824493 and batch: 550, loss is 5.105622262954712 and perplexity is 164.9466788113917
At time: 1395.2040383815765 and batch: 600, loss is 5.13642014503479 and perplexity is 170.10572321844893
At time: 1396.4197590351105 and batch: 650, loss is 5.144109897613525 and perplexity is 171.41883642946587
At time: 1397.63494515419 and batch: 700, loss is 5.168951663970947 and perplexity is 175.7305162540599
At time: 1398.8542330265045 and batch: 750, loss is 5.134591217041016 and perplexity is 169.79489642590698
At time: 1400.0759217739105 and batch: 800, loss is 5.14167426109314 and perplexity is 171.00183049476945
At time: 1401.2917475700378 and batch: 850, loss is 5.1578092193603515 and perplexity is 173.78331713548644
At time: 1402.5074889659882 and batch: 900, loss is 5.137022209167481 and perplexity is 170.20816860941738
At time: 1403.7265281677246 and batch: 950, loss is 5.132876844406128 and perplexity is 169.50405407924475
At time: 1404.9749417304993 and batch: 1000, loss is 5.113739595413208 and perplexity is 166.2910548329284
At time: 1406.1933977603912 and batch: 1050, loss is 5.111324558258056 and perplexity is 165.8899403051367
At time: 1407.409899711609 and batch: 1100, loss is 5.0697104930877686 and perplexity is 159.12825194517634
At time: 1408.6250715255737 and batch: 1150, loss is 5.103731117248535 and perplexity is 164.63503538245666
At time: 1409.839778661728 and batch: 1200, loss is 5.129670076370239 and perplexity is 168.9613645015215
At time: 1411.056779384613 and batch: 1250, loss is 5.175799379348755 and perplexity is 176.93799834103663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188926529710311 and perplexity of 179.2760021395942
Finished 44 epochs...
Completing Train Step...
At time: 1414.072369813919 and batch: 50, loss is 5.1808233737945555 and perplexity is 177.82917060974705
At time: 1415.3185546398163 and batch: 100, loss is 5.198620071411133 and perplexity is 181.02227163642522
At time: 1416.5330276489258 and batch: 150, loss is 5.137453174591064 and perplexity is 170.28153825365382
At time: 1417.7374846935272 and batch: 200, loss is 5.167301778793335 and perplexity is 175.44082012842563
At time: 1418.9420790672302 and batch: 250, loss is 5.1826943588256835 and perplexity is 178.16219777332566
At time: 1420.1562099456787 and batch: 300, loss is 5.169422187805176 and perplexity is 175.81322110613488
At time: 1421.3779938220978 and batch: 350, loss is 5.1772795677185055 and perplexity is 177.20009383584627
At time: 1422.5962030887604 and batch: 400, loss is 5.137602767944336 and perplexity is 170.30701314535122
At time: 1423.8206613063812 and batch: 450, loss is 5.116186943054199 and perplexity is 166.69852526120835
At time: 1425.0334100723267 and batch: 500, loss is 5.103701515197754 and perplexity is 164.6301619199115
At time: 1426.2452692985535 and batch: 550, loss is 5.105616865158081 and perplexity is 164.94578846516748
At time: 1427.457549571991 and batch: 600, loss is 5.136420841217041 and perplexity is 170.1058416430755
At time: 1428.6698215007782 and batch: 650, loss is 5.144115686416626 and perplexity is 171.4198287422299
At time: 1429.8874006271362 and batch: 700, loss is 5.168959493637085 and perplexity is 175.73189217071882
At time: 1431.1042165756226 and batch: 750, loss is 5.134599180221557 and perplexity is 169.7962485387058
At time: 1432.3222303390503 and batch: 800, loss is 5.141677408218384 and perplexity is 171.0023686597938
At time: 1433.53635263443 and batch: 850, loss is 5.157809314727783 and perplexity is 173.7833337087558
At time: 1434.7949740886688 and batch: 900, loss is 5.137021389007568 and perplexity is 170.20802901155798
At time: 1436.008942604065 and batch: 950, loss is 5.132878217697144 and perplexity is 169.50428685779923
At time: 1437.2201130390167 and batch: 1000, loss is 5.113740062713623 and perplexity is 166.2911325408254
At time: 1438.4304113388062 and batch: 1050, loss is 5.111333503723144 and perplexity is 165.8914242744435
At time: 1439.6441040039062 and batch: 1100, loss is 5.069724006652832 and perplexity is 159.13040234969222
At time: 1440.8797755241394 and batch: 1150, loss is 5.103744535446167 and perplexity is 164.63724450271977
At time: 1442.1109533309937 and batch: 1200, loss is 5.129680128097534 and perplexity is 168.96306286361659
At time: 1443.3339297771454 and batch: 1250, loss is 5.175801982879639 and perplexity is 176.93845900517957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188922074589416 and perplexity of 179.27520344511035
Finished 45 epochs...
Completing Train Step...
At time: 1446.3539102077484 and batch: 50, loss is 5.180809621810913 and perplexity is 177.82672512271685
At time: 1447.564493894577 and batch: 100, loss is 5.19860273361206 and perplexity is 181.01913313585928
At time: 1448.775276184082 and batch: 150, loss is 5.137435998916626 and perplexity is 170.27861357850665
At time: 1449.9854474067688 and batch: 200, loss is 5.167282390594482 and perplexity is 175.43741867989218
At time: 1451.1959609985352 and batch: 250, loss is 5.18267744064331 and perplexity is 178.1591836182688
At time: 1452.4092497825623 and batch: 300, loss is 5.169408369064331 and perplexity is 175.81079160558167
At time: 1453.62433385849 and batch: 350, loss is 5.177264766693115 and perplexity is 177.1974711121678
At time: 1454.8372004032135 and batch: 400, loss is 5.137591991424561 and perplexity is 170.30517783834532
At time: 1456.0495212078094 and batch: 450, loss is 5.116177616119384 and perplexity is 166.6969704821802
At time: 1457.2598717212677 and batch: 500, loss is 5.103694715499878 and perplexity is 164.62904248835508
At time: 1458.4731476306915 and batch: 550, loss is 5.105611400604248 and perplexity is 164.9448871124897
At time: 1459.6881031990051 and batch: 600, loss is 5.136421394348145 and perplexity is 170.10593573393345
At time: 1460.8967788219452 and batch: 650, loss is 5.144121685028076 and perplexity is 171.4208570262615
At time: 1462.103003501892 and batch: 700, loss is 5.1689683532714845 and perplexity is 175.7334490979327
At time: 1463.3088068962097 and batch: 750, loss is 5.134608135223389 and perplexity is 169.79776907123065
At time: 1464.5148169994354 and batch: 800, loss is 5.141680736541748 and perplexity is 171.0029378119198
At time: 1465.752283334732 and batch: 850, loss is 5.157809066772461 and perplexity is 173.78329061825866
At time: 1466.9574587345123 and batch: 900, loss is 5.1370202255249025 and perplexity is 170.20783097758186
At time: 1468.1624019145966 and batch: 950, loss is 5.132879409790039 and perplexity is 169.5044889227758
At time: 1469.3674609661102 and batch: 1000, loss is 5.113740491867065 and perplexity is 166.2912039052527
At time: 1470.5726749897003 and batch: 1050, loss is 5.111342430114746 and perplexity is 165.89290509286914
At time: 1471.791665315628 and batch: 1100, loss is 5.069736490249634 and perplexity is 159.13238888187354
At time: 1473.0135414600372 and batch: 1150, loss is 5.103757333755493 and perplexity is 164.6393515945851
At time: 1474.2183873653412 and batch: 1200, loss is 5.129689817428589 and perplexity is 168.96470001060013
At time: 1475.4305157661438 and batch: 1250, loss is 5.175804624557495 and perplexity is 176.93892642020597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188919847028969 and perplexity of 179.27480409920278
Finished 46 epochs...
Completing Train Step...
At time: 1478.4047904014587 and batch: 50, loss is 5.180796346664429 and perplexity is 177.8243644625611
At time: 1479.6191110610962 and batch: 100, loss is 5.198585538864136 and perplexity is 181.01602058425541
At time: 1480.8226163387299 and batch: 150, loss is 5.137419567108155 and perplexity is 170.27581561592945
At time: 1482.026195049286 and batch: 200, loss is 5.167263746261597 and perplexity is 175.43414779674956
At time: 1483.2309079170227 and batch: 250, loss is 5.1826607036590575 and perplexity is 178.15620179577152
At time: 1484.43558883667 and batch: 300, loss is 5.169394016265869 and perplexity is 175.80826824683103
At time: 1485.6404678821564 and batch: 350, loss is 5.177249937057495 and perplexity is 177.1948433577228
At time: 1486.8481390476227 and batch: 400, loss is 5.137581415176392 and perplexity is 170.30337665804495
At time: 1488.0518019199371 and batch: 450, loss is 5.116168413162232 and perplexity is 166.69543638416246
At time: 1489.2558853626251 and batch: 500, loss is 5.1036882495880125 and perplexity is 164.62797801491726
At time: 1490.4598939418793 and batch: 550, loss is 5.1056060695648195 and perplexity is 164.94400778713683
At time: 1491.665880203247 and batch: 600, loss is 5.136422119140625 and perplexity is 170.10605902548122
At time: 1492.87086892128 and batch: 650, loss is 5.1441275501251225 and perplexity is 171.4218624291721
At time: 1494.0740749835968 and batch: 700, loss is 5.168977394104004 and perplexity is 175.73503788179593
At time: 1495.286192893982 and batch: 750, loss is 5.134617586135864 and perplexity is 169.79937382266775
At time: 1496.5367369651794 and batch: 800, loss is 5.141683959960938 and perplexity is 171.00348902695953
At time: 1497.7447986602783 and batch: 850, loss is 5.15780876159668 and perplexity is 173.78323758381538
At time: 1498.9595520496368 and batch: 900, loss is 5.137018613815307 and perplexity is 170.20755665220858
At time: 1500.1635270118713 and batch: 950, loss is 5.132880592346192 and perplexity is 169.5046893714706
At time: 1501.366314649582 and batch: 1000, loss is 5.113740844726562 and perplexity is 166.29126258269366
At time: 1502.5694177150726 and batch: 1050, loss is 5.11135124206543 and perplexity is 165.89436693940846
At time: 1503.7815980911255 and batch: 1100, loss is 5.069748239517212 and perplexity is 159.1342585818747
At time: 1504.996407032013 and batch: 1150, loss is 5.103769464492798 and perplexity is 164.64134880342306
At time: 1506.2109868526459 and batch: 1200, loss is 5.129699544906616 and perplexity is 168.96634361900098
At time: 1507.4231534004211 and batch: 1250, loss is 5.175807266235352 and perplexity is 176.9393938364673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18891895600479 and perplexity of 179.27464436108883
Finished 47 epochs...
Completing Train Step...
At time: 1510.3939208984375 and batch: 50, loss is 5.180783329010009 and perplexity is 177.82204962150414
At time: 1511.6254913806915 and batch: 100, loss is 5.1985687732696535 and perplexity is 181.0129857684998
At time: 1512.8365941047668 and batch: 150, loss is 5.137403478622437 and perplexity is 170.27307615793873
At time: 1514.055470943451 and batch: 200, loss is 5.167245779037476 and perplexity is 175.43099576041436
At time: 1515.2695021629333 and batch: 250, loss is 5.182643833160401 and perplexity is 178.15319623716113
At time: 1516.4817354679108 and batch: 300, loss is 5.1693798923492436 and perplexity is 175.80578516304368
At time: 1517.6951699256897 and batch: 350, loss is 5.177235202789307 and perplexity is 177.19223254061342
At time: 1518.9063341617584 and batch: 400, loss is 5.137570762634278 and perplexity is 170.3015625038156
At time: 1520.1164691448212 and batch: 450, loss is 5.1161594581604 and perplexity is 166.69394363290814
At time: 1521.3304135799408 and batch: 500, loss is 5.103682050704956 and perplexity is 164.6269575084967
At time: 1522.5430204868317 and batch: 550, loss is 5.105600814819336 and perplexity is 164.94314105063418
At time: 1523.7573800086975 and batch: 600, loss is 5.136422624588013 and perplexity is 170.10614500516613
At time: 1524.9686925411224 and batch: 650, loss is 5.1441334533691405 and perplexity is 171.42287437724295
At time: 1526.1771080493927 and batch: 700, loss is 5.168986549377442 and perplexity is 175.73664679148536
At time: 1527.4095273017883 and batch: 750, loss is 5.134627017974854 and perplexity is 169.80097535057487
At time: 1528.6169295310974 and batch: 800, loss is 5.141687173843383 and perplexity is 171.0040386129542
At time: 1529.8238246440887 and batch: 850, loss is 5.157808666229248 and perplexity is 173.78322101055505
At time: 1531.0322830677032 and batch: 900, loss is 5.13701714515686 and perplexity is 170.20730667562626
At time: 1532.2458386421204 and batch: 950, loss is 5.1328818893432615 and perplexity is 169.50490921869857
At time: 1533.458236694336 and batch: 1000, loss is 5.11374119758606 and perplexity is 166.2913212601553
At time: 1534.6668162345886 and batch: 1050, loss is 5.111359786987305 and perplexity is 165.8957844998699
At time: 1535.8756039142609 and batch: 1100, loss is 5.069759569168091 and perplexity is 159.13606152768062
At time: 1537.0843555927277 and batch: 1150, loss is 5.103781499862671 and perplexity is 164.6433303348765
At time: 1538.297064781189 and batch: 1200, loss is 5.129709024429321 and perplexity is 168.96794534688343
At time: 1539.5078384876251 and batch: 1250, loss is 5.175809755325317 and perplexity is 176.9398342550851
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188917619468522 and perplexity of 179.27440475418476
Finished 48 epochs...
Completing Train Step...
At time: 1542.4793167114258 and batch: 50, loss is 5.180770702362061 and perplexity is 177.8198043392613
At time: 1543.6856265068054 and batch: 100, loss is 5.198552141189575 and perplexity is 181.00997517106154
At time: 1544.890486240387 and batch: 150, loss is 5.137387781143189 and perplexity is 170.2704033208377
At time: 1546.0949194431305 and batch: 200, loss is 5.167227916717529 and perplexity is 175.42786218382608
At time: 1547.300112247467 and batch: 250, loss is 5.182627477645874 and perplexity is 178.15028247380022
At time: 1548.50590467453 and batch: 300, loss is 5.169365682601929 and perplexity is 175.80328702500904
At time: 1549.7111568450928 and batch: 350, loss is 5.17722071647644 and perplexity is 177.18966569708738
At time: 1550.9161713123322 and batch: 400, loss is 5.137560014724731 and perplexity is 170.2997321278626
At time: 1552.1211395263672 and batch: 450, loss is 5.116150598526001 and perplexity is 166.69246679205324
At time: 1553.3264331817627 and batch: 500, loss is 5.103675451278686 and perplexity is 164.6258710686136
At time: 1554.5325603485107 and batch: 550, loss is 5.105595645904541 and perplexity is 164.94228847579538
At time: 1555.7378063201904 and batch: 600, loss is 5.136423254013062 and perplexity is 170.1062520742685
At time: 1556.9432289600372 and batch: 650, loss is 5.144139442443848 and perplexity is 171.42390104471852
At time: 1558.1933908462524 and batch: 700, loss is 5.168995952606201 and perplexity is 175.73829929114592
At time: 1559.4015169143677 and batch: 750, loss is 5.134636650085449 and perplexity is 169.80261090022555
At time: 1560.6175293922424 and batch: 800, loss is 5.141690320968628 and perplexity is 171.00457678492793
At time: 1561.8322927951813 and batch: 850, loss is 5.157808046340943 and perplexity is 173.7831132844021
At time: 1563.0482168197632 and batch: 900, loss is 5.137015390396118 and perplexity is 170.20700800278854
At time: 1564.263159275055 and batch: 950, loss is 5.132882976531983 and perplexity is 169.50509350262425
At time: 1565.4717552661896 and batch: 1000, loss is 5.113741455078125 and perplexity is 166.29136407885653
At time: 1566.6769680976868 and batch: 1050, loss is 5.111368055343628 and perplexity is 165.89715619099948
At time: 1567.882640838623 and batch: 1100, loss is 5.069770641326905 and perplexity is 159.13782351718137
At time: 1569.088353395462 and batch: 1150, loss is 5.10379316329956 and perplexity is 164.64525065316784
At time: 1570.294861793518 and batch: 1200, loss is 5.1297186088562015 and perplexity is 168.9695648155616
At time: 1571.5029709339142 and batch: 1250, loss is 5.175812454223633 and perplexity is 176.9403117983501
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188916728444343 and perplexity of 179.27424501642665
Finished 49 epochs...
Completing Train Step...
At time: 1574.4994888305664 and batch: 50, loss is 5.1807583236694335 and perplexity is 177.81760317618412
At time: 1575.7343237400055 and batch: 100, loss is 5.198535604476929 and perplexity is 181.00698188586563
At time: 1576.9449219703674 and batch: 150, loss is 5.137372465133667 and perplexity is 170.26779547769004
At time: 1578.154103755951 and batch: 200, loss is 5.16721040725708 and perplexity is 175.42479056350288
At time: 1579.3742654323578 and batch: 250, loss is 5.182610788345337 and perplexity is 178.14730929500544
At time: 1580.5871658325195 and batch: 300, loss is 5.169351472854614 and perplexity is 175.80078892247204
At time: 1581.8019752502441 and batch: 350, loss is 5.177206144332886 and perplexity is 177.18708368265536
At time: 1583.013679265976 and batch: 400, loss is 5.137549362182617 and perplexity is 170.29791801245653
At time: 1584.23317694664 and batch: 450, loss is 5.116141881942749 and perplexity is 166.6910138096214
At time: 1585.4444029331207 and batch: 500, loss is 5.1036692810058595 and perplexity is 164.62485528520858
At time: 1586.6511125564575 and batch: 550, loss is 5.105590362548828 and perplexity is 164.94141702931535
At time: 1587.8803024291992 and batch: 600, loss is 5.136423768997193 and perplexity is 170.10633967631142
At time: 1589.0900249481201 and batch: 650, loss is 5.144145116806031 and perplexity is 171.42487376877963
At time: 1590.2967669963837 and batch: 700, loss is 5.169005184173584 and perplexity is 175.73992163858608
At time: 1591.5052185058594 and batch: 750, loss is 5.134646406173706 and perplexity is 169.80426751756482
At time: 1592.7175240516663 and batch: 800, loss is 5.14169337272644 and perplexity is 171.0050986502773
At time: 1593.9308784008026 and batch: 850, loss is 5.157807760238647 and perplexity is 173.78306356466155
At time: 1595.142034292221 and batch: 900, loss is 5.137013683319092 and perplexity is 170.20671744656346
At time: 1596.3511848449707 and batch: 950, loss is 5.132884006500245 and perplexity is 169.50526808758067
At time: 1597.5594007968903 and batch: 1000, loss is 5.1137417030334475 and perplexity is 166.2914053116905
At time: 1598.7688570022583 and batch: 1050, loss is 5.111376218795776 and perplexity is 165.8985104900234
At time: 1599.9865067005157 and batch: 1100, loss is 5.069781351089477 and perplexity is 159.1395278546141
At time: 1601.2001802921295 and batch: 1150, loss is 5.1038047790527346 and perplexity is 164.64716314286824
At time: 1602.4132561683655 and batch: 1200, loss is 5.129728031158447 and perplexity is 168.97115690537223
At time: 1603.622852563858 and batch: 1250, loss is 5.175815114974975 and perplexity is 176.94078259314867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.188918510492701 and perplexity of 179.27456449208523
Annealing...
Finished Training.
Improved accuracyfrom -10000000 to -179.27424501642665
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f71b055a860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 2.9585772088594657, 'data': 'wikitext', 'dropout': 0.7353633813958657, 'tune_wordvecs': True, 'lr': 9.223799285570193, 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6753942966461182 and batch: 50, loss is 7.316748895645142 and perplexity is 1505.3021116042332
At time: 2.8803775310516357 and batch: 100, loss is 6.6723595809936525 and perplexity is 790.2580842181858
At time: 4.089878559112549 and batch: 150, loss is 6.457117614746093 and perplexity is 637.2216885608409
At time: 5.300454616546631 and batch: 200, loss is 6.3862859153747555 and perplexity is 593.6476225926133
At time: 6.509574890136719 and batch: 250, loss is 6.382418766021728 and perplexity is 591.3563318103019
At time: 7.719419956207275 and batch: 300, loss is 6.363705320358276 and perplexity is 580.3929187849184
At time: 8.95397686958313 and batch: 350, loss is 6.364035186767578 and perplexity is 580.5844024933073
At time: 10.163190603256226 and batch: 400, loss is 6.300704107284546 and perplexity is 544.9554821969058
At time: 11.382296562194824 and batch: 450, loss is 6.279049930572509 and perplexity is 533.2817683663872
At time: 12.592853307723999 and batch: 500, loss is 6.262360992431641 and perplexity is 524.4557154998453
At time: 13.80347490310669 and batch: 550, loss is 6.2658607006073 and perplexity is 526.294372960776
At time: 15.013821125030518 and batch: 600, loss is 6.28436170578003 and perplexity is 536.1219778413259
At time: 16.224173307418823 and batch: 650, loss is 6.250168838500977 and perplexity is 518.1002925609118
At time: 17.44424295425415 and batch: 700, loss is 6.255069179534912 and perplexity is 520.6453915176429
At time: 18.653613328933716 and batch: 750, loss is 6.1977098846435545 and perplexity is 491.62188010434807
At time: 19.86415719985962 and batch: 800, loss is 6.191102437973022 and perplexity is 488.38422285252915
At time: 21.07547402381897 and batch: 850, loss is 6.229988164901734 and perplexity is 507.7494741953117
At time: 22.286473274230957 and batch: 900, loss is 6.217856712341309 and perplexity is 501.6269481919978
At time: 23.49646520614624 and batch: 950, loss is 6.1860604763031 and perplexity is 485.92800560069713
At time: 24.707030773162842 and batch: 1000, loss is 6.177173299789429 and perplexity is 481.6286106805957
At time: 25.91740655899048 and batch: 1050, loss is 6.15774468421936 and perplexity is 472.36154815595603
At time: 27.128239631652832 and batch: 1100, loss is 6.139828662872315 and perplexity is 463.974068064438
At time: 28.340137243270874 and batch: 1150, loss is 6.177765045166016 and perplexity is 481.91369652499435
At time: 29.551642656326294 and batch: 1200, loss is 6.175005111694336 and perplexity is 480.5854805205091
At time: 30.76348090171814 and batch: 1250, loss is 6.14100736618042 and perplexity is 464.5212782692248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.481141918767108 and perplexity of 240.12074926764672
Finished 1 epochs...
Completing Train Step...
At time: 33.760767459869385 and batch: 50, loss is 5.725615863800049 and perplexity is 306.62204452375397
At time: 34.96274733543396 and batch: 100, loss is 5.655281572341919 and perplexity is 285.79694391296994
At time: 36.16557288169861 and batch: 150, loss is 5.490610103607178 and perplexity is 242.4050539501591
At time: 37.367085695266724 and batch: 200, loss is 5.486002378463745 and perplexity is 241.29068740715172
At time: 38.574135065078735 and batch: 250, loss is 5.482931661605835 and perplexity is 240.55088846353325
At time: 39.781917095184326 and batch: 300, loss is 5.460665512084961 and perplexity is 235.25393661691757
At time: 40.99142527580261 and batch: 350, loss is 5.465603303909302 and perplexity is 236.41844426389403
At time: 42.20045232772827 and batch: 400, loss is 5.414767656326294 and perplexity is 224.70033191248203
At time: 43.40250873565674 and batch: 450, loss is 5.375082492828369 and perplexity is 215.95768653620152
At time: 44.60484170913696 and batch: 500, loss is 5.357990827560425 and perplexity is 212.29797442377597
At time: 45.832345485687256 and batch: 550, loss is 5.35962589263916 and perplexity is 212.64537936548965
At time: 47.034550189971924 and batch: 600, loss is 5.377669086456299 and perplexity is 216.51700436422212
At time: 48.237685441970825 and batch: 650, loss is 5.345552310943604 and perplexity is 209.67365768027997
At time: 49.44467377662659 and batch: 700, loss is 5.353956127166748 and perplexity is 211.44314136007924
At time: 50.64867067337036 and batch: 750, loss is 5.3200876331329345 and perplexity is 204.40179357768108
At time: 51.85044050216675 and batch: 800, loss is 5.328964719772339 and perplexity is 206.22436359589963
At time: 53.052499771118164 and batch: 850, loss is 5.36903715133667 and perplexity is 214.65608684467932
At time: 54.25572419166565 and batch: 900, loss is 5.33892050743103 and perplexity is 208.2877438144685
At time: 55.4579598903656 and batch: 950, loss is 5.311723823547363 and perplexity is 202.69934529818053
At time: 56.660205364227295 and batch: 1000, loss is 5.296640768051147 and perplexity is 199.6649612418436
At time: 57.86247730255127 and batch: 1050, loss is 5.277229928970337 and perplexity is 195.82666955565546
At time: 59.06469988822937 and batch: 1100, loss is 5.270509023666381 and perplexity is 194.514949962757
At time: 60.26640844345093 and batch: 1150, loss is 5.289368839263916 and perplexity is 198.21827832511974
At time: 61.4690158367157 and batch: 1200, loss is 5.287248277664185 and perplexity is 197.7983896130239
At time: 62.67054200172424 and batch: 1250, loss is 5.271477489471436 and perplexity is 194.70342229013883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.036825138286953 and perplexity of 153.98037177546567
Finished 2 epochs...
Completing Train Step...
At time: 65.62398886680603 and batch: 50, loss is 5.206975784301758 and perplexity is 182.54117870347142
At time: 66.82664465904236 and batch: 100, loss is 5.206658859252929 and perplexity is 182.4833359978829
At time: 68.03071355819702 and batch: 150, loss is 5.101822071075439 and perplexity is 164.32103931006455
At time: 69.23346447944641 and batch: 200, loss is 5.128635244369507 and perplexity is 168.78660831189185
At time: 70.43646025657654 and batch: 250, loss is 5.1445206260681156 and perplexity is 171.48925748421573
At time: 71.63901042938232 and batch: 300, loss is 5.144514846801758 and perplexity is 171.48826640498308
At time: 72.84152221679688 and batch: 350, loss is 5.164234466552735 and perplexity is 174.90351281925493
At time: 74.04347562789917 and batch: 400, loss is 5.131192922592163 and perplexity is 169.21886269241455
At time: 75.27070450782776 and batch: 450, loss is 5.089822444915772 and perplexity is 162.36103146743875
At time: 76.47392153739929 and batch: 500, loss is 5.097358283996582 and perplexity is 163.5891798256782
At time: 77.67667555809021 and batch: 550, loss is 5.104647617340088 and perplexity is 164.7859925730304
At time: 78.87846660614014 and batch: 600, loss is 5.118601312637329 and perplexity is 167.10148335929637
At time: 80.08103227615356 and batch: 650, loss is 5.1084560203552245 and perplexity is 165.4147605850268
At time: 81.28347945213318 and batch: 700, loss is 5.114436368942261 and perplexity is 166.40696241403208
At time: 82.4846658706665 and batch: 750, loss is 5.088803262710571 and perplexity is 162.19564028953826
At time: 83.68674659729004 and batch: 800, loss is 5.103401622772217 and perplexity is 164.58079798364201
At time: 84.88888955116272 and batch: 850, loss is 5.139860000610351 and perplexity is 170.69186989087638
At time: 86.09099102020264 and batch: 900, loss is 5.1094158744812015 and perplexity is 165.57361084981687
At time: 87.2931706905365 and batch: 950, loss is 5.096492757797241 and perplexity is 163.4476503621752
At time: 88.49552750587463 and batch: 1000, loss is 5.081898145675659 and perplexity is 161.07951832197287
At time: 89.70271801948547 and batch: 1050, loss is 5.068677701950073 and perplexity is 158.96399053527554
At time: 90.91560459136963 and batch: 1100, loss is 5.049474620819092 and perplexity is 155.94051506094593
At time: 92.12488913536072 and batch: 1150, loss is 5.081936941146851 and perplexity is 161.08576759900657
At time: 93.34246134757996 and batch: 1200, loss is 5.0871305084228515 and perplexity is 161.92455363069078
At time: 94.5557153224945 and batch: 1250, loss is 5.094709901809693 and perplexity is 163.1565063511165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9732951143362225 and perplexity of 144.50225519784794
Finished 3 epochs...
Completing Train Step...
At time: 97.51885962486267 and batch: 50, loss is 5.0433512210845945 and perplexity is 154.98854656881457
At time: 98.74787449836731 and batch: 100, loss is 5.049246416091919 and perplexity is 155.90493275843048
At time: 99.95478987693787 and batch: 150, loss is 4.944421119689942 and perplexity is 140.38955857327082
At time: 101.15918231010437 and batch: 200, loss is 4.993031692504883 and perplexity is 147.38256549045835
At time: 102.36311388015747 and batch: 250, loss is 4.998971920013428 and perplexity is 148.26065690961784
At time: 103.56914496421814 and batch: 300, loss is 4.997577075958252 and perplexity is 148.0540005738973
At time: 104.77260828018188 and batch: 350, loss is 5.005308637619018 and perplexity is 149.20312575026608
At time: 106.00094509124756 and batch: 400, loss is 4.975234022140503 and perplexity is 144.78270354206992
At time: 107.21382880210876 and batch: 450, loss is 4.930570240020752 and perplexity is 138.45844437891392
At time: 108.4186635017395 and batch: 500, loss is 4.940424356460571 and perplexity is 139.8295745549816
At time: 109.6223394870758 and batch: 550, loss is 4.94991470336914 and perplexity is 141.16292268623832
At time: 110.82699584960938 and batch: 600, loss is 4.965029659271241 and perplexity is 143.31280076354048
At time: 112.02994060516357 and batch: 650, loss is 4.959959697723389 and perplexity is 142.5880491581879
At time: 113.23266386985779 and batch: 700, loss is 4.969947967529297 and perplexity is 144.01939349108625
At time: 114.43452787399292 and batch: 750, loss is 4.947639350891113 and perplexity is 140.84209241968728
At time: 115.63787198066711 and batch: 800, loss is 4.964486083984375 and perplexity is 143.23492063552803
At time: 116.8422167301178 and batch: 850, loss is 4.9944341850280765 and perplexity is 147.58941345406342
At time: 118.04648017883301 and batch: 900, loss is 4.960862283706665 and perplexity is 142.71680523071288
At time: 119.24948525428772 and batch: 950, loss is 4.9548659515380855 and perplexity is 141.86358850248834
At time: 120.45338702201843 and batch: 1000, loss is 4.948132152557373 and perplexity is 140.91151674231716
At time: 121.65621995925903 and batch: 1050, loss is 4.9264918804168705 and perplexity is 137.89491097957318
At time: 122.86065697669983 and batch: 1100, loss is 4.91362774848938 and perplexity is 136.1323737147403
At time: 124.07545924186707 and batch: 1150, loss is 4.929163255691528 and perplexity is 138.26377249968027
At time: 125.28299498558044 and batch: 1200, loss is 4.946489381790161 and perplexity is 140.68022145643278
At time: 126.48863816261292 and batch: 1250, loss is 4.964255142211914 and perplexity is 143.20184552843705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.923265888743157 and perplexity of 137.45077991164263
Finished 4 epochs...
Completing Train Step...
At time: 129.44812083244324 and batch: 50, loss is 4.9067203807830815 and perplexity is 135.1952974437388
At time: 130.6518771648407 and batch: 100, loss is 4.9022495079040525 and perplexity is 134.59220563302037
At time: 131.85632824897766 and batch: 150, loss is 4.816416463851929 and perplexity is 123.52165240917374
At time: 133.06146216392517 and batch: 200, loss is 4.877110023498535 and perplexity is 131.25080351847728
At time: 134.2647740840912 and batch: 250, loss is 4.877202558517456 and perplexity is 131.2629493760142
At time: 135.4682240486145 and batch: 300, loss is 4.881579790115357 and perplexity is 131.83877705179313
At time: 136.6975703239441 and batch: 350, loss is 4.888555164337158 and perplexity is 132.76161669280324
At time: 137.91440415382385 and batch: 400, loss is 4.865483503341675 and perplexity is 129.73365010333214
At time: 139.1179256439209 and batch: 450, loss is 4.8199608039855955 and perplexity is 123.96023193756429
At time: 140.32717370986938 and batch: 500, loss is 4.839753170013427 and perplexity is 126.43813915398373
At time: 141.53088331222534 and batch: 550, loss is 4.853123006820678 and perplexity is 128.13994754075088
At time: 142.7371928691864 and batch: 600, loss is 4.872998952865601 and perplexity is 130.71232980470984
At time: 143.94327044487 and batch: 650, loss is 4.879852628707885 and perplexity is 131.6112667341382
At time: 145.14660596847534 and batch: 700, loss is 4.878544359207154 and perplexity is 131.43919630970393
At time: 146.35067558288574 and batch: 750, loss is 4.853618850708008 and perplexity is 128.2035007053837
At time: 147.5538682937622 and batch: 800, loss is 4.870863618850708 and perplexity is 130.43351311004028
At time: 148.75670766830444 and batch: 850, loss is 4.905261173248291 and perplexity is 134.99816331182555
At time: 149.9589684009552 and batch: 900, loss is 4.863055648803711 and perplexity is 129.4190577190059
At time: 151.16424298286438 and batch: 950, loss is 4.864309759140014 and perplexity is 129.58146531424174
At time: 152.3684377670288 and batch: 1000, loss is 4.863787546157837 and perplexity is 129.51381385657714
At time: 153.57160186767578 and batch: 1050, loss is 4.848843097686768 and perplexity is 127.59269214590174
At time: 154.77457332611084 and batch: 1100, loss is 4.831402139663696 and perplexity is 125.38664704867618
At time: 155.97757506370544 and batch: 1150, loss is 4.859025039672852 and perplexity is 128.898469930983
At time: 157.18214297294617 and batch: 1200, loss is 4.857030906677246 and perplexity is 128.64168535540142
At time: 158.38530135154724 and batch: 1250, loss is 4.886733198165894 and perplexity is 132.51994973970557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.917531702640283 and perplexity of 136.66486699855477
Finished 5 epochs...
Completing Train Step...
At time: 161.35802102088928 and batch: 50, loss is 4.831463623046875 and perplexity is 125.3943564809407
At time: 162.5642488002777 and batch: 100, loss is 4.810516443252563 and perplexity is 122.7950178029239
At time: 163.77156329154968 and batch: 150, loss is 4.739358396530151 and perplexity is 114.3608038479611
At time: 164.98493885993958 and batch: 200, loss is 4.799976472854614 and perplexity is 121.50755875910538
At time: 166.2197675704956 and batch: 250, loss is 4.793437833786011 and perplexity is 120.71565649122677
At time: 167.42797899246216 and batch: 300, loss is 4.801442718505859 and perplexity is 121.68584936570069
At time: 168.645836353302 and batch: 350, loss is 4.815691041946411 and perplexity is 123.43207958968112
At time: 169.8570909500122 and batch: 400, loss is 4.7883474636077885 and perplexity is 120.10273044711185
At time: 171.06693601608276 and batch: 450, loss is 4.751275987625122 and perplexity is 115.7318627778905
At time: 172.28057384490967 and batch: 500, loss is 4.765657262802124 and perplexity is 117.4082600154792
At time: 173.49710154533386 and batch: 550, loss is 4.7769174385070805 and perplexity is 118.73776885653083
At time: 174.7100863456726 and batch: 600, loss is 4.794774589538574 and perplexity is 120.87713174190573
At time: 175.91876554489136 and batch: 650, loss is 4.806728448867798 and perplexity is 122.33075084047589
At time: 177.12695217132568 and batch: 700, loss is 4.8005972099304195 and perplexity is 121.58300641997536
At time: 178.3347885608673 and batch: 750, loss is 4.772560930252075 and perplexity is 118.2216119236479
At time: 179.54210233688354 and batch: 800, loss is 4.794223823547363 and perplexity is 120.81057505888842
At time: 180.74945998191833 and batch: 850, loss is 4.834188871383667 and perplexity is 125.73655331627096
At time: 181.95807027816772 and batch: 900, loss is 4.785292987823486 and perplexity is 119.73643926389306
At time: 183.16520404815674 and batch: 950, loss is 4.786116428375244 and perplexity is 119.83507570855689
At time: 184.3737027645111 and batch: 1000, loss is 4.784581928253174 and perplexity is 119.65132978537997
At time: 185.58054089546204 and batch: 1050, loss is 4.76624228477478 and perplexity is 117.4769665228082
At time: 186.7884805202484 and batch: 1100, loss is 4.738494634628296 and perplexity is 114.26206599166986
At time: 188.00034308433533 and batch: 1150, loss is 4.771359586715699 and perplexity is 118.07967243042071
At time: 189.2235085964203 and batch: 1200, loss is 4.769231948852539 and perplexity is 117.82870872310404
At time: 190.4326057434082 and batch: 1250, loss is 4.807622499465943 and perplexity is 122.44016962711372
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.918597813070256 and perplexity of 136.8106445323399
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 193.37629294395447 and batch: 50, loss is 4.747193279266358 and perplexity is 115.26032656176098
At time: 194.6015865802765 and batch: 100, loss is 4.7187098693847656 and perplexity is 112.02363427934937
At time: 195.80601024627686 and batch: 150, loss is 4.625338277816772 and perplexity is 102.03728419492559
At time: 197.03650641441345 and batch: 200, loss is 4.68614803314209 and perplexity is 108.43468749313553
At time: 198.24681544303894 and batch: 250, loss is 4.678389663696289 and perplexity is 107.59666616986993
At time: 199.4533245563507 and batch: 300, loss is 4.665936803817749 and perplexity is 106.26508814760794
At time: 200.66016101837158 and batch: 350, loss is 4.658861999511719 and perplexity is 105.51593661939128
At time: 201.8663067817688 and batch: 400, loss is 4.640511808395385 and perplexity is 103.59735601359158
At time: 203.07066464424133 and batch: 450, loss is 4.586895227432251 and perplexity is 98.18910159026093
At time: 204.27410912513733 and batch: 500, loss is 4.59089412689209 and perplexity is 98.58253606373351
At time: 205.47739720344543 and batch: 550, loss is 4.590787601470947 and perplexity is 98.57203507688295
At time: 206.68147802352905 and batch: 600, loss is 4.607160367965698 and perplexity is 100.19921637042063
At time: 207.88568019866943 and batch: 650, loss is 4.611456499099732 and perplexity is 100.63061134465333
At time: 209.0879030227661 and batch: 700, loss is 4.59272611618042 and perplexity is 98.7633037454895
At time: 210.29139518737793 and batch: 750, loss is 4.565019178390503 and perplexity is 96.06443636013591
At time: 211.5024106502533 and batch: 800, loss is 4.575578069686889 and perplexity is 97.08414431992713
At time: 212.70609760284424 and batch: 850, loss is 4.600459384918213 and perplexity is 99.5300277350576
At time: 213.90927290916443 and batch: 900, loss is 4.546947574615478 and perplexity is 94.34399036066733
At time: 215.11257123947144 and batch: 950, loss is 4.538993663787842 and perplexity is 93.59656309958343
At time: 216.31775164604187 and batch: 1000, loss is 4.523333234786987 and perplexity is 92.14221832293026
At time: 217.53020024299622 and batch: 1050, loss is 4.511976909637451 and perplexity is 91.10174051597373
At time: 218.7373809814453 and batch: 1100, loss is 4.463855657577515 and perplexity is 86.82161900893611
At time: 219.94176769256592 and batch: 1150, loss is 4.476656446456909 and perplexity is 87.9401479771358
At time: 221.14561200141907 and batch: 1200, loss is 4.470328779220581 and perplexity is 87.38544881070972
At time: 222.35020089149475 and batch: 1250, loss is 4.529730739593506 and perplexity is 92.73358823649916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.788568427092837 and perplexity of 120.12927169721088
Finished 7 epochs...
Completing Train Step...
At time: 225.32868719100952 and batch: 50, loss is 4.591218004226684 and perplexity is 98.61446988379248
At time: 226.53527736663818 and batch: 100, loss is 4.573058681488037 and perplexity is 96.83985952575838
At time: 227.76759600639343 and batch: 150, loss is 4.495173082351685 and perplexity is 89.58367299616611
At time: 228.97553944587708 and batch: 200, loss is 4.565064144134522 and perplexity is 96.06875606610927
At time: 230.18265295028687 and batch: 250, loss is 4.553487901687622 and perplexity is 94.9630531447156
At time: 231.39027452468872 and batch: 300, loss is 4.549725313186645 and perplexity is 94.60641761005932
At time: 232.60211658477783 and batch: 350, loss is 4.547330121994019 and perplexity is 94.3800883110088
At time: 233.8131890296936 and batch: 400, loss is 4.537457542419434 and perplexity is 93.45289779092205
At time: 235.01971340179443 and batch: 450, loss is 4.487309331893921 and perplexity is 88.88197196355459
At time: 236.22605443000793 and batch: 500, loss is 4.497584753036499 and perplexity is 89.79998004000758
At time: 237.45291757583618 and batch: 550, loss is 4.49968053817749 and perplexity is 89.988378856598
At time: 238.66018962860107 and batch: 600, loss is 4.519283685684204 and perplexity is 91.76983837970805
At time: 239.86732292175293 and batch: 650, loss is 4.529589557647705 and perplexity is 92.72049685222613
At time: 241.07398891448975 and batch: 700, loss is 4.513810873031616 and perplexity is 91.26897107371313
At time: 242.27958798408508 and batch: 750, loss is 4.489568061828614 and perplexity is 89.08295923681773
At time: 243.48709774017334 and batch: 800, loss is 4.506210107803344 and perplexity is 90.57788676383788
At time: 244.69381093978882 and batch: 850, loss is 4.5372068977355955 and perplexity is 93.42947725414062
At time: 245.90165972709656 and batch: 900, loss is 4.48396427154541 and perplexity is 88.5851531186856
At time: 247.10732555389404 and batch: 950, loss is 4.485708141326905 and perplexity is 88.73976886597978
At time: 248.31426191329956 and batch: 1000, loss is 4.473759164810181 and perplexity is 87.68572933958659
At time: 249.52109241485596 and batch: 1050, loss is 4.4660322380065915 and perplexity is 87.0107990537966
At time: 250.7349214553833 and batch: 1100, loss is 4.418356485366822 and perplexity is 82.95982756330201
At time: 251.94313645362854 and batch: 1150, loss is 4.438256120681762 and perplexity is 84.6272332366158
At time: 253.15010690689087 and batch: 1200, loss is 4.438997478485107 and perplexity is 84.68999555814509
At time: 254.3565158843994 and batch: 1250, loss is 4.49917441368103 and perplexity is 89.94284505751953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.785501967381387 and perplexity of 119.76146434680929
Finished 8 epochs...
Completing Train Step...
At time: 257.3145077228546 and batch: 50, loss is 4.525375480651856 and perplexity is 92.33058767006979
At time: 258.5398120880127 and batch: 100, loss is 4.50650993347168 and perplexity is 90.6050484109493
At time: 259.7441906929016 and batch: 150, loss is 4.436058177947998 and perplexity is 84.4414316896247
At time: 260.9483242034912 and batch: 200, loss is 4.503375482559204 and perplexity is 90.32149595706001
At time: 262.1516890525818 and batch: 250, loss is 4.494085464477539 and perplexity is 89.48629315780957
At time: 263.35659646987915 and batch: 300, loss is 4.491547317504883 and perplexity is 89.25945179387696
At time: 264.56018233299255 and batch: 350, loss is 4.488405857086182 and perplexity is 88.97948673888361
At time: 265.76435947418213 and batch: 400, loss is 4.481019411087036 and perplexity is 88.32466594151113
At time: 266.97364616394043 and batch: 450, loss is 4.433014459609986 and perplexity is 84.18480650101861
At time: 268.1816701889038 and batch: 500, loss is 4.444075565338135 and perplexity is 85.1211525108798
At time: 269.3866980075836 and batch: 550, loss is 4.445588779449463 and perplexity is 85.25005654513635
At time: 270.5909855365753 and batch: 600, loss is 4.46775637626648 and perplexity is 87.16094710227138
At time: 271.7954523563385 and batch: 650, loss is 4.479296035766602 and perplexity is 88.17258047984896
At time: 272.9987666606903 and batch: 700, loss is 4.464899978637695 and perplexity is 86.91233601474124
At time: 274.20304012298584 and batch: 750, loss is 4.44289192199707 and perplexity is 85.02045902986671
At time: 275.40653800964355 and batch: 800, loss is 4.463136196136475 and perplexity is 86.75917666693266
At time: 276.6104464530945 and batch: 850, loss is 4.4952290248870845 and perplexity is 89.58868467414561
At time: 277.81724214553833 and batch: 900, loss is 4.443662185668945 and perplexity is 85.08597242888004
At time: 279.02846598625183 and batch: 950, loss is 4.4477927875518795 and perplexity is 85.43815557023768
At time: 280.23869276046753 and batch: 1000, loss is 4.435954446792603 and perplexity is 84.43267293663796
At time: 281.44949412345886 and batch: 1050, loss is 4.428849287033081 and perplexity is 83.83489148767714
At time: 282.6606857776642 and batch: 1100, loss is 4.384651184082031 and perplexity is 80.21023972424175
At time: 283.87192845344543 and batch: 1150, loss is 4.404761724472046 and perplexity is 81.83964013628992
At time: 285.0827558040619 and batch: 1200, loss is 4.407110748291015 and perplexity is 82.03210936920988
At time: 286.29820132255554 and batch: 1250, loss is 4.466704044342041 and perplexity is 87.06927309927134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.786275376368613 and perplexity of 119.8541247672407
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 289.30463695526123 and batch: 50, loss is 4.485326337814331 and perplexity is 88.70589417767076
At time: 290.508362531662 and batch: 100, loss is 4.469007158279419 and perplexity is 87.27003465529036
At time: 291.7128674983978 and batch: 150, loss is 4.39132908821106 and perplexity is 80.7476684669051
At time: 292.9171636104584 and batch: 200, loss is 4.458831052780152 and perplexity is 86.38646882989434
At time: 294.1214690208435 and batch: 250, loss is 4.449615087509155 and perplexity is 85.59399146422983
At time: 295.32589745521545 and batch: 300, loss is 4.438785963058471 and perplexity is 84.67208421193261
At time: 296.531240940094 and batch: 350, loss is 4.4295411300659175 and perplexity is 83.89291214153975
At time: 297.7361924648285 and batch: 400, loss is 4.419012594223022 and perplexity is 83.01427610100583
At time: 298.94203186035156 and batch: 450, loss is 4.368967514038086 and perplexity is 78.96206237629283
At time: 300.1470425128937 and batch: 500, loss is 4.375042600631714 and perplexity is 79.44322381169478
At time: 301.3520700931549 and batch: 550, loss is 4.373655376434326 and perplexity is 79.33309465388193
At time: 302.55637192726135 and batch: 600, loss is 4.394171085357666 and perplexity is 80.97747951677617
At time: 303.76084327697754 and batch: 650, loss is 4.403576726913452 and perplexity is 81.74271780023973
At time: 304.96479415893555 and batch: 700, loss is 4.384159240722656 and perplexity is 80.1707905336348
At time: 306.16937470436096 and batch: 750, loss is 4.355671615600586 and perplexity is 77.91913947702227
At time: 307.37344431877136 and batch: 800, loss is 4.368710012435913 and perplexity is 78.94173213636704
At time: 308.5782947540283 and batch: 850, loss is 4.397902097702026 and perplexity is 81.28017181568593
At time: 309.7833414077759 and batch: 900, loss is 4.343415279388427 and perplexity is 76.96996488954224
At time: 310.9871768951416 and batch: 950, loss is 4.338608140945435 and perplexity is 76.60084752232348
At time: 312.19146823883057 and batch: 1000, loss is 4.32392840385437 and perplexity is 75.48458051854067
At time: 313.39576840400696 and batch: 1050, loss is 4.314502248764038 and perplexity is 74.77639413664242
At time: 314.60090589523315 and batch: 1100, loss is 4.261451749801636 and perplexity is 70.912856488596
At time: 315.8057825565338 and batch: 1150, loss is 4.2759882259368895 and perplexity is 71.95120823796744
At time: 317.02136635780334 and batch: 1200, loss is 4.2789484310150145 and perplexity is 72.16451412879802
At time: 318.22880601882935 and batch: 1250, loss is 4.352113542556762 and perplexity is 77.64239012642112
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.740223237197765 and perplexity of 114.45975050228785
Finished 10 epochs...
Completing Train Step...
At time: 321.2314429283142 and batch: 50, loss is 4.429921112060547 and perplexity is 83.92479599489201
At time: 322.4704682826996 and batch: 100, loss is 4.41264199256897 and perplexity is 82.48710619401977
At time: 323.6834976673126 and batch: 150, loss is 4.338068599700928 and perplexity is 76.55952935315153
At time: 324.88756465911865 and batch: 200, loss is 4.406093797683716 and perplexity is 81.94872916972696
At time: 326.09205532073975 and batch: 250, loss is 4.400939340591431 and perplexity is 81.52741471789129
At time: 327.29784393310547 and batch: 300, loss is 4.394294319152832 and perplexity is 80.98745929381026
At time: 328.50348114967346 and batch: 350, loss is 4.386771020889282 and perplexity is 80.38045269084282
At time: 329.71681022644043 and batch: 400, loss is 4.379307823181152 and perplexity is 79.78279049023729
At time: 330.93212389945984 and batch: 450, loss is 4.3289690113067625 and perplexity is 75.86602921657325
At time: 332.1466987133026 and batch: 500, loss is 4.33721076965332 and perplexity is 76.49388244941065
At time: 333.3546166419983 and batch: 550, loss is 4.333088693618774 and perplexity is 76.17921783106975
At time: 334.56583642959595 and batch: 600, loss is 4.360389652252198 and perplexity is 78.28763343338089
At time: 335.7867636680603 and batch: 650, loss is 4.371488752365113 and perplexity is 79.16139573219446
At time: 337.00067353248596 and batch: 700, loss is 4.3544074821472165 and perplexity is 77.8207015186187
At time: 338.21326756477356 and batch: 750, loss is 4.331081256866455 and perplexity is 76.02644626036891
At time: 339.4268088340759 and batch: 800, loss is 4.348530988693238 and perplexity is 77.3647297453097
At time: 340.63657331466675 and batch: 850, loss is 4.377086534500122 and perplexity is 79.60576656419003
At time: 341.8460097312927 and batch: 900, loss is 4.321224393844605 and perplexity is 75.28074516784798
At time: 343.05554461479187 and batch: 950, loss is 4.3215237236022945 and perplexity is 75.30328230790784
At time: 344.2651860713959 and batch: 1000, loss is 4.308984804153442 and perplexity is 74.36495561220518
At time: 345.4751307964325 and batch: 1050, loss is 4.300866022109985 and perplexity is 73.76364699563327
At time: 346.6889474391937 and batch: 1100, loss is 4.250938425064087 and perplexity is 70.1712319012917
At time: 347.90245842933655 and batch: 1150, loss is 4.267701787948608 and perplexity is 71.35745247041442
At time: 349.1248333454132 and batch: 1200, loss is 4.274110012054443 and perplexity is 71.81619531106318
At time: 350.39082646369934 and batch: 1250, loss is 4.34391149520874 and perplexity is 77.00816808153958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.73165871279083 and perplexity of 113.48364308991486
Finished 11 epochs...
Completing Train Step...
At time: 353.3640892505646 and batch: 50, loss is 4.402277917861938 and perplexity is 81.63661853472358
At time: 354.6016192436218 and batch: 100, loss is 4.381463356018067 and perplexity is 79.95495039646599
At time: 355.8094666004181 and batch: 150, loss is 4.30791937828064 and perplexity is 74.28576745651183
At time: 357.02221274375916 and batch: 200, loss is 4.374334859848022 and perplexity is 79.38701849395922
At time: 358.23354840278625 and batch: 250, loss is 4.36936318397522 and perplexity is 78.99331147230544
At time: 359.44663977622986 and batch: 300, loss is 4.3654162883758545 and perplexity is 78.68214758850003
At time: 360.6588304042816 and batch: 350, loss is 4.358233547210693 and perplexity is 78.11901891287768
At time: 361.87155294418335 and batch: 400, loss is 4.355907735824585 and perplexity is 77.9375399339639
At time: 363.0831673145294 and batch: 450, loss is 4.307121410369873 and perplexity is 74.22651344239993
At time: 364.29170203208923 and batch: 500, loss is 4.315546731948853 and perplexity is 74.8545376256152
At time: 365.50023579597473 and batch: 550, loss is 4.312661361694336 and perplexity is 74.63886586540421
At time: 366.70785784721375 and batch: 600, loss is 4.340852394104004 and perplexity is 76.77295226739712
At time: 367.9126286506653 and batch: 650, loss is 4.352719202041626 and perplexity is 77.68942921982726
At time: 369.1161346435547 and batch: 700, loss is 4.336983680725098 and perplexity is 76.47651350785141
At time: 370.32175302505493 and batch: 750, loss is 4.314497442245483 and perplexity is 74.77603472338032
At time: 371.52558279037476 and batch: 800, loss is 4.33343991279602 and perplexity is 76.20597813237008
At time: 372.73021245002747 and batch: 850, loss is 4.362982120513916 and perplexity is 78.49085494718514
At time: 373.9330747127533 and batch: 900, loss is 4.308965711593628 and perplexity is 74.36353580839598
At time: 375.1361975669861 and batch: 950, loss is 4.309299898147583 and perplexity is 74.38839125511441
At time: 376.3398129940033 and batch: 1000, loss is 4.297447881698608 and perplexity is 73.51194291795596
At time: 377.5484359264374 and batch: 1050, loss is 4.2907334899902345 and perplexity is 73.02000830695602
At time: 378.752970457077 and batch: 1100, loss is 4.24184983253479 and perplexity is 69.53636356719859
At time: 379.9845199584961 and batch: 1150, loss is 4.258658399581909 and perplexity is 70.71504844753109
At time: 381.1878159046173 and batch: 1200, loss is 4.266642751693726 and perplexity is 71.28192234282321
At time: 382.3916051387787 and batch: 1250, loss is 4.335362644195556 and perplexity is 76.35264271247058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.730509737112226 and perplexity of 113.35332802284269
Finished 12 epochs...
Completing Train Step...
At time: 385.3625054359436 and batch: 50, loss is 4.381270141601562 and perplexity is 79.93950343971392
At time: 386.57041692733765 and batch: 100, loss is 4.360670413970947 and perplexity is 78.30961668978449
At time: 387.7775979042053 and batch: 150, loss is 4.28782527923584 and perplexity is 72.80795922467625
At time: 388.98529720306396 and batch: 200, loss is 4.355069379806519 and perplexity is 77.87222790951674
At time: 390.19331645965576 and batch: 250, loss is 4.349146881103516 and perplexity is 77.41239277132439
At time: 391.40459656715393 and batch: 300, loss is 4.344200735092163 and perplexity is 77.03044513664919
At time: 392.6147155761719 and batch: 350, loss is 4.340583581924438 and perplexity is 76.7523175363242
At time: 393.8261399269104 and batch: 400, loss is 4.338481407165528 and perplexity is 76.59114022250729
At time: 395.0386872291565 and batch: 450, loss is 4.28991961479187 and perplexity is 72.96060331061634
At time: 396.2560980319977 and batch: 500, loss is 4.2989373207092285 and perplexity is 73.6215160544798
At time: 397.4729790687561 and batch: 550, loss is 4.296286039352417 and perplexity is 73.42658322657721
At time: 398.686532497406 and batch: 600, loss is 4.325183477401733 and perplexity is 75.57937869568163
At time: 399.89917969703674 and batch: 650, loss is 4.338112916946411 and perplexity is 76.56292233579121
At time: 401.11102986335754 and batch: 700, loss is 4.322014493942261 and perplexity is 75.34024799545622
At time: 402.32280921936035 and batch: 750, loss is 4.299854526519775 and perplexity is 73.68907311391382
At time: 403.54062819480896 and batch: 800, loss is 4.320996680259705 and perplexity is 75.2636046711279
At time: 404.75799202919006 and batch: 850, loss is 4.350421047210693 and perplexity is 77.51109188464845
At time: 405.97176599502563 and batch: 900, loss is 4.296570615768433 and perplexity is 73.44748167393371
At time: 407.18398237228394 and batch: 950, loss is 4.296331014633179 and perplexity is 73.42988568203708
At time: 408.4067542552948 and batch: 1000, loss is 4.2836629676818845 and perplexity is 72.50553963366845
At time: 409.62287878990173 and batch: 1050, loss is 4.2790687465667725 and perplexity is 72.1731971644744
At time: 410.8639853000641 and batch: 1100, loss is 4.229834060668946 and perplexity is 68.7058302283494
At time: 412.07653307914734 and batch: 1150, loss is 4.249291143417358 and perplexity is 70.05573527270768
At time: 413.2889380455017 and batch: 1200, loss is 4.2565955066680905 and perplexity is 70.56932123668246
At time: 414.5033338069916 and batch: 1250, loss is 4.326003675460815 and perplexity is 75.64139418440959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.730108776231752 and perplexity of 113.30788688330246
Finished 13 epochs...
Completing Train Step...
At time: 417.50467705726624 and batch: 50, loss is 4.3632737731933595 and perplexity is 78.51375035393337
At time: 418.75962471961975 and batch: 100, loss is 4.342376117706299 and perplexity is 76.89002219524048
At time: 419.9637339115143 and batch: 150, loss is 4.271998882293701 and perplexity is 71.66474192896057
At time: 421.16913533210754 and batch: 200, loss is 4.337675704956054 and perplexity is 76.52945542472978
At time: 422.37837171554565 and batch: 250, loss is 4.3331656169891355 and perplexity is 76.18507801864662
At time: 423.588374376297 and batch: 300, loss is 4.328245697021484 and perplexity is 75.81117407501891
At time: 424.7972254753113 and batch: 350, loss is 4.323836240768433 and perplexity is 75.47762394723388
At time: 426.00698256492615 and batch: 400, loss is 4.323767967224121 and perplexity is 75.47247099823784
At time: 427.2167887687683 and batch: 450, loss is 4.275371255874634 and perplexity is 71.90683018791339
At time: 428.43189215660095 and batch: 500, loss is 4.2839604759216305 and perplexity is 72.52711383822938
At time: 429.6449646949768 and batch: 550, loss is 4.282051339149475 and perplexity is 72.38878174767902
At time: 430.8543210029602 and batch: 600, loss is 4.312302494049073 and perplexity is 74.61208519701107
At time: 432.06345319747925 and batch: 650, loss is 4.324840126037597 and perplexity is 75.55343286742857
At time: 433.26892161369324 and batch: 700, loss is 4.307751302719116 and perplexity is 74.27328288363854
At time: 434.47226786613464 and batch: 750, loss is 4.2865088748931885 and perplexity is 72.71217756849923
At time: 435.6759099960327 and batch: 800, loss is 4.30900643825531 and perplexity is 74.36656444863311
At time: 436.88082361221313 and batch: 850, loss is 4.339160623550415 and perplexity is 76.64317985096716
At time: 438.0862543582916 and batch: 900, loss is 4.284904432296753 and perplexity is 72.59560859265022
At time: 439.29020285606384 and batch: 950, loss is 4.287304992675781 and perplexity is 72.77008807480584
At time: 440.4945044517517 and batch: 1000, loss is 4.275225086212158 and perplexity is 71.89632035894289
At time: 441.7313904762268 and batch: 1050, loss is 4.271966142654419 and perplexity is 71.66239568956827
At time: 442.93574810028076 and batch: 1100, loss is 4.222889814376831 and perplexity is 68.23037277900066
At time: 444.14001297950745 and batch: 1150, loss is 4.241408939361572 and perplexity is 69.50571221669357
At time: 445.3443374633789 and batch: 1200, loss is 4.248181495666504 and perplexity is 69.97804119812761
At time: 446.54901099205017 and batch: 1250, loss is 4.3173606491088865 and perplexity is 74.99044077718658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.731334379989735 and perplexity of 113.4468425902002
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 449.5242118835449 and batch: 50, loss is 4.35314037322998 and perplexity is 77.72215666049811
At time: 450.73087215423584 and batch: 100, loss is 4.337250919342041 and perplexity is 76.49695371663479
At time: 451.9396195411682 and batch: 150, loss is 4.264901857376099 and perplexity is 71.15793600413723
At time: 453.1470546722412 and batch: 200, loss is 4.33155387878418 and perplexity is 76.06238651760593
At time: 454.3546185493469 and batch: 250, loss is 4.324566135406494 and perplexity is 75.53273477034703
At time: 455.5627121925354 and batch: 300, loss is 4.31543345451355 and perplexity is 74.84605877581241
At time: 456.7700204849243 and batch: 350, loss is 4.310093297958374 and perplexity is 74.44743440997796
At time: 457.97711634635925 and batch: 400, loss is 4.307822370529175 and perplexity is 74.278561510767
At time: 459.18404269218445 and batch: 450, loss is 4.25714985370636 and perplexity is 70.60845197590531
At time: 460.39146971702576 and batch: 500, loss is 4.26154767036438 and perplexity is 70.91965881593251
At time: 461.59836626052856 and batch: 550, loss is 4.259151339530945 and perplexity is 70.74991531283823
At time: 462.8052532672882 and batch: 600, loss is 4.287105360031128 and perplexity is 72.7555622396355
At time: 464.01177978515625 and batch: 650, loss is 4.298564186096192 and perplexity is 73.59405044307942
At time: 465.2186059951782 and batch: 700, loss is 4.27947003364563 and perplexity is 72.20216514778633
At time: 466.4263758659363 and batch: 750, loss is 4.2541662693023685 and perplexity is 70.39809965779028
At time: 467.6344726085663 and batch: 800, loss is 4.274406323432922 and perplexity is 71.83747841995087
At time: 468.84762048721313 and batch: 850, loss is 4.298018674850464 and perplexity is 73.5539150091047
At time: 470.05826783180237 and batch: 900, loss is 4.242298736572265 and perplexity is 69.56758572890818
At time: 471.2908170223236 and batch: 950, loss is 4.240486497879028 and perplexity is 69.44162682652646
At time: 472.49865341186523 and batch: 1000, loss is 4.227944707870483 and perplexity is 68.57614322656478
At time: 473.7056043148041 and batch: 1050, loss is 4.221862545013428 and perplexity is 68.16031779622064
At time: 474.91246604919434 and batch: 1100, loss is 4.166312093734741 and perplexity is 64.47722713180832
At time: 476.1211910247803 and batch: 1150, loss is 4.184213533401489 and perplexity is 65.6418554838934
At time: 477.3463969230652 and batch: 1200, loss is 4.194428071975708 and perplexity is 66.31579286882891
At time: 478.56609177589417 and batch: 1250, loss is 4.268409867286682 and perplexity is 71.40799710081866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.717897289860858 and perplexity of 111.93264314171456
Finished 15 epochs...
Completing Train Step...
At time: 481.5040249824524 and batch: 50, loss is 4.334723558425903 and perplexity is 76.30386241409516
At time: 482.7332651615143 and batch: 100, loss is 4.317159614562988 and perplexity is 74.97536662324184
At time: 483.9388449192047 and batch: 150, loss is 4.24595609664917 and perplexity is 69.82248528519617
At time: 485.14344573020935 and batch: 200, loss is 4.311496820449829 and perplexity is 74.5519964190047
At time: 486.3468372821808 and batch: 250, loss is 4.30579402923584 and perplexity is 74.12805193124458
At time: 487.55086970329285 and batch: 300, loss is 4.298581714630127 and perplexity is 73.595340450196
At time: 488.7540695667267 and batch: 350, loss is 4.294713191986084 and perplexity is 73.31118519406598
At time: 489.9616949558258 and batch: 400, loss is 4.291901483535766 and perplexity is 73.10534503202383
At time: 491.16604828834534 and batch: 450, loss is 4.243085927963257 and perplexity is 69.62237029362589
At time: 492.37600541114807 and batch: 500, loss is 4.2480313587188725 and perplexity is 69.96753569727254
At time: 493.580317735672 and batch: 550, loss is 4.24617347240448 and perplexity is 69.83766465042585
At time: 494.7839870452881 and batch: 600, loss is 4.275585489273071 and perplexity is 71.92223668274933
At time: 495.9908709526062 and batch: 650, loss is 4.287839660644531 and perplexity is 72.80900631322314
At time: 497.1953728199005 and batch: 700, loss is 4.270237121582031 and perplexity is 71.53859695347646
At time: 498.3994948863983 and batch: 750, loss is 4.247196025848389 and perplexity is 69.90911391905087
At time: 499.60390853881836 and batch: 800, loss is 4.268421206474304 and perplexity is 71.40880681408629
At time: 500.80771565437317 and batch: 850, loss is 4.2930670166015625 and perplexity is 73.19060140385382
At time: 502.0546236038208 and batch: 900, loss is 4.238133463859558 and perplexity is 69.27842040666837
At time: 503.2610692977905 and batch: 950, loss is 4.237678928375244 and perplexity is 69.24693806175978
At time: 504.46511816978455 and batch: 1000, loss is 4.226324100494384 and perplexity is 68.46509822750345
At time: 505.6695673465729 and batch: 1050, loss is 4.221709718704224 and perplexity is 68.14990190234924
At time: 506.87385845184326 and batch: 1100, loss is 4.1676495885849 and perplexity is 64.56352278818315
At time: 508.08068919181824 and batch: 1150, loss is 4.18673526763916 and perplexity is 65.80759568683308
At time: 509.2905504703522 and batch: 1200, loss is 4.197345075607299 and perplexity is 66.50951868955414
At time: 510.49437189102173 and batch: 1250, loss is 4.269739737510681 and perplexity is 71.5030236423976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.718132520244069 and perplexity of 111.95897619730057
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 513.4410216808319 and batch: 50, loss is 4.328563184738159 and perplexity is 75.83524701280325
At time: 514.6709065437317 and batch: 100, loss is 4.313090763092041 and perplexity is 74.67092278087951
At time: 515.8763921260834 and batch: 150, loss is 4.242310438156128 and perplexity is 69.5683997846096
At time: 517.0815138816833 and batch: 200, loss is 4.307057857513428 and perplexity is 74.2217962853427
At time: 518.2877154350281 and batch: 250, loss is 4.30081093788147 and perplexity is 73.75958389395346
At time: 519.4932434558868 and batch: 300, loss is 4.293467311859131 and perplexity is 73.21990511917221
At time: 520.6974380016327 and batch: 350, loss is 4.2889276790618895 and perplexity is 72.88826696374632
At time: 521.9021553993225 and batch: 400, loss is 4.284696207046509 and perplexity is 72.58049392756648
At time: 523.1063759326935 and batch: 450, loss is 4.235425395965576 and perplexity is 69.09106354267382
At time: 524.3123617172241 and batch: 500, loss is 4.239663352966309 and perplexity is 69.38448982391593
At time: 525.517317533493 and batch: 550, loss is 4.236300435066223 and perplexity is 69.15154738378257
At time: 526.7230267524719 and batch: 600, loss is 4.266009693145752 and perplexity is 71.23681099313741
At time: 527.9271476268768 and batch: 650, loss is 4.277522687911987 and perplexity is 72.06169938162024
At time: 529.1312265396118 and batch: 700, loss is 4.25984974861145 and perplexity is 70.79934495518401
At time: 530.3380057811737 and batch: 750, loss is 4.2354923391342165 and perplexity is 69.09568887220749
At time: 531.5430977344513 and batch: 800, loss is 4.255608344078064 and perplexity is 70.49969221599066
At time: 532.7723903656006 and batch: 850, loss is 4.2765389347076415 and perplexity is 71.99084331210005
At time: 533.9765985012054 and batch: 900, loss is 4.221207304000854 and perplexity is 68.11567098937736
At time: 535.1802961826324 and batch: 950, loss is 4.220260524749756 and perplexity is 68.05121100491301
At time: 536.3856029510498 and batch: 1000, loss is 4.207870125770569 and perplexity is 67.21323152028293
At time: 537.5908391475677 and batch: 1050, loss is 4.201970329284668 and perplexity is 66.8178546016738
At time: 538.7955839633942 and batch: 1100, loss is 4.146149044036865 and perplexity is 63.19018851194355
At time: 540.000435590744 and batch: 1150, loss is 4.16446665763855 and perplexity is 64.35834825490036
At time: 541.2054212093353 and batch: 1200, loss is 4.176429872512817 and perplexity is 65.13290485981487
At time: 542.4096629619598 and batch: 1250, loss is 4.2509767532348635 and perplexity is 70.17392148779477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.714639705463047 and perplexity of 111.56860637198648
Finished 17 epochs...
Completing Train Step...
At time: 545.3752994537354 and batch: 50, loss is 4.321707735061645 and perplexity is 75.31714024975017
At time: 546.5825726985931 and batch: 100, loss is 4.3053482723236085 and perplexity is 74.09501620320505
At time: 547.7877745628357 and batch: 150, loss is 4.235213794708252 and perplexity is 69.07644533342861
At time: 548.9914131164551 and batch: 200, loss is 4.299634246826172 and perplexity is 73.67284269514786
At time: 550.1962230205536 and batch: 250, loss is 4.293832921981812 and perplexity is 73.2466799519422
At time: 551.4010906219482 and batch: 300, loss is 4.287087345123291 and perplexity is 72.754251566693
At time: 552.6154522895813 and batch: 350, loss is 4.283124284744263 and perplexity is 72.46649265449967
At time: 553.83025598526 and batch: 400, loss is 4.279008560180664 and perplexity is 72.16885345128068
At time: 555.0361659526825 and batch: 450, loss is 4.230099744796753 and perplexity is 68.72408670204916
At time: 556.2409660816193 and batch: 500, loss is 4.234630575180054 and perplexity is 69.03617034729247
At time: 557.4511518478394 and batch: 550, loss is 4.23160225391388 and perplexity is 68.82742288122044
At time: 558.6594834327698 and batch: 600, loss is 4.261667032241821 and perplexity is 70.92812442478166
At time: 559.8662703037262 and batch: 650, loss is 4.27380669593811 and perplexity is 71.79441560484656
At time: 561.0715153217316 and batch: 700, loss is 4.256738510131836 and perplexity is 70.57941361565884
At time: 562.2823812961578 and batch: 750, loss is 4.232909245491028 and perplexity is 68.91743855525296
At time: 563.5292766094208 and batch: 800, loss is 4.253548088073731 and perplexity is 70.35459432252632
At time: 564.7332010269165 and batch: 850, loss is 4.275220994949341 and perplexity is 71.8960262128024
At time: 565.9404063224792 and batch: 900, loss is 4.220112948417664 and perplexity is 68.04116899779797
At time: 567.1452994346619 and batch: 950, loss is 4.219795236587524 and perplexity is 68.01955494717245
At time: 568.3500339984894 and batch: 1000, loss is 4.208136682510376 and perplexity is 67.23115004819496
At time: 569.5538017749786 and batch: 1050, loss is 4.202961511611939 and perplexity is 66.88411611149503
At time: 570.7594971656799 and batch: 1100, loss is 4.147639546394348 and perplexity is 63.284443863352365
At time: 571.9706394672394 and batch: 1150, loss is 4.1666537952423095 and perplexity is 64.4992628621298
At time: 573.1752760410309 and batch: 1200, loss is 4.179039416313171 and perplexity is 65.30309398930638
At time: 574.3804528713226 and batch: 1250, loss is 4.25243221282959 and perplexity is 70.27613115810335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7143920007413325 and perplexity of 111.54097372390301
Finished 18 epochs...
Completing Train Step...
At time: 577.317055940628 and batch: 50, loss is 4.3177680397033695 and perplexity is 75.02099740125335
At time: 578.5454833507538 and batch: 100, loss is 4.301034784317016 and perplexity is 73.77609656197751
At time: 579.7491843700409 and batch: 150, loss is 4.231058192253113 and perplexity is 68.78998670393922
At time: 580.9519300460815 and batch: 200, loss is 4.295317029953003 and perplexity is 73.35546663914937
At time: 582.158040523529 and batch: 250, loss is 4.289577121734619 and perplexity is 72.93561908923502
At time: 583.3622424602509 and batch: 300, loss is 4.283101234436035 and perplexity is 72.46482229875902
At time: 584.5646803379059 and batch: 350, loss is 4.279500684738159 and perplexity is 72.204378256948
At time: 585.7686522006989 and batch: 400, loss is 4.2753972244262695 and perplexity is 71.90869752839201
At time: 586.9735872745514 and batch: 450, loss is 4.226564998626709 and perplexity is 68.48159332853562
At time: 588.1780776977539 and batch: 500, loss is 4.231450595855713 and perplexity is 68.81698543939874
At time: 589.3905146121979 and batch: 550, loss is 4.228534917831421 and perplexity is 68.61662949590428
At time: 590.5945456027985 and batch: 600, loss is 4.25892297744751 and perplexity is 70.73376055941131
At time: 591.7981362342834 and batch: 650, loss is 4.271405515670776 and perplexity is 71.6222310765671
At time: 593.0283982753754 and batch: 700, loss is 4.2545426654815675 and perplexity is 70.4246022209432
At time: 594.2314486503601 and batch: 750, loss is 4.23113431930542 and perplexity is 68.79522368219058
At time: 595.4350352287292 and batch: 800, loss is 4.251991500854492 and perplexity is 70.24516644929861
At time: 596.6402125358582 and batch: 850, loss is 4.274018621444702 and perplexity is 71.80963228508845
At time: 597.8430910110474 and batch: 900, loss is 4.219046907424927 and perplexity is 67.9686729711863
At time: 599.0462296009064 and batch: 950, loss is 4.219289569854737 and perplexity is 67.98516841585183
At time: 600.2504150867462 and batch: 1000, loss is 4.207949366569519 and perplexity is 67.21855776147363
At time: 601.45392537117 and batch: 1050, loss is 4.203376951217652 and perplexity is 66.91190819488722
At time: 602.6572799682617 and batch: 1100, loss is 4.148116092681885 and perplexity is 63.31460901710943
At time: 603.8636322021484 and batch: 1150, loss is 4.167599954605103 and perplexity is 64.56031832312327
At time: 605.0668349266052 and batch: 1200, loss is 4.180301976203919 and perplexity is 65.38559512687331
At time: 606.2697489261627 and batch: 1250, loss is 4.2529230880737305 and perplexity is 70.31063643934414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.714413385321624 and perplexity of 111.54335900631543
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 609.2259073257446 and batch: 50, loss is 4.315887670516968 and perplexity is 74.88006277549697
At time: 610.4352781772614 and batch: 100, loss is 4.299826517105102 and perplexity is 73.68700915501338
At time: 611.6465446949005 and batch: 150, loss is 4.230355134010315 and perplexity is 68.74164033391347
At time: 612.8720681667328 and batch: 200, loss is 4.29392653465271 and perplexity is 73.2535370902394
At time: 614.0829102993011 and batch: 250, loss is 4.288103046417237 and perplexity is 72.82818569528818
At time: 615.2914595603943 and batch: 300, loss is 4.281776838302612 and perplexity is 72.36891369280961
At time: 616.5045115947723 and batch: 350, loss is 4.277835254669189 and perplexity is 72.08422699382237
At time: 617.7226319313049 and batch: 400, loss is 4.273170652389527 and perplexity is 71.74876574916385
At time: 618.9419941902161 and batch: 450, loss is 4.223982424736023 and perplexity is 68.30496273256803
At time: 620.1545975208282 and batch: 500, loss is 4.228605899810791 and perplexity is 68.6215002129481
At time: 621.363189458847 and batch: 550, loss is 4.224853091239929 and perplexity is 68.36445947282074
At time: 622.5733158588409 and batch: 600, loss is 4.255316104888916 and perplexity is 70.47909245327783
At time: 623.8242557048798 and batch: 650, loss is 4.267209644317627 and perplexity is 71.3223429948191
At time: 625.0315220355988 and batch: 700, loss is 4.250216856002807 and perplexity is 70.12061677471068
At time: 626.238386631012 and batch: 750, loss is 4.22646888256073 and perplexity is 68.47501146350946
At time: 627.4470427036285 and batch: 800, loss is 4.246911063194275 and perplexity is 69.88919527057196
At time: 628.6552567481995 and batch: 850, loss is 4.267565708160401 and perplexity is 71.34774282405351
At time: 629.8630192279816 and batch: 900, loss is 4.212117500305176 and perplexity is 67.49931841724288
At time: 631.0734393596649 and batch: 950, loss is 4.212426128387452 and perplexity is 67.52015381747032
At time: 632.2829267978668 and batch: 1000, loss is 4.200517301559448 and perplexity is 66.72083690816648
At time: 633.4896841049194 and batch: 1050, loss is 4.195695810317993 and perplexity is 66.39991725473571
At time: 634.6974663734436 and batch: 1100, loss is 4.1397112846374515 and perplexity is 62.78469192709961
At time: 635.9064767360687 and batch: 1150, loss is 4.158506002426147 and perplexity is 63.97587136976354
At time: 637.1141097545624 and batch: 1200, loss is 4.171852293014527 and perplexity is 64.83543517486734
At time: 638.3216817378998 and batch: 1250, loss is 4.246123528480529 and perplexity is 69.83417677051361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713941588018932 and perplexity of 111.49074556281941
Finished 20 epochs...
Completing Train Step...
At time: 641.2572057247162 and batch: 50, loss is 4.314148015975952 and perplexity is 74.74991057701395
At time: 642.487423658371 and batch: 100, loss is 4.297549228668213 and perplexity is 73.51939350814146
At time: 643.6919648647308 and batch: 150, loss is 4.227898530960083 and perplexity is 68.57297666525498
At time: 644.896324634552 and batch: 200, loss is 4.2913227462768555 and perplexity is 73.06304848550252
At time: 646.1016147136688 and batch: 250, loss is 4.285767812728881 and perplexity is 72.65831328567491
At time: 647.3060038089752 and batch: 300, loss is 4.279612627029419 and perplexity is 72.21246143290524
At time: 648.5101847648621 and batch: 350, loss is 4.275779695510864 and perplexity is 71.93620578614873
At time: 649.7152514457703 and batch: 400, loss is 4.271100215911865 and perplexity is 71.60036816422752
At time: 650.919193983078 and batch: 450, loss is 4.222046513557434 and perplexity is 68.1728583041387
At time: 652.123482465744 and batch: 500, loss is 4.22697078704834 and perplexity is 68.50938800520076
At time: 653.3277530670166 and batch: 550, loss is 4.223393006324768 and perplexity is 68.2647143926749
At time: 654.5574240684509 and batch: 600, loss is 4.2539423561096195 and perplexity is 70.38233835917964
At time: 655.7690622806549 and batch: 650, loss is 4.266084184646607 and perplexity is 71.24211772775526
At time: 656.9736773967743 and batch: 700, loss is 4.2492320728302 and perplexity is 70.05159716151283
At time: 658.1771228313446 and batch: 750, loss is 4.225790634155273 and perplexity is 68.42858414257869
At time: 659.3963024616241 and batch: 800, loss is 4.246424221992493 and perplexity is 69.85517861178228
At time: 660.6055054664612 and batch: 850, loss is 4.267351512908935 and perplexity is 71.33246211292412
At time: 661.8088295459747 and batch: 900, loss is 4.211935620307923 and perplexity is 67.48704275777685
At time: 663.0146324634552 and batch: 950, loss is 4.2125195789337155 and perplexity is 67.52646390756448
At time: 664.2192015647888 and batch: 1000, loss is 4.200748810768127 and perplexity is 66.7362851844616
At time: 665.4234092235565 and batch: 1050, loss is 4.19630425453186 and perplexity is 66.44033019345322
At time: 666.6289134025574 and batch: 1100, loss is 4.1404557085037235 and perplexity is 62.83144775112311
At time: 667.8344058990479 and batch: 1150, loss is 4.159313716888428 and perplexity is 64.02756648093985
At time: 669.0493497848511 and batch: 1200, loss is 4.17293264389038 and perplexity is 64.90551804427305
At time: 670.25532579422 and batch: 1250, loss is 4.246774263381958 and perplexity is 69.87963509570599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713931341240876 and perplexity of 111.48960314774735
Finished 21 epochs...
Completing Train Step...
At time: 673.2133564949036 and batch: 50, loss is 4.312717542648316 and perplexity is 74.64305926588601
At time: 674.463945388794 and batch: 100, loss is 4.2959443378448485 and perplexity is 73.4014975385459
At time: 675.6704747676849 and batch: 150, loss is 4.226304149627685 and perplexity is 68.46373230308089
At time: 676.8874590396881 and batch: 200, loss is 4.289606714248658 and perplexity is 72.9377774695026
At time: 678.1005892753601 and batch: 250, loss is 4.2841956901550295 and perplexity is 72.54417525417676
At time: 679.3092465400696 and batch: 300, loss is 4.278115701675415 and perplexity is 72.10444563447498
At time: 680.5170567035675 and batch: 350, loss is 4.274364147186279 and perplexity is 71.83444864863549
At time: 681.7254457473755 and batch: 400, loss is 4.269669761657715 and perplexity is 71.49802033238603
At time: 682.9372568130493 and batch: 450, loss is 4.220707821846008 and perplexity is 68.08165692266961
At time: 684.1499514579773 and batch: 500, loss is 4.225830993652344 and perplexity is 68.43134594155197
At time: 685.3918251991272 and batch: 550, loss is 4.222351989746094 and perplexity is 68.19368667018648
At time: 686.6053462028503 and batch: 600, loss is 4.2529905319213865 and perplexity is 70.31537861911038
At time: 687.8123052120209 and batch: 650, loss is 4.265202913284302 and perplexity is 71.17936174619808
At time: 689.0185542106628 and batch: 700, loss is 4.248509635925293 and perplexity is 70.00100757856686
At time: 690.2252314090729 and batch: 750, loss is 4.22531225681305 and perplexity is 68.39585728688168
At time: 691.4408469200134 and batch: 800, loss is 4.246015405654907 and perplexity is 69.82662651018126
At time: 692.648425579071 and batch: 850, loss is 4.2671119976043705 and perplexity is 71.31537894245784
At time: 693.8559105396271 and batch: 900, loss is 4.211719436645508 and perplexity is 67.47245473861042
At time: 695.068902015686 and batch: 950, loss is 4.2124864292144775 and perplexity is 67.52422546134694
At time: 696.2915601730347 and batch: 1000, loss is 4.200821757316589 and perplexity is 66.74115354368583
At time: 697.5114541053772 and batch: 1050, loss is 4.196669597625732 and perplexity is 66.4646081438647
At time: 698.7234516143799 and batch: 1100, loss is 4.140885744094849 and perplexity is 62.85847332045987
At time: 699.9319868087769 and batch: 1150, loss is 4.159797582626343 and perplexity is 64.0585547231118
At time: 701.1397175788879 and batch: 1200, loss is 4.1736026382446285 and perplexity is 64.94901894597426
At time: 702.3523216247559 and batch: 1250, loss is 4.24712296962738 and perplexity is 69.90400680992919
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713966536695939 and perplexity of 111.49352714411796
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 705.3867559432983 and batch: 50, loss is 4.311932487487793 and perplexity is 74.58448334268593
At time: 706.5910751819611 and batch: 100, loss is 4.295634202957153 and perplexity is 73.37873670299713
At time: 707.7952444553375 and batch: 150, loss is 4.226066570281983 and perplexity is 68.44746866638457
At time: 709.0021419525146 and batch: 200, loss is 4.289212741851807 and perplexity is 72.90904765823987
At time: 710.2060053348541 and batch: 250, loss is 4.283710842132568 and perplexity is 72.50901087964128
At time: 711.4103627204895 and batch: 300, loss is 4.2776861572265625 and perplexity is 72.07348022110193
At time: 712.614856004715 and batch: 350, loss is 4.273726348876953 and perplexity is 71.78864736627882
At time: 713.8191583156586 and batch: 400, loss is 4.268690662384033 and perplexity is 71.4280509316932
At time: 715.0482926368713 and batch: 450, loss is 4.219780225753784 and perplexity is 68.01853392460525
At time: 716.2676091194153 and batch: 500, loss is 4.224835014343261 and perplexity is 68.36322366672093
At time: 717.4823334217072 and batch: 550, loss is 4.220964813232422 and perplexity is 68.09915557047611
At time: 718.6950335502625 and batch: 600, loss is 4.251871385574341 and perplexity is 70.23672943816838
At time: 719.9141099452972 and batch: 650, loss is 4.26378791809082 and perplexity is 71.07871451591873
At time: 721.1196303367615 and batch: 700, loss is 4.247026805877685 and perplexity is 69.89728490172278
At time: 722.3234453201294 and batch: 750, loss is 4.223543238639832 and perplexity is 68.27497072915257
At time: 723.527859210968 and batch: 800, loss is 4.24437445640564 and perplexity is 69.71213852001158
At time: 724.7321333885193 and batch: 850, loss is 4.26483983039856 and perplexity is 71.15352242932073
At time: 725.9392983913422 and batch: 900, loss is 4.209266257286072 and perplexity is 67.30713556701373
At time: 727.1435210704803 and batch: 950, loss is 4.209976558685303 and perplexity is 67.35496090277587
At time: 728.3552644252777 and batch: 1000, loss is 4.19803246974945 and perplexity is 66.55525265992948
At time: 729.564165353775 and batch: 1050, loss is 4.193788108825683 and perplexity is 66.27336678214218
At time: 730.7733306884766 and batch: 1100, loss is 4.137827787399292 and perplexity is 62.666548429658576
At time: 731.9818596839905 and batch: 1150, loss is 4.156306524276733 and perplexity is 63.83531247337186
At time: 733.1955101490021 and batch: 1200, loss is 4.170294876098633 and perplexity is 64.7345379612845
At time: 734.4088201522827 and batch: 1250, loss is 4.24471076965332 and perplexity is 69.73558757861348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.71383689267792 and perplexity of 111.47907361220304
Finished 23 epochs...
Completing Train Step...
At time: 737.4024710655212 and batch: 50, loss is 4.31136037826538 and perplexity is 74.54182507567423
At time: 738.6369490623474 and batch: 100, loss is 4.2948495960235595 and perplexity is 73.32118581776581
At time: 739.8471233844757 and batch: 150, loss is 4.225172581672669 and perplexity is 68.38630475305494
At time: 741.0611317157745 and batch: 200, loss is 4.288406839370728 and perplexity is 72.8503137459207
At time: 742.2759671211243 and batch: 250, loss is 4.282967367172241 and perplexity is 72.4551222805485
At time: 743.4878771305084 and batch: 300, loss is 4.276932878494263 and perplexity is 72.01920924444103
At time: 744.6988732814789 and batch: 350, loss is 4.273020648956299 and perplexity is 71.73800399514185
At time: 745.9332985877991 and batch: 400, loss is 4.267988090515137 and perplexity is 71.37788521703195
At time: 747.1471199989319 and batch: 450, loss is 4.219164872169495 and perplexity is 67.97669135126576
At time: 748.36159491539 and batch: 500, loss is 4.22427737236023 and perplexity is 68.32511209040408
At time: 749.5731494426727 and batch: 550, loss is 4.220491495132446 and perplexity is 68.06693063447587
At time: 750.7960398197174 and batch: 600, loss is 4.251402711868286 and perplexity is 70.20381904260235
At time: 752.0005424022675 and batch: 650, loss is 4.263480558395385 and perplexity is 71.05687114093173
At time: 753.2070798873901 and batch: 700, loss is 4.246757392883301 and perplexity is 69.87845620136021
At time: 754.4117724895477 and batch: 750, loss is 4.223401613235474 and perplexity is 68.26530194350454
At time: 755.6165587902069 and batch: 800, loss is 4.244274230003357 and perplexity is 69.70515187330132
At time: 756.821977853775 and batch: 850, loss is 4.264799318313599 and perplexity is 71.1506399101636
At time: 758.0272676944733 and batch: 900, loss is 4.209205389022827 and perplexity is 67.30303882324988
At time: 759.2319991588593 and batch: 950, loss is 4.210041217803955 and perplexity is 67.35931615598659
At time: 760.4377222061157 and batch: 1000, loss is 4.198125615119934 and perplexity is 66.56145226232374
At time: 761.6419546604156 and batch: 1050, loss is 4.194030213356018 and perplexity is 66.28941380693098
At time: 762.8472685813904 and batch: 1100, loss is 4.138126192092895 and perplexity is 62.68525121220215
At time: 764.0525388717651 and batch: 1150, loss is 4.156624765396118 and perplexity is 63.855630727550164
At time: 765.2572298049927 and batch: 1200, loss is 4.170663681030273 and perplexity is 64.7584167811747
At time: 766.4616074562073 and batch: 1250, loss is 4.244990768432618 and perplexity is 69.75511619187543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713814171561359 and perplexity of 111.4765407119527
Finished 24 epochs...
Completing Train Step...
At time: 769.4646995067596 and batch: 50, loss is 4.3108385467529295 and perplexity is 74.50293694976138
At time: 770.669216632843 and batch: 100, loss is 4.294272336959839 and perplexity is 73.27887271267647
At time: 771.8744292259216 and batch: 150, loss is 4.224591898918152 and perplexity is 68.34660553268154
At time: 773.0783410072327 and batch: 200, loss is 4.2877832126617434 and perplexity is 72.80489650768419
At time: 774.2862286567688 and batch: 250, loss is 4.282395524978638 and perplexity is 72.41370122876705
At time: 775.490852355957 and batch: 300, loss is 4.276376390457154 and perplexity is 71.97914256540221
At time: 776.7225694656372 and batch: 350, loss is 4.272479944229126 and perplexity is 71.69922540209275
At time: 777.9367799758911 and batch: 400, loss is 4.267439098358154 and perplexity is 71.33871007227333
At time: 779.1413743495941 and batch: 450, loss is 4.218666844367981 and perplexity is 67.94284549790541
At time: 780.3465256690979 and batch: 500, loss is 4.223848686218262 and perplexity is 68.29582833892023
At time: 781.5535981655121 and batch: 550, loss is 4.220118412971496 and perplexity is 68.0415408134447
At time: 782.7715930938721 and batch: 600, loss is 4.251066226959228 and perplexity is 70.18020049080226
At time: 783.9894745349884 and batch: 650, loss is 4.2632228374481205 and perplexity is 71.03856065639067
At time: 785.199057340622 and batch: 700, loss is 4.2465408992767335 and perplexity is 69.86332959982117
At time: 786.4074485301971 and batch: 750, loss is 4.2232739353179936 and perplexity is 68.2565865283111
At time: 787.6167232990265 and batch: 800, loss is 4.2441897344589234 and perplexity is 69.69926234736684
At time: 788.8251984119415 and batch: 850, loss is 4.264765186309814 and perplexity is 71.1482114376975
At time: 790.0307261943817 and batch: 900, loss is 4.209167194366455 and perplexity is 67.30046825590053
At time: 791.2435111999512 and batch: 950, loss is 4.21008267402649 and perplexity is 67.36210867667023
At time: 792.4589681625366 and batch: 1000, loss is 4.198187103271485 and perplexity is 66.565545128818
At time: 793.6711976528168 and batch: 1050, loss is 4.194196910858154 and perplexity is 66.30046500770887
At time: 794.8825037479401 and batch: 1100, loss is 4.138337836265564 and perplexity is 62.69851958437081
At time: 796.0991151332855 and batch: 1150, loss is 4.156855535507202 and perplexity is 63.870368398988155
At time: 797.318338394165 and batch: 1200, loss is 4.170930290222168 and perplexity is 64.77568427207474
At time: 798.5322697162628 and batch: 1250, loss is 4.245168499946594 and perplexity is 69.76751497607852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7138186266822535 and perplexity of 111.47703735452475
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 801.5181097984314 and batch: 50, loss is 4.310542163848877 and perplexity is 74.48085882489873
At time: 802.7519347667694 and batch: 100, loss is 4.294122295379639 and perplexity is 73.26787865962254
At time: 803.9633395671844 and batch: 150, loss is 4.2245302534103395 and perplexity is 68.34239240133788
At time: 805.1780483722687 and batch: 200, loss is 4.2877795600891115 and perplexity is 72.8046305829974
At time: 806.391608953476 and batch: 250, loss is 4.282269124984741 and perplexity is 72.40454871582469
At time: 807.6314785480499 and batch: 300, loss is 4.276300678253174 and perplexity is 71.97369307217717
At time: 808.839207649231 and batch: 350, loss is 4.272320556640625 and perplexity is 71.68779834614817
At time: 810.0474555492401 and batch: 400, loss is 4.267012128829956 and perplexity is 71.30825711859576
At time: 811.2571170330048 and batch: 450, loss is 4.218408856391907 and perplexity is 67.9253193215753
At time: 812.4656603336334 and batch: 500, loss is 4.2234598159790036 and perplexity is 68.26927528699416
At time: 813.6748831272125 and batch: 550, loss is 4.219407777786255 and perplexity is 67.99320527698522
At time: 814.9001622200012 and batch: 600, loss is 4.250629062652588 and perplexity is 70.1495269172991
At time: 816.121545791626 and batch: 650, loss is 4.262781572341919 and perplexity is 71.00722073349397
At time: 817.3296055793762 and batch: 700, loss is 4.24617374420166 and perplexity is 69.83768363210876
At time: 818.5331680774689 and batch: 750, loss is 4.2226816749572755 and perplexity is 68.21617282665055
At time: 819.7384717464447 and batch: 800, loss is 4.243618540763855 and perplexity is 69.65946193611718
At time: 820.9477818012238 and batch: 850, loss is 4.26394214630127 and perplexity is 71.08967770425433
At time: 822.1614921092987 and batch: 900, loss is 4.208279671669007 and perplexity is 67.24076406110757
At time: 823.3712379932404 and batch: 950, loss is 4.2091898727417 and perplexity is 67.3019945384805
At time: 824.5822637081146 and batch: 1000, loss is 4.197208099365234 and perplexity is 66.5004090895361
At time: 825.7909777164459 and batch: 1050, loss is 4.193110136985779 and perplexity is 66.2284505334246
At time: 827.0056190490723 and batch: 1100, loss is 4.137295665740967 and perplexity is 62.633211072538195
At time: 828.2189073562622 and batch: 1150, loss is 4.1557167053222654 and perplexity is 63.79767229764646
At time: 829.4366104602814 and batch: 1200, loss is 4.16973436832428 and perplexity is 64.6982639163798
At time: 830.645379781723 and batch: 1250, loss is 4.244129114151001 and perplexity is 69.69503728468496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713655123745438 and perplexity of 111.45881202151814
Finished 26 epochs...
Completing Train Step...
At time: 833.6514935493469 and batch: 50, loss is 4.310283946990967 and perplexity is 74.46162909438567
At time: 834.8943309783936 and batch: 100, loss is 4.293775491714477 and perplexity is 73.24247349632128
At time: 836.1058778762817 and batch: 150, loss is 4.224177436828613 and perplexity is 68.31828432517844
At time: 837.3433423042297 and batch: 200, loss is 4.287434720993042 and perplexity is 72.77952902824498
At time: 838.5549559593201 and batch: 250, loss is 4.281978578567505 and perplexity is 72.3835148894031
At time: 839.7679831981659 and batch: 300, loss is 4.276001148223877 and perplexity is 71.9521380181368
At time: 840.9834091663361 and batch: 350, loss is 4.272071323394775 and perplexity is 71.66993358981664
At time: 842.2049458026886 and batch: 400, loss is 4.2667895030975345 and perplexity is 71.29238383259357
At time: 843.4230024814606 and batch: 450, loss is 4.218199062347412 and perplexity is 67.91107048882361
At time: 844.6361384391785 and batch: 500, loss is 4.223280553817749 and perplexity is 68.25703828600732
At time: 845.8491234779358 and batch: 550, loss is 4.219267663955688 and perplexity is 67.98367915592756
At time: 847.0630457401276 and batch: 600, loss is 4.250472087860107 and perplexity is 70.13851607410363
At time: 848.2814033031464 and batch: 650, loss is 4.262685594558715 and perplexity is 71.0004059448959
At time: 849.4991466999054 and batch: 700, loss is 4.246113338470459 and perplexity is 69.83346516317471
At time: 850.7157504558563 and batch: 750, loss is 4.22266273021698 and perplexity is 68.21488050121381
At time: 851.9330570697784 and batch: 800, loss is 4.243584666252136 and perplexity is 69.65710229582359
At time: 853.1424560546875 and batch: 850, loss is 4.263959798812866 and perplexity is 71.09093262669062
At time: 854.3498039245605 and batch: 900, loss is 4.208274307250977 and perplexity is 67.240403354508
At time: 855.5581045150757 and batch: 950, loss is 4.20922082901001 and perplexity is 67.30407798932903
At time: 856.7721529006958 and batch: 1000, loss is 4.197265701293945 and perplexity is 66.50423975168546
At time: 857.9804043769836 and batch: 1050, loss is 4.193228521347046 and perplexity is 66.23629141034814
At time: 859.1933209896088 and batch: 1100, loss is 4.1374282550811765 and perplexity is 62.64151611923766
At time: 860.4021399021149 and batch: 1150, loss is 4.155845785140992 and perplexity is 63.80590782113029
At time: 861.611564874649 and batch: 1200, loss is 4.169858679771424 and perplexity is 64.70630715111768
At time: 862.8216381072998 and batch: 1250, loss is 4.244213333129883 and perplexity is 69.70090717673293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713631066092609 and perplexity of 111.45613061636809
Finished 27 epochs...
Completing Train Step...
At time: 865.8409974575043 and batch: 50, loss is 4.310081548690796 and perplexity is 74.44655971228916
At time: 867.0514674186707 and batch: 100, loss is 4.293544187545776 and perplexity is 73.22553416601998
At time: 868.2879123687744 and batch: 150, loss is 4.223945398330688 and perplexity is 68.30243369215246
At time: 869.4976940155029 and batch: 200, loss is 4.287201356887818 and perplexity is 72.76254688016358
At time: 870.7083356380463 and batch: 250, loss is 4.281759462356567 and perplexity is 72.36765622539485
At time: 871.917316198349 and batch: 300, loss is 4.27578483581543 and perplexity is 71.93657556110617
At time: 873.122475862503 and batch: 350, loss is 4.27187180519104 and perplexity is 71.65563555981103
At time: 874.3326282501221 and batch: 400, loss is 4.2665937137603756 and perplexity is 71.27842691037137
At time: 875.5468280315399 and batch: 450, loss is 4.218023753166198 and perplexity is 67.89916609816585
At time: 876.7591469287872 and batch: 500, loss is 4.223130207061768 and perplexity is 68.24677683313556
At time: 877.9696440696716 and batch: 550, loss is 4.219133501052856 and perplexity is 67.97455888000185
At time: 879.1781811714172 and batch: 600, loss is 4.250349960327148 and perplexity is 70.12995075321153
At time: 880.3895819187164 and batch: 650, loss is 4.262613267898559 and perplexity is 70.99527090836644
At time: 881.6077749729156 and batch: 700, loss is 4.246066331863403 and perplexity is 69.83018260606998
At time: 882.8183255195618 and batch: 750, loss is 4.222646369934082 and perplexity is 68.21376449560006
At time: 884.028281211853 and batch: 800, loss is 4.243558249473572 and perplexity is 69.65526220388155
At time: 885.2395520210266 and batch: 850, loss is 4.263958606719971 and perplexity is 71.09084787974541
At time: 886.4536817073822 and batch: 900, loss is 4.208259749412536 and perplexity is 67.23942448670441
At time: 887.666074514389 and batch: 950, loss is 4.209238138198852 and perplexity is 67.30524297840728
At time: 888.8831632137299 and batch: 1000, loss is 4.197298889160156 and perplexity is 66.50644692212222
At time: 890.091187953949 and batch: 1050, loss is 4.1933092641830445 and perplexity is 66.24163973227908
At time: 891.3015863895416 and batch: 1100, loss is 4.137528738975525 and perplexity is 62.647810898981895
At time: 892.5156879425049 and batch: 1150, loss is 4.155950708389282 and perplexity is 63.81260289546734
At time: 893.7286295890808 and batch: 1200, loss is 4.169959931373596 and perplexity is 64.71285910007948
At time: 894.9368534088135 and batch: 1250, loss is 4.2442764568328855 and perplexity is 69.70530709496465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713627056483805 and perplexity of 111.45568372178144
Finished 28 epochs...
Completing Train Step...
At time: 897.9236350059509 and batch: 50, loss is 4.309897937774658 and perplexity is 74.43289176608752
At time: 899.1763384342194 and batch: 100, loss is 4.293342370986938 and perplexity is 73.2107575318304
At time: 900.3903822898865 and batch: 150, loss is 4.22374541759491 and perplexity is 68.28877588690175
At time: 901.6042325496674 and batch: 200, loss is 4.286996040344238 and perplexity is 72.74760905907951
At time: 902.8205945491791 and batch: 250, loss is 4.281562719345093 and perplexity is 72.3534197952807
At time: 904.049204826355 and batch: 300, loss is 4.275595197677612 and perplexity is 71.92293493630798
At time: 905.2608528137207 and batch: 350, loss is 4.2716907787322995 and perplexity is 71.64266516788432
At time: 906.4683270454407 and batch: 400, loss is 4.266410436630249 and perplexity is 71.26536440191339
At time: 907.6769247055054 and batch: 450, loss is 4.217862253189087 and perplexity is 67.8882012698287
At time: 908.886442899704 and batch: 500, loss is 4.222989549636841 and perplexity is 68.23717809232954
At time: 910.0937592983246 and batch: 550, loss is 4.219002933502197 and perplexity is 67.96568418772776
At time: 911.30087018013 and batch: 600, loss is 4.250239868164062 and perplexity is 70.12223042021785
At time: 912.5166964530945 and batch: 650, loss is 4.262544164657593 and perplexity is 70.99036507455982
At time: 913.7251193523407 and batch: 700, loss is 4.2460218524932865 and perplexity is 69.82707667260802
At time: 914.932719707489 and batch: 750, loss is 4.222625818252563 and perplexity is 68.21236260244265
At time: 916.1415815353394 and batch: 800, loss is 4.243532409667969 and perplexity is 69.65346234870104
At time: 917.3558669090271 and batch: 850, loss is 4.263951425552368 and perplexity is 71.09033736628481
At time: 918.5644512176514 and batch: 900, loss is 4.208242211341858 and perplexity is 67.2382452472662
At time: 919.7744235992432 and batch: 950, loss is 4.209250354766846 and perplexity is 67.30606522250694
At time: 920.9815144538879 and batch: 1000, loss is 4.197322978973388 and perplexity is 66.50804906930495
At time: 922.1887624263763 and batch: 1050, loss is 4.193373460769653 and perplexity is 66.24589235594173
At time: 923.3962278366089 and batch: 1100, loss is 4.137613906860351 and perplexity is 62.65314670774171
At time: 924.6043725013733 and batch: 1150, loss is 4.156042871475219 and perplexity is 63.818484332893426
At time: 925.812549829483 and batch: 1200, loss is 4.170048570632934 and perplexity is 64.71859545420908
At time: 927.0232408046722 and batch: 1250, loss is 4.244327630996704 and perplexity is 69.70887429704246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713630175068431 and perplexity of 111.45603130630516
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 930.0038011074066 and batch: 50, loss is 4.309789047241211 and perplexity is 74.424787170062
At time: 931.2125580310822 and batch: 100, loss is 4.293281936645508 and perplexity is 73.20633322160487
At time: 932.4218730926514 and batch: 150, loss is 4.223713340759278 and perplexity is 68.28658543419368
At time: 933.6264963150024 and batch: 200, loss is 4.2870057010650635 and perplexity is 72.74831185681607
At time: 934.8305022716522 and batch: 250, loss is 4.281516418457032 and perplexity is 72.35006984524334
At time: 936.0358376502991 and batch: 300, loss is 4.275551071166992 and perplexity is 71.91976129817702
At time: 937.2415375709534 and batch: 350, loss is 4.271665601730347 and perplexity is 71.64086144306978
At time: 938.4471778869629 and batch: 400, loss is 4.2662820053100585 and perplexity is 71.25621228480122
At time: 939.656858921051 and batch: 450, loss is 4.217831497192383 and perplexity is 67.88611333264267
At time: 940.8684206008911 and batch: 500, loss is 4.222892532348633 and perplexity is 68.2305582274819
At time: 942.0736379623413 and batch: 550, loss is 4.218722887039185 and perplexity is 67.94665330315513
At time: 943.2805535793304 and batch: 600, loss is 4.250047636032105 and perplexity is 70.10875196990368
At time: 944.4845914840698 and batch: 650, loss is 4.2623920726776126 and perplexity is 70.97956883040801
At time: 945.689240694046 and batch: 700, loss is 4.245940685272217 and perplexity is 69.82140923284736
At time: 946.8941240310669 and batch: 750, loss is 4.222484273910522 and perplexity is 68.20270821173735
At time: 948.1061475276947 and batch: 800, loss is 4.243315434455871 and perplexity is 69.63835091339732
At time: 949.3115091323853 and batch: 850, loss is 4.263596105575561 and perplexity is 71.06508203638796
At time: 950.5177733898163 and batch: 900, loss is 4.207856559753418 and perplexity is 67.21231971061621
At time: 951.7235536575317 and batch: 950, loss is 4.208861169815063 and perplexity is 67.27987581135154
At time: 952.9299850463867 and batch: 1000, loss is 4.196919598579407 and perplexity is 66.48122643649423
At time: 954.134336233139 and batch: 1050, loss is 4.19298912525177 and perplexity is 66.22043659868464
At time: 955.3396687507629 and batch: 1100, loss is 4.137216305732727 and perplexity is 62.62824069761845
At time: 956.5463659763336 and batch: 1150, loss is 4.1556535339355465 and perplexity is 63.793642237511555
At time: 957.7537310123444 and batch: 1200, loss is 4.169649710655213 and perplexity is 64.6927869440011
At time: 958.9650661945343 and batch: 1250, loss is 4.243946752548218 and perplexity is 69.68232874478977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713583396299042 and perplexity of 111.45081765226482
Finished 30 epochs...
Completing Train Step...
At time: 961.9498765468597 and batch: 50, loss is 4.309700317382813 and perplexity is 74.41818376219913
At time: 963.1804795265198 and batch: 100, loss is 4.293167304992676 and perplexity is 73.19794193959234
At time: 964.3859195709229 and batch: 150, loss is 4.223577628135681 and perplexity is 68.2773187113478
At time: 965.5978052616119 and batch: 200, loss is 4.286874303817749 and perplexity is 72.7387535568722
At time: 966.8055114746094 and batch: 250, loss is 4.2814201068878175 and perplexity is 72.34310203202969
At time: 968.0104854106903 and batch: 300, loss is 4.275481786727905 and perplexity is 71.91477855047162
At time: 969.2158060073853 and batch: 350, loss is 4.27157603263855 and perplexity is 71.63444492353999
At time: 970.4235572814941 and batch: 400, loss is 4.266206846237183 and perplexity is 71.25085693520334
At time: 971.6324999332428 and batch: 450, loss is 4.217755579948426 and perplexity is 67.8809598016389
At time: 972.8429102897644 and batch: 500, loss is 4.2228453159332275 and perplexity is 68.22733670115636
At time: 974.0490889549255 and batch: 550, loss is 4.218674073219299 and perplexity is 67.9433366484089
At time: 975.2540411949158 and batch: 600, loss is 4.249994077682495 and perplexity is 70.10499716140656
At time: 976.4596509933472 and batch: 650, loss is 4.262343654632568 and perplexity is 70.97613222164476
At time: 977.6673293113708 and batch: 700, loss is 4.245899066925049 and perplexity is 69.81850344166571
At time: 978.8715541362762 and batch: 750, loss is 4.222460660934448 and perplexity is 68.20109776183399
At time: 980.0759983062744 and batch: 800, loss is 4.243308272361755 and perplexity is 69.63785215876005
At time: 981.2808065414429 and batch: 850, loss is 4.263614358901978 and perplexity is 71.06637922236615
At time: 982.4859850406647 and batch: 900, loss is 4.20786334991455 and perplexity is 67.21277609464656
At time: 983.6900985240936 and batch: 950, loss is 4.208872756958008 and perplexity is 67.28065539740645
At time: 984.8945453166962 and batch: 1000, loss is 4.1969482421875 and perplexity is 66.48313072596257
At time: 986.0985753536224 and batch: 1050, loss is 4.193035106658936 and perplexity is 66.22348157754828
At time: 987.3038010597229 and batch: 1100, loss is 4.137278776168824 and perplexity is 62.632153233334456
At time: 988.5088765621185 and batch: 1150, loss is 4.1557120513916015 and perplexity is 63.79737538839396
At time: 989.7154977321625 and batch: 1200, loss is 4.1696971940994265 and perplexity is 64.69585885327284
At time: 990.9634389877319 and batch: 1250, loss is 4.243965210914612 and perplexity is 69.68361497861581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713559784158303 and perplexity of 111.44818609094139
Finished 31 epochs...
Completing Train Step...
At time: 993.8960673809052 and batch: 50, loss is 4.309624271392822 and perplexity is 74.41252477291609
At time: 995.1258339881897 and batch: 100, loss is 4.293075485229492 and perplexity is 73.19122123044959
At time: 996.3306128978729 and batch: 150, loss is 4.223485279083252 and perplexity is 68.2710136567998
At time: 997.5346336364746 and batch: 200, loss is 4.286779470443726 and perplexity is 72.73185582252306
At time: 998.7392501831055 and batch: 250, loss is 4.281339778900146 and perplexity is 72.33729108961533
At time: 999.9441931247711 and batch: 300, loss is 4.275406303405762 and perplexity is 71.90935038894585
At time: 1001.1495831012726 and batch: 350, loss is 4.2715041542053225 and perplexity is 71.62929613691932
At time: 1002.353931427002 and batch: 400, loss is 4.266139936447144 and perplexity is 71.24608971481433
At time: 1003.5675368309021 and batch: 450, loss is 4.2176935720443725 and perplexity is 67.87675077609426
At time: 1004.7781593799591 and batch: 500, loss is 4.222797832489014 and perplexity is 68.22409710913436
At time: 1005.9932625293732 and batch: 550, loss is 4.218629674911499 and perplexity is 67.94032014619971
At time: 1007.1984388828278 and batch: 600, loss is 4.249950399398804 and perplexity is 70.1019351623243
At time: 1008.4052221775055 and batch: 650, loss is 4.262314128875732 and perplexity is 70.97403662856077
At time: 1009.6101624965668 and batch: 700, loss is 4.245881567001343 and perplexity is 69.81728163387298
At time: 1010.8173317909241 and batch: 750, loss is 4.222454338073731 and perplexity is 68.20066653715534
At time: 1012.021641254425 and batch: 800, loss is 4.243298707008361 and perplexity is 69.63718605128035
At time: 1013.2260086536407 and batch: 850, loss is 4.263621997833252 and perplexity is 71.06692209562641
At time: 1014.4305860996246 and batch: 900, loss is 4.2078637647628785 and perplexity is 67.21280397776013
At time: 1015.6358914375305 and batch: 950, loss is 4.208880729675293 and perplexity is 67.28119180918904
At time: 1016.8509659767151 and batch: 1000, loss is 4.196966094970703 and perplexity is 66.48431764547695
At time: 1018.0571439266205 and batch: 1050, loss is 4.19307400226593 and perplexity is 66.22605743015585
At time: 1019.2624340057373 and batch: 1100, loss is 4.137322845458985 and perplexity is 62.63491344868863
At time: 1020.4945178031921 and batch: 1150, loss is 4.155757684707641 and perplexity is 63.80028674061438
At time: 1021.699381351471 and batch: 1200, loss is 4.169736456871033 and perplexity is 64.69839904186996
At time: 1022.9049847126007 and batch: 1250, loss is 4.243982043266296 and perplexity is 69.68478792760146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713544636747263 and perplexity of 111.4464979522425
Finished 32 epochs...
Completing Train Step...
At time: 1025.9430568218231 and batch: 50, loss is 4.309554128646851 and perplexity is 74.40730545714456
At time: 1027.1502709388733 and batch: 100, loss is 4.292994451522827 and perplexity is 73.18529051479514
At time: 1028.3562302589417 and batch: 150, loss is 4.223406400680542 and perplexity is 68.26562876066995
At time: 1029.559068441391 and batch: 200, loss is 4.286698112487793 and perplexity is 72.72593874810592
At time: 1030.76278758049 and batch: 250, loss is 4.2812662601470945 and perplexity is 72.33197313766229
At time: 1031.9675409793854 and batch: 300, loss is 4.275335273742676 and perplexity is 71.90424287340967
At time: 1033.1711955070496 and batch: 350, loss is 4.2714385986328125 and perplexity is 71.62460059131385
At time: 1034.3746421337128 and batch: 400, loss is 4.2660760498046875 and perplexity is 71.2415381867468
At time: 1035.579870223999 and batch: 450, loss is 4.217636547088623 and perplexity is 67.87288021774509
At time: 1036.7863738536835 and batch: 500, loss is 4.222751617431641 and perplexity is 68.22094420142871
At time: 1037.9922325611115 and batch: 550, loss is 4.218585472106934 and perplexity is 67.9373170598791
At time: 1039.1986033916473 and batch: 600, loss is 4.249909725189209 and perplexity is 70.09908387950772
At time: 1040.4014222621918 and batch: 650, loss is 4.262288913726807 and perplexity is 70.97224703021993
At time: 1041.6064743995667 and batch: 700, loss is 4.245868349075318 and perplexity is 69.81635880030807
At time: 1042.8116052150726 and batch: 750, loss is 4.2224507522583 and perplexity is 68.20042198259138
At time: 1044.0238332748413 and batch: 800, loss is 4.243289046287536 and perplexity is 69.63651330911647
At time: 1045.237148284912 and batch: 850, loss is 4.26362455368042 and perplexity is 71.06710373205009
At time: 1046.4510078430176 and batch: 900, loss is 4.20786033153534 and perplexity is 67.21257322130668
At time: 1047.6607763767242 and batch: 950, loss is 4.208885793685913 and perplexity is 67.28153252272156
At time: 1048.8715937137604 and batch: 1000, loss is 4.196978898048401 and perplexity is 66.48516885481047
At time: 1050.088146686554 and batch: 1050, loss is 4.193106336593628 and perplexity is 66.22819883981927
At time: 1051.323637008667 and batch: 1100, loss is 4.137359619140625 and perplexity is 62.63721680740658
At time: 1052.5298039913177 and batch: 1150, loss is 4.155797519683838 and perplexity is 63.80282827413867
At time: 1053.7336280345917 and batch: 1200, loss is 4.1697718000411985 and perplexity is 64.7006857288059
At time: 1054.9385159015656 and batch: 1250, loss is 4.243998093605041 and perplexity is 69.685906401029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7135375085538325 and perplexity of 111.44570354287933
Finished 33 epochs...
Completing Train Step...
At time: 1057.909172296524 and batch: 50, loss is 4.309488162994385 and perplexity is 74.40239729257877
At time: 1059.1437530517578 and batch: 100, loss is 4.292920122146606 and perplexity is 73.17985089996671
At time: 1060.3518061637878 and batch: 150, loss is 4.223333826065064 and perplexity is 68.260674588688
At time: 1061.5597567558289 and batch: 200, loss is 4.286623401641846 and perplexity is 72.72050553466227
At time: 1062.767260313034 and batch: 250, loss is 4.281197137832642 and perplexity is 72.32697355706337
At time: 1063.9756338596344 and batch: 300, loss is 4.275268268585205 and perplexity is 71.89942507970346
At time: 1065.1833810806274 and batch: 350, loss is 4.27137640953064 and perplexity is 71.62014646021024
At time: 1066.3933100700378 and batch: 400, loss is 4.266014060974121 and perplexity is 71.23712214398093
At time: 1067.6015710830688 and batch: 450, loss is 4.217582626342773 and perplexity is 67.86922056008741
At time: 1068.8134667873383 and batch: 500, loss is 4.222706575393676 and perplexity is 68.21787146027181
At time: 1070.0209488868713 and batch: 550, loss is 4.218541254997254 and perplexity is 67.93431313449221
At time: 1071.2291045188904 and batch: 600, loss is 4.249870767593384 and perplexity is 70.09635304092404
At time: 1072.4388279914856 and batch: 650, loss is 4.26226487159729 and perplexity is 70.97054072677642
At time: 1073.6461861133575 and batch: 700, loss is 4.245856113433838 and perplexity is 69.8155045575985
At time: 1074.854176044464 and batch: 750, loss is 4.222447447776794 and perplexity is 68.20019661593058
At time: 1076.0612914562225 and batch: 800, loss is 4.243279275894165 and perplexity is 69.63583293631216
At time: 1077.2698862552643 and batch: 850, loss is 4.263624267578125 and perplexity is 71.06708339959154
At time: 1078.4773406982422 and batch: 900, loss is 4.207854795455932 and perplexity is 67.21220112819414
At time: 1079.6866717338562 and batch: 950, loss is 4.208888845443726 and perplexity is 67.28173784997739
At time: 1080.8954184055328 and batch: 1000, loss is 4.196989045143128 and perplexity is 66.48584348953958
At time: 1082.1461856365204 and batch: 1050, loss is 4.193134002685547 and perplexity is 66.23003114060218
At time: 1083.3538064956665 and batch: 1100, loss is 4.137392377853393 and perplexity is 62.63926875560998
At time: 1084.7413535118103 and batch: 1150, loss is 4.155833950042725 and perplexity is 63.80515267640984
At time: 1085.9538249969482 and batch: 1200, loss is 4.169804410934448 and perplexity is 64.70279571036538
At time: 1087.1618399620056 and batch: 1250, loss is 4.244013438224792 and perplexity is 69.68697571296879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713535280993385 and perplexity of 111.4454552911146
Finished 34 epochs...
Completing Train Step...
At time: 1090.1247417926788 and batch: 50, loss is 4.309424772262573 and perplexity is 74.3976810196514
At time: 1091.328050851822 and batch: 100, loss is 4.292849950790405 and perplexity is 73.17471595074774
At time: 1092.5341956615448 and batch: 150, loss is 4.223264870643615 and perplexity is 68.25596780738431
At time: 1093.7392206192017 and batch: 200, loss is 4.286552658081055 and perplexity is 72.71536120912434
At time: 1094.968561887741 and batch: 250, loss is 4.281130418777466 and perplexity is 72.32214813069966
At time: 1096.1719434261322 and batch: 300, loss is 4.275204429626465 and perplexity is 71.89483524177912
At time: 1097.383207321167 and batch: 350, loss is 4.271316385269165 and perplexity is 71.61584764283019
At time: 1098.5870435237885 and batch: 400, loss is 4.265953445434571 and perplexity is 71.23280419825477
At time: 1099.7931933403015 and batch: 450, loss is 4.217530226707458 and perplexity is 67.8656643308543
At time: 1100.9978547096252 and batch: 500, loss is 4.222662134170532 and perplexity is 68.21483984198873
At time: 1102.201955795288 and batch: 550, loss is 4.2184970617294315 and perplexity is 67.93131096153596
At time: 1103.405237197876 and batch: 600, loss is 4.249832849502564 and perplexity is 70.09369517143426
At time: 1104.6077334880829 and batch: 650, loss is 4.262241277694702 and perplexity is 70.9688662745054
At time: 1105.8110239505768 and batch: 700, loss is 4.245844144821167 and perplexity is 69.81466896786641
At time: 1107.0143446922302 and batch: 750, loss is 4.222443823814392 and perplexity is 68.19994946143007
At time: 1108.2169427871704 and batch: 800, loss is 4.243269901275635 and perplexity is 69.63518013000231
At time: 1109.4205033779144 and batch: 850, loss is 4.263622245788574 and perplexity is 71.06693971705015
At time: 1110.6235263347626 and batch: 900, loss is 4.20784776687622 and perplexity is 67.21172872354106
At time: 1111.8275294303894 and batch: 950, loss is 4.2088906192779545 and perplexity is 67.28185719673282
At time: 1113.0306432247162 and batch: 1000, loss is 4.196997418403625 and perplexity is 66.48640019515722
At time: 1114.2346684932709 and batch: 1050, loss is 4.193158464431763 and perplexity is 66.23165126263125
At time: 1115.4382684230804 and batch: 1100, loss is 4.137422561645508 and perplexity is 62.641159474810706
At time: 1116.6416218280792 and batch: 1150, loss is 4.155867729187012 and perplexity is 63.80730799627058
At time: 1117.8443665504456 and batch: 1200, loss is 4.169835333824158 and perplexity is 64.70479653871655
At time: 1119.0485572814941 and batch: 1250, loss is 4.24402799129486 and perplexity is 69.68798987978879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713535726505475 and perplexity of 111.44550494142331
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1121.9758865833282 and batch: 50, loss is 4.30938611984253 and perplexity is 74.39480542480908
At time: 1123.2076711654663 and batch: 100, loss is 4.292825841903687 and perplexity is 73.17295181107606
At time: 1124.4133727550507 and batch: 150, loss is 4.223244814872742 and perplexity is 68.25459889506057
At time: 1125.6193115711212 and batch: 200, loss is 4.286548089981079 and perplexity is 72.71502903884327
At time: 1126.8255469799042 and batch: 250, loss is 4.281109943389892 and perplexity is 72.32066732184663
At time: 1128.0320677757263 and batch: 300, loss is 4.275187168121338 and perplexity is 71.89359423942282
At time: 1129.2396259307861 and batch: 350, loss is 4.2713045883178715 and perplexity is 71.61500279914704
At time: 1130.446299791336 and batch: 400, loss is 4.2659115219116215 and perplexity is 71.2298179307511
At time: 1131.653823852539 and batch: 450, loss is 4.217527084350586 and perplexity is 67.86545107305265
At time: 1132.863132238388 and batch: 500, loss is 4.222641544342041 and perplexity is 68.21343532459527
At time: 1134.0716989040375 and batch: 550, loss is 4.218401746749878 and perplexity is 67.92483639858693
At time: 1135.3057730197906 and batch: 600, loss is 4.2497570514678955 and perplexity is 70.08838240844867
At time: 1136.5147030353546 and batch: 650, loss is 4.262176866531372 and perplexity is 70.96429523448302
At time: 1137.7297432422638 and batch: 700, loss is 4.245799646377564 and perplexity is 69.81156239287608
At time: 1138.9421977996826 and batch: 750, loss is 4.222389268875122 and perplexity is 68.19622891881689
At time: 1140.1552698612213 and batch: 800, loss is 4.243184547424317 and perplexity is 69.62923675283965
At time: 1141.3672597408295 and batch: 850, loss is 4.263488388061523 and perplexity is 71.05742749468759
At time: 1142.5747213363647 and batch: 900, loss is 4.207705254554749 and perplexity is 67.2021509065452
At time: 1143.781298160553 and batch: 950, loss is 4.208749408721924 and perplexity is 67.27235695905
At time: 1144.9920573234558 and batch: 1000, loss is 4.196841182708741 and perplexity is 66.47601345763306
At time: 1146.1989114284515 and batch: 1050, loss is 4.193020691871643 and perplexity is 66.22252698702769
At time: 1147.4070134162903 and batch: 1100, loss is 4.137254524230957 and perplexity is 62.630634300664376
At time: 1148.6280550956726 and batch: 1150, loss is 4.155713233947754 and perplexity is 63.797450832417354
At time: 1149.8516149520874 and batch: 1200, loss is 4.169690828323365 and perplexity is 64.69544701523408
At time: 1151.0659682750702 and batch: 1250, loss is 4.24390016078949 and perplexity is 69.67908219817306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713530380360401 and perplexity of 111.44490913917875
Finished 36 epochs...
Completing Train Step...
At time: 1154.0382344722748 and batch: 50, loss is 4.309363079071045 and perplexity is 74.39309133084475
At time: 1155.2631833553314 and batch: 100, loss is 4.292795867919922 and perplexity is 73.17075855907683
At time: 1156.4759006500244 and batch: 150, loss is 4.223211579322815 and perplexity is 68.25233045362788
At time: 1157.685525894165 and batch: 200, loss is 4.286514568328857 and perplexity is 72.71259155178304
At time: 1158.8897967338562 and batch: 250, loss is 4.281083250045777 and perplexity is 72.3187368671524
At time: 1160.0940718650818 and batch: 300, loss is 4.275166330337524 and perplexity is 71.89209615185692
At time: 1161.2974863052368 and batch: 350, loss is 4.271278419494629 and perplexity is 71.61312874331828
At time: 1162.503357887268 and batch: 400, loss is 4.265889253616333 and perplexity is 71.22823178179254
At time: 1163.7074444293976 and batch: 450, loss is 4.2175048017501835 and perplexity is 67.86393887117323
At time: 1164.9430310726166 and batch: 500, loss is 4.222626457214355 and perplexity is 68.21240618755002
At time: 1166.1510467529297 and batch: 550, loss is 4.218386116027832 and perplexity is 67.92377469264684
At time: 1167.356323003769 and batch: 600, loss is 4.249742212295533 and perplexity is 70.0873423625782
At time: 1168.5598831176758 and batch: 650, loss is 4.2621630191802975 and perplexity is 70.96331257377678
At time: 1169.7635490894318 and batch: 700, loss is 4.245787878036499 and perplexity is 69.81074083143376
At time: 1170.9699470996857 and batch: 750, loss is 4.2223823499679565 and perplexity is 68.19575707707227
At time: 1172.1789844036102 and batch: 800, loss is 4.243182792663574 and perplexity is 69.62911457029563
At time: 1173.3864011764526 and batch: 850, loss is 4.263492250442505 and perplexity is 71.0577019460742
At time: 1174.5893886089325 and batch: 900, loss is 4.207705316543579 and perplexity is 67.20215507232805
At time: 1175.793200969696 and batch: 950, loss is 4.208750400543213 and perplexity is 67.27242368123889
At time: 1176.9981143474579 and batch: 1000, loss is 4.196848316192627 and perplexity is 66.47648766489526
At time: 1178.2051537036896 and batch: 1050, loss is 4.19303297996521 and perplexity is 66.22334074063527
At time: 1179.408530473709 and batch: 1100, loss is 4.137273421287537 and perplexity is 62.63181784648713
At time: 1180.6176850795746 and batch: 1150, loss is 4.155731134414673 and perplexity is 63.79859284679673
At time: 1181.8206250667572 and batch: 1200, loss is 4.169705486297607 and perplexity is 64.69639532638017
At time: 1183.0258839130402 and batch: 1250, loss is 4.243906121253968 and perplexity is 69.67949751910508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713526370751596 and perplexity of 111.44446228958564
Finished 37 epochs...
Completing Train Step...
At time: 1185.9791252613068 and batch: 50, loss is 4.309340505599976 and perplexity is 74.39141203950363
At time: 1187.184993982315 and batch: 100, loss is 4.292767181396484 and perplexity is 73.16865957450291
At time: 1188.389351606369 and batch: 150, loss is 4.2231818246841435 and perplexity is 68.25029966040967
At time: 1189.594140291214 and batch: 200, loss is 4.286484260559082 and perplexity is 72.71038782869363
At time: 1190.798727273941 and batch: 250, loss is 4.281057710647583 and perplexity is 72.31688991371983
At time: 1192.0030853748322 and batch: 300, loss is 4.275144357681274 and perplexity is 71.89051650889563
At time: 1193.2158842086792 and batch: 350, loss is 4.271254148483276 and perplexity is 71.61139064135038
At time: 1194.4297909736633 and batch: 400, loss is 4.26586763381958 and perplexity is 71.22669185854483
At time: 1195.6767718791962 and batch: 450, loss is 4.217484092712402 and perplexity is 67.86253348885126
At time: 1196.88157248497 and batch: 500, loss is 4.222611408233643 and perplexity is 68.21137966808902
At time: 1198.086805343628 and batch: 550, loss is 4.218370819091797 and perplexity is 67.92273567495701
At time: 1199.292230606079 and batch: 600, loss is 4.2497280311584475 and perplexity is 70.08634845141563
At time: 1200.4965333938599 and batch: 650, loss is 4.26215163230896 and perplexity is 70.96250452826739
At time: 1201.700321674347 and batch: 700, loss is 4.245779762268066 and perplexity is 69.81017426592615
At time: 1202.904061794281 and batch: 750, loss is 4.222378067970276 and perplexity is 68.19546506362383
At time: 1204.1079087257385 and batch: 800, loss is 4.243180303573609 and perplexity is 69.62894125738096
At time: 1205.3126459121704 and batch: 850, loss is 4.263494997024536 and perplexity is 71.05789711214956
At time: 1206.519559621811 and batch: 900, loss is 4.207704854011536 and perplexity is 67.20212398918517
At time: 1207.7310543060303 and batch: 950, loss is 4.208751316070557 and perplexity is 67.27248527101044
At time: 1208.9373602867126 and batch: 1000, loss is 4.196853785514832 and perplexity is 66.47685124721963
At time: 1210.1443610191345 and batch: 1050, loss is 4.193044767379761 and perplexity is 66.2241213472062
At time: 1211.3508567810059 and batch: 1100, loss is 4.137289543151855 and perplexity is 62.632827596295954
At time: 1212.5562844276428 and batch: 1150, loss is 4.155746936798096 and perplexity is 63.79960102458853
At time: 1213.765972852707 and batch: 1200, loss is 4.1697186803817745 and perplexity is 64.69724894169676
At time: 1214.9701895713806 and batch: 1250, loss is 4.2439116191864015 and perplexity is 69.67988061332757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713520133582345 and perplexity of 111.44376719377993
Finished 38 epochs...
Completing Train Step...
At time: 1217.9296271800995 and batch: 50, loss is 4.309317893981934 and perplexity is 74.38972994832646
At time: 1219.1552298069 and batch: 100, loss is 4.292739772796631 and perplexity is 73.1666541514739
At time: 1220.3592648506165 and batch: 150, loss is 4.223154315948486 and perplexity is 68.24842220678109
At time: 1221.5652318000793 and batch: 200, loss is 4.286455879211426 and perplexity is 72.70832423918233
At time: 1222.7686800956726 and batch: 250, loss is 4.281033229827881 and perplexity is 72.3151195586464
At time: 1223.9723768234253 and batch: 300, loss is 4.2751220607757565 and perplexity is 71.88891359071155
At time: 1225.1763491630554 and batch: 350, loss is 4.271231184005737 and perplexity is 71.60974614206107
At time: 1226.4047327041626 and batch: 400, loss is 4.26584638595581 and perplexity is 71.22517845957775
At time: 1227.6135246753693 and batch: 450, loss is 4.217464280128479 and perplexity is 67.86118897003054
At time: 1228.823434829712 and batch: 500, loss is 4.222596492767334 and perplexity is 68.21036227114118
At time: 1230.027711391449 and batch: 550, loss is 4.218355774879456 and perplexity is 67.92171383858506
At time: 1231.2327237129211 and batch: 600, loss is 4.249714345932007 and perplexity is 70.0853893104297
At time: 1232.4368257522583 and batch: 650, loss is 4.262141437530517 and perplexity is 70.96178108494365
At time: 1233.6436104774475 and batch: 700, loss is 4.245773401260376 and perplexity is 69.80973020428311
At time: 1234.8483085632324 and batch: 750, loss is 4.222375082969665 and perplexity is 68.19526150042282
At time: 1236.0527222156525 and batch: 800, loss is 4.243177433013916 and perplexity is 69.62874138363559
At time: 1237.2566323280334 and batch: 850, loss is 4.263496761322021 and perplexity is 71.05802247952931
At time: 1238.461102962494 and batch: 900, loss is 4.20770387172699 and perplexity is 67.20205797760973
At time: 1239.665293931961 and batch: 950, loss is 4.208752031326294 and perplexity is 67.2725333880587
At time: 1240.871533870697 and batch: 1000, loss is 4.196858444213867 and perplexity is 66.4771609435838
At time: 1242.0762112140656 and batch: 1050, loss is 4.193056063652039 and perplexity is 66.22486943713757
At time: 1243.280942440033 and batch: 1100, loss is 4.137303609848022 and perplexity is 62.6337086394485
At time: 1244.4854352474213 and batch: 1150, loss is 4.155761308670044 and perplexity is 63.80051795087378
At time: 1245.6898818016052 and batch: 1200, loss is 4.169731059074402 and perplexity is 64.69804981401211
At time: 1246.896357536316 and batch: 1250, loss is 4.24391667842865 and perplexity is 69.68023314161519
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713517014997719 and perplexity of 111.44341964750282
Finished 39 epochs...
Completing Train Step...
At time: 1249.8855085372925 and batch: 50, loss is 4.309295806884766 and perplexity is 74.38808691327779
At time: 1251.0892431735992 and batch: 100, loss is 4.292713165283203 and perplexity is 73.16470739464037
At time: 1252.2920048236847 and batch: 150, loss is 4.22312819480896 and perplexity is 68.2466395035054
At time: 1253.495584487915 and batch: 200, loss is 4.286429080963135 and perplexity is 72.70637580956394
At time: 1254.6998081207275 and batch: 250, loss is 4.281009368896484 and perplexity is 72.31339407312558
At time: 1255.9042422771454 and batch: 300, loss is 4.275099773406982 and perplexity is 71.88731139383799
At time: 1257.1613066196442 and batch: 350, loss is 4.271208925247192 and perplexity is 71.60815221575172
At time: 1258.3694515228271 and batch: 400, loss is 4.265825395584106 and perplexity is 71.22368343229785
At time: 1259.5725183486938 and batch: 450, loss is 4.2174452209472655 and perplexity is 67.85989560365788
At time: 1260.7764854431152 and batch: 500, loss is 4.222581663131714 and perplexity is 68.2093507438235
At time: 1261.9808585643768 and batch: 550, loss is 4.218340744972229 and perplexity is 67.92069298919908
At time: 1263.1848559379578 and batch: 600, loss is 4.249701108932495 and perplexity is 70.08446159630572
At time: 1264.3987827301025 and batch: 650, loss is 4.262132024765014 and perplexity is 70.96111314148223
At time: 1265.6014528274536 and batch: 700, loss is 4.245768213272095 and perplexity is 69.8093680331604
At time: 1266.8046517372131 and batch: 750, loss is 4.222372932434082 and perplexity is 68.19511484424403
At time: 1268.010606765747 and batch: 800, loss is 4.2431743574142455 and perplexity is 69.62852723383088
At time: 1269.2151703834534 and batch: 850, loss is 4.2634978771209715 and perplexity is 71.05810176604044
At time: 1270.4192945957184 and batch: 900, loss is 4.207702412605285 and perplexity is 67.20195992169988
At time: 1271.626106262207 and batch: 950, loss is 4.208752698898316 and perplexity is 67.2725782973348
At time: 1272.8314592838287 and batch: 1000, loss is 4.196862473487854 and perplexity is 66.4774287988187
At time: 1274.0365755558014 and batch: 1050, loss is 4.193066682815552 and perplexity is 66.22557269358875
At time: 1275.2414350509644 and batch: 1100, loss is 4.137316207885743 and perplexity is 62.634497706242854
At time: 1276.4481010437012 and batch: 1150, loss is 4.155774745941162 and perplexity is 63.801375261490904
At time: 1277.6640598773956 and batch: 1200, loss is 4.169742622375488 and perplexity is 64.69879794136722
At time: 1278.866773366928 and batch: 1250, loss is 4.243921546936035 and perplexity is 69.68057238117066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713514787437272 and perplexity of 111.4431714008256
Finished 40 epochs...
Completing Train Step...
At time: 1281.8238043785095 and batch: 50, loss is 4.309273900985718 and perplexity is 74.38645739320364
At time: 1283.03191781044 and batch: 100, loss is 4.292687482833863 and perplexity is 73.16282836987828
At time: 1284.2400522232056 and batch: 150, loss is 4.223103094100952 and perplexity is 68.24492648603376
At time: 1285.4472053050995 and batch: 200, loss is 4.286403188705444 and perplexity is 72.70449330171701
At time: 1286.6796412467957 and batch: 250, loss is 4.280986166000366 and perplexity is 72.31171621242062
At time: 1287.8888721466064 and batch: 300, loss is 4.275077819824219 and perplexity is 71.88573322712095
At time: 1289.0959541797638 and batch: 350, loss is 4.271187477111816 and perplexity is 71.60661637087951
At time: 1290.303277015686 and batch: 400, loss is 4.265804796218872 and perplexity is 71.22221628474072
At time: 1291.5116662979126 and batch: 450, loss is 4.217426705360412 and perplexity is 67.85863914949901
At time: 1292.7242274284363 and batch: 500, loss is 4.222566976547241 and perplexity is 68.20834898878815
At time: 1293.9545073509216 and batch: 550, loss is 4.218325848579407 and perplexity is 67.91968122341139
At time: 1295.1819434165955 and batch: 600, loss is 4.2496878528594975 and perplexity is 70.08353255772452
At time: 1296.3943047523499 and batch: 650, loss is 4.262123117446899 and perplexity is 70.96048107108871
At time: 1297.6015446186066 and batch: 700, loss is 4.245763330459595 and perplexity is 69.80902716793774
At time: 1298.8088822364807 and batch: 750, loss is 4.222371096611023 and perplexity is 68.19498965019461
At time: 1300.0227584838867 and batch: 800, loss is 4.243170924186707 and perplexity is 69.62828818366405
At time: 1301.2448704242706 and batch: 850, loss is 4.2634983825683594 and perplexity is 71.05813768218144
At time: 1302.453183889389 and batch: 900, loss is 4.2077006912231445 and perplexity is 67.20184424154581
At time: 1303.6656811237335 and batch: 950, loss is 4.208753061294556 and perplexity is 67.27260267666867
At time: 1304.873349905014 and batch: 1000, loss is 4.19686580657959 and perplexity is 66.47765037455653
At time: 1306.0822896957397 and batch: 1050, loss is 4.19307665348053 and perplexity is 66.22623300987895
At time: 1307.2898135185242 and batch: 1100, loss is 4.137327747344971 and perplexity is 62.63522047864561
At time: 1308.4978642463684 and batch: 1150, loss is 4.1557874011993405 and perplexity is 63.80218268947606
At time: 1309.7059655189514 and batch: 1200, loss is 4.169753799438476 and perplexity is 64.69952108794838
At time: 1310.9153900146484 and batch: 1250, loss is 4.243926138877868 and perplexity is 69.68089235104057
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713513896413093 and perplexity of 111.44307210230956
Finished 41 epochs...
Completing Train Step...
At time: 1313.8634548187256 and batch: 50, loss is 4.309252462387085 and perplexity is 74.38486266889423
At time: 1315.089587688446 and batch: 100, loss is 4.292662467956543 and perplexity is 73.16099823359265
At time: 1316.2991943359375 and batch: 150, loss is 4.223078761100769 and perplexity is 68.24326590242867
At time: 1317.55477309227 and batch: 200, loss is 4.286378164291381 and perplexity is 72.70267393713677
At time: 1318.7583844661713 and batch: 250, loss is 4.280963239669799 and perplexity is 72.31005838911489
At time: 1319.9630856513977 and batch: 300, loss is 4.275055847167969 and perplexity is 71.88415372396848
At time: 1321.1664209365845 and batch: 350, loss is 4.271166477203369 and perplexity is 71.60511265428055
At time: 1322.3704710006714 and batch: 400, loss is 4.265784034729004 and perplexity is 71.22073762076859
At time: 1323.5758726596832 and batch: 450, loss is 4.217408571243286 and perplexity is 67.85740860414612
At time: 1324.7804987430573 and batch: 500, loss is 4.2225524425506595 and perplexity is 68.20735765608116
At time: 1325.984848022461 and batch: 550, loss is 4.218310761451721 and perplexity is 67.91865651823834
At time: 1327.1889207363129 and batch: 600, loss is 4.249674768447876 and perplexity is 70.08261556193582
At time: 1328.3922228813171 and batch: 650, loss is 4.262114515304566 and perplexity is 70.9598706615559
At time: 1329.5959117412567 and batch: 700, loss is 4.245758876800537 and perplexity is 69.80871626302392
At time: 1330.7992708683014 and batch: 750, loss is 4.222369427680969 and perplexity is 68.19487583762185
At time: 1332.0036742687225 and batch: 800, loss is 4.243167462348938 and perplexity is 69.6280471422435
At time: 1333.207530260086 and batch: 850, loss is 4.263498439788818 and perplexity is 71.05814174816078
At time: 1334.410810470581 and batch: 900, loss is 4.20769853591919 and perplexity is 67.20169940130124
At time: 1335.6142148971558 and batch: 950, loss is 4.208753147125244 and perplexity is 67.2726084507227
At time: 1336.8223528862 and batch: 1000, loss is 4.196868858337402 and perplexity is 66.477853248555
At time: 1338.0283279418945 and batch: 1050, loss is 4.193086004257202 and perplexity is 66.22685227948898
At time: 1339.2325315475464 and batch: 1100, loss is 4.137338747978211 and perplexity is 62.63590950952388
At time: 1340.4381489753723 and batch: 1150, loss is 4.155799474716186 and perplexity is 63.8029530108538
At time: 1341.641522884369 and batch: 1200, loss is 4.16976459980011 and perplexity is 64.70021986994719
At time: 1342.8457486629486 and batch: 1250, loss is 4.243930616378784 and perplexity is 69.68120434799837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713513005388914 and perplexity of 111.44297280388199
Finished 42 epochs...
Completing Train Step...
At time: 1345.854719877243 and batch: 50, loss is 4.309231204986572 and perplexity is 74.38328145688268
At time: 1347.0627093315125 and batch: 100, loss is 4.2926380062103275 and perplexity is 73.15920860970976
At time: 1348.2955558300018 and batch: 150, loss is 4.223055081367493 and perplexity is 68.24164993922706
At time: 1349.5019943714142 and batch: 200, loss is 4.2863537693023686 and perplexity is 72.70090037783793
At time: 1350.711927652359 and batch: 250, loss is 4.2809406280517575 and perplexity is 72.30842336017939
At time: 1351.9208822250366 and batch: 300, loss is 4.275034437179565 and perplexity is 71.88261470154613
At time: 1353.1281237602234 and batch: 350, loss is 4.27114580154419 and perplexity is 71.60363218668067
At time: 1354.3356549739838 and batch: 400, loss is 4.265763921737671 and perplexity is 71.21930517309556
At time: 1355.543931722641 and batch: 450, loss is 4.217390871047973 and perplexity is 67.85620752539008
At time: 1356.7517013549805 and batch: 500, loss is 4.2225380134582515 and perplexity is 68.20637349291493
At time: 1357.962468624115 and batch: 550, loss is 4.218295884132385 and perplexity is 67.91764607821281
At time: 1359.16929936409 and batch: 600, loss is 4.249661979675293 and perplexity is 70.08171929703444
At time: 1360.377325296402 and batch: 650, loss is 4.262106113433838 and perplexity is 70.9592744684003
At time: 1361.585479259491 and batch: 700, loss is 4.245754518508911 and perplexity is 69.8084120169434
At time: 1362.8035655021667 and batch: 750, loss is 4.222367835044861 and perplexity is 68.19476722808668
At time: 1364.027683019638 and batch: 800, loss is 4.243163952827453 and perplexity is 69.62780278154489
At time: 1365.2432231903076 and batch: 850, loss is 4.263498096466065 and perplexity is 71.0581173522881
At time: 1366.4567377567291 and batch: 900, loss is 4.207696161270142 and perplexity is 67.20153982103919
At time: 1367.6701562404633 and batch: 950, loss is 4.208753318786621 and perplexity is 67.27261999883227
At time: 1368.8843717575073 and batch: 1000, loss is 4.196871500015259 and perplexity is 66.47802886185985
At time: 1370.1055862903595 and batch: 1050, loss is 4.193095049858093 and perplexity is 66.22745134387245
At time: 1371.3239135742188 and batch: 1100, loss is 4.137349009513855 and perplexity is 62.63655225343968
At time: 1372.5381870269775 and batch: 1150, loss is 4.155811233520508 and perplexity is 63.8037032617044
At time: 1373.7511582374573 and batch: 1200, loss is 4.169775004386902 and perplexity is 64.70089305250238
At time: 1374.9622733592987 and batch: 1250, loss is 4.2439350891113286 and perplexity is 69.68151601408579
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7135134509010035 and perplexity of 111.44302245308471
Annealing...
Finished 43 epochs...
Completing Train Step...
At time: 1377.945006608963 and batch: 50, loss is 4.309218034744263 and perplexity is 74.3823018174932
At time: 1379.1695749759674 and batch: 100, loss is 4.292628898620605 and perplexity is 73.15854230868753
At time: 1380.3731508255005 and batch: 150, loss is 4.2230472755432125 and perplexity is 68.24111725897805
At time: 1381.577207326889 and batch: 200, loss is 4.286350708007813 and perplexity is 72.70067781930807
At time: 1382.7811963558197 and batch: 250, loss is 4.280933103561401 and perplexity is 72.30787927819213
At time: 1383.9857347011566 and batch: 300, loss is 4.275028190612793 and perplexity is 71.88216568339605
At time: 1385.1896793842316 and batch: 350, loss is 4.271140365600586 and perplexity is 71.60324295443222
At time: 1386.3944408893585 and batch: 400, loss is 4.265749969482422 and perplexity is 71.21831151010306
At time: 1387.598613500595 and batch: 450, loss is 4.217389922142029 and perplexity is 67.85614313626198
At time: 1388.8033213615417 and batch: 500, loss is 4.222532253265381 and perplexity is 68.20598061218017
At time: 1390.0076730251312 and batch: 550, loss is 4.218264198303222 and perplexity is 67.91549408537604
At time: 1391.2117748260498 and batch: 600, loss is 4.24963532447815 and perplexity is 70.07985127988677
At time: 1392.4155442714691 and batch: 650, loss is 4.26208122253418 and perplexity is 70.95750825020117
At time: 1393.6194062232971 and batch: 700, loss is 4.245734338760376 and perplexity is 69.80700331495692
At time: 1394.8225519657135 and batch: 750, loss is 4.2223455572128294 and perplexity is 68.1932480134394
At time: 1396.026568889618 and batch: 800, loss is 4.243133182525635 and perplexity is 69.6256603460002
At time: 1397.2359240055084 and batch: 850, loss is 4.26345118522644 and perplexity is 71.05478400610403
At time: 1398.4402532577515 and batch: 900, loss is 4.207646837234497 and perplexity is 67.19822525163835
At time: 1399.6443192958832 and batch: 950, loss is 4.208705282211303 and perplexity is 67.26938853016986
At time: 1400.849083185196 and batch: 1000, loss is 4.196816935539245 and perplexity is 66.47440162200868
At time: 1402.0527458190918 and batch: 1050, loss is 4.193046097755432 and perplexity is 66.22420945022466
At time: 1403.256961107254 and batch: 1100, loss is 4.137287583351135 and perplexity is 62.63270484855562
At time: 1404.4616613388062 and batch: 1150, loss is 4.155755233764649 and perplexity is 63.8001303699403
At time: 1405.6655111312866 and batch: 1200, loss is 4.1697241306304935 and perplexity is 64.69760155875585
At time: 1406.8703405857086 and batch: 1250, loss is 4.243891124725342 and perplexity is 69.67845257636117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7135134509010035 and perplexity of 111.44302245308471
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1409.8327658176422 and batch: 50, loss is 4.309214200973511 and perplexity is 74.38201665334662
At time: 1411.0382027626038 and batch: 100, loss is 4.292625875473022 and perplexity is 73.1583211399515
At time: 1412.2430822849274 and batch: 150, loss is 4.223044176101684 and perplexity is 68.24090574995306
At time: 1413.4465970993042 and batch: 200, loss is 4.286349439620972 and perplexity is 72.70058560678348
At time: 1414.6517496109009 and batch: 250, loss is 4.280930843353271 and perplexity is 72.30771584752021
At time: 1415.8565819263458 and batch: 300, loss is 4.275026845932007 and perplexity is 71.88206902489397
At time: 1417.061388015747 and batch: 350, loss is 4.271138496398926 and perplexity is 71.6031091136567
At time: 1418.2662041187286 and batch: 400, loss is 4.26574592590332 and perplexity is 71.21802353380922
At time: 1419.4708840847015 and batch: 450, loss is 4.2173898935317995 and perplexity is 67.85614119488216
At time: 1420.6770884990692 and batch: 500, loss is 4.222531089782715 and perplexity is 68.20590125575013
At time: 1421.8821964263916 and batch: 550, loss is 4.218254308700562 and perplexity is 67.91482243144625
At time: 1423.086233139038 and batch: 600, loss is 4.249626541137696 and perplexity is 70.07923574739722
At time: 1424.2906908988953 and batch: 650, loss is 4.262072601318359 and perplexity is 70.95689651284545
At time: 1425.4956648349762 and batch: 700, loss is 4.2457265949249265 and perplexity is 69.80646274310308
At time: 1426.699886083603 and batch: 750, loss is 4.22233772277832 and perplexity is 68.19271375999666
At time: 1427.9054772853851 and batch: 800, loss is 4.2431234979629515 and perplexity is 69.62498605519332
At time: 1429.1240141391754 and batch: 850, loss is 4.26343581199646 and perplexity is 71.05369167296466
At time: 1430.338549375534 and batch: 900, loss is 4.207630748748779 and perplexity is 67.19714414264786
At time: 1431.5426743030548 and batch: 950, loss is 4.208689498901367 and perplexity is 67.26832680494026
At time: 1432.7514142990112 and batch: 1000, loss is 4.196798796653748 and perplexity is 66.47319586138475
At time: 1433.9624300003052 and batch: 1050, loss is 4.193029546737671 and perplexity is 66.22311338122842
At time: 1435.167233467102 and batch: 1100, loss is 4.137267055511475 and perplexity is 62.63141914762939
At time: 1436.372148990631 and batch: 1150, loss is 4.155736351013184 and perplexity is 63.798925659309276
At time: 1437.57803940773 and batch: 1200, loss is 4.169706630706787 and perplexity is 64.69646936557125
At time: 1438.783443927765 and batch: 1250, loss is 4.243876090049744 and perplexity is 69.6774049913056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.713513896413093 and perplexity of 111.44307210230956
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 1441.7348880767822 and batch: 50, loss is 4.309213399887085 and perplexity is 74.38195706694664
At time: 1442.9647417068481 and batch: 100, loss is 4.292625322341919 and perplexity is 73.15828067381977
At time: 1444.1690027713776 and batch: 150, loss is 4.223043699264526 and perplexity is 68.24087321016124
At time: 1445.374003648758 and batch: 200, loss is 4.286349439620972 and perplexity is 72.70058560678348
At time: 1446.5784423351288 and batch: 250, loss is 4.280930347442627 and perplexity is 72.30767998936317
At time: 1447.7828705310822 and batch: 300, loss is 4.275027227401734 and perplexity is 71.88209644573244
At time: 1448.9877610206604 and batch: 350, loss is 4.271138477325439 and perplexity is 71.60310774793575
At time: 1450.1914551258087 and batch: 400, loss is 4.265745286941528 and perplexity is 71.21797802822782
At time: 1451.3967196941376 and batch: 450, loss is 4.217390661239624 and perplexity is 67.85619328859268
At time: 1452.6083297729492 and batch: 500, loss is 4.222531385421753 and perplexity is 68.20592142008019
At time: 1453.8205053806305 and batch: 550, loss is 4.218251543045044 and perplexity is 67.91463460270258
At time: 1455.0273361206055 and batch: 600, loss is 4.249624223709106 and perplexity is 70.07907334396096
At time: 1456.238003730774 and batch: 650, loss is 4.26207010269165 and perplexity is 70.95671921827011
At time: 1457.443568944931 and batch: 700, loss is 4.245724620819092 and perplexity is 69.80632493789373
At time: 1458.6475582122803 and batch: 750, loss is 4.222335429191589 and perplexity is 68.19255735427258
At time: 1459.8519568443298 and batch: 800, loss is 4.243120632171631 and perplexity is 69.6247865247985
At time: 1461.0567741394043 and batch: 850, loss is 4.263431167602539 and perplexity is 71.05336167239732
At time: 1462.2613830566406 and batch: 900, loss is 4.207625946998596 and perplexity is 67.19682147952332
At time: 1463.466317653656 and batch: 950, loss is 4.208684968948364 and perplexity is 67.26802208327143
At time: 1464.6735770702362 and batch: 1000, loss is 4.196793212890625 and perplexity is 66.4728246918413
At time: 1465.8849339485168 and batch: 1050, loss is 4.193024353981018 and perplexity is 66.22276950160865
At time: 1467.097193479538 and batch: 1100, loss is 4.137260475158691 and perplexity is 62.63100701215206
At time: 1468.3240330219269 and batch: 1150, loss is 4.155730695724487 and perplexity is 63.79856485898634
At time: 1469.539627313614 and batch: 1200, loss is 4.169701457023621 and perplexity is 64.69613464740264
At time: 1470.7929155826569 and batch: 1250, loss is 4.243871541023254 and perplexity is 69.67708802766549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7135134509010035 and perplexity of 111.44302245308471
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1473.757660627365 and batch: 50, loss is 4.309213190078736 and perplexity is 74.38194146099264
At time: 1474.995230436325 and batch: 100, loss is 4.292625341415405 and perplexity is 73.1582820692033
At time: 1476.208441734314 and batch: 150, loss is 4.223043618202209 and perplexity is 68.24086767839817
At time: 1477.422122001648 and batch: 200, loss is 4.286349515914917 and perplexity is 72.70059115339821
At time: 1478.6355345249176 and batch: 250, loss is 4.280930395126343 and perplexity is 72.3076834372621
At time: 1479.8456423282623 and batch: 300, loss is 4.2750273609161376 and perplexity is 71.88210604302834
At time: 1481.053383588791 and batch: 350, loss is 4.271138772964478 and perplexity is 71.60312891661282
At time: 1482.2632222175598 and batch: 400, loss is 4.265745239257813 and perplexity is 71.21797463229008
At time: 1483.4721398353577 and batch: 450, loss is 4.217391104698181 and perplexity is 67.85622338000891
At time: 1484.6809046268463 and batch: 500, loss is 4.22253189086914 and perplexity is 68.20595589459367
At time: 1485.8954882621765 and batch: 550, loss is 4.218251080513 and perplexity is 67.9146031900151
At time: 1487.1111204624176 and batch: 600, loss is 4.249623718261719 and perplexity is 70.0790379226854
At time: 1488.3261632919312 and batch: 650, loss is 4.262069625854492 and perplexity is 70.95668538347783
At time: 1489.5397553443909 and batch: 700, loss is 4.245724124908447 and perplexity is 69.80629032020269
At time: 1490.7482359409332 and batch: 750, loss is 4.222334957122802 and perplexity is 68.19252516270237
At time: 1491.9546840190887 and batch: 800, loss is 4.243119568824768 and perplexity is 69.62471248953952
At time: 1493.1635344028473 and batch: 850, loss is 4.263430080413818 and perplexity is 71.05328442402595
At time: 1494.3787450790405 and batch: 900, loss is 4.207624745368958 and perplexity is 67.19674073387954
At time: 1495.5975089073181 and batch: 950, loss is 4.208683996200562 and perplexity is 67.26795664848261
At time: 1496.810533285141 and batch: 1000, loss is 4.196791925430298 and perplexity is 66.47273911077178
At time: 1498.0143218040466 and batch: 1050, loss is 4.193023271560669 and perplexity is 66.22269782077419
At time: 1499.2183499336243 and batch: 1100, loss is 4.137258853912353 and perplexity is 62.630905471943606
At time: 1500.44784116745 and batch: 1150, loss is 4.155729274749756 and perplexity is 63.79847420290222
At time: 1501.6522469520569 and batch: 1200, loss is 4.1697002792358395 and perplexity is 64.69605844913065
At time: 1502.8565483093262 and batch: 1250, loss is 4.243870506286621 and perplexity is 69.67701593026732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7135134509010035 and perplexity of 111.44302245308471
Annealing...
Model not improving. Stopping early with 111.44297280388199loss at 46 epochs.
Finished Training.
Improved accuracyfrom -179.27424501642665 to -111.44297280388199
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f71a20f3128>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 3.9830090235067015, 'data': 'wikitext', 'dropout': 0.5225947922390826, 'tune_wordvecs': True, 'lr': 5.2788791078727195, 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7746343612670898 and batch: 50, loss is 7.326071662902832 and perplexity is 1519.401312509507
At time: 2.9854023456573486 and batch: 100, loss is 6.474301481246949 and perplexity is 648.2662432923662
At time: 4.196413516998291 and batch: 150, loss is 6.205604124069214 and perplexity is 495.51822001821495
At time: 5.406264305114746 and batch: 200, loss is 6.100080652236938 and perplexity is 445.8937309591735
At time: 6.616257429122925 and batch: 250, loss is 6.066990404129029 and perplexity is 431.38044515626393
At time: 7.826817512512207 and batch: 300, loss is 6.0280045223236085 and perplexity is 414.88630644516707
At time: 9.038563966751099 and batch: 350, loss is 6.026247043609619 and perplexity is 414.15779295385596
At time: 10.25490117073059 and batch: 400, loss is 5.95230390548706 and perplexity is 384.6384897448808
At time: 11.470154285430908 and batch: 450, loss is 5.908345050811768 and perplexity is 368.0964700950275
At time: 12.681722164154053 and batch: 500, loss is 5.885932722091675 and perplexity is 359.9383339130866
At time: 13.89968729019165 and batch: 550, loss is 5.882634105682373 and perplexity is 358.7529914882871
At time: 15.14423131942749 and batch: 600, loss is 5.902351713180542 and perplexity is 365.89694151153645
At time: 16.359644412994385 and batch: 650, loss is 5.8596546459198 and perplexity is 350.60304091868034
At time: 17.577582597732544 and batch: 700, loss is 5.866317806243896 and perplexity is 352.9469654909771
At time: 18.794620037078857 and batch: 750, loss is 5.80455885887146 and perplexity is 331.8087865503339
At time: 20.01313304901123 and batch: 800, loss is 5.801575860977173 and perplexity is 330.82047643509907
At time: 21.235501289367676 and batch: 850, loss is 5.858131742477417 and perplexity is 350.0695126998504
At time: 22.463336944580078 and batch: 900, loss is 5.832971162796021 and perplexity is 341.37144391588146
At time: 23.685085773468018 and batch: 950, loss is 5.793411064147949 and perplexity is 328.1303914012186
At time: 24.902710437774658 and batch: 1000, loss is 5.782381467819214 and perplexity is 324.5311313217759
At time: 26.127209424972534 and batch: 1050, loss is 5.759027872085571 and perplexity is 317.0399756524043
At time: 27.349599838256836 and batch: 1100, loss is 5.7482528972625735 and perplexity is 313.64221615665923
At time: 28.571484804153442 and batch: 1150, loss is 5.790483741760254 and perplexity is 327.1712524996987
At time: 29.787183046340942 and batch: 1200, loss is 5.781552848815918 and perplexity is 324.2623300415061
At time: 31.00006937980652 and batch: 1250, loss is 5.75227204322815 and perplexity is 314.9053266167067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.196316684249544 and perplexity of 180.60578710667696
Finished 1 epochs...
Completing Train Step...
At time: 33.991541624069214 and batch: 50, loss is 5.444809532165527 and perplexity is 231.55317208046375
At time: 35.19403553009033 and batch: 100, loss is 5.4086507797241214 and perplexity is 223.33006286555874
At time: 36.39865779876709 and batch: 150, loss is 5.264589452743531 and perplexity is 193.36690623643992
At time: 37.60361361503601 and batch: 200, loss is 5.257855348587036 and perplexity is 192.06912794084948
At time: 38.81037402153015 and batch: 250, loss is 5.260503721237183 and perplexity is 192.5784727359312
At time: 40.01471447944641 and batch: 300, loss is 5.2451050662994385 and perplexity is 189.63573850815513
At time: 41.221625566482544 and batch: 350, loss is 5.24888090133667 and perplexity is 190.35312528868516
At time: 42.43225312232971 and batch: 400, loss is 5.207434883117676 and perplexity is 182.62500238267413
At time: 43.63749957084656 and batch: 450, loss is 5.160012483596802 and perplexity is 174.16662981752273
At time: 44.84334111213684 and batch: 500, loss is 5.1504818630218505 and perplexity is 172.51459869766023
At time: 46.05540227890015 and batch: 550, loss is 5.152078914642334 and perplexity is 172.79033353984877
At time: 47.25770378112793 and batch: 600, loss is 5.152093067169189 and perplexity is 172.79277897698907
At time: 48.46207332611084 and batch: 650, loss is 5.12862608909607 and perplexity is 168.78506303141398
At time: 49.67189979553223 and batch: 700, loss is 5.11964204788208 and perplexity is 167.27548229018325
At time: 50.87875461578369 and batch: 750, loss is 5.098130302429199 and perplexity is 163.71552245102308
At time: 52.08838200569153 and batch: 800, loss is 5.0997656631469725 and perplexity is 163.98347542549564
At time: 53.300448179244995 and batch: 850, loss is 5.153292894363403 and perplexity is 173.00022487689918
At time: 54.51278209686279 and batch: 900, loss is 5.12151593208313 and perplexity is 167.5892310462792
At time: 55.72516465187073 and batch: 950, loss is 5.092799549102783 and perplexity is 162.8451174036179
At time: 56.93764114379883 and batch: 1000, loss is 5.074066324234009 and perplexity is 159.82289952672087
At time: 58.14821982383728 and batch: 1050, loss is 5.049194574356079 and perplexity is 155.89685058558896
At time: 59.35726308822632 and batch: 1100, loss is 5.012467880249023 and perplexity is 150.2751399547488
At time: 60.56298017501831 and batch: 1150, loss is 5.053102321624756 and perplexity is 156.50724794020252
At time: 61.770607471466064 and batch: 1200, loss is 5.033245124816895 and perplexity is 153.43010553836237
At time: 62.973737955093384 and batch: 1250, loss is 5.021204833984375 and perplexity is 151.59383921884285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.832172031820256 and perplexity of 125.4832184147748
Finished 2 epochs...
Completing Train Step...
At time: 65.95318913459778 and batch: 50, loss is 4.927262926101685 and perplexity is 138.00127525623054
At time: 67.16214394569397 and batch: 100, loss is 4.944695024490357 and perplexity is 140.4280172140549
At time: 68.36922407150269 and batch: 150, loss is 4.833478212356567 and perplexity is 125.64722924285927
At time: 69.57574725151062 and batch: 200, loss is 4.87917950630188 and perplexity is 131.52270605105065
At time: 70.78292918205261 and batch: 250, loss is 4.881822204589843 and perplexity is 131.87074055370036
At time: 71.99121975898743 and batch: 300, loss is 4.878530855178833 and perplexity is 131.43742136305906
At time: 73.20017719268799 and batch: 350, loss is 4.872976779937744 and perplexity is 130.7094315617825
At time: 74.40634274482727 and batch: 400, loss is 4.869721736907959 and perplexity is 130.28465844009463
At time: 75.61278247833252 and batch: 450, loss is 4.824297618865967 and perplexity is 124.4989919227512
At time: 76.81957077980042 and batch: 500, loss is 4.818803806304931 and perplexity is 123.81689317401637
At time: 78.02691888809204 and batch: 550, loss is 4.8238739395141605 and perplexity is 124.44625544302046
At time: 79.23656034469604 and batch: 600, loss is 4.830939111709594 and perplexity is 125.32860296507519
At time: 80.44610023498535 and batch: 650, loss is 4.83098069190979 and perplexity is 125.33381426181938
At time: 81.65104079246521 and batch: 700, loss is 4.823345613479614 and perplexity is 124.38052461155185
At time: 82.86465454101562 and batch: 750, loss is 4.8100447177886965 and perplexity is 122.73710592651754
At time: 84.07104229927063 and batch: 800, loss is 4.824047737121582 and perplexity is 124.4678857840653
At time: 85.27955937385559 and batch: 850, loss is 4.872713899612426 and perplexity is 130.675075139903
At time: 86.49792528152466 and batch: 900, loss is 4.8451607799530025 and perplexity is 127.12371929635259
At time: 87.70544624328613 and batch: 950, loss is 4.829141845703125 and perplexity is 125.10355642215664
At time: 88.95494389533997 and batch: 1000, loss is 4.8127375411987305 and perplexity is 123.06806068000263
At time: 90.16178965568542 and batch: 1050, loss is 4.780447664260865 and perplexity is 119.15768074187532
At time: 91.36811828613281 and batch: 1100, loss is 4.748118009567261 and perplexity is 115.36696057447712
At time: 92.57472634315491 and batch: 1150, loss is 4.790334396362304 and perplexity is 120.34160373013177
At time: 93.78148746490479 and batch: 1200, loss is 4.776440982818603 and perplexity is 118.6812090463137
At time: 94.98822116851807 and batch: 1250, loss is 4.778414287567139 and perplexity is 118.915634460008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.740479406649179 and perplexity of 114.48907534968767
Finished 3 epochs...
Completing Train Step...
At time: 97.92621088027954 and batch: 50, loss is 4.702464447021485 and perplexity is 110.21846559026872
At time: 99.15138244628906 and batch: 100, loss is 4.728479804992676 and perplexity is 113.12346184673096
At time: 100.35505509376526 and batch: 150, loss is 4.6237436866760255 and perplexity is 101.87470610276443
At time: 101.55952858924866 and batch: 200, loss is 4.674637060165406 and perplexity is 107.19365518364891
At time: 102.76379585266113 and batch: 250, loss is 4.679230127334595 and perplexity is 107.68713526805688
At time: 103.96781635284424 and batch: 300, loss is 4.68232141494751 and perplexity is 108.02054223829523
At time: 105.17211890220642 and batch: 350, loss is 4.67295560836792 and perplexity is 107.01356566781162
At time: 106.3757894039154 and batch: 400, loss is 4.671205501556397 and perplexity is 106.82644428657376
At time: 107.57979011535645 and batch: 450, loss is 4.625819034576416 and perplexity is 102.08635110271673
At time: 108.7908570766449 and batch: 500, loss is 4.629380722045898 and perplexity is 102.45059906388482
At time: 109.99664568901062 and batch: 550, loss is 4.641489486694336 and perplexity is 103.69869042855224
At time: 111.19979786872864 and batch: 600, loss is 4.650049552917481 and perplexity is 104.59016819668012
At time: 112.40305399894714 and batch: 650, loss is 4.65265606880188 and perplexity is 104.86313972920887
At time: 113.60567474365234 and batch: 700, loss is 4.64254014968872 and perplexity is 103.80770006131729
At time: 114.80846619606018 and batch: 750, loss is 4.641551370620728 and perplexity is 103.705107909245
At time: 116.0133695602417 and batch: 800, loss is 4.661181764602661 and perplexity is 105.76099293234545
At time: 117.215487241745 and batch: 850, loss is 4.709231271743774 and perplexity is 110.96682377681392
At time: 118.44535231590271 and batch: 900, loss is 4.675821609497071 and perplexity is 107.32070659072902
At time: 119.6494071483612 and batch: 950, loss is 4.667783603668213 and perplexity is 106.46151982567508
At time: 120.8534882068634 and batch: 1000, loss is 4.646397829055786 and perplexity is 104.20893029520185
At time: 122.05680179595947 and batch: 1050, loss is 4.618944034576416 and perplexity is 101.38691450712307
At time: 123.25937366485596 and batch: 1100, loss is 4.580237913131714 and perplexity is 97.5375969219728
At time: 124.46466088294983 and batch: 1150, loss is 4.6154185390472415 and perplexity is 101.03010472852017
At time: 125.66986560821533 and batch: 1200, loss is 4.612728147506714 and perplexity is 100.75865950012442
At time: 126.87483072280884 and batch: 1250, loss is 4.62441840171814 and perplexity is 101.94346569333946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.689498121721031 and perplexity of 108.7985624676486
Finished 4 epochs...
Completing Train Step...
At time: 129.85047888755798 and batch: 50, loss is 4.549996919631958 and perplexity is 94.63211681272676
At time: 131.05405497550964 and batch: 100, loss is 4.577723960876465 and perplexity is 97.2927000187585
At time: 132.25841808319092 and batch: 150, loss is 4.477627868652344 and perplexity is 88.02561649503716
At time: 133.46284866333008 and batch: 200, loss is 4.534440441131592 and perplexity is 93.17136585139622
At time: 134.66760730743408 and batch: 250, loss is 4.5370269298553465 and perplexity is 93.41266446209308
At time: 135.87288403511047 and batch: 300, loss is 4.5395175647735595 and perplexity is 93.6456112783238
At time: 137.0774416923523 and batch: 350, loss is 4.530907173156738 and perplexity is 92.84274733876565
At time: 138.28056526184082 and batch: 400, loss is 4.537872810363769 and perplexity is 93.49171384265307
At time: 139.48486065864563 and batch: 450, loss is 4.483103466033936 and perplexity is 88.5089313414158
At time: 140.69014930725098 and batch: 500, loss is 4.496134643554687 and perplexity is 89.66985460834874
At time: 141.90193247795105 and batch: 550, loss is 4.505826644897461 and perplexity is 90.54316016277782
At time: 143.11360263824463 and batch: 600, loss is 4.51704047203064 and perplexity is 91.56420974597575
At time: 144.3276174068451 and batch: 650, loss is 4.526505250930786 and perplexity is 92.43495897055075
At time: 145.5393841266632 and batch: 700, loss is 4.511995706558228 and perplexity is 91.10345296426712
At time: 146.75141525268555 and batch: 750, loss is 4.513316679000854 and perplexity is 91.22387763638625
At time: 147.96339559555054 and batch: 800, loss is 4.5341454601287845 and perplexity is 93.14388612166283
At time: 149.22190165519714 and batch: 850, loss is 4.582446489334107 and perplexity is 97.7532541974753
At time: 150.43554615974426 and batch: 900, loss is 4.544499769210815 and perplexity is 94.11333704355123
At time: 151.64783763885498 and batch: 950, loss is 4.541969728469849 and perplexity is 93.87552742742515
At time: 152.86026167869568 and batch: 1000, loss is 4.522131938934326 and perplexity is 92.0315947173467
At time: 154.07245469093323 and batch: 1050, loss is 4.495080528259277 and perplexity is 89.57538204430406
At time: 155.28452229499817 and batch: 1100, loss is 4.45882212638855 and perplexity is 86.38569771388612
At time: 156.49652743339539 and batch: 1150, loss is 4.490618410110474 and perplexity is 89.1765765267682
At time: 157.70823407173157 and batch: 1200, loss is 4.490603055953979 and perplexity is 89.17520730616819
At time: 158.9204490184784 and batch: 1250, loss is 4.506147861480713 and perplexity is 90.57224879894822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.671640215128877 and perplexity of 106.87289328707895
Finished 5 epochs...
Completing Train Step...
At time: 161.90129923820496 and batch: 50, loss is 4.43826904296875 and perplexity is 84.62832682107647
At time: 163.10547494888306 and batch: 100, loss is 4.461571798324585 and perplexity is 86.62355691007801
At time: 164.31036138534546 and batch: 150, loss is 4.363643608093262 and perplexity is 78.54279284906957
At time: 165.51939725875854 and batch: 200, loss is 4.424755382537842 and perplexity is 83.49238102948438
At time: 166.7298412322998 and batch: 250, loss is 4.429654655456543 and perplexity is 83.90243665778827
At time: 167.93645429611206 and batch: 300, loss is 4.428124532699585 and perplexity is 83.7741537993833
At time: 169.14108395576477 and batch: 350, loss is 4.412072839736939 and perplexity is 82.44017178161724
At time: 170.3534882068634 and batch: 400, loss is 4.426860713958741 and perplexity is 83.66834532926042
At time: 171.5705850124359 and batch: 450, loss is 4.372705326080323 and perplexity is 79.25776001074043
At time: 172.78672337532043 and batch: 500, loss is 4.389658451080322 and perplexity is 80.61288103553724
At time: 173.99350929260254 and batch: 550, loss is 4.399813117980957 and perplexity is 81.43564838442191
At time: 175.19897508621216 and batch: 600, loss is 4.4090815353393555 and perplexity is 82.19393659902894
At time: 176.40648555755615 and batch: 650, loss is 4.420533266067505 and perplexity is 83.140609604919
At time: 177.61405229568481 and batch: 700, loss is 4.411245851516724 and perplexity is 82.3720229137208
At time: 178.8212549686432 and batch: 750, loss is 4.411831817626953 and perplexity is 82.42030427181759
At time: 180.06127047538757 and batch: 800, loss is 4.430303297042847 and perplexity is 83.95687692159754
At time: 181.27598571777344 and batch: 850, loss is 4.482443132400513 and perplexity is 88.45050520968567
At time: 182.48423671722412 and batch: 900, loss is 4.444201135635376 and perplexity is 85.13184187042107
At time: 183.7004053592682 and batch: 950, loss is 4.44235897064209 and perplexity is 84.97515933336524
At time: 184.9179859161377 and batch: 1000, loss is 4.422671222686768 and perplexity is 83.3185507690196
At time: 186.1235682964325 and batch: 1050, loss is 4.395753517150879 and perplexity is 81.10572229586224
At time: 187.32943320274353 and batch: 1100, loss is 4.358138246536255 and perplexity is 78.11157447242455
At time: 188.53667616844177 and batch: 1150, loss is 4.389011135101319 and perplexity is 80.56071591500896
At time: 189.75084400177002 and batch: 1200, loss is 4.391713333129883 and perplexity is 80.77870130994465
At time: 190.95878767967224 and batch: 1250, loss is 4.409077568054199 and perplexity is 82.19361051289117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.658407169536953 and perplexity of 105.46795572100386
Finished 6 epochs...
Completing Train Step...
At time: 193.95670175552368 and batch: 50, loss is 4.343686571121216 and perplexity is 76.99084903741006
At time: 195.1829812526703 and batch: 100, loss is 4.3650032424926755 and perplexity is 78.64965496229361
At time: 196.38759422302246 and batch: 150, loss is 4.273402743339538 and perplexity is 71.76541992093526
At time: 197.5921869277954 and batch: 200, loss is 4.333891324996948 and perplexity is 76.24038620621053
At time: 198.80272769927979 and batch: 250, loss is 4.335976057052612 and perplexity is 76.39949277292625
At time: 200.00948095321655 and batch: 300, loss is 4.333104391098022 and perplexity is 76.18041366214653
At time: 201.21461844444275 and batch: 350, loss is 4.324534749984741 and perplexity is 75.53036418081128
At time: 202.4195272922516 and batch: 400, loss is 4.341206130981445 and perplexity is 76.8001144956618
At time: 203.62511229515076 and batch: 450, loss is 4.284076881408692 and perplexity is 72.53555688363973
At time: 204.83028936386108 and batch: 500, loss is 4.3018943786621096 and perplexity is 73.83954134186564
At time: 206.03585362434387 and batch: 550, loss is 4.3128781986236575 and perplexity is 74.65505208270791
At time: 207.25366163253784 and batch: 600, loss is 4.323056001663208 and perplexity is 75.41875632185885
At time: 208.4695110321045 and batch: 650, loss is 4.3354239082336425 and perplexity is 76.35732052697114
At time: 209.67425417900085 and batch: 700, loss is 4.325460786819458 and perplexity is 75.60034047549487
At time: 210.91347575187683 and batch: 750, loss is 4.3219700050354 and perplexity is 75.33689626473827
At time: 212.11820077896118 and batch: 800, loss is 4.349130306243897 and perplexity is 77.41110968241497
At time: 213.32358646392822 and batch: 850, loss is 4.396469631195068 and perplexity is 81.16382404391837
At time: 214.53128504753113 and batch: 900, loss is 4.357268829345703 and perplexity is 78.04369243997604
At time: 215.73622822761536 and batch: 950, loss is 4.35628586769104 and perplexity is 77.96701617400765
At time: 216.94203352928162 and batch: 1000, loss is 4.338557977676391 and perplexity is 76.59700506977605
At time: 218.14777898788452 and batch: 1050, loss is 4.310097541809082 and perplexity is 74.4477503544456
At time: 219.35305953025818 and batch: 1100, loss is 4.272273225784302 and perplexity is 71.68440538156112
At time: 220.55852556228638 and batch: 1150, loss is 4.303831586837768 and perplexity is 73.98272254616253
At time: 221.76977944374084 and batch: 1200, loss is 4.3077693367004395 and perplexity is 74.27462233871272
At time: 222.97442412376404 and batch: 1250, loss is 4.325324754714966 and perplexity is 75.59005710153005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.659998984232436 and perplexity of 105.63597485500813
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 225.96537733078003 and batch: 50, loss is 4.283199405670166 and perplexity is 72.47193660899971
At time: 227.17394614219666 and batch: 100, loss is 4.310437774658203 and perplexity is 74.47308423412399
At time: 228.38225674629211 and batch: 150, loss is 4.210806965827942 and perplexity is 67.41091617301431
At time: 229.59881138801575 and batch: 200, loss is 4.269741697311401 and perplexity is 71.50316377421213
At time: 230.80678367614746 and batch: 250, loss is 4.261715469360351 and perplexity is 70.93156006195707
At time: 232.0161533355713 and batch: 300, loss is 4.256311092376709 and perplexity is 70.54925316714782
At time: 233.22685766220093 and batch: 350, loss is 4.238223705291748 and perplexity is 69.28467247263917
At time: 234.43607878684998 and batch: 400, loss is 4.24874327659607 and perplexity is 70.01736457168757
At time: 235.6504521369934 and batch: 450, loss is 4.18045024394989 and perplexity is 65.39529042041367
At time: 236.85993766784668 and batch: 500, loss is 4.189182896614074 and perplexity is 65.96886554868647
At time: 238.07339525222778 and batch: 550, loss is 4.185890502929688 and perplexity is 65.7520272268232
At time: 239.2897548675537 and batch: 600, loss is 4.1906449842453 and perplexity is 66.06538835625801
At time: 240.50253438949585 and batch: 650, loss is 4.199528102874756 and perplexity is 66.65486937693208
At time: 241.74421668052673 and batch: 700, loss is 4.1781019639968875 and perplexity is 65.2419041383506
At time: 242.95380091667175 and batch: 750, loss is 4.164597229957581 and perplexity is 64.36675222233183
At time: 244.16429662704468 and batch: 800, loss is 4.1831182813644405 and perplexity is 65.57000046482
At time: 245.37797093391418 and batch: 850, loss is 4.218341493606568 and perplexity is 67.92074383698117
At time: 246.58581280708313 and batch: 900, loss is 4.1682226276397705 and perplexity is 64.60053081076924
At time: 247.7944905757904 and batch: 950, loss is 4.154445457458496 and perplexity is 63.716621171975156
At time: 249.00274443626404 and batch: 1000, loss is 4.121706871986389 and perplexity is 61.66440573531221
At time: 250.21124744415283 and batch: 1050, loss is 4.093586668968201 and perplexity is 59.954543632479016
At time: 251.41905879974365 and batch: 1100, loss is 4.040679860115051 and perplexity is 56.86498990524894
At time: 252.63474655151367 and batch: 1150, loss is 4.056461992263794 and perplexity is 57.76955993700383
At time: 253.8514165878296 and batch: 1200, loss is 4.054000496864319 and perplexity is 57.62753529919558
At time: 255.0680696964264 and batch: 1250, loss is 4.064611506462097 and perplexity is 58.24227737841574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.564077112796533 and perplexity of 95.97397997445123
Finished 8 epochs...
Completing Train Step...
At time: 258.0175323486328 and batch: 50, loss is 4.186355705261231 and perplexity is 65.7826223390998
At time: 259.25287199020386 and batch: 100, loss is 4.208045234680176 and perplexity is 67.22500218650981
At time: 260.4583828449249 and batch: 150, loss is 4.112527704238891 and perplexity is 61.100967713211304
At time: 261.66506242752075 and batch: 200, loss is 4.175248327255249 and perplexity is 65.05599283143508
At time: 262.8759398460388 and batch: 250, loss is 4.17080295085907 and perplexity is 64.76743630285199
At time: 264.0858886241913 and batch: 300, loss is 4.170543451309204 and perplexity is 64.7506313628169
At time: 265.2990229129791 and batch: 350, loss is 4.151878204345703 and perplexity is 63.55325426975526
At time: 266.5145320892334 and batch: 400, loss is 4.168095736503601 and perplexity is 64.59233409607374
At time: 267.7351493835449 and batch: 450, loss is 4.10698853969574 and perplexity is 60.76345503141489
At time: 268.94691920280457 and batch: 500, loss is 4.120272960662842 and perplexity is 61.57604780952306
At time: 270.1539235115051 and batch: 550, loss is 4.118895945549011 and perplexity is 61.49131501359868
At time: 271.39338183403015 and batch: 600, loss is 4.128183856010437 and perplexity is 62.0651013548671
At time: 272.5980854034424 and batch: 650, loss is 4.143991847038269 and perplexity is 63.054021749015305
At time: 273.802752494812 and batch: 700, loss is 4.124525032043457 and perplexity is 61.83843100084832
At time: 275.00792241096497 and batch: 750, loss is 4.115078916549683 and perplexity is 61.25704826690097
At time: 276.2129707336426 and batch: 800, loss is 4.13945770740509 and perplexity is 62.76877317708766
At time: 277.4182085990906 and batch: 850, loss is 4.17694577217102 and perplexity is 65.1665155723019
At time: 278.62344789505005 and batch: 900, loss is 4.131954293251038 and perplexity is 62.29955564419508
At time: 279.8292088508606 and batch: 950, loss is 4.123362712860107 and perplexity is 61.76659676147225
At time: 281.0401289463043 and batch: 1000, loss is 4.096573486328125 and perplexity is 60.133884600238524
At time: 282.2453863620758 and batch: 1050, loss is 4.073532719612121 and perplexity is 58.76419375133537
At time: 283.4506859779358 and batch: 1100, loss is 4.023892736434936 and perplexity is 55.91835812954861
At time: 284.6566219329834 and batch: 1150, loss is 4.045477933883667 and perplexity is 57.13848792881764
At time: 285.8670063018799 and batch: 1200, loss is 4.049785075187683 and perplexity is 57.38512223333929
At time: 287.07299757003784 and batch: 1250, loss is 4.0656740808486935 and perplexity is 58.30419702186272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.561317610914689 and perplexity of 95.70950467397667
Finished 9 epochs...
Completing Train Step...
At time: 290.06835532188416 and batch: 50, loss is 4.143251905441284 and perplexity is 63.00738271267551
At time: 291.27689933776855 and batch: 100, loss is 4.16302523612976 and perplexity is 64.26564757388365
At time: 292.4847741127014 and batch: 150, loss is 4.069087891578675 and perplexity is 58.50357664377266
At time: 293.69180822372437 and batch: 200, loss is 4.131938128471375 and perplexity is 62.29854859374437
At time: 294.89916253089905 and batch: 250, loss is 4.128464345932007 and perplexity is 62.08251243198004
At time: 296.1154799461365 and batch: 300, loss is 4.129084215164185 and perplexity is 62.121007400983785
At time: 297.3285460472107 and batch: 350, loss is 4.10989013671875 and perplexity is 60.94002213104677
At time: 298.5377674102783 and batch: 400, loss is 4.128060750961303 and perplexity is 62.05746129778981
At time: 299.74683809280396 and batch: 450, loss is 4.0678713464736935 and perplexity is 58.43244767855249
At time: 300.9550142288208 and batch: 500, loss is 4.084037218093872 and perplexity is 59.384735670094784
At time: 302.1877157688141 and batch: 550, loss is 4.083489532470703 and perplexity is 59.352220409015864
At time: 303.3952057361603 and batch: 600, loss is 4.094129486083984 and perplexity is 59.987096819345886
At time: 304.6211955547333 and batch: 650, loss is 4.112195382118225 and perplexity is 61.08066588360646
At time: 305.83042764663696 and batch: 700, loss is 4.094043021202087 and perplexity is 59.98191026633464
At time: 307.0393478870392 and batch: 750, loss is 4.085516352653503 and perplexity is 59.47263867910175
At time: 308.24717140197754 and batch: 800, loss is 4.111665434837342 and perplexity is 61.048304926364914
At time: 309.4555320739746 and batch: 850, loss is 4.149969267845154 and perplexity is 63.43205086455894
At time: 310.66528940200806 and batch: 900, loss is 4.107604956626892 and perplexity is 60.80092220040383
At time: 311.87447929382324 and batch: 950, loss is 4.1005390882492065 and perplexity is 60.372825106865974
At time: 313.08196687698364 and batch: 1000, loss is 4.075882534980774 and perplexity is 58.902441121223404
At time: 314.2902760505676 and batch: 1050, loss is 4.055284390449524 and perplexity is 57.701570438547776
At time: 315.49949502944946 and batch: 1100, loss is 4.006503968238831 and perplexity is 54.95441196914694
At time: 316.70829463005066 and batch: 1150, loss is 4.030050415992736 and perplexity is 56.26374776832303
At time: 317.91572070121765 and batch: 1200, loss is 4.0369326829910275 and perplexity is 56.65230544794912
At time: 319.1229085922241 and batch: 1250, loss is 4.053732986450195 and perplexity is 57.61212139514463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.562053596886405 and perplexity of 95.77997145487682
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 322.0628182888031 and batch: 50, loss is 4.128662014007569 and perplexity is 62.094785375683145
At time: 323.29318499565125 and batch: 100, loss is 4.160542287826538 and perplexity is 64.10627722932485
At time: 324.4968333244324 and batch: 150, loss is 4.069832706451416 and perplexity is 58.5471672092011
At time: 325.7026164531708 and batch: 200, loss is 4.137649602890015 and perplexity is 62.655383216242114
At time: 326.90796279907227 and batch: 250, loss is 4.134822492599487 and perplexity is 62.47849969002567
At time: 328.1142737865448 and batch: 300, loss is 4.127960910797119 and perplexity is 62.05126577995089
At time: 329.3253424167633 and batch: 350, loss is 4.10818549156189 and perplexity is 60.83622950739223
At time: 330.5335211753845 and batch: 400, loss is 4.1228152990341185 and perplexity is 61.73279412528007
At time: 331.74334692955017 and batch: 450, loss is 4.061182894706726 and perplexity is 58.042929160642394
At time: 333.0033929347992 and batch: 500, loss is 4.071481642723083 and perplexity is 58.643787395187864
At time: 334.21125960350037 and batch: 550, loss is 4.06730767250061 and perplexity is 58.39952010969116
At time: 335.41959023475647 and batch: 600, loss is 4.073248729705811 and perplexity is 58.74750768290718
At time: 336.62573313713074 and batch: 650, loss is 4.08371482372284 and perplexity is 59.36559345142707
At time: 337.83111023902893 and batch: 700, loss is 4.062365026473999 and perplexity is 58.111584122679474
At time: 339.0369646549225 and batch: 750, loss is 4.049952702522278 and perplexity is 57.3947423546998
At time: 340.2437970638275 and batch: 800, loss is 4.071025357246399 and perplexity is 58.61703519048709
At time: 341.451158285141 and batch: 850, loss is 4.1045247745513915 and perplexity is 60.613932418794185
At time: 342.65923738479614 and batch: 900, loss is 4.059111499786377 and perplexity is 57.92282376781163
At time: 343.8667416572571 and batch: 950, loss is 4.04703007221222 and perplexity is 57.22724362871063
At time: 345.0759606361389 and batch: 1000, loss is 4.017485008239746 and perplexity is 55.56119401687608
At time: 346.2837300300598 and batch: 1050, loss is 3.993823809623718 and perplexity is 54.26198065598964
At time: 347.4901428222656 and batch: 1100, loss is 3.9389369344711302 and perplexity is 51.36396900194336
At time: 348.7059555053711 and batch: 1150, loss is 3.9610957145690917 and perplexity is 52.51483570678295
At time: 349.92008566856384 and batch: 1200, loss is 3.966931118965149 and perplexity is 52.82217686777566
At time: 351.1308581829071 and batch: 1250, loss is 3.9801253271102905 and perplexity is 53.523741783052714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.540571003934763 and perplexity of 93.74431321163237
Finished 11 epochs...
Completing Train Step...
At time: 354.11952686309814 and batch: 50, loss is 4.106551847457886 and perplexity is 60.736925895212984
At time: 355.3587443828583 and batch: 100, loss is 4.1326824426651 and perplexity is 62.34493554880416
At time: 356.5707266330719 and batch: 150, loss is 4.03822925567627 and perplexity is 56.725806919452125
At time: 357.7812213897705 and batch: 200, loss is 4.103658747673035 and perplexity is 60.56146184785841
At time: 358.99248003959656 and batch: 250, loss is 4.101576952934265 and perplexity is 60.43551645692337
At time: 360.2085962295532 and batch: 300, loss is 4.09700258731842 and perplexity is 60.15969364661848
At time: 361.4233844280243 and batch: 350, loss is 4.078169379234314 and perplexity is 59.03729596746566
At time: 362.6617908477783 and batch: 400, loss is 4.093980278968811 and perplexity is 59.9781469853879
At time: 363.87417483329773 and batch: 450, loss is 4.034390258789062 and perplexity is 56.50845419841719
At time: 365.0914125442505 and batch: 500, loss is 4.047022833824157 and perplexity is 57.2268293972127
At time: 366.3056514263153 and batch: 550, loss is 4.043920516967773 and perplexity is 57.04956874126526
At time: 367.52712750434875 and batch: 600, loss is 4.0526139402389525 and perplexity is 57.54768682832875
At time: 368.745322227478 and batch: 650, loss is 4.065320754051209 and perplexity is 58.28360022556338
At time: 369.9627935886383 and batch: 700, loss is 4.045254850387574 and perplexity is 57.12574269684728
At time: 371.17585730552673 and batch: 750, loss is 4.035918574333191 and perplexity is 56.59488297574227
At time: 372.38628935813904 and batch: 800, loss is 4.059367232322693 and perplexity is 57.93763841265693
At time: 373.60781359672546 and batch: 850, loss is 4.094907393455506 and perplexity is 60.033779379157394
At time: 374.8326609134674 and batch: 900, loss is 4.051467609405518 and perplexity is 57.481755937051204
At time: 376.0427403450012 and batch: 950, loss is 4.041555910110474 and perplexity is 56.91482830667339
At time: 377.24853229522705 and batch: 1000, loss is 4.014954152107239 and perplexity is 55.420754419439604
At time: 378.4619343280792 and batch: 1050, loss is 3.994104752540588 and perplexity is 54.27722731673066
At time: 379.6688952445984 and batch: 1100, loss is 3.9412614631652834 and perplexity is 51.48350490022138
At time: 380.87526845932007 and batch: 1150, loss is 3.96585159778595 and perplexity is 52.765184976626955
At time: 382.08129692077637 and batch: 1200, loss is 3.973649082183838 and perplexity is 53.178228942393005
At time: 383.28682041168213 and batch: 1250, loss is 3.986553473472595 and perplexity is 53.86890843140621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.540280530052463 and perplexity of 93.7170868914892
Finished 12 epochs...
Completing Train Step...
At time: 386.2556302547455 and batch: 50, loss is 4.0932281827926635 and perplexity is 59.933054609415095
At time: 387.46165704727173 and batch: 100, loss is 4.117951998710632 and perplexity is 61.43329786806234
At time: 388.6729006767273 and batch: 150, loss is 4.023251557350159 and perplexity is 55.88251593972002
At time: 389.88750982284546 and batch: 200, loss is 4.08786563873291 and perplexity is 59.612521168913226
At time: 391.0932605266571 and batch: 250, loss is 4.0867151927948 and perplexity is 59.54397962030673
At time: 392.2991769313812 and batch: 300, loss is 4.082619619369507 and perplexity is 59.300611585747305
At time: 393.5380597114563 and batch: 350, loss is 4.063867621421814 and perplexity is 58.198967930184374
At time: 394.7428812980652 and batch: 400, loss is 4.080268850326538 and perplexity is 59.16137326648825
At time: 395.9494173526764 and batch: 450, loss is 4.021282014846801 and perplexity is 55.772561265128445
At time: 397.1545412540436 and batch: 500, loss is 4.034881138801575 and perplexity is 56.53619987846393
At time: 398.3593647480011 and batch: 550, loss is 4.0320635461807255 and perplexity is 56.37712810384162
At time: 399.5638794898987 and batch: 600, loss is 4.041945910453796 and perplexity is 56.93702943819618
At time: 400.7683918476105 and batch: 650, loss is 4.055678915977478 and perplexity is 57.724339672315494
At time: 401.97389364242554 and batch: 700, loss is 4.036108951568604 and perplexity is 56.605658378764865
At time: 403.179438829422 and batch: 750, loss is 4.02775438785553 and perplexity is 56.13471281097525
At time: 404.3845512866974 and batch: 800, loss is 4.052205386161805 and perplexity is 57.52418028841793
At time: 405.58912467956543 and batch: 850, loss is 4.088523869514465 and perplexity is 59.651772882238475
At time: 406.7933158874512 and batch: 900, loss is 4.045903887748718 and perplexity is 57.16283147283934
At time: 407.9986355304718 and batch: 950, loss is 4.036947364807129 and perplexity is 56.653137212785325
At time: 409.20520305633545 and batch: 1000, loss is 4.0116672706604 and perplexity is 55.238892013782575
At time: 410.4130554199219 and batch: 1050, loss is 3.992160997390747 and perplexity is 54.17182814489098
At time: 411.6179437637329 and batch: 1100, loss is 3.940015964508057 and perplexity is 51.4194221797541
At time: 412.8230745792389 and batch: 1150, loss is 3.965672173500061 and perplexity is 52.7557184702787
At time: 414.028169631958 and batch: 1200, loss is 3.97435745716095 and perplexity is 53.215912414541506
At time: 415.2327473163605 and batch: 1250, loss is 3.987111783027649 and perplexity is 53.89899235499333
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.540583478273266 and perplexity of 93.74548261722191
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 418.17567801475525 and batch: 50, loss is 4.089523725509643 and perplexity is 59.711445892214854
At time: 419.401300907135 and batch: 100, loss is 4.1222489643096925 and perplexity is 61.69784259839694
At time: 420.60590624809265 and batch: 150, loss is 4.030023422241211 and perplexity is 56.262229019194336
At time: 421.80971479415894 and batch: 200, loss is 4.099589524269104 and perplexity is 60.31552445638984
At time: 423.0135409832001 and batch: 250, loss is 4.102436747550964 and perplexity is 60.487500933414516
At time: 424.2450096607208 and batch: 300, loss is 4.096705694198608 and perplexity is 60.141835298626276
At time: 425.46097898483276 and batch: 350, loss is 4.0759651613235475 and perplexity is 58.90730821558599
At time: 426.67149591445923 and batch: 400, loss is 4.0904730033874515 and perplexity is 59.76815555920988
At time: 427.8755660057068 and batch: 450, loss is 4.030322594642639 and perplexity is 56.27906364346163
At time: 429.0799329280853 and batch: 500, loss is 4.041225519180298 and perplexity is 56.89602726962542
At time: 430.28434777259827 and batch: 550, loss is 4.03930094242096 and perplexity is 56.786631801598006
At time: 431.488169670105 and batch: 600, loss is 4.0486486053466795 and perplexity is 57.31994281683465
At time: 432.69245743751526 and batch: 650, loss is 4.06055127620697 and perplexity is 58.00627974824638
At time: 433.896112203598 and batch: 700, loss is 4.038081622123718 and perplexity is 56.71743290521318
At time: 435.09925842285156 and batch: 750, loss is 4.02424168586731 and perplexity is 55.93787421374238
At time: 436.3033971786499 and batch: 800, loss is 4.046237154006958 and perplexity is 57.181885090582284
At time: 437.5078959465027 and batch: 850, loss is 4.075678868293762 and perplexity is 58.890445877740525
At time: 438.7124378681183 and batch: 900, loss is 4.03005211353302 and perplexity is 56.26384327838243
At time: 439.9157774448395 and batch: 950, loss is 4.0203140830993656 and perplexity is 55.71860335044617
At time: 441.13159680366516 and batch: 1000, loss is 3.993150162696838 and perplexity is 54.225439548758466
At time: 442.34708523750305 and batch: 1050, loss is 3.970015034675598 and perplexity is 52.98532745089743
At time: 443.5524559020996 and batch: 1100, loss is 3.914718551635742 and perplexity is 50.13495912293842
At time: 444.7571532726288 and batch: 1150, loss is 3.9417238330841062 and perplexity is 51.507314828275874
At time: 445.9605803489685 and batch: 1200, loss is 3.953840317726135 and perplexity is 52.13519860785681
At time: 447.1642553806305 and batch: 1250, loss is 3.970451998710632 and perplexity is 53.008485192559625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533890550153969 and perplexity of 93.12014584194296
Finished 14 epochs...
Completing Train Step...
At time: 450.12705969810486 and batch: 50, loss is 4.086465859413147 and perplexity is 59.529135169196465
At time: 451.3389108181 and batch: 100, loss is 4.112931118011475 and perplexity is 61.12562165764101
At time: 452.55159306526184 and batch: 150, loss is 4.0186104536056515 and perplexity is 55.62376030607263
At time: 453.75957703590393 and batch: 200, loss is 4.085521368980408 and perplexity is 59.47293701404754
At time: 455.0002462863922 and batch: 250, loss is 4.087114133834839 and perplexity is 59.56773889642458
At time: 456.20753049850464 and batch: 300, loss is 4.080658197402954 and perplexity is 59.18441205895851
At time: 457.4145128726959 and batch: 350, loss is 4.062199335098267 and perplexity is 58.101956332003205
At time: 458.6222803592682 and batch: 400, loss is 4.0764945936203 and perplexity is 58.93850390434397
At time: 459.8300471305847 and batch: 450, loss is 4.0175012636184695 and perplexity is 55.562097192467846
At time: 461.0368547439575 and batch: 500, loss is 4.029876432418823 and perplexity is 56.253959651914975
At time: 462.2446200847626 and batch: 550, loss is 4.027416739463806 and perplexity is 56.11576221497394
At time: 463.4516296386719 and batch: 600, loss is 4.038185591697693 and perplexity is 56.72333009910835
At time: 464.6582944393158 and batch: 650, loss is 4.051643633842469 and perplexity is 57.4918750213519
At time: 465.86491990089417 and batch: 700, loss is 4.030077438354493 and perplexity is 56.26526816821129
At time: 467.07195591926575 and batch: 750, loss is 4.018041672706604 and perplexity is 55.59213156944068
At time: 468.27877974510193 and batch: 800, loss is 4.0419251823425295 and perplexity is 56.93584925334634
At time: 469.4892387390137 and batch: 850, loss is 4.073221735954284 and perplexity is 58.74592188868535
At time: 470.69643783569336 and batch: 900, loss is 4.028987522125244 and perplexity is 56.20397714639883
At time: 471.90409088134766 and batch: 950, loss is 4.020769219398499 and perplexity is 55.74396868127151
At time: 473.11136865615845 and batch: 1000, loss is 3.9950215291976927 and perplexity is 54.32701022816577
At time: 474.3182201385498 and batch: 1050, loss is 3.9732355213165285 and perplexity is 53.15624105488791
At time: 475.5259301662445 and batch: 1100, loss is 3.9192557764053344 and perplexity is 50.36294953203402
At time: 476.73450756073 and batch: 1150, loss is 3.9471866989135744 and perplexity is 51.789462343754785
At time: 477.94247174263 and batch: 1200, loss is 3.959792847633362 and perplexity is 52.44646041533056
At time: 479.15784883499146 and batch: 1250, loss is 3.975495390892029 and perplexity is 53.27650306384177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533643290944343 and perplexity of 93.09712387459582
Finished 15 epochs...
Completing Train Step...
At time: 482.1019687652588 and batch: 50, loss is 4.0833409929275515 and perplexity is 59.34340491205243
At time: 483.3317542076111 and batch: 100, loss is 4.108365483283997 and perplexity is 60.847180510623176
At time: 484.5365307331085 and batch: 150, loss is 4.01348156452179 and perplexity is 55.339202565366435
At time: 485.7663745880127 and batch: 200, loss is 4.079495439529419 and perplexity is 59.115634911180365
At time: 486.9741041660309 and batch: 250, loss is 4.081032538414002 and perplexity is 59.206571358915525
At time: 488.1852867603302 and batch: 300, loss is 4.074704880714417 and perplexity is 58.83311523911069
At time: 489.3903284072876 and batch: 350, loss is 4.056464157104492 and perplexity is 57.76968499903364
At time: 490.5949549674988 and batch: 400, loss is 4.07070378780365 and perplexity is 58.59818877352356
At time: 491.79916501045227 and batch: 450, loss is 4.012092928886414 and perplexity is 55.26240990750151
At time: 493.00448656082153 and batch: 500, loss is 4.025034956932068 and perplexity is 55.98226571569934
At time: 494.2098708152771 and batch: 550, loss is 4.022582273483277 and perplexity is 55.845127186601076
At time: 495.41464591026306 and batch: 600, loss is 4.033859596252442 and perplexity is 56.47847523383196
At time: 496.6184067726135 and batch: 650, loss is 4.0478515577316285 and perplexity is 57.274274295528336
At time: 497.8223385810852 and batch: 700, loss is 4.026698026657105 and perplexity is 56.075445587785815
At time: 499.02609610557556 and batch: 750, loss is 4.015366759300232 and perplexity is 55.44362613954778
At time: 500.2308990955353 and batch: 800, loss is 4.039857311248779 and perplexity is 56.81823490404459
At time: 501.4354078769684 and batch: 850, loss is 4.071842451095581 and perplexity is 58.664950382336905
At time: 502.63923501968384 and batch: 900, loss is 4.028064794540406 and perplexity is 56.15214010571983
At time: 503.84301829338074 and batch: 950, loss is 4.020544018745422 and perplexity is 55.73141651655029
At time: 505.0498471260071 and batch: 1000, loss is 3.9954705572128297 and perplexity is 54.35141005542988
At time: 506.25728726387024 and batch: 1050, loss is 3.9742958307266236 and perplexity is 53.21263300866006
At time: 507.4619691371918 and batch: 1100, loss is 3.9207331943511963 and perplexity is 50.43741164976927
At time: 508.6671004295349 and batch: 1150, loss is 3.9490907287597654 and perplexity is 51.88816496232217
At time: 509.8722457885742 and batch: 1200, loss is 3.9619463682174683 and perplexity is 52.559526648932405
At time: 511.07627749443054 and batch: 1250, loss is 3.977144479751587 and perplexity is 53.36443323392538
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533707444685219 and perplexity of 93.1030965949413
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 514.0136260986328 and batch: 50, loss is 4.08263277053833 and perplexity is 59.30139146322973
At time: 515.247251033783 and batch: 100, loss is 4.110461287498474 and perplexity is 60.97483801381613
At time: 516.4540503025055 and batch: 150, loss is 4.016718406677246 and perplexity is 55.51861704060173
At time: 517.6622803211212 and batch: 200, loss is 4.083716344833374 and perplexity is 59.36568375312533
At time: 518.8696844577789 and batch: 250, loss is 4.086571946144104 and perplexity is 59.535450755537255
At time: 520.0776522159576 and batch: 300, loss is 4.079938683509827 and perplexity is 59.141843368444114
At time: 521.2855195999146 and batch: 350, loss is 4.0614470911026 and perplexity is 58.05826591920158
At time: 522.4936985969543 and batch: 400, loss is 4.072871689796448 and perplexity is 58.72536170315655
At time: 523.7023894786835 and batch: 450, loss is 4.01424207687378 and perplexity is 55.381304720041754
At time: 524.9166266918182 and batch: 500, loss is 4.0261584615707395 and perplexity is 56.04519739631215
At time: 526.1284646987915 and batch: 550, loss is 4.023673615455627 and perplexity is 55.906106586488
At time: 527.3369455337524 and batch: 600, loss is 4.036471877098084 and perplexity is 56.626205745659995
At time: 528.5458598136902 and batch: 650, loss is 4.050533394813538 and perplexity is 57.42808071787247
At time: 529.7556936740875 and batch: 700, loss is 4.030289845466614 and perplexity is 56.2772205806794
At time: 530.9621245861053 and batch: 750, loss is 4.016399755477905 and perplexity is 55.5009287850376
At time: 532.169819355011 and batch: 800, loss is 4.038703165054321 and perplexity is 56.75269618236246
At time: 533.3826296329498 and batch: 850, loss is 4.066743769645691 and perplexity is 58.36659773695778
At time: 534.5921213626862 and batch: 900, loss is 4.020233001708984 and perplexity is 55.71408579176386
At time: 535.7990398406982 and batch: 950, loss is 4.012471885681152 and perplexity is 55.2833559417999
At time: 537.0061719417572 and batch: 1000, loss is 3.9873277044296263 and perplexity is 53.910631557519025
At time: 538.2131879329681 and batch: 1050, loss is 3.9651963996887205 and perplexity is 52.73062465099647
At time: 539.4196004867554 and batch: 1100, loss is 3.908742003440857 and perplexity is 49.836218730895325
At time: 540.6270673274994 and batch: 1150, loss is 3.9364665746688843 and perplexity is 51.237238117528236
At time: 541.8339841365814 and batch: 1200, loss is 3.9510879802703855 and perplexity is 51.99190223840805
At time: 543.0414628982544 and batch: 1250, loss is 3.9695381450653078 and perplexity is 52.96006532284024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533290890881615 and perplexity of 93.06432222229422
Finished 17 epochs...
Completing Train Step...
At time: 546.0120871067047 and batch: 50, loss is 4.081505398750306 and perplexity is 59.23457441840598
At time: 547.2150278091431 and batch: 100, loss is 4.10764362335205 and perplexity is 60.80327321840473
At time: 548.4183979034424 and batch: 150, loss is 4.0131658887863155 and perplexity is 55.32173607891408
At time: 549.6214475631714 and batch: 200, loss is 4.079513397216797 and perplexity is 59.11669650080307
At time: 550.8264718055725 and batch: 250, loss is 4.082123379707337 and perplexity is 59.271191570580434
At time: 552.0304720401764 and batch: 300, loss is 4.075251879692078 and perplexity is 58.86530569627006
At time: 553.2348735332489 and batch: 350, loss is 4.0572387313842775 and perplexity is 57.81444924556412
At time: 554.4382405281067 and batch: 400, loss is 4.069262046813964 and perplexity is 58.513766235187944
At time: 555.6423840522766 and batch: 450, loss is 4.0107732772827145 and perplexity is 55.18953087768351
At time: 556.8522493839264 and batch: 500, loss is 4.022994756698608 and perplexity is 55.86816711568895
At time: 558.0581448078156 and batch: 550, loss is 4.020231046676636 and perplexity is 55.71397686903034
At time: 559.2605876922607 and batch: 600, loss is 4.0332035446167 and perplexity is 56.44143458938681
At time: 560.4633486270905 and batch: 650, loss is 4.047819375991821 and perplexity is 57.272431139393454
At time: 561.6665434837341 and batch: 700, loss is 4.027603311538696 and perplexity is 56.12623282589585
At time: 562.8703835010529 and batch: 750, loss is 4.014410648345947 and perplexity is 55.390641215020224
At time: 564.0746529102325 and batch: 800, loss is 4.037768845558166 and perplexity is 56.69969579536294
At time: 565.2785804271698 and batch: 850, loss is 4.066508512496949 and perplexity is 58.35286819264244
At time: 566.4820642471313 and batch: 900, loss is 4.020423512458802 and perplexity is 55.724700935140795
At time: 567.6862337589264 and batch: 950, loss is 4.013064594268799 and perplexity is 55.31613257415671
At time: 568.8901755809784 and batch: 1000, loss is 3.988269019126892 and perplexity is 53.9614023192262
At time: 570.094818353653 and batch: 1050, loss is 3.9666968393325805 and perplexity is 52.8098031570984
At time: 571.3001322746277 and batch: 1100, loss is 3.910879282951355 and perplexity is 49.94284656622406
At time: 572.5041601657867 and batch: 1150, loss is 3.9389575910568237 and perplexity is 51.365030017129044
At time: 573.70885014534 and batch: 1200, loss is 3.953596453666687 and perplexity is 52.12248625678993
At time: 574.9128222465515 and batch: 1250, loss is 3.971636037826538 and perplexity is 53.071286484758765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533261932595803 and perplexity of 93.06162727807309
Finished 18 epochs...
Completing Train Step...
At time: 577.8625643253326 and batch: 50, loss is 4.080704336166382 and perplexity is 59.18714281754153
At time: 579.0925290584564 and batch: 100, loss is 4.1060785293579105 and perplexity is 60.708184811224136
At time: 580.2968757152557 and batch: 150, loss is 4.011291408538819 and perplexity is 55.21813370801128
At time: 581.5018334388733 and batch: 200, loss is 4.077051644325256 and perplexity is 58.97134478566178
At time: 582.7068939208984 and batch: 250, loss is 4.079569840431214 and perplexity is 59.12003333134913
At time: 583.9121866226196 and batch: 300, loss is 4.072674064636231 and perplexity is 58.7137572408458
At time: 585.1169185638428 and batch: 350, loss is 4.054891757965088 and perplexity is 57.67891937465336
At time: 586.3215873241425 and batch: 400, loss is 4.067014741897583 and perplexity is 58.38241560838716
At time: 587.5274429321289 and batch: 450, loss is 4.008710451126099 and perplexity is 55.075801811792445
At time: 588.7341105937958 and batch: 500, loss is 4.021172270774842 and perplexity is 55.766440892995014
At time: 589.9397721290588 and batch: 550, loss is 4.018313388824463 and perplexity is 55.607238899973886
At time: 591.1448464393616 and batch: 600, loss is 4.0313580083847045 and perplexity is 56.337365937638204
At time: 592.3504323959351 and batch: 650, loss is 4.046266565322876 and perplexity is 57.1835669098016
At time: 593.5542814731598 and batch: 700, loss is 4.026108918190002 and perplexity is 56.042420796540675
At time: 594.7588062286377 and batch: 750, loss is 4.013338327407837 and perplexity is 55.33127650536905
At time: 595.9629924297333 and batch: 800, loss is 4.037240753173828 and perplexity is 56.66976102267753
At time: 597.1682050228119 and batch: 850, loss is 4.066377196311951 and perplexity is 58.345206019702374
At time: 598.373838186264 and batch: 900, loss is 4.020517945289612 and perplexity is 55.72996342486814
At time: 599.5792872905731 and batch: 950, loss is 4.013480892181397 and perplexity is 55.33916535859774
At time: 600.7837700843811 and batch: 1000, loss is 3.9889467573165893 and perplexity is 53.99798641816771
At time: 601.9887540340424 and batch: 1050, loss is 3.9676655197143553 and perplexity is 52.86098376220404
At time: 603.194776058197 and batch: 1100, loss is 3.9121855545043944 and perplexity is 50.00812811440326
At time: 604.4058544635773 and batch: 1150, loss is 3.9404578828811645 and perplexity is 51.44215038878661
At time: 605.6169910430908 and batch: 1200, loss is 3.9551225233078005 and perplexity is 52.20208952530255
At time: 606.8754210472107 and batch: 1250, loss is 3.9728473806381226 and perplexity is 53.1356129589839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533288217809078 and perplexity of 93.06407345494284
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 610.0856184959412 and batch: 50, loss is 4.080611386299133 and perplexity is 59.181641636144825
At time: 611.2955524921417 and batch: 100, loss is 4.106491885185242 and perplexity is 60.733284080289735
At time: 612.5046079158783 and batch: 150, loss is 4.012030992507935 and perplexity is 55.258987259960094
At time: 613.7146694660187 and batch: 200, loss is 4.077756795883179 and perplexity is 59.01294318617495
At time: 614.9285643100739 and batch: 250, loss is 4.0804463434219365 and perplexity is 59.17187493371647
At time: 616.143700838089 and batch: 300, loss is 4.073391346931458 and perplexity is 58.755886686947534
At time: 617.3538444042206 and batch: 350, loss is 4.055586848258972 and perplexity is 57.71902536870127
At time: 618.5708816051483 and batch: 400, loss is 4.066826014518738 and perplexity is 58.37139828778649
At time: 619.7885363101959 and batch: 450, loss is 4.008652396202088 and perplexity is 55.07260448311464
At time: 621.009449005127 and batch: 500, loss is 4.020913276672363 and perplexity is 55.7519995838752
At time: 622.2233068943024 and batch: 550, loss is 4.0173927545547485 and perplexity is 55.55606852841137
At time: 623.434410572052 and batch: 600, loss is 4.031133918762207 and perplexity is 56.32474273299016
At time: 624.6439776420593 and batch: 650, loss is 4.046081614494324 and perplexity is 57.17299173969546
At time: 625.8529629707336 and batch: 700, loss is 4.026500883102417 and perplexity is 56.06439176474275
At time: 627.0603442192078 and batch: 750, loss is 4.012990865707398 and perplexity is 55.31205434562256
At time: 628.2638964653015 and batch: 800, loss is 4.036326050758362 and perplexity is 56.61794875540174
At time: 629.4754645824432 and batch: 850, loss is 4.0644892930984495 and perplexity is 58.23515982872952
At time: 630.6844291687012 and batch: 900, loss is 4.017884821891784 and perplexity is 55.58341258211444
At time: 631.892571926117 and batch: 950, loss is 4.010787615776062 and perplexity is 55.190322218078144
At time: 633.0973401069641 and batch: 1000, loss is 3.986242651939392 and perplexity is 53.85216741656432
At time: 634.3013525009155 and batch: 1050, loss is 3.9646122455596924 and perplexity is 52.69983083392371
At time: 635.5061249732971 and batch: 1100, loss is 3.9085860013961793 and perplexity is 49.828444785265766
At time: 636.7117655277252 and batch: 1150, loss is 3.9367086029052736 and perplexity is 51.2496404767073
At time: 637.9187219142914 and batch: 1200, loss is 3.9517026329040528 and perplexity is 52.02386902127548
At time: 639.1228761672974 and batch: 1250, loss is 3.9702714490890503 and perplexity is 52.998915394555176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5331345161382295 and perplexity of 93.04977045058283
Finished 20 epochs...
Completing Train Step...
At time: 642.0508255958557 and batch: 50, loss is 4.080253229141236 and perplexity is 59.16044910293198
At time: 643.279777765274 and batch: 100, loss is 4.105833878517151 and perplexity is 60.69333431943581
At time: 644.4838526248932 and batch: 150, loss is 4.0112130832672115 and perplexity is 55.21380890206397
At time: 645.6872038841248 and batch: 200, loss is 4.076899418830871 and perplexity is 58.962368526772295
At time: 646.8907301425934 and batch: 250, loss is 4.079544854164124 and perplexity is 59.1185561608605
At time: 648.0959842205048 and batch: 300, loss is 4.072527527809143 and perplexity is 58.70515414350402
At time: 649.3072817325592 and batch: 350, loss is 4.054779362678528 and perplexity is 57.6724369002884
At time: 650.5114994049072 and batch: 400, loss is 4.06618004322052 and perplexity is 58.333704215810755
At time: 651.7148768901825 and batch: 450, loss is 4.008008723258972 and perplexity is 55.03716714395366
At time: 652.9187417030334 and batch: 500, loss is 4.0203220176696775 and perplexity is 55.71904545537609
At time: 654.1225070953369 and batch: 550, loss is 4.016808171272277 and perplexity is 55.5236008704595
At time: 655.3264820575714 and batch: 600, loss is 4.030560932159424 and perplexity is 56.29247865432407
At time: 656.5317420959473 and batch: 650, loss is 4.045639944076538 and perplexity is 57.14774569617343
At time: 657.734760761261 and batch: 700, loss is 4.026076707839966 and perplexity is 56.04061567962183
At time: 658.9379251003265 and batch: 750, loss is 4.01271607875824 and perplexity is 55.29685740301367
At time: 660.1845698356628 and batch: 800, loss is 4.036252284049988 and perplexity is 56.61377238972745
At time: 661.3892056941986 and batch: 850, loss is 4.064525437355042 and perplexity is 58.237264733328935
At time: 662.5930716991425 and batch: 900, loss is 4.017967910766601 and perplexity is 55.58803113719699
At time: 663.805459022522 and batch: 950, loss is 4.010970311164856 and perplexity is 55.20040615656994
At time: 665.0178511142731 and batch: 1000, loss is 3.9865217494964598 and perplexity is 53.86719952254753
At time: 666.2210190296173 and batch: 1050, loss is 3.965036292076111 and perplexity is 52.72218275239633
At time: 667.425181388855 and batch: 1100, loss is 3.9090784692764284 and perplexity is 49.85298973714943
At time: 668.6289999485016 and batch: 1150, loss is 3.9372199392318725 and perplexity is 51.2758529807398
At time: 669.8337650299072 and batch: 1200, loss is 3.952226595878601 and perplexity is 52.05113474492643
At time: 671.0371587276459 and batch: 1250, loss is 3.9706952953338623 and perplexity is 53.02138354700894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533131843065694 and perplexity of 93.04952172212941
Finished 21 epochs...
Completing Train Step...
At time: 673.9763560295105 and batch: 50, loss is 4.079933428764344 and perplexity is 59.14153259392633
At time: 675.2088787555695 and batch: 100, loss is 4.105344395637513 and perplexity is 60.663633241055976
At time: 676.4167907238007 and batch: 150, loss is 4.010636916160584 and perplexity is 55.18200568440672
At time: 677.6229584217072 and batch: 200, loss is 4.07621214389801 and perplexity is 58.921859091056106
At time: 678.829597234726 and batch: 250, loss is 4.078821148872375 and perplexity is 59.07578722684498
At time: 680.037849187851 and batch: 300, loss is 4.071829686164856 and perplexity is 58.66420153308881
At time: 681.2466795444489 and batch: 350, loss is 4.054119143486023 and perplexity is 57.63437301720402
At time: 682.4553318023682 and batch: 400, loss is 4.0656094026565555 and perplexity is 58.30042613375371
At time: 683.662752866745 and batch: 450, loss is 4.007464847564697 and perplexity is 55.00724190500504
At time: 684.8707673549652 and batch: 500, loss is 4.019840049743652 and perplexity is 55.69219713313267
At time: 686.0792694091797 and batch: 550, loss is 4.016324353218079 and perplexity is 55.49674404735608
At time: 687.2877442836761 and batch: 600, loss is 4.0300875091552735 and perplexity is 56.26583480737115
At time: 688.496826171875 and batch: 650, loss is 4.045270090103149 and perplexity is 57.12661328355176
At time: 689.7138509750366 and batch: 700, loss is 4.025716471672058 and perplexity is 56.02043145874366
At time: 690.9485540390015 and batch: 750, loss is 4.012484040260315 and perplexity is 55.2840278918097
At time: 692.1565673351288 and batch: 800, loss is 4.036174345016479 and perplexity is 56.60936013896962
At time: 693.364937543869 and batch: 850, loss is 4.064553074836731 and perplexity is 58.23887428690852
At time: 694.5741820335388 and batch: 900, loss is 4.01804370880127 and perplexity is 55.59224476039844
At time: 695.7834882736206 and batch: 950, loss is 4.01113329410553 and perplexity is 55.209403614287886
At time: 696.9924745559692 and batch: 1000, loss is 3.986759386062622 and perplexity is 53.880001859962356
At time: 698.2015602588654 and batch: 1050, loss is 3.965380086898804 and perplexity is 52.7403114819708
At time: 699.4098744392395 and batch: 1100, loss is 3.909490933418274 and perplexity is 49.87355654902443
At time: 700.619485616684 and batch: 1150, loss is 3.9376652526855467 and perplexity is 51.29869189278072
At time: 701.8423411846161 and batch: 1200, loss is 3.952671947479248 and perplexity is 52.07432096372717
At time: 703.0588982105255 and batch: 1250, loss is 3.9710412979125977 and perplexity is 53.039732256611686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533151891109717 and perplexity of 93.05138720173677
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 706.0652227401733 and batch: 50, loss is 4.0798372602462765 and perplexity is 59.135845313853586
At time: 707.2803876399994 and batch: 100, loss is 4.105337805747986 and perplexity is 60.66323347573185
At time: 708.4857788085938 and batch: 150, loss is 4.010695118904113 and perplexity is 55.18521752199899
At time: 709.69415974617 and batch: 200, loss is 4.076280388832092 and perplexity is 58.9258803466594
At time: 710.9011015892029 and batch: 250, loss is 4.078893227577209 and perplexity is 59.080045486538424
At time: 712.1158726215363 and batch: 300, loss is 4.071836857795716 and perplexity is 58.66462225259551
At time: 713.3276355266571 and batch: 350, loss is 4.054192934036255 and perplexity is 57.638626046215975
At time: 714.5345559120178 and batch: 400, loss is 4.065492844581604 and perplexity is 58.293631144327826
At time: 715.7395279407501 and batch: 450, loss is 4.00740225315094 and perplexity is 55.0037988667042
At time: 716.9445841312408 and batch: 500, loss is 4.019756388664246 and perplexity is 55.68753805870032
At time: 718.1502437591553 and batch: 550, loss is 4.015943155288697 and perplexity is 55.47559283509307
At time: 719.3543264865875 and batch: 600, loss is 4.029899544715882 and perplexity is 56.2552598251661
At time: 720.5585861206055 and batch: 650, loss is 4.045141377449036 and perplexity is 57.119260838722006
At time: 721.8113763332367 and batch: 700, loss is 4.02580509185791 and perplexity is 56.02539621977681
At time: 723.0238251686096 and batch: 750, loss is 4.012425212860108 and perplexity is 55.28077577183367
At time: 724.236531496048 and batch: 800, loss is 4.035840682983398 and perplexity is 56.590474895593786
At time: 725.4433307647705 and batch: 850, loss is 4.064024972915649 and perplexity is 58.20812634525412
At time: 726.6495807170868 and batch: 900, loss is 4.017339224815369 and perplexity is 55.55309470613676
At time: 727.8551924228668 and batch: 950, loss is 4.010384812355041 and perplexity is 55.168095844211706
At time: 729.0616145133972 and batch: 1000, loss is 3.9859915256500242 and perplexity is 53.838645419522116
At time: 730.2672975063324 and batch: 1050, loss is 3.9645574569702147 and perplexity is 52.696943563622064
At time: 731.4737234115601 and batch: 1100, loss is 3.9085101890563965 and perplexity is 49.82466731747
At time: 732.6865718364716 and batch: 1150, loss is 3.9366990756988525 and perplexity is 51.24915221312936
At time: 733.8943047523499 and batch: 1200, loss is 3.951791973114014 and perplexity is 52.02851705228177
At time: 735.0997409820557 and batch: 1250, loss is 3.9703408670425415 and perplexity is 53.00259459849905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533134070626141 and perplexity of 93.04972899579448
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 738.0718915462494 and batch: 50, loss is 4.079799156188965 and perplexity is 59.13359204114429
At time: 739.3026440143585 and batch: 100, loss is 4.105297174453735 and perplexity is 60.66076870011614
At time: 740.5115804672241 and batch: 150, loss is 4.010618185997009 and perplexity is 55.18097212609325
At time: 741.7206346988678 and batch: 200, loss is 4.076228499412537 and perplexity is 58.92282279625933
At time: 742.9305498600006 and batch: 250, loss is 4.078867154121399 and perplexity is 59.07850508566503
At time: 744.1573507785797 and batch: 300, loss is 4.071842651367188 and perplexity is 58.66496213126198
At time: 745.3687191009521 and batch: 350, loss is 4.054179825782776 and perplexity is 57.63787050944747
At time: 746.5781195163727 and batch: 400, loss is 4.0654521703720095 and perplexity is 58.29126014517624
At time: 747.7871315479279 and batch: 450, loss is 4.007364902496338 and perplexity is 55.00174447717757
At time: 748.997843503952 and batch: 500, loss is 4.019739317893982 and perplexity is 55.686587437645485
At time: 750.2065978050232 and batch: 550, loss is 4.015832495689392 and perplexity is 55.469454267870724
At time: 751.4567880630493 and batch: 600, loss is 4.029828753471374 and perplexity is 56.251277586268486
At time: 752.6640124320984 and batch: 650, loss is 4.045067205429077 and perplexity is 57.115024344883636
At time: 753.8726522922516 and batch: 700, loss is 4.025785760879517 and perplexity is 56.02431320452092
At time: 755.0798470973969 and batch: 750, loss is 4.0123844766616825 and perplexity is 55.27852388904961
At time: 756.28688621521 and batch: 800, loss is 4.03574695110321 and perplexity is 56.585170812565806
At time: 757.494861125946 and batch: 850, loss is 4.063893799781799 and perplexity is 58.200491503659876
At time: 758.7023842334747 and batch: 900, loss is 4.0171673250198365 and perplexity is 55.543545961252754
At time: 759.9105107784271 and batch: 950, loss is 4.0102017211914065 and perplexity is 55.15799597797442
At time: 761.1187214851379 and batch: 1000, loss is 3.9857979917526247 and perplexity is 53.8282268248515
At time: 762.3260514736176 and batch: 1050, loss is 3.9643584728240966 and perplexity is 52.68645875049441
At time: 763.5345118045807 and batch: 1100, loss is 3.908250298500061 and perplexity is 49.81172003947215
At time: 764.7503979206085 and batch: 1150, loss is 3.936445379257202 and perplexity is 51.23615213468184
At time: 765.9587264060974 and batch: 1200, loss is 3.9515681362152097 and perplexity is 52.01687245366945
At time: 767.1714382171631 and batch: 1250, loss is 3.9701659631729127 and perplexity is 52.99332505026697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533132734089873 and perplexity of 93.04960463154003
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 770.147210597992 and batch: 50, loss is 4.079791593551636 and perplexity is 59.1331448369247
At time: 771.3526537418365 and batch: 100, loss is 4.105288171768189 and perplexity is 60.660222592748745
At time: 772.556608915329 and batch: 150, loss is 4.010601015090942 and perplexity is 55.18002462693894
At time: 773.7605521678925 and batch: 200, loss is 4.076215395927429 and perplexity is 58.922050706986866
At time: 774.9667994976044 and batch: 250, loss is 4.078859438896179 and perplexity is 59.078049283450966
At time: 776.1738851070404 and batch: 300, loss is 4.071841235160828 and perplexity is 58.664879049628325
At time: 777.3812222480774 and batch: 350, loss is 4.0541746616363525 and perplexity is 57.63757285981318
At time: 778.5864684581757 and batch: 400, loss is 4.065442233085633 and perplexity is 58.29068089110902
At time: 779.7943925857544 and batch: 450, loss is 4.007354340553284 and perplexity is 55.00116355495235
At time: 781.0026807785034 and batch: 500, loss is 4.019733777046204 and perplexity is 55.68627888759604
At time: 782.2550363540649 and batch: 550, loss is 4.015804877281189 and perplexity is 55.46792231099515
At time: 783.4671404361725 and batch: 600, loss is 4.02981059551239 and perplexity is 56.25025618715054
At time: 784.6739499568939 and batch: 650, loss is 4.045046157836914 and perplexity is 57.1138222237958
At time: 785.8812882900238 and batch: 700, loss is 4.025777540206909 and perplexity is 56.023852648877046
At time: 787.0878381729126 and batch: 750, loss is 4.012371582984924 and perplexity is 55.27781115022584
At time: 788.3064677715302 and batch: 800, loss is 4.035723090171814 and perplexity is 56.5838206537951
At time: 789.5159773826599 and batch: 850, loss is 4.063860449790955 and perplexity is 58.19855055016665
At time: 790.725923538208 and batch: 900, loss is 4.017123789787292 and perplexity is 55.54112791269853
At time: 791.9384052753448 and batch: 950, loss is 4.010155601501465 and perplexity is 55.155452166962455
At time: 793.153559923172 and batch: 1000, loss is 3.9857492303848265 and perplexity is 53.82560215087724
At time: 794.3694217205048 and batch: 1050, loss is 3.9643082284927367 and perplexity is 52.68381162110495
At time: 795.5861675739288 and batch: 1100, loss is 3.9081843519210815 and perplexity is 49.80843523525446
At time: 796.8019104003906 and batch: 1150, loss is 3.9363807821273804 and perplexity is 51.23284253320736
At time: 798.014062166214 and batch: 1200, loss is 3.9515112257003784 and perplexity is 52.013912230912894
At time: 799.2277796268463 and batch: 1250, loss is 3.970121750831604 and perplexity is 52.99098214308584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533132734089873 and perplexity of 93.04960463154003
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 802.2377505302429 and batch: 50, loss is 4.079790248870849 and perplexity is 59.13306532177447
At time: 803.4781229496002 and batch: 100, loss is 4.105286707878113 and perplexity is 60.66013379291589
At time: 804.6895077228546 and batch: 150, loss is 4.010597929954529 and perplexity is 55.17985438929829
At time: 805.8996005058289 and batch: 200, loss is 4.076213493347168 and perplexity is 58.92193860316288
At time: 807.1170132160187 and batch: 250, loss is 4.078858466148376 and perplexity is 59.077991815436256
At time: 808.3295559883118 and batch: 300, loss is 4.071841917037964 and perplexity is 58.664919051881675
At time: 809.545490026474 and batch: 350, loss is 4.0541743612289425 and perplexity is 57.637555545061794
At time: 810.7568809986115 and batch: 400, loss is 4.065440893173218 and perplexity is 58.29060278675435
At time: 811.9831788539886 and batch: 450, loss is 4.007352652549744 and perplexity is 55.00107071287194
At time: 813.2244534492493 and batch: 500, loss is 4.0197330617904665 and perplexity is 55.68623905767982
At time: 814.4399082660675 and batch: 550, loss is 4.015798468589782 and perplexity is 55.467566835337145
At time: 815.6514592170715 and batch: 600, loss is 4.02980667591095 and perplexity is 56.250035708997494
At time: 816.8616032600403 and batch: 650, loss is 4.045041522979736 and perplexity is 57.113557510000355
At time: 818.0719020366669 and batch: 700, loss is 4.025776147842407 and perplexity is 56.02377464330767
At time: 819.2839136123657 and batch: 750, loss is 4.012368597984314 and perplexity is 55.27764614617209
At time: 820.5005643367767 and batch: 800, loss is 4.035717363357544 and perplexity is 56.5834966096914
At time: 821.7134957313538 and batch: 850, loss is 4.063852014541626 and perplexity is 58.19805963295272
At time: 822.9238274097443 and batch: 900, loss is 4.017112565040589 and perplexity is 55.54050448110501
At time: 824.1320219039917 and batch: 950, loss is 4.010143551826477 and perplexity is 55.15478756569417
At time: 825.3422403335571 and batch: 1000, loss is 3.9857364082336426 and perplexity is 53.82491199529354
At time: 826.5517866611481 and batch: 1050, loss is 3.9642949151992797 and perplexity is 52.683110230729326
At time: 827.7627229690552 and batch: 1100, loss is 3.9081669521331786 and perplexity is 49.807568586585354
At time: 828.9740047454834 and batch: 1150, loss is 3.9363636922836305 and perplexity is 51.23196697941517
At time: 830.1886134147644 and batch: 1200, loss is 3.9514959955215456 and perplexity is 52.013120055760325
At time: 831.3935670852661 and batch: 1250, loss is 3.970110001564026 and perplexity is 52.990359541514984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.533132734089873 and perplexity of 93.04960463154003
Annealing...
Model not improving. Stopping early with 93.04952172212941loss at 25 epochs.
Finished Training.
Improved accuracyfrom -111.44297280388199 to -93.04952172212941
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f71ae834898>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 4.111335993008976, 'data': 'wikitext', 'dropout': 0.6588457475619254, 'tune_wordvecs': True, 'lr': 6.931668359310156, 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6809024810791016 and batch: 50, loss is 7.304016180038452 and perplexity is 1486.2570330013714
At time: 2.889580726623535 and batch: 100, loss is 6.585833587646484 and perplexity is 724.7549426993147
At time: 4.098123073577881 and batch: 150, loss is 6.333360757827759 and perplexity is 563.045677867273
At time: 5.307203769683838 and batch: 200, loss is 6.2554060745239255 and perplexity is 520.8208238905826
At time: 6.516577482223511 and batch: 250, loss is 6.2460243511199955 and perplexity is 515.9574759417519
At time: 7.752192258834839 and batch: 300, loss is 6.198998107910156 and perplexity is 492.2556069518635
At time: 8.96183967590332 and batch: 350, loss is 6.2079909133911135 and perplexity is 496.7023301631533
At time: 10.172921419143677 and batch: 400, loss is 6.134854507446289 and perplexity is 461.6719193057974
At time: 11.383666515350342 and batch: 450, loss is 6.104208583831787 and perplexity is 447.7381539861364
At time: 12.595203638076782 and batch: 500, loss is 6.086685390472412 and perplexity is 439.9606937222138
At time: 13.807785272598267 and batch: 550, loss is 6.082399654388428 and perplexity is 438.07917303205807
At time: 15.023574590682983 and batch: 600, loss is 6.104349031448364 and perplexity is 447.8010421588578
At time: 16.23816466331482 and batch: 650, loss is 6.075779037475586 and perplexity is 435.1883985642495
At time: 17.451942205429077 and batch: 700, loss is 6.076108913421631 and perplexity is 435.3319804297336
At time: 18.665682315826416 and batch: 750, loss is 6.014774322509766 and perplexity is 409.43342858097805
At time: 19.88337230682373 and batch: 800, loss is 6.017335996627808 and perplexity is 410.4836081326353
At time: 21.10180425643921 and batch: 850, loss is 6.054760847091675 and perplexity is 426.1369813689332
At time: 22.324718475341797 and batch: 900, loss is 6.042212018966675 and perplexity is 420.8228742758569
At time: 23.54016137123108 and batch: 950, loss is 6.009965295791626 and perplexity is 407.46917913322574
At time: 24.754676342010498 and batch: 1000, loss is 5.997396554946899 and perplexity is 402.3798548156121
At time: 25.974848747253418 and batch: 1050, loss is 5.973306493759155 and perplexity is 392.8023243140069
At time: 27.19356107711792 and batch: 1100, loss is 5.957963495254517 and perplexity is 386.8215576127514
At time: 28.40983533859253 and batch: 1150, loss is 6.006064577102661 and perplexity is 405.88285240956213
At time: 29.62873888015747 and batch: 1200, loss is 5.995053062438965 and perplexity is 401.4379847043416
At time: 30.844430923461914 and batch: 1250, loss is 5.962880744934082 and perplexity is 388.7283400134852
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.336550524635037 and perplexity of 207.79468994039422
Finished 1 epochs...
Completing Train Step...
At time: 33.859330892562866 and batch: 50, loss is 5.57959550857544 and perplexity is 264.96440827649025
At time: 35.070189476013184 and batch: 100, loss is 5.534101114273072 and perplexity is 253.18010533920403
At time: 36.27837562561035 and batch: 150, loss is 5.375159959793091 and perplexity is 215.97441677069776
At time: 37.483580112457275 and batch: 200, loss is 5.35852801322937 and perplexity is 212.41204848991617
At time: 38.68558883666992 and batch: 250, loss is 5.3568241405487065 and perplexity is 212.05043356383328
At time: 39.88714909553528 and batch: 300, loss is 5.33145393371582 and perplexity is 206.73833958996275
At time: 41.089604139328 and batch: 350, loss is 5.333325033187866 and perplexity is 207.1255297106733
At time: 42.29161739349365 and batch: 400, loss is 5.281507205963135 and perplexity is 196.66606835476776
At time: 43.4934561252594 and batch: 450, loss is 5.243821897506714 and perplexity is 189.39255989948362
At time: 44.72192621231079 and batch: 500, loss is 5.228170785903931 and perplexity is 186.451431807086
At time: 45.923582315444946 and batch: 550, loss is 5.2335409164428714 and perplexity is 187.45539362536087
At time: 47.124345541000366 and batch: 600, loss is 5.233468627929687 and perplexity is 187.4418432434417
At time: 48.326923847198486 and batch: 650, loss is 5.202813634872436 and perplexity is 181.78299397430536
At time: 49.528971910476685 and batch: 700, loss is 5.194631233215332 and perplexity is 180.30164127946293
At time: 50.73057675361633 and batch: 750, loss is 5.181304607391358 and perplexity is 177.9147685757925
At time: 51.932945251464844 and batch: 800, loss is 5.183755235671997 and perplexity is 178.35130621647994
At time: 53.134462118148804 and batch: 850, loss is 5.234744462966919 and perplexity is 187.68114073409666
At time: 54.33645248413086 and batch: 900, loss is 5.2049579906463626 and perplexity is 182.17321962882352
At time: 55.53803730010986 and batch: 950, loss is 5.1705380630493165 and perplexity is 176.00951622722442
At time: 56.739699363708496 and batch: 1000, loss is 5.143781127929688 and perplexity is 171.3624883760826
At time: 57.94166111946106 and batch: 1050, loss is 5.133320274353028 and perplexity is 169.57923392020902
At time: 59.143882274627686 and batch: 1100, loss is 5.110990056991577 and perplexity is 165.83445918977742
At time: 60.346139430999756 and batch: 1150, loss is 5.132816171646118 and perplexity is 169.49377011243237
At time: 61.54878306388855 and batch: 1200, loss is 5.114442682266235 and perplexity is 166.40801299841362
At time: 62.75080442428589 and batch: 1250, loss is 5.100580625534057 and perplexity is 164.1171702608023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.907552677349453 and perplexity of 135.30786686466382
Finished 2 epochs...
Completing Train Step...
At time: 65.7183141708374 and batch: 50, loss is 5.009954280853272 and perplexity is 149.89788278908554
At time: 66.9263846874237 and batch: 100, loss is 5.028114366531372 and perplexity is 152.64490880295446
At time: 68.13246870040894 and batch: 150, loss is 4.914929838180542 and perplexity is 136.3097457272619
At time: 69.35228610038757 and batch: 200, loss is 4.941513442993164 and perplexity is 139.98194401819225
At time: 70.56571507453918 and batch: 250, loss is 4.956048965454102 and perplexity is 142.03151441162177
At time: 71.77261281013489 and batch: 300, loss is 4.95239089012146 and perplexity is 141.51290157302964
At time: 73.00410914421082 and batch: 350, loss is 4.953951864242554 and perplexity is 141.73397204794205
At time: 74.21204400062561 and batch: 400, loss is 4.937450685501099 and perplexity is 139.4143850343125
At time: 75.41911149024963 and batch: 450, loss is 4.8923076725006105 and perplexity is 133.26074164276616
At time: 76.62524938583374 and batch: 500, loss is 4.897698564529419 and perplexity is 133.98107578993586
At time: 77.83173441886902 and batch: 550, loss is 4.899581127166748 and perplexity is 134.23354112366707
At time: 79.03701186180115 and batch: 600, loss is 4.905134191513062 and perplexity is 134.98102209912898
At time: 80.25007438659668 and batch: 650, loss is 4.897175235748291 and perplexity is 133.9109779805489
At time: 81.47481775283813 and batch: 700, loss is 4.895682725906372 and perplexity is 133.7112636028825
At time: 82.6899721622467 and batch: 750, loss is 4.8715886306762695 and perplexity is 130.52811323845663
At time: 83.89728021621704 and batch: 800, loss is 4.892268028259277 and perplexity is 133.2554587264835
At time: 85.10432267189026 and batch: 850, loss is 4.952742977142334 and perplexity is 141.56273520133172
At time: 86.31131911277771 and batch: 900, loss is 4.904027996063232 and perplexity is 134.83178926222976
At time: 87.52032494544983 and batch: 950, loss is 4.892723808288574 and perplexity is 133.31620774640922
At time: 88.72680139541626 and batch: 1000, loss is 4.872402238845825 and perplexity is 130.63435519154396
At time: 89.93284821510315 and batch: 1050, loss is 4.854948682785034 and perplexity is 128.37410324423115
At time: 91.1394214630127 and batch: 1100, loss is 4.828096990585327 and perplexity is 124.97290959636233
At time: 92.36297917366028 and batch: 1150, loss is 4.8674069595336915 and perplexity is 129.98342723661506
At time: 93.57897710800171 and batch: 1200, loss is 4.856866226196289 and perplexity is 128.6205023250497
At time: 94.78626346588135 and batch: 1250, loss is 4.861617631912232 and perplexity is 129.23308467616278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.809347110943203 and perplexity of 122.65151353961582
Finished 3 epochs...
Completing Train Step...
At time: 97.73092913627625 and batch: 50, loss is 4.788771095275879 and perplexity is 120.15362054572103
At time: 98.95572113990784 and batch: 100, loss is 4.793195390701294 and perplexity is 120.68639336255838
At time: 100.15907073020935 and batch: 150, loss is 4.710797710418701 and perplexity is 111.1407827136443
At time: 101.36184477806091 and batch: 200, loss is 4.747958536148071 and perplexity is 115.34856407772794
At time: 102.56394243240356 and batch: 250, loss is 4.757413778305054 and perplexity is 116.44438514632392
At time: 103.79193544387817 and batch: 300, loss is 4.758008432388306 and perplexity is 116.51364986765657
At time: 104.9964554309845 and batch: 350, loss is 4.750548105239869 and perplexity is 115.64765424423399
At time: 106.20022416114807 and batch: 400, loss is 4.752324571609497 and perplexity is 115.85328100315932
At time: 107.40401911735535 and batch: 450, loss is 4.700972156524658 and perplexity is 110.05411028492436
At time: 108.62102961540222 and batch: 500, loss is 4.70868953704834 and perplexity is 110.90672547846833
At time: 109.82615637779236 and batch: 550, loss is 4.7157988929748536 and perplexity is 111.69801029441778
At time: 111.03063917160034 and batch: 600, loss is 4.708558254241943 and perplexity is 110.89216628800587
At time: 112.23373055458069 and batch: 650, loss is 4.716543025970459 and perplexity is 111.78115940258198
At time: 113.43763542175293 and batch: 700, loss is 4.7250798892974855 and perplexity is 112.73950469413896
At time: 114.64060235023499 and batch: 750, loss is 4.695906171798706 and perplexity is 109.49798768685612
At time: 115.84459710121155 and batch: 800, loss is 4.713238372802734 and perplexity is 111.41237113434943
At time: 117.04797720909119 and batch: 850, loss is 4.776435403823853 and perplexity is 118.68054692631841
At time: 118.2648332118988 and batch: 900, loss is 4.720938005447388 and perplexity is 112.27351646112328
At time: 119.47652959823608 and batch: 950, loss is 4.7128076171875 and perplexity is 111.36438996469779
At time: 120.68461298942566 and batch: 1000, loss is 4.699469003677368 and perplexity is 109.88880640531717
At time: 121.89174509048462 and batch: 1050, loss is 4.6860205268859865 and perplexity is 108.42086227352124
At time: 123.0992476940155 and batch: 1100, loss is 4.647538404464722 and perplexity is 104.32785624760379
At time: 124.31157946586609 and batch: 1150, loss is 4.68103406906128 and perplexity is 107.88157190825022
At time: 125.53194737434387 and batch: 1200, loss is 4.683734531402588 and perplexity is 108.17329574784814
At time: 126.74331736564636 and batch: 1250, loss is 4.696098947525025 and perplexity is 109.51909827570208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.758153316748404 and perplexity of 116.5305320962162
Finished 4 epochs...
Completing Train Step...
At time: 129.71498656272888 and batch: 50, loss is 4.6295793342590335 and perplexity is 102.47094902491045
At time: 130.92362189292908 and batch: 100, loss is 4.630136699676513 and perplexity is 102.52807870776908
At time: 132.13301253318787 and batch: 150, loss is 4.550958738327027 and perplexity is 94.72317953772666
At time: 133.34378814697266 and batch: 200, loss is 4.595068244934082 and perplexity is 98.99489121697076
At time: 134.5782287120819 and batch: 250, loss is 4.608873949050904 and perplexity is 100.37106304690226
At time: 135.78829884529114 and batch: 300, loss is 4.609598636627197 and perplexity is 100.44382707171967
At time: 136.99850726127625 and batch: 350, loss is 4.59578691482544 and perplexity is 99.06606143557003
At time: 138.20690155029297 and batch: 400, loss is 4.597311096191406 and perplexity is 99.21717121050746
At time: 139.41624069213867 and batch: 450, loss is 4.542267055511474 and perplexity is 93.90344331014394
At time: 140.6265869140625 and batch: 500, loss is 4.556869096755982 and perplexity is 95.28468519561612
At time: 141.84390091896057 and batch: 550, loss is 4.560700740814209 and perplexity is 95.65048254857534
At time: 143.05282711982727 and batch: 600, loss is 4.562869453430176 and perplexity is 95.85814605664449
At time: 144.26284337043762 and batch: 650, loss is 4.571934118270874 and perplexity is 96.7310181927307
At time: 145.4709973335266 and batch: 700, loss is 4.5828733730316165 and perplexity is 97.79499237611093
At time: 146.6794707775116 and batch: 750, loss is 4.555419311523438 and perplexity is 95.14664295611105
At time: 147.88753843307495 and batch: 800, loss is 4.577111406326294 and perplexity is 97.23312118217021
At time: 149.0979073047638 and batch: 850, loss is 4.638458185195923 and perplexity is 103.38482438451477
At time: 150.30650663375854 and batch: 900, loss is 4.586718902587891 and perplexity is 98.17178993848718
At time: 151.51473760604858 and batch: 950, loss is 4.581787853240967 and perplexity is 97.6888915741391
At time: 152.72313570976257 and batch: 1000, loss is 4.56789303779602 and perplexity is 96.34090912575023
At time: 153.94282746315002 and batch: 1050, loss is 4.5522385597229 and perplexity is 94.8444858982643
At time: 155.15512585639954 and batch: 1100, loss is 4.5198937606811525 and perplexity is 91.82584194502796
At time: 156.36529421806335 and batch: 1150, loss is 4.551099233627319 and perplexity is 94.73648863419132
At time: 157.57575941085815 and batch: 1200, loss is 4.55714298248291 and perplexity is 95.31078588502604
At time: 158.7849681377411 and batch: 1250, loss is 4.565257606506347 and perplexity is 96.08734355344788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.746699646441606 and perplexity of 115.20344432180387
Finished 5 epochs...
Completing Train Step...
At time: 161.76070499420166 and batch: 50, loss is 4.502909679412841 and perplexity is 90.27943371718138
At time: 162.96524167060852 and batch: 100, loss is 4.508415336608887 and perplexity is 90.77785213253372
At time: 164.1955463886261 and batch: 150, loss is 4.427711067199707 and perplexity is 83.73952323677052
At time: 165.40030980110168 and batch: 200, loss is 4.474093933105468 and perplexity is 87.71508865572855
At time: 166.60534262657166 and batch: 250, loss is 4.485550289154053 and perplexity is 88.72576220616583
At time: 167.81121349334717 and batch: 300, loss is 4.487202138900757 and perplexity is 88.87244494956516
At time: 169.01583099365234 and batch: 350, loss is 4.471163244247436 and perplexity is 87.45839934469983
At time: 170.22080254554749 and batch: 400, loss is 4.472638635635376 and perplexity is 87.5875299495631
At time: 171.4263219833374 and batch: 450, loss is 4.419746923446655 and perplexity is 83.07525829768976
At time: 172.6358494758606 and batch: 500, loss is 4.4461283779144285 and perplexity is 85.29606975799938
At time: 173.8500304222107 and batch: 550, loss is 4.4530524730682375 and perplexity is 85.88871726734568
At time: 175.0623960494995 and batch: 600, loss is 4.451872339248657 and perplexity is 85.7874168731456
At time: 176.26742482185364 and batch: 650, loss is 4.462339553833008 and perplexity is 86.69008815965348
At time: 177.47201895713806 and batch: 700, loss is 4.466568412780762 and perplexity is 87.05746455864497
At time: 178.67763209342957 and batch: 750, loss is 4.4442032623291015 and perplexity is 85.1320229199675
At time: 179.88280630111694 and batch: 800, loss is 4.4699690055847165 and perplexity is 87.35401548484948
At time: 181.08708930015564 and batch: 850, loss is 4.53156720161438 and perplexity is 92.90404642143908
At time: 182.2913670539856 and batch: 900, loss is 4.474991111755371 and perplexity is 87.7938200733372
At time: 183.4959011077881 and batch: 950, loss is 4.4691051006317135 and perplexity is 87.27858250636078
At time: 184.70014071464539 and batch: 1000, loss is 4.451571741104126 and perplexity is 85.76163321026415
At time: 185.90473008155823 and batch: 1050, loss is 4.446935815811157 and perplexity is 85.36496884928941
At time: 187.10926175117493 and batch: 1100, loss is 4.413714227676391 and perplexity is 82.57559919936585
At time: 188.31443881988525 and batch: 1150, loss is 4.438476524353027 and perplexity is 84.64588744516266
At time: 189.5197126865387 and batch: 1200, loss is 4.445645875930786 and perplexity is 85.25492416235828
At time: 190.72578763961792 and batch: 1250, loss is 4.471081352233886 and perplexity is 87.45123749352885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.733952209027144 and perplexity of 113.74421609560818
Finished 6 epochs...
Completing Train Step...
At time: 193.68149662017822 and batch: 50, loss is 4.405201444625854 and perplexity is 81.87563458860137
At time: 194.91124153137207 and batch: 100, loss is 4.398227252960205 and perplexity is 81.30660478811458
At time: 196.12234497070312 and batch: 150, loss is 4.33012656211853 and perplexity is 75.9538988472481
At time: 197.32978534698486 and batch: 200, loss is 4.373743124008179 and perplexity is 79.34005624589102
At time: 198.5376832485199 and batch: 250, loss is 4.383847236633301 and perplexity is 80.14578082091064
At time: 199.74688577651978 and batch: 300, loss is 4.389528160095215 and perplexity is 80.60237858805674
At time: 200.9613778591156 and batch: 350, loss is 4.374638433456421 and perplexity is 79.41112195602581
At time: 202.16937041282654 and batch: 400, loss is 4.373280200958252 and perplexity is 79.30333640495842
At time: 203.37782096862793 and batch: 450, loss is 4.321786489486694 and perplexity is 75.32307204140122
At time: 204.58651161193848 and batch: 500, loss is 4.351709403991699 and perplexity is 77.61101818202064
At time: 205.79553031921387 and batch: 550, loss is 4.353003072738647 and perplexity is 77.71148610275436
At time: 207.00380206108093 and batch: 600, loss is 4.362992753982544 and perplexity is 78.49168958166635
At time: 208.21207690238953 and batch: 650, loss is 4.377160425186157 and perplexity is 79.61164890621632
At time: 209.42000007629395 and batch: 700, loss is 4.377052059173584 and perplexity is 79.60302217670056
At time: 210.6261134147644 and batch: 750, loss is 4.351614980697632 and perplexity is 77.60369023999765
At time: 211.8324635028839 and batch: 800, loss is 4.38260443687439 and perplexity is 80.04623753282837
At time: 213.03974843025208 and batch: 850, loss is 4.437988109588623 and perplexity is 84.60455523844
At time: 214.24927306175232 and batch: 900, loss is 4.382215604782105 and perplexity is 80.0151190371359
At time: 215.455491065979 and batch: 950, loss is 4.379356718063354 and perplexity is 79.78669155575032
At time: 216.66218209266663 and batch: 1000, loss is 4.361735534667969 and perplexity is 78.39307031950102
At time: 217.86930632591248 and batch: 1050, loss is 4.365255146026612 and perplexity is 78.6694695839033
At time: 219.07727789878845 and batch: 1100, loss is 4.317930603027344 and perplexity is 75.03319405529624
At time: 220.28469586372375 and batch: 1150, loss is 4.344273338317871 and perplexity is 77.03603799847129
At time: 221.4928936958313 and batch: 1200, loss is 4.35980749130249 and perplexity is 78.24207069406157
At time: 222.69963765144348 and batch: 1250, loss is 4.376901521682739 and perplexity is 79.5910398393965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.73188815151688 and perplexity of 113.5096836196516
Finished 7 epochs...
Completing Train Step...
At time: 225.67406725883484 and batch: 50, loss is 4.321318187713623 and perplexity is 75.28780637134348
At time: 226.87921595573425 and batch: 100, loss is 4.31274447441101 and perplexity is 74.64506956211524
At time: 228.08636379241943 and batch: 150, loss is 4.2458053302764895 and perplexity is 69.81195919586827
At time: 229.29027342796326 and batch: 200, loss is 4.29473032951355 and perplexity is 73.31244157728146
At time: 230.49854135513306 and batch: 250, loss is 4.301387033462524 and perplexity is 73.80208870653699
At time: 231.70436072349548 and batch: 300, loss is 4.3056511878967285 and perplexity is 74.11746413724633
At time: 232.90971684455872 and batch: 350, loss is 4.291084461212158 and perplexity is 73.04564072635402
At time: 234.1148555278778 and batch: 400, loss is 4.296511926651001 and perplexity is 73.44317123254594
At time: 235.32060885429382 and batch: 450, loss is 4.238219051361084 and perplexity is 69.28435002732773
At time: 236.52576351165771 and batch: 500, loss is 4.271233787536621 and perplexity is 71.60993258048946
At time: 237.73166370391846 and batch: 550, loss is 4.279832310676575 and perplexity is 72.22832707244991
At time: 238.93721270561218 and batch: 600, loss is 4.286921234130859 and perplexity is 72.74216728945503
At time: 240.14250802993774 and batch: 650, loss is 4.294423933029175 and perplexity is 73.28998234381201
At time: 241.34782147407532 and batch: 700, loss is 4.299322910308838 and perplexity is 73.6499092190806
At time: 242.55320000648499 and batch: 750, loss is 4.277281222343444 and perplexity is 72.04430106304088
At time: 243.75867652893066 and batch: 800, loss is 4.315138182640076 and perplexity is 74.82396210223915
At time: 244.96513557434082 and batch: 850, loss is 4.362596397399902 and perplexity is 78.46058504846842
At time: 246.1861605644226 and batch: 900, loss is 4.3076391649246215 and perplexity is 74.26495450847764
At time: 247.40101289749146 and batch: 950, loss is 4.310295124053955 and perplexity is 74.4624613613554
At time: 248.61012077331543 and batch: 1000, loss is 4.2885862159729005 and perplexity is 72.86338255975232
At time: 249.81701278686523 and batch: 1050, loss is 4.296228332519531 and perplexity is 73.42234613326539
At time: 251.0227825641632 and batch: 1100, loss is 4.248059778213501 and perplexity is 69.969524167533
At time: 252.22902059555054 and batch: 1150, loss is 4.278472948074341 and perplexity is 72.13020928973695
At time: 253.43949055671692 and batch: 1200, loss is 4.28944037437439 and perplexity is 72.92564601776903
At time: 254.64462161064148 and batch: 1250, loss is 4.308081359863281 and perplexity is 74.297801357301
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.736501874714873 and perplexity of 114.03459584890314
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 257.578072309494 and batch: 50, loss is 4.260630254745483 and perplexity is 70.85462584893902
At time: 258.8054814338684 and batch: 100, loss is 4.26312816619873 and perplexity is 71.03183566543512
At time: 260.01346731185913 and batch: 150, loss is 4.184426231384277 and perplexity is 65.65581885907952
At time: 261.2181406021118 and batch: 200, loss is 4.233046417236328 and perplexity is 68.92689272898913
At time: 262.4227044582367 and batch: 250, loss is 4.231587080955506 and perplexity is 68.82637857352071
At time: 263.62826204299927 and batch: 300, loss is 4.229377107620239 and perplexity is 68.67444206176803
At time: 264.8330407142639 and batch: 350, loss is 4.204976472854614 and perplexity is 67.01902088150827
At time: 266.0376329421997 and batch: 400, loss is 4.208798027038574 and perplexity is 67.27562770731092
At time: 267.2423360347748 and batch: 450, loss is 4.129448499679565 and perplexity is 62.14364124438883
At time: 268.4481346607208 and batch: 500, loss is 4.149815831184387 and perplexity is 63.42231880913486
At time: 269.6552543640137 and batch: 550, loss is 4.1532232236862185 and perplexity is 63.638792138049006
At time: 270.8615257740021 and batch: 600, loss is 4.139722356796264 and perplexity is 62.78538709302813
At time: 272.06681394577026 and batch: 650, loss is 4.143901405334472 and perplexity is 63.048319293730884
At time: 273.27264952659607 and batch: 700, loss is 4.129842276573181 and perplexity is 62.168116793032524
At time: 274.48771357536316 and batch: 750, loss is 4.099047980308533 and perplexity is 60.282869791158234
At time: 275.6945149898529 and batch: 800, loss is 4.122975611686707 and perplexity is 61.74269146657131
At time: 276.9015221595764 and batch: 850, loss is 4.162308073043823 and perplexity is 64.2195751464466
At time: 278.1076600551605 and batch: 900, loss is 4.094044613838196 and perplexity is 59.982005795766845
At time: 279.31504249572754 and batch: 950, loss is 4.084108195304871 and perplexity is 59.38895078259522
At time: 280.5289783477783 and batch: 1000, loss is 4.057123880386353 and perplexity is 57.807809579667236
At time: 281.74009680747986 and batch: 1050, loss is 4.047681436538697 and perplexity is 57.26453155640757
At time: 282.95172476768494 and batch: 1100, loss is 3.984680790901184 and perplexity is 53.76812346402562
At time: 284.1584131717682 and batch: 1150, loss is 3.9869107103347776 and perplexity is 53.8881558289587
At time: 285.3639361858368 and batch: 1200, loss is 3.9915058326721193 and perplexity is 54.1363482981879
At time: 286.61399030685425 and batch: 1250, loss is 3.9975475215911866 and perplexity is 54.46441330927231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613894720147126 and perplexity of 100.87627038306302
Finished 9 epochs...
Completing Train Step...
At time: 289.60127687454224 and batch: 50, loss is 4.143107495307922 and perplexity is 62.99828446509059
At time: 290.8171169757843 and batch: 100, loss is 4.142077965736389 and perplexity is 62.93345924374495
At time: 292.02717876434326 and batch: 150, loss is 4.065054087638855 and perplexity is 58.26806001910982
At time: 293.23637199401855 and batch: 200, loss is 4.118027181625366 and perplexity is 61.43791677608708
At time: 294.44692873954773 and batch: 250, loss is 4.12004919052124 and perplexity is 61.56227047012157
At time: 295.65794038772583 and batch: 300, loss is 4.1234954690933225 and perplexity is 61.77479720651489
At time: 296.8681926727295 and batch: 350, loss is 4.106843404769897 and perplexity is 60.75463677180916
At time: 298.0787193775177 and batch: 400, loss is 4.114579343795777 and perplexity is 61.22645355738006
At time: 299.2906596660614 and batch: 450, loss is 4.0384659004211425 and perplexity is 56.739232372027026
At time: 300.5006272792816 and batch: 500, loss is 4.066145000457763 and perplexity is 58.33166007746956
At time: 301.7120940685272 and batch: 550, loss is 4.070876307487488 and perplexity is 58.608298986604574
At time: 302.92115235328674 and batch: 600, loss is 4.064961371421814 and perplexity is 58.26265787544752
At time: 304.13118052482605 and batch: 650, loss is 4.074797048568725 and perplexity is 58.83853801100336
At time: 305.3427734375 and batch: 700, loss is 4.064706964492798 and perplexity is 58.24783733688492
At time: 306.5518796443939 and batch: 750, loss is 4.04013313293457 and perplexity is 56.833908766875744
At time: 307.7630407810211 and batch: 800, loss is 4.069612736701965 and perplexity is 58.53429001984701
At time: 308.9771716594696 and batch: 850, loss is 4.111542921066285 and perplexity is 61.04082612644909
At time: 310.18799591064453 and batch: 900, loss is 4.048502569198608 and perplexity is 57.31157264436676
At time: 311.396288394928 and batch: 950, loss is 4.044325914382934 and perplexity is 57.072701177567154
At time: 312.6096725463867 and batch: 1000, loss is 4.023619446754456 and perplexity is 55.90307830732635
At time: 313.8199830055237 and batch: 1050, loss is 4.021518149375916 and perplexity is 55.78573264766818
At time: 315.02931547164917 and batch: 1100, loss is 3.960423426628113 and perplexity is 52.47954248094915
At time: 316.237802028656 and batch: 1150, loss is 3.9745108222961427 and perplexity is 53.2240745060175
At time: 317.47270107269287 and batch: 1200, loss is 3.9868977165222166 and perplexity is 53.8874556209118
At time: 318.6810042858124 and batch: 1250, loss is 3.9998088026046754 and perplexity is 54.587712006961866
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.611149474652144 and perplexity of 100.59972002923205
Finished 10 epochs...
Completing Train Step...
At time: 321.62755155563354 and batch: 50, loss is 4.0950037431716915 and perplexity is 60.03956389542592
At time: 322.8661632537842 and batch: 100, loss is 4.091625065803528 and perplexity is 59.83705188372067
At time: 324.07963395118713 and batch: 150, loss is 4.014764518737793 and perplexity is 55.410245791466544
At time: 325.2876353263855 and batch: 200, loss is 4.068906035423279 and perplexity is 58.49293837559391
At time: 326.49561500549316 and batch: 250, loss is 4.069812235832214 and perplexity is 58.54596872470268
At time: 327.70204973220825 and batch: 300, loss is 4.075288805961609 and perplexity is 58.867479412547596
At time: 328.9082853794098 and batch: 350, loss is 4.059400682449341 and perplexity is 57.93957646641341
At time: 330.11473417282104 and batch: 400, loss is 4.0692363405227665 and perplexity is 58.512262082607194
At time: 331.32156920433044 and batch: 450, loss is 3.9924290561676026 and perplexity is 54.18635132532956
At time: 332.52987265586853 and batch: 500, loss is 4.020843563079834 and perplexity is 55.74811304716727
At time: 333.7418520450592 and batch: 550, loss is 4.028617725372315 and perplexity is 56.18319694061149
At time: 334.9494252204895 and batch: 600, loss is 4.027224111557007 and perplexity is 56.10495379419516
At time: 336.15567994117737 and batch: 650, loss is 4.037833766937256 and perplexity is 56.70337693729904
At time: 337.363285779953 and batch: 700, loss is 4.028613610267639 and perplexity is 56.18296574135076
At time: 338.57520818710327 and batch: 750, loss is 4.006708555221557 and perplexity is 54.96565607663901
At time: 339.7865149974823 and batch: 800, loss is 4.039241685867309 and perplexity is 56.783266921200614
At time: 340.99276518821716 and batch: 850, loss is 4.082265038490295 and perplexity is 59.27958845017463
At time: 342.19922852516174 and batch: 900, loss is 4.0198121786117555 and perplexity is 55.69064495019141
At time: 343.40571570396423 and batch: 950, loss is 4.019164652824402 and perplexity is 55.65459549420757
At time: 344.61558651924133 and batch: 1000, loss is 3.9996828842163086 and perplexity is 54.58083884297927
At time: 345.827353477478 and batch: 1050, loss is 4.000170269012451 and perplexity is 54.6074471977191
At time: 347.07853507995605 and batch: 1100, loss is 3.9421846961975096 and perplexity is 51.53105812053423
At time: 348.2871994972229 and batch: 1150, loss is 3.9578572034835817 and perplexity is 52.345040918744424
At time: 349.50474309921265 and batch: 1200, loss is 3.9734952449798584 and perplexity is 53.17004878156259
At time: 350.72161769866943 and batch: 1250, loss is 3.9867419672012328 and perplexity is 53.8790633398523
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613610037921989 and perplexity of 100.84755678926561
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 353.6855823993683 and batch: 50, loss is 4.080676040649414 and perplexity is 59.185468110431124
At time: 354.93151354789734 and batch: 100, loss is 4.092551145553589 and perplexity is 59.89249143253312
At time: 356.15166330337524 and batch: 150, loss is 4.024218096733093 and perplexity is 55.93655470328288
At time: 357.3659625053406 and batch: 200, loss is 4.081844925880432 and perplexity is 59.25468957809051
At time: 358.5774064064026 and batch: 250, loss is 4.0746065044403075 and perplexity is 58.82732774112014
At time: 359.7884259223938 and batch: 300, loss is 4.077944478988647 and perplexity is 59.02401995804393
At time: 360.99987840652466 and batch: 350, loss is 4.061782660484314 and perplexity is 58.07775176484321
At time: 362.2101049423218 and batch: 400, loss is 4.068336234092713 and perplexity is 58.45961851523067
At time: 363.42553639411926 and batch: 450, loss is 3.988083519935608 and perplexity is 53.95139345108261
At time: 364.6391489505768 and batch: 500, loss is 4.00718683719635 and perplexity is 54.9919514469727
At time: 365.8504204750061 and batch: 550, loss is 4.0058605051040646 and perplexity is 54.91906220530666
At time: 367.0625751018524 and batch: 600, loss is 4.004516367912292 and perplexity is 54.84529304030499
At time: 368.27852845191956 and batch: 650, loss is 4.010957999229431 and perplexity is 55.19972653691763
At time: 369.4885802268982 and batch: 700, loss is 3.998865399360657 and perplexity is 54.53623806652983
At time: 370.7003698348999 and batch: 750, loss is 3.969192852973938 and perplexity is 52.941781787887514
At time: 371.9104416370392 and batch: 800, loss is 3.9967344284057615 and perplexity is 54.42014666485462
At time: 373.121541261673 and batch: 850, loss is 4.03214328289032 and perplexity is 56.38162360975903
At time: 374.3327057361603 and batch: 900, loss is 3.963142876625061 and perplexity is 52.62245220243666
At time: 375.5432186126709 and batch: 950, loss is 3.9551532888412475 and perplexity is 52.20369557513921
At time: 376.753306388855 and batch: 1000, loss is 3.934405484199524 and perplexity is 51.131742289936604
At time: 377.9955005645752 and batch: 1050, loss is 3.935833954811096 and perplexity is 51.20483467384851
At time: 379.2120053768158 and batch: 1100, loss is 3.8677314138412475 and perplexity is 47.833747921443106
At time: 380.4340615272522 and batch: 1150, loss is 3.875494194030762 and perplexity is 48.2065157777254
At time: 381.64568758010864 and batch: 1200, loss is 3.89191623210907 and perplexity is 49.00470099141257
At time: 382.856103181839 and batch: 1250, loss is 3.903330321311951 and perplexity is 49.5672494013003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.582632245808623 and perplexity of 97.7714141839628
Finished 12 epochs...
Completing Train Step...
At time: 385.83352041244507 and batch: 50, loss is 4.05666368484497 and perplexity is 57.78121280376981
At time: 387.0470337867737 and batch: 100, loss is 4.056396260261535 and perplexity is 57.76576275295917
At time: 388.25651574134827 and batch: 150, loss is 3.9834078121185303 and perplexity is 53.69972133014237
At time: 389.4637954235077 and batch: 200, loss is 4.039426927566528 and perplexity is 56.793786524356804
At time: 390.67024993896484 and batch: 250, loss is 4.033191514015198 and perplexity is 56.440755569063604
At time: 391.87691259384155 and batch: 300, loss is 4.039738321304322 and perplexity is 56.81147450764675
At time: 393.0872440338135 and batch: 350, loss is 4.025432481765747 and perplexity is 56.00452448047972
At time: 394.2950248718262 and batch: 400, loss is 4.034011588096619 and perplexity is 56.487060153841455
At time: 395.5029995441437 and batch: 450, loss is 3.9576343631744386 and perplexity is 52.333377633217324
At time: 396.71120142936707 and batch: 500, loss is 3.9778401851654053 and perplexity is 53.401572076380496
At time: 397.9178047180176 and batch: 550, loss is 3.9785924053192137 and perplexity is 53.44175692717389
At time: 399.15275740623474 and batch: 600, loss is 3.978431339263916 and perplexity is 53.4331499673606
At time: 400.37476778030396 and batch: 650, loss is 3.988466796875 and perplexity is 53.97207573930936
At time: 401.58680868148804 and batch: 700, loss is 3.9779923582077026 and perplexity is 53.40969897439854
At time: 402.7918469905853 and batch: 750, loss is 3.952633247375488 and perplexity is 52.07230572109795
At time: 403.99822187423706 and batch: 800, loss is 3.9817714405059816 and perplexity is 53.611920487551245
At time: 405.2064037322998 and batch: 850, loss is 4.020939311981201 and perplexity is 55.75345112329824
At time: 406.4139201641083 and batch: 900, loss is 3.9542542600631716 and perplexity is 52.15678404106452
At time: 407.6258385181427 and batch: 950, loss is 3.9486142683029173 and perplexity is 51.86344819228857
At time: 408.8961224555969 and batch: 1000, loss is 3.9313310384750366 and perplexity is 50.97478193015649
At time: 410.1046152114868 and batch: 1050, loss is 3.935375199317932 and perplexity is 51.18134956203923
At time: 411.31124472618103 and batch: 1100, loss is 3.8691868352890015 and perplexity is 47.903416870637855
At time: 412.5203137397766 and batch: 1150, loss is 3.880547533035278 and perplexity is 48.45073618873553
At time: 413.73498582839966 and batch: 1200, loss is 3.899478011131287 and perplexity is 49.376668306240425
At time: 414.94260001182556 and batch: 1250, loss is 3.9118177461624146 and perplexity is 49.989738089925524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.581461885549726 and perplexity of 97.65705334109236
Finished 13 epochs...
Completing Train Step...
At time: 417.9190969467163 and batch: 50, loss is 4.0416885137557985 and perplexity is 56.9223759207894
At time: 419.16843152046204 and batch: 100, loss is 4.039890699386596 and perplexity is 56.82013199077201
At time: 420.375776052475 and batch: 150, loss is 3.9664135360717774 and perplexity is 52.79484408673838
At time: 421.5855348110199 and batch: 200, loss is 4.02185700416565 and perplexity is 55.804639113467026
At time: 422.79720091819763 and batch: 250, loss is 4.015910983085632 and perplexity is 55.47380809176498
At time: 424.00650930404663 and batch: 300, loss is 4.0233731365203855 and perplexity is 55.88931050266844
At time: 425.2157995700836 and batch: 350, loss is 4.008459749221802 and perplexity is 55.06199593404978
At time: 426.42518877983093 and batch: 400, loss is 4.018350901603699 and perplexity is 55.609324921176636
At time: 427.63539576530457 and batch: 450, loss is 3.9418549060821535 and perplexity is 51.51406648892232
At time: 428.84413719177246 and batch: 500, loss is 3.9634466791152954 and perplexity is 52.638441463123634
At time: 430.05344343185425 and batch: 550, loss is 3.96494665145874 and perplexity is 52.717456915201915
At time: 431.26256132125854 and batch: 600, loss is 3.966284589767456 and perplexity is 52.78803682560119
At time: 432.4702105522156 and batch: 650, loss is 3.9772818279266358 and perplexity is 53.37176324481286
At time: 433.67757391929626 and batch: 700, loss is 3.967224612236023 and perplexity is 52.83768209646811
At time: 434.88579654693604 and batch: 750, loss is 3.943618688583374 and perplexity is 51.60500627338685
At time: 436.10110807418823 and batch: 800, loss is 3.9736767578125 and perplexity is 53.17970070367599
At time: 437.3094985485077 and batch: 850, loss is 4.0139230585098264 and perplexity is 55.36363988466963
At time: 438.51888132095337 and batch: 900, loss is 3.948271059989929 and perplexity is 51.84565127992758
At time: 439.77379608154297 and batch: 950, loss is 3.9435420131683347 and perplexity is 51.601049589804845
At time: 440.98242354393005 and batch: 1000, loss is 3.9280149698257447 and perplexity is 50.80602601163982
At time: 442.1915969848633 and batch: 1050, loss is 3.933338108062744 and perplexity is 51.077194605011755
At time: 443.4002981185913 and batch: 1100, loss is 3.867843255996704 and perplexity is 47.83909805009358
At time: 444.609983921051 and batch: 1150, loss is 3.880430130958557 and perplexity is 48.44504830557948
At time: 445.819109916687 and batch: 1200, loss is 3.9004943656921385 and perplexity is 49.426878019386486
At time: 447.02801418304443 and batch: 1250, loss is 3.913485116958618 and perplexity is 50.07315904683346
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.581391940151688 and perplexity of 97.6502229185063
Finished 14 epochs...
Completing Train Step...
At time: 450.0110275745392 and batch: 50, loss is 4.029211702346802 and perplexity is 56.21657837886871
At time: 451.2187602519989 and batch: 100, loss is 4.026888699531555 and perplexity is 56.086138673590426
At time: 452.4273474216461 and batch: 150, loss is 3.953433084487915 and perplexity is 52.11397174453797
At time: 453.6365623474121 and batch: 200, loss is 4.008563795089722 and perplexity is 55.067725205254455
At time: 454.8441393375397 and batch: 250, loss is 4.0035764503479 and perplexity is 54.793767204864395
At time: 456.0513472557068 and batch: 300, loss is 4.010503296852112 and perplexity is 55.174632795558196
At time: 457.2640926837921 and batch: 350, loss is 3.9955797147750856 and perplexity is 54.357343246677154
At time: 458.47968196868896 and batch: 400, loss is 4.006346225738525 and perplexity is 54.94574400647114
At time: 459.68925523757935 and batch: 450, loss is 3.929994707107544 and perplexity is 50.90670822475643
At time: 460.89652967453003 and batch: 500, loss is 3.9526041078567506 and perplexity is 52.07078838127707
At time: 462.1037244796753 and batch: 550, loss is 3.954707407951355 and perplexity is 52.1804241334316
At time: 463.3119502067566 and batch: 600, loss is 3.957150797843933 and perplexity is 52.3080771438784
At time: 464.51920533180237 and batch: 650, loss is 3.9686796379089357 and perplexity is 52.9146182388734
At time: 465.72646856307983 and batch: 700, loss is 3.9588196325302123 and perplexity is 52.39544355716277
At time: 466.9341652393341 and batch: 750, loss is 3.9362694787979127 and perplexity is 51.22714046459082
At time: 468.141560792923 and batch: 800, loss is 3.9668054389953613 and perplexity is 52.81553859534047
At time: 469.3926770687103 and batch: 850, loss is 4.007733550071716 and perplexity is 55.02202447477694
At time: 470.6005799770355 and batch: 900, loss is 3.9427684545516968 and perplexity is 51.56114858813862
At time: 471.8086657524109 and batch: 950, loss is 3.9394925117492674 and perplexity is 51.39251358466234
At time: 473.0170419216156 and batch: 1000, loss is 3.923767523765564 and perplexity is 50.590687799083845
At time: 474.22400975227356 and batch: 1050, loss is 3.9300394535064695 and perplexity is 50.90898616759514
At time: 475.4314224720001 and batch: 1100, loss is 3.865171637535095 and perplexity is 47.71146080748115
At time: 476.638072013855 and batch: 1150, loss is 3.87839017868042 and perplexity is 48.346323450157094
At time: 477.8465087413788 and batch: 1200, loss is 3.899632444381714 and perplexity is 49.38429429446015
At time: 479.053409576416 and batch: 1250, loss is 3.912698073387146 and perplexity is 50.03376479343282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.581689096715328 and perplexity of 97.67924463497066
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 482.0264129638672 and batch: 50, loss is 4.027956671714783 and perplexity is 56.14606910587959
At time: 483.258829832077 and batch: 100, loss is 4.0371662664413455 and perplexity is 56.66554003455415
At time: 484.4659984111786 and batch: 150, loss is 3.9690387725830076 and perplexity is 52.93362512586005
At time: 485.67328000068665 and batch: 200, loss is 4.02577570438385 and perplexity is 56.023749799090915
At time: 486.88031554222107 and batch: 250, loss is 4.024363293647766 and perplexity is 55.944677108103015
At time: 488.0876986980438 and batch: 300, loss is 4.028268456459045 and perplexity is 56.16357732293276
At time: 489.29357743263245 and batch: 350, loss is 4.011281399726868 and perplexity is 55.21758104286049
At time: 490.49997568130493 and batch: 400, loss is 4.016642537117004 and perplexity is 55.51440502732547
At time: 491.7065200805664 and batch: 450, loss is 3.9386838150024412 and perplexity is 51.350969426692025
At time: 492.9131486415863 and batch: 500, loss is 3.9608952045440673 and perplexity is 52.504307011350996
At time: 494.1202757358551 and batch: 550, loss is 3.9655962371826172 and perplexity is 52.75171254739341
At time: 495.326767206192 and batch: 600, loss is 3.9682105588912964 and perplexity is 52.889802922358314
At time: 496.5322091579437 and batch: 650, loss is 3.9813037490844727 and perplexity is 53.586852514743455
At time: 497.73890256881714 and batch: 700, loss is 3.9678927659988403 and perplexity is 52.872997589355315
At time: 498.9449055194855 and batch: 750, loss is 3.937733554840088 and perplexity is 51.302195823623784
At time: 500.1774606704712 and batch: 800, loss is 3.9666756200790405 and perplexity is 52.80868258438471
At time: 501.38316106796265 and batch: 850, loss is 3.997582025527954 and perplexity is 54.46629257836611
At time: 502.5995669364929 and batch: 900, loss is 3.9293822765350344 and perplexity is 50.875540945165376
At time: 503.83386278152466 and batch: 950, loss is 3.920995931625366 and perplexity is 50.45066517884412
At time: 505.066743850708 and batch: 1000, loss is 3.9033217287063597 and perplexity is 49.566823491305804
At time: 506.29725098609924 and batch: 1050, loss is 3.9078937005996703 and perplexity is 49.793960451395336
At time: 507.5112638473511 and batch: 1100, loss is 3.8433839416503908 and perplexity is 46.68318061430455
At time: 508.735554933548 and batch: 1150, loss is 3.8556807804107667 and perplexity is 47.2607802053338
At time: 509.9423520565033 and batch: 1200, loss is 3.8769060468673704 and perplexity is 48.27462435209759
At time: 511.1488583087921 and batch: 1250, loss is 3.89891480922699 and perplexity is 49.34886710220291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.572678169194799 and perplexity of 96.80301777854679
Finished 16 epochs...
Completing Train Step...
At time: 514.1339046955109 and batch: 50, loss is 4.027949380874634 and perplexity is 56.145659755356974
At time: 515.3908658027649 and batch: 100, loss is 4.026499667167664 and perplexity is 56.0643235941418
At time: 516.601377248764 and batch: 150, loss is 3.9533515644073485 and perplexity is 52.109723582520324
At time: 517.8120801448822 and batch: 200, loss is 4.0087559080123905 and perplexity is 55.078305443156026
At time: 519.0219538211823 and batch: 250, loss is 4.0059770536422725 and perplexity is 54.92546331473917
At time: 520.2336802482605 and batch: 300, loss is 4.009525437355041 and perplexity is 55.12070612753101
At time: 521.4441418647766 and batch: 350, loss is 3.9941217708587646 and perplexity is 54.27815103171489
At time: 522.6548483371735 and batch: 400, loss is 4.001937341690064 and perplexity is 54.70402783302582
At time: 523.864506483078 and batch: 450, loss is 3.9262333154678344 and perplexity is 50.715587822707896
At time: 525.0753457546234 and batch: 500, loss is 3.9492074346542356 and perplexity is 51.89422097040557
At time: 526.2853772640228 and batch: 550, loss is 3.952544174194336 and perplexity is 52.06766768174297
At time: 527.4960722923279 and batch: 600, loss is 3.956594457626343 and perplexity is 52.27898415041001
At time: 528.7081940174103 and batch: 650, loss is 3.971376748085022 and perplexity is 53.057527428472376
At time: 529.9221525192261 and batch: 700, loss is 3.9579153728485106 and perplexity is 52.348085885092864
At time: 531.1757397651672 and batch: 750, loss is 3.929946975708008 and perplexity is 50.904278434316204
At time: 532.3858199119568 and batch: 800, loss is 3.9600460720062256 and perplexity is 52.45974281902135
At time: 533.6028769016266 and batch: 850, loss is 3.993950786590576 and perplexity is 54.268871115164586
At time: 534.8128910064697 and batch: 900, loss is 3.928078284263611 and perplexity is 50.809242868452635
At time: 536.0252227783203 and batch: 950, loss is 3.9216976737976075 and perplexity is 50.48608096313909
At time: 537.2385408878326 and batch: 1000, loss is 3.9045274686813354 and perplexity is 49.62662423665152
At time: 538.4484188556671 and batch: 1050, loss is 3.91106005191803 and perplexity is 49.95187549904149
At time: 539.6578102111816 and batch: 1100, loss is 3.8474344491958616 and perplexity is 46.87265466361037
At time: 540.8674986362457 and batch: 1150, loss is 3.861358575820923 and perplexity is 47.52988047146888
At time: 542.079484462738 and batch: 1200, loss is 3.882926778793335 and perplexity is 48.5661496413266
At time: 543.2908492088318 and batch: 1250, loss is 3.9047535991668703 and perplexity is 49.63784759820989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.572087420164234 and perplexity of 96.74584837768496
Finished 17 epochs...
Completing Train Step...
At time: 546.2931749820709 and batch: 50, loss is 4.025143494606018 and perplexity is 55.98834223036203
At time: 547.5006132125854 and batch: 100, loss is 4.022020440101624 and perplexity is 55.81376034224021
At time: 548.7082726955414 and batch: 150, loss is 3.948259973526001 and perplexity is 51.84507649817098
At time: 549.9157371520996 and batch: 200, loss is 4.002985129356384 and perplexity is 54.761376077832416
At time: 551.1238849163055 and batch: 250, loss is 4.00012770652771 and perplexity is 54.60512301854277
At time: 552.3308413028717 and batch: 300, loss is 4.003617634773255 and perplexity is 54.79602390114979
At time: 553.5388450622559 and batch: 350, loss is 3.9880789852142335 and perplexity is 53.95114879710027
At time: 554.7460246086121 and batch: 400, loss is 3.9960765647888183 and perplexity is 54.3843574038522
At time: 555.9533975124359 and batch: 450, loss is 3.920687074661255 and perplexity is 50.43508554562255
At time: 557.1606912612915 and batch: 500, loss is 3.9440951585769652 and perplexity is 51.629600369104224
At time: 558.3683414459229 and batch: 550, loss is 3.947488398551941 and perplexity is 51.80508956305977
At time: 559.575394153595 and batch: 600, loss is 3.9520423126220705 and perplexity is 52.04154347609179
At time: 560.781566619873 and batch: 650, loss is 3.9674953746795656 and perplexity is 52.85199049338451
At time: 562.0142526626587 and batch: 700, loss is 3.9540929317474367 and perplexity is 52.148370353642285
At time: 563.220755815506 and batch: 750, loss is 3.9270093631744385 and perplexity is 50.75496081401055
At time: 564.4283814430237 and batch: 800, loss is 3.957618374824524 and perplexity is 52.33254091555239
At time: 565.6349639892578 and batch: 850, loss is 3.992488293647766 and perplexity is 54.18956128331529
At time: 566.8417358398438 and batch: 900, loss is 3.927219967842102 and perplexity is 50.76565117134513
At time: 568.048451423645 and batch: 950, loss is 3.921298794746399 and perplexity is 50.4659471388126
At time: 569.2579383850098 and batch: 1000, loss is 3.9045701742172243 and perplexity is 49.62874361348815
At time: 570.4766335487366 and batch: 1050, loss is 3.9117503356933594 and perplexity is 49.986368371811345
At time: 571.6864750385284 and batch: 1100, loss is 3.8485683584213257 and perplexity is 46.92583414380907
At time: 572.8940918445587 and batch: 1150, loss is 3.8631143760681153 and perplexity is 47.61340675362499
At time: 574.1091365814209 and batch: 1200, loss is 3.8849202251434325 and perplexity is 48.6630602159909
At time: 575.316987991333 and batch: 1250, loss is 3.906567816734314 and perplexity is 49.72798319139838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.57199074404083 and perplexity of 96.7364958162004
Finished 18 epochs...
Completing Train Step...
At time: 578.2899479866028 and batch: 50, loss is 4.021781830787659 and perplexity is 55.800444247910384
At time: 579.5265290737152 and batch: 100, loss is 4.018119225502014 and perplexity is 55.59644306182865
At time: 580.7372777462006 and batch: 150, loss is 3.9443000030517577 and perplexity is 51.64017749077106
At time: 581.9488320350647 and batch: 200, loss is 3.998777027130127 and perplexity is 54.53141879047541
At time: 583.1589584350586 and batch: 250, loss is 3.995931377410889 and perplexity is 54.37646205476674
At time: 584.3698086738586 and batch: 300, loss is 3.999429397583008 and perplexity is 54.56700508330889
At time: 585.5812537670135 and batch: 350, loss is 3.9838735246658326 and perplexity is 53.724735788473886
At time: 586.7923195362091 and batch: 400, loss is 3.991995759010315 and perplexity is 54.16287761924881
At time: 588.0036933422089 and batch: 450, loss is 3.9167164516448976 and perplexity is 50.23522388436159
At time: 589.2154030799866 and batch: 500, loss is 3.9403993034362794 and perplexity is 51.4391370244346
At time: 590.425544500351 and batch: 550, loss is 3.9440214681625365 and perplexity is 51.625795902634316
At time: 591.6616675853729 and batch: 600, loss is 3.9489337825775146 and perplexity is 51.88002195195125
At time: 592.8734200000763 and batch: 650, loss is 3.9648362445831298 and perplexity is 52.71163686678638
At time: 594.0838351249695 and batch: 700, loss is 3.951424684524536 and perplexity is 52.0094110805585
At time: 595.294665813446 and batch: 750, loss is 3.924855899810791 and perplexity is 50.645779466583235
At time: 596.5045013427734 and batch: 800, loss is 3.955808525085449 and perplexity is 52.23791253743403
At time: 597.7149245738983 and batch: 850, loss is 3.991219148635864 and perplexity is 54.120830495809045
At time: 598.9257099628448 and batch: 900, loss is 3.926280097961426 and perplexity is 50.71796047986918
At time: 600.1366255283356 and batch: 950, loss is 3.920445485115051 and perplexity is 50.42290242790903
At time: 601.3456127643585 and batch: 1000, loss is 3.9041360569000245 and perplexity is 49.60720359224106
At time: 602.5573449134827 and batch: 1050, loss is 3.9116540718078614 and perplexity is 49.98155672136774
At time: 603.7688772678375 and batch: 1100, loss is 3.8487776374816893 and perplexity is 46.93565576597955
At time: 604.9800584316254 and batch: 1150, loss is 3.863742594718933 and perplexity is 47.64332778126635
At time: 606.1909804344177 and batch: 1200, loss is 3.8857327127456665 and perplexity is 48.70261441557562
At time: 607.4011719226837 and batch: 1250, loss is 3.907218794822693 and perplexity is 49.76036555779718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.572039750370666 and perplexity of 96.74123663298562
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 610.415346622467 and batch: 50, loss is 4.02217725276947 and perplexity is 55.82251333317401
At time: 611.625408411026 and batch: 100, loss is 4.023336181640625 and perplexity is 55.88724515808141
At time: 612.836181640625 and batch: 150, loss is 3.952759289741516 and perplexity is 52.07886945136092
At time: 614.046329498291 and batch: 200, loss is 4.00853440284729 and perplexity is 55.06610666511143
At time: 615.2534070014954 and batch: 250, loss is 4.007028384208679 and perplexity is 54.98323849828266
At time: 616.4605669975281 and batch: 300, loss is 4.009862685203553 and perplexity is 55.139298602040746
At time: 617.6672458648682 and batch: 350, loss is 3.988944025039673 and perplexity is 53.997838880917456
At time: 618.8737850189209 and batch: 400, loss is 3.9962208795547487 and perplexity is 54.392206436013204
At time: 620.0792636871338 and batch: 450, loss is 3.9201081657409667 and perplexity is 50.40589667436884
At time: 621.3030064105988 and batch: 500, loss is 3.943432650566101 and perplexity is 51.59540667331135
At time: 622.5485506057739 and batch: 550, loss is 3.948229055404663 and perplexity is 51.843473570584926
At time: 623.7737717628479 and batch: 600, loss is 3.953690629005432 and perplexity is 52.127395140733675
At time: 624.984869480133 and batch: 650, loss is 3.9712503385543823 and perplexity is 53.05082087522828
At time: 626.1893036365509 and batch: 700, loss is 3.9583080768585206 and perplexity is 52.36864722533243
At time: 627.3951125144958 and batch: 750, loss is 3.928673257827759 and perplexity is 50.83948201962947
At time: 628.6010684967041 and batch: 800, loss is 3.958133339881897 and perplexity is 52.35949728568608
At time: 629.807460308075 and batch: 850, loss is 3.9882956600189208 and perplexity is 53.962839918268486
At time: 631.0120918750763 and batch: 900, loss is 3.9211754846572875 and perplexity is 50.45972456203495
At time: 632.216409444809 and batch: 950, loss is 3.9140341472625733 and perplexity is 50.10065827682948
At time: 633.4311747550964 and batch: 1000, loss is 3.8947521924972532 and perplexity is 49.14387363303172
At time: 634.6371424198151 and batch: 1050, loss is 3.899352011680603 and perplexity is 49.37044726508936
At time: 635.8430867195129 and batch: 1100, loss is 3.836638693809509 and perplexity is 46.36935061152992
At time: 637.0492746829987 and batch: 1150, loss is 3.8521759700775147 and perplexity is 47.095430064213396
At time: 638.255937576294 and batch: 1200, loss is 3.8743757820129394 and perplexity is 48.15263116935581
At time: 639.4626581668854 and batch: 1250, loss is 3.899317407608032 and perplexity is 49.36873887610812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570848451043568 and perplexity of 96.62605748292583
Finished 20 epochs...
Completing Train Step...
At time: 642.4175808429718 and batch: 50, loss is 4.020325531959534 and perplexity is 55.71924126859642
At time: 643.6511120796204 and batch: 100, loss is 4.01861846446991 and perplexity is 55.62420590225078
At time: 644.8590068817139 and batch: 150, loss is 3.9472298622131348 and perplexity is 51.7916977960764
At time: 646.0680527687073 and batch: 200, loss is 4.002585344314575 and perplexity is 54.73948767442728
At time: 647.276909828186 and batch: 250, loss is 4.000965051651001 and perplexity is 54.65086550045842
At time: 648.4923079013824 and batch: 300, loss is 4.003827338218689 and perplexity is 54.80751602108441
At time: 649.7003614902496 and batch: 350, loss is 3.9843082857131957 and perplexity is 53.748098289059214
At time: 650.9104650020599 and batch: 400, loss is 3.9920105028152464 and perplexity is 54.163676192037954
At time: 652.1189455986023 and batch: 450, loss is 3.9162032413482666 and perplexity is 50.2094492646764
At time: 653.3719980716705 and batch: 500, loss is 3.940314793586731 and perplexity is 51.43479009438555
At time: 654.5819578170776 and batch: 550, loss is 3.9446423196792604 and perplexity is 51.657857808134
At time: 655.7915623188019 and batch: 600, loss is 3.9498289155960085 and perplexity is 51.926482263577874
At time: 657.0002958774567 and batch: 650, loss is 3.9675392866134644 and perplexity is 52.854311377454366
At time: 658.2088143825531 and batch: 700, loss is 3.9546830892562865 and perplexity is 52.179155189038156
At time: 659.4178302288055 and batch: 750, loss is 3.925692958831787 and perplexity is 50.688190721046325
At time: 660.6270880699158 and batch: 800, loss is 3.956021089553833 and perplexity is 52.24901764177552
At time: 661.8370995521545 and batch: 850, loss is 3.9874142932891847 and perplexity is 53.9152998197305
At time: 663.0466468334198 and batch: 900, loss is 3.9212031745910645 and perplexity is 50.46112180781121
At time: 664.2545857429504 and batch: 950, loss is 3.9147431087493896 and perplexity is 50.136190307944425
At time: 665.4657397270203 and batch: 1000, loss is 3.8964627742767335 and perplexity is 49.22801018855497
At time: 666.6757004261017 and batch: 1050, loss is 3.901430106163025 and perplexity is 49.47315039558997
At time: 667.897745847702 and batch: 1100, loss is 3.8391727304458616 and perplexity is 46.487001247337886
At time: 669.1209590435028 and batch: 1150, loss is 3.8556536483764647 and perplexity is 47.25949794161942
At time: 670.3310670852661 and batch: 1200, loss is 3.8776207971572876 and perplexity is 48.309140987767115
At time: 671.539603471756 and batch: 1250, loss is 3.9024239873886106 and perplexity is 49.5223452738221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570590499543796 and perplexity of 96.60113586090415
Finished 21 epochs...
Completing Train Step...
At time: 674.5158417224884 and batch: 50, loss is 4.019480810165406 and perplexity is 55.67219388491729
At time: 675.7480535507202 and batch: 100, loss is 4.016679277420044 and perplexity is 55.516444680857774
At time: 676.9533472061157 and batch: 150, loss is 3.944983501434326 and perplexity is 51.67548553368209
At time: 678.1579301357269 and batch: 200, loss is 3.9999332571029664 and perplexity is 54.594506116042496
At time: 679.3627498149872 and batch: 250, loss is 3.9983163976669314 and perplexity is 54.50630579664713
At time: 680.5687596797943 and batch: 300, loss is 4.001173629760742 and perplexity is 54.66226566355113
At time: 681.7751977443695 and batch: 350, loss is 3.981936936378479 and perplexity is 53.62079377333442
At time: 683.0083158016205 and batch: 400, loss is 3.9898977661132813 and perplexity is 54.04936340436577
At time: 684.215175151825 and batch: 450, loss is 3.9142213010787965 and perplexity is 50.110035683702655
At time: 685.4217505455017 and batch: 500, loss is 3.9386698150634767 and perplexity is 51.350250521286604
At time: 686.6364674568176 and batch: 550, loss is 3.942730360031128 and perplexity is 51.559184428315284
At time: 687.8433134555817 and batch: 600, loss is 3.947815361022949 and perplexity is 51.822030652553416
At time: 689.0519437789917 and batch: 650, loss is 3.9656835699081423 and perplexity is 52.75631969940093
At time: 690.259060382843 and batch: 700, loss is 3.952842001914978 and perplexity is 52.08317718599327
At time: 691.466718673706 and batch: 750, loss is 3.924408826828003 and perplexity is 50.623142167530936
At time: 692.6762490272522 and batch: 800, loss is 3.9551536941528322 and perplexity is 52.20371673390608
At time: 693.8865921497345 and batch: 850, loss is 3.987069091796875 and perplexity is 53.89669138978808
At time: 695.0927014350891 and batch: 900, loss is 3.921342678070068 and perplexity is 50.46816180089818
At time: 696.2987139225006 and batch: 950, loss is 3.9152268886566164 and perplexity is 50.160451057398696
At time: 697.5045111179352 and batch: 1000, loss is 3.897293696403503 and perplexity is 49.268931830471885
At time: 698.7106218338013 and batch: 1050, loss is 3.902463450431824 and perplexity is 49.52429961483553
At time: 699.9161140918732 and batch: 1100, loss is 3.8404826641082765 and perplexity is 46.54793603668909
At time: 701.1341457366943 and batch: 1150, loss is 3.857402787208557 and perplexity is 47.34223370172562
At time: 702.3395798206329 and batch: 1200, loss is 3.8792797231674196 and perplexity is 48.389348789288384
At time: 703.5457210540771 and batch: 1250, loss is 3.9040455865859984 and perplexity is 49.60271581596245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570537483605155 and perplexity of 96.59601459676821
Finished 22 epochs...
Completing Train Step...
At time: 706.5237667560577 and batch: 50, loss is 4.018661170005799 and perplexity is 55.626581414495625
At time: 707.7284467220306 and batch: 100, loss is 4.0153735160827635 and perplexity is 55.44400076133799
At time: 708.933616399765 and batch: 150, loss is 3.943491563796997 and perplexity is 51.598446414957486
At time: 710.1377694606781 and batch: 200, loss is 3.998179054260254 and perplexity is 54.49882022898207
At time: 711.3439218997955 and batch: 250, loss is 3.9966226291656493 and perplexity is 54.41406287389863
At time: 712.5488426685333 and batch: 300, loss is 3.9994347953796385 and perplexity is 54.56729962570001
At time: 713.7792310714722 and batch: 350, loss is 3.9802663373947142 and perplexity is 53.53128971326035
At time: 714.9839150905609 and batch: 400, loss is 3.9883770322799683 and perplexity is 53.96723117522602
At time: 716.1892409324646 and batch: 450, loss is 3.9127971601486204 and perplexity is 50.038722722779106
At time: 717.3933436870575 and batch: 500, loss is 3.937458624839783 and perplexity is 51.288093249609524
At time: 718.5988142490387 and batch: 550, loss is 3.941443405151367 and perplexity is 51.492872763531466
At time: 719.8031177520752 and batch: 600, loss is 3.9465027236938477 and perplexity is 51.75405174623478
At time: 721.0100448131561 and batch: 650, loss is 3.9645032358169554 and perplexity is 52.69408635203016
At time: 722.2138838768005 and batch: 700, loss is 3.951716432571411 and perplexity is 52.024586938316155
At time: 723.417671918869 and batch: 750, loss is 3.9235985183715822 and perplexity is 50.58213842242628
At time: 724.6239924430847 and batch: 800, loss is 3.954589219093323 and perplexity is 52.174257353121185
At time: 725.8306005001068 and batch: 850, loss is 3.9867779350280763 and perplexity is 53.881001287524676
At time: 727.0360908508301 and batch: 900, loss is 3.9213600730895997 and perplexity is 50.469039703193964
At time: 728.241224527359 and batch: 950, loss is 3.9154348802566528 and perplexity is 50.17088509493114
At time: 729.4469044208527 and batch: 1000, loss is 3.8976939058303834 and perplexity is 49.288653667612095
At time: 730.6529588699341 and batch: 1050, loss is 3.9030155420303343 and perplexity is 49.55164911359454
At time: 731.8587939739227 and batch: 1100, loss is 3.841223330497742 and perplexity is 46.58242529935258
At time: 733.0652525424957 and batch: 1150, loss is 3.8584019231796263 and perplexity is 47.38955866846915
At time: 734.2716593742371 and batch: 1200, loss is 3.8802362632751466 and perplexity is 48.43565728662874
At time: 735.477703332901 and batch: 1250, loss is 3.9049040126800536 and perplexity is 49.64531436279106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570542829750228 and perplexity of 96.59653101445618
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 738.4490718841553 and batch: 50, loss is 4.018796291351318 and perplexity is 55.63409826085477
At time: 739.6879522800446 and batch: 100, loss is 4.016720371246338 and perplexity is 55.51872611086796
At time: 740.8993587493896 and batch: 150, loss is 3.9458044242858885 and perplexity is 51.71792453781306
At time: 742.1087713241577 and batch: 200, loss is 4.000566921234131 and perplexity is 54.629111659314695
At time: 743.3182382583618 and batch: 250, loss is 3.9992895126342773 and perplexity is 54.55937251445363
At time: 744.5544037818909 and batch: 300, loss is 4.002060203552246 and perplexity is 54.71074928465085
At time: 745.7631912231445 and batch: 350, loss is 3.9816993951797484 and perplexity is 53.6080581383835
At time: 746.9724152088165 and batch: 400, loss is 3.9899098443984986 and perplexity is 54.050016231935295
At time: 748.1816742420197 and batch: 450, loss is 3.9139487504959107 and perplexity is 50.096380025281995
At time: 749.3987379074097 and batch: 500, loss is 3.9378815603256228 and perplexity is 51.30978939195669
At time: 750.6190593242645 and batch: 550, loss is 3.9411947774887084 and perplexity is 51.480071802335154
At time: 751.831865310669 and batch: 600, loss is 3.946028642654419 and perplexity is 51.729521946604166
At time: 753.0425870418549 and batch: 650, loss is 3.9645430994033815 and perplexity is 52.69618696916438
At time: 754.2566950321198 and batch: 700, loss is 3.952452440261841 and perplexity is 52.062891528901666
At time: 755.4700064659119 and batch: 750, loss is 3.923327078819275 and perplexity is 50.56841029268115
At time: 756.6783123016357 and batch: 800, loss is 3.953763518333435 and perplexity is 52.13119481001202
At time: 757.8885338306427 and batch: 850, loss is 3.984470610618591 and perplexity is 53.756823652181886
At time: 759.0975344181061 and batch: 900, loss is 3.9184058046340944 and perplexity is 50.320160633869264
At time: 760.3130955696106 and batch: 950, loss is 3.912706289291382 and perplexity is 50.0341758677416
At time: 761.5236337184906 and batch: 1000, loss is 3.8941214227676393 and perplexity is 49.11288493954039
At time: 762.7327432632446 and batch: 1050, loss is 3.8986420917510984 and perplexity is 49.33541063871848
At time: 763.942165851593 and batch: 1100, loss is 3.8365007638931274 and perplexity is 46.36295533193773
At time: 765.1515996456146 and batch: 1150, loss is 3.8536957359313964 and perplexity is 47.167058506142354
At time: 766.3603229522705 and batch: 1200, loss is 3.876011824607849 and perplexity is 48.231475403682275
At time: 767.5700974464417 and batch: 1250, loss is 3.90192081451416 and perplexity is 49.497433241056314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570479567033531 and perplexity of 96.59042024877454
Finished 24 epochs...
Completing Train Step...
At time: 770.5800306797028 and batch: 50, loss is 4.01831280708313 and perplexity is 55.607206550954
At time: 771.7871406078339 and batch: 100, loss is 4.015796890258789 and perplexity is 55.46747928922639
At time: 772.9939241409302 and batch: 150, loss is 3.9447258281707764 and perplexity is 51.6621718580419
At time: 774.2008347511292 and batch: 200, loss is 3.9994031095504763 and perplexity is 54.5655706429585
At time: 775.4337689876556 and batch: 250, loss is 3.9981458473205564 and perplexity is 54.497010519992756
At time: 776.6409151554108 and batch: 300, loss is 4.000925493240357 and perplexity is 54.64870364183903
At time: 777.848001241684 and batch: 350, loss is 3.9807440757751467 and perplexity is 53.55686977471246
At time: 779.0558149814606 and batch: 400, loss is 3.98897732257843 and perplexity is 53.99963690598512
At time: 780.2629659175873 and batch: 450, loss is 3.9130963945388793 and perplexity is 50.053698269950004
At time: 781.4703979492188 and batch: 500, loss is 3.9372243976593015 and perplexity is 51.27608159091879
At time: 782.6753840446472 and batch: 550, loss is 3.9405978870391847 and perplexity is 51.44935300792508
At time: 783.8793878555298 and batch: 600, loss is 3.945551619529724 and perplexity is 51.70485165302429
At time: 785.0838792324066 and batch: 650, loss is 3.9640996932983397 and perplexity is 52.67282633765589
At time: 786.289253950119 and batch: 700, loss is 3.9519456386566163 and perplexity is 52.03651265689414
At time: 787.5029497146606 and batch: 750, loss is 3.9229591369628904 and perplexity is 50.54980748051013
At time: 788.7081837654114 and batch: 800, loss is 3.953588533401489 and perplexity is 52.12207343451083
At time: 789.9129114151001 and batch: 850, loss is 3.984534273147583 and perplexity is 53.76024605646444
At time: 791.1172869205475 and batch: 900, loss is 3.9186309003829956 and perplexity is 50.331488763021
At time: 792.3235585689545 and batch: 950, loss is 3.91295380115509 and perplexity is 50.0465614525861
At time: 793.5293552875519 and batch: 1000, loss is 3.8945892810821534 and perplexity is 49.13586818714164
At time: 794.7347183227539 and batch: 1050, loss is 3.8991388463974 and perplexity is 49.35992432131922
At time: 795.9403219223022 and batch: 1100, loss is 3.837178416252136 and perplexity is 46.394383945623424
At time: 797.1454169750214 and batch: 1150, loss is 3.8545558214187623 and perplexity is 47.20764365949403
At time: 798.3507995605469 and batch: 1200, loss is 3.8767149305343627 and perplexity is 48.26539916448419
At time: 799.5571036338806 and batch: 1250, loss is 3.902409009933472 and perplexity is 49.52160356067189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570476002936815 and perplexity of 96.59007599178845
Finished 25 epochs...
Completing Train Step...
At time: 802.5336306095123 and batch: 50, loss is 4.017921190261841 and perplexity is 55.58543409699163
At time: 803.7770802974701 and batch: 100, loss is 4.015155277252197 and perplexity is 55.4319020477025
At time: 804.98912358284 and batch: 150, loss is 3.9440088844299317 and perplexity is 51.62514626151063
At time: 806.2256343364716 and batch: 200, loss is 3.9985536909103394 and perplexity is 54.51924130943
At time: 807.4359531402588 and batch: 250, loss is 3.997339973449707 and perplexity is 54.453110494492854
At time: 808.6466810703278 and batch: 300, loss is 4.0001042413711545 and perplexity is 54.60384171581544
At time: 809.8584356307983 and batch: 350, loss is 3.9800594663619995 and perplexity is 53.520216785448305
At time: 811.0707015991211 and batch: 400, loss is 3.98833927154541 and perplexity is 53.96519337140949
At time: 812.281947851181 and batch: 450, loss is 3.9125177526473998 and perplexity is 50.02474348134226
At time: 813.4928693771362 and batch: 500, loss is 3.9367762994766236 and perplexity is 51.253110019087224
At time: 814.7060494422913 and batch: 550, loss is 3.940161428451538 and perplexity is 51.42690239571356
At time: 815.9237248897552 and batch: 600, loss is 3.9451564502716066 and perplexity is 51.68442352170608
At time: 817.1361086368561 and batch: 650, loss is 3.9637426280975343 and perplexity is 52.65402206171872
At time: 818.3473420143127 and batch: 700, loss is 3.951566915512085 and perplexity is 52.01680895654946
At time: 819.5599095821381 and batch: 750, loss is 3.9227097606658936 and perplexity is 50.53720312838539
At time: 820.7705874443054 and batch: 800, loss is 3.95345787525177 and perplexity is 52.11526370571932
At time: 821.981930732727 and batch: 850, loss is 3.984590406417847 and perplexity is 53.76326387958513
At time: 823.192430973053 and batch: 900, loss is 3.9188025045394896 and perplexity is 50.340126596818166
At time: 824.4191102981567 and batch: 950, loss is 3.913174629211426 and perplexity is 50.057614357828825
At time: 825.6333079338074 and batch: 1000, loss is 3.8949365997314453 and perplexity is 49.15293695449121
At time: 826.8432626724243 and batch: 1050, loss is 3.8995199537277223 and perplexity is 49.37873933534439
At time: 828.0547225475311 and batch: 1100, loss is 3.837687163352966 and perplexity is 46.41799295894988
At time: 829.2706971168518 and batch: 1150, loss is 3.8552021408081054 and perplexity is 47.238164737036
At time: 830.4828858375549 and batch: 1200, loss is 3.8772422885894775 and perplexity is 48.2908590241579
At time: 831.6921052932739 and batch: 1250, loss is 3.9027952909469605 and perplexity is 49.540736510994705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570487586251141 and perplexity of 96.59119483147931
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 834.6222131252289 and batch: 50, loss is 4.017838664054871 and perplexity is 55.580847031231976
At time: 835.8538763523102 and batch: 100, loss is 4.015263781547547 and perplexity is 55.43791697349081
At time: 837.0583746433258 and batch: 150, loss is 3.9443346786499025 and perplexity is 51.641968175860214
At time: 838.264839887619 and batch: 200, loss is 3.998815407752991 and perplexity is 54.533511780459115
At time: 839.4696435928345 and batch: 250, loss is 3.997675747871399 and perplexity is 54.47139752616542
At time: 840.6753520965576 and batch: 300, loss is 4.000381832122803 and perplexity is 54.61900134126878
At time: 841.8801431655884 and batch: 350, loss is 3.9802101755142214 and perplexity is 53.5282833797863
At time: 843.0876338481903 and batch: 400, loss is 3.988543438911438 and perplexity is 53.97621242762487
At time: 844.2935798168182 and batch: 450, loss is 3.9127065658569338 and perplexity is 50.034189705472976
At time: 845.5034177303314 and batch: 500, loss is 3.936794457435608 and perplexity is 51.254040679406195
At time: 846.7100546360016 and batch: 550, loss is 3.9398356342315672 and perplexity is 51.41015053713955
At time: 847.9156658649445 and batch: 600, loss is 3.944760880470276 and perplexity is 51.663982767700745
At time: 849.1201522350311 and batch: 650, loss is 3.963546271324158 and perplexity is 52.643684102838726
At time: 850.3251225948334 and batch: 700, loss is 3.9516383838653564 and perplexity is 52.02052664507497
At time: 851.529699087143 and batch: 750, loss is 3.9226008892059325 and perplexity is 50.531701368796185
At time: 852.734751701355 and batch: 800, loss is 3.9531218290328978 and perplexity is 52.09775351068769
At time: 853.941668510437 and batch: 850, loss is 3.9838988637924193 and perplexity is 53.72609714360257
At time: 855.1488275527954 and batch: 900, loss is 3.9179179286956787 and perplexity is 50.29561662598107
At time: 856.3559014797211 and batch: 950, loss is 3.9123861455917357 and perplexity is 50.01816030534804
At time: 857.5623645782471 and batch: 1000, loss is 3.8939670944213867 and perplexity is 49.10530601406454
At time: 858.7688674926758 and batch: 1050, loss is 3.8984532165527344 and perplexity is 49.3260932831841
At time: 859.9765832424164 and batch: 1100, loss is 3.836397171020508 and perplexity is 46.358152708974806
At time: 861.1833579540253 and batch: 1150, loss is 3.8539086532592775 and perplexity is 47.1771022594101
At time: 862.3911533355713 and batch: 1200, loss is 3.8761233425140382 and perplexity is 48.236854376752035
At time: 863.6023559570312 and batch: 1250, loss is 3.902023777961731 and perplexity is 49.502529929810535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570496496492929 and perplexity of 96.59205548621416
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 866.6231150627136 and batch: 50, loss is 4.017797183990479 and perplexity is 55.57854158193357
At time: 867.8309152126312 and batch: 100, loss is 4.015242624282837 and perplexity is 55.43674407121417
At time: 869.0399265289307 and batch: 150, loss is 3.9443244791030883 and perplexity is 51.64144145387439
At time: 870.2464134693146 and batch: 200, loss is 3.998806104660034 and perplexity is 54.53300445248962
At time: 871.4530186653137 and batch: 250, loss is 3.997689514160156 and perplexity is 54.47214740031424
At time: 872.659871339798 and batch: 300, loss is 4.000423154830933 and perplexity is 54.621258392952974
At time: 873.8707444667816 and batch: 350, loss is 3.980194339752197 and perplexity is 53.52743572534077
At time: 875.0816555023193 and batch: 400, loss is 3.9885612440109255 and perplexity is 53.97717348801298
At time: 876.2883214950562 and batch: 450, loss is 3.9127254819869997 and perplexity is 50.03513616766486
At time: 877.4954743385315 and batch: 500, loss is 3.936787781715393 and perplexity is 51.25369852291281
At time: 878.7019453048706 and batch: 550, loss is 3.939740195274353 and perplexity is 51.40524424011173
At time: 879.9094054698944 and batch: 600, loss is 3.9446502494812012 and perplexity is 51.65826744633928
At time: 881.1157944202423 and batch: 650, loss is 3.963454556465149 and perplexity is 52.63885611617598
At time: 882.3219785690308 and batch: 700, loss is 3.951604714393616 and perplexity is 52.018775170908945
At time: 883.5277345180511 and batch: 750, loss is 3.9225443696975706 and perplexity is 50.52884542258723
At time: 884.7345023155212 and batch: 800, loss is 3.953034267425537 and perplexity is 52.09319194736223
At time: 885.9411571025848 and batch: 850, loss is 3.983733334541321 and perplexity is 53.71720463898307
At time: 887.1488513946533 and batch: 900, loss is 3.9177078104019163 and perplexity is 50.28504970702225
At time: 888.3682384490967 and batch: 950, loss is 3.9121938943862915 and perplexity is 50.00854517802476
At time: 889.5786325931549 and batch: 1000, loss is 3.8937416648864747 and perplexity is 49.094237475402714
At time: 890.7848432064056 and batch: 1050, loss is 3.898212013244629 and perplexity is 49.3141971010652
At time: 891.9924275875092 and batch: 1100, loss is 3.8360822200775146 and perplexity is 46.343554464049866
At time: 893.2063941955566 and batch: 1150, loss is 3.8535887718200685 and perplexity is 47.162013593462405
At time: 894.415233373642 and batch: 1200, loss is 3.875848879814148 and perplexity is 48.22361697613504
At time: 895.6244704723358 and batch: 1250, loss is 3.9018384265899657 and perplexity is 49.49335541826262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570501397125913 and perplexity of 96.59252884958714
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 898.5829694271088 and batch: 50, loss is 4.017788844108582 and perplexity is 55.578078065393605
At time: 899.8193871974945 and batch: 100, loss is 4.015237574577331 and perplexity is 55.436464132689245
At time: 901.0267941951752 and batch: 150, loss is 3.944323110580444 and perplexity is 51.64137078144075
At time: 902.2342665195465 and batch: 200, loss is 3.998803596496582 and perplexity is 54.53286767497245
At time: 903.4426147937775 and batch: 250, loss is 3.9976915168762206 and perplexity is 54.47225649266815
At time: 904.6550669670105 and batch: 300, loss is 4.0004300785064695 and perplexity is 54.62163657413271
At time: 905.868869304657 and batch: 350, loss is 3.9801885890960693 and perplexity is 53.52712790834959
At time: 907.0755817890167 and batch: 400, loss is 3.9885647296905518 and perplexity is 53.9773616354748
At time: 908.282913684845 and batch: 450, loss is 3.9127289199829103 and perplexity is 50.0353081885541
At time: 909.4895043373108 and batch: 500, loss is 3.936785116195679 and perplexity is 51.25356190535105
At time: 910.6958138942719 and batch: 550, loss is 3.939716911315918 and perplexity is 51.40404733647588
At time: 911.9021015167236 and batch: 600, loss is 3.9446232080459596 and perplexity is 51.656870551532535
At time: 913.1087002754211 and batch: 650, loss is 3.963430461883545 and perplexity is 52.63758782024133
At time: 914.3213584423065 and batch: 700, loss is 3.951593599319458 and perplexity is 52.01819698157863
At time: 915.5285928249359 and batch: 750, loss is 3.9225285339355467 and perplexity is 50.52804526615133
At time: 916.7332322597504 and batch: 800, loss is 3.9530126667022705 and perplexity is 52.09206670889193
At time: 917.944365978241 and batch: 850, loss is 3.983692836761475 and perplexity is 53.71502925550505
At time: 919.159725189209 and batch: 900, loss is 3.9176563453674316 and perplexity is 50.28246185179762
At time: 920.3665904998779 and batch: 950, loss is 3.912146496772766 and perplexity is 50.00617494849949
At time: 921.5735883712769 and batch: 1000, loss is 3.8936870241165162 and perplexity is 49.09155500175341
At time: 922.7792847156525 and batch: 1050, loss is 3.8981532621383668 and perplexity is 49.31129992253813
At time: 923.9851820468903 and batch: 1100, loss is 3.83600519657135 and perplexity is 46.339985058462744
At time: 925.191232919693 and batch: 1150, loss is 3.8535098695755003 and perplexity is 47.15829255153274
At time: 926.3976452350616 and batch: 1200, loss is 3.8757814836502074 and perplexity is 48.22036699885873
At time: 927.6488356590271 and batch: 1250, loss is 3.901792893409729 and perplexity is 49.491101879695606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570505852246806 and perplexity of 96.59295918193918
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 930.6235458850861 and batch: 50, loss is 4.017787523269654 and perplexity is 55.578004655753034
At time: 931.8284981250763 and batch: 100, loss is 4.015237350463867 and perplexity is 55.43645170863259
At time: 933.0378427505493 and batch: 150, loss is 3.944323844909668 and perplexity is 51.64140870322239
At time: 934.2441742420197 and batch: 200, loss is 3.9988043308258057 and perplexity is 54.53290772006554
At time: 935.4506070613861 and batch: 250, loss is 3.9976929950714113 and perplexity is 54.47233701335524
At time: 936.6589546203613 and batch: 300, loss is 4.0004328346252445 and perplexity is 54.62178711805824
At time: 937.8651053905487 and batch: 350, loss is 3.9801882457733155 and perplexity is 53.52710953127179
At time: 939.071516752243 and batch: 400, loss is 3.988566589355469 and perplexity is 53.97746201537389
At time: 940.2841188907623 and batch: 450, loss is 3.9127307415008543 and perplexity is 50.035399328848804
At time: 941.4902946949005 and batch: 500, loss is 3.9367853307724 and perplexity is 51.25357290317349
At time: 942.6952981948853 and batch: 550, loss is 3.939711961746216 and perplexity is 51.40379290919027
At time: 943.9002249240875 and batch: 600, loss is 3.944617486000061 and perplexity is 51.65657496939394
At time: 945.1053910255432 and batch: 650, loss is 3.9634252166748047 and perplexity is 52.63731172582972
At time: 946.3099899291992 and batch: 700, loss is 3.9515915536880493 and perplexity is 52.0180905716299
At time: 947.5140135288239 and batch: 750, loss is 3.922525210380554 and perplexity is 50.52787733369329
At time: 948.7252194881439 and batch: 800, loss is 3.9530077457427977 and perplexity is 52.091810366573526
At time: 949.9303424358368 and batch: 850, loss is 3.983683342933655 and perplexity is 53.714519296686696
At time: 951.1349012851715 and batch: 900, loss is 3.9176439809799195 and perplexity is 50.28184014379775
At time: 952.3394527435303 and batch: 950, loss is 3.9121353816986084 and perplexity is 50.005619129245595
At time: 953.5441734790802 and batch: 1000, loss is 3.8936739063262937 and perplexity is 49.09091103325694
At time: 954.7485282421112 and batch: 1050, loss is 3.89813898563385 and perplexity is 49.31059593456732
At time: 955.9541120529175 and batch: 1100, loss is 3.8359862613677977 and perplexity is 46.339107609720415
At time: 957.1591145992279 and batch: 1150, loss is 3.8534905195236204 and perplexity is 47.15738004495385
At time: 958.3897793292999 and batch: 1200, loss is 3.8757649087905883 and perplexity is 48.2195677596686
At time: 959.6091513633728 and batch: 1250, loss is 3.901781983375549 and perplexity is 49.490561933027934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.570505852246806 and perplexity of 96.59295918193918
Annealing...
Model not improving. Stopping early with 96.59007599178845loss at 29 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f71ae834898>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 5.195045655249901, 'data': 'wikitext', 'dropout': 0.960824022462355, 'tune_wordvecs': True, 'lr': 14.61428609138214, 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.683605670928955 and batch: 50, loss is 9.634083480834962 and perplexity is 15276.690987137854
At time: 2.8953206539154053 and batch: 100, loss is 8.403173179626465 and perplexity is 4461.200501912169
At time: 4.108588457107544 and batch: 150, loss is 7.952045278549194 and perplexity is 2841.380425978836
At time: 5.320665121078491 and batch: 200, loss is 7.741426801681518 and perplexity is 2301.7541880691538
At time: 6.532341718673706 and batch: 250, loss is 7.672069234848022 and perplexity is 2147.5205752737575
At time: 7.743383407592773 and batch: 300, loss is 7.576714057922363 and perplexity is 1952.2035865490152
At time: 8.954444408416748 and batch: 350, loss is 7.52669204711914 and perplexity is 1856.952620672695
At time: 10.173656225204468 and batch: 400, loss is 7.439978380203247 and perplexity is 1702.7134084349934
At time: 11.38651990890503 and batch: 450, loss is 7.386903142929077 and perplexity is 1614.6978735085127
At time: 12.598295450210571 and batch: 500, loss is 7.3531527423858645 and perplexity is 1561.1105568546204
At time: 13.810118675231934 and batch: 550, loss is 7.3345178031921385 and perplexity is 1532.2887370357444
At time: 15.021857738494873 and batch: 600, loss is 7.314711380004883 and perplexity is 1502.238157495726
At time: 16.23413872718811 and batch: 650, loss is 7.274664001464844 and perplexity is 1443.2661753493683
At time: 17.44595742225647 and batch: 700, loss is 7.243550529479981 and perplexity is 1399.0525401720467
At time: 18.65850329399109 and batch: 750, loss is 7.177283811569214 and perplexity is 1309.3469907502065
At time: 19.870725393295288 and batch: 800, loss is 7.150122547149659 and perplexity is 1274.2621027941505
At time: 21.085171461105347 and batch: 850, loss is 7.173457956314087 and perplexity is 1304.347189042256
At time: 22.298450708389282 and batch: 900, loss is 7.148603830337525 and perplexity is 1272.3283283151657
At time: 23.510815620422363 and batch: 950, loss is 7.113882884979248 and perplexity is 1228.9100102776097
At time: 24.723747730255127 and batch: 1000, loss is 7.094376850128174 and perplexity is 1205.1711272152234
At time: 25.936389684677124 and batch: 1050, loss is 7.058657646179199 and perplexity is 1162.8831170010415
At time: 27.149888038635254 and batch: 1100, loss is 7.041337804794312 and perplexity is 1142.915582279972
At time: 28.36383819580078 and batch: 1150, loss is 7.057463130950928 and perplexity is 1161.494864718401
At time: 29.576554536819458 and batch: 1200, loss is 7.05654200553894 and perplexity is 1160.4254748793064
At time: 30.788103818893433 and batch: 1250, loss is 7.030206680297852 and perplexity is 1130.264189386743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.541110073562956 and perplexity of 693.0554937623103
Finished 1 epochs...
Completing Train Step...
At time: 33.76550054550171 and batch: 50, loss is 6.617383785247803 and perplexity is 747.9856439782137
At time: 34.995619773864746 and batch: 100, loss is 6.190495643615723 and perplexity is 488.087963955112
At time: 36.199697732925415 and batch: 150, loss is 5.9223173713684085 and perplexity is 373.2757308566223
At time: 37.4022421836853 and batch: 200, loss is 5.842342596054078 and perplexity is 344.5856208175112
At time: 38.611037492752075 and batch: 250, loss is 5.80176531791687 and perplexity is 330.8831586077525
At time: 39.8169105052948 and batch: 300, loss is 5.760715618133545 and perplexity is 317.5755104145784
At time: 41.02045965194702 and batch: 350, loss is 5.766052083969116 and perplexity is 319.2747712672545
At time: 42.2250497341156 and batch: 400, loss is 5.707579984664917 and perplexity is 301.1414189057664
At time: 43.428388595581055 and batch: 450, loss is 5.684168214797974 and perplexity is 294.1730544647683
At time: 44.63196396827698 and batch: 500, loss is 5.6673643684387205 and perplexity is 289.27111670483265
At time: 45.836132764816284 and batch: 550, loss is 5.665211191177368 and perplexity is 288.6489347883339
At time: 47.04050803184509 and batch: 600, loss is 5.668279285430908 and perplexity is 289.5358968723222
At time: 48.245436906814575 and batch: 650, loss is 5.647003040313721 and perplexity is 283.4407312071268
At time: 49.45081162452698 and batch: 700, loss is 5.662810459136963 and perplexity is 287.95679719303797
At time: 50.65491509437561 and batch: 750, loss is 5.60783917427063 and perplexity is 272.5546581265361
At time: 51.85829138755798 and batch: 800, loss is 5.626359586715698 and perplexity is 277.64951661956775
At time: 53.06326246261597 and batch: 850, loss is 5.657699279785156 and perplexity is 286.48875327085295
At time: 54.268256425857544 and batch: 900, loss is 5.645637674331665 and perplexity is 283.0539949531503
At time: 55.472986459732056 and batch: 950, loss is 5.621042022705078 and perplexity is 276.17701607014925
At time: 56.67744708061218 and batch: 1000, loss is 5.613929347991943 and perplexity is 274.2196281753271
At time: 57.88393187522888 and batch: 1050, loss is 5.630203065872192 and perplexity is 278.71871014481343
At time: 59.08895659446716 and batch: 1100, loss is 5.599331073760986 and perplexity is 270.2455725958275
At time: 60.29341197013855 and batch: 1150, loss is 5.625173854827881 and perplexity is 277.3204938390256
At time: 61.4988751411438 and batch: 1200, loss is 5.613621921539306 and perplexity is 274.1353387648548
At time: 62.70430064201355 and batch: 1250, loss is 5.595934028625488 and perplexity is 269.3290937294291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.370161293196852 and perplexity of 214.8975264181845
Finished 2 epochs...
Completing Train Step...
At time: 65.6856598854065 and batch: 50, loss is 5.596790418624878 and perplexity is 269.5598432635377
At time: 66.89310884475708 and batch: 100, loss is 5.599154272079468 and perplexity is 270.19779694770165
At time: 68.10114908218384 and batch: 150, loss is 5.505796279907226 and perplexity is 246.1143535883288
At time: 69.3104453086853 and batch: 200, loss is 5.545688457489014 and perplexity is 256.13085276130374
At time: 70.54014754295349 and batch: 250, loss is 5.547567749023438 and perplexity is 256.61264988142904
At time: 71.75187921524048 and batch: 300, loss is 5.529573783874512 and perplexity is 252.03646612169234
At time: 72.96016502380371 and batch: 350, loss is 5.548880290985108 and perplexity is 256.94968589086
At time: 74.16893315315247 and batch: 400, loss is 5.5225200366973874 and perplexity is 250.26491997488876
At time: 75.37743997573853 and batch: 450, loss is 5.527640247344971 and perplexity is 251.549615231337
At time: 76.59481501579285 and batch: 500, loss is 5.523152265548706 and perplexity is 250.42319470542333
At time: 77.80568385124207 and batch: 550, loss is 5.526534309387207 and perplexity is 251.27157074190885
At time: 79.01525807380676 and batch: 600, loss is 5.531238269805908 and perplexity is 252.4563266027469
At time: 80.22474646568298 and batch: 650, loss is 5.527811431884766 and perplexity is 251.5926803223897
At time: 81.43392872810364 and batch: 700, loss is 5.533040914535523 and perplexity is 252.91182609789072
At time: 82.64246249198914 and batch: 750, loss is 5.50272497177124 and perplexity is 245.35962017448625
At time: 83.8517394065857 and batch: 800, loss is 5.535901613235474 and perplexity is 253.63636648176623
At time: 85.06074476242065 and batch: 850, loss is 5.5409073162078855 and perplexity is 254.90917779532188
At time: 86.26988124847412 and batch: 900, loss is 5.520271301269531 and perplexity is 249.70277268014965
At time: 87.47796583175659 and batch: 950, loss is 5.528666896820068 and perplexity is 251.8080011249511
At time: 88.68660569190979 and batch: 1000, loss is 5.543549318313598 and perplexity is 255.58353881918202
At time: 89.89565229415894 and batch: 1050, loss is 5.530519742965698 and perplexity is 252.27499510967576
At time: 91.13062047958374 and batch: 1100, loss is 5.5113121509552006 and perplexity is 247.47563951318162
At time: 92.34036254882812 and batch: 1150, loss is 5.55619891166687 and perplexity is 258.8371013907451
At time: 93.54899144172668 and batch: 1200, loss is 5.558912849426269 and perplexity is 259.5405232635816
At time: 94.75785326957703 and batch: 1250, loss is 5.541156234741211 and perplexity is 254.97263731178722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.358861770072993 and perplexity of 212.48295429675855
Finished 3 epochs...
Completing Train Step...
At time: 97.71458625793457 and batch: 50, loss is 5.526544437408448 and perplexity is 251.27411563860176
At time: 98.94362306594849 and batch: 100, loss is 5.52816294670105 and perplexity is 251.68113442274424
At time: 100.14911341667175 and batch: 150, loss is 5.4321363735198975 and perplexity is 228.63717844852806
At time: 101.35523748397827 and batch: 200, loss is 5.466706867218018 and perplexity is 236.67949099879354
At time: 102.56019735336304 and batch: 250, loss is 5.4916666316986085 and perplexity is 242.66129703933714
At time: 103.76695322990417 and batch: 300, loss is 5.494273233413696 and perplexity is 243.2946434747384
At time: 104.97224760055542 and batch: 350, loss is 5.489965114593506 and perplexity is 242.24875576422752
At time: 106.17783832550049 and batch: 400, loss is 5.470328798294068 and perplexity is 237.53828210457354
At time: 107.38343858718872 and batch: 450, loss is 5.441535511016846 and perplexity is 230.79630177880313
At time: 108.58972215652466 and batch: 500, loss is 5.456628885269165 and perplexity is 234.30621834721006
At time: 109.79628801345825 and batch: 550, loss is 5.455686912536621 and perplexity is 234.08561219726278
At time: 111.01191926002502 and batch: 600, loss is 5.460194711685181 and perplexity is 235.14320503778964
At time: 112.21832060813904 and batch: 650, loss is 5.455348911285401 and perplexity is 234.00650433747828
At time: 113.42488527297974 and batch: 700, loss is 5.480828819274902 and perplexity is 240.04557935142833
At time: 114.63207578659058 and batch: 750, loss is 5.422329750061035 and perplexity is 226.40597788384528
At time: 115.83863663673401 and batch: 800, loss is 5.444086351394653 and perplexity is 231.38577781443303
At time: 117.0445806980133 and batch: 850, loss is 5.518540897369385 and perplexity is 249.2710596551008
At time: 118.25074982643127 and batch: 900, loss is 5.515997667312622 and perplexity is 248.63791146592513
At time: 119.45767498016357 and batch: 950, loss is 5.487478151321411 and perplexity is 241.64704053789853
At time: 120.66531920433044 and batch: 1000, loss is 5.486181192398071 and perplexity is 241.33383740207847
At time: 121.89776754379272 and batch: 1050, loss is 5.476357879638672 and perplexity is 238.9747456563773
At time: 123.10794758796692 and batch: 1100, loss is 5.454543886184692 and perplexity is 233.8181990331753
At time: 124.32247519493103 and batch: 1150, loss is 5.4825053310394285 and perplexity is 240.44835612489308
At time: 125.53469109535217 and batch: 1200, loss is 5.50769079208374 and perplexity is 246.5810621809809
At time: 126.74368596076965 and batch: 1250, loss is 5.487632808685302 and perplexity is 241.68441592229505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.281679473654197 and perplexity of 196.6999504825877
Finished 4 epochs...
Completing Train Step...
At time: 129.7195703983307 and batch: 50, loss is 5.447440576553345 and perplexity is 232.16320090890525
At time: 130.92891716957092 and batch: 100, loss is 5.492075366973877 and perplexity is 242.76050154418866
At time: 132.13827919960022 and batch: 150, loss is 5.389521169662475 and perplexity is 219.09844944816365
At time: 133.34657454490662 and batch: 200, loss is 5.4494537734985355 and perplexity is 232.63106194594747
At time: 134.5558021068573 and batch: 250, loss is 5.444910840988159 and perplexity is 231.5766316480145
At time: 135.7654731273651 and batch: 300, loss is 5.482788372039795 and perplexity is 240.5164225004802
At time: 136.9743857383728 and batch: 350, loss is 5.494088163375855 and perplexity is 243.24962109213942
At time: 138.18441247940063 and batch: 400, loss is 5.4362073993682865 and perplexity is 229.56986351626932
At time: 139.3934862613678 and batch: 450, loss is 5.428028793334961 and perplexity is 227.69995907440492
At time: 140.60265612602234 and batch: 500, loss is 5.431156244277954 and perplexity is 228.41319424879805
At time: 141.81179785728455 and batch: 550, loss is 5.4373619556427 and perplexity is 229.83506790984686
At time: 143.02212834358215 and batch: 600, loss is 5.452760000228881 and perplexity is 233.4014658444608
At time: 144.23650121688843 and batch: 650, loss is 5.439120044708252 and perplexity is 230.23949383384772
At time: 145.44891214370728 and batch: 700, loss is 5.476571683883667 and perplexity is 239.02584493387172
At time: 146.66798496246338 and batch: 750, loss is 5.428817729949952 and perplexity is 227.87967079060581
At time: 147.87889099121094 and batch: 800, loss is 5.451081056594848 and perplexity is 233.009926717244
At time: 149.08987379074097 and batch: 850, loss is 5.49758900642395 and perplexity is 244.1026921998573
At time: 150.3013768196106 and batch: 900, loss is 5.463367700576782 and perplexity is 235.890496762512
At time: 151.5695185661316 and batch: 950, loss is 5.44025429725647 and perplexity is 230.50079172731586
At time: 152.79061627388 and batch: 1000, loss is 5.4181473922729495 and perplexity is 225.46104448116833
At time: 154.01370096206665 and batch: 1050, loss is 5.446128759384155 and perplexity is 231.8588449092582
At time: 155.2338924407959 and batch: 1100, loss is 5.417328691482544 and perplexity is 225.27653488522785
At time: 156.44503164291382 and batch: 1150, loss is 5.446816921234131 and perplexity is 232.01845623381743
At time: 157.6558747291565 and batch: 1200, loss is 5.459957685470581 and perplexity is 235.08747653882918
At time: 158.8948838710785 and batch: 1250, loss is 5.462966337203979 and perplexity is 235.79583795467912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.325195758012089 and perplexity of 205.44857473309847
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 161.87962412834167 and batch: 50, loss is 5.401967420578003 and perplexity is 221.8424445284317
At time: 163.08743238449097 and batch: 100, loss is 5.4101274299621585 and perplexity is 223.66008686108776
At time: 164.2950620651245 and batch: 150, loss is 5.297861394882202 and perplexity is 199.90882645468935
At time: 165.50239324569702 and batch: 200, loss is 5.328428497314453 and perplexity is 206.11381110378989
At time: 166.71069264411926 and batch: 250, loss is 5.326687154769897 and perplexity is 205.75520867094573
At time: 167.9193983078003 and batch: 300, loss is 5.3271486186981205 and perplexity is 205.85017918884026
At time: 169.12777400016785 and batch: 350, loss is 5.359140357971191 and perplexity is 212.54215772269515
At time: 170.33651542663574 and batch: 400, loss is 5.289787769317627 and perplexity is 198.301335315425
At time: 171.54453372955322 and batch: 450, loss is 5.272796697616577 and perplexity is 194.96044612740565
At time: 172.75284147262573 and batch: 500, loss is 5.255032415390015 and perplexity is 191.52769419865967
At time: 173.96032118797302 and batch: 550, loss is 5.2336906051635745 and perplexity is 187.48345568365596
At time: 175.168137550354 and batch: 600, loss is 5.257931404113769 and perplexity is 192.0837364150648
At time: 176.3773536682129 and batch: 650, loss is 5.25093017578125 and perplexity is 190.74361105337996
At time: 177.58770513534546 and batch: 700, loss is 5.261126766204834 and perplexity is 192.69849517005144
At time: 178.79635047912598 and batch: 750, loss is 5.203774356842041 and perplexity is 181.95772080883216
At time: 180.00906085968018 and batch: 800, loss is 5.209928855895996 and perplexity is 183.0810325943524
At time: 181.21777725219727 and batch: 850, loss is 5.24012936592102 and perplexity is 188.69451146604888
At time: 182.46980547904968 and batch: 900, loss is 5.207135772705078 and perplexity is 182.57038551150535
At time: 183.6826503276825 and batch: 950, loss is 5.205682077407837 and perplexity is 182.30517661383942
At time: 184.89531230926514 and batch: 1000, loss is 5.157333660125732 and perplexity is 173.700692522206
At time: 186.10253190994263 and batch: 1050, loss is 5.147148904800415 and perplexity is 171.94057188269588
At time: 187.31049847602844 and batch: 1100, loss is 5.105503063201905 and perplexity is 164.92701837983364
At time: 188.5300624370575 and batch: 1150, loss is 5.164461269378662 and perplexity is 174.94318592904355
At time: 189.74144315719604 and batch: 1200, loss is 5.196161222457886 and perplexity is 180.57771198979012
At time: 190.9528796672821 and batch: 1250, loss is 5.215610065460205 and perplexity is 184.12411448560763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.127217759181113 and perplexity of 168.54752528296848
Finished 6 epochs...
Completing Train Step...
At time: 193.91426253318787 and batch: 50, loss is 5.256371231079101 and perplexity is 191.78428620692372
At time: 195.14391660690308 and batch: 100, loss is 5.280344743728637 and perplexity is 196.4375843052671
At time: 196.35131168365479 and batch: 150, loss is 5.189009399414062 and perplexity is 179.29085930437722
At time: 197.55907201766968 and batch: 200, loss is 5.227601194381714 and perplexity is 186.34526089212054
At time: 198.765718460083 and batch: 250, loss is 5.229979104995728 and perplexity is 186.78890052451723
At time: 199.97289204597473 and batch: 300, loss is 5.2345179748535156 and perplexity is 187.6386379999741
At time: 201.18076491355896 and batch: 350, loss is 5.270527019500732 and perplexity is 194.5184504530723
At time: 202.38754534721375 and batch: 400, loss is 5.2113143253326415 and perplexity is 183.33486156497915
At time: 203.59526467323303 and batch: 450, loss is 5.190985698699951 and perplexity is 179.64554206578873
At time: 204.80747079849243 and batch: 500, loss is 5.178732423782349 and perplexity is 177.45772717345017
At time: 206.0155007839203 and batch: 550, loss is 5.162798137664795 and perplexity is 174.6524741813728
At time: 207.2222638130188 and batch: 600, loss is 5.190687408447266 and perplexity is 179.59196354302503
At time: 208.4290051460266 and batch: 650, loss is 5.190194053649902 and perplexity is 179.5033828388975
At time: 209.63532161712646 and batch: 700, loss is 5.19944465637207 and perplexity is 181.17160143830444
At time: 210.84342193603516 and batch: 750, loss is 5.14538197517395 and perplexity is 171.6370332369297
At time: 212.05035042762756 and batch: 800, loss is 5.162460155487061 and perplexity is 174.5934547321252
At time: 213.30072736740112 and batch: 850, loss is 5.197119512557983 and perplexity is 180.75084076403442
At time: 214.507506608963 and batch: 900, loss is 5.165151700973511 and perplexity is 175.06401393885045
At time: 215.71394658088684 and batch: 950, loss is 5.1615042400360105 and perplexity is 174.4266378951932
At time: 216.9209680557251 and batch: 1000, loss is 5.126419839859008 and perplexity is 168.4130915966027
At time: 218.1276581287384 and batch: 1050, loss is 5.124460716247558 and perplexity is 168.083472520064
At time: 219.33500933647156 and batch: 1100, loss is 5.089935245513916 and perplexity is 162.37934692188136
At time: 220.54202914237976 and batch: 1150, loss is 5.146527814865112 and perplexity is 171.83381448043522
At time: 221.74890232086182 and batch: 1200, loss is 5.173271408081055 and perplexity is 176.49126906236373
At time: 222.95551133155823 and batch: 1250, loss is 5.197224292755127 and perplexity is 180.76978086502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.117984075615876 and perplexity of 166.9983739621694
Finished 7 epochs...
Completing Train Step...
At time: 225.9356427192688 and batch: 50, loss is 5.202307815551758 and perplexity is 181.69106787474098
At time: 227.1451997756958 and batch: 100, loss is 5.228339395523071 and perplexity is 186.482871962473
At time: 228.35461926460266 and batch: 150, loss is 5.142060489654541 and perplexity is 171.06788904178595
At time: 229.56437849998474 and batch: 200, loss is 5.18369610786438 and perplexity is 178.34076100651836
At time: 230.77393174171448 and batch: 250, loss is 5.185483121871949 and perplexity is 178.65974337264865
At time: 231.98409843444824 and batch: 300, loss is 5.18905421257019 and perplexity is 179.2988940736779
At time: 233.19497346878052 and batch: 350, loss is 5.222799673080444 and perplexity is 185.45266477705658
At time: 234.4041874408722 and batch: 400, loss is 5.170423984527588 and perplexity is 175.9894384670454
At time: 235.61415600776672 and batch: 450, loss is 5.146834259033203 and perplexity is 171.88648001987514
At time: 236.82381749153137 and batch: 500, loss is 5.137459697723389 and perplexity is 170.28264902628322
At time: 238.03320956230164 and batch: 550, loss is 5.120776214599609 and perplexity is 167.46530820162255
At time: 239.2432677745819 and batch: 600, loss is 5.150671558380127 and perplexity is 172.5473270203754
At time: 240.45330238342285 and batch: 650, loss is 5.1541182231903075 and perplexity is 173.14306588684434
At time: 241.66214537620544 and batch: 700, loss is 5.16536961555481 and perplexity is 175.1021670970622
At time: 242.87101006507874 and batch: 750, loss is 5.115326175689697 and perplexity is 166.5550983484114
At time: 244.10611534118652 and batch: 800, loss is 5.1321907901763915 and perplexity is 169.38780498713652
At time: 245.31600952148438 and batch: 850, loss is 5.167382040023804 and perplexity is 175.45490178962177
At time: 246.52739477157593 and batch: 900, loss is 5.13626633644104 and perplexity is 170.07956150837148
At time: 247.73791122436523 and batch: 950, loss is 5.132598295211792 and perplexity is 169.4568454368142
At time: 248.94956922531128 and batch: 1000, loss is 5.1042211055755615 and perplexity is 164.71572439473636
At time: 250.17071557044983 and batch: 1050, loss is 5.103628530502319 and perplexity is 164.61814687614688
At time: 251.38365387916565 and batch: 1100, loss is 5.070019855499267 and perplexity is 159.17748786041375
At time: 252.59440064430237 and batch: 1150, loss is 5.123339166641236 and perplexity is 167.8950642419943
At time: 253.8053789138794 and batch: 1200, loss is 5.148093614578247 and perplexity is 172.10308257279212
At time: 255.01550388336182 and batch: 1250, loss is 5.173037471771241 and perplexity is 176.44998617513625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.112493139113823 and perplexity of 166.0839094257687
Finished 8 epochs...
Completing Train Step...
At time: 257.9621272087097 and batch: 50, loss is 5.164084949493408 and perplexity is 174.8773637152696
At time: 259.19551277160645 and batch: 100, loss is 5.189190731048584 and perplexity is 179.32337335677377
At time: 260.4040608406067 and batch: 150, loss is 5.107640047073364 and perplexity is 165.2798416125883
At time: 261.6114671230316 and batch: 200, loss is 5.150989236831665 and perplexity is 172.60215029566587
At time: 262.8179728984833 and batch: 250, loss is 5.151818923950195 and perplexity is 172.7454155008259
At time: 264.0269989967346 and batch: 300, loss is 5.156100311279297 and perplexity is 173.48659103151203
At time: 265.23454546928406 and batch: 350, loss is 5.186913499832153 and perplexity is 178.91547718634786
At time: 266.44185066223145 and batch: 400, loss is 5.137256135940552 and perplexity is 170.2479895144569
At time: 267.64989280700684 and batch: 450, loss is 5.1151642227172855 and perplexity is 166.52812643931296
At time: 268.8616712093353 and batch: 500, loss is 5.108383111953735 and perplexity is 165.40270089888136
At time: 270.0713474750519 and batch: 550, loss is 5.090948400497436 and perplexity is 162.54394573440973
At time: 271.28013920783997 and batch: 600, loss is 5.118905868530273 and perplexity is 167.15238285124983
At time: 272.4888663291931 and batch: 650, loss is 5.1257960224151615 and perplexity is 168.3080653343057
At time: 273.7410011291504 and batch: 700, loss is 5.136392736434937 and perplexity is 170.10106092264235
At time: 274.9506072998047 and batch: 750, loss is 5.089190092086792 and perplexity is 162.25839446469388
At time: 276.1614694595337 and batch: 800, loss is 5.106787691116333 and perplexity is 165.139024376743
At time: 277.37159156799316 and batch: 850, loss is 5.1432481956481935 and perplexity is 171.27118810478163
At time: 278.58038902282715 and batch: 900, loss is 5.110632715225219 and perplexity is 165.77521019790777
At time: 279.7899193763733 and batch: 950, loss is 5.109338607788086 and perplexity is 165.56081801867555
At time: 280.9990930557251 and batch: 1000, loss is 5.085945329666138 and perplexity is 161.73275776817871
At time: 282.20934677124023 and batch: 1050, loss is 5.084750604629517 and perplexity is 161.53964697335206
At time: 283.42592883110046 and batch: 1100, loss is 5.051410465240479 and perplexity is 156.24268401877578
At time: 284.6358494758606 and batch: 1150, loss is 5.101762800216675 and perplexity is 164.3113001495787
At time: 285.8455002307892 and batch: 1200, loss is 5.1254193973541256 and perplexity is 168.24468823437755
At time: 287.056161403656 and batch: 1250, loss is 5.150745735168457 and perplexity is 172.5601265016351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.106826225336451 and perplexity of 165.1453880028663
Finished 9 epochs...
Completing Train Step...
At time: 290.0465030670166 and batch: 50, loss is 5.135883121490479 and perplexity is 170.01439696442773
At time: 291.25603437423706 and batch: 100, loss is 5.158239583969117 and perplexity is 173.8581234206096
At time: 292.46565079689026 and batch: 150, loss is 5.078328485488892 and perplexity is 160.50554423422727
At time: 293.6760573387146 and batch: 200, loss is 5.123328523635864 and perplexity is 167.89327734343277
At time: 294.88534593582153 and batch: 250, loss is 5.124573221206665 and perplexity is 168.10238380805333
At time: 296.09558176994324 and batch: 300, loss is 5.126910057067871 and perplexity is 168.4956708315284
At time: 297.3052957057953 and batch: 350, loss is 5.155836572647095 and perplexity is 173.44084194845348
At time: 298.5154483318329 and batch: 400, loss is 5.109210281372071 and perplexity is 165.5395735554087
At time: 299.7261641025543 and batch: 450, loss is 5.086558103561401 and perplexity is 161.8318937510172
At time: 300.9446425437927 and batch: 500, loss is 5.0813712310791015 and perplexity is 160.9946655296132
At time: 302.16133165359497 and batch: 550, loss is 5.064456853866577 and perplexity is 158.2944417064572
At time: 303.37786054611206 and batch: 600, loss is 5.093568134307861 and perplexity is 162.97032586209428
At time: 304.61312460899353 and batch: 650, loss is 5.102049884796142 and perplexity is 164.3584781617997
At time: 305.822918176651 and batch: 700, loss is 5.112336320877075 and perplexity is 166.05786648199793
At time: 307.03568840026855 and batch: 750, loss is 5.06896900177002 and perplexity is 159.01030346224258
At time: 308.25715041160583 and batch: 800, loss is 5.087127618789673 and perplexity is 161.92408572880413
At time: 309.48731422424316 and batch: 850, loss is 5.122070636749267 and perplexity is 167.6822193628771
At time: 310.70304012298584 and batch: 900, loss is 5.089096841812133 and perplexity is 162.24326453029065
At time: 311.91864347457886 and batch: 950, loss is 5.087050590515137 and perplexity is 161.9116134762389
At time: 313.1333222389221 and batch: 1000, loss is 5.066900415420532 and perplexity is 158.6817168910904
At time: 314.34539246559143 and batch: 1050, loss is 5.067017459869385 and perplexity is 158.70029079215178
At time: 315.557820558548 and batch: 1100, loss is 5.033919773101807 and perplexity is 153.53365182065116
At time: 316.7709164619446 and batch: 1150, loss is 5.082333192825318 and perplexity is 161.1496107529379
At time: 317.9833300113678 and batch: 1200, loss is 5.1045828056335445 and perplexity is 164.77531285772625
At time: 319.19477105140686 and batch: 1250, loss is 5.131416473388672 and perplexity is 169.2566959326217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.103078577640283 and perplexity of 164.5276395447934
Finished 10 epochs...
Completing Train Step...
At time: 322.1761531829834 and batch: 50, loss is 5.109871654510498 and perplexity is 165.64909319538694
At time: 323.4243564605713 and batch: 100, loss is 5.1302293682098385 and perplexity is 169.05588964500149
At time: 324.6339681148529 and batch: 150, loss is 5.054506540298462 and perplexity is 156.7271727154307
At time: 325.84315609931946 and batch: 200, loss is 5.100360078811645 and perplexity is 164.08097874791625
At time: 327.0519495010376 and batch: 250, loss is 5.101769037246704 and perplexity is 164.3123249672878
At time: 328.26146841049194 and batch: 300, loss is 5.102009420394897 and perplexity is 164.3518276289472
At time: 329.47082257270813 and batch: 350, loss is 5.131050605773925 and perplexity is 169.19478171589498
At time: 330.6803824901581 and batch: 400, loss is 5.08668004989624 and perplexity is 161.85162976066275
At time: 331.8900589942932 and batch: 450, loss is 5.062931728363037 and perplexity is 158.05320681984054
At time: 333.0984787940979 and batch: 500, loss is 5.05914626121521 and perplexity is 157.45603260261913
At time: 334.3079469203949 and batch: 550, loss is 5.041253662109375 and perplexity is 154.66378966934835
At time: 335.5408446788788 and batch: 600, loss is 5.0699739837646485 and perplexity is 159.17018628040267
At time: 336.74776005744934 and batch: 650, loss is 5.080357217788697 and perplexity is 160.83149754031783
At time: 337.9590435028076 and batch: 700, loss is 5.093153600692749 and perplexity is 162.9027831840811
At time: 339.16563987731934 and batch: 750, loss is 5.050395374298096 and perplexity is 156.0841639551668
At time: 340.3728847503662 and batch: 800, loss is 5.068110666275024 and perplexity is 158.873877832567
At time: 341.5906226634979 and batch: 850, loss is 5.103416700363159 and perplexity is 164.5832794842984
At time: 342.80928587913513 and batch: 900, loss is 5.070001020431518 and perplexity is 159.1744897698805
At time: 344.0180881023407 and batch: 950, loss is 5.0672839260101314 and perplexity is 158.74258468086887
At time: 345.22591495513916 and batch: 1000, loss is 5.050662984848023 and perplexity is 156.12593931363176
At time: 346.4472322463989 and batch: 1050, loss is 5.052077808380127 and perplexity is 156.34698630091128
At time: 347.65719866752625 and batch: 1100, loss is 5.017874546051026 and perplexity is 151.08982780271575
At time: 348.8649015426636 and batch: 1150, loss is 5.066060781478882 and perplexity is 158.5485382541453
At time: 350.0755875110626 and batch: 1200, loss is 5.086743488311767 and perplexity is 161.86189769729256
At time: 351.2854194641113 and batch: 1250, loss is 5.114842224121094 and perplexity is 166.47451324852233
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.102870523494525 and perplexity of 164.49341244796253
Finished 11 epochs...
Completing Train Step...
At time: 354.2311248779297 and batch: 50, loss is 5.086439619064331 and perplexity is 161.81272031637593
At time: 355.4660313129425 and batch: 100, loss is 5.1078691577911375 and perplexity is 165.31771333397666
At time: 356.67531847953796 and batch: 150, loss is 5.032236280441285 and perplexity is 153.2753964911383
At time: 357.88455295562744 and batch: 200, loss is 5.081467523574829 and perplexity is 161.01016885417076
At time: 359.09517097473145 and batch: 250, loss is 5.081968603134155 and perplexity is 161.09086797527874
At time: 360.30414295196533 and batch: 300, loss is 5.080836753845215 and perplexity is 160.9086405373583
At time: 361.5127577781677 and batch: 350, loss is 5.108930177688599 and perplexity is 165.49321180444724
At time: 362.7216513156891 and batch: 400, loss is 5.06754397392273 and perplexity is 158.7838707265979
At time: 363.9309387207031 and batch: 450, loss is 5.041779022216797 and perplexity is 154.74506520209536
At time: 365.14033603668213 and batch: 500, loss is 5.037995128631592 and perplexity is 154.16063275494673
At time: 366.3756914138794 and batch: 550, loss is 5.021937522888184 and perplexity is 151.70495104295387
At time: 367.584082365036 and batch: 600, loss is 5.05072660446167 and perplexity is 156.1358723015343
At time: 368.7931954860687 and batch: 650, loss is 5.062218828201294 and perplexity is 157.94057081701732
At time: 370.0082631111145 and batch: 700, loss is 5.075150022506714 and perplexity is 159.99619320894902
At time: 371.2170031070709 and batch: 750, loss is 5.033778467178345 and perplexity is 153.51195813895714
At time: 372.4268448352814 and batch: 800, loss is 5.052675943374634 and perplexity is 156.44053087805725
At time: 373.63562774658203 and batch: 850, loss is 5.087589845657349 and perplexity is 161.9989486922556
At time: 374.84311628341675 and batch: 900, loss is 5.052539844512939 and perplexity is 156.4192409486822
At time: 376.05117416381836 and batch: 950, loss is 5.050745401382446 and perplexity is 156.13880720273977
At time: 377.2597885131836 and batch: 1000, loss is 5.035783224105835 and perplexity is 153.82002099288232
At time: 378.46938848495483 and batch: 1050, loss is 5.0397247886657714 and perplexity is 154.42750897629887
At time: 379.6782293319702 and batch: 1100, loss is 5.004466142654419 and perplexity is 149.0774758052741
At time: 380.88705348968506 and batch: 1150, loss is 5.050763320922852 and perplexity is 156.1416051634733
At time: 382.09606981277466 and batch: 1200, loss is 5.071195154190064 and perplexity is 159.36467893466494
At time: 383.3058388233185 and batch: 1250, loss is 5.099443197250366 and perplexity is 163.9306048719973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.103256782476049 and perplexity of 164.55696177838178
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 386.3169596195221 and batch: 50, loss is 5.07569935798645 and perplexity is 160.08410893990578
At time: 387.52588176727295 and batch: 100, loss is 5.098256483078003 and perplexity is 163.7361814852229
At time: 388.73454761505127 and batch: 150, loss is 5.021180124282837 and perplexity is 151.5900934265996
At time: 389.94315123558044 and batch: 200, loss is 5.065253162384034 and perplexity is 158.4205431198053
At time: 391.1529965400696 and batch: 250, loss is 5.062336511611939 and perplexity is 157.95915889580306
At time: 392.36252188682556 and batch: 300, loss is 5.055934143066406 and perplexity is 156.95107664591953
At time: 393.5726387500763 and batch: 350, loss is 5.077717628479004 and perplexity is 160.40752823733317
At time: 394.7810027599335 and batch: 400, loss is 5.032233924865722 and perplexity is 153.27503543978528
At time: 396.01574540138245 and batch: 450, loss is 5.000331621170044 and perplexity is 148.46238420964087
At time: 397.22641468048096 and batch: 500, loss is 4.986479578018188 and perplexity is 146.42005473176104
At time: 398.4360737800598 and batch: 550, loss is 4.972429294586181 and perplexity is 144.37719643835655
At time: 399.6452190876007 and batch: 600, loss is 5.000351800918579 and perplexity is 148.46538017344983
At time: 400.8547079563141 and batch: 650, loss is 5.010364990234375 and perplexity is 149.9594599000376
At time: 402.06362318992615 and batch: 700, loss is 5.018321256637574 and perplexity is 151.15733630557378
At time: 403.2723591327667 and batch: 750, loss is 4.9725088596344 and perplexity is 144.38868427396184
At time: 404.4809682369232 and batch: 800, loss is 4.976085863113403 and perplexity is 144.90608792559
At time: 405.69096279144287 and batch: 850, loss is 5.009191865921021 and perplexity is 149.78364195992512
At time: 406.89987325668335 and batch: 900, loss is 4.9689430809021 and perplexity is 143.8747430193446
At time: 408.10938835144043 and batch: 950, loss is 4.95919093132019 and perplexity is 142.47847438059787
At time: 409.31896328926086 and batch: 1000, loss is 4.942129278182984 and perplexity is 140.06817637499256
At time: 410.52803349494934 and batch: 1050, loss is 4.934903535842896 and perplexity is 139.05972760564842
At time: 411.7359902858734 and batch: 1100, loss is 4.884690380096435 and perplexity is 132.24951191349967
At time: 412.9434492588043 and batch: 1150, loss is 4.928723211288452 and perplexity is 138.20294368511486
At time: 414.1511549949646 and batch: 1200, loss is 4.962539587020874 and perplexity is 142.9563854693153
At time: 415.35776472091675 and batch: 1250, loss is 5.009681129455567 and perplexity is 149.85694356444645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.041270457915146 and perplexity of 154.66638739413483
Finished 13 epochs...
Completing Train Step...
At time: 418.299889087677 and batch: 50, loss is 5.0319215774536135 and perplexity is 153.227167855172
At time: 419.5300316810608 and batch: 100, loss is 5.056837215423584 and perplexity is 157.09287884385313
At time: 420.73813700675964 and batch: 150, loss is 4.982417116165161 and perplexity is 145.82643543916504
At time: 421.9466407299042 and batch: 200, loss is 5.025779695510864 and perplexity is 152.28894884432964
At time: 423.15369510650635 and batch: 250, loss is 5.027771482467651 and perplexity is 152.5925782684837
At time: 424.36160588264465 and batch: 300, loss is 5.02451114654541 and perplexity is 152.09588533696942
At time: 425.569130897522 and batch: 350, loss is 5.047408752441406 and perplexity is 155.61869501546465
At time: 426.8045530319214 and batch: 400, loss is 5.0055898094177245 and perplexity is 149.24508335987773
At time: 428.01383876800537 and batch: 450, loss is 4.973710765838623 and perplexity is 144.56233026160157
At time: 429.22481060028076 and batch: 500, loss is 4.962904195785523 and perplexity is 143.00851812385378
At time: 430.44556045532227 and batch: 550, loss is 4.949787578582764 and perplexity is 141.1449785204458
At time: 431.65842461586 and batch: 600, loss is 4.980563106536866 and perplexity is 145.55632229735647
At time: 432.86845111846924 and batch: 650, loss is 4.99275845527649 and perplexity is 147.34230058793594
At time: 434.0780761241913 and batch: 700, loss is 5.000739717483521 and perplexity is 148.52298352566444
At time: 435.2878224849701 and batch: 750, loss is 4.956333103179932 and perplexity is 142.07187665706974
At time: 436.49753618240356 and batch: 800, loss is 4.965933961868286 and perplexity is 143.4424575170946
At time: 437.7057921886444 and batch: 850, loss is 5.001369380950928 and perplexity is 148.6165324715469
At time: 438.9149842262268 and batch: 900, loss is 4.9619745254516605 and perplexity is 142.8756291281141
At time: 440.12509536743164 and batch: 950, loss is 4.954098625183105 and perplexity is 141.75477458545055
At time: 441.33558773994446 and batch: 1000, loss is 4.940244169235229 and perplexity is 139.80438132173933
At time: 442.54588174819946 and batch: 1050, loss is 4.937386465072632 and perplexity is 139.40543207025584
At time: 443.7554955482483 and batch: 1100, loss is 4.891770219802856 and perplexity is 133.1891395407605
At time: 444.9659044742584 and batch: 1150, loss is 4.9387679767608645 and perplexity is 139.59815539817683
At time: 446.1765367984772 and batch: 1200, loss is 4.973457841873169 and perplexity is 144.52577160725133
At time: 447.3880617618561 and batch: 1250, loss is 5.013825645446778 and perplexity is 150.47931689066883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0391311088617705 and perplexity of 154.33585569208134
Finished 14 epochs...
Completing Train Step...
At time: 450.3931727409363 and batch: 50, loss is 5.019796705245971 and perplexity is 151.38052579889927
At time: 451.6028320789337 and batch: 100, loss is 5.044362926483155 and perplexity is 155.14542866394316
At time: 452.812486410141 and batch: 150, loss is 4.9700323009490965 and perplexity is 144.03153965121322
At time: 454.02284598350525 and batch: 200, loss is 5.012194528579712 and perplexity is 150.23406760823914
At time: 455.2336435317993 and batch: 250, loss is 5.015284938812256 and perplexity is 150.6990706630623
At time: 456.4447269439697 and batch: 300, loss is 5.0123526096344 and perplexity is 150.25781864534179
At time: 457.68052792549133 and batch: 350, loss is 5.03553318977356 and perplexity is 153.7815655144381
At time: 458.8905382156372 and batch: 400, loss is 4.993832063674927 and perplexity is 147.50057346563503
At time: 460.101891040802 and batch: 450, loss is 4.9621559715271 and perplexity is 142.90155570236178
At time: 461.31437253952026 and batch: 500, loss is 4.953134336471558 and perplexity is 141.61814794086223
At time: 462.527366399765 and batch: 550, loss is 4.941365032196045 and perplexity is 139.96117072782664
At time: 463.7397747039795 and batch: 600, loss is 4.972980995178222 and perplexity is 144.4568713994495
At time: 464.95144605636597 and batch: 650, loss is 4.986336393356323 and perplexity is 146.39909112660322
At time: 466.1641561985016 and batch: 700, loss is 4.994160900115967 and perplexity is 147.5490850050086
At time: 467.3758692741394 and batch: 750, loss is 4.9498693466186525 and perplexity is 141.15652013997635
At time: 468.5880045890808 and batch: 800, loss is 4.96218508720398 and perplexity is 142.90571643845428
At time: 469.800124168396 and batch: 850, loss is 4.998490152359008 and perplexity is 148.1892469235877
At time: 471.0104172229767 and batch: 900, loss is 4.959917640686035 and perplexity is 142.58205245338092
At time: 472.2206346988678 and batch: 950, loss is 4.952230157852173 and perplexity is 141.49015771111164
At time: 473.43114709854126 and batch: 1000, loss is 4.940183143615723 and perplexity is 139.79584993307878
At time: 474.64060616493225 and batch: 1050, loss is 4.9387178707122805 and perplexity is 139.59116086145596
At time: 475.8517174720764 and batch: 1100, loss is 4.894770851135254 and perplexity is 133.58939124959574
At time: 477.061842918396 and batch: 1150, loss is 4.942745170593262 and perplexity is 140.1544698727708
At time: 478.27249217033386 and batch: 1200, loss is 4.976990385055542 and perplexity is 145.03721795772339
At time: 479.48144912719727 and batch: 1250, loss is 5.013335971832276 and perplexity is 150.40564917769882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.038551052121351 and perplexity of 154.24635809804946
Finished 15 epochs...
Completing Train Step...
At time: 482.4444041252136 and batch: 50, loss is 5.011403875350952 and perplexity is 150.11533150334287
At time: 483.67728447914124 and batch: 100, loss is 5.035739479064941 and perplexity is 153.81329227694877
At time: 484.8863606452942 and batch: 150, loss is 4.962051172256469 and perplexity is 142.8865805082606
At time: 486.09560441970825 and batch: 200, loss is 5.004111537933349 and perplexity is 149.0246216002776
At time: 487.30292439460754 and batch: 250, loss is 5.007337245941162 and perplexity is 149.50610766476566
At time: 488.5361952781677 and batch: 300, loss is 5.004417610168457 and perplexity is 149.07024088033828
At time: 489.74573516845703 and batch: 350, loss is 5.027515382766723 and perplexity is 152.55350435844724
At time: 490.95583510398865 and batch: 400, loss is 4.985409708023071 and perplexity is 146.26348807643765
At time: 492.16234850883484 and batch: 450, loss is 4.953967142105102 and perplexity is 141.7361374566268
At time: 493.37097334861755 and batch: 500, loss is 4.946216049194336 and perplexity is 140.6417742210016
At time: 494.5783989429474 and batch: 550, loss is 4.935402145385742 and perplexity is 139.12908140165368
At time: 495.7877073287964 and batch: 600, loss is 4.968094844818115 and perplexity is 143.7527550153692
At time: 496.995525598526 and batch: 650, loss is 4.981842765808105 and perplexity is 145.74270402178888
At time: 498.20299768447876 and batch: 700, loss is 4.989645166397095 and perplexity is 146.88429476420754
At time: 499.4213206768036 and batch: 750, loss is 4.945817518234253 and perplexity is 140.58573528706023
At time: 500.6339795589447 and batch: 800, loss is 4.959550619125366 and perplexity is 142.5297313680367
At time: 501.8454146385193 and batch: 850, loss is 4.996289396286011 and perplexity is 147.86347713986677
At time: 503.0548701286316 and batch: 900, loss is 4.957885131835938 and perplexity is 142.29254748027873
At time: 504.2661039829254 and batch: 950, loss is 4.950406723022461 and perplexity is 141.2323947079186
At time: 505.47414803504944 and batch: 1000, loss is 4.939409818649292 and perplexity is 139.68778410251002
At time: 506.6820912361145 and batch: 1050, loss is 4.938458566665649 and perplexity is 139.55496900112135
At time: 507.89116740226746 and batch: 1100, loss is 4.895552673339844 and perplexity is 133.69387524060227
At time: 509.0997586250305 and batch: 1150, loss is 4.944034566879273 and perplexity is 140.3353010821842
At time: 510.3078725337982 and batch: 1200, loss is 4.977342557907105 and perplexity is 145.0883051235827
At time: 511.51529240608215 and batch: 1250, loss is 5.011774663925171 and perplexity is 150.17100287359554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.038269933992929 and perplexity of 154.20300274481843
Finished 16 epochs...
Completing Train Step...
At time: 514.464302778244 and batch: 50, loss is 5.004379415512085 and perplexity is 149.0645473024454
At time: 515.7008316516876 and batch: 100, loss is 5.028888635635376 and perplexity is 152.7631428062853
At time: 516.9122655391693 and batch: 150, loss is 4.956002998352051 and perplexity is 142.02498578455655
At time: 518.1664140224457 and batch: 200, loss is 4.997796058654785 and perplexity is 148.08642538828258
At time: 519.3767867088318 and batch: 250, loss is 5.001089105606079 and perplexity is 148.57488475834435
At time: 520.589186668396 and batch: 300, loss is 4.9980808544158934 and perplexity is 148.1286057806242
At time: 521.8062632083893 and batch: 350, loss is 5.02124475479126 and perplexity is 151.5998910880202
At time: 523.0173270702362 and batch: 400, loss is 4.978998107910156 and perplexity is 145.32870500968062
At time: 524.2283561229706 and batch: 450, loss is 4.947485723495483 and perplexity is 140.8204568777823
At time: 525.4388599395752 and batch: 500, loss is 4.9408704376220705 and perplexity is 139.89196380831174
At time: 526.6507074832916 and batch: 550, loss is 4.930920829772949 and perplexity is 138.5069950007963
At time: 527.8660995960236 and batch: 600, loss is 4.963680553436279 and perplexity is 143.11958699002582
At time: 529.0781092643738 and batch: 650, loss is 4.978303251266479 and perplexity is 145.2277574695838
At time: 530.2904596328735 and batch: 700, loss is 4.986202020645141 and perplexity is 146.37942040544763
At time: 531.5010969638824 and batch: 750, loss is 4.942435178756714 and perplexity is 140.11102986462637
At time: 532.7129590511322 and batch: 800, loss is 4.956949605941772 and perplexity is 142.1594913659886
At time: 533.9229292869568 and batch: 850, loss is 4.9940039253234865 and perplexity is 147.525925335799
At time: 535.1323595046997 and batch: 900, loss is 4.955735063552856 and perplexity is 141.98693744597438
At time: 536.3426530361176 and batch: 950, loss is 4.948465032577515 and perplexity is 140.9584311788546
At time: 537.5535135269165 and batch: 1000, loss is 4.938241720199585 and perplexity is 139.52471028013866
At time: 538.7633738517761 and batch: 1050, loss is 4.9374777126312255 and perplexity is 139.41815305595753
At time: 539.9730937480927 and batch: 1100, loss is 4.8954292583465575 and perplexity is 133.67737643000825
At time: 541.1834151744843 and batch: 1150, loss is 4.943587293624878 and perplexity is 140.27254689054467
At time: 542.3941702842712 and batch: 1200, loss is 4.976567974090576 and perplexity is 144.97596558427782
At time: 543.6044971942902 and batch: 1250, loss is 5.009496479034424 and perplexity is 149.82927497129342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.038024011319571 and perplexity of 154.16508539270302
Finished 17 epochs...
Completing Train Step...
At time: 546.5679171085358 and batch: 50, loss is 4.998623428344726 and perplexity is 148.20899830770756
At time: 547.775550365448 and batch: 100, loss is 5.023041257858276 and perplexity is 151.87248554239102
At time: 549.0155944824219 and batch: 150, loss is 4.950746154785156 and perplexity is 141.2803416054936
At time: 550.2243015766144 and batch: 200, loss is 4.992607984542847 and perplexity is 147.32013155180724
At time: 551.4310617446899 and batch: 250, loss is 4.99579514503479 and perplexity is 147.79041348866832
At time: 552.642008304596 and batch: 300, loss is 4.992954607009888 and perplexity is 147.371204870319
At time: 553.8548355102539 and batch: 350, loss is 5.015824718475342 and perplexity is 150.7804369146005
At time: 555.0625352859497 and batch: 400, loss is 4.973743295669555 and perplexity is 144.56703292625215
At time: 556.2706882953644 and batch: 450, loss is 4.942472696304321 and perplexity is 140.11628658546866
At time: 557.4810931682587 and batch: 500, loss is 4.936400470733642 and perplexity is 139.2680468450676
At time: 558.6892414093018 and batch: 550, loss is 4.926975984573364 and perplexity is 137.96168264005283
At time: 559.8992712497711 and batch: 600, loss is 4.960069894790649 and perplexity is 142.60376280881476
At time: 561.1081020832062 and batch: 650, loss is 4.9750617980957035 and perplexity is 144.75777062633432
At time: 562.3154065608978 and batch: 700, loss is 4.983161754608155 and perplexity is 145.9350638484231
At time: 563.5225710868835 and batch: 750, loss is 4.939745645523072 and perplexity is 139.73470289220467
At time: 564.7379958629608 and batch: 800, loss is 4.954474964141846 and perplexity is 141.80813246941
At time: 565.9458525180817 and batch: 850, loss is 4.9918410587310795 and perplexity is 147.2071912542776
At time: 567.1533644199371 and batch: 900, loss is 4.953539543151855 and perplexity is 141.67554418834536
At time: 568.3625569343567 and batch: 950, loss is 4.946310844421387 and perplexity is 140.6551070218546
At time: 569.570131778717 and batch: 1000, loss is 4.936362190246582 and perplexity is 139.26271569844252
At time: 570.7769525051117 and batch: 1050, loss is 4.936073560714721 and perplexity is 139.22252616622677
At time: 571.9842858314514 and batch: 1100, loss is 4.894414854049683 and perplexity is 133.54184227979712
At time: 573.1922960281372 and batch: 1150, loss is 4.94269588470459 and perplexity is 140.14756240539356
At time: 574.4000568389893 and batch: 1200, loss is 4.975183839797974 and perplexity is 144.7754381891462
At time: 575.6064486503601 and batch: 1250, loss is 5.00728777885437 and perplexity is 149.498712216079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.037808828980383 and perplexity of 154.1319153579379
Finished 18 epochs...
Completing Train Step...
At time: 578.5659673213959 and batch: 50, loss is 4.993475227355957 and perplexity is 147.44794929361993
At time: 579.800044298172 and batch: 100, loss is 5.017872018814087 and perplexity is 151.08944596340436
At time: 581.0084059238434 and batch: 150, loss is 4.946079797744751 and perplexity is 140.62261288080163
At time: 582.2172784805298 and batch: 200, loss is 4.987748355865478 and perplexity is 146.60594715676172
At time: 583.4276976585388 and batch: 250, loss is 4.9911033725738525 and perplexity is 147.09863859086633
At time: 584.6382474899292 and batch: 300, loss is 4.988361015319824 and perplexity is 146.69579419635937
At time: 585.8465623855591 and batch: 350, loss is 5.010871067047119 and perplexity is 150.03537011212347
At time: 587.0557193756104 and batch: 400, loss is 4.968759336471558 and perplexity is 143.84830926522108
At time: 588.265216588974 and batch: 450, loss is 4.937234706878662 and perplexity is 139.38427775886595
At time: 589.4755129814148 and batch: 500, loss is 4.932058563232422 and perplexity is 138.66466872170548
At time: 590.6844153404236 and batch: 550, loss is 4.923207864761353 and perplexity is 137.44280470146904
At time: 591.8969397544861 and batch: 600, loss is 4.956620979309082 and perplexity is 142.11278164648533
At time: 593.1083178520203 and batch: 650, loss is 4.971692609786987 and perplexity is 144.27087511984817
At time: 594.3175585269928 and batch: 700, loss is 4.9802446460723875 and perplexity is 145.50997574351416
At time: 595.5261180400848 and batch: 750, loss is 4.9368868255615235 and perplexity is 139.33579700599375
At time: 596.7353003025055 and batch: 800, loss is 4.9518145656585695 and perplexity is 141.4313677232713
At time: 597.9495723247528 and batch: 850, loss is 4.9898231506347654 and perplexity is 146.9104401801039
At time: 599.1597473621368 and batch: 900, loss is 4.951214942932129 and perplexity is 141.34658768151297
At time: 600.3697855472565 and batch: 950, loss is 4.9441672039031985 and perplexity is 140.35391597335607
At time: 601.5807690620422 and batch: 1000, loss is 4.934836635589599 and perplexity is 139.05042478583226
At time: 602.7918591499329 and batch: 1050, loss is 4.93415135383606 and perplexity is 138.955168709258
At time: 604.0023391246796 and batch: 1100, loss is 4.893045778274536 and perplexity is 133.359138474778
At time: 605.2125368118286 and batch: 1150, loss is 4.941144905090332 and perplexity is 139.93036487112852
At time: 606.4222264289856 and batch: 1200, loss is 4.973250999450683 and perplexity is 144.49588063800726
At time: 607.6329188346863 and batch: 1250, loss is 5.004809761047364 and perplexity is 149.12871036999988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.037609685076414 and perplexity of 154.1012239826889
Finished 19 epochs...
Completing Train Step...
At time: 610.6523568630219 and batch: 50, loss is 4.988812465667724 and perplexity is 146.76203501478005
At time: 611.8619472980499 and batch: 100, loss is 5.013648138046265 and perplexity is 150.45260806887353
At time: 613.0708057880402 and batch: 150, loss is 4.941891746520996 and perplexity is 140.03490969936743
At time: 614.2786657810211 and batch: 200, loss is 4.983586387634277 and perplexity is 145.99704585507467
At time: 615.4879078865051 and batch: 250, loss is 4.986797943115234 and perplexity is 146.46667718778278
At time: 616.697921037674 and batch: 300, loss is 4.9839828586578365 and perplexity is 146.05494092938275
At time: 617.906896352768 and batch: 350, loss is 5.006299562454224 and perplexity is 149.35104811092862
At time: 619.1161971092224 and batch: 400, loss is 4.964239025115967 and perplexity is 143.19953754915193
At time: 620.3252584934235 and batch: 450, loss is 4.933146934509278 and perplexity is 138.81566952181802
At time: 621.5333518981934 and batch: 500, loss is 4.927929334640503 and perplexity is 138.0932711344438
At time: 622.7406425476074 and batch: 550, loss is 4.919564323425293 and perplexity is 136.94293735785425
At time: 623.9497797489166 and batch: 600, loss is 4.953370571136475 and perplexity is 141.6516070085274
At time: 625.1576662063599 and batch: 650, loss is 4.9687115097045895 and perplexity is 143.84142963017175
At time: 626.3675191402435 and batch: 700, loss is 4.977440204620361 and perplexity is 145.10247321143234
At time: 627.5744545459747 and batch: 750, loss is 4.934175119400025 and perplexity is 138.95847109644959
At time: 628.7900266647339 and batch: 800, loss is 4.949367580413818 and perplexity is 141.08571033504725
At time: 629.9969131946564 and batch: 850, loss is 4.987764692306518 and perplexity is 146.6083421957367
At time: 631.2047083377838 and batch: 900, loss is 4.949092388153076 and perplexity is 141.04688998123723
At time: 632.4214899539948 and batch: 950, loss is 4.941851348876953 and perplexity is 140.02925273319659
At time: 633.6326591968536 and batch: 1000, loss is 4.93258430480957 and perplexity is 138.73758967044157
At time: 634.8388469219208 and batch: 1050, loss is 4.9323413276672365 and perplexity is 138.70388370242495
At time: 636.0463833808899 and batch: 1100, loss is 4.8912238597869875 and perplexity is 133.1163901958438
At time: 637.2540714740753 and batch: 1150, loss is 4.9392109203338626 and perplexity is 139.66000320044486
At time: 638.4629106521606 and batch: 1200, loss is 4.971354055404663 and perplexity is 144.22203984999834
At time: 639.6733000278473 and batch: 1250, loss is 5.002298336029053 and perplexity is 148.75465469881908
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.037386483519617 and perplexity of 154.06683218788547
Finished 20 epochs...
Completing Train Step...
At time: 642.6477422714233 and batch: 50, loss is 4.984070672988891 and perplexity is 146.06776720947508
At time: 643.8811883926392 and batch: 100, loss is 5.009020175933838 and perplexity is 149.7579278158505
At time: 645.0878598690033 and batch: 150, loss is 4.937594566345215 and perplexity is 139.43444553683958
At time: 646.2953722476959 and batch: 200, loss is 4.979547138214111 and perplexity is 145.40851678030103
At time: 647.5021514892578 and batch: 250, loss is 4.98298336982727 and perplexity is 145.9090335758064
At time: 648.7096607685089 and batch: 300, loss is 4.979713869094849 and perplexity is 145.4327628916021
At time: 649.9174852371216 and batch: 350, loss is 5.0023590850830075 and perplexity is 148.76369167785452
At time: 651.1258587837219 and batch: 400, loss is 4.960082149505615 and perplexity is 142.605510387989
At time: 652.3330237865448 and batch: 450, loss is 4.928963136672974 and perplexity is 138.23610605760894
At time: 653.5408871173859 and batch: 500, loss is 4.924165468215943 and perplexity is 137.57448344400657
At time: 654.7478587627411 and batch: 550, loss is 4.915939998626709 and perplexity is 136.44751001112314
At time: 655.9569718837738 and batch: 600, loss is 4.950201921463012 and perplexity is 141.2034730549387
At time: 657.16468334198 and batch: 650, loss is 4.966052379608154 and perplexity is 143.45944465448423
At time: 658.3717186450958 and batch: 700, loss is 4.974444808959961 and perplexity is 144.66848420175302
At time: 659.5794179439545 and batch: 750, loss is 4.9317375183105465 and perplexity is 138.6201582792769
At time: 660.7870879173279 and batch: 800, loss is 4.947093200683594 and perplexity is 140.76519248305445
At time: 661.9944438934326 and batch: 850, loss is 4.985493640899659 and perplexity is 146.2757649069393
At time: 663.2009031772614 and batch: 900, loss is 4.946989870071411 and perplexity is 140.75064788100536
At time: 664.4086756706238 and batch: 950, loss is 4.939525947570801 and perplexity is 139.7040068361719
At time: 665.6163349151611 and batch: 1000, loss is 4.930792951583863 and perplexity is 138.48928410954227
At time: 666.8227632045746 and batch: 1050, loss is 4.930501270294189 and perplexity is 138.4488952671684
At time: 668.0304608345032 and batch: 1100, loss is 4.8896697235107425 and perplexity is 132.90966986227068
At time: 669.2396147251129 and batch: 1150, loss is 4.937726020812988 and perplexity is 139.4527760224526
At time: 670.4523103237152 and batch: 1200, loss is 4.969452981948852 and perplexity is 143.9481236082411
At time: 671.6844427585602 and batch: 1250, loss is 4.999786863327026 and perplexity is 148.38153018638067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.037261294622491 and perplexity of 154.0475459383175
Finished 21 epochs...
Completing Train Step...
At time: 674.6342208385468 and batch: 50, loss is 4.980078773498535 and perplexity is 145.4858416309654
At time: 675.8690099716187 and batch: 100, loss is 5.004972610473633 and perplexity is 149.15299787247315
At time: 677.0862810611725 and batch: 150, loss is 4.9339430618286135 and perplexity is 138.92622847233716
At time: 678.2976229190826 and batch: 200, loss is 4.975837898254395 and perplexity is 144.8701607624491
At time: 679.5119168758392 and batch: 250, loss is 4.979523620605469 and perplexity is 145.40509715992087
At time: 680.7247376441956 and batch: 300, loss is 4.976039409637451 and perplexity is 144.89935669046505
At time: 681.936589717865 and batch: 350, loss is 4.998342247009277 and perplexity is 148.16733056200854
At time: 683.1483037471771 and batch: 400, loss is 4.956230621337891 and perplexity is 142.05731761547912
At time: 684.360075712204 and batch: 450, loss is 4.924910316467285 and perplexity is 137.67699372999337
At time: 685.5730330944061 and batch: 500, loss is 4.920800695419311 and perplexity is 137.1123544800936
At time: 686.7848541736603 and batch: 550, loss is 4.912494678497314 and perplexity is 135.97821356075087
At time: 687.997172832489 and batch: 600, loss is 4.947129411697388 and perplexity is 140.77028982567055
At time: 689.217830657959 and batch: 650, loss is 4.963137035369873 and perplexity is 143.0418200446245
At time: 690.4294285774231 and batch: 700, loss is 4.971719579696655 and perplexity is 144.27476614478795
At time: 691.6408491134644 and batch: 750, loss is 4.929165887832641 and perplexity is 138.26413642991926
At time: 692.852588891983 and batch: 800, loss is 4.944714002609253 and perplexity is 140.43068229895098
At time: 694.065507888794 and batch: 850, loss is 4.983101816177368 and perplexity is 145.92631699183835
At time: 695.2785878181458 and batch: 900, loss is 4.944634857177735 and perplexity is 140.41956829181933
At time: 696.4906888008118 and batch: 950, loss is 4.937138061523438 and perplexity is 139.37080756675547
At time: 697.7033092975616 and batch: 1000, loss is 4.928494825363159 and perplexity is 138.17138368201051
At time: 698.9149825572968 and batch: 1050, loss is 4.92834243774414 and perplexity is 138.15032967806022
At time: 700.1262612342834 and batch: 1100, loss is 4.887914609909058 and perplexity is 132.67660288224212
At time: 701.3831861019135 and batch: 1150, loss is 4.936065807342529 and perplexity is 139.22144672634852
At time: 702.5965421199799 and batch: 1200, loss is 4.967298669815063 and perplexity is 143.63834821516693
At time: 703.8082728385925 and batch: 1250, loss is 4.997172164916992 and perplexity is 147.99406400966944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.037246147211451 and perplexity of 154.045212534492
Finished 22 epochs...
Completing Train Step...
At time: 706.797260761261 and batch: 50, loss is 4.976201963424683 and perplexity is 144.92291254415727
At time: 708.0068061351776 and batch: 100, loss is 5.001218023300171 and perplexity is 148.59403992458084
At time: 709.2158510684967 and batch: 150, loss is 4.930344076156616 and perplexity is 138.42713362292517
At time: 710.4279265403748 and batch: 200, loss is 4.972392568588257 and perplexity is 144.3718941391065
At time: 711.6373400688171 and batch: 250, loss is 4.9761488819122315 and perplexity is 144.91522002093777
At time: 712.8442883491516 and batch: 300, loss is 4.972440071105957 and perplexity is 144.37875233045273
At time: 714.0510630607605 and batch: 350, loss is 4.994723176956176 and perplexity is 147.6320717668818
At time: 715.2573008537292 and batch: 400, loss is 4.952518405914307 and perplexity is 141.53094785344018
At time: 716.4652762413025 and batch: 450, loss is 4.92121729850769 and perplexity is 137.1694878105595
At time: 717.6730620861053 and batch: 500, loss is 4.917107067108154 and perplexity is 136.60684655967532
At time: 718.8801484107971 and batch: 550, loss is 4.909258918762207 and perplexity is 135.53893182082086
At time: 720.0870895385742 and batch: 600, loss is 4.94419584274292 and perplexity is 140.35793560421834
At time: 721.2936842441559 and batch: 650, loss is 4.960358448028565 and perplexity is 142.6449175236897
At time: 722.5024890899658 and batch: 700, loss is 4.969172582626343 and perplexity is 143.9077663102488
At time: 723.7089042663574 and batch: 750, loss is 4.92665114402771 and perplexity is 137.91687436995022
At time: 724.918783903122 and batch: 800, loss is 4.94251277923584 and perplexity is 140.12190296954836
At time: 726.1272070407867 and batch: 850, loss is 4.980856637954712 and perplexity is 145.59905392224738
At time: 727.3346846103668 and batch: 900, loss is 4.942169771194458 and perplexity is 140.07384827210106
At time: 728.5421011447906 and batch: 950, loss is 4.934611425399781 and perplexity is 139.0191127393002
At time: 729.7527799606323 and batch: 1000, loss is 4.925941095352173 and perplexity is 137.8189814344571
At time: 730.9598577022552 and batch: 1050, loss is 4.92592209815979 and perplexity is 137.8163632856216
At time: 732.1933200359344 and batch: 1100, loss is 4.885470380783081 and perplexity is 132.3527068644351
At time: 733.4008550643921 and batch: 1150, loss is 4.933588418960571 and perplexity is 138.87696801167726
At time: 734.6090042591095 and batch: 1200, loss is 4.964664735794067 and perplexity is 143.2605120992233
At time: 735.8164746761322 and batch: 1250, loss is 4.994314842224121 and perplexity is 147.57180077062205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.036916022753194 and perplexity of 153.9943668353232
Finished 23 epochs...
Completing Train Step...
At time: 738.7587158679962 and batch: 50, loss is 4.971962022781372 and perplexity is 144.3097488046195
At time: 739.9942562580109 and batch: 100, loss is 4.997029781341553 and perplexity is 147.9729935857687
At time: 741.2046620845795 and batch: 150, loss is 4.92666220664978 and perplexity is 137.91840010064772
At time: 742.4146492481232 and batch: 200, loss is 4.9686376953125 and perplexity is 143.83081245434136
At time: 743.6339960098267 and batch: 250, loss is 4.9718908596038816 and perplexity is 144.2994796297496
At time: 744.85529088974 and batch: 300, loss is 4.968450965881348 and perplexity is 143.8039575159313
At time: 746.0669777393341 and batch: 350, loss is 4.990126008987427 and perplexity is 146.95493997223235
At time: 747.2788944244385 and batch: 400, loss is 4.94822847366333 and perplexity is 140.92509014914415
At time: 748.4902567863464 and batch: 450, loss is 4.916895866394043 and perplexity is 136.57799814263979
At time: 749.7044343948364 and batch: 500, loss is 4.9135653591156006 and perplexity is 136.1238807661308
At time: 750.9160671234131 and batch: 550, loss is 4.906224212646484 and perplexity is 135.12823448355726
At time: 752.1258850097656 and batch: 600, loss is 4.941184606552124 and perplexity is 139.93592042144397
At time: 753.3354289531708 and batch: 650, loss is 4.957446889877319 and perplexity is 142.2302025776481
At time: 754.5443506240845 and batch: 700, loss is 4.966141929626465 and perplexity is 143.4722920256125
At time: 755.7533175945282 and batch: 750, loss is 4.923816719055176 and perplexity is 137.5265128237055
At time: 756.9637897014618 and batch: 800, loss is 4.939796180725097 and perplexity is 139.74176459207604
At time: 758.1741805076599 and batch: 850, loss is 4.978391208648682 and perplexity is 145.24053188474778
At time: 759.3849489688873 and batch: 900, loss is 4.939421758651734 and perplexity is 139.68945198495055
At time: 760.5952150821686 and batch: 950, loss is 4.932101469039917 and perplexity is 138.67061836892435
At time: 761.8053259849548 and batch: 1000, loss is 4.923882255554199 and perplexity is 137.53552612522572
At time: 763.0589792728424 and batch: 1050, loss is 4.923994846343994 and perplexity is 137.55101223051716
At time: 764.2696619033813 and batch: 1100, loss is 4.883759708404541 and perplexity is 132.12648829267533
At time: 765.4804639816284 and batch: 1150, loss is 4.931740503311158 and perplexity is 138.62057206115165
At time: 766.6904919147491 and batch: 1200, loss is 4.962103929519653 and perplexity is 142.8941190120476
At time: 767.9007179737091 and batch: 1250, loss is 4.991785840988159 and perplexity is 147.19906302984808
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.037128086507756 and perplexity of 154.02702692182356
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 770.8823580741882 and batch: 50, loss is 4.972125768661499 and perplexity is 144.33338086622197
At time: 772.0902614593506 and batch: 100, loss is 5.001023550033569 and perplexity is 148.56514516595917
At time: 773.2979128360748 and batch: 150, loss is 4.929506139755249 and perplexity is 138.31118907258937
At time: 774.5043909549713 and batch: 200, loss is 4.971319627761841 and perplexity is 144.21707471059287
At time: 775.7121007442474 and batch: 250, loss is 4.9730839443206785 and perplexity is 144.4717438760232
At time: 776.9229860305786 and batch: 300, loss is 4.9692007923126225 and perplexity is 143.91182596045002
At time: 778.1310646533966 and batch: 350, loss is 4.988344249725341 and perplexity is 146.6933347747785
At time: 779.3391878604889 and batch: 400, loss is 4.94183648109436 and perplexity is 140.02717082418707
At time: 780.5474045276642 and batch: 450, loss is 4.910577917098999 and perplexity is 135.71782540069432
At time: 781.7550847530365 and batch: 500, loss is 4.903706474304199 and perplexity is 134.78844487662758
At time: 782.9630024433136 and batch: 550, loss is 4.896561336517334 and perplexity is 133.82879536265395
At time: 784.170557975769 and batch: 600, loss is 4.931391315460205 and perplexity is 138.57217589166476
At time: 785.3784010410309 and batch: 650, loss is 4.9468425941467284 and perplexity is 140.72992022556883
At time: 786.5863425731659 and batch: 700, loss is 4.955550699234009 and perplexity is 141.96076253390103
At time: 787.7933053970337 and batch: 750, loss is 4.908169527053833 and perplexity is 135.3913572301958
At time: 789.0072944164276 and batch: 800, loss is 4.920527610778809 and perplexity is 137.07491631418864
At time: 790.2303040027618 and batch: 850, loss is 4.953285369873047 and perplexity is 141.63953862677172
At time: 791.4444181919098 and batch: 900, loss is 4.912850551605224 and perplexity is 136.02661316176625
At time: 792.6778657436371 and batch: 950, loss is 4.905719594955444 and perplexity is 135.06006358744455
At time: 793.8865633010864 and batch: 1000, loss is 4.895837125778198 and perplexity is 133.73191019871928
At time: 795.0939197540283 and batch: 1050, loss is 4.8923283958435055 and perplexity is 133.26350327942473
At time: 796.3018755912781 and batch: 1100, loss is 4.84824465751648 and perplexity is 127.51635839635561
At time: 797.5102574825287 and batch: 1150, loss is 4.89173357963562 and perplexity is 133.18425955781592
At time: 798.7194163799286 and batch: 1200, loss is 4.929121723175049 and perplexity is 138.25803017651782
At time: 799.9278612136841 and batch: 1250, loss is 4.96873929977417 and perplexity is 143.84542704905377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0294314196510035 and perplexity of 152.8460827055671
Finished 25 epochs...
Completing Train Step...
At time: 802.9122648239136 and batch: 50, loss is 4.96412055015564 and perplexity is 143.18257299457917
At time: 804.1496651172638 and batch: 100, loss is 4.991759338378906 and perplexity is 147.1951619222932
At time: 805.3671288490295 and batch: 150, loss is 4.921275596618653 and perplexity is 137.177484765682
At time: 806.5787327289581 and batch: 200, loss is 4.9622864913940425 and perplexity is 142.92020841164603
At time: 807.7941224575043 and batch: 250, loss is 4.965181427001953 and perplexity is 143.33455267267163
At time: 809.0089025497437 and batch: 300, loss is 4.962072772979736 and perplexity is 142.88966699507978
At time: 810.2181389331818 and batch: 350, loss is 4.981371831893921 and perplexity is 145.67408499850092
At time: 811.4288561344147 and batch: 400, loss is 4.935779857635498 and perplexity is 139.18164208577855
At time: 812.6409513950348 and batch: 450, loss is 4.904894037246704 and perplexity is 134.94860972293554
At time: 813.8530042171478 and batch: 500, loss is 4.899044637680054 and perplexity is 134.16154555425234
At time: 815.0652680397034 and batch: 550, loss is 4.89187840461731 and perplexity is 133.20354936255683
At time: 816.2833125591278 and batch: 600, loss is 4.9271754455566406 and perplexity is 137.98920335749017
At time: 817.4940366744995 and batch: 650, loss is 4.943100881576538 and perplexity is 140.20433322500512
At time: 818.7065165042877 and batch: 700, loss is 4.951778268814087 and perplexity is 141.42623430407608
At time: 819.9193227291107 and batch: 750, loss is 4.9054975509643555 and perplexity is 135.03007764111166
At time: 821.1331813335419 and batch: 800, loss is 4.918460006713867 and perplexity is 136.79179245489962
At time: 822.3492891788483 and batch: 850, loss is 4.952547283172607 and perplexity is 141.53503493819042
At time: 823.609717130661 and batch: 900, loss is 4.912663364410401 and perplexity is 136.0011531045998
At time: 824.8222393989563 and batch: 950, loss is 4.906034755706787 and perplexity is 135.10263592677398
At time: 826.0403590202332 and batch: 1000, loss is 4.896770820617676 and perplexity is 133.8568333040993
At time: 827.2629554271698 and batch: 1050, loss is 4.894306564331055 and perplexity is 133.52738185424346
At time: 828.4821548461914 and batch: 1100, loss is 4.850844240188598 and perplexity is 127.84827895259583
At time: 829.6965556144714 and batch: 1150, loss is 4.8952889919281 and perplexity is 133.6586272981552
At time: 830.9070327281952 and batch: 1200, loss is 4.932934455871582 and perplexity is 138.78617729081265
At time: 832.1186587810516 and batch: 1250, loss is 4.970149955749512 and perplexity is 144.0484866501927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028947593521898 and perplexity of 152.77214966383366
Finished 26 epochs...
Completing Train Step...
At time: 835.0801212787628 and batch: 50, loss is 4.960924777984619 and perplexity is 142.72572449310627
At time: 836.3152966499329 and batch: 100, loss is 4.988092079162597 and perplexity is 146.65634769771953
At time: 837.5243961811066 and batch: 150, loss is 4.917776346206665 and perplexity is 136.6983052690785
At time: 838.7338600158691 and batch: 200, loss is 4.958294019699097 and perplexity is 142.35074107247743
At time: 839.9426681995392 and batch: 250, loss is 4.961441307067871 and perplexity is 142.79946552378914
At time: 841.1535837650299 and batch: 300, loss is 4.958710451126098 and perplexity is 142.4100327393168
At time: 842.3635811805725 and batch: 350, loss is 4.977822875976562 and perplexity is 145.15801039721163
At time: 843.5742380619049 and batch: 400, loss is 4.932694597244263 and perplexity is 138.75289222085198
At time: 844.7836785316467 and batch: 450, loss is 4.902028894424438 and perplexity is 134.56251605328788
At time: 845.9980869293213 and batch: 500, loss is 4.896749744415283 and perplexity is 133.85401214011867
At time: 847.2083878517151 and batch: 550, loss is 4.889634714126587 and perplexity is 132.90501685803042
At time: 848.4176230430603 and batch: 600, loss is 4.925183353424072 and perplexity is 137.71458976970158
At time: 849.6281425952911 and batch: 650, loss is 4.941195516586304 and perplexity is 139.93744713544703
At time: 850.837988615036 and batch: 700, loss is 4.950009841918945 and perplexity is 141.1763533608662
At time: 852.0467834472656 and batch: 750, loss is 4.904191179275513 and perplexity is 134.8537933420185
At time: 853.2575612068176 and batch: 800, loss is 4.917422666549682 and perplexity is 136.6499664081006
At time: 854.4926269054413 and batch: 850, loss is 4.95215521812439 and perplexity is 141.47955487450056
At time: 855.7096707820892 and batch: 900, loss is 4.91277494430542 and perplexity is 136.0163289456294
At time: 856.932379245758 and batch: 950, loss is 4.906408071517944 and perplexity is 135.15308129233492
At time: 858.1416511535645 and batch: 1000, loss is 4.897620096206665 and perplexity is 133.9705629321064
At time: 859.3509378433228 and batch: 1050, loss is 4.895145025253296 and perplexity is 133.639386295089
At time: 860.5628254413605 and batch: 1100, loss is 4.852441930770874 and perplexity is 128.05270420450356
At time: 861.774977684021 and batch: 1150, loss is 4.897181911468506 and perplexity is 133.9118719357555
At time: 862.9844555854797 and batch: 1200, loss is 4.9347232151031495 and perplexity is 139.0346545133649
At time: 864.1979501247406 and batch: 1250, loss is 4.970593385696411 and perplexity is 144.1123762272079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028721273380475 and perplexity of 152.73757816157743
Finished 27 epochs...
Completing Train Step...
At time: 867.173868894577 and batch: 50, loss is 4.958726921081543 and perplexity is 142.41237824552604
At time: 868.3815765380859 and batch: 100, loss is 4.9855145072937015 and perplexity is 146.2788171865337
At time: 869.5894010066986 and batch: 150, loss is 4.915407581329346 and perplexity is 136.37488233244287
At time: 870.7965252399445 and batch: 200, loss is 4.955569086074829 and perplexity is 141.96337276784126
At time: 872.0047655105591 and batch: 250, loss is 4.9589183902740475 and perplexity is 142.43964843920904
At time: 873.2124125957489 and batch: 300, loss is 4.956235990524292 and perplexity is 142.0580803497447
At time: 874.420337677002 and batch: 350, loss is 4.9751944160461425 and perplexity is 144.77696937820633
At time: 875.6281900405884 and batch: 400, loss is 4.930511016845703 and perplexity is 138.45024467303412
At time: 876.8360302448273 and batch: 450, loss is 4.899906196594238 and perplexity is 134.27718343703677
At time: 878.0450112819672 and batch: 500, loss is 4.895086679458618 and perplexity is 133.6315892263607
At time: 879.2524733543396 and batch: 550, loss is 4.888080959320068 and perplexity is 132.69867539281012
At time: 880.4594299793243 and batch: 600, loss is 4.923789196014404 and perplexity is 137.52272772797477
At time: 881.667319059372 and batch: 650, loss is 4.939939727783203 and perplexity is 139.76182555108915
At time: 882.8748888969421 and batch: 700, loss is 4.9489514446258545 and perplexity is 141.02701173594753
At time: 884.0822887420654 and batch: 750, loss is 4.903509817123413 and perplexity is 134.76194036729206
At time: 885.3324301242828 and batch: 800, loss is 4.916672992706299 and perplexity is 136.54756189237466
At time: 886.5391104221344 and batch: 850, loss is 4.951950798034668 and perplexity is 141.45063656704147
At time: 887.7473456859589 and batch: 900, loss is 4.9128070640563966 and perplexity is 136.02069782640714
At time: 888.9547047615051 and batch: 950, loss is 4.90662371635437 and perplexity is 135.18222949915983
At time: 890.1625945568085 and batch: 1000, loss is 4.8981906700134275 and perplexity is 134.0470248376986
At time: 891.3703699111938 and batch: 1050, loss is 4.895945339202881 and perplexity is 133.7463825697496
At time: 892.5782165527344 and batch: 1100, loss is 4.853405914306641 and perplexity is 128.17620441959036
At time: 893.7863211631775 and batch: 1150, loss is 4.898372993469239 and perplexity is 134.07146698262866
At time: 894.9936201572418 and batch: 1200, loss is 4.935707454681396 and perplexity is 139.1715652885342
At time: 896.2004640102386 and batch: 1250, loss is 4.97068736076355 and perplexity is 144.12591983381006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028705680457345 and perplexity of 152.7351965548303
Finished 28 epochs...
Completing Train Step...
At time: 899.1441788673401 and batch: 50, loss is 4.957006559371949 and perplexity is 142.1675880672193
At time: 900.3811144828796 and batch: 100, loss is 4.983600721359253 and perplexity is 145.99913855157527
At time: 901.5914027690887 and batch: 150, loss is 4.913472967147827 and perplexity is 136.11130459390347
At time: 902.8012499809265 and batch: 200, loss is 4.953484182357788 and perplexity is 141.66770113482002
At time: 904.0111930370331 and batch: 250, loss is 4.957003917694092 and perplexity is 142.16721250674598
At time: 905.2224788665771 and batch: 300, loss is 4.954422225952149 and perplexity is 141.80065396242242
At time: 906.4336552619934 and batch: 350, loss is 4.97335711479187 and perplexity is 144.5112146812555
At time: 907.6434423923492 and batch: 400, loss is 4.928868885040283 and perplexity is 138.2230776928982
At time: 908.8527529239655 and batch: 450, loss is 4.89839262008667 and perplexity is 134.07409837784223
At time: 910.06702709198 and batch: 500, loss is 4.893897075653076 and perplexity is 133.47271509664697
At time: 911.2774040699005 and batch: 550, loss is 4.887064685821533 and perplexity is 132.56388574889024
At time: 912.4883558750153 and batch: 600, loss is 4.922903919219971 and perplexity is 137.40103592182373
At time: 913.6987195014954 and batch: 650, loss is 4.938976755142212 and perplexity is 139.6273035177427
At time: 914.9337258338928 and batch: 700, loss is 4.948133373260498 and perplexity is 140.91168875355103
At time: 916.143146276474 and batch: 750, loss is 4.902749547958374 and perplexity is 134.65952395636268
At time: 917.3531293869019 and batch: 800, loss is 4.916050214767456 and perplexity is 136.4625495578762
At time: 918.5632452964783 and batch: 850, loss is 4.951791744232178 and perplexity is 141.42814009455307
At time: 919.7737567424774 and batch: 900, loss is 4.912785539627075 and perplexity is 136.01777009001955
At time: 920.9840342998505 and batch: 950, loss is 4.906715879440307 and perplexity is 135.19468888473324
At time: 922.1942811012268 and batch: 1000, loss is 4.89846791267395 and perplexity is 134.0841935436371
At time: 923.4057021141052 and batch: 1050, loss is 4.896329183578491 and perplexity is 133.79773022058018
At time: 924.6270158290863 and batch: 1100, loss is 4.854013614654541 and perplexity is 128.25412081612143
At time: 925.8399140834808 and batch: 1150, loss is 4.899121713638306 and perplexity is 134.17188658245374
At time: 927.0525603294373 and batch: 1200, loss is 4.936428861618042 and perplexity is 139.27200084421466
At time: 928.264123916626 and batch: 1250, loss is 4.9706434059143065 and perplexity is 144.11958493995743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028714145187044 and perplexity of 152.7364894224565
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 931.2661285400391 and batch: 50, loss is 4.956759223937988 and perplexity is 142.13242933330918
At time: 932.4756844043732 and batch: 100, loss is 4.983709020614624 and perplexity is 146.01495100578822
At time: 933.6860065460205 and batch: 150, loss is 4.913777389526367 and perplexity is 136.15274622855458
At time: 934.8962295055389 and batch: 200, loss is 4.953239822387696 and perplexity is 141.63308744887954
At time: 936.1053941249847 and batch: 250, loss is 4.957110109329224 and perplexity is 142.18231027711857
At time: 937.3276302814484 and batch: 300, loss is 4.9545423126220705 and perplexity is 141.81768335323045
At time: 938.5456655025482 and batch: 350, loss is 4.972040176391602 and perplexity is 144.3210275732124
At time: 939.7579789161682 and batch: 400, loss is 4.927181081771851 and perplexity is 137.9899810965287
At time: 940.9679391384125 and batch: 450, loss is 4.895996341705322 and perplexity is 133.7532041439103
At time: 942.17799949646 and batch: 500, loss is 4.891636972427368 and perplexity is 133.17139361979812
At time: 943.3871927261353 and batch: 550, loss is 4.884049577713013 and perplexity is 132.16479325791622
At time: 944.5951833724976 and batch: 600, loss is 4.919186983108521 and perplexity is 136.89127301462474
At time: 945.8307342529297 and batch: 650, loss is 4.935409994125366 and perplexity is 139.13017339387306
At time: 947.0376818180084 and batch: 700, loss is 4.945101556777954 and perplexity is 140.48511734287223
At time: 948.2448670864105 and batch: 750, loss is 4.8983565711975094 and perplexity is 134.06926524264557
At time: 949.4518373012543 and batch: 800, loss is 4.911347198486328 and perplexity is 135.8222707664909
At time: 950.6602764129639 and batch: 850, loss is 4.945898742675781 and perplexity is 140.5971547486592
At time: 951.8686118125916 and batch: 900, loss is 4.905791330337524 and perplexity is 135.06975252022522
At time: 953.075670003891 and batch: 950, loss is 4.900127487182617 and perplexity is 134.30690100195287
At time: 954.2821619510651 and batch: 1000, loss is 4.891217002868652 and perplexity is 133.1154774307565
At time: 955.4894006252289 and batch: 1050, loss is 4.8886237144470215 and perplexity is 132.7707178281515
At time: 956.6973166465759 and batch: 1100, loss is 4.845916385650635 and perplexity is 127.21981100210542
At time: 957.9050939083099 and batch: 1150, loss is 4.8894759464263915 and perplexity is 132.88391750915125
At time: 959.1131443977356 and batch: 1200, loss is 4.927915620803833 and perplexity is 138.09137735886375
At time: 960.3198595046997 and batch: 1250, loss is 4.965917596817016 and perplexity is 143.44011009313098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.029087038805885 and perplexity of 152.7934545050242
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 963.2627716064453 and batch: 50, loss is 4.956342878341675 and perplexity is 142.07326543943097
At time: 964.4985394477844 and batch: 100, loss is 4.98252908706665 and perplexity is 145.84276467078448
At time: 965.7103595733643 and batch: 150, loss is 4.912617168426514 and perplexity is 135.99487054263807
At time: 966.9210751056671 and batch: 200, loss is 4.952638139724732 and perplexity is 141.54789490766714
At time: 968.1315302848816 and batch: 250, loss is 4.95624077796936 and perplexity is 142.05876044662878
At time: 969.3454871177673 and batch: 300, loss is 4.953758554458618 and perplexity is 141.70657613246368
At time: 970.5558488368988 and batch: 350, loss is 4.971154708862304 and perplexity is 144.1932925504549
At time: 971.7664885520935 and batch: 400, loss is 4.926141557693481 and perplexity is 137.8466117194754
At time: 972.9775867462158 and batch: 450, loss is 4.89524320602417 and perplexity is 133.6525077571817
At time: 974.1886246204376 and batch: 500, loss is 4.891242427825928 and perplexity is 133.11886192910808
At time: 975.3986902236938 and batch: 550, loss is 4.8829260921478275 and perplexity is 132.01639139974768
At time: 976.6364617347717 and batch: 600, loss is 4.917975692749024 and perplexity is 136.72555831989067
At time: 977.8467285633087 and batch: 650, loss is 4.934883327484131 and perplexity is 139.05691746517746
At time: 979.0565853118896 and batch: 700, loss is 4.944633340835571 and perplexity is 140.41935536786875
At time: 980.267608165741 and batch: 750, loss is 4.8976114463806155 and perplexity is 133.9694041150531
At time: 981.4789226055145 and batch: 800, loss is 4.910587635040283 and perplexity is 135.71914430496125
At time: 982.6891615390778 and batch: 850, loss is 4.9445930862426755 and perplexity is 140.41370295765225
At time: 983.9033091068268 and batch: 900, loss is 4.904485759735107 and perplexity is 134.89352448616128
At time: 985.1250658035278 and batch: 950, loss is 4.898894844055175 and perplexity is 134.14145051511647
At time: 986.3435161113739 and batch: 1000, loss is 4.889744653701782 and perplexity is 132.91962918234668
At time: 987.5536725521088 and batch: 1050, loss is 4.8870866584777835 and perplexity is 132.566798561584
At time: 988.76402592659 and batch: 1100, loss is 4.844508533477783 and perplexity is 127.04083033348027
At time: 989.9747350215912 and batch: 1150, loss is 4.887780752182007 and perplexity is 132.65884428233687
At time: 991.1853783130646 and batch: 1200, loss is 4.926241111755371 and perplexity is 137.8603355927122
At time: 992.395580291748 and batch: 1250, loss is 4.9648932933807375 and perplexity is 143.29325911828107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028672267050639 and perplexity of 152.73009323684954
Finished 31 epochs...
Completing Train Step...
At time: 995.463531255722 and batch: 50, loss is 4.956140756607056 and perplexity is 142.04455224645523
At time: 996.6978199481964 and batch: 100, loss is 4.98211672782898 and perplexity is 145.782637457383
At time: 997.90611577034 and batch: 150, loss is 4.912221803665161 and perplexity is 135.94111359060327
At time: 999.1148045063019 and batch: 200, loss is 4.952223691940308 and perplexity is 141.48924285117977
At time: 1000.3226430416107 and batch: 250, loss is 4.955866394042968 and perplexity is 142.00558588458617
At time: 1001.5320115089417 and batch: 300, loss is 4.953378639221191 and perplexity is 141.65274987030335
At time: 1002.7398886680603 and batch: 350, loss is 4.970859661102295 and perplexity is 144.15075491810464
At time: 1003.9471616744995 and batch: 400, loss is 4.925896482467651 and perplexity is 137.81283306930263
At time: 1005.1549599170685 and batch: 450, loss is 4.895016508102417 and perplexity is 133.62221244550793
At time: 1006.3632776737213 and batch: 500, loss is 4.890996084213257 and perplexity is 133.08607298658498
At time: 1007.5967288017273 and batch: 550, loss is 4.882754936218261 and perplexity is 131.9937979451167
At time: 1008.8049578666687 and batch: 600, loss is 4.917881889343262 and perplexity is 136.71273359837622
At time: 1010.0133237838745 and batch: 650, loss is 4.9348192882537845 and perplexity is 139.04801265234042
At time: 1011.2201681137085 and batch: 700, loss is 4.944580450057983 and perplexity is 140.41192867537845
At time: 1012.4278953075409 and batch: 750, loss is 4.897576179504394 and perplexity is 133.96467951597216
At time: 1013.6352908611298 and batch: 800, loss is 4.91059027671814 and perplexity is 135.71950283169306
At time: 1014.8449873924255 and batch: 850, loss is 4.944596796035767 and perplexity is 140.41422386440365
At time: 1016.0525114536285 and batch: 900, loss is 4.904403972625732 and perplexity is 134.88249238586812
At time: 1017.2607686519623 and batch: 950, loss is 4.898857536315918 and perplexity is 134.13644609420948
At time: 1018.4681005477905 and batch: 1000, loss is 4.8897367858886716 and perplexity is 132.91858339965958
At time: 1019.6754672527313 and batch: 1050, loss is 4.887215414047241 and perplexity is 132.58386837411712
At time: 1020.8834073543549 and batch: 1100, loss is 4.84468376159668 and perplexity is 127.06309340970421
At time: 1022.0922613143921 and batch: 1150, loss is 4.887966194152832 and perplexity is 132.68344708099323
At time: 1023.2993388175964 and batch: 1200, loss is 4.926405506134033 and perplexity is 137.88300091989925
At time: 1024.5070292949677 and batch: 1250, loss is 4.9650375938415525 and perplexity is 143.31393789354607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028600094092154 and perplexity of 152.719070651942
Finished 32 epochs...
Completing Train Step...
At time: 1027.4753324985504 and batch: 50, loss is 4.955957736968994 and perplexity is 142.01855768274484
At time: 1028.6847774982452 and batch: 100, loss is 4.981856756210327 and perplexity is 145.7447430351023
At time: 1029.8980786800385 and batch: 150, loss is 4.911969871520996 and perplexity is 135.90686996809194
At time: 1031.107769727707 and batch: 200, loss is 4.951962661743164 and perplexity is 141.4523147061148
At time: 1032.3170733451843 and batch: 250, loss is 4.955595617294311 and perplexity is 141.96713927920754
At time: 1033.5341973304749 and batch: 300, loss is 4.953132410049438 and perplexity is 141.61787512479228
At time: 1034.743148803711 and batch: 350, loss is 4.970623302459717 and perplexity is 144.11668766754877
At time: 1035.953011751175 and batch: 400, loss is 4.925679941177368 and perplexity is 137.7829941314098
At time: 1037.1878933906555 and batch: 450, loss is 4.894825611114502 and perplexity is 133.59670680218622
At time: 1038.3977057933807 and batch: 500, loss is 4.890830020904541 and perplexity is 133.06397410791809
At time: 1039.6132643222809 and batch: 550, loss is 4.882623968124389 and perplexity is 131.97651210096862
At time: 1040.8356807231903 and batch: 600, loss is 4.91781457901001 and perplexity is 136.70353172841186
At time: 1042.043987751007 and batch: 650, loss is 4.934806070327759 and perplexity is 139.04617473814187
At time: 1043.2532811164856 and batch: 700, loss is 4.944568672180176 and perplexity is 140.41027493057865
At time: 1044.4631140232086 and batch: 750, loss is 4.897581748962402 and perplexity is 133.96542562870695
At time: 1045.672720193863 and batch: 800, loss is 4.910601634979248 and perplexity is 135.72104437799825
At time: 1046.8824427127838 and batch: 850, loss is 4.944595365524292 and perplexity is 140.41402300038888
At time: 1048.0911481380463 and batch: 900, loss is 4.904377307891846 and perplexity is 134.87889582805352
At time: 1049.299358844757 and batch: 950, loss is 4.898855857849121 and perplexity is 134.13622095082732
At time: 1050.508098602295 and batch: 1000, loss is 4.889740200042724 and perplexity is 132.91903720495446
At time: 1051.7163360118866 and batch: 1050, loss is 4.887328557968139 and perplexity is 132.5988702815041
At time: 1052.925778388977 and batch: 1100, loss is 4.844840383529663 and perplexity is 127.08299583554235
At time: 1054.1350128650665 and batch: 1150, loss is 4.888125457763672 and perplexity is 132.7045804087144
At time: 1055.344928264618 and batch: 1200, loss is 4.9265406131744385 and perplexity is 137.9016311425845
At time: 1056.5540919303894 and batch: 1250, loss is 4.965133447647094 and perplexity is 143.32767573828215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028564898637089 and perplexity of 152.7136957293404
Finished 33 epochs...
Completing Train Step...
At time: 1059.5112421512604 and batch: 50, loss is 4.955791339874268 and perplexity is 141.99492817334462
At time: 1060.745897769928 and batch: 100, loss is 4.981630725860596 and perplexity is 145.71180402261106
At time: 1061.954360961914 and batch: 150, loss is 4.9117630767822265 and perplexity is 135.87876804818467
At time: 1063.1612548828125 and batch: 200, loss is 4.95173303604126 and perplexity is 141.41983734802517
At time: 1064.368881225586 and batch: 250, loss is 4.955362348556519 and perplexity is 141.93402664604108
At time: 1065.5788850784302 and batch: 300, loss is 4.952927007675171 and perplexity is 141.58878946423707
At time: 1066.7872245311737 and batch: 350, loss is 4.970409803390503 and perplexity is 144.0859221731926
At time: 1068.0203387737274 and batch: 400, loss is 4.925486583709716 and perplexity is 137.75635533606712
At time: 1069.2272338867188 and batch: 450, loss is 4.894651985168457 and perplexity is 133.57351296116371
At time: 1070.4359567165375 and batch: 500, loss is 4.890692796707153 and perplexity is 133.0457157636418
At time: 1071.6448571681976 and batch: 550, loss is 4.882518033981324 and perplexity is 131.9625320227513
At time: 1072.8526825904846 and batch: 600, loss is 4.917755489349365 and perplexity is 136.69545420176462
At time: 1074.0597295761108 and batch: 650, loss is 4.934797143936157 and perplexity is 139.04493356307503
At time: 1075.2719519138336 and batch: 700, loss is 4.944572677612305 and perplexity is 140.41083733553145
At time: 1076.4856190681458 and batch: 750, loss is 4.89759370803833 and perplexity is 133.9670277409836
At time: 1077.6995677947998 and batch: 800, loss is 4.910612010955811 and perplexity is 135.7224526236798
At time: 1078.9135491847992 and batch: 850, loss is 4.94459719657898 and perplexity is 140.41428010637927
At time: 1080.1282155513763 and batch: 900, loss is 4.904365215301514 and perplexity is 134.87726480268356
At time: 1081.342393398285 and batch: 950, loss is 4.898861999511719 and perplexity is 134.13704477276838
At time: 1082.5560524463654 and batch: 1000, loss is 4.889748163223267 and perplexity is 132.92009566745963
At time: 1083.7697303295135 and batch: 1050, loss is 4.887423906326294 and perplexity is 132.61151396884645
At time: 1084.983559846878 and batch: 1100, loss is 4.844981536865235 and perplexity is 127.10093529037617
At time: 1086.197813987732 and batch: 1150, loss is 4.8882661819458 and perplexity is 132.72325646631276
At time: 1087.412722826004 and batch: 1200, loss is 4.926660108566284 and perplexity is 137.9181107366322
At time: 1088.626727104187 and batch: 1250, loss is 4.965205707550049 and perplexity is 143.338032956423
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028545296105155 and perplexity of 152.71070218358366
Finished 34 epochs...
Completing Train Step...
At time: 1091.601796388626 and batch: 50, loss is 4.955634183883667 and perplexity is 141.97261457315116
At time: 1092.8090591430664 and batch: 100, loss is 4.981423463821411 and perplexity is 145.68160662647114
At time: 1094.0171654224396 and batch: 150, loss is 4.911577348709106 and perplexity is 135.8535338898355
At time: 1095.2326288223267 and batch: 200, loss is 4.9515219402313235 and perplexity is 141.3899873636333
At time: 1096.439857006073 and batch: 250, loss is 4.955149841308594 and perplexity is 141.9038678412474
At time: 1097.6546201705933 and batch: 300, loss is 4.952743501663208 and perplexity is 141.56280945396077
At time: 1098.8867461681366 and batch: 350, loss is 4.970199317932129 and perplexity is 144.0555973733954
At time: 1100.0929870605469 and batch: 400, loss is 4.925315532684326 and perplexity is 137.73279398538693
At time: 1101.3009881973267 and batch: 450, loss is 4.894491662979126 and perplexity is 133.55209987967112
At time: 1102.5091178417206 and batch: 500, loss is 4.890587921142578 and perplexity is 133.03176325073844
At time: 1103.7169027328491 and batch: 550, loss is 4.882412128448486 and perplexity is 131.9485572005014
At time: 1104.93523311615 and batch: 600, loss is 4.917693548202514 and perplexity is 136.68698739078678
At time: 1106.1428894996643 and batch: 650, loss is 4.934767265319824 and perplexity is 139.04077915491635
At time: 1107.3500468730927 and batch: 700, loss is 4.944578723907471 and perplexity is 140.41168630346502
At time: 1108.5585515499115 and batch: 750, loss is 4.897600498199463 and perplexity is 133.96793740177685
At time: 1109.7650361061096 and batch: 800, loss is 4.910616779327393 and perplexity is 135.72309980030894
At time: 1110.971792459488 and batch: 850, loss is 4.944603338241577 and perplexity is 140.4151424861597
At time: 1112.1777744293213 and batch: 900, loss is 4.904358596801758 and perplexity is 134.87637212049344
At time: 1113.3845677375793 and batch: 950, loss is 4.898872890472412 and perplexity is 134.13850566200574
At time: 1114.5916936397552 and batch: 1000, loss is 4.889760236740113 and perplexity is 132.92170049016173
At time: 1115.7984244823456 and batch: 1050, loss is 4.887505979537964 and perplexity is 132.62239826835082
At time: 1117.0058960914612 and batch: 1100, loss is 4.845110406875611 and perplexity is 127.11731584468426
At time: 1118.2131171226501 and batch: 1150, loss is 4.8883942604064945 and perplexity is 132.74025654534802
At time: 1119.421695947647 and batch: 1200, loss is 4.9267681694030765 and perplexity is 137.9330150883611
At time: 1120.628494501114 and batch: 1250, loss is 4.965264263153077 and perplexity is 143.3464264471201
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028537722399635 and perplexity of 152.70954560207542
Finished 35 epochs...
Completing Train Step...
At time: 1123.5950736999512 and batch: 50, loss is 4.955486288070679 and perplexity is 141.9516189705159
At time: 1124.8355948925018 and batch: 100, loss is 4.981231718063355 and perplexity is 145.65367547429923
At time: 1126.045381307602 and batch: 150, loss is 4.911411352157593 and perplexity is 135.83098454330755
At time: 1127.2568664550781 and batch: 200, loss is 4.951323909759521 and perplexity is 141.36199060992425
At time: 1128.4679205417633 and batch: 250, loss is 4.954958276748657 and perplexity is 141.8766866928066
At time: 1129.7048363685608 and batch: 300, loss is 4.952582788467407 and perplexity is 141.5400602705429
At time: 1130.9188578128815 and batch: 350, loss is 4.970025062561035 and perplexity is 144.03049709880935
At time: 1132.1323926448822 and batch: 400, loss is 4.925173816680908 and perplexity is 137.7132764272915
At time: 1133.3435006141663 and batch: 450, loss is 4.894362936019897 and perplexity is 133.53490923042872
At time: 1134.5539984703064 and batch: 500, loss is 4.890493087768554 and perplexity is 133.01914799795998
At time: 1135.7650907039642 and batch: 550, loss is 4.882334718704223 and perplexity is 131.93834349175808
At time: 1136.9761168956757 and batch: 600, loss is 4.917641429901123 and perplexity is 136.6798636828211
At time: 1138.1858010292053 and batch: 650, loss is 4.9347571849823 and perplexity is 139.03937758399707
At time: 1139.3954849243164 and batch: 700, loss is 4.944588737487793 and perplexity is 140.4130923342037
At time: 1140.6054031848907 and batch: 750, loss is 4.897607517242432 and perplexity is 133.96887773178605
At time: 1141.8145697116852 and batch: 800, loss is 4.91062180519104 and perplexity is 135.72378192781642
At time: 1143.0249135494232 and batch: 850, loss is 4.944607496261597 and perplexity is 140.4157263363471
At time: 1144.2354834079742 and batch: 900, loss is 4.904357509613037 and perplexity is 134.87622548450273
At time: 1145.4520137310028 and batch: 950, loss is 4.898884010314942 and perplexity is 134.13999726935904
At time: 1146.662670135498 and batch: 1000, loss is 4.8897702026367185 and perplexity is 132.92302518068632
At time: 1147.8746421337128 and batch: 1050, loss is 4.887579479217529 and perplexity is 132.63214633036213
At time: 1149.0858347415924 and batch: 1100, loss is 4.8452298545837404 and perplexity is 127.13250062360099
At time: 1150.3046116828918 and batch: 1150, loss is 4.888513774871826 and perplexity is 132.75612187418625
At time: 1151.5156466960907 and batch: 1200, loss is 4.92686728477478 and perplexity is 137.9466870479613
At time: 1152.7259800434113 and batch: 1250, loss is 4.965314617156983 and perplexity is 143.3536446953697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.02853415830292 and perplexity of 152.70900133145543
Finished 36 epochs...
Completing Train Step...
At time: 1155.6709938049316 and batch: 50, loss is 4.955347099304199 and perplexity is 141.93186227475857
At time: 1156.9039196968079 and batch: 100, loss is 4.981051321029663 and perplexity is 145.6274023531652
At time: 1158.1119899749756 and batch: 150, loss is 4.911258850097656 and perplexity is 135.8102716177835
At time: 1159.3446145057678 and batch: 200, loss is 4.95113429069519 and perplexity is 141.3351882227349
At time: 1160.5523595809937 and batch: 250, loss is 4.954783582687378 and perplexity is 141.8519038429788
At time: 1161.7607407569885 and batch: 300, loss is 4.952438850402832 and perplexity is 141.51968873436255
At time: 1162.9679772853851 and batch: 350, loss is 4.969882535934448 and perplexity is 144.0099703807689
At time: 1164.1755826473236 and batch: 400, loss is 4.92504355430603 and perplexity is 137.6953387371803
At time: 1165.3833050727844 and batch: 450, loss is 4.894240436553955 and perplexity is 133.51855227724278
At time: 1166.5982160568237 and batch: 500, loss is 4.890405607223511 and perplexity is 133.00751191936456
At time: 1167.8058109283447 and batch: 550, loss is 4.882267189025879 and perplexity is 131.92943403869043
At time: 1169.0131459236145 and batch: 600, loss is 4.917591905593872 and perplexity is 136.6730948748686
At time: 1170.22154545784 and batch: 650, loss is 4.934751920700073 and perplexity is 139.03864564339935
At time: 1171.4285252094269 and batch: 700, loss is 4.944600267410278 and perplexity is 140.41471129560745
At time: 1172.6355876922607 and batch: 750, loss is 4.897615804672241 and perplexity is 133.96998799405745
At time: 1173.841837644577 and batch: 800, loss is 4.910626745223999 and perplexity is 135.72445240942852
At time: 1175.049406528473 and batch: 850, loss is 4.944609870910645 and perplexity is 140.41605977481387
At time: 1176.2579913139343 and batch: 900, loss is 4.904358720779419 and perplexity is 134.8763888421516
At time: 1177.4665701389313 and batch: 950, loss is 4.898896112442016 and perplexity is 134.141620658475
At time: 1178.6740119457245 and batch: 1000, loss is 4.889780178070068 and perplexity is 132.9243511520782
At time: 1179.8824048042297 and batch: 1050, loss is 4.887647218704224 and perplexity is 132.64113106818107
At time: 1181.0907027721405 and batch: 1100, loss is 4.845341701507568 and perplexity is 127.14672079794079
At time: 1182.2996833324432 and batch: 1150, loss is 4.888626756668091 and perplexity is 132.77112174664111
At time: 1183.5095155239105 and batch: 1200, loss is 4.9269599342346195 and perplexity is 137.959468326083
At time: 1184.717592716217 and batch: 1250, loss is 4.965359306335449 and perplexity is 143.3600511951308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0285319307424725 and perplexity of 152.70866116330302
Finished 37 epochs...
Completing Train Step...
At time: 1187.7026612758636 and batch: 50, loss is 4.955215892791748 and perplexity is 141.91324111173944
At time: 1188.912175655365 and batch: 100, loss is 4.9808806037902835 and perplexity is 145.60254336704656
At time: 1190.1497123241425 and batch: 150, loss is 4.911114807128906 and perplexity is 135.79071051192662
At time: 1191.3593456745148 and batch: 200, loss is 4.95095365524292 and perplexity is 141.3096603827744
At time: 1192.5699999332428 and batch: 250, loss is 4.954619646072388 and perplexity is 141.82865102807875
At time: 1193.781482219696 and batch: 300, loss is 4.952305088043213 and perplexity is 141.5007599928697
At time: 1194.992267370224 and batch: 350, loss is 4.969753170013428 and perplexity is 143.99134160330496
At time: 1196.20081949234 and batch: 400, loss is 4.924923591613769 and perplexity is 137.6788214243841
At time: 1197.4125583171844 and batch: 450, loss is 4.894124612808228 and perplexity is 133.50308855394448
At time: 1198.6228604316711 and batch: 500, loss is 4.890323553085327 and perplexity is 132.99659855035173
At time: 1199.832822561264 and batch: 550, loss is 4.882204122543335 and perplexity is 131.92111397570292
At time: 1201.0440242290497 and batch: 600, loss is 4.917543964385986 and perplexity is 136.66654275867432
At time: 1202.2635626792908 and batch: 650, loss is 4.934748449325562 and perplexity is 139.03816298902652
At time: 1203.4891865253448 and batch: 700, loss is 4.94461256980896 and perplexity is 140.41643874399247
At time: 1204.708125591278 and batch: 750, loss is 4.897625036239624 and perplexity is 133.97122475273758
At time: 1205.920301437378 and batch: 800, loss is 4.910631065368652 and perplexity is 135.7250387599625
At time: 1207.134489774704 and batch: 850, loss is 4.944611444473266 and perplexity is 140.41628072845086
At time: 1208.3448369503021 and batch: 900, loss is 4.904360389709472 and perplexity is 134.87661394159832
At time: 1209.5568430423737 and batch: 950, loss is 4.898908901214599 and perplexity is 134.14333617612522
At time: 1210.7677371501923 and batch: 1000, loss is 4.889791088104248 and perplexity is 132.92580136920358
At time: 1211.9811153411865 and batch: 1050, loss is 4.887710647583008 and perplexity is 132.64954461323342
At time: 1213.1922419071198 and batch: 1100, loss is 4.845447397232055 and perplexity is 127.16016037295202
At time: 1214.4044299125671 and batch: 1150, loss is 4.888734264373779 and perplexity is 132.78539643262735
At time: 1215.614624261856 and batch: 1200, loss is 4.927046871185302 and perplexity is 137.97146262294333
At time: 1216.8245034217834 and batch: 1250, loss is 4.965399875640869 and perplexity is 143.36586733081026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028535049327099 and perplexity of 152.70913739892856
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1219.7653052806854 and batch: 50, loss is 4.955167503356933 and perplexity is 141.90637417635423
At time: 1220.9993085861206 and batch: 100, loss is 4.980790452957153 and perplexity is 145.58941776810687
At time: 1222.206622838974 and batch: 150, loss is 4.911082630157471 and perplexity is 135.78634124840846
At time: 1223.414796113968 and batch: 200, loss is 4.950975046157837 and perplexity is 141.31268315802654
At time: 1224.6229031085968 and batch: 250, loss is 4.954519939422608 and perplexity is 141.8145104734073
At time: 1225.8359196186066 and batch: 300, loss is 4.952161264419556 and perplexity is 141.48041030423536
At time: 1227.0438742637634 and batch: 350, loss is 4.969705448150635 and perplexity is 143.9844702322162
At time: 1228.2521336078644 and batch: 400, loss is 4.924757843017578 and perplexity is 137.65600324410065
At time: 1229.4592337608337 and batch: 450, loss is 4.894068822860718 and perplexity is 133.49564063140318
At time: 1230.6675593852997 and batch: 500, loss is 4.890336284637451 and perplexity is 132.99829181425736
At time: 1231.874737739563 and batch: 550, loss is 4.882056140899659 and perplexity is 131.90159351679156
At time: 1233.0820269584656 and batch: 600, loss is 4.917330865859985 and perplexity is 136.6374224227193
At time: 1234.290114402771 and batch: 650, loss is 4.934744892120361 and perplexity is 139.03766840262972
At time: 1235.4980382919312 and batch: 700, loss is 4.94473650932312 and perplexity is 140.43384296770373
At time: 1236.7054948806763 and batch: 750, loss is 4.897737865447998 and perplexity is 133.98634147275902
At time: 1237.9120151996613 and batch: 800, loss is 4.910418968200684 and perplexity is 135.69625491621292
At time: 1239.1203677654266 and batch: 850, loss is 4.944278459548951 and perplexity is 140.3695320075814
At time: 1240.3276567459106 and batch: 900, loss is 4.9039520835876464 and perplexity is 134.82155423580724
At time: 1241.5354375839233 and batch: 950, loss is 4.898584203720093 and perplexity is 134.09978724146166
At time: 1242.7432296276093 and batch: 1000, loss is 4.889439086914063 and perplexity is 132.87901956302406
At time: 1243.9498603343964 and batch: 1050, loss is 4.887391910552979 and perplexity is 132.60727102878496
At time: 1245.1581087112427 and batch: 1100, loss is 4.845077285766601 and perplexity is 127.11310564793278
At time: 1246.3653538227081 and batch: 1150, loss is 4.888371000289917 and perplexity is 132.73716902741438
At time: 1247.5737931728363 and batch: 1200, loss is 4.9266962146759035 and perplexity is 137.9230905129566
At time: 1248.782121181488 and batch: 1250, loss is 4.9651307773590085 and perplexity is 143.32729301260827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0284958442632295 and perplexity of 152.7031505448017
Finished 39 epochs...
Completing Train Step...
At time: 1251.7644658088684 and batch: 50, loss is 4.955102434158325 and perplexity is 141.89714074271853
At time: 1252.9738249778748 and batch: 100, loss is 4.98074405670166 and perplexity is 145.58266312097933
At time: 1254.1827025413513 and batch: 150, loss is 4.9109717464447025 and perplexity is 135.77128558947675
At time: 1255.3917784690857 and batch: 200, loss is 4.950874967575073 and perplexity is 141.29854149262022
At time: 1256.6012058258057 and batch: 250, loss is 4.95447470664978 and perplexity is 141.8080959549457
At time: 1257.8102271556854 and batch: 300, loss is 4.952165393829346 and perplexity is 141.48099453603305
At time: 1259.0186052322388 and batch: 350, loss is 4.969637432098389 and perplexity is 143.97467731000796
At time: 1260.2286586761475 and batch: 400, loss is 4.924728307723999 and perplexity is 137.65193759367233
At time: 1261.437405347824 and batch: 450, loss is 4.894027757644653 and perplexity is 133.4901587166357
At time: 1262.6461300849915 and batch: 500, loss is 4.890328369140625 and perplexity is 132.99723907086715
At time: 1263.8551347255707 and batch: 550, loss is 4.882044115066528 and perplexity is 131.90000729977612
At time: 1265.0629909038544 and batch: 600, loss is 4.917310724258423 and perplexity is 136.63467035391398
At time: 1266.2772076129913 and batch: 650, loss is 4.934706907272339 and perplexity is 139.03238717823004
At time: 1267.4850497245789 and batch: 700, loss is 4.944677734375 and perplexity is 140.42558921842823
At time: 1268.693470954895 and batch: 750, loss is 4.897676286697387 and perplexity is 133.97809101528128
At time: 1269.9044420719147 and batch: 800, loss is 4.910412797927856 and perplexity is 135.6954176358815
At time: 1271.1138620376587 and batch: 850, loss is 4.944281072616577 and perplexity is 140.36989880314042
At time: 1272.3219606876373 and batch: 900, loss is 4.9039503192901615 and perplexity is 134.821316370688
At time: 1273.5315272808075 and batch: 950, loss is 4.898570003509522 and perplexity is 134.09788300976558
At time: 1274.7402076721191 and batch: 1000, loss is 4.889447507858276 and perplexity is 132.8801385345463
At time: 1275.949376821518 and batch: 1050, loss is 4.887395067214966 and perplexity is 132.60768962577728
At time: 1277.1589691638947 and batch: 1100, loss is 4.845135040283203 and perplexity is 127.12044721590557
At time: 1278.3688354492188 and batch: 1150, loss is 4.888423175811767 and perplexity is 132.74409483915468
At time: 1279.5785982608795 and batch: 1200, loss is 4.9267286109924315 and perplexity is 137.9275587854308
At time: 1280.7881886959076 and batch: 1250, loss is 4.965146007537842 and perplexity is 143.32947592953565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028459757783987 and perplexity of 152.6976401251558
Finished 40 epochs...
Completing Train Step...
At time: 1283.7712771892548 and batch: 50, loss is 4.955063667297363 and perplexity is 141.89163994261747
At time: 1285.0156226158142 and batch: 100, loss is 4.980702457427978 and perplexity is 145.57660711389647
At time: 1286.2254388332367 and batch: 150, loss is 4.910912427902222 and perplexity is 135.76323207356862
At time: 1287.4344260692596 and batch: 200, loss is 4.95081335067749 and perplexity is 141.2898353830849
At time: 1288.6437530517578 and batch: 250, loss is 4.95443657875061 and perplexity is 141.80268921323622
At time: 1289.853645324707 and batch: 300, loss is 4.952145195007324 and perplexity is 141.47813681546629
At time: 1291.0626542568207 and batch: 350, loss is 4.969601812362671 and perplexity is 143.96954906138615
At time: 1292.2734370231628 and batch: 400, loss is 4.924706211090088 and perplexity is 137.648895982805
At time: 1293.4836122989655 and batch: 450, loss is 4.894002685546875 and perplexity is 133.4868118802802
At time: 1294.693964958191 and batch: 500, loss is 4.890318517684936 and perplexity is 132.9959288609135
At time: 1295.903386592865 and batch: 550, loss is 4.882040271759033 and perplexity is 131.89950036846358
At time: 1297.1121439933777 and batch: 600, loss is 4.917305355072021 and perplexity is 136.6339367388694
At time: 1298.3217730522156 and batch: 650, loss is 4.934699974060059 and perplexity is 139.03142324051743
At time: 1299.531650543213 and batch: 700, loss is 4.944668655395508 and perplexity is 140.42431430317095
At time: 1300.7408046722412 and batch: 750, loss is 4.897661533355713 and perplexity is 133.97611440530846
At time: 1301.9549610614777 and batch: 800, loss is 4.910413560867309 and perplexity is 135.6955211633087
At time: 1303.1641364097595 and batch: 850, loss is 4.9442823791503905 and perplexity is 140.37008220127942
At time: 1304.374012708664 and batch: 900, loss is 4.9039445495605465 and perplexity is 134.8205384903903
At time: 1305.5839264392853 and batch: 950, loss is 4.898561286926269 and perplexity is 134.09671413949863
At time: 1306.7939813137054 and batch: 1000, loss is 4.889451818466187 and perplexity is 132.88071132995717
At time: 1308.0034873485565 and batch: 1050, loss is 4.887408275604248 and perplexity is 132.60944117133116
At time: 1309.2141876220703 and batch: 1100, loss is 4.845172319412232 and perplexity is 127.12518624379233
At time: 1310.4253313541412 and batch: 1150, loss is 4.88845609664917 and perplexity is 132.74846495785067
At time: 1311.636302471161 and batch: 1200, loss is 4.926752653121948 and perplexity is 137.93087489752602
At time: 1312.87331032753 and batch: 1250, loss is 4.965156707763672 and perplexity is 143.33100959550146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028434363594891 and perplexity of 152.69376254164212
Finished 41 epochs...
Completing Train Step...
At time: 1315.8342492580414 and batch: 50, loss is 4.9550290870666505 and perplexity is 141.88673338180755
At time: 1317.06973528862 and batch: 100, loss is 4.980661659240723 and perplexity is 145.57066797337336
At time: 1318.2789597511292 and batch: 150, loss is 4.910867557525635 and perplexity is 135.7571404628863
At time: 1319.4889283180237 and batch: 200, loss is 4.950764169692993 and perplexity is 141.28288678075234
At time: 1320.6982626914978 and batch: 250, loss is 4.954399166107177 and perplexity is 141.79738409902663
At time: 1321.9079728126526 and batch: 300, loss is 4.952118330001831 and perplexity is 141.47433605559755
At time: 1323.1161880493164 and batch: 350, loss is 4.969572534561157 and perplexity is 143.96533401120868
At time: 1324.3233513832092 and batch: 400, loss is 4.92468373298645 and perplexity is 137.64580193142976
At time: 1325.5301253795624 and batch: 450, loss is 4.893980913162231 and perplexity is 133.48390558570568
At time: 1326.7375180721283 and batch: 500, loss is 4.8903084564208985 and perplexity is 132.99459076048876
At time: 1327.9426293373108 and batch: 550, loss is 4.8820390701293945 and perplexity is 131.89934187420985
At time: 1329.148061990738 and batch: 600, loss is 4.91730263710022 and perplexity is 136.63356537218692
At time: 1330.3568696975708 and batch: 650, loss is 4.9347012424468994 and perplexity is 139.031599586257
At time: 1331.5639097690582 and batch: 700, loss is 4.944672403335571 and perplexity is 140.42484060607066
At time: 1332.7702479362488 and batch: 750, loss is 4.897660274505615 and perplexity is 133.9759457495699
At time: 1333.9772243499756 and batch: 800, loss is 4.910414447784424 and perplexity is 135.6956415140422
At time: 1335.1830973625183 and batch: 850, loss is 4.944281206130982 and perplexity is 140.3699175445452
At time: 1336.3902432918549 and batch: 900, loss is 4.903936767578125 and perplexity is 134.81948932341206
At time: 1337.599871635437 and batch: 950, loss is 4.89855525970459 and perplexity is 134.09590591131177
At time: 1338.806694984436 and batch: 1000, loss is 4.8894539642333985 and perplexity is 132.88099646133654
At time: 1340.0139729976654 and batch: 1050, loss is 4.887422485351562 and perplexity is 132.61132553136986
At time: 1341.2210524082184 and batch: 1100, loss is 4.84520073890686 and perplexity is 127.1287991286779
At time: 1342.469685792923 and batch: 1150, loss is 4.88848219871521 and perplexity is 132.75193001227206
At time: 1343.6760153770447 and batch: 1200, loss is 4.926773433685303 and perplexity is 137.93374120859207
At time: 1344.883706331253 and batch: 1250, loss is 4.965164632797241 and perplexity is 143.33214550306502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028420107208029 and perplexity of 152.69158569580893
Finished 42 epochs...
Completing Train Step...
At time: 1347.8548073768616 and batch: 50, loss is 4.954996414184571 and perplexity is 141.88209760903152
At time: 1349.063814163208 and batch: 100, loss is 4.980622310638427 and perplexity is 145.56494008374634
At time: 1350.2723202705383 and batch: 150, loss is 4.910828866958618 and perplexity is 135.7518880437554
At time: 1351.4968748092651 and batch: 200, loss is 4.950720386505127 and perplexity is 141.2767011009935
At time: 1352.713923215866 and batch: 250, loss is 4.954362211227417 and perplexity is 141.7921440905694
At time: 1353.92489528656 and batch: 300, loss is 4.952090120315551 and perplexity is 141.47034516525198
At time: 1355.1356837749481 and batch: 350, loss is 4.969545421600341 and perplexity is 143.96143073766368
At time: 1356.3554332256317 and batch: 400, loss is 4.924660930633545 and perplexity is 137.6426633190622
At time: 1357.5657494068146 and batch: 450, loss is 4.893960628509522 and perplexity is 133.4811979385005
At time: 1358.7767810821533 and batch: 500, loss is 4.890298843383789 and perplexity is 132.99331228469745
At time: 1359.9859898090363 and batch: 550, loss is 4.88203908920288 and perplexity is 131.89934438999012
At time: 1361.1956503391266 and batch: 600, loss is 4.917300910949707 and perplexity is 136.6333295222915
At time: 1362.4085936546326 and batch: 650, loss is 4.9347049999237065 and perplexity is 139.03212199524933
At time: 1363.618609905243 and batch: 700, loss is 4.944679489135742 and perplexity is 140.42583563195558
At time: 1364.828941822052 and batch: 750, loss is 4.897663307189942 and perplexity is 133.9763520569368
At time: 1366.0408654212952 and batch: 800, loss is 4.910414991378784 and perplexity is 135.69571527744762
At time: 1367.253378868103 and batch: 850, loss is 4.944278459548951 and perplexity is 140.3695320075814
At time: 1368.4665648937225 and batch: 900, loss is 4.9039285945892335 and perplexity is 134.81838744972623
At time: 1369.6793212890625 and batch: 950, loss is 4.898550443649292 and perplexity is 134.09526009956872
At time: 1370.8905408382416 and batch: 1000, loss is 4.8894549751281735 and perplexity is 132.88113079010947
At time: 1372.1016414165497 and batch: 1050, loss is 4.887436008453369 and perplexity is 132.61311885995136
At time: 1373.33926653862 and batch: 1100, loss is 4.845224943161011 and perplexity is 127.13187622368108
At time: 1374.551645040512 and batch: 1150, loss is 4.888505439758301 and perplexity is 132.75501534145087
At time: 1375.7658195495605 and batch: 1200, loss is 4.926792583465576 and perplexity is 137.9363826347198
At time: 1376.9797608852386 and batch: 1250, loss is 4.96517168045044 and perplexity is 143.33315566187838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0284098604299725 and perplexity of 152.69002110703525
Finished 43 epochs...
Completing Train Step...
At time: 1379.9217240810394 and batch: 50, loss is 4.954965620040894 and perplexity is 141.8777285386038
At time: 1381.1549291610718 and batch: 100, loss is 4.980584363937378 and perplexity is 145.55941647928373
At time: 1382.362089395523 and batch: 150, loss is 4.910793170928955 and perplexity is 135.74704232681995
At time: 1383.5695824623108 and batch: 200, loss is 4.950679244995118 and perplexity is 141.27088888374357
At time: 1384.7816100120544 and batch: 250, loss is 4.954326362609863 and perplexity is 141.7870611293329
At time: 1385.9968450069427 and batch: 300, loss is 4.952062377929687 and perplexity is 141.4664204947882
At time: 1387.2055261135101 and batch: 350, loss is 4.969519453048706 and perplexity is 143.95769231635697
At time: 1388.414743423462 and batch: 400, loss is 4.924637823104859 and perplexity is 137.63948277401852
At time: 1389.625039100647 and batch: 450, loss is 4.893940992355347 and perplexity is 133.47857690685194
At time: 1390.835988521576 and batch: 500, loss is 4.890289916992187 and perplexity is 132.99212513961004
At time: 1392.0415441989899 and batch: 550, loss is 4.882039880752563 and perplexity is 131.89944879491563
At time: 1393.2482509613037 and batch: 600, loss is 4.917299070358276 and perplexity is 136.6330780363875
At time: 1394.4555170536041 and batch: 650, loss is 4.934709882736206 and perplexity is 139.03280086468982
At time: 1395.6628139019012 and batch: 700, loss is 4.944687442779541 and perplexity is 140.42695253347404
At time: 1396.8765950202942 and batch: 750, loss is 4.897667760848999 and perplexity is 133.97694874325936
At time: 1398.090957403183 and batch: 800, loss is 4.910415115356446 and perplexity is 135.69573210068617
At time: 1399.298645734787 and batch: 850, loss is 4.944275007247925 and perplexity is 140.36904741053857
At time: 1400.50585603714 and batch: 900, loss is 4.903920106887817 and perplexity is 134.81724315636433
At time: 1401.7135367393494 and batch: 950, loss is 4.8985464382171635 and perplexity is 134.09472299118136
At time: 1402.9208998680115 and batch: 1000, loss is 4.889455528259277 and perplexity is 132.8812042908163
At time: 1404.1709949970245 and batch: 1050, loss is 4.887448501586914 and perplexity is 132.61477562370422
At time: 1405.3783791065216 and batch: 1100, loss is 4.84524694442749 and perplexity is 127.13467331673762
At time: 1406.7792282104492 and batch: 1150, loss is 4.88852741241455 and perplexity is 132.75793235381545
At time: 1407.9870383739471 and batch: 1200, loss is 4.92681116104126 and perplexity is 137.93894518211067
At time: 1409.1943867206573 and batch: 1250, loss is 4.965177917480469 and perplexity is 143.3340496378623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028406741845346 and perplexity of 152.68954493102535
Finished 44 epochs...
Completing Train Step...
At time: 1412.161236524582 and batch: 50, loss is 4.9549357700347905 and perplexity is 141.87349355074852
At time: 1413.3767199516296 and batch: 100, loss is 4.980547571182251 and perplexity is 145.55406104583795
At time: 1414.5973207950592 and batch: 150, loss is 4.9107592487335205 and perplexity is 135.74243756722265
At time: 1415.8081557750702 and batch: 200, loss is 4.950639781951904 and perplexity is 141.2653140145521
At time: 1417.0450177192688 and batch: 250, loss is 4.954291181564331 and perplexity is 141.78207300002381
At time: 1418.2570295333862 and batch: 300, loss is 4.952035264968872 and perplexity is 141.46258497326906
At time: 1419.466985464096 and batch: 350, loss is 4.969494209289551 and perplexity is 143.95405832891157
At time: 1420.6777007579803 and batch: 400, loss is 4.924614744186401 and perplexity is 137.6363062402747
At time: 1421.8872125148773 and batch: 450, loss is 4.89392168045044 and perplexity is 133.47599920615778
At time: 1423.0989456176758 and batch: 500, loss is 4.890281229019165 and perplexity is 132.99096971263387
At time: 1424.3128473758698 and batch: 550, loss is 4.882041034698486 and perplexity is 131.89960099983463
At time: 1425.5205190181732 and batch: 600, loss is 4.9172975540161135 and perplexity is 136.63287085404752
At time: 1426.7282140254974 and batch: 650, loss is 4.93471529006958 and perplexity is 139.0335526634266
At time: 1427.938048839569 and batch: 700, loss is 4.944695606231689 and perplexity is 140.4280989068605
At time: 1429.147296667099 and batch: 750, loss is 4.897672891616821 and perplexity is 133.97763614964035
At time: 1430.3591694831848 and batch: 800, loss is 4.910414934158325 and perplexity is 135.6957075128768
At time: 1431.5690009593964 and batch: 850, loss is 4.944271097183227 and perplexity is 140.36849855955467
At time: 1432.7787780761719 and batch: 900, loss is 4.903911428451538 and perplexity is 134.8160731585872
At time: 1433.9876928329468 and batch: 950, loss is 4.898543033599854 and perplexity is 134.09426645074348
At time: 1435.1975395679474 and batch: 1000, loss is 4.889456033706665 and perplexity is 132.8812714552909
At time: 1436.4084794521332 and batch: 1050, loss is 4.887460250854492 and perplexity is 132.61633375934122
At time: 1437.618938922882 and batch: 1100, loss is 4.845267829895019 and perplexity is 127.13732861155741
At time: 1438.8288877010345 and batch: 1150, loss is 4.888548374176025 and perplexity is 132.7607152230942
At time: 1440.0385229587555 and batch: 1200, loss is 4.926828908920288 and perplexity is 137.9413933275476
At time: 1441.2479059696198 and batch: 1250, loss is 4.965183992385864 and perplexity is 143.33492038129856
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028403623260721 and perplexity of 152.6890687565006
Finished 45 epochs...
Completing Train Step...
At time: 1444.1886932849884 and batch: 50, loss is 4.954907159805298 and perplexity is 141.86943457560358
At time: 1445.421375989914 and batch: 100, loss is 4.980511798858642 and perplexity is 145.5488543319923
At time: 1446.6293542385101 and batch: 150, loss is 4.910726375579834 and perplexity is 135.7379753585546
At time: 1447.8359212875366 and batch: 200, loss is 4.950601434707641 and perplexity is 141.2598969829145
At time: 1449.0434730052948 and batch: 250, loss is 4.9542567157745365 and perplexity is 141.77718645310898
At time: 1450.2589192390442 and batch: 300, loss is 4.9520087242126465 and perplexity is 141.45883049910975
At time: 1451.4660584926605 and batch: 350, loss is 4.969469747543335 and perplexity is 143.950537004339
At time: 1452.6729953289032 and batch: 400, loss is 4.924591808319092 and perplexity is 137.63314946841962
At time: 1453.8801681995392 and batch: 450, loss is 4.893902702331543 and perplexity is 133.4734661068118
At time: 1455.089435338974 and batch: 500, loss is 4.890272827148437 and perplexity is 132.98985234439235
At time: 1456.3039195537567 and batch: 550, loss is 4.8820425891876225 and perplexity is 131.89980603649084
At time: 1457.5440864562988 and batch: 600, loss is 4.917295637130738 and perplexity is 136.63260894474655
At time: 1458.7518508434296 and batch: 650, loss is 4.9347208309173585 and perplexity is 139.0343230293123
At time: 1459.9728391170502 and batch: 700, loss is 4.944703903198242 and perplexity is 140.42926403893375
At time: 1461.1868152618408 and batch: 750, loss is 4.897678337097168 and perplexity is 133.97836572421133
At time: 1462.4044923782349 and batch: 800, loss is 4.91041446685791 and perplexity is 135.69564410223114
At time: 1463.6161608695984 and batch: 850, loss is 4.9442668151855464 and perplexity is 140.36789750325622
At time: 1464.8265175819397 and batch: 900, loss is 4.903902797698975 and perplexity is 134.8149095994394
At time: 1466.0351810455322 and batch: 950, loss is 4.898539628982544 and perplexity is 134.09380991185992
At time: 1467.244823217392 and batch: 1000, loss is 4.889456281661987 and perplexity is 132.88130440391345
At time: 1468.4546313285828 and batch: 1050, loss is 4.887471446990967 and perplexity is 132.61781855822477
At time: 1469.6645803451538 and batch: 1100, loss is 4.8452873802185055 and perplexity is 127.139814211756
At time: 1470.8755433559418 and batch: 1150, loss is 4.888568859100342 and perplexity is 132.76343484415332
At time: 1472.0899665355682 and batch: 1200, loss is 4.9268464183807374 and perplexity is 137.94380862806366
At time: 1473.2992453575134 and batch: 1250, loss is 4.965189733505249 and perplexity is 143.33574328655072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028402732236542 and perplexity of 152.6889327069091
Finished 46 epochs...
Completing Train Step...
At time: 1476.2619745731354 and batch: 50, loss is 4.954879379272461 and perplexity is 141.8654934218616
At time: 1477.493881702423 and batch: 100, loss is 4.980476999282837 and perplexity is 145.5437893817322
At time: 1478.7043006420135 and batch: 150, loss is 4.910694627761841 and perplexity is 135.7336660424242
At time: 1479.9160714149475 and batch: 200, loss is 4.950564012527466 and perplexity is 141.25461082850845
At time: 1481.1305465698242 and batch: 250, loss is 4.954222860336304 and perplexity is 141.77238660558118
At time: 1482.343891620636 and batch: 300, loss is 4.951982774734497 and perplexity is 141.4551597639057
At time: 1483.5536272525787 and batch: 350, loss is 4.9694454383850095 and perplexity is 143.9470377304763
At time: 1484.7643628120422 and batch: 400, loss is 4.92456883430481 and perplexity is 137.6299875187995
At time: 1485.974663734436 and batch: 450, loss is 4.893883981704712 and perplexity is 133.4709674232495
At time: 1487.2106380462646 and batch: 500, loss is 4.890264568328857 and perplexity is 132.9887540097313
At time: 1488.4261362552643 and batch: 550, loss is 4.882044267654419 and perplexity is 131.90002742612157
At time: 1489.6373465061188 and batch: 600, loss is 4.917293615341187 and perplexity is 136.63233270264473
At time: 1490.844702720642 and batch: 650, loss is 4.934726419448853 and perplexity is 139.0351000291765
At time: 1492.05410695076 and batch: 700, loss is 4.944712171554565 and perplexity is 140.43042516292732
At time: 1493.2633850574493 and batch: 750, loss is 4.897683820724487 and perplexity is 133.9791004136522
At time: 1494.4723167419434 and batch: 800, loss is 4.9104138851165775 and perplexity is 135.6955651624893
At time: 1495.6818144321442 and batch: 850, loss is 4.94426233291626 and perplexity is 140.3672683379505
At time: 1496.8909277915955 and batch: 900, loss is 4.903894271850586 and perplexity is 134.8137601928594
At time: 1498.099746465683 and batch: 950, loss is 4.898536672592163 and perplexity is 134.09341347879624
At time: 1499.3084254264832 and batch: 1000, loss is 4.889456644058227 and perplexity is 132.88135255960728
At time: 1500.524602651596 and batch: 1050, loss is 4.88748197555542 and perplexity is 132.6192148408255
At time: 1501.7330293655396 and batch: 1100, loss is 4.845306262969971 and perplexity is 127.14221498393567
At time: 1502.9433369636536 and batch: 1150, loss is 4.888588638305664 and perplexity is 132.7660608253601
At time: 1504.1529049873352 and batch: 1200, loss is 4.926863269805908 and perplexity is 137.94613319741865
At time: 1505.3614201545715 and batch: 1250, loss is 4.965195255279541 and perplexity is 143.3365347563583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028403623260721 and perplexity of 152.6890687565006
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1508.316068649292 and batch: 50, loss is 4.954866495132446 and perplexity is 141.863665618756
At time: 1509.526690006256 and batch: 100, loss is 4.980456161499023 and perplexity is 145.54075660331188
At time: 1510.7348046302795 and batch: 150, loss is 4.9106747341156005 and perplexity is 135.73096583174762
At time: 1511.942390203476 and batch: 200, loss is 4.950554943084716 and perplexity is 141.2533297337118
At time: 1513.1496727466583 and batch: 250, loss is 4.9542004585266115 and perplexity is 141.7692106831301
At time: 1514.3576838970184 and batch: 300, loss is 4.9519580078125 and perplexity is 141.45165639838177
At time: 1515.5650329589844 and batch: 350, loss is 4.9694380283355715 and perplexity is 143.94597107976222
At time: 1516.7733495235443 and batch: 400, loss is 4.924545555114746 and perplexity is 137.62678364145359
At time: 1518.0054037570953 and batch: 450, loss is 4.8938924598693845 and perplexity is 133.47209901688723
At time: 1519.2133967876434 and batch: 500, loss is 4.890283641815185 and perplexity is 132.99129059310337
At time: 1520.420551776886 and batch: 550, loss is 4.882007417678833 and perplexity is 131.89516700288485
At time: 1521.6269438266754 and batch: 600, loss is 4.917248525619507 and perplexity is 136.62617212768103
At time: 1522.8340764045715 and batch: 650, loss is 4.934713401794434 and perplexity is 139.03329013007252
At time: 1524.0429849624634 and batch: 700, loss is 4.944705238342285 and perplexity is 140.4294515323543
At time: 1525.2495539188385 and batch: 750, loss is 4.897696475982666 and perplexity is 133.98079596448733
At time: 1526.4562981128693 and batch: 800, loss is 4.910374631881714 and perplexity is 135.69023877713965
At time: 1527.6646518707275 and batch: 850, loss is 4.944196109771728 and perplexity is 140.3579730838357
At time: 1528.8733294010162 and batch: 900, loss is 4.903818283081055 and perplexity is 134.80351625032372
At time: 1530.0807588100433 and batch: 950, loss is 4.8984692192077635 and perplexity is 134.08436872928428
At time: 1531.2886633872986 and batch: 1000, loss is 4.8893770980834965 and perplexity is 132.87078280329072
At time: 1532.5019762516022 and batch: 1050, loss is 4.887426300048828 and perplexity is 132.61183140439564
At time: 1533.7106256484985 and batch: 1100, loss is 4.8452074909210205 and perplexity is 127.12965750702767
At time: 1534.9200060367584 and batch: 1150, loss is 4.888493318557739 and perplexity is 132.75340620103663
At time: 1536.1280584335327 and batch: 1200, loss is 4.926786365509034 and perplexity is 137.93552495495345
At time: 1537.335469007492 and batch: 1250, loss is 4.9651423358917235 and perplexity is 143.32894967538778
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028401395700274 and perplexity of 152.68872863274913
Finished 48 epochs...
Completing Train Step...
At time: 1540.275408744812 and batch: 50, loss is 4.95486011505127 and perplexity is 141.8627605199407
At time: 1541.504557609558 and batch: 100, loss is 4.98044976234436 and perplexity is 145.53982526848048
At time: 1542.7119555473328 and batch: 150, loss is 4.9106661987304685 and perplexity is 135.72980732062408
At time: 1543.9207916259766 and batch: 200, loss is 4.95054669380188 and perplexity is 141.25216449984944
At time: 1545.1289236545563 and batch: 250, loss is 4.95419397354126 and perplexity is 141.7682913148566
At time: 1546.3363461494446 and batch: 300, loss is 4.951954679489136 and perplexity is 141.4511856023123
At time: 1547.5430126190186 and batch: 350, loss is 4.969430732727051 and perplexity is 143.9449209101399
At time: 1548.7798223495483 and batch: 400, loss is 4.924542331695557 and perplexity is 137.62634001335323
At time: 1549.9976675510406 and batch: 450, loss is 4.8938875579833985 and perplexity is 133.4714447534791
At time: 1551.212292432785 and batch: 500, loss is 4.890280752182007 and perplexity is 132.99090629761292
At time: 1552.4192645549774 and batch: 550, loss is 4.882007083892822 and perplexity is 131.89512297813062
At time: 1553.6256189346313 and batch: 600, loss is 4.9172492027282715 and perplexity is 136.62626463849097
At time: 1554.8308081626892 and batch: 650, loss is 4.934716920852662 and perplexity is 139.03377939717697
At time: 1556.0382359027863 and batch: 700, loss is 4.944705867767334 and perplexity is 140.42953992219654
At time: 1557.2463386058807 and batch: 750, loss is 4.897696933746338 and perplexity is 133.98085729604244
At time: 1558.4551479816437 and batch: 800, loss is 4.910376329421997 and perplexity is 135.69046911698143
At time: 1559.661238193512 and batch: 850, loss is 4.944196014404297 and perplexity is 140.357959698257
At time: 1560.8681399822235 and batch: 900, loss is 4.903818302154541 and perplexity is 134.8035188214967
At time: 1562.0751857757568 and batch: 950, loss is 4.898467321395874 and perplexity is 134.0841142626165
At time: 1563.2828483581543 and batch: 1000, loss is 4.889376630783081 and perplexity is 132.87072071273326
At time: 1564.4908213615417 and batch: 1050, loss is 4.8874281215667725 and perplexity is 132.61207295944618
At time: 1565.6991579532623 and batch: 1100, loss is 4.84521294593811 and perplexity is 127.13035100337349
At time: 1566.9117045402527 and batch: 1150, loss is 4.888498229980469 and perplexity is 132.75405821073446
At time: 1568.1187427043915 and batch: 1200, loss is 4.926790361404419 and perplexity is 137.9360761319823
At time: 1569.3299560546875 and batch: 1250, loss is 4.965144138336182 and perplexity is 143.3292080180917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0283978316035585 and perplexity of 152.6881844363227
Finished 49 epochs...
Completing Train Step...
At time: 1572.322420835495 and batch: 50, loss is 4.954854850769043 and perplexity is 141.8620137162975
At time: 1573.539918422699 and batch: 100, loss is 4.980443382263184 and perplexity is 145.53889671554302
At time: 1574.750118970871 and batch: 150, loss is 4.910658531188965 and perplexity is 135.72876661068298
At time: 1575.9604103565216 and batch: 200, loss is 4.950538282394409 and perplexity is 141.25097637533457
At time: 1577.1711049079895 and batch: 250, loss is 4.95418794631958 and perplexity is 141.7674368485127
At time: 1578.382581949234 and batch: 300, loss is 4.951951313018799 and perplexity is 141.45070941189337
At time: 1579.6176962852478 and batch: 350, loss is 4.969425811767578 and perplexity is 143.94421256476065
At time: 1580.827471256256 and batch: 400, loss is 4.9245396423339844 and perplexity is 137.62596988686073
At time: 1582.0378217697144 and batch: 450, loss is 4.893883819580078 and perplexity is 133.47094578431955
At time: 1583.2488503456116 and batch: 500, loss is 4.890278482437134 and perplexity is 132.99060444252777
At time: 1584.4678087234497 and batch: 550, loss is 4.882005777359009 and perplexity is 131.89495065280516
At time: 1585.6776070594788 and batch: 600, loss is 4.917249755859375 and perplexity is 136.62634021074842
At time: 1586.887146472931 and batch: 650, loss is 4.934718894958496 and perplexity is 139.03405386484297
At time: 1588.0970900058746 and batch: 700, loss is 4.944706659317017 and perplexity is 140.4296510791983
At time: 1589.3070197105408 and batch: 750, loss is 4.897697324752808 and perplexity is 133.98090968343473
At time: 1590.5171251296997 and batch: 800, loss is 4.910377435684204 and perplexity is 135.69061922630232
At time: 1591.7270154953003 and batch: 850, loss is 4.944196081161499 and perplexity is 140.3579690681619
At time: 1592.9375777244568 and batch: 900, loss is 4.903817720413208 and perplexity is 134.80344040074087
At time: 1594.1495008468628 and batch: 950, loss is 4.898465642929077 and perplexity is 134.08388920707165
At time: 1595.3626792430878 and batch: 1000, loss is 4.88937668800354 and perplexity is 132.87072831565706
At time: 1596.5768551826477 and batch: 1050, loss is 4.887430038452148 and perplexity is 132.61232716183312
At time: 1597.790804862976 and batch: 1100, loss is 4.845217905044556 and perplexity is 127.13098145787981
At time: 1599.0050444602966 and batch: 1150, loss is 4.888503103256226 and perplexity is 132.7547051594444
At time: 1600.2182593345642 and batch: 1200, loss is 4.926794099807739 and perplexity is 137.93659179363118
At time: 1601.432415008545 and batch: 1250, loss is 4.9651456069946285 and perplexity is 143.32941851989824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.028395158531022 and perplexity of 152.6877762902757
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f71ae834898>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 8.0, 'data': 'wikitext', 'dropout': 1.0, 'tune_wordvecs': True, 'lr': 2.473495794700573, 'batch_size': 50, 'wordvec_dim': 200}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6595799922943115 and batch: 50, loss is 8.527824125289918 and perplexity is 5053.438215150674
At time: 2.884838104248047 and batch: 100, loss is 7.300923385620117 and perplexity is 1481.6674465276342
At time: 4.085104465484619 and batch: 150, loss is 6.972896795272828 and perplexity is 1067.310055457327
At time: 5.284754991531372 and batch: 200, loss is 6.833815793991089 and perplexity is 928.7278941176166
At time: 6.485138893127441 and batch: 250, loss is 6.805434322357177 and perplexity is 902.7397648587203
At time: 7.683910846710205 and batch: 300, loss is 6.777865610122681 and perplexity is 878.1923175356679
At time: 8.883280515670776 and batch: 350, loss is 6.780139083862305 and perplexity is 880.1911359776059
At time: 10.07955288887024 and batch: 400, loss is 6.743151512145996 and perplexity is 848.2297342616556
At time: 11.274597644805908 and batch: 450, loss is 6.737092618942261 and perplexity is 843.1059388288086
At time: 12.472321510314941 and batch: 500, loss is 6.726848278045654 and perplexity is 834.512964011269
At time: 13.671028852462769 and batch: 550, loss is 6.740287666320801 and perplexity is 845.8040101861711
At time: 14.867679595947266 and batch: 600, loss is 6.753880004882813 and perplexity is 857.3789517047773
At time: 16.06660032272339 and batch: 650, loss is 6.729173374176026 and perplexity is 836.4555443424977
At time: 17.265939235687256 and batch: 700, loss is 6.742437801361084 and perplexity is 847.6245595377825
At time: 18.45971655845642 and batch: 750, loss is 6.711683778762818 and perplexity is 821.9534627242526
At time: 19.65651226043701 and batch: 800, loss is 6.724631996154785 and perplexity is 832.6654960525409
At time: 20.852940320968628 and batch: 850, loss is 6.7548772239685055 and perplexity is 858.234372809325
At time: 22.05072546005249 and batch: 900, loss is 6.7574011325836185 and perplexity is 860.4032137635155
At time: 23.24609112739563 and batch: 950, loss is 6.73666464805603 and perplexity is 842.7451912332223
At time: 24.440332412719727 and batch: 1000, loss is 6.732020349502563 and perplexity is 838.8403057066073
At time: 25.635382413864136 and batch: 1050, loss is 6.720025959014893 and perplexity is 828.8390270468541
At time: 26.83265733718872 and batch: 1100, loss is 6.7145117664337155 and perplexity is 824.2812268756888
At time: 28.02811908721924 and batch: 1150, loss is 6.756932039260864 and perplexity is 859.9996990114996
At time: 29.22323989868164 and batch: 1200, loss is 6.752921113967895 and perplexity is 856.5572128592664
At time: 30.421419858932495 and batch: 1250, loss is 6.748570032119751 and perplexity is 852.8383587049844
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.465284361456432 and perplexity of 642.4470246455141
Finished 1 epochs...
Completing Train Step...
At time: 33.408783197402954 and batch: 50, loss is 6.222772378921508 and perplexity is 504.09884956088933
At time: 34.6151385307312 and batch: 100, loss is 5.9305097770690915 and perplexity is 376.3463176547658
At time: 35.820919036865234 and batch: 150, loss is 5.716846961975097 and perplexity is 303.9450601779838
At time: 37.05308389663696 and batch: 200, loss is 5.6666419506073 and perplexity is 289.0622175573298
At time: 38.25872874259949 and batch: 250, loss is 5.644152278900147 and perplexity is 282.63385995272785
At time: 39.46467113494873 and batch: 300, loss is 5.606266326904297 and perplexity is 272.1263082031505
At time: 40.67075276374817 and batch: 350, loss is 5.598689823150635 and perplexity is 270.0723330083878
At time: 41.8774676322937 and batch: 400, loss is 5.536008262634278 and perplexity is 253.66341809025954
At time: 43.08435082435608 and batch: 450, loss is 5.488643398284912 and perplexity is 241.9287831360988
At time: 44.290915727615356 and batch: 500, loss is 5.449503288269043 and perplexity is 232.64258090476963
At time: 45.49651527404785 and batch: 550, loss is 5.446705989837646 and perplexity is 231.99271952998782
At time: 46.702428102493286 and batch: 600, loss is 5.449601001739502 and perplexity is 232.6653143293892
At time: 47.90867495536804 and batch: 650, loss is 5.409771604537964 and perplexity is 223.58051707312134
At time: 49.11412310600281 and batch: 700, loss is 5.399217910766602 and perplexity is 221.23332432533877
At time: 50.320815324783325 and batch: 750, loss is 5.350156831741333 and perplexity is 210.64133052298482
At time: 51.535555601119995 and batch: 800, loss is 5.342780599594116 and perplexity is 209.09330747676117
At time: 52.74184966087341 and batch: 850, loss is 5.383209114074707 and perplexity is 217.71984335241217
At time: 53.94823503494263 and batch: 900, loss is 5.349464111328125 and perplexity is 210.49546550113826
At time: 55.154075384140015 and batch: 950, loss is 5.307855787277222 and perplexity is 201.9168112892678
At time: 56.36016011238098 and batch: 1000, loss is 5.285357694625855 and perplexity is 197.42478860575653
At time: 57.56634831428528 and batch: 1050, loss is 5.264188642501831 and perplexity is 193.28941833002523
At time: 58.77365183830261 and batch: 1100, loss is 5.224887580871582 and perplexity is 185.84027734957508
At time: 59.98213338851929 and batch: 1150, loss is 5.251549510955811 and perplexity is 190.86178187092267
At time: 61.19040775299072 and batch: 1200, loss is 5.252538623809815 and perplexity is 191.0506591077618
At time: 62.39836406707764 and batch: 1250, loss is 5.24196078300476 and perplexity is 189.04040646027642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.9298011055828015 and perplexity of 138.35199216439693
Finished 2 epochs...
Completing Train Step...
At time: 65.40102171897888 and batch: 50, loss is 5.1778232192993165 and perplexity is 177.2964551380938
At time: 66.60642576217651 and batch: 100, loss is 5.185984764099121 and perplexity is 178.74938912738833
At time: 67.81063508987427 and batch: 150, loss is 5.058829431533813 and perplexity is 157.4061537599419
At time: 69.01607203483582 and batch: 200, loss is 5.083834381103515 and perplexity is 161.3917083311944
At time: 70.22147250175476 and batch: 250, loss is 5.0984382152557375 and perplexity is 163.7659403220454
At time: 71.4269859790802 and batch: 300, loss is 5.099257869720459 and perplexity is 163.90022683295058
At time: 72.63201785087585 and batch: 350, loss is 5.100276861190796 and perplexity is 164.067324887367
At time: 73.83668661117554 and batch: 400, loss is 5.076844472885131 and perplexity is 160.2675286363655
At time: 75.04141736030579 and batch: 450, loss is 5.036143808364868 and perplexity is 153.87549607229573
At time: 76.24672555923462 and batch: 500, loss is 5.026532926559448 and perplexity is 152.4037008209058
At time: 77.45677161216736 and batch: 550, loss is 5.033058433532715 and perplexity is 153.40146414855974
At time: 78.66230034828186 and batch: 600, loss is 5.042565622329712 and perplexity is 154.86683557387173
At time: 79.86724400520325 and batch: 650, loss is 5.033136615753174 and perplexity is 153.4134578844908
At time: 81.07227373123169 and batch: 700, loss is 5.026713886260986 and perplexity is 152.4312822446075
At time: 82.27762937545776 and batch: 750, loss is 5.007830810546875 and perplexity is 149.5799168011372
At time: 83.48371052742004 and batch: 800, loss is 5.0168109798431395 and perplexity is 150.92921919159485
At time: 84.68882513046265 and batch: 850, loss is 5.06059552192688 and perplexity is 157.6843928813057
At time: 85.89322972297668 and batch: 900, loss is 5.028802947998047 and perplexity is 152.75005345431313
At time: 87.09769058227539 and batch: 950, loss is 5.000847244262696 and perplexity is 148.53895458235917
At time: 88.30242419242859 and batch: 1000, loss is 4.989524021148681 and perplexity is 146.86650150763631
At time: 89.51463842391968 and batch: 1050, loss is 4.975992240905762 and perplexity is 144.89252213277734
At time: 90.72007989883423 and batch: 1100, loss is 4.9349682426452635 and perplexity is 139.06872600708567
At time: 91.93402743339539 and batch: 1150, loss is 4.963659963607788 and perplexity is 143.1166402126129
At time: 93.165851354599 and batch: 1200, loss is 4.970964870452881 and perplexity is 144.16592172324698
At time: 94.37011313438416 and batch: 1250, loss is 4.977383346557617 and perplexity is 145.09422320044817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.790138411695939 and perplexity of 120.31802093208377
Finished 3 epochs...
Completing Train Step...
At time: 97.332190990448 and batch: 50, loss is 4.921698532104492 and perplexity is 137.23551426234928
At time: 98.56108546257019 and batch: 100, loss is 4.940124893188477 and perplexity is 139.78770700225982
At time: 99.7681393623352 and batch: 150, loss is 4.82131217956543 and perplexity is 124.1278620079564
At time: 100.97449445724487 and batch: 200, loss is 4.867159032821656 and perplexity is 129.9512048674392
At time: 102.18078374862671 and batch: 250, loss is 4.881204252243042 and perplexity is 131.78927589333216
At time: 103.38748693466187 and batch: 300, loss is 4.886132307052613 and perplexity is 132.44034359928094
At time: 104.59243988990784 and batch: 350, loss is 4.882686796188355 and perplexity is 131.9848041901981
At time: 105.79712271690369 and batch: 400, loss is 4.871112804412842 and perplexity is 130.46601930819514
At time: 107.00235247612 and batch: 450, loss is 4.827333793640137 and perplexity is 124.87756704072832
At time: 108.2155122756958 and batch: 500, loss is 4.828859605789185 and perplexity is 125.06825218752483
At time: 109.42900252342224 and batch: 550, loss is 4.837563591003418 and perplexity is 126.16159572575543
At time: 110.64283108711243 and batch: 600, loss is 4.846189355850219 and perplexity is 127.25454295948478
At time: 111.85546922683716 and batch: 650, loss is 4.84625319480896 and perplexity is 127.26266701631523
At time: 113.0691442489624 and batch: 700, loss is 4.842360849380493 and perplexity is 126.76827954372777
At time: 114.28306293487549 and batch: 750, loss is 4.831170072555542 and perplexity is 125.35755230819117
At time: 115.49810671806335 and batch: 800, loss is 4.845225887298584 and perplexity is 127.13199625371887
At time: 116.71431374549866 and batch: 850, loss is 4.8877339553833 and perplexity is 132.65263641835958
At time: 117.9317512512207 and batch: 900, loss is 4.859179744720459 and perplexity is 128.91841271749524
At time: 119.14950776100159 and batch: 950, loss is 4.8360592555999755 and perplexity is 125.97194905261725
At time: 120.37321019172668 and batch: 1000, loss is 4.826870250701904 and perplexity is 124.81969434065986
At time: 121.58948731422424 and batch: 1050, loss is 4.814224157333374 and perplexity is 123.25115170398432
At time: 122.80434083938599 and batch: 1100, loss is 4.767320203781128 and perplexity is 117.60366545115498
At time: 124.0780897140503 and batch: 1150, loss is 4.7939864444732665 and perplexity is 120.78190055993748
At time: 125.29333925247192 and batch: 1200, loss is 4.807654657363892 and perplexity is 122.44410710890367
At time: 126.5076014995575 and batch: 1250, loss is 4.815120668411255 and perplexity is 123.36169727210759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.720112375969435 and perplexity of 112.18085839221952
Finished 4 epochs...
Completing Train Step...
At time: 129.47554683685303 and batch: 50, loss is 4.760080995559693 and perplexity is 116.75538218355331
At time: 130.68038415908813 and batch: 100, loss is 4.786952896118164 and perplexity is 119.93535581854907
At time: 131.8849081993103 and batch: 150, loss is 4.674421987533569 and perplexity is 107.17060324112224
At time: 133.08943629264832 and batch: 200, loss is 4.727818965911865 and perplexity is 113.04873013772263
At time: 134.2940366268158 and batch: 250, loss is 4.739476728439331 and perplexity is 114.37433718091252
At time: 135.50052976608276 and batch: 300, loss is 4.750286979675293 and perplexity is 115.61745962769233
At time: 136.70549368858337 and batch: 350, loss is 4.74307354927063 and perplexity is 114.78646190453752
At time: 137.91674971580505 and batch: 400, loss is 4.737082509994507 and perplexity is 114.10082758473233
At time: 139.12267398834229 and batch: 450, loss is 4.687134304046631 and perplexity is 108.54168622664841
At time: 140.3273630142212 and batch: 500, loss is 4.6952399635314945 and perplexity is 109.42506351625597
At time: 141.53258991241455 and batch: 550, loss is 4.707681150436401 and perplexity is 110.79494498976798
At time: 142.7378900051117 and batch: 600, loss is 4.715399618148804 and perplexity is 111.65342099307253
At time: 143.94215989112854 and batch: 650, loss is 4.721083517074585 and perplexity is 112.28985475187152
At time: 145.1466839313507 and batch: 700, loss is 4.716956586837768 and perplexity is 111.82739727624187
At time: 146.35194420814514 and batch: 750, loss is 4.706574993133545 and perplexity is 110.67245611072165
At time: 147.55675220489502 and batch: 800, loss is 4.727073335647583 and perplexity is 112.96446900092401
At time: 148.76228213310242 and batch: 850, loss is 4.768692255020142 and perplexity is 117.76513445259077
At time: 149.96728348731995 and batch: 900, loss is 4.738966178894043 and perplexity is 114.31595831899102
At time: 151.1721794605255 and batch: 950, loss is 4.720922746658325 and perplexity is 112.27180331628858
At time: 152.37653470039368 and batch: 1000, loss is 4.707993669509888 and perplexity is 110.82957593445668
At time: 153.60683512687683 and batch: 1050, loss is 4.699804344177246 and perplexity is 109.92566275195557
At time: 154.82033944129944 and batch: 1100, loss is 4.644496994018555 and perplexity is 104.01103445256604
At time: 156.03647446632385 and batch: 1150, loss is 4.672013301849365 and perplexity is 106.91277358328631
At time: 157.2417070865631 and batch: 1200, loss is 4.689585380554199 and perplexity is 108.8080565174737
At time: 158.44609189033508 and batch: 1250, loss is 4.701166143417359 and perplexity is 110.07546141065976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.678592876796305 and perplexity of 107.61853344373668
Finished 5 epochs...
Completing Train Step...
At time: 161.4103856086731 and batch: 50, loss is 4.647928581237793 and perplexity is 104.36857049625647
At time: 162.61530590057373 and batch: 100, loss is 4.67653959274292 and perplexity is 107.39778872852426
At time: 163.82060408592224 and batch: 150, loss is 4.568582429885864 and perplexity is 96.40734868525192
At time: 165.0335991382599 and batch: 200, loss is 4.62622896194458 and perplexity is 102.12820767043992
At time: 166.24048686027527 and batch: 250, loss is 4.634763126373291 and perplexity is 103.00351628881593
At time: 167.44635844230652 and batch: 300, loss is 4.645340423583985 and perplexity is 104.09879743990074
At time: 168.65206360816956 and batch: 350, loss is 4.637687950134278 and perplexity is 103.30522442720613
At time: 169.8597071170807 and batch: 400, loss is 4.634551992416382 and perplexity is 102.98177104450686
At time: 171.07199954986572 and batch: 450, loss is 4.580771980285644 and perplexity is 97.58970246145223
At time: 172.27894592285156 and batch: 500, loss is 4.595347995758057 and perplexity is 99.02258899341523
At time: 173.4842963218689 and batch: 550, loss is 4.605882434844971 and perplexity is 100.07125025663278
At time: 174.690936088562 and batch: 600, loss is 4.614965839385986 and perplexity is 100.98437878517343
At time: 175.89707946777344 and batch: 650, loss is 4.624882488250733 and perplexity is 101.99078726265574
At time: 177.10534477233887 and batch: 700, loss is 4.619444618225097 and perplexity is 101.43767984380422
At time: 178.31084871292114 and batch: 750, loss is 4.611509170532226 and perplexity is 100.63591184269684
At time: 179.51521134376526 and batch: 800, loss is 4.635901412963867 and perplexity is 103.1208305661571
At time: 180.72644543647766 and batch: 850, loss is 4.675301742553711 and perplexity is 107.26492860285676
At time: 181.9301679134369 and batch: 900, loss is 4.642405185699463 and perplexity is 103.79369070540193
At time: 183.14108872413635 and batch: 950, loss is 4.630304555892945 and perplexity is 102.54529012762045
At time: 184.3874831199646 and batch: 1000, loss is 4.615047779083252 and perplexity is 100.99265375361921
At time: 185.59101915359497 and batch: 1050, loss is 4.607003707885742 and perplexity is 100.18352038267192
At time: 186.79474091529846 and batch: 1100, loss is 4.550491037368775 and perplexity is 94.67888777434567
At time: 187.99867057800293 and batch: 1150, loss is 4.577003383636475 and perplexity is 97.22261836616205
At time: 189.20508289337158 and batch: 1200, loss is 4.598663272857666 and perplexity is 99.35142109863874
At time: 190.4089195728302 and batch: 1250, loss is 4.611559286117553 and perplexity is 100.64095539670303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.652779015311359 and perplexity of 104.87603307879151
Finished 6 epochs...
Completing Train Step...
At time: 193.3589768409729 and batch: 50, loss is 4.559967079162598 and perplexity is 95.58033319366368
At time: 194.58862280845642 and batch: 100, loss is 4.588629217147827 and perplexity is 98.35950818154137
At time: 195.79712009429932 and batch: 150, loss is 4.483092002868652 and perplexity is 88.50791675472195
At time: 197.00626397132874 and batch: 200, loss is 4.539494752883911 and perplexity is 93.64347506933883
At time: 198.21656107902527 and batch: 250, loss is 4.5491814708709715 and perplexity is 94.554980624901
At time: 199.4261724948883 and batch: 300, loss is 4.5598783111572265 and perplexity is 95.57184909469721
At time: 200.63416719436646 and batch: 350, loss is 4.552384643554688 and perplexity is 94.85834215625124
At time: 201.84307026863098 and batch: 400, loss is 4.553637218475342 and perplexity is 94.97723378144046
At time: 203.05214738845825 and batch: 450, loss is 4.495393600463867 and perplexity is 89.60342999692575
At time: 204.26100373268127 and batch: 500, loss is 4.514040603637695 and perplexity is 91.28994075835104
At time: 205.47050309181213 and batch: 550, loss is 4.52338755607605 and perplexity is 92.14722374295583
At time: 206.6782464981079 and batch: 600, loss is 4.535234508514404 and perplexity is 93.24537957608413
At time: 207.88613986968994 and batch: 650, loss is 4.544473123550415 and perplexity is 94.11082936494279
At time: 209.09409403800964 and batch: 700, loss is 4.539632444381714 and perplexity is 93.65636986741224
At time: 210.3155255317688 and batch: 750, loss is 4.532974786758423 and perplexity is 93.03490885540592
At time: 211.5372233390808 and batch: 800, loss is 4.558112468719482 and perplexity is 95.40323318609049
At time: 212.7504539489746 and batch: 850, loss is 4.598430633544922 and perplexity is 99.3283107406073
At time: 213.96743178367615 and batch: 900, loss is 4.562939548492432 and perplexity is 95.86486547485632
At time: 215.20537996292114 and batch: 950, loss is 4.553927431106567 and perplexity is 95.00480137440103
At time: 216.42491698265076 and batch: 1000, loss is 4.537967367172241 and perplexity is 93.50055453870003
At time: 217.64802026748657 and batch: 1050, loss is 4.530949869155884 and perplexity is 92.8467114372517
At time: 218.8832266330719 and batch: 1100, loss is 4.472414960861206 and perplexity is 87.56794101943797
At time: 220.0915002822876 and batch: 1150, loss is 4.498578748703003 and perplexity is 89.88928520814157
At time: 221.2996165752411 and batch: 1200, loss is 4.524212970733642 and perplexity is 92.22331481111108
At time: 222.50814390182495 and batch: 1250, loss is 4.535609083175659 and perplexity is 93.28031347482066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.633049957943658 and perplexity of 102.8272049851704
Finished 7 epochs...
Completing Train Step...
At time: 225.48304986953735 and batch: 50, loss is 4.485913209915161 and perplexity is 88.75796847112319
At time: 226.69179844856262 and batch: 100, loss is 4.515378084182739 and perplexity is 91.41212096668583
At time: 227.89615988731384 and batch: 150, loss is 4.413728857040406 and perplexity is 82.57680723670164
At time: 229.10103344917297 and batch: 200, loss is 4.468925447463989 and perplexity is 87.26290404092417
At time: 230.30597352981567 and batch: 250, loss is 4.476777105331421 and perplexity is 87.95075937658204
At time: 231.51057124137878 and batch: 300, loss is 4.490936183929444 and perplexity is 89.20491901106499
At time: 232.71508049964905 and batch: 350, loss is 4.4814575672149655 and perplexity is 88.3633744146988
At time: 233.9205515384674 and batch: 400, loss is 4.484514646530151 and perplexity is 88.63392159022389
At time: 235.12629747390747 and batch: 450, loss is 4.422108612060547 and perplexity is 83.27168805096329
At time: 236.33197450637817 and batch: 500, loss is 4.443723115921021 and perplexity is 85.0911568965721
At time: 237.53684091567993 and batch: 550, loss is 4.452852210998535 and perplexity is 85.87151873722574
At time: 238.7418930530548 and batch: 600, loss is 4.4675382804870605 and perplexity is 87.14193974036623
At time: 239.9489552974701 and batch: 650, loss is 4.477277879714966 and perplexity is 87.9948138936573
At time: 241.1586730480194 and batch: 700, loss is 4.472369441986084 and perplexity is 87.56395511598357
At time: 242.36398124694824 and batch: 750, loss is 4.467221326828003 and perplexity is 87.11432416036892
At time: 243.5711386203766 and batch: 800, loss is 4.492131118774414 and perplexity is 89.3115767889906
At time: 244.77678394317627 and batch: 850, loss is 4.533282861709595 and perplexity is 93.06357499584202
At time: 246.0392951965332 and batch: 900, loss is 4.495142135620117 and perplexity is 89.58090071718178
At time: 247.24531054496765 and batch: 950, loss is 4.487328796386719 and perplexity is 88.88370202289506
At time: 248.45101165771484 and batch: 1000, loss is 4.472863788604736 and perplexity is 87.60725276225178
At time: 249.65694665908813 and batch: 1050, loss is 4.465634813308716 and perplexity is 86.97622568388134
At time: 250.8623149394989 and batch: 1100, loss is 4.405963459014893 and perplexity is 81.93804877750439
At time: 252.06919193267822 and batch: 1150, loss is 4.432059936523437 and perplexity is 84.10448849846887
At time: 253.27458119392395 and batch: 1200, loss is 4.458836069107056 and perplexity is 86.38690217374898
At time: 254.4796028137207 and batch: 1250, loss is 4.470790500640869 and perplexity is 87.42580586038733
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.623877309534672 and perplexity of 101.88831980174807
Finished 8 epochs...
Completing Train Step...
At time: 257.44391322135925 and batch: 50, loss is 4.424707221984863 and perplexity is 83.48836008707069
At time: 258.68432998657227 and batch: 100, loss is 4.452239198684692 and perplexity is 85.81889457011061
At time: 259.8951737880707 and batch: 150, loss is 4.352225637435913 and perplexity is 77.651093928576
At time: 261.10780024528503 and batch: 200, loss is 4.407975568771362 and perplexity is 82.10308310278407
At time: 262.3185844421387 and batch: 250, loss is 4.417716932296753 and perplexity is 82.9067873137267
At time: 263.53242111206055 and batch: 300, loss is 4.4313279724121095 and perplexity is 84.04294955618259
At time: 264.7487404346466 and batch: 350, loss is 4.421100358963013 and perplexity is 83.18777142525691
At time: 265.9656894207001 and batch: 400, loss is 4.424344997406006 and perplexity is 83.45812402745128
At time: 267.18263936042786 and batch: 450, loss is 4.36052339553833 and perplexity is 78.29810457894688
At time: 268.40102100372314 and batch: 500, loss is 4.385349731445313 and perplexity is 80.2662899502989
At time: 269.6169445514679 and batch: 550, loss is 4.39382435798645 and perplexity is 80.94940727516412
At time: 270.83774423599243 and batch: 600, loss is 4.408507528305054 and perplexity is 82.14677023945032
At time: 272.054892539978 and batch: 650, loss is 4.420721845626831 and perplexity is 83.15628970286694
At time: 273.2763023376465 and batch: 700, loss is 4.414447011947632 and perplexity is 82.6361314754887
At time: 274.492258310318 and batch: 750, loss is 4.408870725631714 and perplexity is 82.17661114553567
At time: 275.73127913475037 and batch: 800, loss is 4.436567239761352 and perplexity is 84.48442854106192
At time: 276.9406328201294 and batch: 850, loss is 4.47628007888794 and perplexity is 87.9070563851185
At time: 278.15008425712585 and batch: 900, loss is 4.436657228469849 and perplexity is 84.49203152776073
At time: 279.3597550392151 and batch: 950, loss is 4.4298194408416744 and perplexity is 83.91626369234174
At time: 280.5685770511627 and batch: 1000, loss is 4.41668586730957 and perplexity is 82.82134908188148
At time: 281.77692675590515 and batch: 1050, loss is 4.40851526260376 and perplexity is 82.14740558956603
At time: 282.9867522716522 and batch: 1100, loss is 4.34879623413086 and perplexity is 77.38525310865143
At time: 284.20857095718384 and batch: 1150, loss is 4.37330545425415 and perplexity is 79.30533910086558
At time: 285.4245901107788 and batch: 1200, loss is 4.401523647308349 and perplexity is 81.57506565394924
At time: 286.6340289115906 and batch: 1250, loss is 4.415867166519165 and perplexity is 82.75357092672614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.617563957715556 and perplexity of 101.2470892793912
Finished 9 epochs...
Completing Train Step...
At time: 289.60104751586914 and batch: 50, loss is 4.368651485443115 and perplexity is 78.93711204938005
At time: 290.81958651542664 and batch: 100, loss is 4.396079931259155 and perplexity is 81.13220066910378
At time: 292.0345411300659 and batch: 150, loss is 4.297468128204346 and perplexity is 73.5134312929972
At time: 293.23907351493835 and batch: 200, loss is 4.355893535614014 and perplexity is 77.93643321234332
At time: 294.4440186023712 and batch: 250, loss is 4.363175754547119 and perplexity is 78.50605491956661
At time: 295.6492247581482 and batch: 300, loss is 4.376750535964966 and perplexity is 79.57902363627842
At time: 296.8536114692688 and batch: 350, loss is 4.365340032577515 and perplexity is 78.676147847281
At time: 298.058162689209 and batch: 400, loss is 4.374494590759277 and perplexity is 79.39970006755777
At time: 299.26335859298706 and batch: 450, loss is 4.30418360710144 and perplexity is 74.00877054810367
At time: 300.4692180156708 and batch: 500, loss is 4.33325608253479 and perplexity is 76.19197045505963
At time: 301.67431640625 and batch: 550, loss is 4.341130790710449 and perplexity is 76.79432857218242
At time: 302.88090205192566 and batch: 600, loss is 4.3563150215148925 and perplexity is 77.9692892437977
At time: 304.0855712890625 and batch: 650, loss is 4.3720394897460935 and perplexity is 79.2050048794463
At time: 305.3003966808319 and batch: 700, loss is 4.362980051040649 and perplexity is 78.49069251262723
At time: 306.5501890182495 and batch: 750, loss is 4.356442594528199 and perplexity is 77.97923665546902
At time: 307.7553744316101 and batch: 800, loss is 4.386330895423889 and perplexity is 80.34508299081756
At time: 308.96023082733154 and batch: 850, loss is 4.425534181594848 and perplexity is 83.55743014390403
At time: 310.16534543037415 and batch: 900, loss is 4.384277238845825 and perplexity is 80.18025109460395
At time: 311.37122321128845 and batch: 950, loss is 4.382467002868652 and perplexity is 80.03523721368629
At time: 312.57499861717224 and batch: 1000, loss is 4.364522132873535 and perplexity is 78.61182495766845
At time: 313.78077602386475 and batch: 1050, loss is 4.357358694076538 and perplexity is 78.05070613052754
At time: 314.9862251281738 and batch: 1100, loss is 4.297517337799072 and perplexity is 73.51704894816899
At time: 316.1920871734619 and batch: 1150, loss is 4.3207197093963625 and perplexity is 75.24276173214054
At time: 317.39855194091797 and batch: 1200, loss is 4.354477396011353 and perplexity is 77.82614245476788
At time: 318.6028447151184 and batch: 1250, loss is 4.364128160476684 and perplexity is 78.58086016860565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.613255855810904 and perplexity of 100.81184471336675
Finished 10 epochs...
Completing Train Step...
At time: 321.54442143440247 and batch: 50, loss is 4.31804368019104 and perplexity is 75.04167907578501
At time: 322.7768919467926 and batch: 100, loss is 4.347922153472901 and perplexity is 77.31764170891095
At time: 323.98370456695557 and batch: 150, loss is 4.249892702102661 and perplexity is 70.09789058689272
At time: 325.1904079914093 and batch: 200, loss is 4.310151681900025 and perplexity is 74.451781071531
At time: 326.3962125778198 and batch: 250, loss is 4.317219457626343 and perplexity is 74.97985351310999
At time: 327.6043016910553 and batch: 300, loss is 4.329965887069702 and perplexity is 75.94169593122042
At time: 328.8109407424927 and batch: 350, loss is 4.317078876495361 and perplexity is 74.96931350138301
At time: 330.0175700187683 and batch: 400, loss is 4.328374109268188 and perplexity is 75.82090978328588
At time: 331.22392416000366 and batch: 450, loss is 4.256463851928711 and perplexity is 70.56003106264798
At time: 332.43008399009705 and batch: 500, loss is 4.285222816467285 and perplexity is 72.61872556512003
At time: 333.63582491874695 and batch: 550, loss is 4.292585744857788 and perplexity is 73.15538531042334
At time: 334.8432152271271 and batch: 600, loss is 4.308507194519043 and perplexity is 74.32944667332379
At time: 336.05057430267334 and batch: 650, loss is 4.327178192138672 and perplexity is 75.73028845709938
At time: 337.2828323841095 and batch: 700, loss is 4.316558246612549 and perplexity is 74.93029239513865
At time: 338.4910283088684 and batch: 750, loss is 4.309309930801391 and perplexity is 74.38913757183498
At time: 339.6990487575531 and batch: 800, loss is 4.340998106002807 and perplexity is 76.78413981510838
At time: 340.90647077560425 and batch: 850, loss is 4.379939708709717 and perplexity is 79.83322001214115
At time: 342.11451292037964 and batch: 900, loss is 4.336752157211304 and perplexity is 76.45880944625375
At time: 343.3255820274353 and batch: 950, loss is 4.335539026260376 and perplexity is 76.36611113700597
At time: 344.53294825553894 and batch: 1000, loss is 4.318003807067871 and perplexity is 75.03868698932477
At time: 345.7394540309906 and batch: 1050, loss is 4.311156530380249 and perplexity is 74.52663143092747
At time: 346.94680643081665 and batch: 1100, loss is 4.253850660324097 and perplexity is 70.37588489125929
At time: 348.15392804145813 and batch: 1150, loss is 4.275778617858887 and perplexity is 71.93612826399608
At time: 349.3604848384857 and batch: 1200, loss is 4.306328630447387 and perplexity is 74.16769147234594
At time: 350.56712198257446 and batch: 1250, loss is 4.317853116989136 and perplexity is 75.02738025560186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6166216996464415 and perplexity of 101.15173332455741
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 353.51598620414734 and batch: 50, loss is 4.310408735275269 and perplexity is 74.47092161311339
At time: 354.7479512691498 and batch: 100, loss is 4.363741598129272 and perplexity is 78.55048963726736
At time: 355.9562277793884 and batch: 150, loss is 4.272594308853149 and perplexity is 71.70742572595317
At time: 357.16597962379456 and batch: 200, loss is 4.340035915374756 and perplexity is 76.71029436779119
At time: 358.3758795261383 and batch: 250, loss is 4.347987184524536 and perplexity is 77.32266991995441
At time: 359.58153796195984 and batch: 300, loss is 4.3449454021453855 and perplexity is 77.08782853434863
At time: 360.78635334968567 and batch: 350, loss is 4.325847053527832 and perplexity is 75.62954801074842
At time: 361.9913172721863 and batch: 400, loss is 4.322379083633423 and perplexity is 75.367721281139
At time: 363.20026564598083 and batch: 450, loss is 4.251050157546997 and perplexity is 70.17907274529124
At time: 364.4049892425537 and batch: 500, loss is 4.268884944915771 and perplexity is 71.4419295024036
At time: 365.61046600341797 and batch: 550, loss is 4.260120949745178 and perplexity is 70.8185484216863
At time: 366.8169288635254 and batch: 600, loss is 4.268921546936035 and perplexity is 71.44454446921115
At time: 368.0649199485779 and batch: 650, loss is 4.2895310115814205 and perplexity is 72.93225609419987
At time: 369.2697911262512 and batch: 700, loss is 4.268961563110351 and perplexity is 71.44740346375922
At time: 370.4750998020172 and batch: 750, loss is 4.256598773002625 and perplexity is 70.56955174006994
At time: 371.6793348789215 and batch: 800, loss is 4.2726064300537105 and perplexity is 71.70829491130992
At time: 372.88545989990234 and batch: 850, loss is 4.300057287216187 and perplexity is 73.7040158765476
At time: 374.0904207229614 and batch: 900, loss is 4.245544281005859 and perplexity is 69.79373721336002
At time: 375.2956154346466 and batch: 950, loss is 4.237576246261597 and perplexity is 69.23982800484006
At time: 376.50099444389343 and batch: 1000, loss is 4.206192388534546 and perplexity is 67.10055992210478
At time: 377.7071375846863 and batch: 1050, loss is 4.188447780609131 and perplexity is 65.92038860006714
At time: 378.91274666786194 and batch: 1100, loss is 4.115415692329407 and perplexity is 61.27768163130832
At time: 380.1186294555664 and batch: 1150, loss is 4.125352072715759 and perplexity is 61.88959505285701
At time: 381.32326078414917 and batch: 1200, loss is 4.139980869293213 and perplexity is 62.80161999833147
At time: 382.52792716026306 and batch: 1250, loss is 4.135711884498596 and perplexity is 62.534092279653315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.557411360914689 and perplexity of 95.33636867705476
Finished 12 epochs...
Completing Train Step...
At time: 385.49630522727966 and batch: 50, loss is 4.2678129100799564 and perplexity is 71.3653823032023
At time: 386.700074672699 and batch: 100, loss is 4.308031482696533 and perplexity is 74.29409568588866
At time: 387.90512228012085 and batch: 150, loss is 4.21547037601471 and perplexity is 67.72601507285646
At time: 389.10872864723206 and batch: 200, loss is 4.2777618980407714 and perplexity is 72.07893933191288
At time: 390.3125922679901 and batch: 250, loss is 4.283530068397522 and perplexity is 72.49590433961485
At time: 391.5293724536896 and batch: 300, loss is 4.287231273651123 and perplexity is 72.76472373261815
At time: 392.7371242046356 and batch: 350, loss is 4.272284183502197 and perplexity is 71.68519088335646
At time: 393.9410080909729 and batch: 400, loss is 4.273258352279663 and perplexity is 71.75505838396516
At time: 395.1472659111023 and batch: 450, loss is 4.202746748924255 and perplexity is 66.86975344129362
At time: 396.3512797355652 and batch: 500, loss is 4.224364404678345 and perplexity is 68.33105884207082
At time: 397.58104372024536 and batch: 550, loss is 4.219241447448731 and perplexity is 67.98189688469256
At time: 398.786495923996 and batch: 600, loss is 4.228719658851624 and perplexity is 68.62930697302917
At time: 399.9910011291504 and batch: 650, loss is 4.253010606765747 and perplexity is 70.31679020356091
At time: 401.19512367248535 and batch: 700, loss is 4.2344246768951415 and perplexity is 69.02195738148389
At time: 402.3991765975952 and batch: 750, loss is 4.226092596054077 and perplexity is 68.44925008778581
At time: 403.6045231819153 and batch: 800, loss is 4.246029539108276 and perplexity is 69.82761340852511
At time: 404.80961537361145 and batch: 850, loss is 4.277343254089356 and perplexity is 72.04877023543264
At time: 406.0149850845337 and batch: 900, loss is 4.225035624504089 and perplexity is 68.37693939972714
At time: 407.21965503692627 and batch: 950, loss is 4.221045970916748 and perplexity is 68.10468256452275
At time: 408.42502069473267 and batch: 1000, loss is 4.193058953285218 and perplexity is 66.22506080299407
At time: 409.62974786758423 and batch: 1050, loss is 4.181434135437012 and perplexity is 65.45966395304855
At time: 410.8341381549835 and batch: 1100, loss is 4.112336535453796 and perplexity is 61.08928823185712
At time: 412.04402351379395 and batch: 1150, loss is 4.126879930496216 and perplexity is 61.98422582496011
At time: 413.25109672546387 and batch: 1200, loss is 4.148893709182739 and perplexity is 63.36386264956301
At time: 414.4560458660126 and batch: 1250, loss is 4.150578188896179 and perplexity is 63.470687737849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.554245997519389 and perplexity of 95.03507153444785
Finished 13 epochs...
Completing Train Step...
At time: 417.4492027759552 and batch: 50, loss is 4.249867134094238 and perplexity is 70.09609834634784
At time: 418.68454480171204 and batch: 100, loss is 4.286645994186402 and perplexity is 72.72214849448295
At time: 419.8960511684418 and batch: 150, loss is 4.193508734703064 and perplexity is 66.25485430451842
At time: 421.1065676212311 and batch: 200, loss is 4.253481340408325 and perplexity is 70.34989847432495
At time: 422.3166756629944 and batch: 250, loss is 4.259044299125671 and perplexity is 70.74234261852943
At time: 423.5327482223511 and batch: 300, loss is 4.264243278503418 and perplexity is 71.1110883190024
At time: 424.74566102027893 and batch: 350, loss is 4.249438209533691 and perplexity is 70.06603885525645
At time: 425.9568221569061 and batch: 400, loss is 4.252291870117188 and perplexity is 70.26626910728913
At time: 427.1694700717926 and batch: 450, loss is 4.181913051605225 and perplexity is 65.49102115261702
At time: 428.4083676338196 and batch: 500, loss is 4.204948048591614 and perplexity is 67.01711594230609
At time: 429.6255066394806 and batch: 550, loss is 4.200247664451599 and perplexity is 66.70284891989282
At time: 430.8419153690338 and batch: 600, loss is 4.2105454111099245 and perplexity is 67.39328683546209
At time: 432.06581473350525 and batch: 650, loss is 4.236350054740906 and perplexity is 69.15497874619841
At time: 433.28352642059326 and batch: 700, loss is 4.2179740619659425 and perplexity is 67.8957921909335
At time: 434.50994300842285 and batch: 750, loss is 4.211676549911499 and perplexity is 67.46956112744036
At time: 435.736323595047 and batch: 800, loss is 4.233394961357117 and perplexity is 68.95092097942354
At time: 436.9528172016144 and batch: 850, loss is 4.266399164199829 and perplexity is 71.26456107257954
At time: 438.1642804145813 and batch: 900, loss is 4.215095024108887 and perplexity is 67.70059875435528
At time: 439.3740532398224 and batch: 950, loss is 4.212313070297241 and perplexity is 67.51252054933848
At time: 440.5846893787384 and batch: 1000, loss is 4.186148762702942 and perplexity is 65.76901052342257
At time: 441.7963128089905 and batch: 1050, loss is 4.177393007278442 and perplexity is 65.19566684414438
At time: 443.0084581375122 and batch: 1100, loss is 4.109679403305054 and perplexity is 60.927181385187
At time: 444.2201862335205 and batch: 1150, loss is 4.125850777626038 and perplexity is 61.92046739525958
At time: 445.4305844306946 and batch: 1200, loss is 4.151121397018432 and perplexity is 63.505174896957634
At time: 446.6424078941345 and batch: 1250, loss is 4.154884586334228 and perplexity is 63.74460712446608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5534743705805205 and perplexity of 94.96176819816621
Finished 14 epochs...
Completing Train Step...
At time: 449.6266665458679 and batch: 50, loss is 4.235370073318482 and perplexity is 69.08724134787602
At time: 450.83558416366577 and batch: 100, loss is 4.270740900039673 and perplexity is 71.57464563702422
At time: 452.0404369831085 and batch: 150, loss is 4.177667531967163 and perplexity is 65.21356712121231
At time: 453.25390791893005 and batch: 200, loss is 4.236599588394165 and perplexity is 69.17223739390319
At time: 454.46461844444275 and batch: 250, loss is 4.242757625579834 and perplexity is 69.59951685514338
At time: 455.671528339386 and batch: 300, loss is 4.2486922073364255 and perplexity is 70.01378892802016
At time: 456.8775134086609 and batch: 350, loss is 4.234738330841065 and perplexity is 69.04360978627554
At time: 458.082316160202 and batch: 400, loss is 4.237583093643188 and perplexity is 69.24030211798696
At time: 459.32047843933105 and batch: 450, loss is 4.167291984558106 and perplexity is 64.54043874017034
At time: 460.52551341056824 and batch: 500, loss is 4.191260604858399 and perplexity is 66.10607209273098
At time: 461.7303078174591 and batch: 550, loss is 4.187111673355102 and perplexity is 65.8323707044386
At time: 462.9400007724762 and batch: 600, loss is 4.198005566596985 and perplexity is 66.55346213790521
At time: 464.1471266746521 and batch: 650, loss is 4.22483913898468 and perplexity is 68.36350564108629
At time: 465.35098600387573 and batch: 700, loss is 4.2067889308929445 and perplexity is 67.14060019004113
At time: 466.5543556213379 and batch: 750, loss is 4.201476969718933 and perplexity is 66.78489750446225
At time: 467.75826954841614 and batch: 800, loss is 4.224282264709473 and perplexity is 68.32544636153219
At time: 468.9632890224457 and batch: 850, loss is 4.258228979110718 and perplexity is 70.68468847716933
At time: 470.16988921165466 and batch: 900, loss is 4.207735104560852 and perplexity is 67.20415692109955
At time: 471.3752336502075 and batch: 950, loss is 4.205445499420166 and perplexity is 67.05046195547203
At time: 472.5804693698883 and batch: 1000, loss is 4.180299034118653 and perplexity is 65.38540275716026
At time: 473.78487730026245 and batch: 1050, loss is 4.173340578079223 and perplexity is 64.93200062533572
At time: 474.98994517326355 and batch: 1100, loss is 4.106182541847229 and perplexity is 60.714499549048455
At time: 476.1946725845337 and batch: 1150, loss is 4.123428525924683 and perplexity is 61.770661944263146
At time: 477.4012668132782 and batch: 1200, loss is 4.150522532463074 and perplexity is 63.46715528406559
At time: 478.6066257953644 and batch: 1250, loss is 4.155258960723877 and perplexity is 63.768475940509646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.553262306825958 and perplexity of 94.9416323841758
Finished 15 epochs...
Completing Train Step...
At time: 481.5715425014496 and batch: 50, loss is 4.222954654693604 and perplexity is 68.2347970014176
At time: 482.80676555633545 and batch: 100, loss is 4.257577886581421 and perplexity is 70.63868118369402
At time: 484.0202407836914 and batch: 150, loss is 4.164666051864624 and perplexity is 64.37118221740856
At time: 485.23163294792175 and batch: 200, loss is 4.223144035339356 and perplexity is 68.24772057503523
At time: 486.44103264808655 and batch: 250, loss is 4.229750466346741 and perplexity is 68.70008705109178
At time: 487.65003299713135 and batch: 300, loss is 4.236269617080689 and perplexity is 69.14941630523357
At time: 488.85818696022034 and batch: 350, loss is 4.221803426742554 and perplexity is 68.15628839519707
At time: 490.0927369594574 and batch: 400, loss is 4.22585509300232 and perplexity is 68.43299511237903
At time: 491.3017816543579 and batch: 450, loss is 4.155741004943848 and perplexity is 63.79922257577664
At time: 492.51074266433716 and batch: 500, loss is 4.180485110282898 and perplexity is 65.39757055413637
At time: 493.720782995224 and batch: 550, loss is 4.176643242835999 and perplexity is 65.14680377152948
At time: 494.93129444122314 and batch: 600, loss is 4.187988343238831 and perplexity is 65.89010926634091
At time: 496.142050743103 and batch: 650, loss is 4.215470914840698 and perplexity is 67.7260515654033
At time: 497.35265350341797 and batch: 700, loss is 4.197379951477051 and perplexity is 66.51183830731428
At time: 498.56328654289246 and batch: 750, loss is 4.192946033477783 and perplexity is 66.21758310407897
At time: 499.77504420280457 and batch: 800, loss is 4.216584930419922 and perplexity is 67.80154148263127
At time: 500.9859552383423 and batch: 850, loss is 4.25107741355896 and perplexity is 70.18098557300544
At time: 502.1965363025665 and batch: 900, loss is 4.201414217948914 and perplexity is 66.7807067654228
At time: 503.40769505500793 and batch: 950, loss is 4.1995438289642335 and perplexity is 66.65591760561428
At time: 504.6199917793274 and batch: 1000, loss is 4.175009655952453 and perplexity is 65.0404676856484
At time: 505.82989144325256 and batch: 1050, loss is 4.169106731414795 and perplexity is 64.65766963853244
At time: 507.0419170856476 and batch: 1100, loss is 4.1023051023483275 and perplexity is 60.479538568212234
At time: 508.2516920566559 and batch: 1150, loss is 4.119989461898804 and perplexity is 61.558593550322094
At time: 509.4624238014221 and batch: 1200, loss is 4.148332176208496 and perplexity is 63.32829173936402
At time: 510.67262387275696 and batch: 1250, loss is 4.1535391521453855 and perplexity is 63.658900619846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.553184787722399 and perplexity of 94.93427287919779
Finished 16 epochs...
Completing Train Step...
At time: 513.6360692977905 and batch: 50, loss is 4.212041997909546 and perplexity is 67.49422224938786
At time: 514.8685352802277 and batch: 100, loss is 4.246002626419068 and perplexity is 69.82573418495484
At time: 516.075516462326 and batch: 150, loss is 4.153461375236511 and perplexity is 63.65394961987269
At time: 517.2816891670227 and batch: 200, loss is 4.2117502784729 and perplexity is 67.47453574450407
At time: 518.4886791706085 and batch: 250, loss is 4.218735456466675 and perplexity is 67.94750735905448
At time: 519.7210383415222 and batch: 300, loss is 4.2256020736694335 and perplexity is 68.41568243192208
At time: 520.9276509284973 and batch: 350, loss is 4.2108015441894535 and perplexity is 67.41055069638739
At time: 522.1341223716736 and batch: 400, loss is 4.215818481445313 and perplexity is 67.74959497040348
At time: 523.3413405418396 and batch: 450, loss is 4.145714440345764 and perplexity is 63.162731789603505
At time: 524.5492923259735 and batch: 500, loss is 4.1712100791931155 and perplexity is 64.79381032973689
At time: 525.7567620277405 and batch: 550, loss is 4.167536945343017 and perplexity is 64.55625055326067
At time: 526.9641575813293 and batch: 600, loss is 4.179194111824035 and perplexity is 65.31319686620677
At time: 528.1705975532532 and batch: 650, loss is 4.207199563980103 and perplexity is 67.168176003354
At time: 529.3768343925476 and batch: 700, loss is 4.189293918609619 and perplexity is 65.9761899503608
At time: 530.5837693214417 and batch: 750, loss is 4.185358028411866 and perplexity is 65.71702526748248
At time: 531.7894556522369 and batch: 800, loss is 4.209640913009643 and perplexity is 67.33235729502846
At time: 532.9965462684631 and batch: 850, loss is 4.24463791847229 and perplexity is 69.73050744374773
At time: 534.2042813301086 and batch: 900, loss is 4.195687689781189 and perplexity is 66.39937805395317
At time: 535.4105935096741 and batch: 950, loss is 4.193649654388428 and perplexity is 66.264191575628
At time: 536.6167874336243 and batch: 1000, loss is 4.169734773635864 and perplexity is 64.69829013934097
At time: 537.8236985206604 and batch: 1050, loss is 4.164463391304016 and perplexity is 64.35813803934826
At time: 539.0313601493835 and batch: 1100, loss is 4.098080825805664 and perplexity is 60.224595127038235
At time: 540.2393283843994 and batch: 1150, loss is 4.116145796775818 and perplexity is 61.32243707521934
At time: 541.4465229511261 and batch: 1200, loss is 4.145418238639832 and perplexity is 63.144025651227885
At time: 542.6585221290588 and batch: 1250, loss is 4.150793099403382 and perplexity is 63.484329721393536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.553249832487454 and perplexity of 94.94044805750222
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 545.6436967849731 and batch: 50, loss is 4.214960412979126 and perplexity is 67.69148611361665
At time: 546.8504843711853 and batch: 100, loss is 4.265611610412598 and perplexity is 71.20845849241097
At time: 548.0575568675995 and batch: 150, loss is 4.179981250762939 and perplexity is 65.3646276656214
At time: 549.2647070884705 and batch: 200, loss is 4.245372762680054 and perplexity is 69.78176733495758
At time: 550.4960849285126 and batch: 250, loss is 4.2558277988433835 and perplexity is 70.51516540717166
At time: 551.7033598423004 and batch: 300, loss is 4.265840196609497 and perplexity is 71.22473762364632
At time: 552.9103763103485 and batch: 350, loss is 4.246149349212646 and perplexity is 69.83597996336438
At time: 554.1178014278412 and batch: 400, loss is 4.248532743453979 and perplexity is 70.00262514754647
At time: 555.3251445293427 and batch: 450, loss is 4.1789260721206665 and perplexity is 65.29569268230561
At time: 556.5459332466125 and batch: 500, loss is 4.19639573097229 and perplexity is 66.44640819635309
At time: 557.7610850334167 and batch: 550, loss is 4.194863562583923 and perplexity is 66.34467906317626
At time: 558.9680993556976 and batch: 600, loss is 4.204762549400329 and perplexity is 67.00468547445307
At time: 560.1754553318024 and batch: 650, loss is 4.229516210556031 and perplexity is 68.68399554271588
At time: 561.381561756134 and batch: 700, loss is 4.209174604415893 and perplexity is 67.3009669575452
At time: 562.5874729156494 and batch: 750, loss is 4.1951998472213745 and perplexity is 66.3669935113146
At time: 563.7937424182892 and batch: 800, loss is 4.215600938796997 and perplexity is 67.7348581470932
At time: 564.9994988441467 and batch: 850, loss is 4.239978380203247 and perplexity is 69.40635127132643
At time: 566.2067074775696 and batch: 900, loss is 4.188502659797669 and perplexity is 65.92400635677046
At time: 567.4131193161011 and batch: 950, loss is 4.179493188858032 and perplexity is 65.33273346473473
At time: 568.6197698116302 and batch: 1000, loss is 4.154173378944397 and perplexity is 63.69928760651557
At time: 569.8265681266785 and batch: 1050, loss is 4.144084033966064 and perplexity is 63.059834773503816
At time: 571.0334734916687 and batch: 1100, loss is 4.07298171043396 and perplexity is 58.73182306032408
At time: 572.2409424781799 and batch: 1150, loss is 4.089471883773804 and perplexity is 59.70835042744815
At time: 573.4481692314148 and batch: 1200, loss is 4.121900725364685 and perplexity is 61.67636074740688
At time: 574.6581416130066 and batch: 1250, loss is 4.132200694084167 and perplexity is 62.31490819797273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.538723019787865 and perplexity of 93.5712351789415
Finished 18 epochs...
Completing Train Step...
At time: 577.6242425441742 and batch: 50, loss is 4.214037914276123 and perplexity is 67.62906959947553
At time: 578.8553833961487 and batch: 100, loss is 4.252362565994263 and perplexity is 70.27123681840875
At time: 580.0604960918427 and batch: 150, loss is 4.1621108865737915 and perplexity is 64.20691316354343
At time: 581.2920405864716 and batch: 200, loss is 4.224170875549317 and perplexity is 68.31783607130447
At time: 582.4981861114502 and batch: 250, loss is 4.233380680084228 and perplexity is 68.94993627953652
At time: 583.706036567688 and batch: 300, loss is 4.242484273910523 and perplexity is 69.58049431106829
At time: 584.9115595817566 and batch: 350, loss is 4.224142618179322 and perplexity is 68.31590561620827
At time: 586.117104768753 and batch: 400, loss is 4.2273759937286375 and perplexity is 68.53715409200558
At time: 587.3230526447296 and batch: 450, loss is 4.158494181632996 and perplexity is 63.97511512869109
At time: 588.5299227237701 and batch: 500, loss is 4.179032096862793 and perplexity is 65.30261600829965
At time: 589.7381348609924 and batch: 550, loss is 4.176693019866943 and perplexity is 65.1500466667069
At time: 590.9445312023163 and batch: 600, loss is 4.1880699634552006 and perplexity is 65.89548745079716
At time: 592.1530725955963 and batch: 650, loss is 4.2144486951828 and perplexity is 67.65685603668156
At time: 593.3597388267517 and batch: 700, loss is 4.194470405578613 and perplexity is 66.31860031469478
At time: 594.5665438175201 and batch: 750, loss is 4.184190940856934 and perplexity is 65.64037248410112
At time: 595.7737226486206 and batch: 800, loss is 4.206298675537109 and perplexity is 67.10769221851773
At time: 596.9815592765808 and batch: 850, loss is 4.233929967880249 and perplexity is 68.98782004166283
At time: 598.1897525787354 and batch: 900, loss is 4.185169725418091 and perplexity is 65.70465171990725
At time: 599.4006366729736 and batch: 950, loss is 4.178617115020752 and perplexity is 65.27552223052018
At time: 600.6075866222382 and batch: 1000, loss is 4.155641260147095 and perplexity is 63.7928592526473
At time: 601.8219084739685 and batch: 1050, loss is 4.147690343856811 and perplexity is 63.287658634164416
At time: 603.0340240001678 and batch: 1100, loss is 4.078422675132751 and perplexity is 59.052251766430345
At time: 604.242196559906 and batch: 1150, loss is 4.096938581466675 and perplexity is 60.155843197412814
At time: 605.4496476650238 and batch: 1200, loss is 4.1312094402313235 and perplexity is 62.253168909840845
At time: 606.66610455513 and batch: 1250, loss is 4.141976590156555 and perplexity is 62.92707965119608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.537505435247491 and perplexity of 93.45737362166665
Finished 19 epochs...
Completing Train Step...
At time: 609.7048590183258 and batch: 50, loss is 4.213940992355346 and perplexity is 67.62251517778833
At time: 610.9125673770905 and batch: 100, loss is 4.249611911773681 and perplexity is 70.0782105402467
At time: 612.148179769516 and batch: 150, loss is 4.157554402351379 and perplexity is 63.91502088304046
At time: 613.3563661575317 and batch: 200, loss is 4.217942008972168 and perplexity is 67.89361596240659
At time: 614.5641589164734 and batch: 250, loss is 4.226321320533753 and perplexity is 68.46490789749029
At time: 615.7803049087524 and batch: 300, loss is 4.235107488632202 and perplexity is 69.06910247787975
At time: 616.9962227344513 and batch: 350, loss is 4.217359275817871 and perplexity is 67.85406362676811
At time: 618.2111418247223 and batch: 400, loss is 4.220572605133056 and perplexity is 68.07245176716768
At time: 619.4194490909576 and batch: 450, loss is 4.151841597557068 and perplexity is 63.55092783179111
At time: 620.6242160797119 and batch: 500, loss is 4.1729414892196655 and perplexity is 64.90609215749167
At time: 621.8279273509979 and batch: 550, loss is 4.1706222772598265 and perplexity is 64.7557355940578
At time: 623.0322785377502 and batch: 600, loss is 4.182575511932373 and perplexity is 65.53442072957685
At time: 624.2357618808746 and batch: 650, loss is 4.209526762962342 and perplexity is 67.32467174191977
At time: 625.4397661685944 and batch: 700, loss is 4.189564647674561 and perplexity is 65.99405404062963
At time: 626.6450004577637 and batch: 750, loss is 4.180680680274963 and perplexity is 65.41036160722301
At time: 627.8489027023315 and batch: 800, loss is 4.2035009479522705 and perplexity is 66.92020556742341
At time: 629.0544426441193 and batch: 850, loss is 4.232142028808593 and perplexity is 68.86458422463741
At time: 630.2587711811066 and batch: 900, loss is 4.184275140762329 and perplexity is 65.64589962994373
At time: 631.4628436565399 and batch: 950, loss is 4.178613319396972 and perplexity is 65.27527446966597
At time: 632.6679515838623 and batch: 1000, loss is 4.156637344360352 and perplexity is 63.85643397029718
At time: 633.871933221817 and batch: 1050, loss is 4.1495211315155025 and perplexity is 63.40363102655881
At time: 635.0767512321472 and batch: 1100, loss is 4.0809206914901734 and perplexity is 59.19994965635404
At time: 636.2812054157257 and batch: 1150, loss is 4.1002376794815065 and perplexity is 60.354630950124864
At time: 637.4866392612457 and batch: 1200, loss is 4.134941239356994 and perplexity is 62.48591924979338
At time: 638.6917135715485 and batch: 1250, loss is 4.145647473335266 and perplexity is 63.1585021119067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5371539262089415 and perplexity of 93.42452828317441
Finished 20 epochs...
Completing Train Step...
At time: 641.6378710269928 and batch: 50, loss is 4.212296237945557 and perplexity is 67.51138416441356
At time: 642.8726170063019 and batch: 100, loss is 4.246977796554566 and perplexity is 69.89385936704409
At time: 644.0818250179291 and batch: 150, loss is 4.15429262638092 and perplexity is 63.70688403618939
At time: 645.2910106182098 and batch: 200, loss is 4.213925924301147 and perplexity is 67.62149624574124
At time: 646.5125260353088 and batch: 250, loss is 4.222016730308533 and perplexity is 68.17082792516733
At time: 647.7292139530182 and batch: 300, loss is 4.230859756469727 and perplexity is 68.7763376633123
At time: 648.9398713111877 and batch: 350, loss is 4.2132580375671385 and perplexity is 67.57634782415909
At time: 650.1488554477692 and batch: 400, loss is 4.2166609191894535 and perplexity is 67.80669383409884
At time: 651.3575811386108 and batch: 450, loss is 4.148083643913269 and perplexity is 63.31255456934375
At time: 652.5661447048187 and batch: 500, loss is 4.16937771320343 and perplexity is 64.67519306365742
At time: 653.774153470993 and batch: 550, loss is 4.1671651840209964 and perplexity is 64.53225549670361
At time: 654.9822325706482 and batch: 600, loss is 4.179455881118774 and perplexity is 65.3302960936163
At time: 656.1900730133057 and batch: 650, loss is 4.206738390922546 and perplexity is 67.1372069918418
At time: 657.3992145061493 and batch: 700, loss is 4.186745929718017 and perplexity is 65.8082973363482
At time: 658.6076555252075 and batch: 750, loss is 4.178636040687561 and perplexity is 65.27675762499499
At time: 659.8151552677155 and batch: 800, loss is 4.2018926191329955 and perplexity is 66.81266237780513
At time: 661.0237429141998 and batch: 850, loss is 4.231049194335937 and perplexity is 68.78936774012104
At time: 662.2319707870483 and batch: 900, loss is 4.183603677749634 and perplexity is 65.60183563173516
At time: 663.4447493553162 and batch: 950, loss is 4.178438587188721 and perplexity is 65.26386977322656
At time: 664.6562683582306 and batch: 1000, loss is 4.157009725570679 and perplexity is 63.88021733443033
At time: 665.8670604228973 and batch: 1050, loss is 4.1503970384597775 and perplexity is 63.45919103641426
At time: 667.0793733596802 and batch: 1100, loss is 4.082188968658447 and perplexity is 59.27507923335907
At time: 668.2919793128967 and batch: 1150, loss is 4.101900835037231 and perplexity is 60.45509360926074
At time: 669.5160329341888 and batch: 1200, loss is 4.136810636520385 and perplexity is 62.60283950113436
At time: 670.7278108596802 and batch: 1250, loss is 4.147371368408203 and perplexity is 63.26747464413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536984186102874 and perplexity of 93.40867173961807
Finished 21 epochs...
Completing Train Step...
At time: 673.7083415985107 and batch: 50, loss is 4.210372724533081 and perplexity is 67.38164992425435
At time: 674.9497616291046 and batch: 100, loss is 4.244494323730469 and perplexity is 69.72049522840219
At time: 676.1688220500946 and batch: 150, loss is 4.151529355049133 and perplexity is 63.53108762834197
At time: 677.3794913291931 and batch: 200, loss is 4.210763540267944 and perplexity is 67.40798887978967
At time: 678.5982992649078 and batch: 250, loss is 4.218735599517823 and perplexity is 67.94751707902411
At time: 679.8237340450287 and batch: 300, loss is 4.227703065872192 and perplexity is 68.559574352229
At time: 681.041020154953 and batch: 350, loss is 4.210276317596436 and perplexity is 67.37515417892153
At time: 682.253589630127 and batch: 400, loss is 4.213770723342895 and perplexity is 67.61100213909423
At time: 683.4647181034088 and batch: 450, loss is 4.1453323078155515 and perplexity is 63.13859986617973
At time: 684.6763260364532 and batch: 500, loss is 4.166811723709106 and perplexity is 64.50944993621644
At time: 685.8971960544586 and batch: 550, loss is 4.164580135345459 and perplexity is 64.36565190707383
At time: 687.1110517978668 and batch: 600, loss is 4.177182383537293 and perplexity is 65.18193653490225
At time: 688.3227937221527 and batch: 650, loss is 4.2046449184417725 and perplexity is 66.99680411262807
At time: 689.5332064628601 and batch: 700, loss is 4.184667911529541 and perplexity is 65.67168848452823
At time: 690.7461535930634 and batch: 750, loss is 4.1770948219299315 and perplexity is 65.17622934963714
At time: 691.9589116573334 and batch: 800, loss is 4.200655913352966 and perplexity is 66.7300858440263
At time: 693.1705090999603 and batch: 850, loss is 4.230115900039673 and perplexity is 68.72519696533251
At time: 694.3814535140991 and batch: 900, loss is 4.182933578491211 and perplexity is 65.55789061573284
At time: 695.599600315094 and batch: 950, loss is 4.178113298416138 and perplexity is 65.2426436216356
At time: 696.8151261806488 and batch: 1000, loss is 4.1571070671081545 and perplexity is 63.88643583565428
At time: 698.0333170890808 and batch: 1050, loss is 4.150833983421325 and perplexity is 63.486925268926846
At time: 699.2443234920502 and batch: 1100, loss is 4.082770481109619 and perplexity is 59.309558454053914
At time: 700.4609823226929 and batch: 1150, loss is 4.102771768569946 and perplexity is 60.50776891252283
At time: 701.6716711521149 and batch: 1200, loss is 4.137830142974853 and perplexity is 62.66669604562242
At time: 702.9089276790619 and batch: 1250, loss is 4.148238554000854 and perplexity is 63.322363082416516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536887509979471 and perplexity of 93.3996417878395
Finished 22 epochs...
Completing Train Step...
At time: 705.9017152786255 and batch: 50, loss is 4.208375158309937 and perplexity is 67.24718496235161
At time: 707.1110575199127 and batch: 100, loss is 4.2421237659454345 and perplexity is 69.55541450966008
At time: 708.3334639072418 and batch: 150, loss is 4.1490813779830935 and perplexity is 63.37575518554633
At time: 709.5517160892487 and batch: 200, loss is 4.208106260299683 and perplexity is 67.22910475909434
At time: 710.7688448429108 and batch: 250, loss is 4.216030859947205 and perplexity is 67.76398505590542
At time: 711.9858062267303 and batch: 300, loss is 4.2251544952392575 and perplexity is 68.38506789989295
At time: 713.1993188858032 and batch: 350, loss is 4.207831411361695 and perplexity is 67.21062945012523
At time: 714.4076800346375 and batch: 400, loss is 4.211428136825561 and perplexity is 67.45280288712308
At time: 715.6158702373505 and batch: 450, loss is 4.143285756111145 and perplexity is 63.009515590886
At time: 716.8247723579407 and batch: 500, loss is 4.164692549705506 and perplexity is 64.37288793735117
At time: 718.0336346626282 and batch: 550, loss is 4.162653393745423 and perplexity is 64.24175532460954
At time: 719.2429552078247 and batch: 600, loss is 4.175335493087768 and perplexity is 65.06166373836166
At time: 720.470513343811 and batch: 650, loss is 4.202993664741516 and perplexity is 66.8862666797206
At time: 721.690732717514 and batch: 700, loss is 4.1830182552337645 and perplexity is 65.5634420793955
At time: 722.9128029346466 and batch: 750, loss is 4.175716619491578 and perplexity is 65.08646518223044
At time: 724.1327393054962 and batch: 800, loss is 4.19960545539856 and perplexity is 66.66002549871916
At time: 725.3472661972046 and batch: 850, loss is 4.229266386032105 and perplexity is 68.66683873941244
At time: 726.5616819858551 and batch: 900, loss is 4.182276043891907 and perplexity is 65.51479820332428
At time: 727.7757987976074 and batch: 950, loss is 4.177708077430725 and perplexity is 65.2162112891259
At time: 728.9911937713623 and batch: 1000, loss is 4.156963958740234 and perplexity is 63.87729380625564
At time: 730.2051277160645 and batch: 1050, loss is 4.151132860183716 and perplexity is 63.50590287144629
At time: 731.41574883461 and batch: 1100, loss is 4.083136987686157 and perplexity is 59.331299781205246
At time: 732.6249639987946 and batch: 1150, loss is 4.103287172317505 and perplexity is 60.53896288143048
At time: 733.8823466300964 and batch: 1200, loss is 4.138437061309815 and perplexity is 62.70474115641378
At time: 735.0907549858093 and batch: 1250, loss is 4.148710923194885 and perplexity is 63.352281681785925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536825583799042 and perplexity of 93.39385808385335
Finished 23 epochs...
Completing Train Step...
At time: 738.0386323928833 and batch: 50, loss is 4.206521406173706 and perplexity is 67.12264082222029
At time: 739.2689003944397 and batch: 100, loss is 4.239958639144898 and perplexity is 69.4049811300202
At time: 740.4731163978577 and batch: 150, loss is 4.146885581016541 and perplexity is 63.23674756666324
At time: 741.6784508228302 and batch: 200, loss is 4.205734367370606 and perplexity is 67.06983348276513
At time: 742.883190870285 and batch: 250, loss is 4.21364933013916 and perplexity is 67.60279512108498
At time: 744.0891795158386 and batch: 300, loss is 4.222906007766723 and perplexity is 68.23147766897549
At time: 745.2945578098297 and batch: 350, loss is 4.205628113746643 and perplexity is 67.06270744848922
At time: 746.4984903335571 and batch: 400, loss is 4.209365873336792 and perplexity is 67.31384077201169
At time: 747.7040770053864 and batch: 450, loss is 4.141295232772827 and perplexity is 62.88421842440415
At time: 748.9089956283569 and batch: 500, loss is 4.162819504737854 and perplexity is 64.25242747269806
At time: 750.1141052246094 and batch: 550, loss is 4.160861005783081 and perplexity is 64.12671230734513
At time: 751.3191003799438 and batch: 600, loss is 4.173679661750794 and perplexity is 64.95402173979936
At time: 752.5258436203003 and batch: 650, loss is 4.201489553451538 and perplexity is 66.78573791304224
At time: 753.7385582923889 and batch: 700, loss is 4.181527147293091 and perplexity is 65.46575276105237
At time: 754.9485509395599 and batch: 750, loss is 4.17451132774353 and perplexity is 65.00806426032008
At time: 756.158584356308 and batch: 800, loss is 4.198628296852112 and perplexity is 66.59491989952345
At time: 757.3652262687683 and batch: 850, loss is 4.2284170627594 and perplexity is 68.60854315461069
At time: 758.5744404792786 and batch: 900, loss is 4.181588311195373 and perplexity is 65.46975702441404
At time: 759.7856078147888 and batch: 950, loss is 4.177208256721497 and perplexity is 65.18362302097027
At time: 761.0091331005096 and batch: 1000, loss is 4.156713304519653 and perplexity is 63.86128469942272
At time: 762.2295527458191 and batch: 1050, loss is 4.151017799377441 and perplexity is 63.49859625141955
At time: 763.4427378177643 and batch: 1100, loss is 4.0831587266922 and perplexity is 59.33258959870932
At time: 764.6818397045135 and batch: 1150, loss is 4.1034850025177 and perplexity is 60.550940501305305
At time: 765.8917973041534 and batch: 1200, loss is 4.138754053115845 and perplexity is 62.724621196297996
At time: 767.1096694469452 and batch: 1250, loss is 4.148927946090698 and perplexity is 63.36603206943334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536812218436359 and perplexity of 93.39260984940924
Finished 24 epochs...
Completing Train Step...
At time: 770.1478490829468 and batch: 50, loss is 4.204701461791992 and perplexity is 67.00059244348806
At time: 771.3618605136871 and batch: 100, loss is 4.237941780090332 and perplexity is 69.26514213057445
At time: 772.5764353275299 and batch: 150, loss is 4.144849815368652 and perplexity is 63.1081433167549
At time: 773.7878606319427 and batch: 200, loss is 4.203550977706909 and perplexity is 66.92355365263955
At time: 774.9954876899719 and batch: 250, loss is 4.211490511894226 and perplexity is 67.45701039155513
At time: 776.2063035964966 and batch: 300, loss is 4.22091851234436 and perplexity is 68.09600259209033
At time: 777.413227558136 and batch: 350, loss is 4.203697829246521 and perplexity is 66.93338220118085
At time: 778.6193931102753 and batch: 400, loss is 4.207533340454102 and perplexity is 67.19059890221526
At time: 779.8265488147736 and batch: 450, loss is 4.1395109844207765 and perplexity is 62.77211739908512
At time: 781.0337085723877 and batch: 500, loss is 4.161148457527161 and perplexity is 64.14514829224103
At time: 782.2412078380585 and batch: 550, loss is 4.159154286384583 and perplexity is 64.01735934744167
At time: 783.4512410163879 and batch: 600, loss is 4.172226004600525 and perplexity is 64.8596694562092
At time: 784.659646987915 and batch: 650, loss is 4.20015465259552 and perplexity is 66.69664505263013
At time: 785.8655142784119 and batch: 700, loss is 4.180214939117431 and perplexity is 65.37990440283085
At time: 787.0732243061066 and batch: 750, loss is 4.173400964736938 and perplexity is 64.93592177022347
At time: 788.2799279689789 and batch: 800, loss is 4.1977311754226685 and perplexity is 66.53520296046752
At time: 789.4875447750092 and batch: 850, loss is 4.227605590820312 and perplexity is 68.55289182985706
At time: 790.6945695877075 and batch: 900, loss is 4.180892848968506 and perplexity is 65.42424111053536
At time: 791.9018633365631 and batch: 950, loss is 4.176674771308899 and perplexity is 65.14885778314645
At time: 793.1090431213379 and batch: 1000, loss is 4.15636218547821 and perplexity is 63.838865722448936
At time: 794.3171241283417 and batch: 1050, loss is 4.150841341018677 and perplexity is 63.48739238187849
At time: 795.5532732009888 and batch: 1100, loss is 4.083069653511047 and perplexity is 59.327304891574066
At time: 796.7601296901703 and batch: 1150, loss is 4.103543381690979 and perplexity is 60.55447551833775
At time: 797.9772238731384 and batch: 1200, loss is 4.138906321525574 and perplexity is 62.73417290181146
At time: 799.1846573352814 and batch: 1250, loss is 4.14898202419281 and perplexity is 63.3694588768428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536805090242929 and perplexity of 93.39194413119397
Finished 25 epochs...
Completing Train Step...
At time: 802.1279938220978 and batch: 50, loss is 4.202966947555542 and perplexity is 66.88447969076633
At time: 803.3584318161011 and batch: 100, loss is 4.236065616607666 and perplexity is 69.13531123036796
At time: 804.563191652298 and batch: 150, loss is 4.142955799102783 and perplexity is 62.988728589220315
At time: 805.7688608169556 and batch: 200, loss is 4.201528453826905 and perplexity is 66.78833595384822
At time: 806.9741387367249 and batch: 250, loss is 4.2095052433013915 and perplexity is 67.32322295339908
At time: 808.1799936294556 and batch: 300, loss is 4.219079313278198 and perplexity is 67.9708755897183
At time: 809.3854441642761 and batch: 350, loss is 4.201881604194641 and perplexity is 66.81192644450088
At time: 810.5912592411041 and batch: 400, loss is 4.205834817886353 and perplexity is 67.07657102051846
At time: 811.7971935272217 and batch: 450, loss is 4.1378840684890745 and perplexity is 62.670075470549016
At time: 813.00292801857 and batch: 500, loss is 4.15960871219635 and perplexity is 64.04645709881426
At time: 814.2088379859924 and batch: 550, loss is 4.157664051055908 and perplexity is 63.92202946651428
At time: 815.4151511192322 and batch: 600, loss is 4.170757541656494 and perplexity is 64.76449533199067
At time: 816.6194925308228 and batch: 650, loss is 4.198905367851257 and perplexity is 66.61337397694476
At time: 817.8248248100281 and batch: 700, loss is 4.17896466255188 and perplexity is 65.29821251986309
At time: 819.0292339324951 and batch: 750, loss is 4.172319993972779 and perplexity is 64.8657658623202
At time: 820.2374589443207 and batch: 800, loss is 4.196839332580566 and perplexity is 66.47589046860142
At time: 821.4475564956665 and batch: 850, loss is 4.226796350479126 and perplexity is 68.4974385048434
At time: 822.6589033603668 and batch: 900, loss is 4.180181565284729 and perplexity is 65.37772246124932
At time: 823.866194486618 and batch: 950, loss is 4.17610719203949 and perplexity is 65.1118911337874
At time: 825.0963470935822 and batch: 1000, loss is 4.155954899787903 and perplexity is 63.81287036008365
At time: 826.3011136054993 and batch: 1050, loss is 4.1505967712402345 and perplexity is 63.471867182964374
At time: 827.5065689086914 and batch: 1100, loss is 4.082922115325927 and perplexity is 59.3185524943546
At time: 828.7109379768372 and batch: 1150, loss is 4.10349636554718 and perplexity is 60.5516285473364
At time: 829.9159107208252 and batch: 1200, loss is 4.138954768180847 and perplexity is 62.73721223628209
At time: 831.120765209198 and batch: 1250, loss is 4.148953852653503 and perplexity is 63.3676736867871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5368077633154655 and perplexity of 93.39219377496863
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 834.0994944572449 and batch: 50, loss is 4.203861837387085 and perplexity is 66.94436072099599
At time: 835.3033847808838 and batch: 100, loss is 4.240370655059815 and perplexity is 69.4335829786241
At time: 836.5092282295227 and batch: 150, loss is 4.148379940986633 and perplexity is 63.33131667341083
At time: 837.7130722999573 and batch: 200, loss is 4.208497924804687 and perplexity is 67.25544117031377
At time: 838.9245433807373 and batch: 250, loss is 4.218331518173218 and perplexity is 67.92006630150733
At time: 840.1291999816895 and batch: 300, loss is 4.229487009048462 and perplexity is 68.6819898957843
At time: 841.33376288414 and batch: 350, loss is 4.211532907485962 and perplexity is 67.45987033205142
At time: 842.5405559539795 and batch: 400, loss is 4.216702251434326 and perplexity is 67.80949649489214
At time: 843.7516458034515 and batch: 450, loss is 4.149177737236023 and perplexity is 63.38186232020461
At time: 844.9570486545563 and batch: 500, loss is 4.169008731842041 and perplexity is 64.6513335250059
At time: 846.1619009971619 and batch: 550, loss is 4.167142949104309 and perplexity is 64.53082064333097
At time: 847.3660650253296 and batch: 600, loss is 4.17951425075531 and perplexity is 65.33410951054685
At time: 848.5700917243958 and batch: 650, loss is 4.208578557968139 and perplexity is 67.26086440793816
At time: 849.7736413478851 and batch: 700, loss is 4.187677488327027 and perplexity is 65.86963018541252
At time: 850.9789361953735 and batch: 750, loss is 4.178166761398315 and perplexity is 65.24613178117161
At time: 852.183263540268 and batch: 800, loss is 4.19976851940155 and perplexity is 66.67089623560526
At time: 853.3908908367157 and batch: 850, loss is 4.227507457733155 and perplexity is 68.54616485302311
At time: 854.5985000133514 and batch: 900, loss is 4.178336834907531 and perplexity is 65.25722936344205
At time: 855.8343026638031 and batch: 950, loss is 4.172103772163391 and perplexity is 64.8517419852468
At time: 857.0490942001343 and batch: 1000, loss is 4.149139614105224 and perplexity is 63.37944605123526
At time: 858.2562656402588 and batch: 1050, loss is 4.141363334655762 and perplexity is 62.88850110391335
At time: 859.4596853256226 and batch: 1100, loss is 4.07250717163086 and perplexity is 58.70395914308298
At time: 860.663423538208 and batch: 1150, loss is 4.092191748619079 and perplexity is 59.87097012229024
At time: 861.8672857284546 and batch: 1200, loss is 4.129148073196411 and perplexity is 62.12497445293906
At time: 863.0714535713196 and batch: 1250, loss is 4.142256169319153 and perplexity is 62.94467521099101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5364094755075275 and perplexity of 93.35500420939837
Finished 27 epochs...
Completing Train Step...
At time: 866.0365431308746 and batch: 50, loss is 4.202729034423828 and perplexity is 66.86856888750964
At time: 867.244485616684 and batch: 100, loss is 4.236894779205322 and perplexity is 69.19265941681748
At time: 868.453444480896 and batch: 150, loss is 4.1447227001190186 and perplexity is 63.10012181920139
At time: 869.6615877151489 and batch: 200, loss is 4.2043941211700435 and perplexity is 66.980003603791
At time: 870.869754076004 and batch: 250, loss is 4.214254350662231 and perplexity is 67.64370857504186
At time: 872.0797386169434 and batch: 300, loss is 4.224934406280518 and perplexity is 68.37001875764129
At time: 873.2868947982788 and batch: 350, loss is 4.207279615402221 and perplexity is 67.17355312658451
At time: 874.4947648048401 and batch: 400, loss is 4.212178144454956 and perplexity is 67.5034119801431
At time: 875.7047896385193 and batch: 450, loss is 4.144882030487061 and perplexity is 63.11017638581199
At time: 876.9135675430298 and batch: 500, loss is 4.165240607261658 and perplexity is 64.4081776545113
At time: 878.1228277683258 and batch: 550, loss is 4.1632257843017575 and perplexity is 64.27853722448154
At time: 879.3356311321259 and batch: 600, loss is 4.176076583862304 and perplexity is 65.10989820798675
At time: 880.5456459522247 and batch: 650, loss is 4.205225853919983 and perplexity is 67.03573624049606
At time: 881.7539255619049 and batch: 700, loss is 4.18488564491272 and perplexity is 65.68598896023003
At time: 882.9623308181763 and batch: 750, loss is 4.175859003067017 and perplexity is 65.09573308563874
At time: 884.1705088615417 and batch: 800, loss is 4.198065004348755 and perplexity is 66.55741804363112
At time: 885.3783085346222 and batch: 850, loss is 4.225965414047241 and perplexity is 68.44054512836212
At time: 886.6325860023499 and batch: 900, loss is 4.17708589553833 and perplexity is 65.17564756368746
At time: 887.8471119403839 and batch: 950, loss is 4.171407222747803 and perplexity is 64.80658527103432
At time: 889.056746006012 and batch: 1000, loss is 4.149367861747741 and perplexity is 63.393913911445175
At time: 890.2644762992859 and batch: 1050, loss is 4.142240028381348 and perplexity is 62.943659233102714
At time: 891.472865819931 and batch: 1100, loss is 4.073869414329529 and perplexity is 58.78398267617047
At time: 892.6826074123383 and batch: 1150, loss is 4.094103422164917 and perplexity is 59.98553334088457
At time: 893.8907828330994 and batch: 1200, loss is 4.131373734474182 and perplexity is 62.26339758732583
At time: 895.099485874176 and batch: 1250, loss is 4.144299626350403 and perplexity is 63.07343145925693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536351558935904 and perplexity of 93.34959756417932
Finished 28 epochs...
Completing Train Step...
At time: 898.0373692512512 and batch: 50, loss is 4.202512674331665 and perplexity is 66.85410276278478
At time: 899.270359992981 and batch: 100, loss is 4.235466728210449 and perplexity is 69.09391929044753
At time: 900.4757401943207 and batch: 150, loss is 4.143073983192444 and perplexity is 62.99617329468176
At time: 901.6793739795685 and batch: 200, loss is 4.20232006072998 and perplexity is 66.84122699332823
At time: 902.8906874656677 and batch: 250, loss is 4.212180862426758 and perplexity is 67.5035954527627
At time: 904.1017959117889 and batch: 300, loss is 4.222661037445068 and perplexity is 68.21476502907787
At time: 905.3231477737427 and batch: 350, loss is 4.204987387657166 and perplexity is 67.01975238488049
At time: 906.5390501022339 and batch: 400, loss is 4.209811916351319 and perplexity is 67.34387233765611
At time: 907.7536306381226 and batch: 450, loss is 4.1426119613647465 and perplexity is 62.96707441023571
At time: 908.9605715274811 and batch: 500, loss is 4.163258843421936 and perplexity is 64.2806622514941
At time: 910.1682612895966 and batch: 550, loss is 4.161130356788635 and perplexity is 64.14398722819219
At time: 911.37562251091 and batch: 600, loss is 4.174225459098816 and perplexity is 64.98948314909957
At time: 912.5840287208557 and batch: 650, loss is 4.203451128005981 and perplexity is 66.91687168942387
At time: 913.8001096248627 and batch: 700, loss is 4.183288850784302 and perplexity is 65.58118565565822
At time: 915.0097181797028 and batch: 750, loss is 4.174632301330567 and perplexity is 65.01592899474294
At time: 916.2198491096497 and batch: 800, loss is 4.197247204780578 and perplexity is 66.5030096665021
At time: 917.459311246872 and batch: 850, loss is 4.225279750823975 and perplexity is 68.39363404802644
At time: 918.6684432029724 and batch: 900, loss is 4.176669201850891 and perplexity is 65.14849494032921
At time: 919.8834595680237 and batch: 950, loss is 4.171306190490722 and perplexity is 64.80003804619673
At time: 921.0983424186707 and batch: 1000, loss is 4.149815878868103 and perplexity is 63.42232183334674
At time: 922.3078696727753 and batch: 1050, loss is 4.1430229139328 and perplexity is 62.992956208898974
At time: 923.5141606330872 and batch: 1100, loss is 4.074908304214477 and perplexity is 58.845084494704764
At time: 924.7231662273407 and batch: 1150, loss is 4.095521578788757 and perplexity is 60.07066257135014
At time: 925.9364702701569 and batch: 1200, loss is 4.132962350845337 and perplexity is 62.36238884880737
At time: 927.1532638072968 and batch: 1250, loss is 4.1457496976852415 and perplexity is 63.16495877873915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536354232008439 and perplexity of 93.34984709475829
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 930.2061471939087 and batch: 50, loss is 4.202761058807373 and perplexity is 66.87071034649605
At time: 931.4288468360901 and batch: 100, loss is 4.23535418510437 and perplexity is 69.08614368371309
At time: 932.6455762386322 and batch: 150, loss is 4.142890481948853 and perplexity is 62.98461447910164
At time: 933.8657994270325 and batch: 200, loss is 4.201947860717773 and perplexity is 66.81635331710382
At time: 935.0827827453613 and batch: 250, loss is 4.212006635665894 and perplexity is 67.49183554445541
At time: 936.2948398590088 and batch: 300, loss is 4.222457571029663 and perplexity is 68.20088702726122
At time: 937.5123071670532 and batch: 350, loss is 4.204798822402954 and perplexity is 67.00711597966574
At time: 938.7216160297394 and batch: 400, loss is 4.209817352294922 and perplexity is 67.34423841614317
At time: 939.9306402206421 and batch: 450, loss is 4.142861523628235 and perplexity is 62.98279057685026
At time: 941.1394238471985 and batch: 500, loss is 4.163592586517334 and perplexity is 64.30211905901966
At time: 942.3482253551483 and batch: 550, loss is 4.161239919662475 and perplexity is 64.15101541277949
At time: 943.5603139400482 and batch: 600, loss is 4.1744250345230105 and perplexity is 65.00245474713029
At time: 944.7722723484039 and batch: 650, loss is 4.203946695327759 and perplexity is 66.95004172262217
At time: 945.982884645462 and batch: 700, loss is 4.1838923263549805 and perplexity is 65.62077424326885
At time: 947.1946260929108 and batch: 750, loss is 4.175096263885498 and perplexity is 65.04610095005809
At time: 948.4546666145325 and batch: 800, loss is 4.197414340972901 and perplexity is 66.51412565523188
At time: 949.6671166419983 and batch: 850, loss is 4.2251801013946535 and perplexity is 68.38681900098774
At time: 950.8790621757507 and batch: 900, loss is 4.176309061050415 and perplexity is 65.12503653362984
At time: 952.0909821987152 and batch: 950, loss is 4.170703883171082 and perplexity is 64.76102026049703
At time: 953.3094460964203 and batch: 1000, loss is 4.148783349990845 and perplexity is 63.356870250730545
At time: 954.5228469371796 and batch: 1050, loss is 4.141669306755066 and perplexity is 62.90774617469541
At time: 955.7435841560364 and batch: 1100, loss is 4.073145184516907 and perplexity is 58.741424976049466
At time: 956.9630641937256 and batch: 1150, loss is 4.093770117759704 and perplexity is 59.965543229954115
At time: 958.1816439628601 and batch: 1200, loss is 4.131412296295166 and perplexity is 62.26579862361133
At time: 959.3921735286713 and batch: 1250, loss is 4.144704995155334 and perplexity is 63.099004643725905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536329283331432 and perplexity of 93.34751816862641
Finished 30 epochs...
Completing Train Step...
At time: 962.4001386165619 and batch: 50, loss is 4.202711219787598 and perplexity is 66.86737765889035
At time: 963.6358835697174 and batch: 100, loss is 4.2351923847198485 and perplexity is 69.07496642336685
At time: 964.8415434360504 and batch: 150, loss is 4.142696304321289 and perplexity is 62.97238546342812
At time: 966.0484330654144 and batch: 200, loss is 4.2016955089569095 and perplexity is 66.79949421999065
At time: 967.2563557624817 and batch: 250, loss is 4.211760563850403 and perplexity is 67.47522974914506
At time: 968.4700644016266 and batch: 300, loss is 4.22222713470459 and perplexity is 68.18517287611309
At time: 969.678656578064 and batch: 350, loss is 4.204539256095886 and perplexity is 66.98972544711944
At time: 970.8859262466431 and batch: 400, loss is 4.209547624588013 and perplexity is 67.3260762586703
At time: 972.0928101539612 and batch: 450, loss is 4.142595338821411 and perplexity is 62.96602774601175
At time: 973.2996053695679 and batch: 500, loss is 4.163360209465027 and perplexity is 64.28717845812926
At time: 974.508496761322 and batch: 550, loss is 4.160999321937561 and perplexity is 64.13558268103479
At time: 975.7151920795441 and batch: 600, loss is 4.1742007398605345 and perplexity is 64.98787667843528
At time: 976.922144651413 and batch: 650, loss is 4.203704595565796 and perplexity is 66.93383509534716
At time: 978.1356654167175 and batch: 700, loss is 4.183669767379761 and perplexity is 65.60617137605999
At time: 979.4002265930176 and batch: 750, loss is 4.174932518005371 and perplexity is 65.03545079099283
At time: 980.6078929901123 and batch: 800, loss is 4.197319083213806 and perplexity is 66.50778997044017
At time: 981.8152449131012 and batch: 850, loss is 4.225104026794433 and perplexity is 68.38161669895595
At time: 983.0215456485748 and batch: 900, loss is 4.176246075630188 and perplexity is 65.12093473501457
At time: 984.22851395607 and batch: 950, loss is 4.170682816505432 and perplexity is 64.75965597610661
At time: 985.4363489151001 and batch: 1000, loss is 4.14884644985199 and perplexity is 63.36086818657922
At time: 986.6422107219696 and batch: 1050, loss is 4.141765990257263 and perplexity is 62.913828609941746
At time: 987.8493945598602 and batch: 1100, loss is 4.073332052230835 and perplexity is 58.75240287752337
At time: 989.0556011199951 and batch: 1150, loss is 4.093976674079895 and perplexity is 59.97793077122033
At time: 990.2629177570343 and batch: 1200, loss is 4.131629767417908 and perplexity is 62.279341109242836
At time: 991.4696021080017 and batch: 1250, loss is 4.144899930953979 and perplexity is 63.11130609754776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536323046162181 and perplexity of 93.34693594617211
Finished 31 epochs...
Completing Train Step...
At time: 994.4751410484314 and batch: 50, loss is 4.202673139572144 and perplexity is 66.86483138322394
At time: 995.6841502189636 and batch: 100, loss is 4.235049247741699 and perplexity is 69.06507994898416
At time: 996.893518447876 and batch: 150, loss is 4.142530856132507 and perplexity is 62.96196765813718
At time: 998.1041090488434 and batch: 200, loss is 4.2014724636077885 and perplexity is 66.78459656496935
At time: 999.314106464386 and batch: 250, loss is 4.211538367271423 and perplexity is 67.46023864947614
At time: 1000.5252571105957 and batch: 300, loss is 4.2220048999786375 and perplexity is 68.1700214465542
At time: 1001.7358591556549 and batch: 350, loss is 4.204296116828918 and perplexity is 66.97343959432523
At time: 1002.9464998245239 and batch: 400, loss is 4.209302253723145 and perplexity is 67.30955842769063
At time: 1004.1731984615326 and batch: 450, loss is 4.142353286743164 and perplexity is 62.95078853255294
At time: 1005.3890745639801 and batch: 500, loss is 4.163145980834961 and perplexity is 64.2734077790469
At time: 1006.6073744297028 and batch: 550, loss is 4.160779619216919 and perplexity is 64.12149346680582
At time: 1007.8171203136444 and batch: 600, loss is 4.173999290466309 and perplexity is 64.97478622862226
At time: 1009.0539345741272 and batch: 650, loss is 4.203497123718262 and perplexity is 66.91994964938677
At time: 1010.2639162540436 and batch: 700, loss is 4.183480081558227 and perplexity is 65.59372799574857
At time: 1011.4742891788483 and batch: 750, loss is 4.174794869422913 and perplexity is 65.02649936947127
At time: 1012.6863176822662 and batch: 800, loss is 4.197233076095581 and perplexity is 66.50207007306479
At time: 1013.8969552516937 and batch: 850, loss is 4.22503623008728 and perplexity is 68.37698080766481
At time: 1015.1093862056732 and batch: 900, loss is 4.176196641921997 and perplexity is 65.11771564529619
At time: 1016.3262457847595 and batch: 950, loss is 4.170670914649963 and perplexity is 64.75888522062768
At time: 1017.539858341217 and batch: 1000, loss is 4.14890869140625 and perplexity is 63.36481198822731
At time: 1018.7510917186737 and batch: 1050, loss is 4.141865644454956 and perplexity is 62.92009854946337
At time: 1019.9616158008575 and batch: 1100, loss is 4.073499178886413 and perplexity is 58.76222279068522
At time: 1021.1725888252258 and batch: 1150, loss is 4.094166355133057 and perplexity is 59.98930852733377
At time: 1022.3833146095276 and batch: 1200, loss is 4.131835284233094 and perplexity is 62.29214187642086
At time: 1023.6004478931427 and batch: 1250, loss is 4.145083465576172 and perplexity is 63.122890270284316
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536323046162181 and perplexity of 93.34693594617211
Finished 32 epochs...
Completing Train Step...
At time: 1026.6106238365173 and batch: 50, loss is 4.202639436721801 and perplexity is 66.86257788579351
At time: 1027.8475069999695 and batch: 100, loss is 4.234918870925903 and perplexity is 69.05607605074027
At time: 1029.0705695152283 and batch: 150, loss is 4.142382755279541 and perplexity is 62.952643627488065
At time: 1030.2893521785736 and batch: 200, loss is 4.201269760131836 and perplexity is 66.77106046705892
At time: 1031.4986596107483 and batch: 250, loss is 4.211333823204041 and perplexity is 67.44644146899
At time: 1032.7082467079163 and batch: 300, loss is 4.221793231964111 and perplexity is 68.15559356047925
At time: 1033.9245536327362 and batch: 350, loss is 4.203998713493347 and perplexity is 66.95352443155993
At time: 1035.1293077468872 and batch: 400, loss is 4.209073305130005 and perplexity is 67.29414976294711
At time: 1036.334745645523 and batch: 450, loss is 4.1421285915374755 and perplexity is 62.936645381184
At time: 1037.5468769073486 and batch: 500, loss is 4.162945919036865 and perplexity is 64.26055041169377
At time: 1038.7516527175903 and batch: 550, loss is 4.160574984550476 and perplexity is 64.10837332884172
At time: 1040.0106146335602 and batch: 600, loss is 4.173813548088074 and perplexity is 64.962718778056
At time: 1041.2163066864014 and batch: 650, loss is 4.203311319351196 and perplexity is 66.90751678557406
At time: 1042.42218875885 and batch: 700, loss is 4.183310642242431 and perplexity is 65.58261478089082
At time: 1043.6291909217834 and batch: 750, loss is 4.174671502113342 and perplexity is 65.0184777200081
At time: 1044.8360002040863 and batch: 800, loss is 4.197152371406555 and perplexity is 66.49670326074633
At time: 1046.042106628418 and batch: 850, loss is 4.224973602294922 and perplexity is 68.37269864240137
At time: 1047.249229669571 and batch: 900, loss is 4.176157155036926 and perplexity is 65.11514440030798
At time: 1048.4553761482239 and batch: 950, loss is 4.170665378570557 and perplexity is 64.75852671128918
At time: 1049.662488937378 and batch: 1000, loss is 4.148970346450806 and perplexity is 63.368718868971946
At time: 1050.8698105812073 and batch: 1050, loss is 4.141964931488037 and perplexity is 62.92634600951023
At time: 1052.0781464576721 and batch: 1100, loss is 4.073652076721191 and perplexity is 58.77120809421588
At time: 1053.2853667736053 and batch: 1150, loss is 4.094344635009765 and perplexity is 60.00000436726003
At time: 1054.493234872818 and batch: 1200, loss is 4.132030849456787 and perplexity is 62.304325244363966
At time: 1055.700145483017 and batch: 1250, loss is 4.145257840156555 and perplexity is 63.13389825751645
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536327055770985 and perplexity of 93.34731023161869
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 1058.6867306232452 and batch: 50, loss is 4.202652149200439 and perplexity is 66.86342788028931
At time: 1059.893393754959 and batch: 100, loss is 4.234883975982666 and perplexity is 69.05366638492912
At time: 1061.1008749008179 and batch: 150, loss is 4.14234429359436 and perplexity is 62.950222409289935
At time: 1062.306301832199 and batch: 200, loss is 4.201194877624512 and perplexity is 66.76606066983548
At time: 1063.5137162208557 and batch: 250, loss is 4.211279759407043 and perplexity is 67.44279515683785
At time: 1064.7199881076813 and batch: 300, loss is 4.221742153167725 and perplexity is 68.15211234370213
At time: 1065.9261028766632 and batch: 350, loss is 4.203947100639343 and perplexity is 66.95006885825515
At time: 1067.1341047286987 and batch: 400, loss is 4.209043254852295 and perplexity is 67.29212758544215
At time: 1068.3407969474792 and batch: 450, loss is 4.142136578559875 and perplexity is 62.937148059587884
At time: 1069.546281337738 and batch: 500, loss is 4.162968354225159 and perplexity is 64.26199212541462
At time: 1070.7896564006805 and batch: 550, loss is 4.160565123558045 and perplexity is 64.10774115977443
At time: 1072.0054292678833 and batch: 600, loss is 4.173812522888183 and perplexity is 64.96265217831798
At time: 1073.2125906944275 and batch: 650, loss is 4.203334507942199 and perplexity is 66.90906829460441
At time: 1074.4185581207275 and batch: 700, loss is 4.183357439041138 and perplexity is 65.58568390912549
At time: 1075.6242215633392 and batch: 750, loss is 4.174707636833191 and perplexity is 65.02082718693393
At time: 1076.8288855552673 and batch: 800, loss is 4.197163443565369 and perplexity is 66.49743952688145
At time: 1078.0345814228058 and batch: 850, loss is 4.224955167770386 and perplexity is 68.37143823582818
At time: 1079.2416906356812 and batch: 900, loss is 4.176108613014221 and perplexity is 65.11198365620511
At time: 1080.4485595226288 and batch: 950, loss is 4.170589952468872 and perplexity is 64.75364241227265
At time: 1081.6610703468323 and batch: 1000, loss is 4.148841114044189 and perplexity is 63.36053010606644
At time: 1082.8690114021301 and batch: 1050, loss is 4.141799573898315 and perplexity is 62.91594152085842
At time: 1084.0817375183105 and batch: 1100, loss is 4.073418641090393 and perplexity is 58.75749040134309
At time: 1085.289907693863 and batch: 1150, loss is 4.094112024307251 and perplexity is 59.98604934719976
At time: 1086.4990067481995 and batch: 1200, loss is 4.131829581260681 and perplexity is 62.291786627067204
At time: 1087.707142829895 and batch: 1250, loss is 4.145122056007385 and perplexity is 63.12532625684185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5363283923072535 and perplexity of 93.34743499376773
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1090.7397565841675 and batch: 50, loss is 4.2026541137695315 and perplexity is 66.86355923824216
At time: 1091.950124502182 and batch: 100, loss is 4.234879856109619 and perplexity is 69.05338189317621
At time: 1093.1600890159607 and batch: 150, loss is 4.142339630126953 and perplexity is 62.94992884366399
At time: 1094.3700866699219 and batch: 200, loss is 4.201185483932495 and perplexity is 66.7654334929701
At time: 1095.5808546543121 and batch: 250, loss is 4.2112730550765995 and perplexity is 67.44234299956878
At time: 1096.7922720909119 and batch: 300, loss is 4.22173641204834 and perplexity is 68.15172107541203
At time: 1098.0032436847687 and batch: 350, loss is 4.2039415073394775 and perplexity is 66.94969438749125
At time: 1099.2132453918457 and batch: 400, loss is 4.209040260314941 and perplexity is 67.29192607695416
At time: 1100.425522327423 and batch: 450, loss is 4.142149858474731 and perplexity is 62.93798386510508
At time: 1101.6641013622284 and batch: 500, loss is 4.162972059249878 and perplexity is 64.26223021812501
At time: 1102.874848127365 and batch: 550, loss is 4.160564932823181 and perplexity is 64.10772893219435
At time: 1104.0846252441406 and batch: 600, loss is 4.173813214302063 and perplexity is 64.9626970944129
At time: 1105.2951183319092 and batch: 650, loss is 4.203337836265564 and perplexity is 66.90929098999031
At time: 1106.5062820911407 and batch: 700, loss is 4.18336389541626 and perplexity is 65.58610735627043
At time: 1107.7159850597382 and batch: 750, loss is 4.174712872505188 and perplexity is 65.0211676155492
At time: 1108.92520236969 and batch: 800, loss is 4.197165951728821 and perplexity is 66.49760631353809
At time: 1110.1375622749329 and batch: 850, loss is 4.224953765869141 and perplexity is 68.37134238589098
At time: 1111.3473062515259 and batch: 900, loss is 4.176103143692017 and perplexity is 65.11162753876101
At time: 1112.5584523677826 and batch: 950, loss is 4.170580911636352 and perplexity is 64.75305698808292
At time: 1113.7685508728027 and batch: 1000, loss is 4.148824534416199 and perplexity is 63.35947962075635
At time: 1114.9789328575134 and batch: 1050, loss is 4.141777839660644 and perplexity is 62.91457410569203
At time: 1116.1893475055695 and batch: 1100, loss is 4.073387789726257 and perplexity is 58.755677680573584
At time: 1117.4003126621246 and batch: 1150, loss is 4.094080815315246 and perplexity is 59.98417727227821
At time: 1118.6204948425293 and batch: 1200, loss is 4.131802272796631 and perplexity is 62.29008555727839
At time: 1119.8402581214905 and batch: 1250, loss is 4.14510356426239 and perplexity is 63.124158970198614
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536328837819343 and perplexity of 93.3474765811878
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1122.8035666942596 and batch: 50, loss is 4.202654247283935 and perplexity is 66.863568165491
At time: 1124.0447862148285 and batch: 100, loss is 4.234879760742188 and perplexity is 69.05337530773286
At time: 1125.2512600421906 and batch: 150, loss is 4.142339525222778 and perplexity is 62.94992223995398
At time: 1126.4615182876587 and batch: 200, loss is 4.201185092926026 and perplexity is 66.76540738725882
At time: 1127.6732189655304 and batch: 250, loss is 4.211272993087769 and perplexity is 67.4423388188969
At time: 1128.88023853302 and batch: 300, loss is 4.221736345291138 and perplexity is 68.15171652579393
At time: 1130.091975927353 and batch: 350, loss is 4.203941316604614 and perplexity is 66.94968161785164
At time: 1131.2985875606537 and batch: 400, loss is 4.209040307998658 and perplexity is 67.29192928568337
At time: 1132.548586845398 and batch: 450, loss is 4.142152161598205 and perplexity is 62.93812881922006
At time: 1133.75714969635 and batch: 500, loss is 4.162972631454468 and perplexity is 64.26226698927864
At time: 1134.9655697345734 and batch: 550, loss is 4.160565066337585 and perplexity is 64.10773749150015
At time: 1136.180125951767 and batch: 600, loss is 4.1738134956359865 and perplexity is 64.96271537062592
At time: 1137.3885350227356 and batch: 650, loss is 4.203337955474853 and perplexity is 66.90929896619983
At time: 1138.5990512371063 and batch: 700, loss is 4.183364458084107 and perplexity is 65.58614425947461
At time: 1139.8082361221313 and batch: 750, loss is 4.174713411331177 and perplexity is 65.0212026506536
At time: 1141.0181066989899 and batch: 800, loss is 4.197166118621826 and perplexity is 66.49761741152437
At time: 1142.234154701233 and batch: 850, loss is 4.224953603744507 and perplexity is 68.37133130121305
At time: 1143.44708609581 and batch: 900, loss is 4.1761026954650875 and perplexity is 65.11159835398267
At time: 1144.6559524536133 and batch: 950, loss is 4.170580267906189 and perplexity is 64.75301530460038
At time: 1145.865146636963 and batch: 1000, loss is 4.148823251724243 and perplexity is 63.35939835011367
At time: 1147.0726399421692 and batch: 1050, loss is 4.141776213645935 and perplexity is 62.914471805752264
At time: 1148.2820048332214 and batch: 1100, loss is 4.073385367393493 and perplexity is 58.755535354942865
At time: 1149.4924442768097 and batch: 1150, loss is 4.09407823562622 and perplexity is 59.98402253195396
At time: 1150.707965373993 and batch: 1200, loss is 4.131800022125244 and perplexity is 62.2899453629229
At time: 1151.916323184967 and batch: 1250, loss is 4.145102124214173 and perplexity is 63.124068068431455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536328837819343 and perplexity of 93.3474765811878
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1154.9376266002655 and batch: 50, loss is 4.202654285430908 and perplexity is 66.86357071613376
At time: 1156.1540653705597 and batch: 100, loss is 4.234879741668701 and perplexity is 69.05337399064423
At time: 1157.3791544437408 and batch: 150, loss is 4.142339496612549 and perplexity is 62.94992043894232
At time: 1158.5967228412628 and batch: 200, loss is 4.201185111999512 and perplexity is 66.76540866070788
At time: 1159.8079748153687 and batch: 250, loss is 4.211272859573365 and perplexity is 67.44232981437384
At time: 1161.0182809829712 and batch: 300, loss is 4.221736288070678 and perplexity is 68.15171262612151
At time: 1162.2548668384552 and batch: 350, loss is 4.2039413118362425 and perplexity is 66.9496812986107
At time: 1163.465788602829 and batch: 400, loss is 4.209040336608886 and perplexity is 67.2919312109209
At time: 1164.8549480438232 and batch: 450, loss is 4.142152199745178 and perplexity is 62.93813122011919
At time: 1166.065532207489 and batch: 500, loss is 4.162972612380981 and perplexity is 64.26226576357315
At time: 1167.2772998809814 and batch: 550, loss is 4.160565056800842 and perplexity is 64.10773688012112
At time: 1168.4897937774658 and batch: 600, loss is 4.173813481330871 and perplexity is 64.96271444132678
At time: 1169.7003393173218 and batch: 650, loss is 4.203337993621826 and perplexity is 66.90930151858709
At time: 1170.9164996147156 and batch: 700, loss is 4.183364419937134 and perplexity is 65.5861417575618
At time: 1172.1257739067078 and batch: 750, loss is 4.174713478088379 and perplexity is 65.02120699128733
At time: 1173.3360295295715 and batch: 800, loss is 4.197166156768799 and perplexity is 66.49761994820722
At time: 1174.545910835266 and batch: 850, loss is 4.224953603744507 and perplexity is 68.37133130121305
At time: 1175.7570579051971 and batch: 900, loss is 4.17610273361206 and perplexity is 65.11160083779309
At time: 1176.9723608493805 and batch: 950, loss is 4.170580153465271 and perplexity is 64.75300789420628
At time: 1178.1827976703644 and batch: 1000, loss is 4.1488232421875 and perplexity is 63.359397745871384
At time: 1179.3956480026245 and batch: 1050, loss is 4.141776194572449 and perplexity is 62.91447060575398
At time: 1180.6064929962158 and batch: 1100, loss is 4.073385267257691 and perplexity is 58.75552947141046
At time: 1181.8189661502838 and batch: 1150, loss is 4.094078130722046 and perplexity is 59.98401623937994
At time: 1183.0315036773682 and batch: 1200, loss is 4.131800050735474 and perplexity is 62.28994714505257
At time: 1184.2403736114502 and batch: 1250, loss is 4.145102043151855 and perplexity is 63.1240629514484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.536328837819343 and perplexity of 93.3474765811878
Annealing...
Model not improving. Stopping early with 93.34693594617211loss at 36 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f71ae834898>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 2.505901894233711, 'data': 'wikitext', 'dropout': 0.9342705742519514, 'tune_wordvecs': True, 'lr': 26.084625863899376, 'batch_size': 50, 'wordvec_dim': 200}, 'best_accuracy': -179.27424501642665}, {'params': {'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 2.9585772088594657, 'data': 'wikitext', 'dropout': 0.7353633813958657, 'tune_wordvecs': True, 'lr': 9.223799285570193, 'batch_size': 50, 'wordvec_dim': 200}, 'best_accuracy': -111.44297280388199}, {'params': {'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 3.9830090235067015, 'data': 'wikitext', 'dropout': 0.5225947922390826, 'tune_wordvecs': True, 'lr': 5.2788791078727195, 'batch_size': 50, 'wordvec_dim': 200}, 'best_accuracy': -93.04952172212941}, {'params': {'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 4.111335993008976, 'data': 'wikitext', 'dropout': 0.6588457475619254, 'tune_wordvecs': True, 'lr': 6.931668359310156, 'batch_size': 50, 'wordvec_dim': 200}, 'best_accuracy': -96.59007599178845}, {'params': {'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 5.195045655249901, 'data': 'wikitext', 'dropout': 0.960824022462355, 'tune_wordvecs': True, 'lr': 14.61428609138214, 'batch_size': 50, 'wordvec_dim': 200}, 'best_accuracy': -152.6877762902757}, {'params': {'wordvec_source': '', 'seq_len': 35, 'num_layers': 1, 'anneal': 8.0, 'data': 'wikitext', 'dropout': 1.0, 'tune_wordvecs': True, 'lr': 2.473495794700573, 'batch_size': 50, 'wordvec_dim': 200}, 'best_accuracy': -93.34693594617211}]
