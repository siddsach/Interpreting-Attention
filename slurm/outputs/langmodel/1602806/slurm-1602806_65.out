Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'name': 'lr', 'domain': [0, 30], 'type': 'continuous'}, {'name': 'dropout', 'domain': [0, 1], 'type': 'continuous'}, {'name': 'anneal', 'domain': [2, 8], 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 2.575919372238502, 'wordvec_dim': 200, 'lr': 16.85910374052814, 'dropout': 0.4790889205209622, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1556224822998047 and batch: 50, loss is 7.014270439147949 and perplexity is 1112.3947904073248
At time: 3.4223644733428955 and batch: 100, loss is 6.242863712310791 and perplexity is 514.3292951205482
At time: 4.628042936325073 and batch: 150, loss is 6.124113254547119 and perplexity is 456.73952195043944
At time: 5.827946901321411 and batch: 200, loss is 6.077644767761231 and perplexity is 436.00110064510693
At time: 7.0282251834869385 and batch: 250, loss is 6.088683052062988 and perplexity is 440.840464751251
At time: 8.228957891464233 and batch: 300, loss is 6.065838356018066 and perplexity is 430.8837602866823
At time: 9.428237676620483 and batch: 350, loss is 5.968958244323731 and perplexity is 391.098029863063
At time: 10.639752626419067 and batch: 400, loss is 5.992128953933716 and perplexity is 400.2658510371358
At time: 11.842361211776733 and batch: 450, loss is 5.950342454910278 and perplexity is 383.8847797813845
At time: 13.052131175994873 and batch: 500, loss is 5.94952163696289 and perplexity is 383.5698095487048
At time: 14.24694299697876 and batch: 550, loss is 5.952865409851074 and perplexity is 384.8545265825777
At time: 15.457509994506836 and batch: 600, loss is 5.924168376922608 and perplexity is 373.9673061650967
At time: 16.660866022109985 and batch: 650, loss is 5.916591806411743 and perplexity is 371.1446231325525
At time: 17.863049745559692 and batch: 700, loss is 5.901461544036866 and perplexity is 365.5713762700018
At time: 19.06302571296692 and batch: 750, loss is 5.886249904632568 and perplexity is 360.0525181760721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.366897228152253 and perplexity of 214.19723044159423
Finished 1 epochs...
Completing Train Step...
At time: 22.551403284072876 and batch: 50, loss is 5.6803652286529545 and perplexity is 293.05644298915445
At time: 23.745381832122803 and batch: 100, loss is 5.6458713340759275 and perplexity is 283.1201410047574
At time: 24.938063621520996 and batch: 150, loss is 5.605129308700562 and perplexity is 271.8170714742186
At time: 26.138824224472046 and batch: 200, loss is 5.588586397171021 and perplexity is 267.35741526262206
At time: 27.33476710319519 and batch: 250, loss is 5.624989604949951 and perplexity is 277.26940227883966
At time: 28.528664588928223 and batch: 300, loss is 5.619843788146973 and perplexity is 275.84628940896755
At time: 29.723881721496582 and batch: 350, loss is 5.568718128204345 and perplexity is 262.09790789338325
At time: 30.917969226837158 and batch: 400, loss is 5.60878846168518 and perplexity is 272.8135136779924
At time: 32.11218047142029 and batch: 450, loss is 5.55697039604187 and perplexity is 259.0368672183375
At time: 33.30532908439636 and batch: 500, loss is 5.5614040184021 and perplexity is 260.1878885765069
At time: 34.503899812698364 and batch: 550, loss is 5.566982936859131 and perplexity is 261.6435122177409
At time: 35.69976305961609 and batch: 600, loss is 5.546041049957275 and perplexity is 256.2211784940275
At time: 36.9546217918396 and batch: 650, loss is 5.525513648986816 and perplexity is 251.01523863617618
At time: 38.14941072463989 and batch: 700, loss is 5.496248893737793 and perplexity is 243.7757861796918
At time: 39.34622526168823 and batch: 750, loss is 5.436198196411133 and perplexity is 229.56775080437322
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.26102110396984 and perplexity of 192.67813529202593
Finished 2 epochs...
Completing Train Step...
At time: 42.799219846725464 and batch: 50, loss is 5.481049613952637 and perplexity is 240.0985859893393
At time: 43.993059396743774 and batch: 100, loss is 5.516701164245606 and perplexity is 248.81288901493306
At time: 45.18534064292908 and batch: 150, loss is 5.537436285018921 and perplexity is 254.02591389391696
At time: 46.37724471092224 and batch: 200, loss is 5.509627809524536 and perplexity is 247.05915688840312
At time: 47.57721281051636 and batch: 250, loss is 5.572356014251709 and perplexity is 263.0531266505824
At time: 48.773099422454834 and batch: 300, loss is 5.538304557800293 and perplexity is 254.24657346320035
At time: 49.964881896972656 and batch: 350, loss is 5.462221412658692 and perplexity is 235.62025325410616
At time: 51.15797686576843 and batch: 400, loss is 5.516765842437744 and perplexity is 248.82898230321197
At time: 52.35065245628357 and batch: 450, loss is 5.482982749938965 and perplexity is 240.56317812138423
At time: 53.544015407562256 and batch: 500, loss is 5.4348265743255615 and perplexity is 229.25308645685206
At time: 54.74229407310486 and batch: 550, loss is 5.448362960815429 and perplexity is 232.3774433834125
At time: 55.93386197090149 and batch: 600, loss is 5.422852087020874 and perplexity is 226.5242689852401
At time: 57.12747550010681 and batch: 650, loss is 5.443204574584961 and perplexity is 231.18183712976088
At time: 58.3209114074707 and batch: 700, loss is 5.484156866073608 and perplexity is 240.84579310915953
At time: 59.51779770851135 and batch: 750, loss is 5.414337749481201 and perplexity is 224.60375246326885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.250278206758721 and perplexity of 190.61929265800276
Finished 3 epochs...
Completing Train Step...
At time: 62.957890033721924 and batch: 50, loss is 5.437752914428711 and perplexity is 229.9249415162729
At time: 64.18089270591736 and batch: 100, loss is 5.419312953948975 and perplexity is 225.7239864418204
At time: 65.37263774871826 and batch: 150, loss is 5.452546157836914 and perplexity is 233.35156005289303
At time: 66.56311273574829 and batch: 200, loss is 5.462560672760009 and perplexity is 235.7002033662724
At time: 67.75410580635071 and batch: 250, loss is 5.466494045257568 and perplexity is 236.62912576512502
At time: 68.94739055633545 and batch: 300, loss is 5.486524133682251 and perplexity is 241.41661493129808
At time: 70.1365897655487 and batch: 350, loss is 5.3878280067443844 and perplexity is 218.72779395676312
At time: 71.32832098007202 and batch: 400, loss is 5.405661859512329 and perplexity is 222.6635437092516
At time: 72.52395606040955 and batch: 450, loss is 5.395837564468383 and perplexity is 220.48674164133627
At time: 73.71730875968933 and batch: 500, loss is 5.384684295654297 and perplexity is 218.0412566680509
At time: 74.91001343727112 and batch: 550, loss is 5.391462459564209 and perplexity is 219.52419617076345
At time: 76.10306763648987 and batch: 600, loss is 5.357612743377685 and perplexity is 212.21772308945478
At time: 77.30294418334961 and batch: 650, loss is 5.40217885017395 and perplexity is 221.8893535456449
At time: 78.49421000480652 and batch: 700, loss is 5.435769300460816 and perplexity is 229.4693112374047
At time: 79.68646836280823 and batch: 750, loss is 5.411453294754028 and perplexity is 223.9568265704996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.281177609465843 and perplexity of 196.60125858862443
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 83.13089990615845 and batch: 50, loss is 5.377661361694336 and perplexity is 216.51533182836235
At time: 84.38722491264343 and batch: 100, loss is 5.33980351448059 and perplexity is 208.4717445856272
At time: 85.58237791061401 and batch: 150, loss is 5.306244974136352 and perplexity is 201.59182285437245
At time: 86.78747820854187 and batch: 200, loss is 5.313859176635742 and perplexity is 203.13264242771402
At time: 87.98803210258484 and batch: 250, loss is 5.333834705352783 and perplexity is 207.23112273443718
At time: 89.2073655128479 and batch: 300, loss is 5.359037704467774 and perplexity is 212.52034064539984
At time: 90.41951847076416 and batch: 350, loss is 5.2835916233062745 and perplexity is 197.07643005239512
At time: 91.624746799469 and batch: 400, loss is 5.296681041717529 and perplexity is 199.67300264380816
At time: 92.83100295066833 and batch: 450, loss is 5.268469648361206 and perplexity is 194.11866520134106
At time: 94.08889746665955 and batch: 500, loss is 5.235581607818603 and perplexity is 187.8383228177269
At time: 95.28729438781738 and batch: 550, loss is 5.232001781463623 and perplexity is 187.16709639307376
At time: 96.48377990722656 and batch: 600, loss is 5.178462142944336 and perplexity is 177.40977023145106
At time: 97.68284487724304 and batch: 650, loss is 5.19816746711731 and perplexity is 180.9403587174727
At time: 98.881418466568 and batch: 700, loss is 5.2056560134887695 and perplexity is 182.30042508839256
At time: 100.07778453826904 and batch: 750, loss is 5.181819715499878 and perplexity is 178.0064375233936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.174628679142442 and perplexity of 176.73097819278345
Finished 5 epochs...
Completing Train Step...
At time: 103.54455399513245 and batch: 50, loss is 5.242540607452392 and perplexity is 189.15004849302613
At time: 104.80275630950928 and batch: 100, loss is 5.232110013961792 and perplexity is 187.1873550517943
At time: 106.00208926200867 and batch: 150, loss is 5.223185043334961 and perplexity is 185.52414649026102
At time: 107.20155477523804 and batch: 200, loss is 5.218976202011109 and perplexity is 184.7449457124093
At time: 108.40297031402588 and batch: 250, loss is 5.238788366317749 and perplexity is 188.44164178799664
At time: 109.60199308395386 and batch: 300, loss is 5.272981996536255 and perplexity is 194.99657543471034
At time: 110.79996991157532 and batch: 350, loss is 5.210149927139282 and perplexity is 183.12151101999314
At time: 111.99898099899292 and batch: 400, loss is 5.229138774871826 and perplexity is 186.63200211705797
At time: 113.20170378684998 and batch: 450, loss is 5.196358804702759 and perplexity is 180.6133944644947
At time: 114.4007785320282 and batch: 500, loss is 5.169253969192505 and perplexity is 175.78364853738924
At time: 115.59692907333374 and batch: 550, loss is 5.1733931541442875 and perplexity is 176.51275748760602
At time: 116.79370498657227 and batch: 600, loss is 5.129413766860962 and perplexity is 168.9180636465468
At time: 117.99251079559326 and batch: 650, loss is 5.151115274429321 and perplexity is 172.62390602703059
At time: 119.19246482849121 and batch: 700, loss is 5.1480216121673585 and perplexity is 172.09069118203587
At time: 120.39313507080078 and batch: 750, loss is 5.131319427490235 and perplexity is 169.24027106149447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.165337496025618 and perplexity of 175.09654298821692
Finished 6 epochs...
Completing Train Step...
At time: 123.84356164932251 and batch: 50, loss is 5.1838160991668705 and perplexity is 178.3621616306374
At time: 125.07771325111389 and batch: 100, loss is 5.1782626533508305 and perplexity is 177.37438235837723
At time: 126.27893018722534 and batch: 150, loss is 5.174839487075806 and perplexity is 176.7682384122946
At time: 127.47993445396423 and batch: 200, loss is 5.170360240936279 and perplexity is 175.97822062574173
At time: 128.67985677719116 and batch: 250, loss is 5.195598430633545 and perplexity is 180.47611292206813
At time: 129.87897181510925 and batch: 300, loss is 5.222614974975586 and perplexity is 185.41841518434614
At time: 131.08432745933533 and batch: 350, loss is 5.161445760726929 and perplexity is 174.41643784417258
At time: 132.28591871261597 and batch: 400, loss is 5.179847726821899 and perplexity is 177.65575672691244
At time: 133.48773002624512 and batch: 450, loss is 5.142458190917969 and perplexity is 171.13593648777058
At time: 134.6917507648468 and batch: 500, loss is 5.117253665924072 and perplexity is 166.87644126714395
At time: 135.8919005393982 and batch: 550, loss is 5.122912540435791 and perplexity is 167.82345108493473
At time: 137.09231162071228 and batch: 600, loss is 5.083295936584473 and perplexity is 161.30483124176982
At time: 138.28918385505676 and batch: 650, loss is 5.112203693389892 and perplexity is 166.03584410485803
At time: 139.48931765556335 and batch: 700, loss is 5.116557760238647 and perplexity is 166.76035140138654
At time: 140.68951964378357 and batch: 750, loss is 5.098750791549683 and perplexity is 163.81713767386807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.157551965048147 and perplexity of 173.738616377742
Finished 7 epochs...
Completing Train Step...
At time: 144.13217043876648 and batch: 50, loss is 5.1466746997833255 and perplexity is 171.8590561299849
At time: 145.36081671714783 and batch: 100, loss is 5.142297143936157 and perplexity is 171.10837778090257
At time: 146.55977630615234 and batch: 150, loss is 5.137770433425903 and perplexity is 170.33557014666926
At time: 147.7584114074707 and batch: 200, loss is 5.136906614303589 and perplexity is 170.1884945564654
At time: 148.95567536354065 and batch: 250, loss is 5.155742149353028 and perplexity is 173.4244658659849
At time: 150.15430235862732 and batch: 300, loss is 5.1700934886932375 and perplexity is 175.9312843011265
At time: 151.35166025161743 and batch: 350, loss is 5.10430718421936 and perplexity is 164.72990351115536
At time: 152.55230736732483 and batch: 400, loss is 5.139556894302368 and perplexity is 170.6401399486229
At time: 153.75048780441284 and batch: 450, loss is 5.108285684585571 and perplexity is 165.38658693402886
At time: 155.00698924064636 and batch: 500, loss is 5.089244117736817 and perplexity is 162.26716081672862
At time: 156.21125054359436 and batch: 550, loss is 5.0939045238494876 and perplexity is 163.0251565970356
At time: 157.41041374206543 and batch: 600, loss is 5.0575590991973876 and perplexity is 157.20632258574247
At time: 158.61240887641907 and batch: 650, loss is 5.079688158035278 and perplexity is 160.72392764758726
At time: 159.81526255607605 and batch: 700, loss is 5.0791416835784915 and perplexity is 160.6361201210038
At time: 161.0158200263977 and batch: 750, loss is 5.054319915771484 and perplexity is 156.69792631009125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.170265730037245 and perplexity of 175.96158955182122
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 164.49819564819336 and batch: 50, loss is 5.092889394760132 and perplexity is 162.85974898751934
At time: 165.73204398155212 and batch: 100, loss is 5.0740157508850094 and perplexity is 159.8148169518282
At time: 166.94984698295593 and batch: 150, loss is 5.044276733398437 and perplexity is 155.13205677715715
At time: 168.1783730983734 and batch: 200, loss is 5.043638381958008 and perplexity is 155.0330596061118
At time: 169.38735246658325 and batch: 250, loss is 5.080330600738526 and perplexity is 160.82721673725015
At time: 170.58253049850464 and batch: 300, loss is 5.085659799575805 and perplexity is 161.68658479144426
At time: 171.78055453300476 and batch: 350, loss is 5.002799739837647 and perplexity is 148.82925955124665
At time: 172.98227977752686 and batch: 400, loss is 5.014648447036743 and perplexity is 150.6031824632703
At time: 174.1811125278473 and batch: 450, loss is 4.97691427230835 and perplexity is 145.02617919672008
At time: 175.37697196006775 and batch: 500, loss is 4.94744644165039 and perplexity is 140.81492529905532
At time: 176.57494020462036 and batch: 550, loss is 4.9543538093566895 and perplexity is 141.79095277630927
At time: 177.77757620811462 and batch: 600, loss is 4.914108562469482 and perplexity is 136.19784380137207
At time: 178.97506880760193 and batch: 650, loss is 4.92595986366272 and perplexity is 137.82156808817336
At time: 180.17395901679993 and batch: 700, loss is 4.923966884613037 and perplexity is 137.54716611989247
At time: 181.37213730812073 and batch: 750, loss is 4.916393146514893 and perplexity is 136.50935492352005
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.059547956599745 and perplexity of 157.51929466937239
Finished 9 epochs...
Completing Train Step...
At time: 184.85425066947937 and batch: 50, loss is 5.018912601470947 and perplexity is 151.24674884964202
At time: 186.0788984298706 and batch: 100, loss is 5.016821632385254 and perplexity is 150.93082698002206
At time: 187.2756633758545 and batch: 150, loss is 4.994816389083862 and perplexity is 147.64583350777644
At time: 188.4728193283081 and batch: 200, loss is 4.999469518661499 and perplexity is 148.3344495700996
At time: 189.66682529449463 and batch: 250, loss is 5.037025403976441 and perplexity is 154.01121184878807
At time: 190.86271476745605 and batch: 300, loss is 5.047844409942627 and perplexity is 155.6865062374396
At time: 192.05630135536194 and batch: 350, loss is 4.973473834991455 and perplexity is 144.52808304349546
At time: 193.25024580955505 and batch: 400, loss is 4.989183292388916 and perplexity is 146.81646839107154
At time: 194.4460530281067 and batch: 450, loss is 4.952311058044433 and perplexity is 141.50160475510114
At time: 195.6383934020996 and batch: 500, loss is 4.929171142578125 and perplexity is 138.26486297467457
At time: 196.83248233795166 and batch: 550, loss is 4.941772689819336 and perplexity is 140.01823859732426
At time: 198.02789092063904 and batch: 600, loss is 4.906865139007568 and perplexity is 135.21486949152896
At time: 199.22530460357666 and batch: 650, loss is 4.91862340927124 and perplexity is 136.81414640991136
At time: 200.42099595069885 and batch: 700, loss is 4.917952194213867 and perplexity is 136.7223455073
At time: 201.61450386047363 and batch: 750, loss is 4.909806423187256 and perplexity is 135.6131603041164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.05785245673601 and perplexity of 157.25244701073527
Finished 10 epochs...
Completing Train Step...
At time: 205.0553503036499 and batch: 50, loss is 4.996886110305786 and perplexity is 147.95173567962397
At time: 206.27701139450073 and batch: 100, loss is 4.995846214294434 and perplexity is 147.79796122839483
At time: 207.4675018787384 and batch: 150, loss is 4.9733825778961185 and perplexity is 144.51489443222872
At time: 208.6595480442047 and batch: 200, loss is 4.978930454254151 and perplexity is 145.31887332404267
At time: 209.85264420509338 and batch: 250, loss is 5.013302536010742 and perplexity is 150.40062032532765
At time: 211.04589891433716 and batch: 300, loss is 5.027520217895508 and perplexity is 152.55424197607056
At time: 212.23919868469238 and batch: 350, loss is 4.956109619140625 and perplexity is 142.0401294078363
At time: 213.43267583847046 and batch: 400, loss is 4.973074464797974 and perplexity is 144.47037435933817
At time: 214.62121081352234 and batch: 450, loss is 4.938482875823975 and perplexity is 139.5583615061922
At time: 215.87043738365173 and batch: 500, loss is 4.916079587936402 and perplexity is 136.46655795426852
At time: 217.0621452331543 and batch: 550, loss is 4.931759471893311 and perplexity is 138.62320152179936
At time: 218.25091695785522 and batch: 600, loss is 4.897949323654175 and perplexity is 134.01467697996128
At time: 219.44445180892944 and batch: 650, loss is 4.90798544883728 and perplexity is 135.36643692433339
At time: 220.64069175720215 and batch: 700, loss is 4.908661756515503 and perplexity is 135.45801724974692
At time: 221.83554077148438 and batch: 750, loss is 4.89849232673645 and perplexity is 134.08746712347914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.058470615120822 and perplexity of 157.34968398021468
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 225.26401948928833 and batch: 50, loss is 4.9765989780426025 and perplexity is 144.98046048183915
At time: 226.4926369190216 and batch: 100, loss is 4.9684177684783934 and perplexity is 143.79918367724716
At time: 227.69112610816956 and batch: 150, loss is 4.9425100898742675 and perplexity is 140.12152613159375
At time: 228.89082288742065 and batch: 200, loss is 4.942958784103394 and perplexity is 140.18441195894422
At time: 230.08898091316223 and batch: 250, loss is 4.970468330383301 and perplexity is 144.09435533570334
At time: 231.2843873500824 and batch: 300, loss is 4.9835466957092285 and perplexity is 145.99125106627716
At time: 232.4812524318695 and batch: 350, loss is 4.910029048919678 and perplexity is 135.643354644149
At time: 233.67706155776978 and batch: 400, loss is 4.918744707107544 and perplexity is 136.83074267637193
At time: 234.877836227417 and batch: 450, loss is 4.8821110439300535 and perplexity is 131.90883551279148
At time: 236.07569813728333 and batch: 500, loss is 4.850030670166015 and perplexity is 127.744307725073
At time: 237.2790219783783 and batch: 550, loss is 4.861685514450073 and perplexity is 129.24185764368596
At time: 238.48197221755981 and batch: 600, loss is 4.824768133163452 and perplexity is 124.55758426165903
At time: 239.68481588363647 and batch: 650, loss is 4.830074100494385 and perplexity is 125.22023919262001
At time: 240.88823819160461 and batch: 700, loss is 4.834581089019776 and perplexity is 125.78587908257053
At time: 242.08802676200867 and batch: 750, loss is 4.83103012084961 and perplexity is 125.34000953249343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.020880055982013 and perplexity of 151.54461286882145
Finished 12 epochs...
Completing Train Step...
At time: 245.54160165786743 and batch: 50, loss is 4.942730989456177 and perplexity is 140.15248233711324
At time: 246.76782321929932 and batch: 100, loss is 4.936407699584961 and perplexity is 139.26905359671048
At time: 247.96663522720337 and batch: 150, loss is 4.91569655418396 and perplexity is 136.4142966660501
At time: 249.16236686706543 and batch: 200, loss is 4.92017050743103 and perplexity is 137.02597514172473
At time: 250.3599226474762 and batch: 250, loss is 4.9488249683380126 and perplexity is 141.00917629092194
At time: 251.56074929237366 and batch: 300, loss is 4.965716791152954 and perplexity is 143.4113093983387
At time: 252.75974774360657 and batch: 350, loss is 4.8966788482666015 and perplexity is 133.84452274255722
At time: 253.9561848640442 and batch: 400, loss is 4.907026929855347 and perplexity is 135.23674778970275
At time: 255.15356159210205 and batch: 450, loss is 4.872582626342774 and perplexity is 130.6579221214183
At time: 256.35251355171204 and batch: 500, loss is 4.843659706115723 and perplexity is 126.93304035460805
At time: 257.5493755340576 and batch: 550, loss is 4.858665046691894 and perplexity is 128.85207573784467
At time: 258.74848318099976 and batch: 600, loss is 4.824098148345947 and perplexity is 124.47416052073912
At time: 259.9473569393158 and batch: 650, loss is 4.832315855026245 and perplexity is 125.50126711142464
At time: 261.14392852783203 and batch: 700, loss is 4.837993116378784 and perplexity is 126.21579697208833
At time: 262.34088826179504 and batch: 750, loss is 4.83152060508728 and perplexity is 125.40150191080694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.019343886264535 and perplexity of 151.3119933409676
Finished 13 epochs...
Completing Train Step...
At time: 265.78218030929565 and batch: 50, loss is 4.929250087738037 and perplexity is 138.2757787472604
At time: 267.01451992988586 and batch: 100, loss is 4.923602695465088 and perplexity is 137.49708205525042
At time: 268.21323227882385 and batch: 150, loss is 4.90380145072937 and perplexity is 134.80124720922655
At time: 269.4164037704468 and batch: 200, loss is 4.909510335922241 and perplexity is 135.57301291824012
At time: 270.61540484428406 and batch: 250, loss is 4.938973159790039 and perplexity is 139.62680150931598
At time: 271.8149037361145 and batch: 300, loss is 4.95751914024353 and perplexity is 142.24047913310855
At time: 273.0135135650635 and batch: 350, loss is 4.889803743362426 and perplexity is 132.9274835901829
At time: 274.21133065223694 and batch: 400, loss is 4.901131792068481 and perplexity is 134.44185383438577
At time: 275.4109778404236 and batch: 450, loss is 4.867558631896973 and perplexity is 130.0031436253893
At time: 276.66490864753723 and batch: 500, loss is 4.839505243301391 and perplexity is 126.40679564747415
At time: 277.8653197288513 and batch: 550, loss is 4.855888710021973 and perplexity is 128.49483513454055
At time: 279.0684726238251 and batch: 600, loss is 4.822301940917969 and perplexity is 124.25077978814487
At time: 280.267929315567 and batch: 650, loss is 4.831524477005005 and perplexity is 125.40198745604495
At time: 281.47101950645447 and batch: 700, loss is 4.837183694839478 and perplexity is 126.11367652223977
At time: 282.6712152957916 and batch: 750, loss is 4.828921346664429 and perplexity is 125.07597424926114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.019110036450763 and perplexity of 151.27661319648467
Finished 14 epochs...
Completing Train Step...
At time: 286.15303015708923 and batch: 50, loss is 4.919123840332031 and perplexity is 136.88262959245716
At time: 287.4118070602417 and batch: 100, loss is 4.914116239547729 and perplexity is 136.19888940688958
At time: 288.6097786426544 and batch: 150, loss is 4.895059833526611 and perplexity is 133.6280018099563
At time: 289.80936765670776 and batch: 200, loss is 4.901182117462159 and perplexity is 134.4486198438563
At time: 291.0071849822998 and batch: 250, loss is 4.930982465744019 and perplexity is 138.51553227703204
At time: 292.2065064907074 and batch: 300, loss is 4.950762844085693 and perplexity is 141.28269949525034
At time: 293.4063148498535 and batch: 350, loss is 4.884599847793579 and perplexity is 132.23753960258315
At time: 294.60674118995667 and batch: 400, loss is 4.8961096858978275 and perplexity is 133.76836515201887
At time: 295.8060655593872 and batch: 450, loss is 4.862858734130859 and perplexity is 129.39357571656677
At time: 297.00394344329834 and batch: 500, loss is 4.835388975143433 and perplexity is 125.88754080885296
At time: 298.19926476478577 and batch: 550, loss is 4.852646608352661 and perplexity is 128.07891640477473
At time: 299.39648628234863 and batch: 600, loss is 4.819423379898072 and perplexity is 123.89363062120485
At time: 300.59430503845215 and batch: 650, loss is 4.82919017791748 and perplexity is 125.10960310018574
At time: 301.7916271686554 and batch: 700, loss is 4.8347820281982425 and perplexity is 125.81115693334637
At time: 302.99003171920776 and batch: 750, loss is 4.825687341690063 and perplexity is 124.67213129340503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.019236719885538 and perplexity of 151.2957786513928
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 306.44940400123596 and batch: 50, loss is 4.912320184707641 and perplexity is 135.9544882769713
At time: 307.6786332130432 and batch: 100, loss is 4.906531496047974 and perplexity is 135.16976352735276
At time: 308.88029074668884 and batch: 150, loss is 4.8847567749023435 and perplexity is 132.2582928856769
At time: 310.07572531700134 and batch: 200, loss is 4.8901009845733645 and perplexity is 132.96700098918103
At time: 311.27214884757996 and batch: 250, loss is 4.91609227180481 and perplexity is 136.46828888910917
At time: 312.4701473712921 and batch: 300, loss is 4.9311105823516845 and perplexity is 138.5332795539726
At time: 313.662654876709 and batch: 350, loss is 4.864206371307373 and perplexity is 129.56806885991946
At time: 314.86105847358704 and batch: 400, loss is 4.873782157897949 and perplexity is 130.81474445980436
At time: 316.0871732234955 and batch: 450, loss is 4.83651951789856 and perplexity is 126.02994253652864
At time: 317.29770851135254 and batch: 500, loss is 4.802980489730835 and perplexity is 121.87311831483919
At time: 318.50214076042175 and batch: 550, loss is 4.817547235488892 and perplexity is 123.66140619018341
At time: 319.7111146450043 and batch: 600, loss is 4.78507140159607 and perplexity is 119.70991025737206
At time: 320.9085659980774 and batch: 650, loss is 4.792461328506469 and perplexity is 120.59783455157537
At time: 322.1044328212738 and batch: 700, loss is 4.7976142692565915 and perplexity is 121.22087190628481
At time: 323.30414056777954 and batch: 750, loss is 4.796442594528198 and perplexity is 121.07892364895046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.007133661314499 and perplexity of 149.47567361771
Finished 16 epochs...
Completing Train Step...
At time: 326.74380922317505 and batch: 50, loss is 4.899760351181031 and perplexity is 134.25760115376184
At time: 327.9700644016266 and batch: 100, loss is 4.894713954925537 and perplexity is 133.58179073579748
At time: 329.16677260398865 and batch: 150, loss is 4.873979482650757 and perplexity is 130.84055999385666
At time: 330.3661036491394 and batch: 200, loss is 4.880048742294312 and perplexity is 131.63708002275715
At time: 331.5630536079407 and batch: 250, loss is 4.906536483764649 and perplexity is 135.17043771751761
At time: 332.7588474750519 and batch: 300, loss is 4.923279542922973 and perplexity is 137.4526567021204
At time: 333.953910112381 and batch: 350, loss is 4.858637228012085 and perplexity is 128.84849129306443
At time: 335.1549994945526 and batch: 400, loss is 4.869114265441895 and perplexity is 130.20553826168486
At time: 336.3476405143738 and batch: 450, loss is 4.833208665847779 and perplexity is 125.61336603493764
At time: 337.5979199409485 and batch: 500, loss is 4.801264905929566 and perplexity is 121.66421401490229
At time: 338.7953689098358 and batch: 550, loss is 4.817808923721313 and perplexity is 123.69377115956918
At time: 339.9954240322113 and batch: 600, loss is 4.78649377822876 and perplexity is 119.88030398972747
At time: 341.1910057067871 and batch: 650, loss is 4.7949043560028075 and perplexity is 120.89281855768787
At time: 342.3861427307129 and batch: 700, loss is 4.79981692314148 and perplexity is 121.48817380943439
At time: 343.58202052116394 and batch: 750, loss is 4.797584571838379 and perplexity is 121.21727201280974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.00641756279524 and perplexity of 149.36867262536566
Finished 17 epochs...
Completing Train Step...
At time: 347.02662086486816 and batch: 50, loss is 4.894229993820191 and perplexity is 133.51715798582893
At time: 348.2491526603699 and batch: 100, loss is 4.889094934463501 and perplexity is 132.83329679106652
At time: 349.44003653526306 and batch: 150, loss is 4.868528804779053 and perplexity is 130.12933035149203
At time: 350.63559341430664 and batch: 200, loss is 4.875118780136108 and perplexity is 130.9897112625391
At time: 351.83251762390137 and batch: 250, loss is 4.901652479171753 and perplexity is 134.5118742015864
At time: 353.02741169929504 and batch: 300, loss is 4.9190518760681154 and perplexity is 136.8727792892148
At time: 354.21734166145325 and batch: 350, loss is 4.85571120262146 and perplexity is 128.47202837462075
At time: 355.41076827049255 and batch: 400, loss is 4.866654376983643 and perplexity is 129.88564077828775
At time: 356.60117197036743 and batch: 450, loss is 4.831125822067261 and perplexity is 125.35200529802161
At time: 357.7925729751587 and batch: 500, loss is 4.799895544052124 and perplexity is 121.49772569577584
At time: 358.98306679725647 and batch: 550, loss is 4.817445068359375 and perplexity is 123.64877270465546
At time: 360.17421650886536 and batch: 600, loss is 4.7867640781402585 and perplexity is 119.91271200503085
At time: 361.363653421402 and batch: 650, loss is 4.79511664390564 and perplexity is 120.91848536488695
At time: 362.5522289276123 and batch: 700, loss is 4.8001118087768555 and perplexity is 121.52400420943434
At time: 363.7428469657898 and batch: 750, loss is 4.79713282585144 and perplexity is 121.16252496344126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.006262490915698 and perplexity of 149.3455115404199
Finished 18 epochs...
Completing Train Step...
At time: 367.22332549095154 and batch: 50, loss is 4.890165252685547 and perplexity is 132.97554680192582
At time: 368.45361828804016 and batch: 100, loss is 4.885223922729492 and perplexity is 132.32009149323954
At time: 369.6497769355774 and batch: 150, loss is 4.864715023040771 and perplexity is 129.63399064693252
At time: 370.8468060493469 and batch: 200, loss is 4.871451053619385 and perplexity is 130.5101568000219
At time: 372.0454320907593 and batch: 250, loss is 4.897951526641846 and perplexity is 134.01497221296754
At time: 373.2472369670868 and batch: 300, loss is 4.915761785507202 and perplexity is 136.4231954413671
At time: 374.4460606575012 and batch: 350, loss is 4.853280782699585 and perplexity is 128.1601665285895
At time: 375.6411192417145 and batch: 400, loss is 4.864573764801025 and perplexity is 129.61568007089275
At time: 376.8412253856659 and batch: 450, loss is 4.829347124099732 and perplexity is 125.12924011569228
At time: 378.0357322692871 and batch: 500, loss is 4.798465433120728 and perplexity is 121.3240946555357
At time: 379.23979210853577 and batch: 550, loss is 4.816708688735962 and perplexity is 123.55775378432776
At time: 380.43784189224243 and batch: 600, loss is 4.786469678878785 and perplexity is 119.87741498713817
At time: 381.63279032707214 and batch: 650, loss is 4.7946578311920165 and perplexity is 120.86301915176432
At time: 382.8290305137634 and batch: 700, loss is 4.799781455993652 and perplexity is 121.4838650468248
At time: 384.02519822120667 and batch: 750, loss is 4.796244173049927 and perplexity is 121.05490137327911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.006150001703307 and perplexity of 149.32871272631294
Finished 19 epochs...
Completing Train Step...
At time: 387.5054261684418 and batch: 50, loss is 4.886680603027344 and perplexity is 132.51298001787669
At time: 388.76131200790405 and batch: 100, loss is 4.881672639846801 and perplexity is 131.85101881514984
At time: 389.95936942100525 and batch: 150, loss is 4.861350755691529 and perplexity is 129.19860004070387
At time: 391.16268038749695 and batch: 200, loss is 4.868330783843994 and perplexity is 130.1035645709835
At time: 392.36888337135315 and batch: 250, loss is 4.894813270568847 and perplexity is 133.59505815609924
At time: 393.6181290149689 and batch: 300, loss is 4.912942857742309 and perplexity is 136.03916983248791
At time: 394.8486773967743 and batch: 350, loss is 4.8510098552703855 and perplexity is 127.86945430920174
At time: 396.04610300064087 and batch: 400, loss is 4.862483854293823 and perplexity is 129.3450777650087
At time: 397.2453770637512 and batch: 450, loss is 4.827403802871704 and perplexity is 124.88630992927463
At time: 398.49924755096436 and batch: 500, loss is 4.7970130443573 and perplexity is 121.14801280432873
At time: 399.6950349807739 and batch: 550, loss is 4.815763006210327 and perplexity is 123.44096260805851
At time: 400.8925724029541 and batch: 600, loss is 4.785757207870484 and perplexity is 119.7920362229685
At time: 402.08847212791443 and batch: 650, loss is 4.793855171203614 and perplexity is 120.76604616558797
At time: 403.28448700904846 and batch: 700, loss is 4.799042663574219 and perplexity is 121.39414683389333
At time: 404.4799838066101 and batch: 750, loss is 4.795289573669433 and perplexity is 120.93939757811926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.006125161814135 and perplexity of 149.32500346370776
Finished 20 epochs...
Completing Train Step...
At time: 407.96025037765503 and batch: 50, loss is 4.883580102920532 and perplexity is 132.10275978174425
At time: 409.1896376609802 and batch: 100, loss is 4.878790225982666 and perplexity is 131.47151681418282
At time: 410.38581347465515 and batch: 150, loss is 4.858549423217774 and perplexity is 128.83717827446506
At time: 411.58545446395874 and batch: 200, loss is 4.86573655128479 and perplexity is 129.76648309062836
At time: 412.78753757476807 and batch: 250, loss is 4.891998615264892 and perplexity is 133.2195628099602
At time: 413.98577427864075 and batch: 300, loss is 4.910311574935913 and perplexity is 135.681682834866
At time: 415.1839530467987 and batch: 350, loss is 4.849003572463989 and perplexity is 127.61316919773013
At time: 416.38656759262085 and batch: 400, loss is 4.860669689178467 and perplexity is 129.1106371583686
At time: 417.5840747356415 and batch: 450, loss is 4.825598583221436 and perplexity is 124.66106607702402
At time: 418.7862546443939 and batch: 500, loss is 4.795475454330444 and perplexity is 120.96187996273886
At time: 419.98704838752747 and batch: 550, loss is 4.814594144821167 and perplexity is 123.2967615249821
At time: 421.18621706962585 and batch: 600, loss is 4.784920282363892 and perplexity is 119.69182115449
At time: 422.3832747936249 and batch: 650, loss is 4.792943811416626 and perplexity is 120.65603498497656
At time: 423.5880491733551 and batch: 700, loss is 4.798205604553223 and perplexity is 121.2925752848113
At time: 424.79252648353577 and batch: 750, loss is 4.794142780303955 and perplexity is 120.80078457477923
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.00608648255814 and perplexity of 149.31922779537226
Finished 21 epochs...
Completing Train Step...
At time: 428.2605676651001 and batch: 50, loss is 4.880405235290527 and perplexity is 131.68401608551923
At time: 429.5182640552521 and batch: 100, loss is 4.875896463394165 and perplexity is 131.09161938895144
At time: 430.7144296169281 and batch: 150, loss is 4.855821371078491 and perplexity is 128.48618271942524
At time: 431.91139006614685 and batch: 200, loss is 4.863221921920776 and perplexity is 129.44057841824775
At time: 433.10880517959595 and batch: 250, loss is 4.8891962623596195 and perplexity is 132.8467571915103
At time: 434.3019721508026 and batch: 300, loss is 4.907710103988648 and perplexity is 135.32916960416628
At time: 435.4999544620514 and batch: 350, loss is 4.846908664703369 and perplexity is 127.3461112078167
At time: 436.69465804100037 and batch: 400, loss is 4.858522920608521 and perplexity is 128.83376379831847
At time: 437.8920028209686 and batch: 450, loss is 4.823817501068115 and perplexity is 124.43923208794256
At time: 439.0886812210083 and batch: 500, loss is 4.7937814807891845 and perplexity is 120.75714719348555
At time: 440.28392124176025 and batch: 550, loss is 4.8133221626281735 and perplexity is 123.14002994089242
At time: 441.48024559020996 and batch: 600, loss is 4.784031038284302 and perplexity is 119.58543322058374
At time: 442.67740178108215 and batch: 650, loss is 4.791795606613159 and perplexity is 120.51757665051362
At time: 443.87337923049927 and batch: 700, loss is 4.797273101806641 and perplexity is 121.17952234449912
At time: 445.0709671974182 and batch: 750, loss is 4.793000974655151 and perplexity is 120.66293227181772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0060612878134085 and perplexity of 149.31546578293614
Finished 22 epochs...
Completing Train Step...
At time: 448.52641439437866 and batch: 50, loss is 4.8775778579711915 and perplexity is 131.3122215345395
At time: 449.7480309009552 and batch: 100, loss is 4.873264245986938 and perplexity is 130.74701148688348
At time: 450.94146156311035 and batch: 150, loss is 4.8531359100341795 and perplexity is 128.14160096851927
At time: 452.1376757621765 and batch: 200, loss is 4.860721340179444 and perplexity is 129.1173060242399
At time: 453.3331151008606 and batch: 250, loss is 4.886628036499023 and perplexity is 132.50601445363912
At time: 454.5273325443268 and batch: 300, loss is 4.905429944992066 and perplexity is 135.02094910999946
At time: 455.72249937057495 and batch: 350, loss is 4.844845008850098 and perplexity is 127.08358363647932
At time: 456.9187903404236 and batch: 400, loss is 4.8567595958709715 and perplexity is 128.60678821022748
At time: 458.1142406463623 and batch: 450, loss is 4.822141304016113 and perplexity is 124.23082213083755
At time: 459.36756348609924 and batch: 500, loss is 4.792167415618897 and perplexity is 120.56239450218605
At time: 460.5618619918823 and batch: 550, loss is 4.811887035369873 and perplexity is 122.96343507569806
At time: 461.75547528266907 and batch: 600, loss is 4.782989921569825 and perplexity is 119.46099561552516
At time: 462.9498600959778 and batch: 650, loss is 4.790683012008667 and perplexity is 120.38356400968054
At time: 464.1520276069641 and batch: 700, loss is 4.7962282943725585 and perplexity is 121.05297919681718
At time: 465.3566243648529 and batch: 750, loss is 4.791823568344117 and perplexity is 120.52094657768183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.006205359170603 and perplexity of 149.33697941445345
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 468.7535569667816 and batch: 50, loss is 4.876104793548584 and perplexity is 131.11893257124274
At time: 470.0048735141754 and batch: 100, loss is 4.871577796936035 and perplexity is 130.52669913844449
At time: 471.19937682151794 and batch: 150, loss is 4.8502037048339846 and perplexity is 127.76641383145139
At time: 472.39419078826904 and batch: 200, loss is 4.856459102630615 and perplexity is 128.56814854547616
At time: 473.58902382850647 and batch: 250, loss is 4.880625257492065 and perplexity is 131.71299268027593
At time: 474.78252053260803 and batch: 300, loss is 4.897399854660034 and perplexity is 133.9410602970955
At time: 475.9761266708374 and batch: 350, loss is 4.835498476028443 and perplexity is 125.90132636073456
At time: 477.16900086402893 and batch: 400, loss is 4.846702003479004 and perplexity is 127.31979642377284
At time: 478.36347556114197 and batch: 450, loss is 4.811110153198242 and perplexity is 122.86794407265693
At time: 479.5562047958374 and batch: 500, loss is 4.777346916198731 and perplexity is 118.7887750316342
At time: 480.7481231689453 and batch: 550, loss is 4.794458875656128 and perplexity is 120.83897517694011
At time: 481.9442551136017 and batch: 600, loss is 4.7663122653961185 and perplexity is 117.48518792158438
At time: 483.1370542049408 and batch: 650, loss is 4.772676029205322 and perplexity is 118.2352198905479
At time: 484.3287491798401 and batch: 700, loss is 4.778201456069946 and perplexity is 118.89032816057062
At time: 485.51932287216187 and batch: 750, loss is 4.777356109619141 and perplexity is 118.789867111803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.998732810796693 and perplexity of 148.2252106580032
Finished 24 epochs...
Completing Train Step...
At time: 489.01505184173584 and batch: 50, loss is 4.871015768051148 and perplexity is 130.4533599745745
At time: 490.23430275917053 and batch: 100, loss is 4.8665211963653565 and perplexity is 129.86834368018702
At time: 491.42468643188477 and batch: 150, loss is 4.845726127624512 and perplexity is 127.19560871439316
At time: 492.6157052516937 and batch: 200, loss is 4.8522045707702635 and perplexity is 128.02231322149993
At time: 493.80649733543396 and batch: 250, loss is 4.876941680908203 and perplexity is 131.22871027789873
At time: 494.9946360588074 and batch: 300, loss is 4.894401893615723 and perplexity is 133.54011153078505
At time: 496.18355226516724 and batch: 350, loss is 4.8334689044952395 and perplexity is 125.64605974131604
At time: 497.37312030792236 and batch: 400, loss is 4.845337800979614 and perplexity is 127.14622485956927
At time: 498.5630421638489 and batch: 450, loss is 4.810721864700318 and perplexity is 122.8202451242857
At time: 499.7514753341675 and batch: 500, loss is 4.777393579483032 and perplexity is 118.79431823534637
At time: 500.94252276420593 and batch: 550, loss is 4.7946499061584475 and perplexity is 120.86206131207577
At time: 502.1313123703003 and batch: 600, loss is 4.767095890045166 and perplexity is 117.57728829208496
At time: 503.3197965621948 and batch: 650, loss is 4.7740178966522215 and perplexity is 118.39398237848074
At time: 504.5164306163788 and batch: 700, loss is 4.779809045791626 and perplexity is 119.08160873928021
At time: 505.70951867103577 and batch: 750, loss is 4.7780795001983645 and perplexity is 118.87582967108307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.998182074968205 and perplexity of 148.1436001987733
Finished 25 epochs...
Completing Train Step...
At time: 509.1918103694916 and batch: 50, loss is 4.868322858810425 and perplexity is 130.10253349995236
At time: 510.4225835800171 and batch: 100, loss is 4.863756866455078 and perplexity is 129.50984047221633
At time: 511.61807012557983 and batch: 150, loss is 4.843147821426392 and perplexity is 126.86808190171743
At time: 512.8142325878143 and batch: 200, loss is 4.849759349822998 and perplexity is 127.70965279718854
At time: 514.0085458755493 and batch: 250, loss is 4.874858713150024 and perplexity is 130.95564959247292
At time: 515.2033460140228 and batch: 300, loss is 4.8925573444366455 and perplexity is 133.29401726395108
At time: 516.4000880718231 and batch: 350, loss is 4.832436990737915 and perplexity is 125.51647071756295
At time: 517.5969605445862 and batch: 400, loss is 4.844648494720459 and perplexity is 127.05861237033325
At time: 518.7939851284027 and batch: 450, loss is 4.810640068054199 and perplexity is 122.81019925102404
At time: 520.0436947345734 and batch: 500, loss is 4.777464809417725 and perplexity is 118.80278024824688
At time: 521.2408638000488 and batch: 550, loss is 4.79469711303711 and perplexity is 120.867766967411
At time: 522.4368071556091 and batch: 600, loss is 4.767514429092407 and perplexity is 117.62650927803772
At time: 523.6326830387115 and batch: 650, loss is 4.7746470928192135 and perplexity is 118.46849885867171
At time: 524.8291931152344 and batch: 700, loss is 4.78054253578186 and perplexity is 119.16898594854821
At time: 526.0245351791382 and batch: 750, loss is 4.778262243270874 and perplexity is 118.89755539049622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.998030906499818 and perplexity of 148.1212072502265
Finished 26 epochs...
Completing Train Step...
At time: 529.4615926742554 and batch: 50, loss is 4.866276779174805 and perplexity is 129.8366055033191
At time: 530.6866724491119 and batch: 100, loss is 4.861648445129394 and perplexity is 129.2370668246166
At time: 531.8833119869232 and batch: 150, loss is 4.841142721176148 and perplexity is 126.61395354051066
At time: 533.0781302452087 and batch: 200, loss is 4.847916479110718 and perplexity is 127.47451714722129
At time: 534.277604341507 and batch: 250, loss is 4.873250236511231 and perplexity is 130.74517980263272
At time: 535.472400188446 and batch: 300, loss is 4.891114015579223 and perplexity is 133.10176893446743
At time: 536.6680498123169 and batch: 350, loss is 4.831598806381225 and perplexity is 125.41130885397189
At time: 537.8640475273132 and batch: 400, loss is 4.84405987739563 and perplexity is 126.98384547652655
At time: 539.064929485321 and batch: 450, loss is 4.8104557704925535 and perplexity is 122.78756771628927
At time: 540.2686288356781 and batch: 500, loss is 4.777396955490112 and perplexity is 118.79471928648277
At time: 541.4747881889343 and batch: 550, loss is 4.794511480331421 and perplexity is 120.84533203919091
At time: 542.6917788982391 and batch: 600, loss is 4.767603626251221 and perplexity is 117.63700169640649
At time: 543.9150984287262 and batch: 650, loss is 4.7749100685119625 and perplexity is 118.4996572910036
At time: 545.1182482242584 and batch: 700, loss is 4.780907096862793 and perplexity is 119.21243824290576
At time: 546.3283715248108 and batch: 750, loss is 4.7782055950164795 and perplexity is 118.89082024230053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9979432571765985 and perplexity of 148.10822509560333
Finished 27 epochs...
Completing Train Step...
At time: 549.8089680671692 and batch: 50, loss is 4.864486637115479 and perplexity is 129.60438744863484
At time: 551.0533151626587 and batch: 100, loss is 4.859754381179809 and perplexity is 128.99251522668698
At time: 552.2575979232788 and batch: 150, loss is 4.839427928924561 and perplexity is 126.39702296263091
At time: 553.4563672542572 and batch: 200, loss is 4.846333332061768 and perplexity is 127.27286590548493
At time: 554.6616342067719 and batch: 250, loss is 4.871842918395996 and perplexity is 130.56130915521032
At time: 555.8582143783569 and batch: 300, loss is 4.889832439422608 and perplexity is 132.9312981399828
At time: 557.0570757389069 and batch: 350, loss is 4.8308752822875975 and perplexity is 125.32060356808942
At time: 558.2585289478302 and batch: 400, loss is 4.8434993934631345 and perplexity is 126.91269301321941
At time: 559.4577736854553 and batch: 450, loss is 4.810257921218872 and perplexity is 122.76327668825984
At time: 560.6592891216278 and batch: 500, loss is 4.777216424942017 and perplexity is 118.77327514641856
At time: 561.8587427139282 and batch: 550, loss is 4.79421950340271 and perplexity is 120.81005314085586
At time: 563.0609803199768 and batch: 600, loss is 4.767546586990356 and perplexity is 117.6302919601404
At time: 564.2616732120514 and batch: 650, loss is 4.7749604225158695 and perplexity is 118.50562437344179
At time: 565.4613089561462 and batch: 700, loss is 4.781063861846924 and perplexity is 119.23112804381
At time: 566.6687066555023 and batch: 750, loss is 4.778008832931518 and perplexity is 118.86742933792392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.997957451398983 and perplexity of 148.11032739160746
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 570.1218342781067 and batch: 50, loss is 4.863371906280517 and perplexity is 129.45999393650195
At time: 571.3728232383728 and batch: 100, loss is 4.85872483253479 and perplexity is 128.85977949808714
At time: 572.5726919174194 and batch: 150, loss is 4.837781209945678 and perplexity is 126.18905386636852
At time: 573.7713565826416 and batch: 200, loss is 4.844380779266357 and perplexity is 127.0246013690635
At time: 574.9687170982361 and batch: 250, loss is 4.869378595352173 and perplexity is 130.23996002908223
At time: 576.1661005020142 and batch: 300, loss is 4.8863573741912845 and perplexity is 132.47015492310837
At time: 577.369038105011 and batch: 350, loss is 4.826482343673706 and perplexity is 124.77128529367936
At time: 578.5687725543976 and batch: 400, loss is 4.839022045135498 and perplexity is 126.34573087004317
At time: 579.76491355896 and batch: 450, loss is 4.80545524597168 and perplexity is 122.17509808401834
At time: 581.0234191417694 and batch: 500, loss is 4.771235933303833 and perplexity is 118.06507237874446
At time: 582.222404718399 and batch: 550, loss is 4.78664867401123 and perplexity is 119.89887438141731
At time: 583.4297614097595 and batch: 600, loss is 4.760177841186524 and perplexity is 116.76668997927321
At time: 584.6293654441833 and batch: 650, loss is 4.767385492324829 and perplexity is 117.61134387386
At time: 585.8248374462128 and batch: 700, loss is 4.773676700592041 and perplexity is 118.35359370876347
At time: 587.0197198390961 and batch: 750, loss is 4.7712015914917 and perplexity is 118.06101787982911
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.996018875476926 and perplexity of 147.82348240234504
Finished 29 epochs...
Completing Train Step...
At time: 590.4511857032776 and batch: 50, loss is 4.861362962722779 and perplexity is 129.2001771816781
At time: 591.6708753108978 and batch: 100, loss is 4.8570161819458 and perplexity is 128.6397911550777
At time: 592.8736546039581 and batch: 150, loss is 4.836491918563842 and perplexity is 126.02646424195972
At time: 594.0731160640717 and batch: 200, loss is 4.843162202835083 and perplexity is 126.86990645657296
At time: 595.2751500606537 and batch: 250, loss is 4.868462400436401 and perplexity is 130.12068948574907
At time: 596.4702959060669 and batch: 300, loss is 4.8855866527557374 and perplexity is 132.3680966694328
At time: 597.6660685539246 and batch: 350, loss is 4.82593469619751 and perplexity is 124.70297332133347
At time: 598.8597342967987 and batch: 400, loss is 4.838536405563355 and perplexity is 126.28438728000438
At time: 600.0572285652161 and batch: 450, loss is 4.8051578807830815 and perplexity is 122.13877286413019
At time: 601.2523429393768 and batch: 500, loss is 4.771161766052246 and perplexity is 118.0563161415347
At time: 602.446784734726 and batch: 550, loss is 4.786867513656616 and perplexity is 119.92511587980444
At time: 603.6418159008026 and batch: 600, loss is 4.760485754013062 and perplexity is 116.80264947674138
At time: 604.8365018367767 and batch: 650, loss is 4.767877187728882 and perplexity is 117.66918705055394
At time: 606.0310740470886 and batch: 700, loss is 4.774192628860473 and perplexity is 118.41467142793446
At time: 607.2296621799469 and batch: 750, loss is 4.77124252319336 and perplexity is 118.06585041709197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.995874804119731 and perplexity of 147.80218680668963
Finished 30 epochs...
Completing Train Step...
At time: 610.679850101471 and batch: 50, loss is 4.860154399871826 and perplexity is 129.04412496564984
At time: 611.9003572463989 and batch: 100, loss is 4.8560197639465335 and perplexity is 128.51167599047673
At time: 613.0948972702026 and batch: 150, loss is 4.835655736923218 and perplexity is 125.92112727288517
At time: 614.2873554229736 and batch: 200, loss is 4.842376165390014 and perplexity is 126.77022114277297
At time: 615.484452009201 and batch: 250, loss is 4.867897996902466 and perplexity is 130.0472696299272
At time: 616.6767477989197 and batch: 300, loss is 4.885084352493286 and perplexity is 132.30162483554295
At time: 617.8732635974884 and batch: 350, loss is 4.8255003070831295 and perplexity is 124.64881547083381
At time: 619.0661072731018 and batch: 400, loss is 4.838183460235595 and perplexity is 126.23982366026821
At time: 620.2602078914642 and batch: 450, loss is 4.8049938774108885 and perplexity is 122.11874333600493
At time: 621.4522404670715 and batch: 500, loss is 4.771127033233642 and perplexity is 118.05221578413006
At time: 622.6455147266388 and batch: 550, loss is 4.786999273300171 and perplexity is 119.94091821135797
At time: 623.8379855155945 and batch: 600, loss is 4.7606564807891845 and perplexity is 116.82259251888632
At time: 625.0289726257324 and batch: 650, loss is 4.7681636619567875 and perplexity is 117.70290106893067
At time: 626.2236270904541 and batch: 700, loss is 4.774468469619751 and perplexity is 118.44733952620012
At time: 627.4217600822449 and batch: 750, loss is 4.771140375137329 and perplexity is 118.05379083593013
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9958311568859015 and perplexity of 147.79573579086704
Finished 31 epochs...
Completing Train Step...
At time: 630.9356949329376 and batch: 50, loss is 4.859140701293946 and perplexity is 128.91337939918176
At time: 632.1768946647644 and batch: 100, loss is 4.855238676071167 and perplexity is 128.41133627068822
At time: 633.3668825626373 and batch: 150, loss is 4.834960823059082 and perplexity is 125.83365333270267
At time: 634.5551760196686 and batch: 200, loss is 4.841764001846314 and perplexity is 126.6926407833191
At time: 635.7449402809143 and batch: 250, loss is 4.867444114685059 and perplexity is 129.98825688025178
At time: 636.9369957447052 and batch: 300, loss is 4.884657363891602 and perplexity is 132.24514560860504
At time: 638.127281665802 and batch: 350, loss is 4.825136919021606 and perplexity is 124.60352780840495
At time: 639.315826177597 and batch: 400, loss is 4.837887001037598 and perplexity is 126.20240425032874
At time: 640.5068602561951 and batch: 450, loss is 4.804808225631714 and perplexity is 122.09607387841169
At time: 641.7326185703278 and batch: 500, loss is 4.771041927337646 and perplexity is 118.04216927204618
At time: 642.9268398284912 and batch: 550, loss is 4.787047815322876 and perplexity is 119.94674052744544
At time: 644.1175079345703 and batch: 600, loss is 4.760761442184449 and perplexity is 116.83485502472908
At time: 645.3073718547821 and batch: 650, loss is 4.768345909118652 and perplexity is 117.72435404340624
At time: 646.4948625564575 and batch: 700, loss is 4.774641704559326 and perplexity is 118.46786052133118
At time: 647.6860847473145 and batch: 750, loss is 4.77097864151001 and perplexity is 118.03469911204795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.995813768963481 and perplexity of 147.79316595242122
Finished 32 epochs...
Completing Train Step...
At time: 651.1624364852905 and batch: 50, loss is 4.858265161514282 and perplexity is 128.80056000352835
At time: 652.3878264427185 and batch: 100, loss is 4.854551601409912 and perplexity is 128.3231383980501
At time: 653.5853202342987 and batch: 150, loss is 4.834341268539429 and perplexity is 125.75571666955491
At time: 654.7838342189789 and batch: 200, loss is 4.841259078979492 and perplexity is 126.62868691917257
At time: 655.9813015460968 and batch: 250, loss is 4.867052183151245 and perplexity is 129.93732036582014
At time: 657.1822443008423 and batch: 300, loss is 4.884273920059204 and perplexity is 132.19444674386156
At time: 658.3812761306763 and batch: 350, loss is 4.8248128414154055 and perplexity is 124.56315313800529
At time: 659.5801010131836 and batch: 400, loss is 4.837614545822143 and perplexity is 126.16802443078109
At time: 660.7783000469208 and batch: 450, loss is 4.804641675949097 and perplexity is 122.07574050936356
At time: 661.9745471477509 and batch: 500, loss is 4.770948839187622 and perplexity is 118.0311814563095
At time: 663.1719012260437 and batch: 550, loss is 4.787046356201172 and perplexity is 119.94656551068066
At time: 664.3687031269073 and batch: 600, loss is 4.760816526412964 and perplexity is 116.84129095983947
At time: 665.5655310153961 and batch: 650, loss is 4.768452472686768 and perplexity is 117.73689983907768
At time: 666.7626957893372 and batch: 700, loss is 4.774757823944092 and perplexity is 118.48161773513354
At time: 667.9590899944305 and batch: 750, loss is 4.770814847946167 and perplexity is 118.01536737127385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.995815898096839 and perplexity of 147.79348062411586
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 671.4218595027924 and batch: 50, loss is 4.857735538482666 and perplexity is 128.7323623215911
At time: 672.6493601799011 and batch: 100, loss is 4.854163255691528 and perplexity is 128.27331433179125
At time: 673.8451843261719 and batch: 150, loss is 4.833819246292114 and perplexity is 125.69008651941596
At time: 675.0433542728424 and batch: 200, loss is 4.840808620452881 and perplexity is 126.57165879280345
At time: 676.239408493042 and batch: 250, loss is 4.866432609558106 and perplexity is 129.85683956782128
At time: 677.4407474994659 and batch: 300, loss is 4.883475866317749 and perplexity is 132.08899055648746
At time: 678.6412928104401 and batch: 350, loss is 4.822972087860108 and perplexity is 124.33407397483325
At time: 679.8407089710236 and batch: 400, loss is 4.83546555519104 and perplexity is 125.89718165186466
At time: 681.0459063053131 and batch: 450, loss is 4.8024877834320066 and perplexity is 121.81308545228678
At time: 682.2416815757751 and batch: 500, loss is 4.7683757781982425 and perplexity is 117.72787041402196
At time: 683.4372637271881 and batch: 550, loss is 4.783916063308716 and perplexity is 119.5716846787031
At time: 684.632749080658 and batch: 600, loss is 4.757721338272095 and perplexity is 116.48020428556593
At time: 685.8250629901886 and batch: 650, loss is 4.765286083221436 and perplexity is 117.364688553676
At time: 687.0200622081757 and batch: 700, loss is 4.771636419296264 and perplexity is 118.11236525586283
At time: 688.214024066925 and batch: 750, loss is 4.7677962017059325 and perplexity is 117.6596578769412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.99627685546875 and perplexity of 147.86162282264775
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 691.6839122772217 and batch: 50, loss is 4.857307062149048 and perplexity is 128.67721536639183
At time: 692.907044172287 and batch: 100, loss is 4.853713874816894 and perplexity is 128.21568370762463
At time: 694.1000852584839 and batch: 150, loss is 4.8332593059539795 and perplexity is 125.61972727019935
At time: 695.2946228981018 and batch: 200, loss is 4.840400648117066 and perplexity is 126.52003158946916
At time: 696.4904541969299 and batch: 250, loss is 4.866218318939209 and perplexity is 129.8290154466329
At time: 697.6838960647583 and batch: 300, loss is 4.883010969161988 and perplexity is 132.02759703241347
At time: 698.8807735443115 and batch: 350, loss is 4.822117328643799 and perplexity is 124.22784368632888
At time: 700.0852115154266 and batch: 400, loss is 4.834674053192138 and perplexity is 125.79757320627323
At time: 701.2916023731232 and batch: 450, loss is 4.801698541641235 and perplexity is 121.7169834034501
At time: 702.5212914943695 and batch: 500, loss is 4.767222213745117 and perplexity is 117.59214202834195
At time: 703.7214002609253 and batch: 550, loss is 4.782719125747681 and perplexity is 119.42865045666794
At time: 704.9206721782684 and batch: 600, loss is 4.756533098220825 and perplexity is 116.34188003913377
At time: 706.1134562492371 and batch: 650, loss is 4.764058904647827 and perplexity is 117.22074946013443
At time: 707.3086953163147 and batch: 700, loss is 4.770449800491333 and perplexity is 117.97229402416984
At time: 708.5037093162537 and batch: 750, loss is 4.766674785614014 and perplexity is 117.52778639844723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.996963855832122 and perplexity of 147.96323871233557
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 711.9125764369965 and batch: 50, loss is 4.85704930305481 and perplexity is 128.64405191818366
At time: 713.1368269920349 and batch: 100, loss is 4.853458585739136 and perplexity is 128.18295582169043
At time: 714.3317918777466 and batch: 150, loss is 4.833044729232788 and perplexity is 125.5927750927631
At time: 715.5355231761932 and batch: 200, loss is 4.840231580734253 and perplexity is 126.4986429869634
At time: 716.7286627292633 and batch: 250, loss is 4.866138820648193 and perplexity is 129.818694672028
At time: 717.9252638816833 and batch: 300, loss is 4.882807331085205 and perplexity is 132.00071392377674
At time: 719.1231553554535 and batch: 350, loss is 4.821729888916016 and perplexity is 124.17972220707635
At time: 720.3247668743134 and batch: 400, loss is 4.83435658454895 and perplexity is 125.75764276005873
At time: 721.5205252170563 and batch: 450, loss is 4.801406173706055 and perplexity is 121.68140246195273
At time: 722.7171621322632 and batch: 500, loss is 4.766731643676758 and perplexity is 117.53446899067829
At time: 723.91255402565 and batch: 550, loss is 4.78219428062439 and perplexity is 119.36598535806874
At time: 725.1082091331482 and batch: 600, loss is 4.7560749435424805 and perplexity is 116.2885896710543
At time: 726.306720495224 and batch: 650, loss is 4.763565034866333 and perplexity is 117.16287196736069
At time: 727.5017032623291 and batch: 700, loss is 4.769965419769287 and perplexity is 117.9151643565808
At time: 728.6968388557434 and batch: 750, loss is 4.766241750717163 and perplexity is 117.47690378335618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.997362713481104 and perplexity of 148.02226675297442
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 732.1280736923218 and batch: 50, loss is 4.856930427551269 and perplexity is 128.62876020065772
At time: 733.3513126373291 and batch: 100, loss is 4.853352699279785 and perplexity is 128.16938370091393
At time: 734.544150352478 and batch: 150, loss is 4.832963981628418 and perplexity is 125.58263418648063
At time: 735.73779463768 and batch: 200, loss is 4.840149822235108 and perplexity is 126.48830107054445
At time: 736.9321866035461 and batch: 250, loss is 4.866049175262451 and perplexity is 129.80705754668267
At time: 738.133364200592 and batch: 300, loss is 4.882709970474243 and perplexity is 131.98786287922468
At time: 739.3271698951721 and batch: 350, loss is 4.821568412780762 and perplexity is 124.15967176433499
At time: 740.5239894390106 and batch: 400, loss is 4.834230270385742 and perplexity is 125.74175879185265
At time: 741.7205181121826 and batch: 450, loss is 4.801272420883179 and perplexity is 121.66512831926246
At time: 742.9159233570099 and batch: 500, loss is 4.766513242721557 and perplexity is 117.50880215331462
At time: 744.1087625026703 and batch: 550, loss is 4.78198974609375 and perplexity is 119.34157338890955
At time: 745.3032801151276 and batch: 600, loss is 4.75588960647583 and perplexity is 116.26703908208293
At time: 746.4988436698914 and batch: 650, loss is 4.763377599716186 and perplexity is 117.14091358481254
At time: 747.6938989162445 and batch: 700, loss is 4.769752616882324 and perplexity is 117.89007433889783
At time: 748.8909385204315 and batch: 750, loss is 4.766059827804566 and perplexity is 117.45553398674373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9974720089934594 and perplexity of 148.03844580659202
Annealing...
Model not improving. Stopping early with 147.79316595242122loss at 36 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -147.79316595242122
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fda7a4860>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.1690291668353847, 'wordvec_dim': 200, 'lr': 6.484969183534676, 'dropout': 0.00015306102629486507, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.842315673828125 and batch: 50, loss is 7.162471733093262 and perplexity is 1290.095767656079
At time: 3.0199177265167236 and batch: 100, loss is 6.10756175994873 and perplexity is 449.2420188234599
At time: 4.192317485809326 and batch: 150, loss is 5.800650844573974 and perplexity is 330.5146035584069
At time: 5.358006000518799 and batch: 200, loss is 5.623258514404297 and perplexity is 276.7898390414067
At time: 6.535750150680542 and batch: 250, loss is 5.562305183410644 and perplexity is 260.42246647816245
At time: 7.7059948444366455 and batch: 300, loss is 5.513922023773193 and perplexity is 248.12236302382505
At time: 8.932190418243408 and batch: 350, loss is 5.396237115859986 and perplexity is 220.57485502752976
At time: 10.105222940444946 and batch: 400, loss is 5.389906396865845 and perplexity is 219.18286839029966
At time: 11.279072046279907 and batch: 450, loss is 5.304871187210083 and perplexity is 201.31506878777788
At time: 12.444282531738281 and batch: 500, loss is 5.271751785278321 and perplexity is 194.7568359476954
At time: 13.613077402114868 and batch: 550, loss is 5.272724504470825 and perplexity is 194.94637182754258
At time: 14.787265062332153 and batch: 600, loss is 5.20969554901123 and perplexity is 183.03832351133332
At time: 15.95673394203186 and batch: 650, loss is 5.178076858520508 and perplexity is 177.34143017637072
At time: 17.13116955757141 and batch: 700, loss is 5.157006282806396 and perplexity is 173.64383616237038
At time: 18.29792857170105 and batch: 750, loss is 5.137672691345215 and perplexity is 170.31892200725312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.93946235124455 and perplexity of 139.69512245708447
Finished 1 epochs...
Completing Train Step...
At time: 21.745290756225586 and batch: 50, loss is 5.170352687835694 and perplexity is 175.9768914495601
At time: 22.904753923416138 and batch: 100, loss is 5.10535195350647 and perplexity is 164.90209819120577
At time: 24.122159481048584 and batch: 150, loss is 5.06399188041687 and perplexity is 158.22085610282917
At time: 25.27778720855713 and batch: 200, loss is 5.022680950164795 and perplexity is 151.81777457440523
At time: 26.437620162963867 and batch: 250, loss is 5.016601572036743 and perplexity is 150.8976167438979
At time: 27.596571445465088 and batch: 300, loss is 5.036619367599488 and perplexity is 153.9486903881715
At time: 28.748992204666138 and batch: 350, loss is 4.961243648529052 and perplexity is 142.77124277941465
At time: 29.908538103103638 and batch: 400, loss is 4.979558420181275 and perplexity is 145.41015728366662
At time: 31.06881284713745 and batch: 450, loss is 4.919676361083984 and perplexity is 136.95828098344597
At time: 32.22724509239197 and batch: 500, loss is 4.89502197265625 and perplexity is 133.62294263327612
At time: 33.3795440196991 and batch: 550, loss is 4.903683643341065 and perplexity is 134.78536756174083
At time: 34.53296136856079 and batch: 600, loss is 4.858005046844482 and perplexity is 128.76706144531508
At time: 35.68892025947571 and batch: 650, loss is 4.830878381729126 and perplexity is 125.32099199257443
At time: 36.84780406951904 and batch: 700, loss is 4.824363842010498 and perplexity is 124.50723691048873
At time: 38.00785732269287 and batch: 750, loss is 4.823416166305542 and perplexity is 124.38930031862535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.803334790606831 and perplexity of 121.91630571763403
Finished 2 epochs...
Completing Train Step...
At time: 41.39863896369934 and batch: 50, loss is 4.882088689804077 and perplexity is 131.9058868390227
At time: 42.59792375564575 and batch: 100, loss is 4.841322870254516 and perplexity is 126.636764982218
At time: 43.76189684867859 and batch: 150, loss is 4.8049883460998535 and perplexity is 122.11806786112047
At time: 44.931519985198975 and batch: 200, loss is 4.7793098735809325 and perplexity is 119.0221813428793
At time: 46.16271710395813 and batch: 250, loss is 4.769852504730225 and perplexity is 117.90185071286093
At time: 47.33229351043701 and batch: 300, loss is 4.808650245666504 and perplexity is 122.56607173286736
At time: 48.50567412376404 and batch: 350, loss is 4.7481628513336185 and perplexity is 115.37213394875937
At time: 49.67021417617798 and batch: 400, loss is 4.7729855823516845 and perplexity is 118.27182564029603
At time: 50.8402681350708 and batch: 450, loss is 4.713808422088623 and perplexity is 111.47589978246366
At time: 52.00651407241821 and batch: 500, loss is 4.691749248504639 and perplexity is 109.04375770487505
At time: 53.171257734298706 and batch: 550, loss is 4.695909490585327 and perplexity is 109.49835108791576
At time: 54.336302518844604 and batch: 600, loss is 4.661132717132569 and perplexity is 105.75580575041776
At time: 55.503703594207764 and batch: 650, loss is 4.634952983856201 and perplexity is 103.02307413369228
At time: 56.66866850852966 and batch: 700, loss is 4.632858381271363 and perplexity is 102.80750757825994
At time: 57.836742877960205 and batch: 750, loss is 4.63780011177063 and perplexity is 103.31681196004777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.747027374977289 and perplexity of 115.24120596536328
Finished 3 epochs...
Completing Train Step...
At time: 61.22000455856323 and batch: 50, loss is 4.702727909088135 and perplexity is 110.24750780058852
At time: 62.43957257270813 and batch: 100, loss is 4.673875102996826 and perplexity is 107.11200931893158
At time: 63.60440468788147 and batch: 150, loss is 4.638847017288208 and perplexity is 103.42503153850014
At time: 64.76870536804199 and batch: 200, loss is 4.625628461837769 and perplexity is 102.06689808087526
At time: 65.93057751655579 and batch: 250, loss is 4.609842643737793 and perplexity is 100.46833907016998
At time: 67.10442519187927 and batch: 300, loss is 4.6521527957916256 and perplexity is 104.81037821904826
At time: 68.2696783542633 and batch: 350, loss is 4.597489700317383 and perplexity is 99.23489338923342
At time: 69.43567991256714 and batch: 400, loss is 4.625484590530395 and perplexity is 102.05221463909713
At time: 70.60492706298828 and batch: 450, loss is 4.566938982009888 and perplexity is 96.24903835584594
At time: 71.77251243591309 and batch: 500, loss is 4.550611705780029 and perplexity is 94.69031321464372
At time: 72.93922853469849 and batch: 550, loss is 4.555330715179443 and perplexity is 95.13821368480862
At time: 74.13686323165894 and batch: 600, loss is 4.519262237548828 and perplexity is 91.76787010889898
At time: 75.30747747421265 and batch: 650, loss is 4.488975868225098 and perplexity is 89.03022049548935
At time: 76.49261116981506 and batch: 700, loss is 4.489638414382934 and perplexity is 89.08922667100892
At time: 77.6781210899353 and batch: 750, loss is 4.505254640579223 and perplexity is 90.49138389371433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.724799577579942 and perplexity of 112.7079069187634
Finished 4 epochs...
Completing Train Step...
At time: 81.07983374595642 and batch: 50, loss is 4.5713132381439205 and perplexity is 96.67097846654003
At time: 82.27543306350708 and batch: 100, loss is 4.546987705230713 and perplexity is 94.3477765190141
At time: 83.44077038764954 and batch: 150, loss is 4.511757192611694 and perplexity is 91.08172611133948
At time: 84.60646677017212 and batch: 200, loss is 4.502448635101318 and perplexity is 90.2378204913262
At time: 85.77448892593384 and batch: 250, loss is 4.491178874969482 and perplexity is 89.2265708728865
At time: 86.94664883613586 and batch: 300, loss is 4.533849325180054 and perplexity is 93.1163070454873
At time: 88.12029528617859 and batch: 350, loss is 4.478755254745483 and perplexity is 88.12491131219468
At time: 89.2886073589325 and batch: 400, loss is 4.510447340011597 and perplexity is 90.96250057655598
At time: 90.45406770706177 and batch: 450, loss is 4.454812526702881 and perplexity is 86.0400191269825
At time: 91.62172317504883 and batch: 500, loss is 4.438171262741089 and perplexity is 84.62005224856469
At time: 92.78608083724976 and batch: 550, loss is 4.441939630508423 and perplexity is 84.93953330892501
At time: 93.95128798484802 and batch: 600, loss is 4.408214244842529 and perplexity is 82.12268148282891
At time: 95.12148451805115 and batch: 650, loss is 4.379225606918335 and perplexity is 79.77623131700508
At time: 96.2877266407013 and batch: 700, loss is 4.37983808517456 and perplexity is 79.82510749031913
At time: 97.45328855514526 and batch: 750, loss is 4.393523406982422 and perplexity is 80.92504913525639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.715717404387718 and perplexity of 111.6889085522221
Finished 5 epochs...
Completing Train Step...
At time: 100.8300507068634 and batch: 50, loss is 4.462015027999878 and perplexity is 86.66195955104631
At time: 102.07051873207092 and batch: 100, loss is 4.443752870559693 and perplexity is 85.09368879086736
At time: 103.23590350151062 and batch: 150, loss is 4.409980726242066 and perplexity is 82.26787787775724
At time: 104.43802762031555 and batch: 200, loss is 4.402595319747925 and perplexity is 81.66253426404518
At time: 105.60224533081055 and batch: 250, loss is 4.386771783828736 and perplexity is 80.38051401628488
At time: 106.76772260665894 and batch: 300, loss is 4.435471353530883 and perplexity is 84.39189393209756
At time: 107.93461894989014 and batch: 350, loss is 4.385455646514893 and perplexity is 80.27479181021347
At time: 109.10425472259521 and batch: 400, loss is 4.41213243484497 and perplexity is 82.44508495895992
At time: 110.27168273925781 and batch: 450, loss is 4.361506099700928 and perplexity is 78.37508627115969
At time: 111.43716621398926 and batch: 500, loss is 4.347640895843506 and perplexity is 77.2958985901459
At time: 112.60340094566345 and batch: 550, loss is 4.350470094680786 and perplexity is 77.51489370084354
At time: 113.77208113670349 and batch: 600, loss is 4.312902956008911 and perplexity is 74.65690036947275
At time: 114.93510460853577 and batch: 650, loss is 4.286670455932617 and perplexity is 72.72392742698146
At time: 116.09811067581177 and batch: 700, loss is 4.288432559967041 and perplexity is 72.85218752352908
At time: 117.26892900466919 and batch: 750, loss is 4.30614218711853 and perplexity is 74.15386469004987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.713281320971112 and perplexity of 111.41715619437835
Finished 6 epochs...
Completing Train Step...
At time: 120.68089294433594 and batch: 50, loss is 4.375797090530395 and perplexity is 79.5031855389937
At time: 121.90193176269531 and batch: 100, loss is 4.358309125900268 and perplexity is 78.12492326907694
At time: 123.06637263298035 and batch: 150, loss is 4.3237162208557125 and perplexity is 75.46856567299291
At time: 124.23078107833862 and batch: 200, loss is 4.319541149139404 and perplexity is 75.15413583927526
At time: 125.39519548416138 and batch: 250, loss is 4.30230131149292 and perplexity is 73.86959518998366
At time: 126.56893587112427 and batch: 300, loss is 4.352953042984009 and perplexity is 77.70759831342542
At time: 127.7340784072876 and batch: 350, loss is 4.306621427536011 and perplexity is 74.18941073599238
At time: 128.9008014202118 and batch: 400, loss is 4.330818538665771 and perplexity is 76.00647535267832
At time: 130.07571148872375 and batch: 450, loss is 4.282855978012085 and perplexity is 72.44705201480706
At time: 131.24717712402344 and batch: 500, loss is 4.267339725494384 and perplexity is 71.33162129257781
At time: 132.41500091552734 and batch: 550, loss is 4.267945804595947 and perplexity is 71.37486700136022
At time: 133.5805470943451 and batch: 600, loss is 4.234375638961792 and perplexity is 69.018572770326
At time: 134.81894898414612 and batch: 650, loss is 4.207934761047364 and perplexity is 67.21757600650854
At time: 135.98737263679504 and batch: 700, loss is 4.21378231048584 and perplexity is 67.61178556197945
At time: 137.15141129493713 and batch: 750, loss is 4.227900304794312 and perplexity is 68.57309830245605
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.71889832962391 and perplexity of 112.0447482697668
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 140.5312716960907 and batch: 50, loss is 4.2951802730560305 and perplexity is 73.34543545908802
At time: 141.7515504360199 and batch: 100, loss is 4.276494541168213 and perplexity is 71.98764745469722
At time: 142.9150037765503 and batch: 150, loss is 4.237328276634217 and perplexity is 69.22266075905559
At time: 144.0793468952179 and batch: 200, loss is 4.21568865776062 and perplexity is 67.74080003925555
At time: 145.2416603565216 and batch: 250, loss is 4.185331130027771 and perplexity is 65.71525760946899
At time: 146.40765023231506 and batch: 300, loss is 4.225331125259399 and perplexity is 68.39714782262058
At time: 147.5673909187317 and batch: 350, loss is 4.16308162689209 and perplexity is 64.26927166492365
At time: 148.73083329200745 and batch: 400, loss is 4.167965168952942 and perplexity is 64.58390098377684
At time: 149.9055461883545 and batch: 450, loss is 4.108128538131714 and perplexity is 60.832764774108
At time: 151.0728418827057 and batch: 500, loss is 4.076215581893921 and perplexity is 58.922061664514956
At time: 152.24212980270386 and batch: 550, loss is 4.059559507369995 and perplexity is 57.94877944586033
At time: 153.41451954841614 and batch: 600, loss is 4.015242280960083 and perplexity is 55.43672503852183
At time: 154.57500338554382 and batch: 650, loss is 3.970084466934204 and perplexity is 52.98900646957514
At time: 155.72861552238464 and batch: 700, loss is 3.962097101211548 and perplexity is 52.56744970086928
At time: 156.8856716156006 and batch: 750, loss is 3.9618571329116823 and perplexity is 52.55483669275787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.641720084256904 and perplexity of 103.72260585111961
Finished 8 epochs...
Completing Train Step...
At time: 160.2527232170105 and batch: 50, loss is 4.188425750732422 and perplexity is 65.91893639802962
At time: 161.45880794525146 and batch: 100, loss is 4.170920004844666 and perplexity is 64.77501803313528
At time: 162.61602473258972 and batch: 150, loss is 4.136513128280639 and perplexity is 62.58421741080129
At time: 163.82558417320251 and batch: 200, loss is 4.124329695701599 and perplexity is 61.826352887636
At time: 164.97943902015686 and batch: 250, loss is 4.096473755836487 and perplexity is 60.12788771740428
At time: 166.1362373828888 and batch: 300, loss is 4.144576759338379 and perplexity is 63.090913610107584
At time: 167.28872537612915 and batch: 350, loss is 4.091854095458984 and perplexity is 59.85075791258077
At time: 168.4438235759735 and batch: 400, loss is 4.103406443595886 and perplexity is 60.546183871545
At time: 169.5959665775299 and batch: 450, loss is 4.050510840415955 and perplexity is 57.42678547671426
At time: 170.74584412574768 and batch: 500, loss is 4.024974226951599 and perplexity is 55.97886601702871
At time: 171.90364599227905 and batch: 550, loss is 4.016087379455566 and perplexity is 55.48359433324214
At time: 173.05810642242432 and batch: 600, loss is 3.9772986507415773 and perplexity is 53.372661115661366
At time: 174.21584010124207 and batch: 650, loss is 3.9406630325317384 and perplexity is 51.45270481054459
At time: 175.36695432662964 and batch: 700, loss is 3.9408366823196412 and perplexity is 51.4616403376257
At time: 176.51974153518677 and batch: 750, loss is 3.9468491649627686 and perplexity is 51.77198459175306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.643124957417333 and perplexity of 103.86842536118434
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 179.9162917137146 and batch: 50, loss is 4.151144895553589 and perplexity is 63.50666719307591
At time: 181.13197827339172 and batch: 100, loss is 4.1452944946289065 and perplexity is 63.13621243965688
At time: 182.29268431663513 and batch: 150, loss is 4.112733545303345 and perplexity is 61.11354609597399
At time: 183.45504570007324 and batch: 200, loss is 4.095656251907348 and perplexity is 60.07875301958518
At time: 184.61930203437805 and batch: 250, loss is 4.064284482002258 and perplexity is 58.22323384313691
At time: 185.77845430374146 and batch: 300, loss is 4.107887525558471 and perplexity is 60.818105079588626
At time: 186.9397623538971 and batch: 350, loss is 4.052753839492798 and perplexity is 57.55573826995877
At time: 188.09981417655945 and batch: 400, loss is 4.052653713226318 and perplexity is 57.54997571726758
At time: 189.26030778884888 and batch: 450, loss is 3.994945707321167 and perplexity is 54.32289120846211
At time: 190.42121505737305 and batch: 500, loss is 3.959990644454956 and perplexity is 52.45683518451893
At time: 191.58178877830505 and batch: 550, loss is 3.9459727668762206 and perplexity is 51.72663160006049
At time: 192.7455770969391 and batch: 600, loss is 3.8992508029937745 and perplexity is 49.36545079980063
At time: 193.9669806957245 and batch: 650, loss is 3.852257800102234 and perplexity is 47.09928404210312
At time: 195.13924193382263 and batch: 700, loss is 3.8481575202941896 and perplexity is 46.906559181710904
At time: 196.3089051246643 and batch: 750, loss is 3.8495570850372314 and perplexity is 46.972253909447936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.611794050349745 and perplexity of 100.66458506691598
Finished 10 epochs...
Completing Train Step...
At time: 199.69752478599548 and batch: 50, loss is 4.1169719314575195 and perplexity is 61.37311859925348
At time: 200.91405200958252 and batch: 100, loss is 4.103636651039124 and perplexity is 60.560123658191635
At time: 202.0805630683899 and batch: 150, loss is 4.070878314971924 and perplexity is 58.60841664197068
At time: 203.24911546707153 and batch: 200, loss is 4.056203565597534 and perplexity is 57.75463267110285
At time: 204.414048910141 and batch: 250, loss is 4.0257218551635745 and perplexity is 56.020733045072966
At time: 205.5815761089325 and batch: 300, loss is 4.073406567573548 and perplexity is 58.75678099607543
At time: 206.74933409690857 and batch: 350, loss is 4.022195167541504 and perplexity is 55.82351338973259
At time: 207.91666913032532 and batch: 400, loss is 4.026615171432495 and perplexity is 56.070799636619896
At time: 209.08381962776184 and batch: 450, loss is 3.9732638359069825 and perplexity is 53.15774617339176
At time: 210.2515468597412 and batch: 500, loss is 3.9420824575424196 and perplexity is 51.52578992376789
At time: 211.41639637947083 and batch: 550, loss is 3.9325259017944334 and perplexity is 51.03572623010251
At time: 212.58587002754211 and batch: 600, loss is 3.8898158121109008 and perplexity is 48.90187856039345
At time: 213.7565267086029 and batch: 650, loss is 3.8475754261016846 and perplexity is 46.879263091239444
At time: 214.93509340286255 and batch: 700, loss is 3.848086423873901 and perplexity is 46.90322441181155
At time: 216.10179042816162 and batch: 750, loss is 3.8526029443740843 and perplexity is 47.11554289586223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.611412225767624 and perplexity of 100.62615619080002
Finished 11 epochs...
Completing Train Step...
At time: 219.47678017616272 and batch: 50, loss is 4.096059546470642 and perplexity is 60.10298734050394
At time: 220.67591857910156 and batch: 100, loss is 4.081853613853455 and perplexity is 59.255204383471344
At time: 221.8416085243225 and batch: 150, loss is 4.0490875864028935 and perplexity is 57.34511070958441
At time: 223.0414173603058 and batch: 200, loss is 4.035550675392151 and perplexity is 56.57406560780579
At time: 224.21074771881104 and batch: 250, loss is 4.005434632301331 and perplexity is 54.895678649925316
At time: 225.3910653591156 and batch: 300, loss is 4.054501314163208 and perplexity is 57.65640339398298
At time: 226.56471371650696 and batch: 350, loss is 4.004935569763184 and perplexity is 54.86828910832055
At time: 227.76095414161682 and batch: 400, loss is 4.0114098739624025 and perplexity is 55.224675535093006
At time: 228.95585536956787 and batch: 450, loss is 3.9597882080078124 and perplexity is 52.44621708395732
At time: 230.13173007965088 and batch: 500, loss is 3.9302088737487795 and perplexity is 50.91761191103455
At time: 231.3223898410797 and batch: 550, loss is 3.923084297180176 and perplexity is 50.556134701348036
At time: 232.4981586933136 and batch: 600, loss is 3.8817067670822145 and perplexity is 48.506934498928544
At time: 233.67274522781372 and batch: 650, loss is 3.84132257938385 and perplexity is 46.58704878260974
At time: 234.84299731254578 and batch: 700, loss is 3.844164924621582 and perplexity is 46.71965362395124
At time: 236.0181941986084 and batch: 750, loss is 3.8499357318878173 and perplexity is 46.99004317316747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.612490631813227 and perplexity of 100.73473057909804
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 239.40740633010864 and batch: 50, loss is 4.087435970306396 and perplexity is 59.586913052635545
At time: 240.64441585540771 and batch: 100, loss is 4.0824885129928585 and perplexity is 59.29283740706259
At time: 241.81326508522034 and batch: 150, loss is 4.051211662292481 and perplexity is 57.467045530190205
At time: 242.9812889099121 and batch: 200, loss is 4.037186985015869 and perplexity is 56.66671407593046
At time: 244.14891815185547 and batch: 250, loss is 4.00585533618927 and perplexity is 54.91877833408716
At time: 245.32173371315002 and batch: 300, loss is 4.051546821594238 and perplexity is 57.4863093730919
At time: 246.49307012557983 and batch: 350, loss is 4.002125902175903 and perplexity is 54.714343823654936
At time: 247.66391825675964 and batch: 400, loss is 4.002275590896606 and perplexity is 54.722534556800895
At time: 248.82894229888916 and batch: 450, loss is 3.9456212234497072 and perplexity is 51.708450638631795
At time: 249.9964039325714 and batch: 500, loss is 3.9126268100738524 and perplexity is 50.03019934862131
At time: 251.16483402252197 and batch: 550, loss is 3.9033795166015626 and perplexity is 49.56968793647159
At time: 252.33376049995422 and batch: 600, loss is 3.860600595474243 and perplexity is 47.49386740651342
At time: 253.561053276062 and batch: 650, loss is 3.8101913213729857 and perplexity is 45.15907793666658
At time: 254.73074460029602 and batch: 700, loss is 3.807935047149658 and perplexity is 45.05730153408239
At time: 255.8982219696045 and batch: 750, loss is 3.814314045906067 and perplexity is 45.34564068432748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.598777948423874 and perplexity of 99.3628149323914
Finished 13 epochs...
Completing Train Step...
At time: 259.31013536453247 and batch: 50, loss is 4.077663860321045 and perplexity is 59.00745903996681
At time: 260.5395586490631 and batch: 100, loss is 4.066842970848083 and perplexity is 58.372388060831625
At time: 261.70836997032166 and batch: 150, loss is 4.033992662429809 and perplexity is 56.48599110867815
At time: 262.8795282840729 and batch: 200, loss is 4.019770960807801 and perplexity is 55.68834955141171
At time: 264.0469949245453 and batch: 250, loss is 3.9890472507476806 and perplexity is 54.003413133764965
At time: 265.2141077518463 and batch: 300, loss is 4.036466455459594 and perplexity is 56.62589873967566
At time: 266.38387966156006 and batch: 350, loss is 3.988126015663147 and perplexity is 53.953686203514806
At time: 267.5513882637024 and batch: 400, loss is 3.9907764530181886 and perplexity is 54.096876743825305
At time: 268.71462297439575 and batch: 450, loss is 3.9372168827056884 and perplexity is 51.27569625499206
At time: 269.8888578414917 and batch: 500, loss is 3.9063014125823976 and perplexity is 49.7147372146797
At time: 271.0548655986786 and batch: 550, loss is 3.9000023412704468 and perplexity is 49.402564770159664
At time: 272.22603249549866 and batch: 600, loss is 3.8592926359176634 and perplexity is 47.43178795631546
At time: 273.3942928314209 and batch: 650, loss is 3.8113448286056517 and perplexity is 45.211199315101254
At time: 274.5600016117096 and batch: 700, loss is 3.810960540771484 and perplexity is 45.19382853913904
At time: 275.73090505599976 and batch: 750, loss is 3.818552589416504 and perplexity is 45.53824805438262
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.598233954851017 and perplexity of 99.30877689919126
Finished 14 epochs...
Completing Train Step...
At time: 279.1389956474304 and batch: 50, loss is 4.070490932464599 and perplexity is 58.5857171635559
At time: 280.36091017723083 and batch: 100, loss is 4.058558163642883 and perplexity is 57.890781841683115
At time: 281.52597641944885 and batch: 150, loss is 4.025168008804322 and perplexity is 55.98971475650926
At time: 282.7227189540863 and batch: 200, loss is 4.011339282989502 and perplexity is 55.220777309110254
At time: 283.8845045566559 and batch: 250, loss is 3.980855622291565 and perplexity is 53.56284419017252
At time: 285.0521378517151 and batch: 300, loss is 4.0289595460891725 and perplexity is 56.20240480390086
At time: 286.21506094932556 and batch: 350, loss is 3.981231656074524 and perplexity is 53.58298941650484
At time: 287.38042187690735 and batch: 400, loss is 3.984783935546875 and perplexity is 53.77366964409424
At time: 288.54485607147217 and batch: 450, loss is 3.932362685203552 and perplexity is 51.02739703260418
At time: 289.7145917415619 and batch: 500, loss is 3.9024828004837038 and perplexity is 49.52525792187401
At time: 290.8815207481384 and batch: 550, loss is 3.8974152183532715 and perplexity is 49.27491945093721
At time: 292.0462658405304 and batch: 600, loss is 3.857648768424988 and perplexity is 47.35388043434013
At time: 293.2131645679474 and batch: 650, loss is 3.810775303840637 and perplexity is 45.18545774836095
At time: 294.37780809402466 and batch: 700, loss is 3.8111504745483398 and perplexity is 45.20241318891571
At time: 295.5450131893158 and batch: 750, loss is 3.8194439744949342 and perplexity is 45.5788582661756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.59833579839662 and perplexity of 99.31889137217833
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 298.9179136753082 and batch: 50, loss is 4.068365907669067 and perplexity is 58.46135324692205
At time: 300.10798263549805 and batch: 100, loss is 4.061048040390014 and perplexity is 58.03510234884203
At time: 301.2716042995453 and batch: 150, loss is 4.029474940299988 and perplexity is 56.231378663809146
At time: 302.4306974411011 and batch: 200, loss is 4.01551387310028 and perplexity is 55.45178326207558
At time: 303.5862514972687 and batch: 250, loss is 3.984637665748596 and perplexity is 53.76580475549455
At time: 304.749813079834 and batch: 300, loss is 4.0305597639083866 and perplexity is 56.29241289061591
At time: 305.9063358306885 and batch: 350, loss is 3.9830617713928222 and perplexity is 53.68114225434679
At time: 307.0880687236786 and batch: 400, loss is 3.9860014152526855 and perplexity is 53.83917786496597
At time: 308.27859330177307 and batch: 450, loss is 3.9293583393096925 and perplexity is 50.87432314045287
At time: 309.43793630599976 and batch: 500, loss is 3.895753288269043 and perplexity is 49.19309599117156
At time: 310.59613823890686 and batch: 550, loss is 3.8867376804351808 and perplexity is 48.751583571546504
At time: 311.74661350250244 and batch: 600, loss is 3.848220009803772 and perplexity is 46.9094904411759
At time: 312.99063062667847 and batch: 650, loss is 3.79947811126709 and perplexity is 44.67786153528991
At time: 314.1596381664276 and batch: 700, loss is 3.7951686859130858 and perplexity is 44.485739890297545
At time: 315.32907462120056 and batch: 750, loss is 3.803870344161987 and perplexity is 44.87452869621527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5917479049327765 and perplexity of 98.66673960867378
Finished 16 epochs...
Completing Train Step...
At time: 318.72283720970154 and batch: 50, loss is 4.065172576904297 and perplexity is 58.27496456778958
At time: 319.9497537612915 and batch: 100, loss is 4.054581265449524 and perplexity is 57.661013281879505
At time: 321.1219871044159 and batch: 150, loss is 4.022281193733216 and perplexity is 55.8283158805645
At time: 322.29591703414917 and batch: 200, loss is 4.008175349235534 and perplexity is 55.04633852975058
At time: 323.4642143249512 and batch: 250, loss is 3.977102270126343 and perplexity is 53.36218078873499
At time: 324.6335678100586 and batch: 300, loss is 4.023600373268128 and perplexity is 55.90201205089521
At time: 325.80246353149414 and batch: 350, loss is 3.976850304603577 and perplexity is 53.348737052706724
At time: 326.97253942489624 and batch: 400, loss is 3.980727028846741 and perplexity is 53.55595680236949
At time: 328.1415524482727 and batch: 450, loss is 3.925693378448486 and perplexity is 50.68821199066207
At time: 329.3140478134155 and batch: 500, loss is 3.8936394834518433 and perplexity is 49.0892212120742
At time: 330.48734855651855 and batch: 550, loss is 3.886775002479553 and perplexity is 48.7534031142661
At time: 331.6576392650604 and batch: 600, loss is 3.8493752479553223 and perplexity is 46.9637133883816
At time: 332.8263912200928 and batch: 650, loss is 3.801600151062012 and perplexity is 44.77277039999874
At time: 333.9958300590515 and batch: 700, loss is 3.7978647470474245 and perplexity is 44.60583798780772
At time: 335.1660387516022 and batch: 750, loss is 3.8068005800247193 and perplexity is 45.006214490518396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.591023999591206 and perplexity of 98.59534007520557
Finished 17 epochs...
Completing Train Step...
At time: 338.56924700737 and batch: 50, loss is 4.06287175655365 and perplexity is 58.14103847240286
At time: 339.7912497520447 and batch: 100, loss is 4.051152868270874 and perplexity is 57.46366691079591
At time: 340.95974373817444 and batch: 150, loss is 4.018523559570313 and perplexity is 55.61892714306849
At time: 342.16067385673523 and batch: 200, loss is 4.004356689453125 and perplexity is 54.83653612758352
At time: 343.3289620876312 and batch: 250, loss is 3.97335458278656 and perplexity is 53.162570291865904
At time: 344.498247385025 and batch: 300, loss is 4.020157532691956 and perplexity is 55.709881263132324
At time: 345.66468048095703 and batch: 350, loss is 3.973768606185913 and perplexity is 53.18458539700635
At time: 346.8324456214905 and batch: 400, loss is 3.9781798839569094 and perplexity is 53.41971560737257
At time: 348.0004131793976 and batch: 450, loss is 3.92387149810791 and perplexity is 50.59594820604553
At time: 349.17119336128235 and batch: 500, loss is 3.892419261932373 and perplexity is 49.02935801858337
At time: 350.34342408180237 and batch: 550, loss is 3.886431965827942 and perplexity is 48.73668177828666
At time: 351.51216650009155 and batch: 600, loss is 3.849518370628357 and perplexity is 46.97043544160502
At time: 352.6797606945038 and batch: 650, loss is 3.802253885269165 and perplexity is 44.802049460877306
At time: 353.84705114364624 and batch: 700, loss is 3.7989094972610475 and perplexity is 44.65246429875741
At time: 355.01463556289673 and batch: 750, loss is 3.8079442310333254 and perplexity is 45.0577153369982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.59088134765625 and perplexity of 98.5812762623051
Finished 18 epochs...
Completing Train Step...
At time: 358.42077112197876 and batch: 50, loss is 4.060561099052429 and perplexity is 58.006849537766456
At time: 359.62992548942566 and batch: 100, loss is 4.048364453315735 and perplexity is 57.30365755252464
At time: 360.79721999168396 and batch: 150, loss is 4.015635476112366 and perplexity is 55.45852677595325
At time: 361.96918296813965 and batch: 200, loss is 4.001482357978821 and perplexity is 54.679144052709056
At time: 363.13715720176697 and batch: 250, loss is 3.9705677604675294 and perplexity is 53.01462190312732
At time: 364.3093535900116 and batch: 300, loss is 4.017614049911499 and perplexity is 55.56836418885212
At time: 365.4798617362976 and batch: 350, loss is 3.9715390729904176 and perplexity is 53.066140685646886
At time: 366.6480860710144 and batch: 400, loss is 3.9762971925735475 and perplexity is 53.31923738351904
At time: 367.81762981414795 and batch: 450, loss is 3.9224307775497436 and perplexity is 50.52310606848741
At time: 368.9886932373047 and batch: 500, loss is 3.891332120895386 and perplexity is 48.97608515426955
At time: 370.15635418891907 and batch: 550, loss is 3.885858249664307 and perplexity is 48.7087287755
At time: 371.3287525177002 and batch: 600, loss is 3.8492843675613404 and perplexity is 46.95944550154257
At time: 372.555379152298 and batch: 650, loss is 3.802367577552795 and perplexity is 44.807143397757
At time: 373.7234616279602 and batch: 700, loss is 3.7993114614486694 and perplexity is 44.670416598143575
At time: 374.8902542591095 and batch: 750, loss is 3.808450713157654 and perplexity is 45.08054204455117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.590885251067405 and perplexity of 98.58166106630958
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 378.29488134384155 and batch: 50, loss is 4.059895119667053 and perplexity is 57.96823103275518
At time: 379.498327255249 and batch: 100, loss is 4.049248871803283 and perplexity is 57.35436038462433
At time: 380.6698007583618 and batch: 150, loss is 4.017748031616211 and perplexity is 55.575809831793045
At time: 381.8397181034088 and batch: 200, loss is 4.003434729576111 and perplexity is 54.7860023401212
At time: 383.00648307800293 and batch: 250, loss is 3.9720627164840696 and perplexity is 53.093935701654665
At time: 384.1783640384674 and batch: 300, loss is 4.017950496673584 and perplexity is 55.58706313048062
At time: 385.34800839424133 and batch: 350, loss is 3.971524591445923 and perplexity is 53.06537221153374
At time: 386.5166702270508 and batch: 400, loss is 3.9768103122711183 and perplexity is 53.34660355494033
At time: 387.69076657295227 and batch: 450, loss is 3.922027688026428 and perplexity is 50.50274483772095
At time: 388.8747229576111 and batch: 500, loss is 3.888130235671997 and perplexity is 48.81952013628303
At time: 390.05557560920715 and batch: 550, loss is 3.879763402938843 and perplexity is 48.41275939961302
At time: 391.22894263267517 and batch: 600, loss is 3.8425337648391724 and perplexity is 46.64350852320788
At time: 392.4133689403534 and batch: 650, loss is 3.7958263158798218 and perplexity is 44.51500466758439
At time: 393.58411478996277 and batch: 700, loss is 3.792277398109436 and perplexity is 44.357304574319436
At time: 394.75305247306824 and batch: 750, loss is 3.8014798545837403 and perplexity is 44.76738471734298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.589962271756904 and perplexity of 98.49071421024017
Finished 20 epochs...
Completing Train Step...
At time: 398.1900336742401 and batch: 50, loss is 4.058503799438476 and perplexity is 57.887634740931404
At time: 399.41694927215576 and batch: 100, loss is 4.047219653129577 and perplexity is 57.23809385052061
At time: 400.5855402946472 and batch: 150, loss is 4.015418205261231 and perplexity is 55.446478563548006
At time: 401.8072500228882 and batch: 200, loss is 4.001127557754517 and perplexity is 54.65974732132018
At time: 402.9708831310272 and batch: 250, loss is 3.9697979211807253 and perplexity is 52.973824870005544
At time: 404.1369540691376 and batch: 300, loss is 4.015928387641907 and perplexity is 55.47477359718174
At time: 405.30360102653503 and batch: 350, loss is 3.969864640235901 and perplexity is 52.97735935145723
At time: 406.47191429138184 and batch: 400, loss is 3.975252618789673 and perplexity is 53.26357058507284
At time: 407.640296459198 and batch: 450, loss is 3.920890712738037 and perplexity is 50.44535709524921
At time: 408.80681228637695 and batch: 500, loss is 3.8876624298095703 and perplexity is 48.79668741961818
At time: 409.9717130661011 and batch: 550, loss is 3.88007905960083 and perplexity is 48.428043621799056
At time: 411.14080286026 and batch: 600, loss is 3.843397536277771 and perplexity is 46.68381525906379
At time: 412.31184482574463 and batch: 650, loss is 3.797018222808838 and perplexity is 44.56809404260476
At time: 413.47797894477844 and batch: 700, loss is 3.7934130239486694 and perplexity is 44.40770648899439
At time: 414.6419973373413 and batch: 750, loss is 3.8026078128814698 and perplexity is 44.817908949659305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.589596060819404 and perplexity of 98.45465243696512
Finished 21 epochs...
Completing Train Step...
At time: 418.0235183238983 and batch: 50, loss is 4.057603945732117 and perplexity is 57.83556776810001
At time: 419.2201588153839 and batch: 100, loss is 4.045878434181214 and perplexity is 57.161376493366994
At time: 420.38358879089355 and batch: 150, loss is 4.013879904747009 and perplexity is 55.36125078683499
At time: 421.5455222129822 and batch: 200, loss is 3.9996160459518433 and perplexity is 54.57719087635118
At time: 422.71224761009216 and batch: 250, loss is 3.9683137702941895 and perplexity is 52.89526203483315
At time: 423.87839245796204 and batch: 300, loss is 4.01459969997406 and perplexity is 55.40111389583159
At time: 425.0517506599426 and batch: 350, loss is 3.968795475959778 and perplexity is 52.92074812014114
At time: 426.2158455848694 and batch: 400, loss is 3.974296183586121 and perplexity is 53.2126517852463
At time: 427.392080783844 and batch: 450, loss is 3.9202386140823364 and perplexity is 50.412472468876686
At time: 428.5657448768616 and batch: 500, loss is 3.8873845863342287 and perplexity is 48.78313146170479
At time: 429.73379945755005 and batch: 550, loss is 3.880230770111084 and perplexity is 48.43539122234755
At time: 430.9026131629944 and batch: 600, loss is 3.843823642730713 and perplexity is 46.70371177270901
At time: 432.1220819950104 and batch: 650, loss is 3.7976609373092653 and perplexity is 44.59674781001189
At time: 433.2894916534424 and batch: 700, loss is 3.7940901565551757 and perplexity is 44.43778657799345
At time: 434.453298330307 and batch: 750, loss is 3.8032694816589356 and perplexity is 44.847573373611894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.589436020962028 and perplexity of 98.43889702921132
Finished 22 epochs...
Completing Train Step...
At time: 437.8280794620514 and batch: 50, loss is 4.05680166721344 and perplexity is 57.78918614244296
At time: 439.0461308956146 and batch: 100, loss is 4.0447860622406 and perplexity is 57.09896910184627
At time: 440.2064232826233 and batch: 150, loss is 4.012666349411011 and perplexity is 55.294107594764824
At time: 441.3671500682831 and batch: 200, loss is 3.9984340143203734 and perplexity is 54.51271702295252
At time: 442.5260751247406 and batch: 250, loss is 3.9671610450744628 and perplexity is 52.83432346174438
At time: 443.68391585350037 and batch: 300, loss is 4.013568072319031 and perplexity is 55.343990044955625
At time: 444.8452413082123 and batch: 350, loss is 3.967957549095154 and perplexity is 52.87642297680294
At time: 446.00393414497375 and batch: 400, loss is 3.9735658311843873 and perplexity is 53.17380198596137
At time: 447.1630914211273 and batch: 450, loss is 3.919743776321411 and perplexity is 50.387532644969696
At time: 448.32965564727783 and batch: 500, loss is 3.8871260023117067 and perplexity is 48.77051855415867
At time: 449.4910843372345 and batch: 550, loss is 3.880254588127136 and perplexity is 48.43654487101194
At time: 450.64955711364746 and batch: 600, loss is 3.8440180921554568 and perplexity is 46.71279416560103
At time: 451.8096113204956 and batch: 650, loss is 3.7980192565917967 and perplexity is 44.61273054798101
At time: 452.97000098228455 and batch: 700, loss is 3.7945172929763795 and perplexity is 44.45677162943341
At time: 454.12932991981506 and batch: 750, loss is 3.8036876630783083 and perplexity is 44.866331717422746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.58936469499455 and perplexity of 98.431876030036
Finished 23 epochs...
Completing Train Step...
At time: 457.50956296920776 and batch: 50, loss is 4.056035079956055 and perplexity is 57.74490266447507
At time: 458.7040922641754 and batch: 100, loss is 4.043820567131043 and perplexity is 57.043866931136264
At time: 459.8725757598877 and batch: 150, loss is 4.011627836227417 and perplexity is 55.23671374234713
At time: 461.07226300239563 and batch: 200, loss is 3.9974257230758665 and perplexity is 54.45778002856353
At time: 462.2410147190094 and batch: 250, loss is 3.9661837577819825 and perplexity is 52.78271437138013
At time: 463.4059772491455 and batch: 300, loss is 4.01269238948822 and perplexity is 55.29554747634304
At time: 464.57226967811584 and batch: 350, loss is 3.9672370958328247 and perplexity is 52.83834170490447
At time: 465.73808193206787 and batch: 400, loss is 3.972942943572998 and perplexity is 53.1406909967369
At time: 466.93508744239807 and batch: 450, loss is 3.9193110942840574 and perplexity is 50.36573558062681
At time: 468.1354115009308 and batch: 500, loss is 3.8868590259552 and perplexity is 48.757499716748356
At time: 469.30677556991577 and batch: 550, loss is 3.8801959705352784 and perplexity is 48.43370572060658
At time: 470.4847707748413 and batch: 600, loss is 3.8440840291976928 and perplexity is 46.71587437063158
At time: 471.654908657074 and batch: 650, loss is 3.7982184219360353 and perplexity is 44.62161674269966
At time: 472.8243131637573 and batch: 700, loss is 3.794797248840332 and perplexity is 44.46921930566214
At time: 473.995450258255 and batch: 750, loss is 3.8039654874801636 and perplexity is 44.87879841089122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.589338790538699 and perplexity of 98.4293262388747
Finished 24 epochs...
Completing Train Step...
At time: 477.3756091594696 and batch: 50, loss is 4.0552906322479245 and perplexity is 57.70193060124189
At time: 478.576340675354 and batch: 100, loss is 4.042933559417724 and perplexity is 56.99329101510067
At time: 479.7446343898773 and batch: 150, loss is 4.0106971740722654 and perplexity is 55.185330937017056
At time: 480.9109103679657 and batch: 200, loss is 3.996524939537048 and perplexity is 54.408747443941216
At time: 482.08091163635254 and batch: 250, loss is 3.9653116273880005 and perplexity is 52.73670102963344
At time: 483.24907398223877 and batch: 300, loss is 4.011908731460571 and perplexity is 55.25223165128376
At time: 484.41591572761536 and batch: 350, loss is 3.9665859603881835 and perplexity is 52.80394798648409
At time: 485.58426094055176 and batch: 400, loss is 3.9723792982101442 and perplexity is 53.11074693238701
At time: 486.7555730342865 and batch: 450, loss is 3.9189072942733763 and perplexity is 50.345402001687376
At time: 487.9230456352234 and batch: 500, loss is 3.8865813493728636 and perplexity is 48.743962780395826
At time: 489.0928244590759 and batch: 550, loss is 3.880084137916565 and perplexity is 48.428289555319466
At time: 490.2701189517975 and batch: 600, loss is 3.844072871208191 and perplexity is 46.71535311830386
At time: 491.4971354007721 and batch: 650, loss is 3.7983213329315184 and perplexity is 44.6262090339933
At time: 492.6679015159607 and batch: 700, loss is 3.794983649253845 and perplexity is 44.47750915912138
At time: 493.8361978530884 and batch: 750, loss is 3.8041558361053465 and perplexity is 44.88734184155794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5893370162609015 and perplexity of 98.42915159806145
Finished 25 epochs...
Completing Train Step...
At time: 497.2257387638092 and batch: 50, loss is 4.054564771652221 and perplexity is 57.660062240657304
At time: 498.4236669540405 and batch: 100, loss is 4.042101340293884 and perplexity is 56.94587983936726
At time: 499.5934908390045 and batch: 150, loss is 4.009838786125183 and perplexity is 55.13798083936893
At time: 500.7706274986267 and batch: 200, loss is 3.9956964540481565 and perplexity is 54.363689253820766
At time: 501.9398739337921 and batch: 250, loss is 3.9645089673995972 and perplexity is 52.69438837340635
At time: 503.109334230423 and batch: 300, loss is 4.011184344291687 and perplexity is 55.212222136562424
At time: 504.2779402732849 and batch: 350, loss is 3.96598030090332 and perplexity is 52.77197645745507
At time: 505.45073318481445 and batch: 400, loss is 3.971852707862854 and perplexity is 53.08278668815849
At time: 506.61994338035583 and batch: 450, loss is 3.9185186433792114 and perplexity is 50.32583901801412
At time: 507.78598976135254 and batch: 500, loss is 3.886294779777527 and perplexity is 48.72999624399467
At time: 508.9550383090973 and batch: 550, loss is 3.8799369049072268 and perplexity is 48.42115983738893
At time: 510.1237187385559 and batch: 600, loss is 3.8440119791030884 and perplexity is 46.71250860871684
At time: 511.29824662208557 and batch: 650, loss is 3.7983619356155396 and perplexity is 44.62802101464316
At time: 512.4709219932556 and batch: 700, loss is 3.7951063394546507 and perplexity is 44.48296644842256
At time: 513.644210100174 and batch: 750, loss is 3.8042874050140383 and perplexity is 44.89324800866357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5893505007721656 and perplexity of 98.43047887601371
Annealing...
Finished 26 epochs...
Completing Train Step...
At time: 517.025931596756 and batch: 50, loss is 4.054351019859314 and perplexity is 57.64773861611955
At time: 518.2237629890442 and batch: 100, loss is 4.042368469238281 and perplexity is 56.96109376408584
At time: 519.3888721466064 and batch: 150, loss is 4.010497517585755 and perplexity is 55.17431392758053
At time: 520.590972661972 and batch: 200, loss is 3.996312212944031 and perplexity is 54.39717448744858
At time: 521.7593710422516 and batch: 250, loss is 3.9649297189712525 and perplexity is 52.71656428507956
At time: 522.9344232082367 and batch: 300, loss is 4.011105904579162 and perplexity is 55.20789147558032
At time: 524.1015129089355 and batch: 350, loss is 3.965647883415222 and perplexity is 52.75443704496434
At time: 525.2748310565948 and batch: 400, loss is 3.9717087936401367 and perplexity is 53.0751478698531
At time: 526.4416108131409 and batch: 450, loss is 3.918159689903259 and perplexity is 50.307777624962235
At time: 527.6100845336914 and batch: 500, loss is 3.884923219680786 and perplexity is 48.66320593956065
At time: 528.7769129276276 and batch: 550, loss is 3.877513494491577 and perplexity is 48.303957566263385
At time: 529.9422376155853 and batch: 600, loss is 3.841134705543518 and perplexity is 46.57829711697548
At time: 531.1114449501038 and batch: 650, loss is 3.7954397869110106 and perplexity is 44.4978016536794
At time: 532.2780556678772 and batch: 700, loss is 3.792331032752991 and perplexity is 44.35968372634125
At time: 533.4511671066284 and batch: 750, loss is 3.801607999801636 and perplexity is 44.77312181119492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.58959144769713 and perplexity of 98.45419825466254
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 536.8684146404266 and batch: 50, loss is 4.0541471576690675 and perplexity is 57.63598761969515
At time: 538.0640230178833 and batch: 100, loss is 4.042287149429321 and perplexity is 56.95646188715704
At time: 539.2321906089783 and batch: 150, loss is 4.010480451583862 and perplexity is 55.173372330669295
At time: 540.3991148471832 and batch: 200, loss is 3.996299819946289 and perplexity is 54.396500347565315
At time: 541.5677649974823 and batch: 250, loss is 3.9648756122589113 and perplexity is 52.71371204226361
At time: 542.733003616333 and batch: 300, loss is 4.010957531929016 and perplexity is 55.19970074206853
At time: 543.899111032486 and batch: 350, loss is 3.96541983127594 and perplexity is 52.74240765445587
At time: 545.0691285133362 and batch: 400, loss is 3.9715603494644167 and perplexity is 53.067269758020714
At time: 546.2588911056519 and batch: 450, loss is 3.917993869781494 and perplexity is 50.2994362747517
At time: 547.4430661201477 and batch: 500, loss is 3.884460687637329 and perplexity is 48.640702852076956
At time: 548.6110982894897 and batch: 550, loss is 3.876775903701782 and perplexity is 48.268342148468626
At time: 549.780809879303 and batch: 600, loss is 3.840295147895813 and perplexity is 46.53920836234129
At time: 550.9775605201721 and batch: 650, loss is 3.7945687580108642 and perplexity is 44.45905965759462
At time: 552.1422348022461 and batch: 700, loss is 3.7914837837219237 and perplexity is 44.32211594416646
At time: 553.3133454322815 and batch: 750, loss is 3.800771589279175 and perplexity is 44.73568875787075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.589629417242006 and perplexity of 98.45793658673236
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 556.7024686336517 and batch: 50, loss is 4.05407681465149 and perplexity is 57.63193347299703
At time: 557.9110867977142 and batch: 100, loss is 4.042245078086853 and perplexity is 56.95406570274889
At time: 559.0798330307007 and batch: 150, loss is 4.0104566049575805 and perplexity is 55.17205664756599
At time: 560.2479197978973 and batch: 200, loss is 3.9962750339508055 and perplexity is 54.39515209286236
At time: 561.4202878475189 and batch: 250, loss is 3.964834370613098 and perplexity is 52.71153808685113
At time: 562.5870380401611 and batch: 300, loss is 4.010901832580567 and perplexity is 55.19662624032728
At time: 563.7504072189331 and batch: 350, loss is 3.96532488822937 and perplexity is 52.73740036729703
At time: 564.9209377765656 and batch: 400, loss is 3.9714997100830076 and perplexity is 53.06405188917529
At time: 566.0849275588989 and batch: 450, loss is 3.917936964035034 and perplexity is 50.29657402922385
At time: 567.2501361370087 and batch: 500, loss is 3.884311308860779 and perplexity is 48.63343750605214
At time: 568.4163436889648 and batch: 550, loss is 3.87654390335083 and perplexity is 48.257145175051406
At time: 569.5804033279419 and batch: 600, loss is 3.8400330209732054 and perplexity is 46.527010781599664
At time: 570.7472307682037 and batch: 650, loss is 3.7942968463897704 and perplexity is 44.44697236602281
At time: 571.9126324653625 and batch: 700, loss is 3.7912159633636473 and perplexity is 44.31024716861553
At time: 573.0788471698761 and batch: 750, loss is 3.800507216453552 and perplexity is 44.72386342064542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.589638998342115 and perplexity of 98.45887992659843
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 576.4600129127502 and batch: 50, loss is 4.054053916931152 and perplexity is 57.6306138482101
At time: 577.6808884143829 and batch: 100, loss is 4.042230095863342 and perplexity is 56.953212410598816
At time: 578.8427193164825 and batch: 150, loss is 4.010447773933411 and perplexity is 55.17156942395159
At time: 580.033474445343 and batch: 200, loss is 3.996264352798462 and perplexity is 54.39457109305899
At time: 581.1929950714111 and batch: 250, loss is 3.964819459915161 and perplexity is 52.71075212688853
At time: 582.3542230129242 and batch: 300, loss is 4.010883169174194 and perplexity is 55.19559609287443
At time: 583.5146136283875 and batch: 350, loss is 3.9652931547164916 and perplexity is 52.73572685087672
At time: 584.6724228858948 and batch: 400, loss is 3.9714791536331178 and perplexity is 53.06296109186318
At time: 585.8323624134064 and batch: 450, loss is 3.9179180812835694 and perplexity is 50.29562430048371
At time: 586.9913878440857 and batch: 500, loss is 3.88426372051239 and perplexity is 48.63112317615275
At time: 588.1503620147705 and batch: 550, loss is 3.8764707326889036 and perplexity is 48.25361429697619
At time: 589.308650970459 and batch: 600, loss is 3.8399502277374267 and perplexity is 46.52315881928636
At time: 590.4705564975739 and batch: 650, loss is 3.7942109394073484 and perplexity is 44.443154224753904
At time: 591.6292223930359 and batch: 700, loss is 3.7911311769485474 and perplexity is 44.306490420868755
At time: 592.7928411960602 and batch: 750, loss is 3.8004233551025393 and perplexity is 44.72011297429741
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.58964325660883 and perplexity of 98.4592991916623
Annealing...
Model not improving. Stopping early with 98.42915159806145loss at 29 epochs.
Finished Training.
Improved accuracyfrom -147.79316595242122 to -98.42915159806145
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fc65da978>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.986431260250071, 'wordvec_dim': 200, 'lr': 20.356485438410314, 'dropout': 0.17501787693936444, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.80653715133667 and batch: 50, loss is 6.959033536911011 and perplexity is 1052.6157512151262
At time: 2.9748053550720215 and batch: 100, loss is 5.976562099456787 and perplexity is 394.0832177096596
At time: 4.14018988609314 and batch: 150, loss is 5.875228815078735 and perplexity is 356.1061338023272
At time: 5.3101770877838135 and batch: 200, loss is 5.832815752029419 and perplexity is 341.31839524036104
At time: 6.479211330413818 and batch: 250, loss is 5.839687147140503 and perplexity is 343.6718051374792
At time: 7.647343397140503 and batch: 300, loss is 5.829724006652832 and perplexity is 340.2647553024379
At time: 8.819443941116333 and batch: 350, loss is 5.7469604969024655 and perplexity is 313.2371266688383
At time: 9.994417667388916 and batch: 400, loss is 5.7671936511993405 and perplexity is 319.63945299832363
At time: 11.219771146774292 and batch: 450, loss is 5.7236276054382325 and perplexity is 306.01300634307614
At time: 12.38827133178711 and batch: 500, loss is 5.739964199066162 and perplexity is 311.0532747843404
At time: 13.559139490127563 and batch: 550, loss is 5.73538290977478 and perplexity is 309.6315089944083
At time: 14.73116159439087 and batch: 600, loss is 5.6997131252288815 and perplexity is 298.78167574660245
At time: 15.900723695755005 and batch: 650, loss is 5.700250616073609 and perplexity is 298.9423113280984
At time: 17.069315433502197 and batch: 700, loss is 5.6992443180084225 and perplexity is 298.6416375676911
At time: 18.243518829345703 and batch: 750, loss is 5.679613380432129 and perplexity is 292.8361918319872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.30644048646439 and perplexity of 201.63124039415396
Finished 1 epochs...
Completing Train Step...
At time: 21.684651374816895 and batch: 50, loss is 5.601087436676026 and perplexity is 270.7206389698451
At time: 22.84834122657776 and batch: 100, loss is 5.596612777709961 and perplexity is 269.5119626592584
At time: 24.011885166168213 and batch: 150, loss is 5.600468988418579 and perplexity is 270.55326402412595
At time: 25.175119400024414 and batch: 200, loss is 5.599546461105347 and perplexity is 270.3037863410612
At time: 26.333543062210083 and batch: 250, loss is 5.634057321548462 and perplexity is 279.79503620095005
At time: 27.557090997695923 and batch: 300, loss is 5.643980131149292 and perplexity is 282.5852093570872
At time: 28.725148916244507 and batch: 350, loss is 5.5734968185424805 and perplexity is 263.3533900244744
At time: 29.891860723495483 and batch: 400, loss is 5.596299276351929 and perplexity is 269.42748353581084
At time: 31.05533003807068 and batch: 450, loss is 5.551046648025513 and perplexity is 257.5069340332025
At time: 32.216596364974976 and batch: 500, loss is 5.562590951919556 and perplexity is 260.49689765263133
At time: 33.37922644615173 and batch: 550, loss is 5.570774526596069 and perplexity is 262.63744016616835
At time: 34.5422420501709 and batch: 600, loss is 5.540956325531006 and perplexity is 254.92167102772282
At time: 35.712007999420166 and batch: 650, loss is 5.536612520217895 and perplexity is 253.8167424533663
At time: 36.87975215911865 and batch: 700, loss is 5.467242221832276 and perplexity is 236.80623237916436
At time: 38.040263652801514 and batch: 750, loss is 5.4790780735015865 and perplexity is 239.62568823652413
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.319341260333394 and perplexity of 204.2492905579098
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 41.46024775505066 and batch: 50, loss is 5.470564489364624 and perplexity is 237.59427435475837
At time: 42.65620756149292 and batch: 100, loss is 5.433128461837769 and perplexity is 228.8641192763563
At time: 43.82301878929138 and batch: 150, loss is 5.405657482147217 and perplexity is 222.6625690317569
At time: 44.997857332229614 and batch: 200, loss is 5.392719078063965 and perplexity is 219.8002277336767
At time: 46.16602158546448 and batch: 250, loss is 5.401214714050293 and perplexity is 221.6755251008644
At time: 47.32633996009827 and batch: 300, loss is 5.4096864891052245 and perplexity is 223.56148773051544
At time: 48.48869800567627 and batch: 350, loss is 5.3478145503997805 and perplexity is 210.14852663272472
At time: 49.704227447509766 and batch: 400, loss is 5.351732778549194 and perplexity is 210.97355176817265
At time: 50.864827394485474 and batch: 450, loss is 5.307043075561523 and perplexity is 201.752777796136
At time: 52.02597212791443 and batch: 500, loss is 5.29126482963562 and perplexity is 198.59445477309234
At time: 53.18895888328552 and batch: 550, loss is 5.281458196640014 and perplexity is 196.6564301200605
At time: 54.35352039337158 and batch: 600, loss is 5.25512038230896 and perplexity is 191.5445430408706
At time: 55.512349128723145 and batch: 650, loss is 5.2860843944549565 and perplexity is 197.56830930781555
At time: 56.680198431015015 and batch: 700, loss is 5.273931131362915 and perplexity is 195.18174133533677
At time: 57.845763206481934 and batch: 750, loss is 5.232979974746704 and perplexity is 187.3502715653347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.149024076240007 and perplexity of 172.26329241599666
Finished 3 epochs...
Completing Train Step...
At time: 61.27198672294617 and batch: 50, loss is 5.311409940719605 and perplexity is 202.63573143866483
At time: 62.459534883499146 and batch: 100, loss is 5.289360113143921 and perplexity is 198.2165486561846
At time: 63.61891198158264 and batch: 150, loss is 5.273557109832764 and perplexity is 195.10875281227524
At time: 64.78075766563416 and batch: 200, loss is 5.26992434501648 and perplexity is 194.40125446533096
At time: 65.94116592407227 and batch: 250, loss is 5.281738939285279 and perplexity is 196.71164771706512
At time: 67.09782361984253 and batch: 300, loss is 5.302215900421142 and perplexity is 200.78122860825445
At time: 68.2642252445221 and batch: 350, loss is 5.236267700195312 and perplexity is 187.96724147906423
At time: 69.47510027885437 and batch: 400, loss is 5.247016887664795 and perplexity is 189.99863495072236
At time: 70.6788980960846 and batch: 450, loss is 5.208090972900391 and perplexity is 182.74486009521652
At time: 71.85956287384033 and batch: 500, loss is 5.201480894088745 and perplexity is 181.5408857340533
At time: 73.02680850028992 and batch: 550, loss is 5.20615520477295 and perplexity is 182.39145058937711
At time: 74.19726037979126 and batch: 600, loss is 5.179583463668823 and perplexity is 177.60881505922745
At time: 75.36019611358643 and batch: 650, loss is 5.194924373626709 and perplexity is 180.3545027242938
At time: 76.5234923362732 and batch: 700, loss is 5.19304633140564 and perplexity is 180.0161072134074
At time: 77.69127464294434 and batch: 750, loss is 5.165265531539917 and perplexity is 175.08394270894445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.13582948196766 and perplexity of 170.0052777178804
Finished 4 epochs...
Completing Train Step...
At time: 81.08625268936157 and batch: 50, loss is 5.232207727432251 and perplexity is 187.20564667153673
At time: 82.27842307090759 and batch: 100, loss is 5.215337686538696 and perplexity is 184.07396978736963
At time: 83.44368815422058 and batch: 150, loss is 5.196117372512817 and perplexity is 180.5697938406452
At time: 84.60767698287964 and batch: 200, loss is 5.198606300354004 and perplexity is 181.01977878554547
At time: 85.76951217651367 and batch: 250, loss is 5.210643653869629 and perplexity is 183.2119453279657
At time: 86.9300856590271 and batch: 300, loss is 5.233470544815064 and perplexity is 187.44220254831436
At time: 88.09336566925049 and batch: 350, loss is 5.174707202911377 and perplexity is 176.74485632015305
At time: 89.2608528137207 and batch: 400, loss is 5.186886415481568 and perplexity is 178.9106314424608
At time: 90.431480884552 and batch: 450, loss is 5.151722297668457 and perplexity is 172.7287245600436
At time: 91.59796714782715 and batch: 500, loss is 5.14328854560852 and perplexity is 171.27809902985362
At time: 92.76839804649353 and batch: 550, loss is 5.153857173919678 and perplexity is 173.0978729148329
At time: 93.9318778514862 and batch: 600, loss is 5.127647867202759 and perplexity is 168.62003451790517
At time: 95.09397840499878 and batch: 650, loss is 5.1409178066253665 and perplexity is 170.87252430938105
At time: 96.25638699531555 and batch: 700, loss is 5.132561159133911 and perplexity is 169.4505525910512
At time: 97.4249358177185 and batch: 750, loss is 5.118026905059814 and perplexity is 167.00552656283463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.124173541401708 and perplexity of 168.0352101049606
Finished 5 epochs...
Completing Train Step...
At time: 100.81758761405945 and batch: 50, loss is 5.172213230133057 and perplexity is 176.30460867080038
At time: 102.02349901199341 and batch: 100, loss is 5.162619171142578 and perplexity is 174.6212200322786
At time: 103.1909441947937 and batch: 150, loss is 5.144494352340698 and perplexity is 171.4847518813993
At time: 104.35755372047424 and batch: 200, loss is 5.147333736419678 and perplexity is 171.97235487417439
At time: 105.51793026924133 and batch: 250, loss is 5.161731452941894 and perplexity is 174.46627438124298
At time: 106.68720507621765 and batch: 300, loss is 5.188314609527588 and perplexity is 179.1663330933806
At time: 107.85854148864746 and batch: 350, loss is 5.126595115661621 and perplexity is 168.44261292351428
At time: 109.07042241096497 and batch: 400, loss is 5.139156560897828 and perplexity is 170.57184067259905
At time: 110.23254776000977 and batch: 450, loss is 5.107505941390992 and perplexity is 165.257678132804
At time: 111.3957633972168 and batch: 500, loss is 5.102379989624024 and perplexity is 164.41274264493532
At time: 112.55809783935547 and batch: 550, loss is 5.112233352661133 and perplexity is 166.04076868002332
At time: 113.72227120399475 and batch: 600, loss is 5.086245098114014 and perplexity is 161.78124741341534
At time: 114.88649559020996 and batch: 650, loss is 5.093825969696045 and perplexity is 163.01235079685026
At time: 116.04618000984192 and batch: 700, loss is 5.082637929916382 and perplexity is 161.19872649985973
At time: 117.20649909973145 and batch: 750, loss is 5.07333927154541 and perplexity is 159.7067420893257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.125315466592478 and perplexity of 168.2272033444109
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 120.58701491355896 and batch: 50, loss is 5.1156808948516845 and perplexity is 166.61418911301595
At time: 121.77309489250183 and batch: 100, loss is 5.09261833190918 and perplexity is 162.81560974217862
At time: 122.93395209312439 and batch: 150, loss is 5.063212575912476 and perplexity is 158.0976019094911
At time: 124.09724426269531 and batch: 200, loss is 5.057558326721192 and perplexity is 157.20620114764733
At time: 125.25559544563293 and batch: 250, loss is 5.066528558731079 and perplexity is 158.6227210028582
At time: 126.41502857208252 and batch: 300, loss is 5.085497732162476 and perplexity is 161.66038278817982
At time: 127.57101559638977 and batch: 350, loss is 5.012739953994751 and perplexity is 150.3160314374632
At time: 128.73083519935608 and batch: 400, loss is 5.0116987133026125 and perplexity is 150.1595977255566
At time: 129.89872860908508 and batch: 450, loss is 4.980898895263672 and perplexity is 145.6052066764517
At time: 131.05701613426208 and batch: 500, loss is 4.956799869537353 and perplexity is 142.13820650849627
At time: 132.2237298488617 and batch: 550, loss is 4.959562110900879 and perplexity is 142.53136929712477
At time: 133.38361930847168 and batch: 600, loss is 4.925268678665161 and perplexity is 137.7263408015979
At time: 134.54011607170105 and batch: 650, loss is 4.9264607429504395 and perplexity is 137.89061734825827
At time: 135.69737219810486 and batch: 700, loss is 4.91675311088562 and perplexity is 136.55850227268004
At time: 136.8555130958557 and batch: 750, loss is 4.922245178222656 and perplexity is 137.31055403172576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0302312096884085 and perplexity of 152.9683763778829
Finished 7 epochs...
Completing Train Step...
At time: 140.24539828300476 and batch: 50, loss is 5.047883949279785 and perplexity is 155.69266210039928
At time: 141.46510791778564 and batch: 100, loss is 5.033735313415527 and perplexity is 153.50533366326252
At time: 142.6315314769745 and batch: 150, loss is 5.012121858596802 and perplexity is 150.22315049779158
At time: 143.7962851524353 and batch: 200, loss is 5.011794195175171 and perplexity is 150.1739359296385
At time: 144.96301007270813 and batch: 250, loss is 5.021905555725097 and perplexity is 151.70010154355563
At time: 146.13206243515015 and batch: 300, loss is 5.04601580619812 and perplexity is 155.40207744220055
At time: 147.30430674552917 and batch: 350, loss is 4.97806155204773 and perplexity is 145.19266027571567
At time: 148.4872603416443 and batch: 400, loss is 4.9823449516296385 and perplexity is 145.81591232188717
At time: 149.67738723754883 and batch: 450, loss is 4.95528883934021 and perplexity is 141.92359357044307
At time: 150.85859394073486 and batch: 500, loss is 4.934941549301147 and perplexity is 139.06501384727173
At time: 152.02378821372986 and batch: 550, loss is 4.945793323516845 and perplexity is 140.58233389607148
At time: 153.1857635974884 and batch: 600, loss is 4.91697247505188 and perplexity is 136.5884616005582
At time: 154.353661775589 and batch: 650, loss is 4.924286737442016 and perplexity is 137.59116800678285
At time: 155.51738953590393 and batch: 700, loss is 4.915694589614868 and perplexity is 136.41402867100248
At time: 156.68102312088013 and batch: 750, loss is 4.921880655288696 and perplexity is 137.26051030730315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.02529481399891 and perplexity of 152.21512464800045
Finished 8 epochs...
Completing Train Step...
At time: 160.06815338134766 and batch: 50, loss is 5.023723888397217 and perplexity is 151.9761937322086
At time: 161.28568172454834 and batch: 100, loss is 5.010750999450684 and perplexity is 150.01735680728262
At time: 162.448388338089 and batch: 150, loss is 4.990285243988037 and perplexity is 146.97834220536632
At time: 163.6130530834198 and batch: 200, loss is 4.990883493423462 and perplexity is 147.066298222806
At time: 164.78092551231384 and batch: 250, loss is 5.001981477737427 and perplexity is 148.7075280196936
At time: 165.94437503814697 and batch: 300, loss is 5.02703673362732 and perplexity is 152.4805022274704
At time: 167.10954570770264 and batch: 350, loss is 4.960850744247437 and perplexity is 142.71515836545976
At time: 168.2759108543396 and batch: 400, loss is 4.96687421798706 and perplexity is 143.5773935927612
At time: 169.4710657596588 and batch: 450, loss is 4.94181643486023 and perplexity is 140.02436383487097
At time: 170.6322705745697 and batch: 500, loss is 4.9233690357208255 and perplexity is 137.4649582753838
At time: 171.80244970321655 and batch: 550, loss is 4.936862277984619 and perplexity is 139.33237669178158
At time: 172.96665120124817 and batch: 600, loss is 4.909858016967774 and perplexity is 135.62015728024318
At time: 174.13401675224304 and batch: 650, loss is 4.918597984313965 and perplexity is 136.8106679603041
At time: 175.2967655658722 and batch: 700, loss is 4.911161499023438 and perplexity is 135.7970509854834
At time: 176.4596462249756 and batch: 750, loss is 4.915436964035035 and perplexity is 136.37888945434355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.023805840070858 and perplexity of 151.98864894599438
Finished 9 epochs...
Completing Train Step...
At time: 179.8526964187622 and batch: 50, loss is 5.006638345718383 and perplexity is 149.4016543182973
At time: 181.04629111289978 and batch: 100, loss is 4.994279184341431 and perplexity is 147.56653876647835
At time: 182.21238660812378 and batch: 150, loss is 4.975021953582764 and perplexity is 144.7520029383756
At time: 183.3817970752716 and batch: 200, loss is 4.975797538757324 and perplexity is 144.8643139936073
At time: 184.55153131484985 and batch: 250, loss is 4.987439279556274 and perplexity is 146.56064173348432
At time: 185.72686409950256 and batch: 300, loss is 5.013518781661987 and perplexity is 150.43314732220162
At time: 186.8907651901245 and batch: 350, loss is 4.947735548019409 and perplexity is 140.85564167621058
At time: 188.053644657135 and batch: 400, loss is 4.954729843139648 and perplexity is 141.84428099064777
At time: 189.22748017311096 and batch: 450, loss is 4.930929470062256 and perplexity is 138.50819174647418
At time: 190.39437913894653 and batch: 500, loss is 4.913069381713867 and perplexity is 136.0563831374674
At time: 191.56056213378906 and batch: 550, loss is 4.9283311080932615 and perplexity is 138.1487644919227
At time: 192.74494338035583 and batch: 600, loss is 4.902380571365357 and perplexity is 134.60984690939335
At time: 193.92418479919434 and batch: 650, loss is 4.911245450973511 and perplexity is 135.80845189128527
At time: 195.10062885284424 and batch: 700, loss is 4.905109510421753 and perplexity is 134.97769066130957
At time: 196.26789712905884 and batch: 750, loss is 4.908123035430908 and perplexity is 135.38506281258816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.023192294808322 and perplexity of 151.89542563176013
Finished 10 epochs...
Completing Train Step...
At time: 199.70621824264526 and batch: 50, loss is 4.992485752105713 and perplexity is 147.30212535358197
At time: 200.9044270515442 and batch: 100, loss is 4.98126787185669 and perplexity is 145.6589415023737
At time: 202.07283329963684 and batch: 150, loss is 4.961730899810791 and perplexity is 142.84082520113984
At time: 203.24076104164124 and batch: 200, loss is 4.963006267547607 and perplexity is 143.02311600029455
At time: 204.4131851196289 and batch: 250, loss is 4.975051317214966 and perplexity is 144.75625344535527
At time: 205.58173513412476 and batch: 300, loss is 5.001742973327636 and perplexity is 148.67206484772137
At time: 206.74714422225952 and batch: 350, loss is 4.936384496688842 and perplexity is 139.26582218881654
At time: 207.9119167327881 and batch: 400, loss is 4.944127779006958 and perplexity is 140.34838264385812
At time: 209.08203411102295 and batch: 450, loss is 4.920875873565674 and perplexity is 137.12266272021913
At time: 210.24880599975586 and batch: 500, loss is 4.903396577835083 and perplexity is 134.7466808850736
At time: 211.4170844554901 and batch: 550, loss is 4.919964323043823 and perplexity is 136.99772543743273
At time: 212.58202576637268 and batch: 600, loss is 4.894481945037842 and perplexity is 133.55080203451226
At time: 213.7471809387207 and batch: 650, loss is 4.903523969650268 and perplexity is 134.76384760276824
At time: 214.9144310951233 and batch: 700, loss is 4.897169055938721 and perplexity is 133.91015043876268
At time: 216.08024072647095 and batch: 750, loss is 4.899724349975586 and perplexity is 134.25276780528392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.022816147915153 and perplexity of 151.83830138354995
Finished 11 epochs...
Completing Train Step...
At time: 219.5235755443573 and batch: 50, loss is 4.9803431034088135 and perplexity is 145.52430297344813
At time: 220.7390832901001 and batch: 100, loss is 4.969040412902832 and perplexity is 143.8887473174596
At time: 221.90899538993835 and batch: 150, loss is 4.94957389831543 and perplexity is 141.11482184576064
At time: 223.07800889015198 and batch: 200, loss is 4.951908102035523 and perplexity is 141.44459731971028
At time: 224.24580335617065 and batch: 250, loss is 4.964362087249756 and perplexity is 143.2171610741724
At time: 225.4231870174408 and batch: 300, loss is 4.9907636547088625 and perplexity is 147.04867504265678
At time: 226.5868682861328 and batch: 350, loss is 4.925101728439331 and perplexity is 137.70334927717175
At time: 227.81008553504944 and batch: 400, loss is 4.933902797698974 and perplexity is 138.92063484127573
At time: 228.97373723983765 and batch: 450, loss is 4.911383199691772 and perplexity is 135.82716061998445
At time: 230.1375367641449 and batch: 500, loss is 4.894998550415039 and perplexity is 133.6198129211351
At time: 231.30402278900146 and batch: 550, loss is 4.912170619964599 and perplexity is 135.93415579941552
At time: 232.46870636940002 and batch: 600, loss is 4.886408262252807 and perplexity is 132.47689624402696
At time: 233.637615442276 and batch: 650, loss is 4.895751571655273 and perplexity is 133.72046937184712
At time: 234.80612015724182 and batch: 700, loss is 4.8899054431915285 and perplexity is 132.94100297999518
At time: 235.9826271533966 and batch: 750, loss is 4.891758670806885 and perplexity is 133.18760134880685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.021805519281432 and perplexity of 151.68492676392165
Finished 12 epochs...
Completing Train Step...
At time: 239.38145327568054 and batch: 50, loss is 4.968793811798096 and perplexity is 143.8532685681413
At time: 240.61748886108398 and batch: 100, loss is 4.958020143508911 and perplexity is 142.3117599320891
At time: 241.77809691429138 and batch: 150, loss is 4.939003915786743 and perplexity is 139.63109593680238
At time: 242.93710255622864 and batch: 200, loss is 4.9418040370941165 and perplexity is 140.02262785631913
At time: 244.0988805294037 and batch: 250, loss is 4.953968963623047 and perplexity is 141.7363956317797
At time: 245.26196932792664 and batch: 300, loss is 4.980963659286499 and perplexity is 145.61463696074892
At time: 246.42160844802856 and batch: 350, loss is 4.915515279769897 and perplexity is 136.38957048553186
At time: 247.58633184432983 and batch: 400, loss is 4.924349403381347 and perplexity is 137.5997905567369
At time: 248.74881386756897 and batch: 450, loss is 4.902495956420898 and perplexity is 134.62537977016817
At time: 249.913316488266 and batch: 500, loss is 4.886563501358032 and perplexity is 132.49746343523773
At time: 251.07590746879578 and batch: 550, loss is 4.904711208343506 and perplexity is 134.92393947191798
At time: 252.23794388771057 and batch: 600, loss is 4.8790504837036135 and perplexity is 131.5057377444559
At time: 253.40094017982483 and batch: 650, loss is 4.887801914215088 and perplexity is 132.66165164289274
At time: 254.56259894371033 and batch: 700, loss is 4.883019561767578 and perplexity is 132.02873149835585
At time: 255.72563219070435 and batch: 750, loss is 4.8839643192291256 and perplexity is 132.1535255683597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0224084188771805 and perplexity of 151.7764051183093
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 259.16618156433105 and batch: 50, loss is 4.96027081489563 and perplexity is 142.63241765037938
At time: 260.3820102214813 and batch: 100, loss is 4.947882871627808 and perplexity is 140.87639456626343
At time: 261.53967905044556 and batch: 150, loss is 4.9255491065979005 and perplexity is 137.76496853052655
At time: 262.6992607116699 and batch: 200, loss is 4.926796836853027 and perplexity is 137.93696933284647
At time: 263.8557987213135 and batch: 250, loss is 4.93356559753418 and perplexity is 138.87379867733884
At time: 265.01257491111755 and batch: 300, loss is 4.952443895339965 and perplexity is 141.52040269409633
At time: 266.1718428134918 and batch: 350, loss is 4.886020498275757 and perplexity is 132.42553643425768
At time: 267.33159470558167 and batch: 400, loss is 4.891951694488525 and perplexity is 133.2133121912889
At time: 268.49285221099854 and batch: 450, loss is 4.864559202194214 and perplexity is 129.61379254245102
At time: 269.64701628685 and batch: 500, loss is 4.83767520904541 and perplexity is 126.17567842197182
At time: 270.80434465408325 and batch: 550, loss is 4.850731773376465 and perplexity is 127.8339010727576
At time: 271.96341371536255 and batch: 600, loss is 4.823787860870361 and perplexity is 124.43554373915708
At time: 273.12170243263245 and batch: 650, loss is 4.827272596359253 and perplexity is 124.86992510701722
At time: 274.2804961204529 and batch: 700, loss is 4.82040828704834 and perplexity is 124.01571445462777
At time: 275.44033789634705 and batch: 750, loss is 4.833604068756103 and perplexity is 125.66304374590078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.995404265647711 and perplexity of 147.73265655116126
Finished 14 epochs...
Completing Train Step...
At time: 278.8468425273895 and batch: 50, loss is 4.941691837310791 and perplexity is 140.00691822913785
At time: 280.03997015953064 and batch: 100, loss is 4.930355758666992 and perplexity is 138.42875080879642
At time: 281.20492243766785 and batch: 150, loss is 4.910134468078613 and perplexity is 135.6576548062531
At time: 282.36978220939636 and batch: 200, loss is 4.911709127426147 and perplexity is 135.87143767387988
At time: 283.5351822376251 and batch: 250, loss is 4.919639673233032 and perplexity is 136.9532563706183
At time: 284.7029812335968 and batch: 300, loss is 4.940962257385254 and perplexity is 139.9048092450234
At time: 285.8714406490326 and batch: 350, loss is 4.877378902435303 and perplexity is 131.28609883984888
At time: 287.0381529331207 and batch: 400, loss is 4.884701862335205 and perplexity is 132.25103044269065
At time: 288.2582845687866 and batch: 450, loss is 4.859729433059693 and perplexity is 128.98929714606567
At time: 289.4237949848175 and batch: 500, loss is 4.833969478607178 and perplexity is 125.70897065056218
At time: 290.58963418006897 and batch: 550, loss is 4.849781475067139 and perplexity is 127.71247843569468
At time: 291.7512490749359 and batch: 600, loss is 4.824725131988526 and perplexity is 124.55222825434788
At time: 292.9165189266205 and batch: 650, loss is 4.830151300430298 and perplexity is 125.22990656021601
At time: 294.0868134498596 and batch: 700, loss is 4.824077205657959 and perplexity is 124.47155372452949
At time: 295.25589895248413 and batch: 750, loss is 4.8357582283020015 and perplexity is 125.93403376422813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.993651988894441 and perplexity of 147.4740147235982
Finished 15 epochs...
Completing Train Step...
At time: 298.6600561141968 and batch: 50, loss is 4.934162569046021 and perplexity is 138.9567271293892
At time: 299.93092823028564 and batch: 100, loss is 4.922383012771607 and perplexity is 137.32948147440467
At time: 301.10956287384033 and batch: 150, loss is 4.90241530418396 and perplexity is 134.61452236998377
At time: 302.2823405265808 and batch: 200, loss is 4.904491577148438 and perplexity is 134.8943092198314
At time: 303.46185517311096 and batch: 250, loss is 4.912903852462769 and perplexity is 136.0338636901246
At time: 304.6306393146515 and batch: 300, loss is 4.935053586959839 and perplexity is 139.08059523866413
At time: 305.79567766189575 and batch: 350, loss is 4.87316123008728 and perplexity is 130.73354315960543
At time: 306.96368622779846 and batch: 400, loss is 4.881188507080078 and perplexity is 131.7872008660422
At time: 308.1334638595581 and batch: 450, loss is 4.857162790298462 and perplexity is 128.65865220550563
At time: 309.3023462295532 and batch: 500, loss is 4.83222466468811 and perplexity is 125.48982313023858
At time: 310.4668300151825 and batch: 550, loss is 4.849090995788575 and perplexity is 127.62432605291991
At time: 311.6326994895935 and batch: 600, loss is 4.824922971725464 and perplexity is 124.57687207210357
At time: 312.79985427856445 and batch: 650, loss is 4.830719842910766 and perplexity is 125.30112532544885
At time: 313.9663293361664 and batch: 700, loss is 4.824886798858643 and perplexity is 124.57236585100308
At time: 315.13357949256897 and batch: 750, loss is 4.835604581832886 and perplexity is 125.91468593099997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.993188547533612 and perplexity of 147.40568500016082
Finished 16 epochs...
Completing Train Step...
At time: 318.5561537742615 and batch: 50, loss is 4.928885326385498 and perplexity is 138.2253502849174
At time: 319.7518935203552 and batch: 100, loss is 4.916796054840088 and perplexity is 136.5643667607051
At time: 320.9225842952728 and batch: 150, loss is 4.896862926483155 and perplexity is 133.869162871385
At time: 322.08906269073486 and batch: 200, loss is 4.899423751831055 and perplexity is 134.21241773726507
At time: 323.2542107105255 and batch: 250, loss is 4.907938480377197 and perplexity is 135.36007912055342
At time: 324.42559123039246 and batch: 300, loss is 4.93054347038269 and perplexity is 138.45473794608145
At time: 325.59447479248047 and batch: 350, loss is 4.869963932037353 and perplexity is 130.31621657126223
At time: 326.76164174079895 and batch: 400, loss is 4.878727312088013 and perplexity is 131.46324568921116
At time: 327.93187141418457 and batch: 450, loss is 4.8550533676147465 and perplexity is 128.38754276881366
At time: 329.10586619377136 and batch: 500, loss is 4.830572156906128 and perplexity is 125.28262146928681
At time: 330.2761721611023 and batch: 550, loss is 4.84815712928772 and perplexity is 127.50519760381658
At time: 331.45035433769226 and batch: 600, loss is 4.824342193603516 and perplexity is 124.5045415563271
At time: 332.6185579299927 and batch: 650, loss is 4.830381784439087 and perplexity is 125.25877337764236
At time: 333.78870248794556 and batch: 700, loss is 4.824781179428101 and perplexity is 124.55920928346745
At time: 334.9593753814697 and batch: 750, loss is 4.834709920883179 and perplexity is 125.80208535568232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.992592035337936 and perplexity of 147.31778193157902
Finished 17 epochs...
Completing Train Step...
At time: 338.35653591156006 and batch: 50, loss is 4.924210987091064 and perplexity is 137.5807458222655
At time: 339.5527129173279 and batch: 100, loss is 4.912146244049072 and perplexity is 135.93084232030122
At time: 340.71555376052856 and batch: 150, loss is 4.892384748458863 and perplexity is 133.2710132379671
At time: 341.88515543937683 and batch: 200, loss is 4.895389709472656 and perplexity is 133.67208974484632
At time: 343.0588972568512 and batch: 250, loss is 4.904118432998657 and perplexity is 134.8439835874489
At time: 344.2304594516754 and batch: 300, loss is 4.926972942352295 and perplexity is 137.96126293075363
At time: 345.3953666687012 and batch: 350, loss is 4.86726261138916 and perplexity is 129.96466572420093
At time: 346.6162257194519 and batch: 400, loss is 4.876465969085693 and perplexity is 131.16629807525513
At time: 347.78285002708435 and batch: 450, loss is 4.853117990493774 and perplexity is 128.13930475049676
At time: 348.9468357563019 and batch: 500, loss is 4.828905420303345 and perplexity is 125.0739822599949
At time: 350.11605644226074 and batch: 550, loss is 4.846980485916138 and perplexity is 127.3552576884168
At time: 351.2828469276428 and batch: 600, loss is 4.8235174083709715 and perplexity is 124.4018943858209
At time: 352.45289158821106 and batch: 650, loss is 4.829479713439941 and perplexity is 125.14583201901056
At time: 353.62037777900696 and batch: 700, loss is 4.823987073898316 and perplexity is 124.46033538993848
At time: 354.787633895874 and batch: 750, loss is 4.833318023681641 and perplexity is 125.6271035916924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.992408929869186 and perplexity of 147.29080970951927
Finished 18 epochs...
Completing Train Step...
At time: 358.2261760234833 and batch: 50, loss is 4.920283098220825 and perplexity is 137.04140387303852
At time: 359.4233591556549 and batch: 100, loss is 4.908047552108765 and perplexity is 135.37484388396274
At time: 360.5874330997467 and batch: 150, loss is 4.888320398330689 and perplexity is 132.73045243653695
At time: 361.7505340576172 and batch: 200, loss is 4.891814527511596 and perplexity is 133.19504097710123
At time: 362.9172501564026 and batch: 250, loss is 4.9007131862640385 and perplexity is 134.3855874715453
At time: 364.0863268375397 and batch: 300, loss is 4.9238864612579345 and perplexity is 137.53610456011808
At time: 365.2497079372406 and batch: 350, loss is 4.86480637550354 and perplexity is 129.64583357216858
At time: 366.41716408729553 and batch: 400, loss is 4.874449949264527 and perplexity is 130.9021305913803
At time: 367.583429813385 and batch: 450, loss is 4.851233587265015 and perplexity is 127.89806599782224
At time: 368.74835205078125 and batch: 500, loss is 4.827127113342285 and perplexity is 124.85175997497517
At time: 369.91322898864746 and batch: 550, loss is 4.845584478378296 and perplexity is 127.17759282829525
At time: 371.08603167533875 and batch: 600, loss is 4.822310218811035 and perplexity is 124.25180832707039
At time: 372.25347924232483 and batch: 650, loss is 4.828363542556763 and perplexity is 125.00622581184547
At time: 373.41669368743896 and batch: 700, loss is 4.822884006500244 and perplexity is 124.32312294281815
At time: 374.5872874259949 and batch: 750, loss is 4.831808614730835 and perplexity is 125.43762395417222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9922606002452765 and perplexity of 147.26896373935202
Finished 19 epochs...
Completing Train Step...
At time: 378.03977966308594 and batch: 50, loss is 4.916642522811889 and perplexity is 136.54340136596767
At time: 379.2575294971466 and batch: 100, loss is 4.904314212799072 and perplexity is 134.87038590007822
At time: 380.4218850135803 and batch: 150, loss is 4.8847348022460935 and perplexity is 132.25538685159785
At time: 381.5820937156677 and batch: 200, loss is 4.888647384643555 and perplexity is 132.77386057433094
At time: 382.7484669685364 and batch: 250, loss is 4.897333793640136 and perplexity is 133.93221230630294
At time: 383.9101495742798 and batch: 300, loss is 4.920769710540771 and perplexity is 137.1081061362612
At time: 385.07799530029297 and batch: 350, loss is 4.862501745223999 and perplexity is 129.3473918894644
At time: 386.23882508277893 and batch: 400, loss is 4.872412195205689 and perplexity is 130.63565584066964
At time: 387.40270805358887 and batch: 450, loss is 4.849293985366821 and perplexity is 127.65023509058277
At time: 388.5891270637512 and batch: 500, loss is 4.825418949127197 and perplexity is 124.63867471051908
At time: 389.7748899459839 and batch: 550, loss is 4.844170007705689 and perplexity is 126.99783101690318
At time: 390.9811007976532 and batch: 600, loss is 4.821024560928345 and perplexity is 124.09216565517566
At time: 392.16448283195496 and batch: 650, loss is 4.827015018463134 and perplexity is 124.83776551639728
At time: 393.3272337913513 and batch: 700, loss is 4.821760377883911 and perplexity is 124.18350837635786
At time: 394.4878611564636 and batch: 750, loss is 4.830034255981445 and perplexity is 125.21524995257677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.992044138353925 and perplexity of 147.23708907087354
Finished 20 epochs...
Completing Train Step...
At time: 397.9196982383728 and batch: 50, loss is 4.913184871673584 and perplexity is 136.0720971910655
At time: 399.13572573661804 and batch: 100, loss is 4.901052961349487 and perplexity is 134.4312561041039
At time: 400.29689502716064 and batch: 150, loss is 4.881437931060791 and perplexity is 131.820075854024
At time: 401.45491790771484 and batch: 200, loss is 4.885776567459106 and perplexity is 132.3932377044978
At time: 402.61191630363464 and batch: 250, loss is 4.8943737125396725 and perplexity is 133.53634827977268
At time: 403.77259039878845 and batch: 300, loss is 4.918042259216309 and perplexity is 136.73465996022423
At time: 404.9290614128113 and batch: 350, loss is 4.860253562927246 and perplexity is 129.056922009852
At time: 406.09002780914307 and batch: 400, loss is 4.870450248718262 and perplexity is 130.37960693381973
At time: 407.3092575073242 and batch: 450, loss is 4.847562847137451 and perplexity is 127.42944605194114
At time: 408.46858406066895 and batch: 500, loss is 4.82370192527771 and perplexity is 124.42485075641953
At time: 409.6265389919281 and batch: 550, loss is 4.8426931285858155 and perplexity is 126.8104090058991
At time: 410.78504943847656 and batch: 600, loss is 4.819761009216308 and perplexity is 123.93546780557794
At time: 411.9400637149811 and batch: 650, loss is 4.825693283081055 and perplexity is 124.67287202148329
At time: 413.09434175491333 and batch: 700, loss is 4.820522174835205 and perplexity is 124.0298391341825
At time: 414.2510471343994 and batch: 750, loss is 4.828524465560913 and perplexity is 125.0263438079214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9920654296875 and perplexity of 147.24022397822466
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 417.67754197120667 and batch: 50, loss is 4.911189432144165 and perplexity is 135.80084427388195
At time: 418.896710395813 and batch: 100, loss is 4.898501453399658 and perplexity is 134.0886909002164
At time: 420.06036496162415 and batch: 150, loss is 4.878169546127319 and perplexity is 131.3899404111783
At time: 421.22768688201904 and batch: 200, loss is 4.881677894592285 and perplexity is 131.8517116605159
At time: 422.3928520679474 and batch: 250, loss is 4.888499765396118 and perplexity is 132.7542620435495
At time: 423.55874609947205 and batch: 300, loss is 4.909674234390259 and perplexity is 135.5952349483906
At time: 424.72416162490845 and batch: 350, loss is 4.850686883926391 and perplexity is 127.82816280803242
At time: 425.89232897758484 and batch: 400, loss is 4.860184850692749 and perplexity is 129.04805452501924
At time: 427.0578532218933 and batch: 450, loss is 4.836312875747681 and perplexity is 126.00390212874338
At time: 428.2250416278839 and batch: 500, loss is 4.808368663787842 and perplexity is 122.53156420670364
At time: 429.39396357536316 and batch: 550, loss is 4.824400329589844 and perplexity is 124.51177996105666
At time: 430.55610823631287 and batch: 600, loss is 4.801300392150879 and perplexity is 121.66853149473181
At time: 431.7191517353058 and batch: 650, loss is 4.806160354614258 and perplexity is 122.26127518012244
At time: 432.8835098743439 and batch: 700, loss is 4.800018243789673 and perplexity is 121.51263434945676
At time: 434.0476875305176 and batch: 750, loss is 4.811736154556274 and perplexity is 122.9448836521331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.985331335733103 and perplexity of 146.25202552111762
Finished 22 epochs...
Completing Train Step...
At time: 437.4395160675049 and batch: 50, loss is 4.906685438156128 and perplexity is 135.19057344742924
At time: 438.64296650886536 and batch: 100, loss is 4.894467945098877 and perplexity is 133.5489323445229
At time: 439.8105206489563 and batch: 150, loss is 4.874617223739624 and perplexity is 130.9240290080361
At time: 440.9730019569397 and batch: 200, loss is 4.878186597824096 and perplexity is 131.39218085170336
At time: 442.139057636261 and batch: 250, loss is 4.885636720657349 and perplexity is 132.3747242281859
At time: 443.3002562522888 and batch: 300, loss is 4.907289056777954 and perplexity is 135.2722016287242
At time: 444.4674038887024 and batch: 350, loss is 4.849020509719849 and perplexity is 127.61533063293228
At time: 445.6321976184845 and batch: 400, loss is 4.859038190841675 and perplexity is 128.90016510766887
At time: 446.79509234428406 and batch: 450, loss is 4.835536680221558 and perplexity is 125.90613641120173
At time: 447.96258640289307 and batch: 500, loss is 4.808084173202515 and perplexity is 122.4967100883507
At time: 449.13276839256287 and batch: 550, loss is 4.82443413734436 and perplexity is 124.51598949590498
At time: 450.29595923423767 and batch: 600, loss is 4.801520795822143 and perplexity is 121.69535064116116
At time: 451.4600508213043 and batch: 650, loss is 4.807121248245239 and perplexity is 122.37881172177212
At time: 452.6329417228699 and batch: 700, loss is 4.801279916763305 and perplexity is 121.666040309898
At time: 453.7997770309448 and batch: 750, loss is 4.812150039672852 and perplexity is 122.99577924137296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.98481927916061 and perplexity of 146.1771553807188
Finished 23 epochs...
Completing Train Step...
At time: 457.2016406059265 and batch: 50, loss is 4.9043153953552245 and perplexity is 134.87054539197717
At time: 458.4220266342163 and batch: 100, loss is 4.8921707916259765 and perplexity is 133.2425020442503
At time: 459.58713150024414 and batch: 150, loss is 4.87254114151001 and perplexity is 130.65250191179885
At time: 460.7572932243347 and batch: 200, loss is 4.876197633743286 and perplexity is 131.13110624356605
At time: 461.93744921684265 and batch: 250, loss is 4.884022960662842 and perplexity is 132.16127546780004
At time: 463.10704946517944 and batch: 300, loss is 4.905877628326416 and perplexity is 135.08140927119766
At time: 464.2695310115814 and batch: 350, loss is 4.848075332641602 and perplexity is 127.49476853282735
At time: 465.5055181980133 and batch: 400, loss is 4.8584615516662595 and perplexity is 128.8258576491051
At time: 466.670214176178 and batch: 450, loss is 4.835240259170532 and perplexity is 125.86882071276864
At time: 467.836895942688 and batch: 500, loss is 4.808032159805298 and perplexity is 122.49033878400914
At time: 469.00172758102417 and batch: 550, loss is 4.824506607055664 and perplexity is 124.52501346069458
At time: 470.1720986366272 and batch: 600, loss is 4.801734285354614 and perplexity is 121.72133409817279
At time: 471.3360300064087 and batch: 650, loss is 4.807700548171997 and perplexity is 122.4497262968602
At time: 472.49878096580505 and batch: 700, loss is 4.8019443702697755 and perplexity is 121.74690860063464
At time: 473.6640405654907 and batch: 750, loss is 4.812175331115722 and perplexity is 122.99889002143486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984621624613917 and perplexity of 146.14826565652183
Finished 24 epochs...
Completing Train Step...
At time: 477.0598576068878 and batch: 50, loss is 4.902387685775757 and perplexity is 134.6108045824948
At time: 478.26342821121216 and batch: 100, loss is 4.890357475280762 and perplexity is 133.0011101634814
At time: 479.42622661590576 and batch: 150, loss is 4.870970878601074 and perplexity is 130.4475041264189
At time: 480.5931558609009 and batch: 200, loss is 4.874670915603637 and perplexity is 130.93105875191603
At time: 481.7588756084442 and batch: 250, loss is 4.882799606323243 and perplexity is 131.99969425362116
At time: 482.92583417892456 and batch: 300, loss is 4.904768533706665 and perplexity is 134.93167425744468
At time: 484.09250950813293 and batch: 350, loss is 4.8473546504974365 and perplexity is 127.40291843101083
At time: 485.2579391002655 and batch: 400, loss is 4.858020315170288 and perplexity is 128.7690275177715
At time: 486.422456741333 and batch: 450, loss is 4.83500675201416 and perplexity is 125.83943287363705
At time: 487.58961939811707 and batch: 500, loss is 4.807915067672729 and perplexity is 122.47599696869513
At time: 488.7542493343353 and batch: 550, loss is 4.82446593284607 and perplexity is 124.51994860720252
At time: 489.9226531982422 and batch: 600, loss is 4.8017978096008305 and perplexity is 121.72906659976788
At time: 491.08891463279724 and batch: 650, loss is 4.807997617721558 and perplexity is 122.48610778554371
At time: 492.2537326812744 and batch: 700, loss is 4.802240524291992 and perplexity is 121.78296977687681
At time: 493.41980838775635 and batch: 750, loss is 4.812043056488037 and perplexity is 122.98262146503397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9845226199127906 and perplexity of 146.13379700740435
Finished 25 epochs...
Completing Train Step...
At time: 496.8303954601288 and batch: 50, loss is 4.900826864242553 and perplexity is 134.40086502181467
At time: 498.0237259864807 and batch: 100, loss is 4.88893982887268 and perplexity is 132.8126952018392
At time: 499.19244384765625 and batch: 150, loss is 4.869627199172974 and perplexity is 130.272342205766
At time: 500.3550138473511 and batch: 200, loss is 4.8734209346771244 and perplexity is 130.7674996699505
At time: 501.5178904533386 and batch: 250, loss is 4.881791839599609 and perplexity is 131.86673636074522
At time: 502.6854166984558 and batch: 300, loss is 4.903841333389282 and perplexity is 134.80662354873542
At time: 503.848379611969 and batch: 350, loss is 4.846748342514038 and perplexity is 127.32569643697924
At time: 505.01318740844727 and batch: 400, loss is 4.857647953033447 and perplexity is 128.7210877335602
At time: 506.1759309768677 and batch: 450, loss is 4.834747676849365 and perplexity is 125.80683522463056
At time: 507.33999133110046 and batch: 500, loss is 4.80774416923523 and perplexity is 122.45506780061402
At time: 508.5034146308899 and batch: 550, loss is 4.824368991851807 and perplexity is 124.50787810465167
At time: 509.66494059562683 and batch: 600, loss is 4.801730690002441 and perplexity is 121.72089646789642
At time: 510.8271450996399 and batch: 650, loss is 4.80815013885498 and perplexity is 122.50479093028265
At time: 511.9914562702179 and batch: 700, loss is 4.80233681678772 and perplexity is 121.79469712759258
At time: 513.1572742462158 and batch: 750, loss is 4.8117741680145265 and perplexity is 122.94955730116536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984431422033976 and perplexity of 146.12047052277688
Finished 26 epochs...
Completing Train Step...
At time: 516.5173070430756 and batch: 50, loss is 4.899353351593017 and perplexity is 134.20296948369236
At time: 517.7158889770508 and batch: 100, loss is 4.887560558319092 and perplexity is 132.6296368347329
At time: 518.876795053482 and batch: 150, loss is 4.86841124534607 and perplexity is 130.11403332037423
At time: 520.0370104312897 and batch: 200, loss is 4.8723228168487545 and perplexity is 130.6239803621689
At time: 521.1989831924438 and batch: 250, loss is 4.8809281349182125 and perplexity is 131.75289161442734
At time: 522.3600351810455 and batch: 300, loss is 4.902977533340454 and perplexity is 134.69022785927018
At time: 523.5211453437805 and batch: 350, loss is 4.846215915679932 and perplexity is 127.25792286336055
At time: 524.6824688911438 and batch: 400, loss is 4.85729043006897 and perplexity is 128.67507521443923
At time: 525.8946225643158 and batch: 450, loss is 4.834509963989258 and perplexity is 125.77693287623548
At time: 527.0561792850494 and batch: 500, loss is 4.807540521621704 and perplexity is 122.43013265736971
At time: 528.2193915843964 and batch: 550, loss is 4.824194583892822 and perplexity is 124.48616483329268
At time: 529.3827610015869 and batch: 600, loss is 4.801606874465943 and perplexity is 121.70582646276685
At time: 530.5458390712738 and batch: 650, loss is 4.808218402862549 and perplexity is 122.513153883699
At time: 531.7133297920227 and batch: 700, loss is 4.802354040145874 and perplexity is 121.7967948593475
At time: 532.8742322921753 and batch: 750, loss is 4.81144570350647 and perplexity is 122.90917936703282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984375 and perplexity of 146.11222634120347
Finished 27 epochs...
Completing Train Step...
At time: 536.2335839271545 and batch: 50, loss is 4.898006601333618 and perplexity is 134.02235324950735
At time: 537.4241855144501 and batch: 100, loss is 4.886298036575317 and perplexity is 132.46229469313437
At time: 538.5836005210876 and batch: 150, loss is 4.867287635803223 and perplexity is 129.9679180545032
At time: 539.7522122859955 and batch: 200, loss is 4.871310777664185 and perplexity is 130.49185064710002
At time: 540.9337503910065 and batch: 250, loss is 4.880103063583374 and perplexity is 131.64423091285343
At time: 542.1353528499603 and batch: 300, loss is 4.90217173576355 and perplexity is 134.58173851612364
At time: 543.3054211139679 and batch: 350, loss is 4.8457152462005615 and perplexity is 127.19422465258042
At time: 544.4719822406769 and batch: 400, loss is 4.856977272033691 and perplexity is 128.63478588948794
At time: 545.6465525627136 and batch: 450, loss is 4.834262294769287 and perplexity is 125.74578565864245
At time: 546.8094382286072 and batch: 500, loss is 4.8073744964599605 and perplexity is 122.40980786205334
At time: 547.9713854789734 and batch: 550, loss is 4.824010543823242 and perplexity is 124.46325649894533
At time: 549.1324837207794 and batch: 600, loss is 4.801463642120361 and perplexity is 121.68839550014029
At time: 550.2911403179169 and batch: 650, loss is 4.808189573287964 and perplexity is 122.5096219325041
At time: 551.4560739994049 and batch: 700, loss is 4.80228630065918 and perplexity is 121.78854468641708
At time: 552.6138615608215 and batch: 750, loss is 4.811076583862305 and perplexity is 122.86381954659568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984324255654978 and perplexity of 146.10481216009356
Finished 28 epochs...
Completing Train Step...
At time: 556.0250165462494 and batch: 50, loss is 4.896724376678467 and perplexity is 133.85061660983558
At time: 557.2412247657776 and batch: 100, loss is 4.885134420394897 and perplexity is 132.3082490661075
At time: 558.4056768417358 and batch: 150, loss is 4.866253585815429 and perplexity is 129.83359419118898
At time: 559.5745215415955 and batch: 200, loss is 4.870392379760742 and perplexity is 130.37206222018906
At time: 560.7405996322632 and batch: 250, loss is 4.8793309211730955 and perplexity is 131.5426220524006
At time: 561.9086680412292 and batch: 300, loss is 4.901432027816773 and perplexity is 134.48222414497536
At time: 563.074784040451 and batch: 350, loss is 4.845209312438965 and perplexity is 127.129889076191
At time: 564.2402534484863 and batch: 400, loss is 4.856683444976807 and perplexity is 128.5969950611929
At time: 565.4055197238922 and batch: 450, loss is 4.834046020507812 and perplexity is 125.71859302235521
At time: 566.5727667808533 and batch: 500, loss is 4.807172307968139 and perplexity is 122.38506050951686
At time: 567.738757610321 and batch: 550, loss is 4.82377293586731 and perplexity is 124.4336865521464
At time: 568.9043543338776 and batch: 600, loss is 4.801288499832153 and perplexity is 121.66708458237991
At time: 570.071320772171 and batch: 650, loss is 4.808122291564941 and perplexity is 122.50137955133748
At time: 571.2388355731964 and batch: 700, loss is 4.8021774578094485 and perplexity is 121.77528959552279
At time: 572.4033756256104 and batch: 750, loss is 4.810667333602905 and perplexity is 122.81354778414288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.98428735067678 and perplexity of 146.09942026468067
Finished 29 epochs...
Completing Train Step...
At time: 575.8013911247253 and batch: 50, loss is 4.8954988288879395 and perplexity is 133.6866767609678
At time: 577.0404648780823 and batch: 100, loss is 4.884048776626587 and perplexity is 132.1646873825368
At time: 578.2052731513977 and batch: 150, loss is 4.865307302474975 and perplexity is 129.71079293552933
At time: 579.3710114955902 and batch: 200, loss is 4.869509677886963 and perplexity is 130.2570333321553
At time: 580.5382754802704 and batch: 250, loss is 4.878585643768311 and perplexity is 131.44462283125745
At time: 581.7028214931488 and batch: 300, loss is 4.900703849792481 and perplexity is 134.38433279018727
At time: 582.8686628341675 and batch: 350, loss is 4.844715518951416 and perplexity is 127.06712866150961
At time: 584.0645604133606 and batch: 400, loss is 4.856356182098389 and perplexity is 128.55491692411493
At time: 585.2300314903259 and batch: 450, loss is 4.833762216567993 and perplexity is 125.6829186528497
At time: 586.3939170837402 and batch: 500, loss is 4.806928358078003 and perplexity is 122.35520832881735
At time: 587.5570802688599 and batch: 550, loss is 4.823494567871093 and perplexity is 124.39905301681661
At time: 588.7253017425537 and batch: 600, loss is 4.80107626914978 and perplexity is 121.64126583385887
At time: 589.891922712326 and batch: 650, loss is 4.808030662536621 and perplexity is 122.49015538319894
At time: 591.0581011772156 and batch: 700, loss is 4.802029371261597 and perplexity is 121.75725764845029
At time: 592.2251284122467 and batch: 750, loss is 4.810259199142456 and perplexity is 122.76343357044658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984257542809774 and perplexity of 146.0950654174965
Finished 30 epochs...
Completing Train Step...
At time: 595.6286602020264 and batch: 50, loss is 4.894358367919922 and perplexity is 133.5342992310064
At time: 596.8256847858429 and batch: 100, loss is 4.883015470504761 and perplexity is 132.0281913352208
At time: 597.9981682300568 and batch: 150, loss is 4.864384813308716 and perplexity is 129.59119130838363
At time: 599.1664848327637 and batch: 200, loss is 4.868671703338623 and perplexity is 130.14792697404073
At time: 600.3340954780579 and batch: 250, loss is 4.8778915691375735 and perplexity is 131.35342210691923
At time: 601.5008363723755 and batch: 300, loss is 4.899992609024048 and perplexity is 134.28878715607115
At time: 602.6706657409668 and batch: 350, loss is 4.844241123199463 and perplexity is 127.00686285151214
At time: 603.83802318573 and batch: 400, loss is 4.856053762435913 and perplexity is 128.516045267602
At time: 605.006787776947 and batch: 450, loss is 4.833483963012696 and perplexity is 125.6479517989457
At time: 606.1773569583893 and batch: 500, loss is 4.806674537658691 and perplexity is 122.32415601955613
At time: 607.3487753868103 and batch: 550, loss is 4.823216342926026 and perplexity is 124.36444691148866
At time: 608.5176477432251 and batch: 600, loss is 4.8008737468719485 and perplexity is 121.61663326203036
At time: 609.6838731765747 and batch: 650, loss is 4.807919940948486 and perplexity is 122.47659382945632
At time: 610.8529093265533 and batch: 700, loss is 4.801888513565063 and perplexity is 121.74010840943122
At time: 612.0246815681458 and batch: 750, loss is 4.8098594665527346 and perplexity is 122.71437083186348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984233767487282 and perplexity of 146.09159200149264
Finished 31 epochs...
Completing Train Step...
At time: 615.494829416275 and batch: 50, loss is 4.893262042999267 and perplexity is 133.38798247100766
At time: 616.6992626190186 and batch: 100, loss is 4.882015609741211 and perplexity is 131.8962475007461
At time: 617.870364189148 and batch: 150, loss is 4.863534355163575 and perplexity is 129.48102627621134
At time: 619.0561373233795 and batch: 200, loss is 4.867882032394409 and perplexity is 130.0451935058156
At time: 620.2440056800842 and batch: 250, loss is 4.877214078903198 and perplexity is 131.26446158453527
At time: 621.4311594963074 and batch: 300, loss is 4.899316902160645 and perplexity is 134.19807795077926
At time: 622.5991117954254 and batch: 350, loss is 4.843759183883667 and perplexity is 126.94566799821536
At time: 623.7722125053406 and batch: 400, loss is 4.855741558074951 and perplexity is 128.475928260494
At time: 624.9381332397461 and batch: 450, loss is 4.8332066822052 and perplexity is 125.61311686316353
At time: 626.1074995994568 and batch: 500, loss is 4.806425094604492 and perplexity is 122.29364691377677
At time: 627.273533821106 and batch: 550, loss is 4.822915248870849 and perplexity is 124.32700715257556
At time: 628.4386258125305 and batch: 600, loss is 4.80064287185669 and perplexity is 121.58855826100314
At time: 629.6025829315186 and batch: 650, loss is 4.807780199050903 and perplexity is 122.45947991361845
At time: 630.7667319774628 and batch: 700, loss is 4.801697626113891 and perplexity is 121.7168719682746
At time: 631.9302370548248 and batch: 750, loss is 4.809432821273804 and perplexity is 122.66202649191787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984224896098292 and perplexity of 146.09029597190056
Finished 32 epochs...
Completing Train Step...
At time: 635.3370718955994 and batch: 50, loss is 4.892249689102173 and perplexity is 133.25301495609938
At time: 636.5291345119476 and batch: 100, loss is 4.8810628890991214 and perplexity is 131.77064706370285
At time: 637.6967616081238 and batch: 150, loss is 4.862719106674194 and perplexity is 129.37551008193932
At time: 638.8586480617523 and batch: 200, loss is 4.867119197845459 and perplexity is 129.94602836739014
At time: 640.0284881591797 and batch: 250, loss is 4.876568269729614 and perplexity is 131.17971715838772
At time: 641.2009654045105 and batch: 300, loss is 4.898659801483154 and perplexity is 134.10992526860534
At time: 642.3633263111115 and batch: 350, loss is 4.843290205001831 and perplexity is 126.8861471188923
At time: 643.5243623256683 and batch: 400, loss is 4.855428428649902 and perplexity is 128.43570496482258
At time: 644.7487478256226 and batch: 450, loss is 4.8329529953002925 and perplexity is 125.58125450203345
At time: 645.9132897853851 and batch: 500, loss is 4.806149082183838 and perplexity is 122.25989700617258
At time: 647.0776717662811 and batch: 550, loss is 4.8225917148590085 and perplexity is 124.28678964338819
At time: 648.2421193122864 and batch: 600, loss is 4.800381593704223 and perplexity is 121.55679397697708
At time: 649.4039170742035 and batch: 650, loss is 4.807609519958496 and perplexity is 122.43858042433384
At time: 650.5659301280975 and batch: 700, loss is 4.801496295928955 and perplexity is 121.69236915459217
At time: 651.7304215431213 and batch: 750, loss is 4.808994026184082 and perplexity is 122.60821480401349
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984204669331396 and perplexity of 146.08734106742233
Finished 33 epochs...
Completing Train Step...
At time: 655.1353719234467 and batch: 50, loss is 4.891252946853638 and perplexity is 133.1202622174703
At time: 656.3278291225433 and batch: 100, loss is 4.8801484966278075 and perplexity is 131.6502120469154
At time: 657.4880619049072 and batch: 150, loss is 4.86192834854126 and perplexity is 129.2732457836311
At time: 658.6488180160522 and batch: 200, loss is 4.866399936676025 and perplexity is 129.85259683992615
At time: 659.8099777698517 and batch: 250, loss is 4.875960063934326 and perplexity is 131.09995715189552
At time: 660.975335597992 and batch: 300, loss is 4.898030948638916 and perplexity is 134.02561637238264
At time: 662.1479113101959 and batch: 350, loss is 4.842811651229859 and perplexity is 126.8254398015939
At time: 663.3143448829651 and batch: 400, loss is 4.855080337524414 and perplexity is 128.39100541593797
At time: 664.4751856327057 and batch: 450, loss is 4.832675428390503 and perplexity is 125.54640213844897
At time: 665.6365113258362 and batch: 500, loss is 4.805833244323731 and perplexity is 122.22128879921291
At time: 666.7980270385742 and batch: 550, loss is 4.822267551422119 and perplexity is 124.24650693993993
At time: 667.9647564888 and batch: 600, loss is 4.800115289688111 and perplexity is 121.52442722444461
At time: 669.1267228126526 and batch: 650, loss is 4.807449913024902 and perplexity is 122.41903993739878
At time: 670.2838463783264 and batch: 700, loss is 4.801283721923828 and perplexity is 121.66650326959234
At time: 671.4372375011444 and batch: 750, loss is 4.808572807312012 and perplexity is 122.5565807854422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984209992164789 and perplexity of 146.08811866806926
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 674.8163583278656 and batch: 50, loss is 4.890601015090942 and perplexity is 133.03350517318148
At time: 676.0051486492157 and batch: 100, loss is 4.879617834091187 and perplexity is 131.5803687446949
At time: 677.1594169139862 and batch: 150, loss is 4.86110366821289 and perplexity is 129.1666806279822
At time: 678.3203346729279 and batch: 200, loss is 4.8654491996765135 and perplexity is 129.7291998399694
At time: 679.4776177406311 and batch: 250, loss is 4.874699230194092 and perplexity is 130.93476606370763
At time: 680.6326258182526 and batch: 300, loss is 4.896368818283081 and perplexity is 133.8030333591858
At time: 681.7921762466431 and batch: 350, loss is 4.839834880828858 and perplexity is 126.44847093953905
At time: 682.9539058208466 and batch: 400, loss is 4.851988153457642 and perplexity is 127.99460997435617
At time: 684.1124134063721 and batch: 450, loss is 4.829256935119629 and perplexity is 125.11795534603378
At time: 685.2688677310944 and batch: 500, loss is 4.801385250091553 and perplexity is 121.67885647383125
At time: 686.4249184131622 and batch: 550, loss is 4.817015953063965 and perplexity is 123.59572450773958
At time: 687.5868556499481 and batch: 600, loss is 4.794645442962646 and perplexity is 120.86152188223495
At time: 688.7429246902466 and batch: 650, loss is 4.801886186599732 and perplexity is 121.73982512474909
At time: 689.9058067798615 and batch: 700, loss is 4.795616111755371 and perplexity is 120.97889534593068
At time: 691.0625674724579 and batch: 750, loss is 4.803248672485352 and perplexity is 121.90580696648078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983994239984557 and perplexity of 146.0566032378442
Finished 35 epochs...
Completing Train Step...
At time: 694.482232093811 and batch: 50, loss is 4.889860754013061 and perplexity is 132.93506208853518
At time: 695.6809380054474 and batch: 100, loss is 4.878849401473999 and perplexity is 131.47929693598059
At time: 696.8517608642578 and batch: 150, loss is 4.860509128570556 and perplexity is 129.08990874010698
At time: 698.0210132598877 and batch: 200, loss is 4.86483283996582 and perplexity is 129.64926462484124
At time: 699.1936128139496 and batch: 250, loss is 4.874198884963989 and perplexity is 130.86926986477476
At time: 700.354663848877 and batch: 300, loss is 4.895855922698974 and perplexity is 133.73442397046605
At time: 701.5197062492371 and batch: 350, loss is 4.839581327438355 and perplexity is 126.41641356530812
At time: 702.7393362522125 and batch: 400, loss is 4.85162784576416 and perplexity is 127.94850083889514
At time: 703.9082088470459 and batch: 450, loss is 4.8290363216400145 and perplexity is 125.09035568308344
At time: 705.0759763717651 and batch: 500, loss is 4.80130223274231 and perplexity is 121.66875543699432
At time: 706.2459750175476 and batch: 550, loss is 4.8169822502136235 and perplexity is 123.59155904972792
At time: 707.415509223938 and batch: 600, loss is 4.794687786102295 and perplexity is 120.86663964688454
At time: 708.5870621204376 and batch: 650, loss is 4.802093505859375 and perplexity is 121.76506675161033
At time: 709.7548675537109 and batch: 700, loss is 4.795940532684326 and perplexity is 121.01814979868145
At time: 710.9196934700012 and batch: 750, loss is 4.8032838726043705 and perplexity is 121.91009814091956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.98388671875 and perplexity of 146.0408998957854
Finished 36 epochs...
Completing Train Step...
At time: 714.3165609836578 and batch: 50, loss is 4.889431381225586 and perplexity is 132.87799564263923
At time: 715.5177948474884 and batch: 100, loss is 4.878353042602539 and perplexity is 131.41405221427087
At time: 716.684210062027 and batch: 150, loss is 4.860106458663941 and perplexity is 129.037938582721
At time: 717.8530972003937 and batch: 200, loss is 4.864510097503662 and perplexity is 129.60742805354138
At time: 719.0219922065735 and batch: 250, loss is 4.873944301605224 and perplexity is 130.8359569671281
At time: 720.1929111480713 and batch: 300, loss is 4.89560094833374 and perplexity is 133.70032946740113
At time: 721.3686668872833 and batch: 350, loss is 4.839384489059448 and perplexity is 126.39153241225799
At time: 722.5350706577301 and batch: 400, loss is 4.851396608352661 and perplexity is 127.91891777924289
At time: 723.7060894966125 and batch: 450, loss is 4.8288957881927494 and perplexity is 125.07277753936718
At time: 724.8761084079742 and batch: 500, loss is 4.801234884262085 and perplexity is 121.66056150715227
At time: 726.050290107727 and batch: 550, loss is 4.8169636726379395 and perplexity is 123.58926303951299
At time: 727.2147583961487 and batch: 600, loss is 4.794719219207764 and perplexity is 120.87043892042738
At time: 728.3808937072754 and batch: 650, loss is 4.80221981048584 and perplexity is 121.78044721417426
At time: 729.5463440418243 and batch: 700, loss is 4.796147966384888 and perplexity is 121.04325564513869
At time: 730.7133874893188 and batch: 750, loss is 4.8032558631896975 and perplexity is 121.90668355824836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983842361805051 and perplexity of 146.03442211129683
Finished 37 epochs...
Completing Train Step...
At time: 734.1432545185089 and batch: 50, loss is 4.88907320022583 and perplexity is 132.83040979199694
At time: 735.3410601615906 and batch: 100, loss is 4.877947883605957 and perplexity is 131.3608194133413
At time: 736.507000207901 and batch: 150, loss is 4.859767408370971 and perplexity is 128.99419564778688
At time: 737.6770339012146 and batch: 200, loss is 4.864254159927368 and perplexity is 129.5742608870788
At time: 738.8440134525299 and batch: 250, loss is 4.8737537288665775 and perplexity is 130.8110255761926
At time: 740.0091390609741 and batch: 300, loss is 4.8953979873657225 and perplexity is 133.673196272691
At time: 741.1780433654785 and batch: 350, loss is 4.839212446212769 and perplexity is 126.3697895236335
At time: 742.3436324596405 and batch: 400, loss is 4.8511985206604 and perplexity is 127.89358112554599
At time: 743.5069341659546 and batch: 450, loss is 4.828771247863769 and perplexity is 125.05720190442344
At time: 744.6715130805969 and batch: 500, loss is 4.801169719696045 and perplexity is 121.65263380776281
At time: 745.839421749115 and batch: 550, loss is 4.816934547424316 and perplexity is 123.58566352824396
At time: 747.0108785629272 and batch: 600, loss is 4.794732913970948 and perplexity is 120.87209422379878
At time: 748.1790325641632 and batch: 650, loss is 4.802304782867432 and perplexity is 121.79079562846378
At time: 749.3420648574829 and batch: 700, loss is 4.79629810333252 and perplexity is 121.06143007436515
At time: 750.5076262950897 and batch: 750, loss is 4.803201007843017 and perplexity is 121.89999650827104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9838171670603195 and perplexity of 146.03074285765885
Finished 38 epochs...
Completing Train Step...
At time: 753.9027855396271 and batch: 50, loss is 4.888753280639649 and perplexity is 132.78792153903848
At time: 755.0996124744415 and batch: 100, loss is 4.8775869846344 and perplexity is 131.31341998242948
At time: 756.2671711444855 and batch: 150, loss is 4.859465923309326 and perplexity is 128.95531168652136
At time: 757.4330394268036 and batch: 200, loss is 4.86402907371521 and perplexity is 129.54509878961872
At time: 758.6027293205261 and batch: 250, loss is 4.873592205047608 and perplexity is 130.78989818611248
At time: 759.7698442935944 and batch: 300, loss is 4.895216455459595 and perplexity is 133.64893252496228
At time: 760.9349186420441 and batch: 350, loss is 4.83905460357666 and perplexity is 126.349844557055
At time: 762.0971474647522 and batch: 400, loss is 4.851016969680786 and perplexity is 127.87036402821342
At time: 763.3235802650452 and batch: 450, loss is 4.828652849197388 and perplexity is 125.04239617500319
At time: 764.4894526004791 and batch: 500, loss is 4.8011049556732175 and perplexity is 121.64475534893297
At time: 765.6567013263702 and batch: 550, loss is 4.816897525787353 and perplexity is 123.58108826936727
At time: 766.8209631443024 and batch: 600, loss is 4.794730882644654 and perplexity is 120.87184869338498
At time: 767.9945390224457 and batch: 650, loss is 4.802365856170654 and perplexity is 121.79823402179625
At time: 769.1626975536346 and batch: 700, loss is 4.796408309936523 and perplexity is 121.074772578652
At time: 770.3258125782013 and batch: 750, loss is 4.803128929138183 and perplexity is 121.89121043105179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983798004860102 and perplexity of 146.02794461413663
Finished 39 epochs...
Completing Train Step...
At time: 773.752147436142 and batch: 50, loss is 4.888457117080688 and perplexity is 132.74860041863738
At time: 774.9942507743835 and batch: 100, loss is 4.877252492904663 and perplexity is 131.26950407460538
At time: 776.1720325946808 and batch: 150, loss is 4.859187860488891 and perplexity is 128.9194589937252
At time: 777.3502190113068 and batch: 200, loss is 4.86382399559021 and perplexity is 129.51853464761132
At time: 778.5269010066986 and batch: 250, loss is 4.873445472717285 and perplexity is 130.770708487478
At time: 779.6916162967682 and batch: 300, loss is 4.8950455951690675 and perplexity is 133.62609918023387
At time: 780.857980966568 and batch: 350, loss is 4.838905744552612 and perplexity is 126.33103764232986
At time: 782.0215764045715 and batch: 400, loss is 4.8508454608917235 and perplexity is 127.84843501748476
At time: 783.1859185695648 and batch: 450, loss is 4.828538236618042 and perplexity is 125.02806556469972
At time: 784.3502840995789 and batch: 500, loss is 4.801040954589844 and perplexity is 121.63697020193553
At time: 785.5124588012695 and batch: 550, loss is 4.816855869293213 and perplexity is 123.5759404217093
At time: 786.6804659366608 and batch: 600, loss is 4.794716634750366 and perplexity is 120.87012653633104
At time: 787.8376269340515 and batch: 650, loss is 4.802407312393188 and perplexity is 121.80328342115389
At time: 789.0093529224396 and batch: 700, loss is 4.796491374969483 and perplexity is 121.08483007633421
At time: 790.178891658783 and batch: 750, loss is 4.803046226501465 and perplexity is 121.88113012339609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983787359193314 and perplexity of 146.02639005757115
Finished 40 epochs...
Completing Train Step...
At time: 793.5697457790375 and batch: 50, loss is 4.8881771278381345 and perplexity is 132.71143744141582
At time: 794.7611734867096 and batch: 100, loss is 4.876936874389648 and perplexity is 131.22807952618373
At time: 795.930570602417 and batch: 150, loss is 4.858925170898438 and perplexity is 128.88559764154775
At time: 797.0955183506012 and batch: 200, loss is 4.863633604049682 and perplexity is 129.4938777615786
At time: 798.262567281723 and batch: 250, loss is 4.873305673599243 and perplexity is 130.75242813558404
At time: 799.4304604530334 and batch: 300, loss is 4.8948830795288085 and perplexity is 133.60438461369642
At time: 800.5931277275085 and batch: 350, loss is 4.83876148223877 and perplexity is 126.31281414904274
At time: 801.7572028636932 and batch: 400, loss is 4.8506842041015625 and perplexity is 127.82782025140692
At time: 802.9197466373444 and batch: 450, loss is 4.828427057266236 and perplexity is 125.01416579811054
At time: 804.0817015171051 and batch: 500, loss is 4.800978708267212 and perplexity is 121.62939898348702
At time: 805.2436661720276 and batch: 550, loss is 4.816809501647949 and perplexity is 123.57021062918022
At time: 806.4096531867981 and batch: 600, loss is 4.794694700241089 and perplexity is 120.86747533849565
At time: 807.5730726718903 and batch: 650, loss is 4.802431554794311 and perplexity is 121.80623626100049
At time: 808.7425150871277 and batch: 700, loss is 4.796554298400879 and perplexity is 121.09244938904676
At time: 809.912663936615 and batch: 750, loss is 4.802956237792968 and perplexity is 121.87016269138611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983779197515443 and perplexity of 146.02519824207855
Finished 41 epochs...
Completing Train Step...
At time: 813.3453414440155 and batch: 50, loss is 4.887909431457519 and perplexity is 132.67591582466272
At time: 814.5604634284973 and batch: 100, loss is 4.876636810302735 and perplexity is 131.18870859951926
At time: 815.7216110229492 and batch: 150, loss is 4.858672971725464 and perplexity is 128.8530968989168
At time: 816.8807239532471 and batch: 200, loss is 4.8634537506103515 and perplexity is 129.47058993655136
At time: 818.0393948554993 and batch: 250, loss is 4.873171997070313 and perplexity is 130.73495077302425
At time: 819.1980834007263 and batch: 300, loss is 4.8947278499603275 and perplexity is 133.58364687232265
At time: 820.3558719158173 and batch: 350, loss is 4.838620805740357 and perplexity is 126.29504615444252
At time: 821.5696666240692 and batch: 400, loss is 4.850530471801758 and perplexity is 127.80817049706029
At time: 822.7314021587372 and batch: 450, loss is 4.828322038650513 and perplexity is 125.00103767283352
At time: 823.8915817737579 and batch: 500, loss is 4.800916242599487 and perplexity is 121.62180155915614
At time: 825.0526530742645 and batch: 550, loss is 4.816754522323609 and perplexity is 123.56341700924726
At time: 826.2136967182159 and batch: 600, loss is 4.794665861129761 and perplexity is 120.8639896781803
At time: 827.3769619464874 and batch: 650, loss is 4.80244327545166 and perplexity is 121.80766391852517
At time: 828.5383756160736 and batch: 700, loss is 4.7966030502319335 and perplexity is 121.09835301158641
At time: 829.6991715431213 and batch: 750, loss is 4.802860193252563 and perplexity is 121.85845828970226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983775294104288 and perplexity of 146.02462824680325
Finished 42 epochs...
Completing Train Step...
At time: 833.1030161380768 and batch: 50, loss is 4.887651243209839 and perplexity is 132.641664884232
At time: 834.328831911087 and batch: 100, loss is 4.876350984573365 and perplexity is 131.1512168495073
At time: 835.4925475120544 and batch: 150, loss is 4.858428745269776 and perplexity is 128.82163140626596
At time: 836.6514666080475 and batch: 200, loss is 4.8632815647125245 and perplexity is 129.44829884594176
At time: 837.8165526390076 and batch: 250, loss is 4.873043050765991 and perplexity is 130.7180940711045
At time: 838.9847900867462 and batch: 300, loss is 4.894576854705811 and perplexity is 133.56347789831187
At time: 840.1507399082184 and batch: 350, loss is 4.838483238220215 and perplexity is 126.27767325313775
At time: 841.3167765140533 and batch: 400, loss is 4.850379142761231 and perplexity is 127.78883087260776
At time: 842.4837050437927 and batch: 450, loss is 4.828219842910767 and perplexity is 124.98826375205076
At time: 843.6504726409912 and batch: 500, loss is 4.800851001739502 and perplexity is 121.61386710705749
At time: 844.8193826675415 and batch: 550, loss is 4.81669095993042 and perplexity is 123.55556327235537
At time: 845.9855303764343 and batch: 600, loss is 4.794630298614502 and perplexity is 120.85969152713011
At time: 847.1528308391571 and batch: 650, loss is 4.802445096969604 and perplexity is 121.80788579357285
At time: 848.3283250331879 and batch: 700, loss is 4.796640377044678 and perplexity is 121.10287331149664
At time: 849.4975848197937 and batch: 750, loss is 4.8027604389190675 and perplexity is 121.8463029866969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983772455259811 and perplexity of 146.02421370618228
Finished 43 epochs...
Completing Train Step...
At time: 852.8697159290314 and batch: 50, loss is 4.887399263381958 and perplexity is 132.60824607095492
At time: 854.108941078186 and batch: 100, loss is 4.876075944900513 and perplexity is 131.1151500218625
At time: 855.2857029438019 and batch: 150, loss is 4.85818998336792 and perplexity is 128.79087738014223
At time: 856.4536213874817 and batch: 200, loss is 4.863115491867066 and perplexity is 129.42680278361814
At time: 857.6218822002411 and batch: 250, loss is 4.872914543151856 and perplexity is 130.70129688001765
At time: 858.7886207103729 and batch: 300, loss is 4.894426794052124 and perplexity is 133.54343677923907
At time: 859.9593515396118 and batch: 350, loss is 4.838348236083984 and perplexity is 126.26062664818024
At time: 861.1375052928925 and batch: 400, loss is 4.8502275753021244 and perplexity is 127.769463711963
At time: 862.3080055713654 and batch: 450, loss is 4.828121490478516 and perplexity is 124.9759714568064
At time: 863.4839375019073 and batch: 500, loss is 4.800783748626709 and perplexity is 121.60568847095824
At time: 864.648110628128 and batch: 550, loss is 4.816619291305542 and perplexity is 123.54670853234688
At time: 865.8373413085938 and batch: 600, loss is 4.794590492248535 and perplexity is 120.85488063777117
At time: 867.0104117393494 and batch: 650, loss is 4.802437715530395 and perplexity is 121.80698667938705
At time: 868.1762669086456 and batch: 700, loss is 4.796668586730957 and perplexity is 121.10628963374664
At time: 869.3412206172943 and batch: 750, loss is 4.802657928466797 and perplexity is 121.83381310725275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983766067859738 and perplexity of 146.02328099408783
Finished 44 epochs...
Completing Train Step...
At time: 872.7724621295929 and batch: 50, loss is 4.8871518325805665 and perplexity is 132.57543876529476
At time: 873.9737527370453 and batch: 100, loss is 4.875809879302978 and perplexity is 131.08026943159444
At time: 875.1412553787231 and batch: 150, loss is 4.857957906723023 and perplexity is 128.7609914934688
At time: 876.3130643367767 and batch: 200, loss is 4.862948904037475 and perplexity is 129.40524364924616
At time: 877.4780654907227 and batch: 250, loss is 4.872787761688232 and perplexity is 130.68472742867357
At time: 878.6463704109192 and batch: 300, loss is 4.894278192520142 and perplexity is 133.52359349435534
At time: 879.8157789707184 and batch: 350, loss is 4.8382160854339595 and perplexity is 126.24394232674238
At time: 880.985621213913 and batch: 400, loss is 4.8500807571411135 and perplexity is 127.75070621127203
At time: 882.2094974517822 and batch: 450, loss is 4.828023662567139 and perplexity is 124.96374591655575
At time: 883.3757224082947 and batch: 500, loss is 4.800711812973023 and perplexity is 121.59694100089739
At time: 884.5462806224823 and batch: 550, loss is 4.816541843414306 and perplexity is 123.53714047081986
At time: 885.7151193618774 and batch: 600, loss is 4.79454176902771 and perplexity is 120.84899234218359
At time: 886.8823935985565 and batch: 650, loss is 4.802421426773071 and perplexity is 121.80500261109967
At time: 888.0473732948303 and batch: 700, loss is 4.7966891956329345 and perplexity is 121.10878552711733
At time: 889.2137916088104 and batch: 750, loss is 4.8025455665588375 and perplexity is 121.82012439661729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983758970748546 and perplexity of 146.02224465430348
Finished 45 epochs...
Completing Train Step...
At time: 892.6464545726776 and batch: 50, loss is 4.88690595626831 and perplexity is 132.54284561242173
At time: 893.8601365089417 and batch: 100, loss is 4.87555009841919 and perplexity is 131.04622170601985
At time: 895.027740240097 and batch: 150, loss is 4.857728939056397 and perplexity is 128.73151276466075
At time: 896.1958487033844 and batch: 200, loss is 4.862779893875122 and perplexity is 129.38337469609735
At time: 897.3606824874878 and batch: 250, loss is 4.872661075592041 and perplexity is 130.66817253938308
At time: 898.5238299369812 and batch: 300, loss is 4.894128341674804 and perplexity is 133.50358637007736
At time: 899.6896243095398 and batch: 350, loss is 4.838083219528198 and perplexity is 126.22716992526313
At time: 900.8581519126892 and batch: 400, loss is 4.8499353313446045 and perplexity is 127.73212931387945
At time: 902.0233144760132 and batch: 450, loss is 4.827925834655762 and perplexity is 124.95152157224568
At time: 903.1943678855896 and batch: 500, loss is 4.800636625289917 and perplexity is 121.58779875232727
At time: 904.3578777313232 and batch: 550, loss is 4.816450462341309 and perplexity is 123.52585203015138
At time: 905.522075176239 and batch: 600, loss is 4.794470186233521 and perplexity is 120.84034194325037
At time: 906.6899638175964 and batch: 650, loss is 4.80240083694458 and perplexity is 121.80249469280544
At time: 907.8561305999756 and batch: 700, loss is 4.796704835891724 and perplexity is 121.1106797146774
At time: 909.0208237171173 and batch: 750, loss is 4.802424001693725 and perplexity is 121.8053162497205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983747970226199 and perplexity of 146.02063834217313
Finished 46 epochs...
Completing Train Step...
At time: 912.4092571735382 and batch: 50, loss is 4.886660232543945 and perplexity is 132.51028069191048
At time: 913.6017639636993 and batch: 100, loss is 4.875294389724732 and perplexity is 131.0127163317441
At time: 914.7615835666656 and batch: 150, loss is 4.857501068115234 and perplexity is 128.70218193563852
At time: 915.9339025020599 and batch: 200, loss is 4.862617597579956 and perplexity is 129.36237795762065
At time: 917.1043674945831 and batch: 250, loss is 4.872535305023193 and perplexity is 130.65173936241922
At time: 918.2728242874146 and batch: 300, loss is 4.89397632598877 and perplexity is 133.48329327328082
At time: 919.4363193511963 and batch: 350, loss is 4.837951259613037 and perplexity is 126.21051409760372
At time: 920.5986394882202 and batch: 400, loss is 4.849792222976685 and perplexity is 127.71385108523738
At time: 921.7660572528839 and batch: 450, loss is 4.827828197479248 and perplexity is 124.93932225404147
At time: 922.932014465332 and batch: 500, loss is 4.800559129714966 and perplexity is 121.57837660104818
At time: 924.10067486763 and batch: 550, loss is 4.816368808746338 and perplexity is 123.51576611204263
At time: 925.2741703987122 and batch: 600, loss is 4.794406385421753 and perplexity is 120.83263247727778
At time: 926.4388206005096 and batch: 650, loss is 4.802381629943848 and perplexity is 121.80015525466757
At time: 927.6033821105957 and batch: 700, loss is 4.79671353340149 and perplexity is 121.11173308057778
At time: 928.7700498104095 and batch: 750, loss is 4.802312326431275 and perplexity is 121.79171436857138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983743711959484 and perplexity of 146.02001654867303
Finished 47 epochs...
Completing Train Step...
At time: 932.183235168457 and batch: 50, loss is 4.886426668167115 and perplexity is 132.4793346248672
At time: 933.3953936100006 and batch: 100, loss is 4.875047607421875 and perplexity is 130.98038870101186
At time: 934.5613486766815 and batch: 150, loss is 4.857282924652099 and perplexity is 128.67410945798312
At time: 935.7359924316406 and batch: 200, loss is 4.862464828491211 and perplexity is 129.34261689450034
At time: 936.9052290916443 and batch: 250, loss is 4.872420425415039 and perplexity is 130.63673100389028
At time: 938.0768585205078 and batch: 300, loss is 4.893832607269287 and perplexity is 133.46411060378665
At time: 939.2365992069244 and batch: 350, loss is 4.8378233718872075 and perplexity is 126.19437435403952
At time: 940.4698262214661 and batch: 400, loss is 4.849652299880981 and perplexity is 127.69598221799205
At time: 941.6401286125183 and batch: 450, loss is 4.827730665206909 and perplexity is 124.92713723226359
At time: 942.8005785942078 and batch: 500, loss is 4.800479564666748 and perplexity is 121.56870359647334
At time: 943.9646489620209 and batch: 550, loss is 4.8162929058074955 and perplexity is 123.5063912581953
At time: 945.1324410438538 and batch: 600, loss is 4.7943502616882325 and perplexity is 120.82585108911228
At time: 946.300666809082 and batch: 650, loss is 4.802356538772583 and perplexity is 121.7970991844523
At time: 947.4662990570068 and batch: 700, loss is 4.79671464920044 and perplexity is 121.11186821699779
At time: 948.630318403244 and batch: 750, loss is 4.802201290130615 and perplexity is 121.77819181791777
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983740873115007 and perplexity of 146.01960202114392
Finished 48 epochs...
Completing Train Step...
At time: 952.0357048511505 and batch: 50, loss is 4.886198987960816 and perplexity is 132.44917513611904
At time: 953.2470753192902 and batch: 100, loss is 4.874804887771607 and perplexity is 130.9486010447748
At time: 954.4072873592377 and batch: 150, loss is 4.857069568634033 and perplexity is 128.6466589908263
At time: 955.5691645145416 and batch: 200, loss is 4.862314882278443 and perplexity is 129.3232239129359
At time: 956.7257430553436 and batch: 250, loss is 4.872308597564698 and perplexity is 130.62212299589274
At time: 957.8947896957397 and batch: 300, loss is 4.893690223693848 and perplexity is 133.4451088593263
At time: 959.0558776855469 and batch: 350, loss is 4.837695837020874 and perplexity is 126.17828119761533
At time: 960.21302318573 and batch: 400, loss is 4.849513902664184 and perplexity is 127.6783106723315
At time: 961.3700468540192 and batch: 450, loss is 4.827632350921631 and perplexity is 124.914855713789
At time: 962.528636932373 and batch: 500, loss is 4.80040023803711 and perplexity is 121.55906034343597
At time: 963.6861915588379 and batch: 550, loss is 4.8162150478363035 and perplexity is 123.49677567547243
At time: 964.8414137363434 and batch: 600, loss is 4.794291906356811 and perplexity is 120.81880046225055
At time: 966.004796743393 and batch: 650, loss is 4.802326822280884 and perplexity is 121.79347985574263
At time: 967.1604301929474 and batch: 700, loss is 4.7967100429534915 and perplexity is 121.11131034710925
At time: 968.3165321350098 and batch: 750, loss is 4.802090320587158 and perplexity is 121.76467889734396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983741582826126 and perplexity of 146.01970565291586
Annealing...
Finished 49 epochs...
Completing Train Step...
At time: 971.7102053165436 and batch: 50, loss is 4.886098375320435 and perplexity is 132.43584974525496
At time: 972.9382145404816 and batch: 100, loss is 4.8746685218811034 and perplexity is 130.9307453396654
At time: 974.1059460639954 and batch: 150, loss is 4.856815195083618 and perplexity is 128.61393884517594
At time: 975.2712712287903 and batch: 200, loss is 4.862099952697754 and perplexity is 129.29543151345837
At time: 976.4383354187012 and batch: 250, loss is 4.872266654968262 and perplexity is 130.6166444797947
At time: 977.604615688324 and batch: 300, loss is 4.893387908935547 and perplexity is 133.404772530936
At time: 978.7696025371552 and batch: 350, loss is 4.83685188293457 and perplexity is 126.07183744473387
At time: 979.9394090175629 and batch: 400, loss is 4.84866847038269 and perplexity is 127.57041292338921
At time: 981.1119236946106 and batch: 450, loss is 4.826763944625855 and perplexity is 124.80642595400836
At time: 982.2817192077637 and batch: 500, loss is 4.798952102661133 and perplexity is 121.38315376689138
At time: 983.4467887878418 and batch: 550, loss is 4.81477240562439 and perplexity is 123.31874246383799
At time: 984.6148238182068 and batch: 600, loss is 4.792839632034302 and perplexity is 120.64346576851618
At time: 985.7804503440857 and batch: 650, loss is 4.80076397895813 and perplexity is 121.6032843905638
At time: 986.9448313713074 and batch: 700, loss is 4.795051202774048 and perplexity is 120.91057258128787
At time: 988.1089816093445 and batch: 750, loss is 4.800744524002075 and perplexity is 121.60091862702293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984625528025073 and perplexity of 146.14883613440583
Annealing...
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fc65da978>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 6.4920193942967295, 'wordvec_dim': 200, 'lr': 3.5614289339720253, 'dropout': 0.6602744902126817, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8015656471252441 and batch: 50, loss is 7.528976621627808 and perplexity is 1861.1998169646029
At time: 2.964582920074463 and batch: 100, loss is 6.597754631042481 and perplexity is 733.4464809950744
At time: 4.130180358886719 and batch: 150, loss is 6.409867153167725 and perplexity is 607.8129296754865
At time: 5.29569149017334 and batch: 200, loss is 6.321159057617187 and perplexity is 556.2173068868525
At time: 6.4609644412994385 and batch: 250, loss is 6.311584529876709 and perplexity is 550.9172023403526
At time: 7.631061315536499 and batch: 300, loss is 6.240781106948853 and perplexity is 513.2592647849341
At time: 8.79948091506958 and batch: 350, loss is 6.125250444412232 and perplexity is 457.25921694573293
At time: 9.964043140411377 and batch: 400, loss is 6.128442668914795 and perplexity is 458.72122330683146
At time: 11.126185894012451 and batch: 450, loss is 6.076969871520996 and perplexity is 435.70694441517537
At time: 12.288186311721802 and batch: 500, loss is 6.057437553405761 and perplexity is 427.2791528645078
At time: 13.454508066177368 and batch: 550, loss is 6.0579641151428225 and perplexity is 427.50420096310916
At time: 14.615559816360474 and batch: 600, loss is 6.021550693511963 and perplexity is 412.21732309191947
At time: 15.776976823806763 and batch: 650, loss is 6.0036044216156 and perplexity is 404.88554475206166
At time: 16.944493770599365 and batch: 700, loss is 5.966057901382446 and perplexity is 389.96535481983665
At time: 18.10958695411682 and batch: 750, loss is 5.938962926864624 and perplexity is 379.54111347285425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.377166748046875 and perplexity of 216.40826687043295
Finished 1 epochs...
Completing Train Step...
At time: 21.51024866104126 and batch: 50, loss is 5.667044153213501 and perplexity is 289.1785025180753
At time: 22.693780660629272 and batch: 100, loss is 5.541171321868896 and perplexity is 254.97648414554126
At time: 23.86010766029358 and batch: 150, loss is 5.477243404388428 and perplexity is 239.18645743239685
At time: 25.02407431602478 and batch: 200, loss is 5.423080825805664 and perplexity is 226.57608979774227
At time: 26.184207916259766 and batch: 250, loss is 5.411392660140991 and perplexity is 223.94324744667009
At time: 27.345248222351074 and batch: 300, loss is 5.3940585231781 and perplexity is 220.09483533611066
At time: 28.505424976348877 and batch: 350, loss is 5.298300065994263 and perplexity is 199.99653991917594
At time: 29.66570520401001 and batch: 400, loss is 5.307568302154541 and perplexity is 201.85877155318303
At time: 30.828638076782227 and batch: 450, loss is 5.239908571243286 and perplexity is 188.65285332131714
At time: 31.999826431274414 and batch: 500, loss is 5.211923236846924 and perplexity is 183.44653026788464
At time: 33.16794729232788 and batch: 550, loss is 5.213280525207519 and perplexity is 183.6956891601904
At time: 34.33664274215698 and batch: 600, loss is 5.168948478698731 and perplexity is 175.72995650542032
At time: 35.501253843307495 and batch: 650, loss is 5.135240592956543 and perplexity is 169.90519295030276
At time: 36.66248297691345 and batch: 700, loss is 5.121921968460083 and perplexity is 167.657292187184
At time: 37.8241868019104 and batch: 750, loss is 5.105586194992066 and perplexity is 164.94072962802989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.88593345464662 and perplexity of 132.41401013662878
Finished 2 epochs...
Completing Train Step...
At time: 41.177875995635986 and batch: 50, loss is 5.095095195770264 and perplexity is 163.21938167960582
At time: 42.37438344955444 and batch: 100, loss is 5.046343669891358 and perplexity is 155.4530364946026
At time: 43.57026171684265 and batch: 150, loss is 5.025539674758911 and perplexity is 152.2524007226432
At time: 44.73286151885986 and batch: 200, loss is 5.003678293228149 and perplexity is 148.9600714560386
At time: 45.89807200431824 and batch: 250, loss is 5.0140014743804935 and perplexity is 150.50577783473562
At time: 47.06278109550476 and batch: 300, loss is 5.039332389831543 and perplexity is 154.36692368937912
At time: 48.22845768928528 and batch: 350, loss is 4.961468811035156 and perplexity is 142.80339312962934
At time: 49.39187979698181 and batch: 400, loss is 4.989282598495484 and perplexity is 146.83104888688155
At time: 50.55808067321777 and batch: 450, loss is 4.930927762985229 and perplexity is 138.5079553025239
At time: 51.72574853897095 and batch: 500, loss is 4.904439401626587 and perplexity is 134.88727122246036
At time: 52.88945913314819 and batch: 550, loss is 4.921306200027466 and perplexity is 137.181682928567
At time: 54.054853677749634 and batch: 600, loss is 4.881189041137695 and perplexity is 131.78727124801944
At time: 55.22598671913147 and batch: 650, loss is 4.853157663345337 and perplexity is 128.14438850295633
At time: 56.39502549171448 and batch: 700, loss is 4.850368337631226 and perplexity is 127.7874501051367
At time: 57.56265473365784 and batch: 750, loss is 4.853199043273926 and perplexity is 128.14969121831385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.772759016170058 and perplexity of 118.24503227971603
Finished 3 epochs...
Completing Train Step...
At time: 60.97663497924805 and batch: 50, loss is 4.876584300994873 and perplexity is 131.1818201520868
At time: 62.16720175743103 and batch: 100, loss is 4.841175556182861 and perplexity is 126.61811097877958
At time: 63.33522343635559 and batch: 150, loss is 4.823969984054566 and perplexity is 124.4582084004286
At time: 64.50381588935852 and batch: 200, loss is 4.808905191421509 and perplexity is 122.597323416136
At time: 65.66874122619629 and batch: 250, loss is 4.812837476730347 and perplexity is 123.0803601666393
At time: 66.83417177200317 and batch: 300, loss is 4.848869285583496 and perplexity is 127.59603357389926
At time: 67.99270725250244 and batch: 350, loss is 4.781045579910279 and perplexity is 119.22894828780608
At time: 69.1563720703125 and batch: 400, loss is 4.812099409103394 and perplexity is 122.98955205267343
At time: 70.32230114936829 and batch: 450, loss is 4.756062965393067 and perplexity is 116.28719675729438
At time: 71.54469919204712 and batch: 500, loss is 4.730258131027222 and perplexity is 113.32481122340725
At time: 72.70952200889587 and batch: 550, loss is 4.749648065567016 and perplexity is 115.54361359472132
At time: 73.87252426147461 and batch: 600, loss is 4.710981378555298 and perplexity is 111.16119760883069
At time: 75.03527307510376 and batch: 650, loss is 4.6875347137451175 and perplexity is 108.58515607279686
At time: 76.21892523765564 and batch: 700, loss is 4.689165134429931 and perplexity is 108.76233996020896
At time: 77.411776304245 and batch: 750, loss is 4.699682083129883 and perplexity is 109.91222394683334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.713229866914971 and perplexity of 111.41142347725548
Finished 4 epochs...
Completing Train Step...
At time: 80.83190774917603 and batch: 50, loss is 4.733490076065063 and perplexity is 113.6916632882319
At time: 82.0241961479187 and batch: 100, loss is 4.706540842056274 and perplexity is 110.66867659165894
At time: 83.18612694740295 and batch: 150, loss is 4.688080739974976 and perplexity is 108.64446260617378
At time: 84.34962010383606 and batch: 200, loss is 4.673602561950684 and perplexity is 107.08282087756228
At time: 85.50941848754883 and batch: 250, loss is 4.674389371871948 and perplexity is 107.16710785799361
At time: 86.67257881164551 and batch: 300, loss is 4.716731796264648 and perplexity is 111.80226235666994
At time: 87.83806157112122 and batch: 350, loss is 4.650407562255859 and perplexity is 104.62761915709387
At time: 89.0043466091156 and batch: 400, loss is 4.683251972198486 and perplexity is 108.12110832112128
At time: 90.16758418083191 and batch: 450, loss is 4.633953714370728 and perplexity is 102.9201777385869
At time: 91.33092260360718 and batch: 500, loss is 4.605402355194092 and perplexity is 100.02321961593563
At time: 92.49613332748413 and batch: 550, loss is 4.624335870742798 and perplexity is 101.93505254686343
At time: 93.66142702102661 and batch: 600, loss is 4.586027517318725 and perplexity is 98.10393886739442
At time: 94.82711958885193 and batch: 650, loss is 4.564713554382324 and perplexity is 96.03508124809413
At time: 95.99723505973816 and batch: 700, loss is 4.569647035598755 and perplexity is 96.51003915215676
At time: 97.17105531692505 and batch: 750, loss is 4.584364175796509 and perplexity is 97.9408941494993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.678333726040153 and perplexity of 107.59064763288929
Finished 5 epochs...
Completing Train Step...
At time: 100.55343508720398 and batch: 50, loss is 4.6238689041137695 and perplexity is 101.88746339113435
At time: 101.7861397266388 and batch: 100, loss is 4.602817497253418 and perplexity is 99.76500766683317
At time: 102.94268202781677 and batch: 150, loss is 4.5822493267059325 and perplexity is 97.73398280882641
At time: 104.09968757629395 and batch: 200, loss is 4.570842256546021 and perplexity is 96.62545893490363
At time: 105.26000809669495 and batch: 250, loss is 4.565835409164428 and perplexity is 96.14287911871456
At time: 106.41644644737244 and batch: 300, loss is 4.61680380821228 and perplexity is 101.17015559902131
At time: 107.57324600219727 and batch: 350, loss is 4.550790796279907 and perplexity is 94.70727286878207
At time: 108.72717809677124 and batch: 400, loss is 4.5822618865966795 and perplexity is 97.7352103446816
At time: 109.88541412353516 and batch: 450, loss is 4.535620422363281 and perplexity is 93.28137120379346
At time: 111.04092526435852 and batch: 500, loss is 4.506553230285644 and perplexity is 90.60897140580055
At time: 112.20310091972351 and batch: 550, loss is 4.523395538330078 and perplexity is 92.14795928843942
At time: 113.36187982559204 and batch: 600, loss is 4.487315692901611 and perplexity is 88.88253734425999
At time: 114.51998972892761 and batch: 650, loss is 4.4661280536651615 and perplexity is 87.01913645023082
At time: 115.67675542831421 and batch: 700, loss is 4.472813835144043 and perplexity is 87.60287658609795
At time: 116.83772611618042 and batch: 750, loss is 4.491301898956299 and perplexity is 89.2375485566106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.653863241506177 and perplexity of 104.98980408667235
Finished 6 epochs...
Completing Train Step...
At time: 120.23217368125916 and batch: 50, loss is 4.531963863372803 and perplexity is 92.94090521361055
At time: 121.4590117931366 and batch: 100, loss is 4.517056665420532 and perplexity is 91.56569249292964
At time: 122.62432837486267 and batch: 150, loss is 4.495829353332519 and perplexity is 89.64248345679863
At time: 123.79409837722778 and batch: 200, loss is 4.484717435836792 and perplexity is 88.65189742431993
At time: 124.96126461029053 and batch: 250, loss is 4.477701301574707 and perplexity is 88.03208071063943
At time: 126.13223695755005 and batch: 300, loss is 4.531313943862915 and perplexity is 92.88052073069787
At time: 127.30242705345154 and batch: 350, loss is 4.467971696853637 and perplexity is 87.17971666924322
At time: 128.47712516784668 and batch: 400, loss is 4.498745412826538 and perplexity is 89.90426777556952
At time: 129.64680886268616 and batch: 450, loss is 4.45436619758606 and perplexity is 86.00162552996177
At time: 130.81883883476257 and batch: 500, loss is 4.424802103042603 and perplexity is 83.496281926795
At time: 132.04331278800964 and batch: 550, loss is 4.4412829780578615 and perplexity is 84.88377586486281
At time: 133.2097294330597 and batch: 600, loss is 4.403000450134277 and perplexity is 81.69562494066878
At time: 134.3763599395752 and batch: 650, loss is 4.384524030685425 and perplexity is 80.20004136820974
At time: 135.5427176952362 and batch: 700, loss is 4.391138114929199 and perplexity is 80.73224929202748
At time: 136.7057273387909 and batch: 750, loss is 4.412352094650268 and perplexity is 82.46319681942082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.638180045194404 and perplexity of 103.35607292795392
Finished 7 epochs...
Completing Train Step...
At time: 140.13495659828186 and batch: 50, loss is 4.458055906295776 and perplexity is 86.3195326083232
At time: 141.35059094429016 and batch: 100, loss is 4.443337926864624 and perplexity is 85.05838702584236
At time: 142.52430272102356 and batch: 150, loss is 4.4199854850769045 and perplexity is 83.09507923090571
At time: 143.69140577316284 and batch: 200, loss is 4.411871795654297 and perplexity is 82.42359933886014
At time: 144.85586977005005 and batch: 250, loss is 4.40470365524292 and perplexity is 81.83488790945569
At time: 146.01889872550964 and batch: 300, loss is 4.458448028564453 and perplexity is 86.35338705638839
At time: 147.18373727798462 and batch: 350, loss is 4.396677856445312 and perplexity is 81.1807261611534
At time: 148.3582727909088 and batch: 400, loss is 4.425920906066895 and perplexity is 83.58975009601741
At time: 149.54982209205627 and batch: 450, loss is 4.382659406661987 and perplexity is 80.05063777844269
At time: 150.71563124656677 and batch: 500, loss is 4.355022163391113 and perplexity is 77.86855114885759
At time: 151.8817503452301 and batch: 550, loss is 4.369047393798828 and perplexity is 78.96837009886949
At time: 153.0491077899933 and batch: 600, loss is 4.332146301269531 and perplexity is 76.10746093589957
At time: 154.21914434432983 and batch: 650, loss is 4.314513950347901 and perplexity is 74.77726914400886
At time: 155.38210892677307 and batch: 700, loss is 4.32096158027649 and perplexity is 75.26096296622941
At time: 156.54898381233215 and batch: 750, loss is 4.344264144897461 and perplexity is 77.03532977704275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.630871794944586 and perplexity of 102.603474321356
Finished 8 epochs...
Completing Train Step...
At time: 159.928537607193 and batch: 50, loss is 4.390899333953858 and perplexity is 80.71297426814608
At time: 161.13190412521362 and batch: 100, loss is 4.378224649429321 and perplexity is 79.6964186520292
At time: 162.297372341156 and batch: 150, loss is 4.355008993148804 and perplexity is 77.86752560792402
At time: 163.46336150169373 and batch: 200, loss is 4.349317255020142 and perplexity is 77.42558294747533
At time: 164.63210725784302 and batch: 250, loss is 4.338285665512085 and perplexity is 76.57614961327228
At time: 165.8017611503601 and batch: 300, loss is 4.395125169754028 and perplexity is 81.05477573413388
At time: 166.97138357162476 and batch: 350, loss is 4.333961544036865 and perplexity is 76.24573992089698
At time: 168.1372673511505 and batch: 400, loss is 4.362099828720093 and perplexity is 78.42163365115621
At time: 169.29124069213867 and batch: 450, loss is 4.320773448944092 and perplexity is 75.24680535277612
At time: 170.45397567749023 and batch: 500, loss is 4.294521389007568 and perplexity is 73.297125238801
At time: 171.62218737602234 and batch: 550, loss is 4.306923933029175 and perplexity is 74.21185683513761
At time: 172.79218339920044 and batch: 600, loss is 4.2707417201995845 and perplexity is 71.57470433970333
At time: 173.96225142478943 and batch: 650, loss is 4.253782138824463 and perplexity is 70.37106279529901
At time: 175.12844133377075 and batch: 700, loss is 4.259004220962525 and perplexity is 70.73950745219511
At time: 176.29703855514526 and batch: 750, loss is 4.283592929840088 and perplexity is 72.50046167998076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.628406968227653 and perplexity of 102.35088595775272
Finished 9 epochs...
Completing Train Step...
At time: 179.6774036884308 and batch: 50, loss is 4.331411466598511 and perplexity is 76.05155507817754
At time: 180.87395858764648 and batch: 100, loss is 4.321477899551391 and perplexity is 75.29983168552754
At time: 182.04422545433044 and batch: 150, loss is 4.296098070144653 and perplexity is 73.41278258698762
At time: 183.21500611305237 and batch: 200, loss is 4.29184326171875 and perplexity is 73.10108882990556
At time: 184.37936091423035 and batch: 250, loss is 4.280205726623535 and perplexity is 72.2553033179677
At time: 185.54476594924927 and batch: 300, loss is 4.338549633026123 and perplexity is 76.59636589722396
At time: 186.71031641960144 and batch: 350, loss is 4.279058771133423 and perplexity is 72.1724772091474
At time: 187.88008737564087 and batch: 400, loss is 4.305181217193604 and perplexity is 74.08263928448318
At time: 189.04580450057983 and batch: 450, loss is 4.264535055160523 and perplexity is 71.13183990190173
At time: 190.21141958236694 and batch: 500, loss is 4.23957332611084 and perplexity is 69.37824363764523
At time: 191.43981337547302 and batch: 550, loss is 4.249543151855469 and perplexity is 70.07339213387999
At time: 192.60989212989807 and batch: 600, loss is 4.214670343399048 and perplexity is 67.67185372018251
At time: 193.77760195732117 and batch: 650, loss is 4.197641801834107 and perplexity is 66.5292567363349
At time: 194.9478211402893 and batch: 700, loss is 4.206090831756592 and perplexity is 67.09374575145868
At time: 196.1125361919403 and batch: 750, loss is 4.229033327102661 and perplexity is 68.6508371842126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.627169586891352 and perplexity of 102.22431720477034
Finished 10 epochs...
Completing Train Step...
At time: 199.52383017539978 and batch: 50, loss is 4.278967065811157 and perplexity is 72.16585891233738
At time: 200.72688102722168 and batch: 100, loss is 4.270451717376709 and perplexity is 71.55395048288229
At time: 201.89336156845093 and batch: 150, loss is 4.242726440429688 and perplexity is 69.597346417603
At time: 203.0600974559784 and batch: 200, loss is 4.242364177703857 and perplexity is 69.5721384594067
At time: 204.22491717338562 and batch: 250, loss is 4.230458745956421 and perplexity is 68.74876315804542
At time: 205.39410638809204 and batch: 300, loss is 4.287375345230102 and perplexity is 72.7752078164714
At time: 206.56137013435364 and batch: 350, loss is 4.2293667888641355 and perplexity is 68.67373343060594
At time: 207.72890639305115 and batch: 400, loss is 4.2520189952850345 and perplexity is 70.2470978266994
At time: 208.89437770843506 and batch: 450, loss is 4.213790159225464 and perplexity is 67.61231623136241
At time: 210.05738925933838 and batch: 500, loss is 4.190670757293701 and perplexity is 66.06709108465192
At time: 211.22090315818787 and batch: 550, loss is 4.20013258934021 and perplexity is 66.69517352375544
At time: 212.38200998306274 and batch: 600, loss is 4.1654416370391845 and perplexity is 64.42112691768624
At time: 213.5495045185089 and batch: 650, loss is 4.147842020988464 and perplexity is 63.2972586527283
At time: 214.71258640289307 and batch: 700, loss is 4.156532797813416 and perplexity is 63.84975834958856
At time: 215.87807250022888 and batch: 750, loss is 4.179139099121094 and perplexity is 65.30960390953945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.629702900731286 and perplexity of 102.48361178091976
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 219.25763201713562 and batch: 50, loss is 4.255157318115234 and perplexity is 70.46790219403127
At time: 220.45059776306152 and batch: 100, loss is 4.262361135482788 and perplexity is 70.97737295562581
At time: 221.60965085029602 and batch: 150, loss is 4.2344337797164915 and perplexity is 69.02258567889079
At time: 222.7771987915039 and batch: 200, loss is 4.224443254470825 and perplexity is 68.33644694430305
At time: 223.94518423080444 and batch: 250, loss is 4.201031370162964 and perplexity is 66.75514481317413
At time: 225.12081265449524 and batch: 300, loss is 4.244821977615357 and perplexity is 69.74334316242286
At time: 226.29327082633972 and batch: 350, loss is 4.181335468292236 and perplexity is 65.45320555352946
At time: 227.47214460372925 and batch: 400, loss is 4.185311751365662 and perplexity is 65.71398414803535
At time: 228.66573929786682 and batch: 450, loss is 4.131687932014465 and perplexity is 62.28296366734358
At time: 229.84805607795715 and batch: 500, loss is 4.089831018447876 and perplexity is 59.72979761740258
At time: 231.01546239852905 and batch: 550, loss is 4.084978566169739 and perplexity is 59.44066369649372
At time: 232.1977722644806 and batch: 600, loss is 4.031062841415405 and perplexity is 56.32073946199002
At time: 233.37288784980774 and batch: 650, loss is 3.989055428504944 and perplexity is 54.00385476237473
At time: 234.5367066860199 and batch: 700, loss is 3.972229952812195 and perplexity is 53.102815679013844
At time: 235.70121359825134 and batch: 750, loss is 3.976021752357483 and perplexity is 53.30455314366165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.573801439861919 and perplexity of 96.91181486175276
Finished 12 epochs...
Completing Train Step...
At time: 239.06766152381897 and batch: 50, loss is 4.1979310131073 and perplexity is 66.54850053000712
At time: 240.25713658332825 and batch: 100, loss is 4.197227029800415 and perplexity is 66.50166798313556
At time: 241.4133105278015 and batch: 150, loss is 4.1695018482208255 and perplexity is 64.6832220182003
At time: 242.57193636894226 and batch: 200, loss is 4.162364454269409 and perplexity is 64.22319602687385
At time: 243.72850513458252 and batch: 250, loss is 4.141369242668151 and perplexity is 62.88887265105458
At time: 244.8900556564331 and batch: 300, loss is 4.189928140640259 and perplexity is 66.01804677538226
At time: 246.04961681365967 and batch: 350, loss is 4.131127524375915 and perplexity is 62.24806959711786
At time: 247.20577239990234 and batch: 400, loss is 4.140286960601807 and perplexity is 62.82084597067997
At time: 248.3629584312439 and batch: 450, loss is 4.092852134704589 and perplexity is 59.91052113591714
At time: 249.52514028549194 and batch: 500, loss is 4.057268443107605 and perplexity is 57.81616703800362
At time: 250.7172303199768 and batch: 550, loss is 4.059215512275696 and perplexity is 57.928848778232805
At time: 251.8756275177002 and batch: 600, loss is 4.011419305801391 and perplexity is 55.22519640779726
At time: 253.0368971824646 and batch: 650, loss is 3.978389949798584 and perplexity is 53.43093844361966
At time: 254.18724131584167 and batch: 700, loss is 3.9716180896759035 and perplexity is 53.0703339618626
At time: 255.3454384803772 and batch: 750, loss is 3.9846713972091674 and perplexity is 53.767618385205765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.570787651594295 and perplexity of 96.62018285043452
Finished 13 epochs...
Completing Train Step...
At time: 258.73436212539673 and batch: 50, loss is 4.174835357666016 and perplexity is 65.0291322314855
At time: 259.9527802467346 and batch: 100, loss is 4.17024335861206 and perplexity is 64.73120308649722
At time: 261.11531019210815 and batch: 150, loss is 4.141839523315429 and perplexity is 62.91845502625556
At time: 262.2792465686798 and batch: 200, loss is 4.136058735847473 and perplexity is 62.55578607595508
At time: 263.44463181495667 and batch: 250, loss is 4.115332107543946 and perplexity is 61.27255996348536
At time: 264.6039628982544 and batch: 300, loss is 4.16574800491333 and perplexity is 64.44086650502338
At time: 265.76594829559326 and batch: 350, loss is 4.108450698852539 and perplexity is 60.85236585863762
At time: 266.9296233654022 and batch: 400, loss is 4.119332308769226 and perplexity is 61.518153417053504
At time: 268.09290504455566 and batch: 450, loss is 4.074250712394714 and perplexity is 58.806401168821075
At time: 269.255982875824 and batch: 500, loss is 4.041210384368896 and perplexity is 56.89516616549954
At time: 270.4158396720886 and batch: 550, loss is 4.046259994506836 and perplexity is 57.1831911683374
At time: 271.5756914615631 and batch: 600, loss is 4.000923590660095 and perplexity is 54.64859966839308
At time: 272.73773741722107 and batch: 650, loss is 3.97204553604126 and perplexity is 53.09302353216456
At time: 273.8978807926178 and batch: 700, loss is 3.9688183498382568 and perplexity is 52.921958636747206
At time: 275.05661511421204 and batch: 750, loss is 3.985654563903809 and perplexity is 53.820506911710936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.570596739303234 and perplexity of 96.60173863063393
Finished 14 epochs...
Completing Train Step...
At time: 278.4455215930939 and batch: 50, loss is 4.157240114212036 and perplexity is 63.8949363063885
At time: 279.6339750289917 and batch: 100, loss is 4.151402316093445 and perplexity is 63.52301721795502
At time: 280.79365968704224 and batch: 150, loss is 4.1226541757583615 and perplexity is 61.72284833653952
At time: 281.9541938304901 and batch: 200, loss is 4.1180726480484005 and perplexity is 61.440710201904636
At time: 283.11776089668274 and batch: 250, loss is 4.097318544387817 and perplexity is 60.178704530272256
At time: 284.2773959636688 and batch: 300, loss is 4.1490816974639895 and perplexity is 63.37577543289262
At time: 285.4379994869232 and batch: 350, loss is 4.092700381278991 and perplexity is 59.90143019891326
At time: 286.5975661277771 and batch: 400, loss is 4.104615569114685 and perplexity is 60.619436084165315
At time: 287.76178669929504 and batch: 450, loss is 4.0611122655868535 and perplexity is 58.03882978441032
At time: 288.9247682094574 and batch: 500, loss is 4.029503507614136 and perplexity is 56.23298506621361
At time: 290.08419704437256 and batch: 550, loss is 4.036495518684387 and perplexity is 56.627544494815204
At time: 291.24564123153687 and batch: 600, loss is 3.9923452949523925 and perplexity is 54.18181280077357
At time: 292.40863704681396 and batch: 650, loss is 3.9656768798828126 and perplexity is 52.755966759466425
At time: 293.5698935985565 and batch: 700, loss is 3.9644861125946047 and perplexity is 52.69318406719802
At time: 294.7270531654358 and batch: 750, loss is 3.9831945753097533 and perplexity is 53.688271793708395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.571081117142079 and perplexity of 96.64854170629172
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 298.10445189476013 and batch: 50, loss is 4.1543621063232425 and perplexity is 63.71131054059261
At time: 299.3041260242462 and batch: 100, loss is 4.160665979385376 and perplexity is 64.11420712510711
At time: 300.4619679450989 and batch: 150, loss is 4.138103775978088 and perplexity is 62.68384606816326
At time: 301.6291878223419 and batch: 200, loss is 4.136254391670227 and perplexity is 62.5680266771812
At time: 302.7906765937805 and batch: 250, loss is 4.114091777801514 and perplexity is 61.196608896888186
At time: 303.95277643203735 and batch: 300, loss is 4.160755767822265 and perplexity is 64.11996409799819
At time: 305.11239314079285 and batch: 350, loss is 4.103598103523255 and perplexity is 60.55778926085679
At time: 306.28432393074036 and batch: 400, loss is 4.108495779037476 and perplexity is 60.8551091563781
At time: 307.4559864997864 and batch: 450, loss is 4.059044995307922 and perplexity is 57.91897176871533
At time: 308.6516954898834 and batch: 500, loss is 4.017181034088135 and perplexity is 55.54430741673728
At time: 309.9043779373169 and batch: 550, loss is 4.014303736686706 and perplexity is 55.384719626210575
At time: 311.0775411128998 and batch: 600, loss is 3.9657303953170775 and perplexity is 52.75879009348292
At time: 312.2375717163086 and batch: 650, loss is 3.9305654764175415 and perplexity is 50.935772505194734
At time: 313.40003776550293 and batch: 700, loss is 3.9216237545013426 and perplexity is 50.48234920548927
At time: 314.5645091533661 and batch: 750, loss is 3.939156651496887 and perplexity is 51.37525578034682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5554011145303415 and perplexity of 95.14491158905892
Finished 16 epochs...
Completing Train Step...
At time: 317.97546195983887 and batch: 50, loss is 4.147384662628173 and perplexity is 63.26831574144573
At time: 319.19552636146545 and batch: 100, loss is 4.147277040481567 and perplexity is 63.26150703588381
At time: 320.3600187301636 and batch: 150, loss is 4.121099352836609 and perplexity is 61.62695480519864
At time: 321.5196044445038 and batch: 200, loss is 4.118258957862854 and perplexity is 61.45215827563337
At time: 322.6783094406128 and batch: 250, loss is 4.095413293838501 and perplexity is 60.06415817481223
At time: 323.8410472869873 and batch: 300, loss is 4.143702421188355 and perplexity is 63.035774925858625
At time: 325.0075042247772 and batch: 350, loss is 4.08748456954956 and perplexity is 59.58980900188229
At time: 326.1731107234955 and batch: 400, loss is 4.094850282669068 and perplexity is 60.0303509007067
At time: 327.33188915252686 and batch: 450, loss is 4.047940254211426 and perplexity is 57.27935454733819
At time: 328.4960427284241 and batch: 500, loss is 4.0092113590240475 and perplexity is 55.10339662655917
At time: 329.65667033195496 and batch: 550, loss is 4.010519204139709 and perplexity is 55.175510481290935
At time: 330.8120987415314 and batch: 600, loss is 3.9651736783981324 and perplexity is 52.72942655676205
At time: 331.98150396347046 and batch: 650, loss is 3.933629379272461 and perplexity is 51.092074088150625
At time: 333.15192008018494 and batch: 700, loss is 3.9274947881698608 and perplexity is 50.77960452148323
At time: 334.3237133026123 and batch: 750, loss is 3.9467588663101196 and perplexity is 51.767309862363525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.553439827852471 and perplexity of 94.95848801628578
Finished 17 epochs...
Completing Train Step...
At time: 337.74382400512695 and batch: 50, loss is 4.143675012588501 and perplexity is 63.034047227204226
At time: 338.9664463996887 and batch: 100, loss is 4.141252088546753 and perplexity is 62.881505391994196
At time: 340.1361451148987 and batch: 150, loss is 4.113501367568969 and perplexity is 61.1604884567864
At time: 341.3021833896637 and batch: 200, loss is 4.110372519493103 and perplexity is 60.969425639294215
At time: 342.46890211105347 and batch: 250, loss is 4.087363619804382 and perplexity is 59.58260206551562
At time: 343.6447732448578 and batch: 300, loss is 4.136274185180664 and perplexity is 62.56926513032688
At time: 344.81116342544556 and batch: 350, loss is 4.080510249137879 and perplexity is 59.175656475577526
At time: 345.9785695075989 and batch: 400, loss is 4.088944253921508 and perplexity is 59.67685482907095
At time: 347.1457335948944 and batch: 450, loss is 4.043260960578919 and perplexity is 57.01195373969145
At time: 348.31560802459717 and batch: 500, loss is 4.005988121032715 and perplexity is 54.92607119965074
At time: 349.48516631126404 and batch: 550, loss is 4.009036340713501 and perplexity is 55.09375336707432
At time: 350.65481185913086 and batch: 600, loss is 3.9651284980773926 and perplexity is 52.727044278174255
At time: 351.8259599208832 and batch: 650, loss is 3.935105209350586 and perplexity is 51.167532976395925
At time: 352.9926781654358 and batch: 700, loss is 3.9302611637115477 and perplexity is 50.92027446067732
At time: 354.15646052360535 and batch: 750, loss is 3.9502086544036867 and perplexity is 51.94620450845345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.552857509879178 and perplexity of 94.90320807880987
Finished 18 epochs...
Completing Train Step...
At time: 357.5700523853302 and batch: 50, loss is 4.140122060775757 and perplexity is 62.81048767817088
At time: 358.77194690704346 and batch: 100, loss is 4.136707153320312 and perplexity is 62.59636149415736
At time: 359.9395561218262 and batch: 150, loss is 4.10820511341095 and perplexity is 60.837423238416584
At time: 361.10334300994873 and batch: 200, loss is 4.105062580108642 and perplexity is 60.64653969589732
At time: 362.26706886291504 and batch: 250, loss is 4.082082571983338 and perplexity is 59.268772897504334
At time: 363.4315700531006 and batch: 300, loss is 4.131414875984192 and perplexity is 62.26595925021592
At time: 364.5965151786804 and batch: 350, loss is 4.076084933280945 and perplexity is 58.914364081734995
At time: 365.7609326839447 and batch: 400, loss is 4.08514865398407 and perplexity is 59.45077468891815
At time: 366.92331171035767 and batch: 450, loss is 4.040212078094482 and perplexity is 56.83839570600052
At time: 368.08499121665955 and batch: 500, loss is 4.003786902427674 and perplexity is 54.80529988062653
At time: 369.3051769733429 and batch: 550, loss is 4.007843499183655 and perplexity is 55.028074430092516
At time: 370.4699454307556 and batch: 600, loss is 3.964810485839844 and perplexity is 52.71027909875142
At time: 371.64121890068054 and batch: 650, loss is 3.9356220674514772 and perplexity is 51.193986166001174
At time: 372.8058910369873 and batch: 700, loss is 3.9315938472747805 and perplexity is 50.98818031194196
At time: 373.97198462486267 and batch: 750, loss is 3.9519655466079713 and perplexity is 52.03754860756859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5526921471884085 and perplexity of 94.8875159264433
Finished 19 epochs...
Completing Train Step...
At time: 377.3867254257202 and batch: 50, loss is 4.136808128356933 and perplexity is 62.60268248317725
At time: 378.6012496948242 and batch: 100, loss is 4.132871069908142 and perplexity is 62.35669661130789
At time: 379.76154041290283 and batch: 150, loss is 4.103937363624572 and perplexity is 60.57833758799369
At time: 380.92463183403015 and batch: 200, loss is 4.100869655609131 and perplexity is 60.3927856912498
At time: 382.09449529647827 and batch: 250, loss is 4.077967123985291 and perplexity is 59.025356571911495
At time: 383.2657117843628 and batch: 300, loss is 4.127643995285034 and perplexity is 62.03160388702468
At time: 384.4253315925598 and batch: 350, loss is 4.072688970565796 and perplexity is 58.71463243049849
At time: 385.58333587646484 and batch: 400, loss is 4.0822099637985225 and perplexity is 59.276323735014756
At time: 386.7555887699127 and batch: 450, loss is 4.03778796672821 and perplexity is 56.70077997025299
At time: 387.91339635849 and batch: 500, loss is 4.001935992240906 and perplexity is 54.70395401277134
At time: 389.07383918762207 and batch: 550, loss is 4.0066842222213745 and perplexity is 54.964318613591956
At time: 390.235387802124 and batch: 600, loss is 3.9642744302749633 and perplexity is 52.682031032257186
At time: 391.39471650123596 and batch: 650, loss is 3.935634288787842 and perplexity is 51.19461182874917
At time: 392.55190229415894 and batch: 700, loss is 3.9321950006484987 and perplexity is 51.018841243594125
At time: 393.71329140663147 and batch: 750, loss is 3.9528899145126344 and perplexity is 52.08567268608893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.55269959915516 and perplexity of 94.88822302769174
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 397.143230676651 and batch: 50, loss is 4.136711006164551 and perplexity is 62.59660266865268
At time: 398.37566804885864 and batch: 100, loss is 4.136448769569397 and perplexity is 62.580189700835135
At time: 399.5461845397949 and batch: 150, loss is 4.108808751106262 and perplexity is 60.874158086523046
At time: 400.7142095565796 and batch: 200, loss is 4.105864701271057 and perplexity is 60.695205083974166
At time: 401.8827471733093 and batch: 250, loss is 4.082046885490417 and perplexity is 59.26665784059961
At time: 403.05281686782837 and batch: 300, loss is 4.131394867897034 and perplexity is 62.264713439939406
At time: 404.2194628715515 and batch: 350, loss is 4.075955901145935 and perplexity is 58.90676272597492
At time: 405.38530468940735 and batch: 400, loss is 4.083958487510682 and perplexity is 59.38006045926631
At time: 406.55508947372437 and batch: 450, loss is 4.038320446014405 and perplexity is 56.730980000830016
At time: 407.7223172187805 and batch: 500, loss is 3.9986597061157227 and perplexity is 54.52502148438257
At time: 408.89153265953064 and batch: 550, loss is 3.9986508798599245 and perplexity is 54.52454023471936
At time: 410.06369256973267 and batch: 600, loss is 3.9545611906051636 and perplexity is 52.1727950080605
At time: 411.23185086250305 and batch: 650, loss is 3.9241334629058837 and perplexity is 50.60920429963473
At time: 412.4017732143402 and batch: 700, loss is 3.919193773269653 and perplexity is 50.35982696804626
At time: 413.569011926651 and batch: 750, loss is 3.940488295555115 and perplexity is 51.44371490592414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.551080038381177 and perplexity of 94.73467016142644
Finished 21 epochs...
Completing Train Step...
At time: 416.9576323032379 and batch: 50, loss is 4.135461406707764 and perplexity is 62.51843083987072
At time: 418.1529974937439 and batch: 100, loss is 4.134087724685669 and perplexity is 62.43260935458757
At time: 419.3194899559021 and batch: 150, loss is 4.105858740806579 and perplexity is 60.69484331343847
At time: 420.4882469177246 and batch: 200, loss is 4.10278504371643 and perplexity is 60.50857216735023
At time: 421.65710520744324 and batch: 250, loss is 4.079003210067749 and perplexity is 59.086543614415824
At time: 422.82720136642456 and batch: 300, loss is 4.128501605987549 and perplexity is 62.084825672936944
At time: 423.997802734375 and batch: 350, loss is 4.073411269187927 and perplexity is 58.757057248451275
At time: 425.1649258136749 and batch: 400, loss is 4.081882419586182 and perplexity is 59.25691129763584
At time: 426.32985186576843 and batch: 450, loss is 4.036762027740479 and perplexity is 56.64263825947043
At time: 427.4951548576355 and batch: 500, loss is 3.9977018642425537 and perplexity is 54.472820139977195
At time: 428.71421241760254 and batch: 550, loss is 3.9984613990783693 and perplexity is 54.5142098609563
At time: 429.88020968437195 and batch: 600, loss is 3.95505597114563 and perplexity is 52.198615478978745
At time: 431.04693698883057 and batch: 650, loss is 3.925205683708191 and perplexity is 50.66349764329771
At time: 432.2122814655304 and batch: 700, loss is 3.920578236579895 and perplexity is 50.42959658638786
At time: 433.37989568710327 and batch: 750, loss is 3.94221049785614 and perplexity is 51.53238772445764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550541012786155 and perplexity of 94.68361950951353
Finished 22 epochs...
Completing Train Step...
At time: 436.77247500419617 and batch: 50, loss is 4.134739065170288 and perplexity is 62.473287486839716
At time: 437.9717676639557 and batch: 100, loss is 4.132670006752014 and perplexity is 62.344160237425186
At time: 439.1432292461395 and batch: 150, loss is 4.104065670967102 and perplexity is 60.586110732169516
At time: 440.31412625312805 and batch: 200, loss is 4.100872020721436 and perplexity is 60.39292852713929
At time: 441.4818425178528 and batch: 250, loss is 4.077082738876343 and perplexity is 58.973178501664016
At time: 442.64928555488586 and batch: 300, loss is 4.1267493534088135 and perplexity is 61.97613263369117
At time: 443.82849168777466 and batch: 350, loss is 4.071904129981995 and perplexity is 58.66856888273942
At time: 444.9995336532593 and batch: 400, loss is 4.080691208839417 and perplexity is 59.18636585366537
At time: 446.17229175567627 and batch: 450, loss is 4.035848751068115 and perplexity is 56.59093147418126
At time: 447.3495638370514 and batch: 500, loss is 3.9972090244293215 and perplexity is 54.44598037986762
At time: 448.5196523666382 and batch: 550, loss is 3.9985045862197874 and perplexity is 54.516564224685624
At time: 449.69414925575256 and batch: 600, loss is 3.9555585861206053 and perplexity is 52.22485787915066
At time: 450.8625075817108 and batch: 650, loss is 3.9260733985900877 and perplexity is 50.70747819270084
At time: 452.0328814983368 and batch: 700, loss is 3.9216506338119506 and perplexity is 50.48370615447063
At time: 453.2021949291229 and batch: 750, loss is 3.9434308052062987 and perplexity is 51.59531146130974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550278064816497 and perplexity of 94.65872591700683
Finished 23 epochs...
Completing Train Step...
At time: 456.5887486934662 and batch: 50, loss is 4.134104127883911 and perplexity is 62.43363345745486
At time: 457.8192403316498 and batch: 100, loss is 4.1315593957901005 and perplexity is 62.27495856483539
At time: 458.9853858947754 and batch: 150, loss is 4.102696933746338 and perplexity is 60.50324099373445
At time: 460.17748379707336 and batch: 200, loss is 4.09943046092987 and perplexity is 60.30593123064767
At time: 461.3777196407318 and batch: 250, loss is 4.07564028263092 and perplexity is 58.88817359469034
At time: 462.58192920684814 and batch: 300, loss is 4.125460395812988 and perplexity is 61.89629948859545
At time: 463.76018953323364 and batch: 350, loss is 4.070803947448731 and perplexity is 58.6040582412505
At time: 464.94849705696106 and batch: 400, loss is 4.07981550693512 and perplexity is 59.134558927401585
At time: 466.1244580745697 and batch: 450, loss is 4.03518084526062 and perplexity is 56.55314668214378
At time: 467.2973418235779 and batch: 500, loss is 3.996864161491394 and perplexity is 54.42720721638569
At time: 468.46637320518494 and batch: 550, loss is 3.998566541671753 and perplexity is 54.519941927694255
At time: 469.6371738910675 and batch: 600, loss is 3.955950503349304 and perplexity is 52.24532971208983
At time: 470.8024649620056 and batch: 650, loss is 3.9267326688766477 and perplexity is 50.7409191484827
At time: 471.970618724823 and batch: 700, loss is 3.922467727661133 and perplexity is 50.52497293737466
At time: 473.1432864665985 and batch: 750, loss is 3.9443141746520998 and perplexity is 51.64090931991363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550124767214753 and perplexity of 94.64421607353019
Finished 24 epochs...
Completing Train Step...
At time: 476.55258560180664 and batch: 50, loss is 4.133496780395507 and perplexity is 62.395726059629226
At time: 477.7659935951233 and batch: 100, loss is 4.130605759620667 and perplexity is 62.21559922000566
At time: 478.9334080219269 and batch: 150, loss is 4.101554274559021 and perplexity is 60.434145893144205
At time: 480.10647106170654 and batch: 200, loss is 4.098248229026795 and perplexity is 60.23467776216304
At time: 481.2743811607361 and batch: 250, loss is 4.074464654922485 and perplexity is 58.818983704858034
At time: 482.44062662124634 and batch: 300, loss is 4.12441888332367 and perplexity is 61.83186727893542
At time: 483.6067087650299 and batch: 350, loss is 4.069915428161621 and perplexity is 58.55201053132145
At time: 484.7749054431915 and batch: 400, loss is 4.0790970993041995 and perplexity is 59.09209146531794
At time: 485.93888092041016 and batch: 450, loss is 4.034637045860291 and perplexity is 56.522401475261475
At time: 487.10329151153564 and batch: 500, loss is 3.996578960418701 and perplexity is 54.411686731838515
At time: 488.330201625824 and batch: 550, loss is 3.998603639602661 and perplexity is 54.521964542250174
At time: 489.5032203197479 and batch: 600, loss is 3.95623743057251 and perplexity is 52.26032247028197
At time: 490.6704783439636 and batch: 650, loss is 3.927231993675232 and perplexity is 50.766261674265735
At time: 491.8363993167877 and batch: 700, loss is 3.9230993509292604 and perplexity is 50.55689576644294
At time: 493.00422382354736 and batch: 750, loss is 3.94497435092926 and perplexity is 51.67501267905335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550027181935865 and perplexity of 94.63498064193787
Finished 25 epochs...
Completing Train Step...
At time: 496.391233921051 and batch: 50, loss is 4.132906265258789 and perplexity is 62.358891315731796
At time: 497.5833125114441 and batch: 100, loss is 4.129751949310303 and perplexity is 62.16250157081196
At time: 498.7465331554413 and batch: 150, loss is 4.1005559825897215 and perplexity is 60.373845074547
At time: 499.9131076335907 and batch: 200, loss is 4.097231106758118 and perplexity is 60.17344287702635
At time: 501.0880606174469 and batch: 250, loss is 4.073459143638611 and perplexity is 58.75987027762642
At time: 502.2669711112976 and batch: 300, loss is 4.123531184196472 and perplexity is 61.77700353916863
At time: 503.43177008628845 and batch: 350, loss is 4.069156565666199 and perplexity is 58.50759446147398
At time: 504.60137581825256 and batch: 400, loss is 4.078478422164917 and perplexity is 59.05554384596999
At time: 505.76878571510315 and batch: 450, loss is 4.0341661214828495 and perplexity is 56.495789965031506
At time: 506.9269154071808 and batch: 500, loss is 3.996320652961731 and perplexity is 54.39763360250154
At time: 508.10284996032715 and batch: 550, loss is 3.9986088514328 and perplexity is 54.522248702208714
At time: 509.2742202281952 and batch: 600, loss is 3.9564412307739256 and perplexity is 52.27097421990496
At time: 510.43696188926697 and batch: 650, loss is 3.9276124143600466 and perplexity is 50.7855778842073
At time: 511.61335611343384 and batch: 700, loss is 3.9235957765579226 and perplexity is 50.58199973581835
At time: 512.7757081985474 and batch: 750, loss is 3.9454834604263307 and perplexity is 51.70132761679348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.549961888512899 and perplexity of 94.62880180184051
Finished 26 epochs...
Completing Train Step...
At time: 516.19873046875 and batch: 50, loss is 4.132331142425537 and perplexity is 62.32303760461204
At time: 517.4144344329834 and batch: 100, loss is 4.128969430923462 and perplexity is 62.1138772975359
At time: 518.5736403465271 and batch: 150, loss is 4.099659104347229 and perplexity is 60.31972136130245
At time: 519.7340402603149 and batch: 200, loss is 4.096328854560852 and perplexity is 60.11917574097627
At time: 520.8991105556488 and batch: 250, loss is 4.07257158279419 and perplexity is 58.70774045516151
At time: 522.0706236362457 and batch: 300, loss is 4.122747836112976 and perplexity is 61.72862959113547
At time: 523.2340495586395 and batch: 350, loss is 4.068485202789307 and perplexity is 58.46832781709486
At time: 524.3969831466675 and batch: 400, loss is 4.07792528629303 and perplexity is 59.022887138865705
At time: 525.5587069988251 and batch: 450, loss is 4.033742051124573 and perplexity is 56.471836854401005
At time: 526.7152597904205 and batch: 500, loss is 3.9960759973526 and perplexity is 54.38432654420685
At time: 527.875807762146 and batch: 550, loss is 3.998585286140442 and perplexity is 54.52096388461662
At time: 529.0375094413757 and batch: 600, loss is 3.95658109664917 and perplexity is 52.27828565676243
At time: 530.2015037536621 and batch: 650, loss is 3.927902727127075 and perplexity is 50.800323726197526
At time: 531.3632776737213 and batch: 700, loss is 3.923991708755493 and perplexity is 50.60203074332999
At time: 532.5207953453064 and batch: 750, loss is 3.9458860540390015 and perplexity is 51.72214643153832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.549915757290153 and perplexity of 94.62443656019408
Finished 27 epochs...
Completing Train Step...
At time: 535.9255683422089 and batch: 50, loss is 4.131769800186158 and perplexity is 62.288062868434906
At time: 537.1481823921204 and batch: 100, loss is 4.1282408952713014 and perplexity is 62.0686416033395
At time: 538.3148379325867 and batch: 150, loss is 4.098837723731995 and perplexity is 60.27019625372605
At time: 539.4919848442078 and batch: 200, loss is 4.095511136054992 and perplexity is 60.070035272689196
At time: 540.660900592804 and batch: 250, loss is 4.071769580841065 and perplexity is 58.66067560822521
At time: 541.830198764801 and batch: 300, loss is 4.122039918899536 and perplexity is 61.68494629558861
At time: 543.0007319450378 and batch: 350, loss is 4.067876853942871 and perplexity is 58.43276949434325
At time: 544.1727163791656 and batch: 400, loss is 4.077418813705444 and perplexity is 58.99300123332451
At time: 545.3386693000793 and batch: 450, loss is 4.033350219726563 and perplexity is 56.449713750164605
At time: 546.5068488121033 and batch: 500, loss is 3.9958385372161866 and perplexity is 54.37141396777867
At time: 547.7410247325897 and batch: 550, loss is 3.99853684425354 and perplexity is 54.51832285021921
At time: 548.9077289104462 and batch: 600, loss is 3.956671290397644 and perplexity is 52.28300104395565
At time: 550.0752549171448 and batch: 650, loss is 3.9281241035461427 and perplexity is 50.81157096484229
At time: 551.2491428852081 and batch: 700, loss is 3.9243112659454344 and perplexity is 50.61820357001356
At time: 552.4308896064758 and batch: 750, loss is 3.946210198402405 and perplexity is 51.738914591272646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.549887013989825 and perplexity of 94.62171678068363
Finished 28 epochs...
Completing Train Step...
At time: 555.95356798172 and batch: 50, loss is 4.131221895217895 and perplexity is 62.253944277052256
At time: 557.1785113811493 and batch: 100, loss is 4.1275549364089965 and perplexity is 62.02607966809772
At time: 558.3450543880463 and batch: 150, loss is 4.098074789047241 and perplexity is 60.224231566803695
At time: 559.516294002533 and batch: 200, loss is 4.094757657051087 and perplexity is 60.024790809864314
At time: 560.6816866397858 and batch: 250, loss is 4.071032223701477 and perplexity is 58.61743768310784
At time: 561.8511996269226 and batch: 300, loss is 4.121387915611267 and perplexity is 61.64474061630906
At time: 563.0206291675568 and batch: 350, loss is 4.0673154878616335 and perplexity is 58.399976524807926
At time: 564.1894299983978 and batch: 400, loss is 4.076946821212768 and perplexity is 58.965163549727606
At time: 565.3583142757416 and batch: 450, loss is 4.0329824590682986 and perplexity is 56.428957583161775
At time: 566.5273129940033 and batch: 500, loss is 3.995605878829956 and perplexity is 54.358765473794016
At time: 567.6964845657349 and batch: 550, loss is 3.9984682416915893 and perplexity is 54.514582881885595
At time: 568.8667216300964 and batch: 600, loss is 3.9567230844497683 and perplexity is 52.28570906256595
At time: 570.0345199108124 and batch: 650, loss is 3.9282915925979616 and perplexity is 50.820082059422305
At time: 571.2007420063019 and batch: 700, loss is 3.924571647644043 and perplexity is 50.631385339910985
At time: 572.3684043884277 and batch: 750, loss is 3.946475305557251 and perplexity is 51.75263276602756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.549868561500727 and perplexity of 94.61997079059523
Finished 29 epochs...
Completing Train Step...
At time: 575.7971985340118 and batch: 50, loss is 4.13068639755249 and perplexity is 62.22061635953707
At time: 577.0199382305145 and batch: 100, loss is 4.126903300285339 and perplexity is 61.985674400173004
At time: 578.1907474994659 and batch: 150, loss is 4.097357869148254 and perplexity is 60.18107108994323
At time: 579.3663761615753 and batch: 200, loss is 4.094054174423218 and perplexity is 59.982579261574394
At time: 580.5357773303986 and batch: 250, loss is 4.070345330238342 and perplexity is 58.57718757368872
At time: 581.705064535141 and batch: 300, loss is 4.1207791566848755 and perplexity is 61.60722525025932
At time: 582.8770065307617 and batch: 350, loss is 4.066790518760681 and perplexity is 58.36932638752745
At time: 584.0493080615997 and batch: 400, loss is 4.076501369476318 and perplexity is 58.93890326451337
At time: 585.2254958152771 and batch: 450, loss is 4.032632188796997 and perplexity is 56.409195658089345
At time: 586.4018585681915 and batch: 500, loss is 3.9953755855560305 and perplexity is 54.346248457074196
At time: 587.574777841568 and batch: 550, loss is 3.9983828544616697 and perplexity is 54.50992823138977
At time: 588.7478723526001 and batch: 600, loss is 3.9567446231842043 and perplexity is 52.28683524269645
At time: 589.9216167926788 and batch: 650, loss is 3.928416666984558 and perplexity is 50.82643874753382
At time: 591.0909779071808 and batch: 700, loss is 3.9247850227355956 and perplexity is 50.64218996907169
At time: 592.2598452568054 and batch: 750, loss is 3.9466947317123413 and perplexity is 51.76398989323101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5498572061228195 and perplexity of 94.61889635116967
Finished 30 epochs...
Completing Train Step...
At time: 595.6479852199554 and batch: 50, loss is 4.1301620578765865 and perplexity is 62.18800017344807
At time: 596.871782541275 and batch: 100, loss is 4.126280064582825 and perplexity is 61.94705475066464
At time: 598.0425229072571 and batch: 150, loss is 4.096678490638733 and perplexity is 60.14019924886203
At time: 599.2115032672882 and batch: 200, loss is 4.093391227722168 and perplexity is 59.9428271867418
At time: 600.3798670768738 and batch: 250, loss is 4.06969865322113 and perplexity is 58.539319298343266
At time: 601.5459935665131 and batch: 300, loss is 4.120204472541809 and perplexity is 61.57183072611971
At time: 602.7122821807861 and batch: 350, loss is 4.066294403076172 and perplexity is 58.3403756312683
At time: 603.8786723613739 and batch: 400, loss is 4.076077146530151 and perplexity is 58.91390533204982
At time: 605.0501053333282 and batch: 450, loss is 4.032295837402343 and perplexity is 56.390225536950645
At time: 606.2172608375549 and batch: 500, loss is 3.9951467609405515 and perplexity is 54.3338141203637
At time: 607.4469223022461 and batch: 550, loss is 3.9982834339141844 and perplexity is 54.5045090938729
At time: 608.6158201694489 and batch: 600, loss is 3.956741662025452 and perplexity is 52.286680413305874
At time: 609.7881808280945 and batch: 650, loss is 3.928507618904114 and perplexity is 50.83106171993301
At time: 610.954372882843 and batch: 700, loss is 3.924960708618164 and perplexity is 50.65108786850638
At time: 612.1233477592468 and batch: 750, loss is 3.94687798500061 and perplexity is 51.7734766838091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.549853302711664 and perplexity of 94.61852701543499
Finished 31 epochs...
Completing Train Step...
At time: 615.4913692474365 and batch: 50, loss is 4.129647912979126 and perplexity is 62.156034748609834
At time: 616.7132394313812 and batch: 100, loss is 4.125680665969849 and perplexity is 61.90993489787466
At time: 617.8780424594879 and batch: 150, loss is 4.096030306816101 and perplexity is 60.101229975609705
At time: 619.0418565273285 and batch: 200, loss is 4.0927615737915035 and perplexity is 59.90509583008365
At time: 620.2075245380402 and batch: 250, loss is 4.06908483505249 and perplexity is 58.50339782633206
At time: 621.3746523857117 and batch: 300, loss is 4.119657769203186 and perplexity is 61.538178400454136
At time: 622.5413417816162 and batch: 350, loss is 4.0658214569091795 and perplexity is 58.312790297935635
At time: 623.7114021778107 and batch: 400, loss is 4.07566997051239 and perplexity is 58.889921885759385
At time: 624.881504535675 and batch: 450, loss is 4.031969814300537 and perplexity is 56.371844017272394
At time: 626.0650527477264 and batch: 500, loss is 3.994918775558472 and perplexity is 54.32142821694773
At time: 627.2446937561035 and batch: 550, loss is 3.99817232131958 and perplexity is 54.49845329289395
At time: 628.4095370769501 and batch: 600, loss is 3.9567191123962404 and perplexity is 52.28550138134327
At time: 629.5877776145935 and batch: 650, loss is 3.92857102394104 and perplexity is 50.834284767456005
At time: 630.756062746048 and batch: 700, loss is 3.9251053714752198 and perplexity is 50.658415729612294
At time: 631.9253962039948 and batch: 750, loss is 3.94703173160553 and perplexity is 51.78143729201669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.549855076989462 and perplexity of 94.61869489513565
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 635.3589887619019 and batch: 50, loss is 4.129659051895142 and perplexity is 62.156727103316776
At time: 636.5533132553101 and batch: 100, loss is 4.126268625259399 and perplexity is 61.94634612232322
At time: 637.7213268280029 and batch: 150, loss is 4.096773290634156 and perplexity is 60.14590080972524
At time: 638.885223865509 and batch: 200, loss is 4.0934672451019285 and perplexity is 59.94738405659845
At time: 640.0496144294739 and batch: 250, loss is 4.069496383666992 and perplexity is 58.52747977375729
At time: 641.2168483734131 and batch: 300, loss is 4.119838681221008 and perplexity is 61.5493124035888
At time: 642.3876178264618 and batch: 350, loss is 4.065804891586303 and perplexity is 58.3118243357373
At time: 643.5499668121338 and batch: 400, loss is 4.075466852188111 and perplexity is 58.8779614782391
At time: 644.7140791416168 and batch: 450, loss is 4.031496071815491 and perplexity is 56.34514460462341
At time: 645.8805322647095 and batch: 500, loss is 3.9938389587402345 and perplexity is 54.26280268328348
At time: 647.0428910255432 and batch: 550, loss is 3.9964705181121825 and perplexity is 54.40578652294966
At time: 648.2085847854614 and batch: 600, loss is 3.954638786315918 and perplexity is 52.1768435502439
At time: 649.3765952587128 and batch: 650, loss is 3.926136236190796 and perplexity is 50.710664629081386
At time: 650.5410716533661 and batch: 700, loss is 3.922599983215332 and perplexity is 50.53165558757047
At time: 651.7072689533234 and batch: 750, loss is 3.9446239614486696 and perplexity is 51.656909469973456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550122638081396 and perplexity of 94.64401456358716
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 655.1978228092194 and batch: 50, loss is 4.129569005966187 and perplexity is 62.15113039506814
At time: 656.4458029270172 and batch: 100, loss is 4.1262438774108885 and perplexity is 61.944813102503126
At time: 657.6079437732697 and batch: 150, loss is 4.096772975921631 and perplexity is 60.14588188105994
At time: 658.7738289833069 and batch: 200, loss is 4.093466486930847 and perplexity is 59.947338606242674
At time: 659.9393496513367 and batch: 250, loss is 4.0694433784484865 and perplexity is 58.52437759411987
At time: 661.1005685329437 and batch: 300, loss is 4.119785542488098 and perplexity is 61.54604183801382
At time: 662.2614786624908 and batch: 350, loss is 4.065707087516785 and perplexity is 58.30612148090196
At time: 663.4234805107117 and batch: 400, loss is 4.075384230613708 and perplexity is 58.87309708931866
At time: 664.5815043449402 and batch: 450, loss is 4.031417708396912 and perplexity is 56.34072937947018
At time: 665.7456018924713 and batch: 500, loss is 3.993657684326172 and perplexity is 54.252967117016865
At time: 666.965256690979 and batch: 550, loss is 3.9962100696563723 and perplexity is 54.391618464967124
At time: 668.1288340091705 and batch: 600, loss is 3.9543221282958982 and perplexity is 52.16032394994441
At time: 669.2866172790527 and batch: 650, loss is 3.9257709980010986 and perplexity is 50.692146539696495
At time: 670.4506692886353 and batch: 700, loss is 3.922218599319458 and perplexity is 50.51238730243789
At time: 671.6177859306335 and batch: 750, loss is 3.9442532682418823 and perplexity is 51.63776415328797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550150316815044 and perplexity of 94.64663422631197
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 675.0240569114685 and batch: 50, loss is 4.1295555973052975 and perplexity is 62.15029703722391
At time: 676.2248828411102 and batch: 100, loss is 4.126238641738891 and perplexity is 61.94448878062881
At time: 677.393317937851 and batch: 150, loss is 4.0967721319198604 and perplexity is 60.145831117850555
At time: 678.5577492713928 and batch: 200, loss is 4.093462743759155 and perplexity is 59.94711421348178
At time: 679.7239012718201 and batch: 250, loss is 4.069433550834656 and perplexity is 58.523802441963404
At time: 680.8885881900787 and batch: 300, loss is 4.11977602481842 and perplexity is 61.545456065905206
At time: 682.0513548851013 and batch: 350, loss is 4.065691556930542 and perplexity is 58.30521595968547
At time: 683.2171387672424 and batch: 400, loss is 4.0753703737258915 and perplexity is 58.87228129706905
At time: 684.3832449913025 and batch: 450, loss is 4.03140468120575 and perplexity is 56.33999542279905
At time: 685.5496973991394 and batch: 500, loss is 3.993630051612854 and perplexity is 54.251467981042566
At time: 686.7162806987762 and batch: 550, loss is 3.9961701583862306 and perplexity is 54.389447669709014
At time: 687.8828318119049 and batch: 600, loss is 3.954273066520691 and perplexity is 52.157764934631466
At time: 689.0484819412231 and batch: 650, loss is 3.9257143783569335 and perplexity is 50.68927644964996
At time: 690.2122731208801 and batch: 700, loss is 3.922159414291382 and perplexity is 50.50939781384455
At time: 691.3811025619507 and batch: 750, loss is 3.944195809364319 and perplexity is 51.63479719055982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550155284792878 and perplexity of 94.64710442986089
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 694.7948000431061 and batch: 50, loss is 4.129553937911988 and perplexity is 62.150193905522364
At time: 695.9914047718048 and batch: 100, loss is 4.126238431930542 and perplexity is 61.94447578415923
At time: 697.1565482616425 and batch: 150, loss is 4.096772561073303 and perplexity is 60.14585692964657
At time: 698.320200920105 and batch: 200, loss is 4.093462996482849 and perplexity is 59.947129363539815
At time: 699.4839780330658 and batch: 250, loss is 4.069432721138001 and perplexity is 58.5237538849804
At time: 700.6431157588959 and batch: 300, loss is 4.119775218963623 and perplexity is 61.54540646922418
At time: 701.805438041687 and batch: 350, loss is 4.065689620971679 and perplexity is 58.30510308329516
At time: 702.9739856719971 and batch: 400, loss is 4.075368666648865 and perplexity is 58.87218079763594
At time: 704.1501820087433 and batch: 450, loss is 4.0314030265808105 and perplexity is 56.339902201314665
At time: 705.3446245193481 and batch: 500, loss is 3.9936261129379274 and perplexity is 54.2512543025667
At time: 706.5305399894714 and batch: 550, loss is 3.9961642694473265 and perplexity is 54.389127374517756
At time: 707.7078986167908 and batch: 600, loss is 3.9542655515670777 and perplexity is 52.15737297292021
At time: 708.8892741203308 and batch: 650, loss is 3.9257056665420533 and perplexity is 50.688834855980666
At time: 710.0699369907379 and batch: 700, loss is 3.9221502542495728 and perplexity is 50.50893514776785
At time: 711.24107837677 and batch: 750, loss is 3.944187035560608 and perplexity is 51.63434415897204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.550155994503997 and perplexity of 94.64717160198705
Annealing...
Model not improving. Stopping early with 94.61852701543499loss at 35 epochs.
Finished Training.
Improved accuracyfrom -98.42915159806145 to -94.61852701543499
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fccc82d30>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.972988016755215, 'wordvec_dim': 200, 'lr': 18.104274377656584, 'dropout': 0.8044130915304769, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.77294921875 and batch: 50, loss is 7.414853153228759 and perplexity is 1660.4653165222849
At time: 2.938321113586426 and batch: 100, loss is 6.788601808547973 and perplexity is 887.6715589681243
At time: 4.103111267089844 and batch: 150, loss is 6.659620866775513 and perplexity is 780.255060425023
At time: 5.269378185272217 and batch: 200, loss is 6.634229068756103 and perplexity is 760.6923981748749
At time: 6.433366298675537 and batch: 250, loss is 6.641417951583862 and perplexity is 766.1806301857677
At time: 7.604029178619385 and batch: 300, loss is 6.5884756755828855 and perplexity is 726.6723408421319
At time: 8.773813486099243 and batch: 350, loss is 6.496354446411133 and perplexity is 662.7212380972543
At time: 10.00068712234497 and batch: 400, loss is 6.506846780776978 and perplexity is 669.7113380238691
At time: 11.170966386795044 and batch: 450, loss is 6.463549365997315 and perplexity is 641.3333483660409
At time: 12.34203839302063 and batch: 500, loss is 6.450802307128907 and perplexity is 633.2101180609204
At time: 13.513068199157715 and batch: 550, loss is 6.465926418304443 and perplexity is 642.8596446057328
At time: 14.684947729110718 and batch: 600, loss is 6.4509299659729 and perplexity is 633.2909580924598
At time: 15.854001522064209 and batch: 650, loss is 6.431857500076294 and perplexity is 621.3269917263258
At time: 17.022181034088135 and batch: 700, loss is 6.398264083862305 and perplexity is 600.8011916353396
At time: 18.19263982772827 and batch: 750, loss is 6.362708864212036 and perplexity is 579.8148707413534
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.731720325558684 and perplexity of 308.4995317631404
Finished 1 epochs...
Completing Train Step...
At time: 21.588207960128784 and batch: 50, loss is 6.069932079315185 and perplexity is 432.6512946042804
At time: 22.751883506774902 and batch: 100, loss is 5.958395948410034 and perplexity is 386.9888759920362
At time: 23.919223070144653 and batch: 150, loss is 5.869705486297607 and perplexity is 354.1446644509906
At time: 25.110786199569702 and batch: 200, loss is 5.8295480823516845 and perplexity is 340.20489972833565
At time: 26.276408672332764 and batch: 250, loss is 5.8495695018768314 and perplexity is 347.0849289012588
At time: 27.439289093017578 and batch: 300, loss is 5.8299342632293705 and perplexity is 340.3363057267146
At time: 28.60358452796936 and batch: 350, loss is 5.758983478546143 and perplexity is 317.02590143814916
At time: 29.769888401031494 and batch: 400, loss is 5.77461669921875 and perplexity is 322.02098016659824
At time: 30.935248613357544 and batch: 450, loss is 5.713587560653687 and perplexity is 302.95599400462777
At time: 32.10320854187012 and batch: 500, loss is 5.725213232040406 and perplexity is 306.49861360069434
At time: 33.267956018447876 and batch: 550, loss is 5.740706529617309 and perplexity is 311.28426485813424
At time: 34.43783450126648 and batch: 600, loss is 5.7099283409118655 and perplexity is 301.8494372523421
At time: 35.60133981704712 and batch: 650, loss is 5.668835296630859 and perplexity is 289.6969268368544
At time: 36.765698194503784 and batch: 700, loss is 5.661076574325562 and perplexity is 287.45794587638295
At time: 37.93200969696045 and batch: 750, loss is 5.64011215209961 and perplexity is 281.4942868803173
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.334844544876454 and perplexity of 207.44049861293476
Finished 2 epochs...
Completing Train Step...
At time: 41.36257886886597 and batch: 50, loss is 5.61990478515625 and perplexity is 275.8631157208139
At time: 42.57080340385437 and batch: 100, loss is 5.643272647857666 and perplexity is 282.385355747887
At time: 43.73796319961548 and batch: 150, loss is 5.631480703353882 and perplexity is 279.07503919694767
At time: 44.90342664718628 and batch: 200, loss is 5.640595693588256 and perplexity is 281.6304339605692
At time: 46.06904649734497 and batch: 250, loss is 5.681320362091064 and perplexity is 293.3364847144251
At time: 47.23972034454346 and batch: 300, loss is 5.660213766098022 and perplexity is 287.2100317621576
At time: 48.46409368515015 and batch: 350, loss is 5.610290784835815 and perplexity is 273.2236757562458
At time: 49.628910064697266 and batch: 400, loss is 5.636308727264404 and perplexity is 280.4256779936289
At time: 50.792545557022095 and batch: 450, loss is 5.592907447814941 and perplexity is 268.51517977210835
At time: 51.95526146888733 and batch: 500, loss is 5.548515300750733 and perplexity is 256.85591887784716
At time: 53.11722946166992 and batch: 550, loss is 5.5760540103912355 and perplexity is 264.0276969654395
At time: 54.28035616874695 and batch: 600, loss is 5.559917011260986 and perplexity is 259.80127484808963
At time: 55.444661140441895 and batch: 650, loss is 5.536530294418335 and perplexity is 253.79587302679067
At time: 56.606696128845215 and batch: 700, loss is 5.562409248352051 and perplexity is 260.44956873705047
At time: 57.77008390426636 and batch: 750, loss is 5.566436128616333 and perplexity is 261.5004824970646
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.433622848155887 and perplexity of 228.97729453948025
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 61.138420820236206 and batch: 50, loss is 5.544563150405883 and perplexity is 255.84278900897195
At time: 62.331465005874634 and batch: 100, loss is 5.489675846099853 and perplexity is 242.1786909658163
At time: 63.49167013168335 and batch: 150, loss is 5.471870231628418 and perplexity is 237.90471387331138
At time: 64.65466976165771 and batch: 200, loss is 5.439777021408081 and perplexity is 230.39080551535912
At time: 65.81327700614929 and batch: 250, loss is 5.484890298843384 and perplexity is 241.02250210044957
At time: 66.97552227973938 and batch: 300, loss is 5.461137466430664 and perplexity is 235.36499193910723
At time: 68.13244605064392 and batch: 350, loss is 5.394327116012573 and perplexity is 220.15395917155033
At time: 69.29432964324951 and batch: 400, loss is 5.412526931762695 and perplexity is 224.19740403117254
At time: 70.4564688205719 and batch: 450, loss is 5.401628007888794 and perplexity is 221.76716116454097
At time: 71.61946892738342 and batch: 500, loss is 5.359404048919678 and perplexity is 212.59821055584595
At time: 72.77724647521973 and batch: 550, loss is 5.384481964111328 and perplexity is 217.99714450694967
At time: 73.9369547367096 and batch: 600, loss is 5.320119619369507 and perplexity is 204.40833172637107
At time: 75.09785437583923 and batch: 650, loss is 5.3039388179779055 and perplexity is 201.12745628731903
At time: 76.26136231422424 and batch: 700, loss is 5.274597711563111 and perplexity is 195.3118889916569
At time: 77.47956848144531 and batch: 750, loss is 5.269590940475464 and perplexity is 194.3364510077968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.215413293173147 and perplexity of 184.08788752684538
Finished 4 epochs...
Completing Train Step...
At time: 80.81322574615479 and batch: 50, loss is 5.375505952835083 and perplexity is 216.04915534491644
At time: 81.99944829940796 and batch: 100, loss is 5.360977573394775 and perplexity is 212.93300237600096
At time: 83.15323853492737 and batch: 150, loss is 5.351140508651733 and perplexity is 210.84863548003008
At time: 84.30694365501404 and batch: 200, loss is 5.339888305664062 and perplexity is 208.4894219010007
At time: 85.46048760414124 and batch: 250, loss is 5.371195755004883 and perplexity is 215.11994472369122
At time: 86.61507201194763 and batch: 300, loss is 5.365448532104492 and perplexity is 213.88714842194148
At time: 87.77166867256165 and batch: 350, loss is 5.303873424530029 and perplexity is 201.11430429952142
At time: 88.92616438865662 and batch: 400, loss is 5.327054491043091 and perplexity is 205.8308039060751
At time: 90.08365559577942 and batch: 450, loss is 5.31132381439209 and perplexity is 202.61827991882112
At time: 91.24083518981934 and batch: 500, loss is 5.275189790725708 and perplexity is 195.42756333214052
At time: 92.39950966835022 and batch: 550, loss is 5.303118953704834 and perplexity is 200.96262664977365
At time: 93.55677509307861 and batch: 600, loss is 5.256442756652832 and perplexity is 191.7980041786142
At time: 94.71392607688904 and batch: 650, loss is 5.243606452941894 and perplexity is 189.35176069697704
At time: 95.86795568466187 and batch: 700, loss is 5.218898096084595 and perplexity is 184.73051660076257
At time: 97.02881979942322 and batch: 750, loss is 5.22378984451294 and perplexity is 185.63638565036857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1979284951853195 and perplexity of 180.89712421649682
Finished 5 epochs...
Completing Train Step...
At time: 100.43624758720398 and batch: 50, loss is 5.307897033691407 and perplexity is 201.92513980545658
At time: 101.65794968605042 and batch: 100, loss is 5.293795213699341 and perplexity is 199.09761133780069
At time: 102.82118272781372 and batch: 150, loss is 5.2777699375152585 and perplexity is 195.93244618811121
At time: 103.98570990562439 and batch: 200, loss is 5.275959205627442 and perplexity is 195.57798607290604
At time: 105.14878177642822 and batch: 250, loss is 5.307764854431152 and perplexity is 201.8984512537257
At time: 106.37166047096252 and batch: 300, loss is 5.314612512588501 and perplexity is 203.28572720532722
At time: 107.54169988632202 and batch: 350, loss is 5.259430027008056 and perplexity is 192.37181330547298
At time: 108.70622634887695 and batch: 400, loss is 5.273539018630982 and perplexity is 195.10522309238723
At time: 109.87433171272278 and batch: 450, loss is 5.2549213027954105 and perplexity is 191.5064142418762
At time: 111.03739857673645 and batch: 500, loss is 5.220531044006347 and perplexity is 185.03241834175475
At time: 112.20791864395142 and batch: 550, loss is 5.247795171737671 and perplexity is 190.14656542066987
At time: 113.39589858055115 and batch: 600, loss is 5.204973344802856 and perplexity is 182.17601676642045
At time: 114.56046485900879 and batch: 650, loss is 5.1941399574279785 and perplexity is 180.2130852031914
At time: 115.72437834739685 and batch: 700, loss is 5.175204057693481 and perplexity is 176.83269466687227
At time: 116.88695216178894 and batch: 750, loss is 5.1798481750488286 and perplexity is 177.6558363570246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1902820675872094 and perplexity of 179.5191823336588
Finished 6 epochs...
Completing Train Step...
At time: 120.27585005760193 and batch: 50, loss is 5.259857931137085 and perplexity is 192.4541476130339
At time: 121.47508788108826 and batch: 100, loss is 5.251380195617676 and perplexity is 190.8294687794164
At time: 122.63612461090088 and batch: 150, loss is 5.230532083511353 and perplexity is 186.89221933737358
At time: 123.80160927772522 and batch: 200, loss is 5.229398746490478 and perplexity is 186.68052744807173
At time: 124.96714615821838 and batch: 250, loss is 5.260281038284302 and perplexity is 192.5355935673682
At time: 126.13202047348022 and batch: 300, loss is 5.270983448028565 and perplexity is 194.6072544878546
At time: 127.29657411575317 and batch: 350, loss is 5.212449674606323 and perplexity is 183.543128872594
At time: 128.45904779434204 and batch: 400, loss is 5.226001596450805 and perplexity is 186.04742167336894
At time: 129.62844276428223 and batch: 450, loss is 5.211731510162354 and perplexity is 183.41136204429162
At time: 130.7968578338623 and batch: 500, loss is 5.177522745132446 and perplexity is 177.24319013622636
At time: 131.9626100063324 and batch: 550, loss is 5.202308807373047 and perplexity is 181.6912480798995
At time: 133.1276078224182 and batch: 600, loss is 5.159045057296753 and perplexity is 173.99821791543297
At time: 134.2956097126007 and batch: 650, loss is 5.140838193893432 and perplexity is 170.85892122240494
At time: 135.4605209827423 and batch: 700, loss is 5.1285568046569825 and perplexity is 168.77336925809848
At time: 136.68034052848816 and batch: 750, loss is 5.136857900619507 and perplexity is 170.1802042498345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.179308514262354 and perplexity of 177.55998833369043
Finished 7 epochs...
Completing Train Step...
At time: 140.04686737060547 and batch: 50, loss is 5.211596755981446 and perplexity is 183.38664826161028
At time: 141.26735019683838 and batch: 100, loss is 5.20602409362793 and perplexity is 182.36753860504658
At time: 142.43279790878296 and batch: 150, loss is 5.185872688293457 and perplexity is 178.72935676818219
At time: 143.59628701210022 and batch: 200, loss is 5.1902374649047855 and perplexity is 179.51117547514517
At time: 144.76237607002258 and batch: 250, loss is 5.216595783233642 and perplexity is 184.30569837830683
At time: 145.92866945266724 and batch: 300, loss is 5.231009101867675 and perplexity is 186.98139162336915
At time: 147.09638452529907 and batch: 350, loss is 5.169853200912476 and perplexity is 175.88901524182785
At time: 148.26498436927795 and batch: 400, loss is 5.191636428833008 and perplexity is 179.76248087695797
At time: 149.4262261390686 and batch: 450, loss is 5.175792989730835 and perplexity is 176.9368677784436
At time: 150.5867223739624 and batch: 500, loss is 5.147003555297852 and perplexity is 171.9155822222637
At time: 151.74905157089233 and batch: 550, loss is 5.178740272521972 and perplexity is 177.4591199984109
At time: 152.91218161582947 and batch: 600, loss is 5.141788311004639 and perplexity is 171.0213343505889
At time: 154.08048367500305 and batch: 650, loss is 5.120270299911499 and perplexity is 167.38060647018602
At time: 155.25000739097595 and batch: 700, loss is 5.1028729343414305 and perplexity is 164.49380901687488
At time: 156.41931009292603 and batch: 750, loss is 5.103527812957764 and perplexity is 164.60156777552146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.174555224041606 and perplexity of 176.71799687773725
Finished 8 epochs...
Completing Train Step...
At time: 159.79290294647217 and batch: 50, loss is 5.171323909759521 and perplexity is 176.1478870885384
At time: 161.00047135353088 and batch: 100, loss is 5.169051275253296 and perplexity is 175.74802186799562
At time: 162.16818022727966 and batch: 150, loss is 5.153029613494873 and perplexity is 172.95468322282377
At time: 163.33261823654175 and batch: 200, loss is 5.158589782714844 and perplexity is 173.91901897951786
At time: 164.49920082092285 and batch: 250, loss is 5.187323541641235 and perplexity is 178.98885505525507
At time: 165.69705963134766 and batch: 300, loss is 5.20339750289917 and perplexity is 181.88916224340372
At time: 166.8631796836853 and batch: 350, loss is 5.139959564208985 and perplexity is 170.7088654337548
At time: 168.03031063079834 and batch: 400, loss is 5.156900968551636 and perplexity is 173.6255499540873
At time: 169.19529056549072 and batch: 450, loss is 5.139570932388306 and perplexity is 170.64253542638593
At time: 170.36127591133118 and batch: 500, loss is 5.10793607711792 and perplexity is 165.32877665422896
At time: 171.52674293518066 and batch: 550, loss is 5.142410869598389 and perplexity is 171.12783830103834
At time: 172.6961109638214 and batch: 600, loss is 5.115946607589722 and perplexity is 166.65846650767438
At time: 173.85719418525696 and batch: 650, loss is 5.094388437271118 and perplexity is 163.10406574943545
At time: 175.02264142036438 and batch: 700, loss is 5.075653839111328 and perplexity is 160.07682225718392
At time: 176.18632245063782 and batch: 750, loss is 5.074642734527588 and perplexity is 159.91504964674522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.168193018713663 and perplexity of 175.5972496887592
Finished 9 epochs...
Completing Train Step...
At time: 179.55117559432983 and batch: 50, loss is 5.13222186088562 and perplexity is 169.39306806813553
At time: 180.74616646766663 and batch: 100, loss is 5.1319355869293215 and perplexity is 169.34458218482408
At time: 181.9099884033203 and batch: 150, loss is 5.117581844329834 and perplexity is 166.93121549896665
At time: 183.07889461517334 and batch: 200, loss is 5.129418001174927 and perplexity is 168.91877890017702
At time: 184.2458336353302 and batch: 250, loss is 5.15556492805481 and perplexity is 173.3937340802453
At time: 185.4146227836609 and batch: 300, loss is 5.1746673583984375 and perplexity is 176.73781414773526
At time: 186.57649087905884 and batch: 350, loss is 5.110797653198242 and perplexity is 165.80255508009793
At time: 187.7390763759613 and batch: 400, loss is 5.130234460830689 and perplexity is 169.0567505847422
At time: 188.8965208530426 and batch: 450, loss is 5.120774211883545 and perplexity is 167.46497281649548
At time: 190.060129404068 and batch: 500, loss is 5.090450630187989 and perplexity is 162.46305631808778
At time: 191.22078943252563 and batch: 550, loss is 5.119607915878296 and perplexity is 167.26977294022504
At time: 192.39687323570251 and batch: 600, loss is 5.090697317123413 and perplexity is 162.50313877527662
At time: 193.58501434326172 and batch: 650, loss is 5.067872104644775 and perplexity is 158.83598114179512
At time: 194.7656500339508 and batch: 700, loss is 5.050341100692749 and perplexity is 156.0756929347299
At time: 196.01668906211853 and batch: 750, loss is 5.057067461013794 and perplexity is 157.1290529507649
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.172876402389171 and perplexity of 176.42156777362553
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 199.478910446167 and batch: 50, loss is 5.109584636688233 and perplexity is 165.6015557757674
At time: 200.68560409545898 and batch: 100, loss is 5.095378618240357 and perplexity is 163.26564827611534
At time: 201.85068583488464 and batch: 150, loss is 5.063560495376587 and perplexity is 158.15261671223
At time: 203.0126371383667 and batch: 200, loss is 5.064631547927856 and perplexity is 158.32209722091352
At time: 204.18314290046692 and batch: 250, loss is 5.084205980300903 and perplexity is 161.4516925048205
At time: 205.34515142440796 and batch: 300, loss is 5.096984510421753 and perplexity is 163.52804593895885
At time: 206.50436878204346 and batch: 350, loss is 5.023080797195434 and perplexity is 151.8784905985191
At time: 207.6700723171234 and batch: 400, loss is 5.031654014587402 and perplexity is 153.18617543922795
At time: 208.8318693637848 and batch: 450, loss is 5.014449501037598 and perplexity is 150.57322354286322
At time: 210.00033569335938 and batch: 500, loss is 4.9730942440032955 and perplexity is 144.4732318967953
At time: 211.16386461257935 and batch: 550, loss is 4.997066526412964 and perplexity is 147.97843096388277
At time: 212.32386922836304 and batch: 600, loss is 4.954767036437988 and perplexity is 141.84955674541922
At time: 213.4895143508911 and batch: 650, loss is 4.922508001327515 and perplexity is 137.34664716071254
At time: 214.6528217792511 and batch: 700, loss is 4.904428663253785 and perplexity is 134.88582276043275
At time: 215.816792011261 and batch: 750, loss is 4.920271549224854 and perplexity is 137.03982119155654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.093243975972021 and perplexity of 162.91750623389535
Finished 11 epochs...
Completing Train Step...
At time: 219.20164036750793 and batch: 50, loss is 5.048661003112793 and perplexity is 155.8136906970331
At time: 220.4164514541626 and batch: 100, loss is 5.040564756393433 and perplexity is 154.55727759319745
At time: 221.57656502723694 and batch: 150, loss is 5.017148027420044 and perplexity is 150.980098093031
At time: 222.73367047309875 and batch: 200, loss is 5.022593908309936 and perplexity is 151.80456064879638
At time: 223.89557433128357 and batch: 250, loss is 5.045427417755127 and perplexity is 155.31066755071538
At time: 225.1149125099182 and batch: 300, loss is 5.061758880615234 and perplexity is 157.8679431364727
At time: 226.28015851974487 and batch: 350, loss is 4.995021018981934 and perplexity is 147.67604935106488
At time: 227.43971228599548 and batch: 400, loss is 5.005490837097168 and perplexity is 149.23031295858837
At time: 228.60271739959717 and batch: 450, loss is 4.9911537361145015 and perplexity is 147.1060471856904
At time: 229.76047492027283 and batch: 500, loss is 4.953089160919189 and perplexity is 141.61175040731078
At time: 230.9191701412201 and batch: 550, loss is 4.983412494659424 and perplexity is 145.9716602017085
At time: 232.07852959632874 and batch: 600, loss is 4.947286148071289 and perplexity is 140.7923553796431
At time: 233.24006748199463 and batch: 650, loss is 4.923334617614746 and perplexity is 137.4602270732876
At time: 234.3962004184723 and batch: 700, loss is 4.907624530792236 and perplexity is 135.31758955003374
At time: 235.55151891708374 and batch: 750, loss is 4.922680416107178 and perplexity is 137.37032979417958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.090898025867551 and perplexity of 162.53575784953583
Finished 12 epochs...
Completing Train Step...
At time: 239.00622510910034 and batch: 50, loss is 5.029049634933472 and perplexity is 152.78773954502773
At time: 240.20642924308777 and batch: 100, loss is 5.020240983963013 and perplexity is 151.44779588691395
At time: 241.37249898910522 and batch: 150, loss is 4.99809947013855 and perplexity is 148.13136332733373
At time: 242.53550577163696 and batch: 200, loss is 5.005555753707886 and perplexity is 149.24000079916954
At time: 243.70332860946655 and batch: 250, loss is 5.029513311386109 and perplexity is 152.8586000490104
At time: 244.87397146224976 and batch: 300, loss is 5.047820405960083 and perplexity is 155.6827691861138
At time: 246.04626560211182 and batch: 350, loss is 4.982650480270386 and perplexity is 145.86047006586506
At time: 247.2152805328369 and batch: 400, loss is 4.99398904800415 and perplexity is 147.52373056182358
At time: 248.3835060596466 and batch: 450, loss is 4.981121587753296 and perplexity is 145.63763547311936
At time: 249.5507297515869 and batch: 500, loss is 4.945393495559692 and perplexity is 140.52613638411484
At time: 250.71128821372986 and batch: 550, loss is 4.977383632659912 and perplexity is 145.09426471224432
At time: 251.8725700378418 and batch: 600, loss is 4.943367872238159 and perplexity is 140.24177147030352
At time: 253.03878736495972 and batch: 650, loss is 4.920969285964966 and perplexity is 137.13547227541747
At time: 254.20476269721985 and batch: 700, loss is 4.906055326461792 and perplexity is 135.10541511858312
At time: 255.42755126953125 and batch: 750, loss is 4.9187386608123775 and perplexity is 136.82991535981492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.089238011559774 and perplexity of 162.26616998774162
Finished 13 epochs...
Completing Train Step...
At time: 258.8019139766693 and batch: 50, loss is 5.015254850387573 and perplexity is 150.6945364336392
At time: 259.99763655662537 and batch: 100, loss is 5.006716079711914 and perplexity is 149.41326835692456
At time: 261.16161823272705 and batch: 150, loss is 4.985582799911499 and perplexity is 146.28880729100896
At time: 262.3284742832184 and batch: 200, loss is 4.9937386798858645 and perplexity is 147.48679994631763
At time: 263.4915449619293 and batch: 250, loss is 5.017967929840088 and perplexity is 151.10393780213712
At time: 264.6571321487427 and batch: 300, loss is 5.037388687133789 and perplexity is 154.06717169211439
At time: 265.8218972682953 and batch: 350, loss is 4.9739478397369385 and perplexity is 144.5966062796003
At time: 266.98882484436035 and batch: 400, loss is 4.985021123886108 and perplexity is 146.20666344644425
At time: 268.15851879119873 and batch: 450, loss is 4.972214269638061 and perplexity is 144.3461550766359
At time: 269.3217234611511 and batch: 500, loss is 4.938310823440552 and perplexity is 139.53435222295488
At time: 270.4851806163788 and batch: 550, loss is 4.970771102905274 and perplexity is 144.13798975238828
At time: 271.6603171825409 and batch: 600, loss is 4.938128643035888 and perplexity is 139.5089341136135
At time: 272.8408386707306 and batch: 650, loss is 4.916610164642334 and perplexity is 136.5389831429178
At time: 274.0304479598999 and batch: 700, loss is 4.902069149017334 and perplexity is 134.56793292161677
At time: 275.1968722343445 and batch: 750, loss is 4.913169918060302 and perplexity is 136.07006243675914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.089214236237282 and perplexity of 162.262312103082
Finished 14 epochs...
Completing Train Step...
At time: 278.53250670433044 and batch: 50, loss is 5.003860940933228 and perplexity is 148.98728115605738
At time: 279.729745388031 and batch: 100, loss is 4.995790300369262 and perplexity is 147.7896974952812
At time: 280.88995814323425 and batch: 150, loss is 4.975134792327881 and perplexity is 144.76833749430847
At time: 282.0537259578705 and batch: 200, loss is 4.983561964035034 and perplexity is 145.99348012528014
At time: 283.21781396865845 and batch: 250, loss is 5.008072137832642 and perplexity is 149.6160188724962
At time: 284.44534397125244 and batch: 300, loss is 5.0283393287658695 and perplexity is 152.67925200554043
At time: 285.6100172996521 and batch: 350, loss is 4.965829849243164 and perplexity is 143.4275241236794
At time: 286.77540016174316 and batch: 400, loss is 4.976584005355835 and perplexity is 144.9782897510678
At time: 287.94820976257324 and batch: 450, loss is 4.964311532974243 and perplexity is 143.2099210173627
At time: 289.12231373786926 and batch: 500, loss is 4.932064800262451 and perplexity is 138.66553358010538
At time: 290.2922954559326 and batch: 550, loss is 4.964097871780395 and perplexity is 143.17932588328011
At time: 291.45988965034485 and batch: 600, loss is 4.932010192871093 and perplexity is 138.65796162378956
At time: 292.62890625 and batch: 650, loss is 4.911600542068482 and perplexity is 135.85668482620966
At time: 293.79415249824524 and batch: 700, loss is 4.8971921253204345 and perplexity is 133.91323969877197
At time: 294.9601860046387 and batch: 750, loss is 4.907441682815552 and perplexity is 135.29284926449924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.088573367096657 and perplexity of 162.15835650909276
Finished 15 epochs...
Completing Train Step...
At time: 298.36390471458435 and batch: 50, loss is 4.993666410446167 and perplexity is 147.47614154306578
At time: 299.58632230758667 and batch: 100, loss is 4.986013317108155 and perplexity is 146.35180069712536
At time: 300.7540383338928 and batch: 150, loss is 4.965963096618652 and perplexity is 143.44663673816623
At time: 301.91907811164856 and batch: 200, loss is 4.974877653121948 and perplexity is 144.73111666463325
At time: 303.0820348262787 and batch: 250, loss is 4.999863910675049 and perplexity is 148.39296303020654
At time: 304.24908542633057 and batch: 300, loss is 5.0202897357940675 and perplexity is 151.45517942425167
At time: 305.41694355010986 and batch: 350, loss is 4.958547077178955 and perplexity is 142.3867685505883
At time: 306.58522844314575 and batch: 400, loss is 4.969346675872803 and perplexity is 143.93282186141437
At time: 307.75066781044006 and batch: 450, loss is 4.95723780632019 and perplexity is 142.2004676896084
At time: 308.9186270236969 and batch: 500, loss is 4.925900936126709 and perplexity is 137.81344684204163
At time: 310.0877776145935 and batch: 550, loss is 4.957754774093628 and perplexity is 142.2739997539878
At time: 311.25500226020813 and batch: 600, loss is 4.9265598487854 and perplexity is 137.90428379022475
At time: 312.42206478118896 and batch: 650, loss is 4.906296443939209 and perplexity is 135.13799532313243
At time: 313.58589935302734 and batch: 700, loss is 4.892279329299927 and perplexity is 133.2569646603487
At time: 314.8115019798279 and batch: 750, loss is 4.901124963760376 and perplexity is 134.44093582711972
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.088242286859557 and perplexity of 162.10467796841422
Finished 16 epochs...
Completing Train Step...
At time: 318.16690731048584 and batch: 50, loss is 4.984423570632934 and perplexity is 146.11932327686623
At time: 319.3614673614502 and batch: 100, loss is 4.977475986480713 and perplexity is 145.107665340757
At time: 320.5234613418579 and batch: 150, loss is 4.9573890972137455 and perplexity is 142.22198295291977
At time: 321.68593096733093 and batch: 200, loss is 4.967135820388794 and perplexity is 143.61495869710632
At time: 322.8494653701782 and batch: 250, loss is 4.992478733062744 and perplexity is 147.30109143726324
At time: 324.0150372982025 and batch: 300, loss is 5.013344926834106 and perplexity is 150.40699606659325
At time: 325.17899322509766 and batch: 350, loss is 4.951731185913086 and perplexity is 141.41957570344186
At time: 326.3418252468109 and batch: 400, loss is 4.962326145172119 and perplexity is 142.92587585023995
At time: 327.50617241859436 and batch: 450, loss is 4.950326385498047 and perplexity is 141.22104890271382
At time: 328.67132329940796 and batch: 500, loss is 4.9194660663604735 and perplexity is 136.92948240781448
At time: 329.83541083335876 and batch: 550, loss is 4.951783885955811 and perplexity is 141.42702871750885
At time: 331.0043478012085 and batch: 600, loss is 4.920657691955566 and perplexity is 137.092748340387
At time: 332.170325756073 and batch: 650, loss is 4.901006345748901 and perplexity is 134.42498965642136
At time: 333.33575534820557 and batch: 700, loss is 4.887000093460083 and perplexity is 132.55532341100044
At time: 334.5026435852051 and batch: 750, loss is 4.89546495437622 and perplexity is 133.68214826676987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.088057407113009 and perplexity of 162.07471086687767
Finished 17 epochs...
Completing Train Step...
At time: 337.8736848831177 and batch: 50, loss is 4.975924940109253 and perplexity is 144.88277107876013
At time: 339.0662171840668 and batch: 100, loss is 4.969271068572998 and perplexity is 143.92193990078312
At time: 340.22994232177734 and batch: 150, loss is 4.949454250335694 and perplexity is 141.09793875245012
At time: 341.39352107048035 and batch: 200, loss is 4.9596912288665775 and perplexity is 142.54977384573004
At time: 342.5561318397522 and batch: 250, loss is 4.9846111679077145 and perplexity is 146.146737435036
At time: 343.7486255168915 and batch: 300, loss is 5.005871829986572 and perplexity is 149.28717947889126
At time: 344.90849208831787 and batch: 350, loss is 4.945079822540283 and perplexity is 140.4820640391234
At time: 346.07664346694946 and batch: 400, loss is 4.955295305252076 and perplexity is 141.92451123885752
At time: 347.23866605758667 and batch: 450, loss is 4.943841495513916 and perplexity is 140.30820896943627
At time: 348.40024614334106 and batch: 500, loss is 4.913727550506592 and perplexity is 136.14596067823683
At time: 349.5638077259064 and batch: 550, loss is 4.945476436614991 and perplexity is 140.5377922535319
At time: 350.72576427459717 and batch: 600, loss is 4.915072917938232 and perplexity is 136.3292502879721
At time: 351.89236760139465 and batch: 650, loss is 4.895410766601563 and perplexity is 133.67490452490674
At time: 353.0556938648224 and batch: 700, loss is 4.8811751651763915 and perplexity is 131.78544258563056
At time: 354.2218577861786 and batch: 750, loss is 4.88942099571228 and perplexity is 132.8766156436134
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.087919723155887 and perplexity of 162.05239731548028
Finished 18 epochs...
Completing Train Step...
At time: 357.5817530155182 and batch: 50, loss is 4.967782402038575 and perplexity is 143.70784752090216
At time: 358.7659020423889 and batch: 100, loss is 4.961395349502563 and perplexity is 142.79290295882728
At time: 359.948312997818 and batch: 150, loss is 4.941980772018432 and perplexity is 140.0473769318044
At time: 361.1143274307251 and batch: 200, loss is 4.951890649795533 and perplexity is 141.4421288161931
At time: 362.2600073814392 and batch: 250, loss is 4.976600294113159 and perplexity is 144.98065128648003
At time: 363.41372990608215 and batch: 300, loss is 4.998997011184692 and perplexity is 148.2643769898225
At time: 364.5774595737457 and batch: 350, loss is 4.939126510620117 and perplexity is 139.64821503707773
At time: 365.73326563835144 and batch: 400, loss is 4.948146572113037 and perplexity is 140.91354863842605
At time: 366.8903794288635 and batch: 450, loss is 4.936983261108399 and perplexity is 139.34923457769668
At time: 368.04525876045227 and batch: 500, loss is 4.907775774002075 and perplexity is 135.33805696436463
At time: 369.20403957366943 and batch: 550, loss is 4.939194717407227 and perplexity is 139.65774031799174
At time: 370.36746764183044 and batch: 600, loss is 4.909211206436157 and perplexity is 135.53246509738577
At time: 371.52763652801514 and batch: 650, loss is 4.890067672729492 and perplexity is 132.96257168697838
At time: 372.68516397476196 and batch: 700, loss is 4.87588508605957 and perplexity is 131.0901279242196
At time: 373.904620885849 and batch: 750, loss is 4.883596963882447 and perplexity is 132.10498718012374
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.088609207508176 and perplexity of 162.16416843547154
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 377.29962730407715 and batch: 50, loss is 4.96262749671936 and perplexity is 142.96895327446646
At time: 378.5132899284363 and batch: 100, loss is 4.956449527740478 and perplexity is 142.08841827578215
At time: 379.68198895454407 and batch: 150, loss is 4.9338276576995845 and perplexity is 138.91019673702303
At time: 380.84378480911255 and batch: 200, loss is 4.940894145965576 and perplexity is 139.89528045435927
At time: 382.01857447624207 and batch: 250, loss is 4.961300401687622 and perplexity is 142.77934572832714
At time: 383.1863877773285 and batch: 300, loss is 4.974741458892822 and perplexity is 144.7114064640069
At time: 384.3547785282135 and batch: 350, loss is 4.915431108474731 and perplexity is 136.37809088187032
At time: 385.5193815231323 and batch: 400, loss is 4.9201904392242435 and perplexity is 137.02870634234486
At time: 386.68582367897034 and batch: 450, loss is 4.903783979415894 and perplexity is 134.7988920749533
At time: 387.8508598804474 and batch: 500, loss is 4.865825757980347 and perplexity is 129.77805964612435
At time: 389.02243518829346 and batch: 550, loss is 4.890722694396973 and perplexity is 133.04969358264705
At time: 390.18877601623535 and batch: 600, loss is 4.859505386352539 and perplexity is 128.96040075597338
At time: 391.3529636859894 and batch: 650, loss is 4.834543933868408 and perplexity is 125.78120557601633
At time: 392.52039980888367 and batch: 700, loss is 4.817545576095581 and perplexity is 123.66120098744344
At time: 393.6886284351349 and batch: 750, loss is 4.8382248687744145 and perplexity is 126.24505117513792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.065130899118823 and perplexity of 158.40117529093814
Finished 20 epochs...
Completing Train Step...
At time: 397.0656478404999 and batch: 50, loss is 4.945892391204834 and perplexity is 140.59626175275142
At time: 398.28926730155945 and batch: 100, loss is 4.940967988967896 and perplexity is 139.90561112329752
At time: 399.4535653591156 and batch: 150, loss is 4.919432830810547 and perplexity is 136.92493155679094
At time: 400.6201090812683 and batch: 200, loss is 4.926971559524536 and perplexity is 137.96107215422148
At time: 401.78699350357056 and batch: 250, loss is 4.94814167022705 and perplexity is 140.9128578979696
At time: 403.00683975219727 and batch: 300, loss is 4.963700323104859 and perplexity is 143.12241644479644
At time: 404.17831206321716 and batch: 350, loss is 4.906141233444214 and perplexity is 135.11702211565873
At time: 405.3456242084503 and batch: 400, loss is 4.912906579971313 and perplexity is 136.03423472415614
At time: 406.5131757259369 and batch: 450, loss is 4.898709688186646 and perplexity is 134.11661573756385
At time: 407.67857217788696 and batch: 500, loss is 4.862090139389038 and perplexity is 129.29416270369902
At time: 408.84505796432495 and batch: 550, loss is 4.889546890258789 and perplexity is 132.8933451379354
At time: 410.0108308792114 and batch: 600, loss is 4.8602476978302 and perplexity is 129.05616508069966
At time: 411.1792151927948 and batch: 650, loss is 4.837903804779053 and perplexity is 126.2045249407185
At time: 412.34407138824463 and batch: 700, loss is 4.822188196182251 and perplexity is 124.23664771977451
At time: 413.5129735469818 and batch: 750, loss is 4.841222677230835 and perplexity is 126.62407749743456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.063196226607921 and perplexity of 158.0950171447777
Finished 21 epochs...
Completing Train Step...
At time: 416.91985058784485 and batch: 50, loss is 4.939404735565185 and perplexity is 139.68707405955936
At time: 418.1417362689972 and batch: 100, loss is 4.9338148021698 and perplexity is 138.90841098432992
At time: 419.3087251186371 and batch: 150, loss is 4.912498102188111 and perplexity is 135.97867910890614
At time: 420.47430086135864 and batch: 200, loss is 4.920555686950683 and perplexity is 137.07876490712536
At time: 421.63966512680054 and batch: 250, loss is 4.941818189620972 and perplexity is 140.02460954434312
At time: 422.807288646698 and batch: 300, loss is 4.9581025695800784 and perplexity is 142.32349061479164
At time: 423.9740333557129 and batch: 350, loss is 4.901763706207276 and perplexity is 134.52683639068096
At time: 425.1393325328827 and batch: 400, loss is 4.909652996063232 and perplexity is 135.59235516302851
At time: 426.3085217475891 and batch: 450, loss is 4.896298303604126 and perplexity is 133.79359861389315
At time: 427.472861289978 and batch: 500, loss is 4.860464954376221 and perplexity is 129.08420642334013
At time: 428.640900850296 and batch: 550, loss is 4.8892457389831545 and perplexity is 132.85333016310662
At time: 429.8075006008148 and batch: 600, loss is 4.860690240859985 and perplexity is 129.11329062633064
At time: 430.97718620300293 and batch: 650, loss is 4.83946590423584 and perplexity is 126.40182302006389
At time: 432.1445827484131 and batch: 700, loss is 4.823968048095703 and perplexity is 124.45796745469026
At time: 433.3658437728882 and batch: 750, loss is 4.841876544952393 and perplexity is 126.70689996900167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062460256177325 and perplexity of 157.9787066927113
Finished 22 epochs...
Completing Train Step...
At time: 436.6819357872009 and batch: 50, loss is 4.93470175743103 and perplexity is 139.0316711853429
At time: 437.8980677127838 and batch: 100, loss is 4.928958568572998 and perplexity is 138.23547458269854
At time: 439.09789538383484 and batch: 150, loss is 4.907807502746582 and perplexity is 135.34235113912018
At time: 440.29096484184265 and batch: 200, loss is 4.916343660354614 and perplexity is 136.50259976684748
At time: 441.46966099739075 and batch: 250, loss is 4.937596836090088 and perplexity is 139.4347620178166
At time: 442.64903712272644 and batch: 300, loss is 4.954163990020752 and perplexity is 141.76404066611158
At time: 443.82988119125366 and batch: 350, loss is 4.89867564201355 and perplexity is 134.11204965777864
At time: 444.9979958534241 and batch: 400, loss is 4.907435369491577 and perplexity is 135.2919951196066
At time: 446.16569566726685 and batch: 450, loss is 4.89462233543396 and perplexity is 133.5695526006807
At time: 447.33364844322205 and batch: 500, loss is 4.859089260101318 and perplexity is 128.90674811176197
At time: 448.50437021255493 and batch: 550, loss is 4.888649492263794 and perplexity is 132.77414041150163
At time: 449.6718065738678 and batch: 600, loss is 4.860397396087646 and perplexity is 129.07548600984353
At time: 450.8392388820648 and batch: 650, loss is 4.839696407318115 and perplexity is 126.43096238810331
At time: 452.00375533103943 and batch: 700, loss is 4.82429196357727 and perplexity is 124.49828784700028
At time: 453.1715326309204 and batch: 750, loss is 4.841530961990356 and perplexity is 126.6631197884779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062136982762536 and perplexity of 157.92764463068303
Finished 23 epochs...
Completing Train Step...
At time: 456.57147574424744 and batch: 50, loss is 4.930868139266968 and perplexity is 138.49969718941213
At time: 457.77235674858093 and batch: 100, loss is 4.92520809173584 and perplexity is 137.7179966382982
At time: 458.9416160583496 and batch: 150, loss is 4.904128637313843 and perplexity is 134.84535958497884
At time: 460.1229944229126 and batch: 200, loss is 4.913085222244263 and perplexity is 136.05853835980994
At time: 461.28624534606934 and batch: 250, loss is 4.934534149169922 and perplexity is 139.00837028146307
At time: 462.5098330974579 and batch: 300, loss is 4.951249160766602 and perplexity is 141.351424338403
At time: 463.68347239494324 and batch: 350, loss is 4.8966787242889405 and perplexity is 133.84450614882738
At time: 464.85084319114685 and batch: 400, loss is 4.905515613555909 and perplexity is 135.0325166562789
At time: 466.01604080200195 and batch: 450, loss is 4.8930674648284915 and perplexity is 133.36203060629003
At time: 467.19423723220825 and batch: 500, loss is 4.85772219657898 and perplexity is 128.73064479826917
At time: 468.37139654159546 and batch: 550, loss is 4.887741117477417 and perplexity is 132.65358649242873
At time: 469.5359134674072 and batch: 600, loss is 4.859897842407227 and perplexity is 129.01102197871802
At time: 470.700541973114 and batch: 650, loss is 4.839659729003906 and perplexity is 126.42632519858178
At time: 471.8668637275696 and batch: 700, loss is 4.824161233901978 and perplexity is 124.48201329006257
At time: 473.035014629364 and batch: 750, loss is 4.8409383010864255 and perplexity is 126.58807375003249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.061929747115734 and perplexity of 157.89491978409455
Finished 24 epochs...
Completing Train Step...
At time: 476.40737986564636 and batch: 50, loss is 4.9277314281463624 and perplexity is 138.06594428345906
At time: 477.6174416542053 and batch: 100, loss is 4.921968402862549 and perplexity is 137.27255511251198
At time: 478.77950501441956 and batch: 150, loss is 4.900814752578736 and perplexity is 134.3992372135785
At time: 479.9414451122284 and batch: 200, loss is 4.910288467407226 and perplexity is 135.67854760271146
At time: 481.1023848056793 and batch: 250, loss is 4.931696987152099 and perplexity is 138.614539957537
At time: 482.2705674171448 and batch: 300, loss is 4.948467864990234 and perplexity is 140.9588304318734
At time: 483.4346797466278 and batch: 350, loss is 4.894562797546387 and perplexity is 133.5616003884061
At time: 484.59765005111694 and batch: 400, loss is 4.90376543045044 and perplexity is 134.79639171815052
At time: 485.7600667476654 and batch: 450, loss is 4.891639747619629 and perplexity is 133.1717631965319
At time: 486.91943407058716 and batch: 500, loss is 4.856491279602051 and perplexity is 128.57228554567718
At time: 488.0824372768402 and batch: 550, loss is 4.886748752593994 and perplexity is 132.52201102776667
At time: 489.25022983551025 and batch: 600, loss is 4.859042739868164 and perplexity is 128.90075147926808
At time: 490.41542983055115 and batch: 650, loss is 4.839058742523194 and perplexity is 126.35036751338843
At time: 491.5809850692749 and batch: 700, loss is 4.823656167984009 and perplexity is 124.41915754223372
At time: 492.7996265888214 and batch: 750, loss is 4.8401335334777835 and perplexity is 126.48624075008402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0617487707803415 and perplexity of 157.86634712570043
Finished 25 epochs...
Completing Train Step...
At time: 496.14823842048645 and batch: 50, loss is 4.924859714508057 and perplexity is 137.67002718063202
At time: 497.3344461917877 and batch: 100, loss is 4.919247722625732 and perplexity is 136.89958797697895
At time: 498.49107336997986 and batch: 150, loss is 4.898100700378418 and perplexity is 134.03496521830772
At time: 499.648113489151 and batch: 200, loss is 4.907750616073608 and perplexity is 135.3346521820375
At time: 500.8062376976013 and batch: 250, loss is 4.928968677520752 and perplexity is 138.236872004952
At time: 501.9628736972809 and batch: 300, loss is 4.9457572555541995 and perplexity is 140.57726346914438
At time: 503.12272024154663 and batch: 350, loss is 4.892527894973755 and perplexity is 133.2900918845366
At time: 504.2805709838867 and batch: 400, loss is 4.902089967727661 and perplexity is 134.57073448159395
At time: 505.44106221199036 and batch: 450, loss is 4.890126600265503 and perplexity is 132.97040707456733
At time: 506.59883522987366 and batch: 500, loss is 4.855145378112793 and perplexity is 128.39935631404353
At time: 507.7561469078064 and batch: 550, loss is 4.885604085922242 and perplexity is 132.3704042846163
At time: 508.9122974872589 and batch: 600, loss is 4.858140687942505 and perplexity is 128.78452873553312
At time: 510.07312655448914 and batch: 650, loss is 4.838481044769287 and perplexity is 126.27739626956189
At time: 511.23641300201416 and batch: 700, loss is 4.823032913208007 and perplexity is 124.34163686814404
At time: 512.3915565013885 and batch: 750, loss is 4.839114217758179 and perplexity is 126.35737702414204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.061561407044876 and perplexity of 157.8367714679881
Finished 26 epochs...
Completing Train Step...
At time: 515.7696731090546 and batch: 50, loss is 4.922294034957885 and perplexity is 137.31726274098972
At time: 516.9710276126862 and batch: 100, loss is 4.9168109035491945 and perplexity is 136.56639458031668
At time: 518.1619381904602 and batch: 150, loss is 4.895587387084961 and perplexity is 133.69851633626558
At time: 519.3662939071655 and batch: 200, loss is 4.905332918167114 and perplexity is 135.00784909154228
At time: 520.552000284195 and batch: 250, loss is 4.9263441944122315 and perplexity is 137.87454733486024
At time: 521.7746238708496 and batch: 300, loss is 4.943334617614746 and perplexity is 140.2371078605502
At time: 522.9434909820557 and batch: 350, loss is 4.890505895614624 and perplexity is 133.02085169764223
At time: 524.1093838214874 and batch: 400, loss is 4.900400199890137 and perplexity is 134.34353319536893
At time: 525.2781693935394 and batch: 450, loss is 4.888587198257446 and perplexity is 132.76586963596853
At time: 526.4480967521667 and batch: 500, loss is 4.85372329711914 and perplexity is 128.21689180024072
At time: 527.6145374774933 and batch: 550, loss is 4.884274244308472 and perplexity is 132.19448960782103
At time: 528.7795531749725 and batch: 600, loss is 4.857127571105957 and perplexity is 128.65412103145883
At time: 529.945060968399 and batch: 650, loss is 4.837442989349365 and perplexity is 126.14638134609109
At time: 531.1125140190125 and batch: 700, loss is 4.822171936035156 and perplexity is 124.23462763003157
At time: 532.2832314968109 and batch: 750, loss is 4.837940816879272 and perplexity is 126.20919612168832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.06147056402162 and perplexity of 157.82243374973768
Finished 27 epochs...
Completing Train Step...
At time: 535.66161942482 and batch: 50, loss is 4.919738082885742 and perplexity is 136.9667345561962
At time: 536.8565559387207 and batch: 100, loss is 4.914618873596192 and perplexity is 136.2673648136621
At time: 538.0251190662384 and batch: 150, loss is 4.893250617980957 and perplexity is 133.3864585195712
At time: 539.1967334747314 and batch: 200, loss is 4.903173542022705 and perplexity is 134.71663090087304
At time: 540.3630466461182 and batch: 250, loss is 4.924131898880005 and perplexity is 137.56986523747088
At time: 541.5282039642334 and batch: 300, loss is 4.941229438781738 and perplexity is 139.94219420139228
At time: 542.6931896209717 and batch: 350, loss is 4.888856525421143 and perplexity is 132.80163190673258
At time: 543.8605391979218 and batch: 400, loss is 4.898978662490845 and perplexity is 134.15269451287764
At time: 545.0227687358856 and batch: 450, loss is 4.887184658050537 and perplexity is 132.57979068780534
At time: 546.1942477226257 and batch: 500, loss is 4.852120246887207 and perplexity is 128.01151833807083
At time: 547.3661420345306 and batch: 550, loss is 4.882852859497071 and perplexity is 132.00672384345683
At time: 548.5355682373047 and batch: 600, loss is 4.856005277633667 and perplexity is 128.50981434361555
At time: 549.7074100971222 and batch: 650, loss is 4.836501178741455 and perplexity is 126.0276312748059
At time: 550.8745357990265 and batch: 700, loss is 4.821224393844605 and perplexity is 124.11696583238687
At time: 552.1008949279785 and batch: 750, loss is 4.836783437728882 and perplexity is 126.06320872718904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.061402076898619 and perplexity of 157.81162531542864
Finished 28 epochs...
Completing Train Step...
At time: 555.5016658306122 and batch: 50, loss is 4.917466669082642 and perplexity is 136.65597948504572
At time: 556.6988332271576 and batch: 100, loss is 4.912439594268799 and perplexity is 135.97072351205566
At time: 557.8720767498016 and batch: 150, loss is 4.8910609722137455 and perplexity is 133.09470895593628
At time: 559.0368115901947 and batch: 200, loss is 4.901207466125488 and perplexity is 134.45202797985147
At time: 560.207585811615 and batch: 250, loss is 4.922156744003296 and perplexity is 137.29841161698036
At time: 561.3753497600555 and batch: 300, loss is 4.939410562515259 and perplexity is 139.68788801153727
At time: 562.5392229557037 and batch: 350, loss is 4.887104053497314 and perplexity is 132.56910458369066
At time: 563.7056777477264 and batch: 400, loss is 4.897487649917602 and perplexity is 133.95282020320766
At time: 564.8732011318207 and batch: 450, loss is 4.885735301971436 and perplexity is 132.38777454570038
At time: 566.0449912548065 and batch: 500, loss is 4.850787715911865 and perplexity is 127.84105262532987
At time: 567.2169888019562 and batch: 550, loss is 4.881563282012939 and perplexity is 131.83660066172305
At time: 568.3862409591675 and batch: 600, loss is 4.854843978881836 and perplexity is 128.36066267820405
At time: 569.5519819259644 and batch: 650, loss is 4.835386610031128 and perplexity is 125.88724307103323
At time: 570.7180271148682 and batch: 700, loss is 4.820249834060669 and perplexity is 123.99606535092497
At time: 571.8854291439056 and batch: 750, loss is 4.835691633224488 and perplexity is 125.92564745673448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.061463112054869 and perplexity of 157.82125766659087
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 575.2613015174866 and batch: 50, loss is 4.91609489440918 and perplexity is 136.46864679190932
At time: 576.4621341228485 and batch: 100, loss is 4.911092462539673 and perplexity is 135.78767635817715
At time: 577.6287951469421 and batch: 150, loss is 4.889105472564697 and perplexity is 132.83469660916603
At time: 578.7947909832001 and batch: 200, loss is 4.898062133789063 and perplexity is 134.02979604652424
At time: 579.9598345756531 and batch: 250, loss is 4.9174676036834715 and perplexity is 136.6561072038972
At time: 581.1814081668854 and batch: 300, loss is 4.932255945205688 and perplexity is 138.69204132898085
At time: 582.3455264568329 and batch: 350, loss is 4.878782157897949 and perplexity is 131.4704560951263
At time: 583.5148100852966 and batch: 400, loss is 4.8885129737854 and perplexity is 132.7560155251017
At time: 584.6805012226105 and batch: 450, loss is 4.875831060409546 and perplexity is 131.08304588615422
At time: 585.8465960025787 and batch: 500, loss is 4.837283363342285 and perplexity is 126.12624670997776
At time: 587.0101282596588 and batch: 550, loss is 4.8652849197387695 and perplexity is 129.70788968555948
At time: 588.1785976886749 and batch: 600, loss is 4.837915143966675 and perplexity is 126.20595600561906
At time: 589.3445360660553 and batch: 650, loss is 4.817215642929077 and perplexity is 123.62040778571314
At time: 590.5157084465027 and batch: 700, loss is 4.801612062454224 and perplexity is 121.70645787280611
At time: 591.6951591968536 and batch: 750, loss is 4.820331926345825 and perplexity is 124.00624488910506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.055327659429506 and perplexity of 156.85591724536633
Finished 30 epochs...
Completing Train Step...
At time: 595.0752565860748 and batch: 50, loss is 4.912141418457031 and perplexity is 135.930186375093
At time: 596.317590713501 and batch: 100, loss is 4.907451267242432 and perplexity is 135.29414597513448
At time: 597.4852087497711 and batch: 150, loss is 4.885958909988403 and perplexity is 132.41738082341445
At time: 598.6526637077332 and batch: 200, loss is 4.895122022628784 and perplexity is 133.63631227382146
At time: 599.817182302475 and batch: 250, loss is 4.914883708953857 and perplexity is 136.30345800912684
At time: 600.982913017273 and batch: 300, loss is 4.930122995376587 and perplexity is 138.3965334269289
At time: 602.1497354507446 and batch: 350, loss is 4.877168045043946 and perplexity is 131.258419113866
At time: 603.3112494945526 and batch: 400, loss is 4.887401332855225 and perplexity is 132.60852050045904
At time: 604.4784686565399 and batch: 450, loss is 4.875084533691406 and perplexity is 130.98522540744852
At time: 605.6412076950073 and batch: 500, loss is 4.83692928314209 and perplexity is 126.0815958087593
At time: 606.8061156272888 and batch: 550, loss is 4.865233211517334 and perplexity is 129.70118289467732
At time: 607.9704728126526 and batch: 600, loss is 4.838233480453491 and perplexity is 126.24613836168487
At time: 609.136127948761 and batch: 650, loss is 4.818096904754639 and perplexity is 123.72939774924852
At time: 610.2994437217712 and batch: 700, loss is 4.802744970321656 and perplexity is 121.84441820986734
At time: 611.5188021659851 and batch: 750, loss is 4.821003103256226 and perplexity is 124.08950295474025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054835474768351 and perplexity of 156.77873416460906
Finished 31 epochs...
Completing Train Step...
At time: 614.8989131450653 and batch: 50, loss is 4.910160999298096 and perplexity is 135.6612540170128
At time: 616.0974590778351 and batch: 100, loss is 4.905503492355347 and perplexity is 135.03087990998185
At time: 617.26384806633 and batch: 150, loss is 4.884204902648926 and perplexity is 132.18532334033458
At time: 618.4268860816956 and batch: 200, loss is 4.893442010879516 and perplexity is 133.41199018370745
At time: 619.5883009433746 and batch: 250, loss is 4.913407764434814 and perplexity is 136.10243005689733
At time: 620.7499537467957 and batch: 300, loss is 4.928897275924682 and perplexity is 138.22700202402558
At time: 621.9116268157959 and batch: 350, loss is 4.8761616134643555 and perplexity is 131.12638294961044
At time: 623.0765643119812 and batch: 400, loss is 4.886695108413696 and perplexity is 132.51490218378942
At time: 624.2409641742706 and batch: 450, loss is 4.874716053009033 and perplexity is 130.9369687735743
At time: 625.4064025878906 and batch: 500, loss is 4.8369106101989745 and perplexity is 126.07924151627374
At time: 626.5685956478119 and batch: 550, loss is 4.865258073806762 and perplexity is 129.70440760311226
At time: 627.7308940887451 and batch: 600, loss is 4.838438835144043 and perplexity is 126.2720662604782
At time: 628.8925557136536 and batch: 650, loss is 4.8186040878295895 and perplexity is 123.792167122097
At time: 630.0533323287964 and batch: 700, loss is 4.803312501907349 and perplexity is 121.9135883920168
At time: 631.2160468101501 and batch: 750, loss is 4.8212226867675785 and perplexity is 124.11675395534675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054628948832667 and perplexity of 156.74635863314893
Finished 32 epochs...
Completing Train Step...
At time: 634.5798709392548 and batch: 50, loss is 4.908587980270386 and perplexity is 135.44802403449864
At time: 635.7702209949493 and batch: 100, loss is 4.90404088973999 and perplexity is 134.83352775094497
At time: 636.9259603023529 and batch: 150, loss is 4.88284628868103 and perplexity is 132.00585645440805
At time: 638.0898795127869 and batch: 200, loss is 4.892224750518799 and perplexity is 133.24969185611286
At time: 639.2485554218292 and batch: 250, loss is 4.912368335723877 and perplexity is 135.96103478135277
At time: 640.4521656036377 and batch: 300, loss is 4.928005180358887 and perplexity is 138.10374531500287
At time: 641.6099896430969 and batch: 350, loss is 4.87551176071167 and perplexity is 131.04119779060383
At time: 642.7689671516418 and batch: 400, loss is 4.886197662353515 and perplexity is 132.4489995606419
At time: 643.9239025115967 and batch: 450, loss is 4.874536418914795 and perplexity is 130.9134501422237
At time: 645.0785534381866 and batch: 500, loss is 4.8368605041503905 and perplexity is 126.07292434193853
At time: 646.2407734394073 and batch: 550, loss is 4.86523591041565 and perplexity is 129.70153294545372
At time: 647.3960273265839 and batch: 600, loss is 4.838526983261108 and perplexity is 126.2831973959441
At time: 648.5584464073181 and batch: 650, loss is 4.818871278762817 and perplexity is 123.82524768597317
At time: 649.7198405265808 and batch: 700, loss is 4.803617506027222 and perplexity is 121.95077820998046
At time: 650.880578994751 and batch: 750, loss is 4.8212967872619625 and perplexity is 124.12595140893991
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054497652275618 and perplexity of 156.7257797269298
Finished 33 epochs...
Completing Train Step...
At time: 654.2977590560913 and batch: 50, loss is 4.907266359329224 and perplexity is 135.2691313297072
At time: 655.5220174789429 and batch: 100, loss is 4.902832517623901 and perplexity is 134.67069707553398
At time: 656.6917307376862 and batch: 150, loss is 4.881742391586304 and perplexity is 131.86021597382253
At time: 657.8556520938873 and batch: 200, loss is 4.8912214660644535 and perplexity is 133.11607155252233
At time: 659.028000831604 and batch: 250, loss is 4.911501817703247 and perplexity is 135.8432731232791
At time: 660.1973242759705 and batch: 300, loss is 4.927246789932251 and perplexity is 137.99904846223697
At time: 661.3591539859772 and batch: 350, loss is 4.874971284866333 and perplexity is 130.9703923244996
At time: 662.5234444141388 and batch: 400, loss is 4.885766010284424 and perplexity is 132.39184001333848
At time: 663.6884989738464 and batch: 450, loss is 4.874314336776734 and perplexity is 130.88437983143297
At time: 664.8544602394104 and batch: 500, loss is 4.836752967834473 and perplexity is 126.05936765304907
At time: 666.0206723213196 and batch: 550, loss is 4.865142574310303 and perplexity is 129.6894276744493
At time: 667.1841788291931 and batch: 600, loss is 4.838520517349243 and perplexity is 126.28238086255958
At time: 668.3457133769989 and batch: 650, loss is 4.818984069824219 and perplexity is 123.83921485475909
At time: 669.5029671192169 and batch: 700, loss is 4.803755283355713 and perplexity is 121.96758141993372
At time: 670.7626793384552 and batch: 750, loss is 4.821196212768554 and perplexity is 124.1134681320183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054386937341024 and perplexity of 156.70842880299912
Finished 34 epochs...
Completing Train Step...
At time: 674.3037724494934 and batch: 50, loss is 4.906077671051025 and perplexity is 135.10843402731513
At time: 675.541850566864 and batch: 100, loss is 4.901742277145385 and perplexity is 134.5239536376655
At time: 676.7407381534576 and batch: 150, loss is 4.880733795166016 and perplexity is 131.72728927797624
At time: 677.9170427322388 and batch: 200, loss is 4.890289316177368 and perplexity is 132.9920452359944
At time: 679.0879623889923 and batch: 250, loss is 4.9106496238708495 and perplexity is 135.72755763676577
At time: 680.2584319114685 and batch: 300, loss is 4.926610479354858 and perplexity is 137.9112661394024
At time: 681.4311244487762 and batch: 350, loss is 4.87442476272583 and perplexity is 130.89883366132204
At time: 682.5966975688934 and batch: 400, loss is 4.88540018081665 and perplexity is 132.34341603496796
At time: 683.7632436752319 and batch: 450, loss is 4.874096746444702 and perplexity is 130.85590375393963
At time: 684.9305608272552 and batch: 500, loss is 4.836632099151611 and perplexity is 126.04413194409707
At time: 686.0971915721893 and batch: 550, loss is 4.865023517608643 and perplexity is 129.673988198055
At time: 687.2598333358765 and batch: 600, loss is 4.838485116958618 and perplexity is 126.2779104960748
At time: 688.4281063079834 and batch: 650, loss is 4.819044065475464 and perplexity is 123.84664489198674
At time: 689.5963804721832 and batch: 700, loss is 4.803843450546265 and perplexity is 121.97833543299453
At time: 690.7632813453674 and batch: 750, loss is 4.8210428524017335 and perplexity is 124.09443550448083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054339031840479 and perplexity of 156.7009217870928
Finished 35 epochs...
Completing Train Step...
At time: 694.1276636123657 and batch: 50, loss is 4.90500036239624 and perplexity is 134.96295891687063
At time: 695.3233335018158 and batch: 100, loss is 4.900794048309326 and perplexity is 134.39645460436876
At time: 696.4866423606873 and batch: 150, loss is 4.879865932464599 and perplexity is 131.61301767005867
At time: 697.652471780777 and batch: 200, loss is 4.88950101852417 and perplexity is 132.88724922949066
At time: 698.818154335022 and batch: 250, loss is 4.909905471801758 and perplexity is 135.62659326499988
At time: 700.030611038208 and batch: 300, loss is 4.926007556915283 and perplexity is 137.828141403779
At time: 701.1978828907013 and batch: 350, loss is 4.873969011306762 and perplexity is 130.83918992451777
At time: 702.3675458431244 and batch: 400, loss is 4.885050706863403 and perplexity is 132.29717353892457
At time: 703.5363686084747 and batch: 450, loss is 4.873856906890869 and perplexity is 130.8245230956788
At time: 704.7044830322266 and batch: 500, loss is 4.836505537033081 and perplexity is 126.02818054117289
At time: 705.86958527565 and batch: 550, loss is 4.8648299217224125 and perplexity is 129.64888627728146
At time: 707.0344548225403 and batch: 600, loss is 4.838355560302734 and perplexity is 126.26155141201586
At time: 708.2031753063202 and batch: 650, loss is 4.819040222167969 and perplexity is 123.84616891216282
At time: 709.3724100589752 and batch: 700, loss is 4.803844547271728 and perplexity is 121.97846920981434
At time: 710.5391414165497 and batch: 750, loss is 4.820792541503907 and perplexity is 124.06337720219231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054278351539789 and perplexity of 156.69141341652866
Finished 36 epochs...
Completing Train Step...
At time: 713.9029722213745 and batch: 50, loss is 4.904003858566284 and perplexity is 134.82853479960542
At time: 715.0990447998047 and batch: 100, loss is 4.899889841079712 and perplexity is 134.2749872825721
At time: 716.2627744674683 and batch: 150, loss is 4.879049367904663 and perplexity is 131.50559101057365
At time: 717.4289236068726 and batch: 200, loss is 4.888757219314575 and perplexity is 132.78844454852558
At time: 718.5953516960144 and batch: 250, loss is 4.909253740310669 and perplexity is 135.53822994084825
At time: 719.7595596313477 and batch: 300, loss is 4.925422077178955 and perplexity is 137.74746943809637
At time: 720.9254832267761 and batch: 350, loss is 4.873559999465942 and perplexity is 130.7856860891923
At time: 722.090368270874 and batch: 400, loss is 4.8847306442260745 and perplexity is 132.254836932195
At time: 723.2599415779114 and batch: 450, loss is 4.873666028976441 and perplexity is 130.7995539666577
At time: 724.427220582962 and batch: 500, loss is 4.83634617805481 and perplexity is 126.00809841926419
At time: 725.5923035144806 and batch: 550, loss is 4.864651794433594 and perplexity is 129.62579432938463
At time: 726.7584750652313 and batch: 600, loss is 4.838251075744629 and perplexity is 126.24835971878703
At time: 727.9244503974915 and batch: 650, loss is 4.818976564407349 and perplexity is 123.83828539331473
At time: 729.0896818637848 and batch: 700, loss is 4.803809938430786 and perplexity is 121.9742477494254
At time: 730.3090014457703 and batch: 750, loss is 4.820587215423584 and perplexity is 124.03790637024228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054245704828307 and perplexity of 156.68629804066362
Finished 37 epochs...
Completing Train Step...
At time: 733.6631190776825 and batch: 50, loss is 4.90308069229126 and perplexity is 134.7041230785557
At time: 734.8577811717987 and batch: 100, loss is 4.8991005229949955 and perplexity is 134.1690434239872
At time: 736.0215716362 and batch: 150, loss is 4.878302030563354 and perplexity is 131.40734868647166
At time: 737.1845307350159 and batch: 200, loss is 4.888082218170166 and perplexity is 132.69884244065577
At time: 738.3495118618011 and batch: 250, loss is 4.908658018112183 and perplexity is 135.45751085399203
At time: 739.5161254405975 and batch: 300, loss is 4.924887084960938 and perplexity is 137.67379532319174
At time: 740.6845664978027 and batch: 350, loss is 4.87313247680664 and perplexity is 130.72978419539143
At time: 741.8520123958588 and batch: 400, loss is 4.884432954788208 and perplexity is 132.21547192369786
At time: 743.0162708759308 and batch: 450, loss is 4.873458318710327 and perplexity is 130.7723883778792
At time: 744.1839826107025 and batch: 500, loss is 4.836190938949585 and perplexity is 125.98853855308184
At time: 745.3539202213287 and batch: 550, loss is 4.864446716308594 and perplexity is 129.59921364018402
At time: 746.5273842811584 and batch: 600, loss is 4.838098611831665 and perplexity is 126.22911286712147
At time: 747.6933100223541 and batch: 650, loss is 4.818903322219849 and perplexity is 123.82921553854835
At time: 748.8610599040985 and batch: 700, loss is 4.803740749359131 and perplexity is 121.9658087564042
At time: 750.0282893180847 and batch: 750, loss is 4.820363969802856 and perplexity is 124.01021854154918
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.05421625181686 and perplexity of 156.68168322529434
Finished 38 epochs...
Completing Train Step...
At time: 753.4958488941193 and batch: 50, loss is 4.902200860977173 and perplexity is 134.58565829508962
At time: 754.7330706119537 and batch: 100, loss is 4.898339490890503 and perplexity is 134.0669753179915
At time: 755.90407538414 and batch: 150, loss is 4.87759768486023 and perplexity is 131.31482507319515
At time: 757.0654683113098 and batch: 200, loss is 4.887412843704223 and perplexity is 132.61004694589974
At time: 758.22620677948 and batch: 250, loss is 4.908115634918213 and perplexity is 135.38406089741946
At time: 759.4166586399078 and batch: 300, loss is 4.92438907623291 and perplexity is 137.60524964109078
At time: 760.5809323787689 and batch: 350, loss is 4.87270975112915 and perplexity is 130.67453303766362
At time: 761.7437648773193 and batch: 400, loss is 4.884159784317017 and perplexity is 132.17935949358306
At time: 762.9057865142822 and batch: 450, loss is 4.87327844619751 and perplexity is 130.74886813516062
At time: 764.066400051117 and batch: 500, loss is 4.8360022449493405 and perplexity is 125.96476751455401
At time: 765.2327864170074 and batch: 550, loss is 4.864253883361816 and perplexity is 129.5742250513068
At time: 766.4029183387756 and batch: 600, loss is 4.837935609817505 and perplexity is 126.20853894431947
At time: 767.5707488059998 and batch: 650, loss is 4.818786277770996 and perplexity is 123.81472286442383
At time: 768.7320339679718 and batch: 700, loss is 4.8036480903625485 and perplexity is 121.95450805051162
At time: 769.8967652320862 and batch: 750, loss is 4.8201210498809814 and perplexity is 123.98009764758011
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054192831349927 and perplexity of 156.6780137100844
Finished 39 epochs...
Completing Train Step...
At time: 773.293417930603 and batch: 50, loss is 4.901362552642822 and perplexity is 134.47288129361147
At time: 774.4775750637054 and batch: 100, loss is 4.897608041763306 and perplexity is 133.96894800127728
At time: 775.6356399059296 and batch: 150, loss is 4.876939926147461 and perplexity is 131.22848000311174
At time: 776.7934408187866 and batch: 200, loss is 4.886807527542114 and perplexity is 132.52980023099235
At time: 777.9492959976196 and batch: 250, loss is 4.907598161697388 and perplexity is 135.314021394725
At time: 779.1090178489685 and batch: 300, loss is 4.923908033370972 and perplexity is 137.5390715365142
At time: 780.2677707672119 and batch: 350, loss is 4.872295751571655 and perplexity is 130.62044503578707
At time: 781.4260325431824 and batch: 400, loss is 4.883854656219483 and perplexity is 132.13903400961945
At time: 782.5850992202759 and batch: 450, loss is 4.873047370910644 and perplexity is 130.71865879339947
At time: 783.7445757389069 and batch: 500, loss is 4.835842790603638 and perplexity is 125.94468348625378
At time: 784.9100725650787 and batch: 550, loss is 4.864020957946777 and perplexity is 129.54404743586156
At time: 786.06902384758 and batch: 600, loss is 4.837746543884277 and perplexity is 126.18467946470113
At time: 787.2346966266632 and batch: 650, loss is 4.8186717700958255 and perplexity is 123.80054594005558
At time: 788.3947892189026 and batch: 700, loss is 4.803534774780274 and perplexity is 121.94068948736489
At time: 789.6146855354309 and batch: 750, loss is 4.819843111038208 and perplexity is 123.94564355100063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054161958916243 and perplexity of 156.67317675316107
Finished 40 epochs...
Completing Train Step...
At time: 792.9995019435883 and batch: 50, loss is 4.900547494888306 and perplexity is 134.36332278326287
At time: 794.2228407859802 and batch: 100, loss is 4.89690055847168 and perplexity is 133.87420072897777
At time: 795.3905513286591 and batch: 150, loss is 4.876316967010498 and perplexity is 131.14675548062604
At time: 796.5566999912262 and batch: 200, loss is 4.886236009597778 and perplexity is 132.4540787121655
At time: 797.722065448761 and batch: 250, loss is 4.907109384536743 and perplexity is 135.24789915238955
At time: 798.8941707611084 and batch: 300, loss is 4.923440132141113 and perplexity is 137.47473188926259
At time: 800.0579144954681 and batch: 350, loss is 4.871899681091309 and perplexity is 130.56872037736002
At time: 801.2244894504547 and batch: 400, loss is 4.883561134338379 and perplexity is 132.1002540034583
At time: 802.3935158252716 and batch: 450, loss is 4.872807083129882 and perplexity is 130.6872524704028
At time: 803.5593473911285 and batch: 500, loss is 4.835643424987793 and perplexity is 125.91957694964131
At time: 804.7245972156525 and batch: 550, loss is 4.863787517547608 and perplexity is 129.51381015115734
At time: 805.8920862674713 and batch: 600, loss is 4.837533721923828 and perplexity is 126.15782745128921
At time: 807.0559396743774 and batch: 650, loss is 4.81842978477478 and perplexity is 123.77059164959617
At time: 808.2263209819794 and batch: 700, loss is 4.803321475982666 and perplexity is 121.91468245865038
At time: 809.3969237804413 and batch: 750, loss is 4.819499778747558 and perplexity is 123.90309631362246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0541193762490915 and perplexity of 156.66650533346825
Finished 41 epochs...
Completing Train Step...
At time: 812.773158788681 and batch: 50, loss is 4.899656019210815 and perplexity is 134.2435945243994
At time: 813.9694559574127 and batch: 100, loss is 4.896171941757202 and perplexity is 133.7766932757831
At time: 815.1340990066528 and batch: 150, loss is 4.875635118484497 and perplexity is 131.05736373798896
At time: 816.2985870838165 and batch: 200, loss is 4.885669584274292 and perplexity is 132.3790746119002
At time: 817.465343952179 and batch: 250, loss is 4.906609554290771 and perplexity is 135.18031505338448
At time: 818.6891407966614 and batch: 300, loss is 4.9228965854644775 and perplexity is 137.4000282599167
At time: 819.8527400493622 and batch: 350, loss is 4.871466655731201 and perplexity is 130.51219304996624
At time: 821.0195600986481 and batch: 400, loss is 4.883230333328247 and perplexity is 132.05656233301806
At time: 822.1861817836761 and batch: 450, loss is 4.8725802612304685 and perplexity is 130.65761310112435
At time: 823.3503525257111 and batch: 500, loss is 4.835426263809204 and perplexity is 125.89223507480784
At time: 824.5348360538483 and batch: 550, loss is 4.863508090972901 and perplexity is 129.47762560650668
At time: 825.7159299850464 and batch: 600, loss is 4.837301483154297 and perplexity is 126.12853211456338
At time: 826.8783226013184 and batch: 650, loss is 4.81822509765625 and perplexity is 123.74525999645263
At time: 828.0364201068878 and batch: 700, loss is 4.803195152282715 and perplexity is 121.89928271758045
At time: 829.2162189483643 and batch: 750, loss is 4.819187641143799 and perplexity is 123.8644275333449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054076793581941 and perplexity of 156.65983419785636
Finished 42 epochs...
Completing Train Step...
At time: 832.5713174343109 and batch: 50, loss is 4.898863258361817 and perplexity is 134.13721363130662
At time: 833.7813076972961 and batch: 100, loss is 4.895507755279541 and perplexity is 133.68787010592266
At time: 834.9495887756348 and batch: 150, loss is 4.8749893856048585 and perplexity is 130.9727630067812
At time: 836.1128585338593 and batch: 200, loss is 4.88508939743042 and perplexity is 132.30229229060657
At time: 837.2788450717926 and batch: 250, loss is 4.90607723236084 and perplexity is 135.10837475658414
At time: 838.4442708492279 and batch: 300, loss is 4.922393026351929 and perplexity is 137.3308566410832
At time: 839.6080405712128 and batch: 350, loss is 4.8710442066192625 and perplexity is 130.45706993409064
At time: 840.7756299972534 and batch: 400, loss is 4.882899427413941 and perplexity is 132.01287126473426
At time: 841.9411337375641 and batch: 450, loss is 4.872317581176758 and perplexity is 130.6232964596432
At time: 843.10608959198 and batch: 500, loss is 4.835161209106445 and perplexity is 125.85887116768606
At time: 844.2737243175507 and batch: 550, loss is 4.863221826553345 and perplexity is 129.44056607383288
At time: 845.4379570484161 and batch: 600, loss is 4.837065267562866 and perplexity is 126.09874210732421
At time: 846.6029176712036 and batch: 650, loss is 4.818025979995728 and perplexity is 123.72062258273489
At time: 847.7726473808289 and batch: 700, loss is 4.803027601242065 and perplexity is 121.87886007687177
At time: 848.9819393157959 and batch: 750, loss is 4.818881149291992 and perplexity is 123.82646991272509
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054060825081759 and perplexity of 156.65733259523898
Finished 43 epochs...
Completing Train Step...
At time: 852.3411958217621 and batch: 50, loss is 4.898108835220337 and perplexity is 134.03605557599636
At time: 853.5520160198212 and batch: 100, loss is 4.894879741668701 and perplexity is 133.6039386616951
At time: 854.7160341739655 and batch: 150, loss is 4.874357481002807 and perplexity is 130.89002685852333
At time: 855.8815426826477 and batch: 200, loss is 4.884603900909424 and perplexity is 132.23807557773637
At time: 857.0451238155365 and batch: 250, loss is 4.905632019042969 and perplexity is 135.04823609704604
At time: 858.2090606689453 and batch: 300, loss is 4.921962881088257 and perplexity is 137.27179712653887
At time: 859.3740971088409 and batch: 350, loss is 4.87067569732666 and perplexity is 130.40900414842247
At time: 860.5410571098328 and batch: 400, loss is 4.882625455856323 and perplexity is 131.9767084467863
At time: 861.7112364768982 and batch: 450, loss is 4.872111501693726 and perplexity is 130.5963804517506
At time: 862.876425743103 and batch: 500, loss is 4.834960012435913 and perplexity is 125.83355132906924
At time: 864.0391783714294 and batch: 550, loss is 4.86293794631958 and perplexity is 129.40382567086093
At time: 865.2047832012177 and batch: 600, loss is 4.83681791305542 and perplexity is 126.06755487239123
At time: 866.3700592517853 and batch: 650, loss is 4.817842226028443 and perplexity is 123.69789051611802
At time: 867.5332081317902 and batch: 700, loss is 4.802859287261963 and perplexity is 121.85834788713454
At time: 868.7021143436432 and batch: 750, loss is 4.8185616302490235 and perplexity is 123.78691131776313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054077858148619 and perplexity of 156.66000097278447
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 872.0652668476105 and batch: 50, loss is 4.897591190338135 and perplexity is 133.96669045259634
At time: 873.2648451328278 and batch: 100, loss is 4.894541511535644 and perplexity is 133.55875742500322
At time: 874.4281387329102 and batch: 150, loss is 4.873950328826904 and perplexity is 130.83674554682094
At time: 875.5939650535583 and batch: 200, loss is 4.883967905044556 and perplexity is 132.1539994473605
At time: 876.7645876407623 and batch: 250, loss is 4.904932880401612 and perplexity is 134.95385165449352
At time: 877.9977798461914 and batch: 300, loss is 4.920689382553101 and perplexity is 137.09709296034103
At time: 879.1642479896545 and batch: 350, loss is 4.868269634246826 and perplexity is 130.09560903366128
At time: 880.327136516571 and batch: 400, loss is 4.879699983596802 and perplexity is 131.59117845093584
At time: 881.4941720962524 and batch: 450, loss is 4.869005661010743 and perplexity is 130.19139813112338
At time: 882.6588554382324 and batch: 500, loss is 4.830982980728149 and perplexity is 125.33410112848274
At time: 883.8251352310181 and batch: 550, loss is 4.8582977771759035 and perplexity is 128.80476098751862
At time: 884.9902710914612 and batch: 600, loss is 4.831821098327636 and perplexity is 125.43918987666757
At time: 886.1568067073822 and batch: 650, loss is 4.8127345371246335 and perplexity is 123.06769097498467
At time: 887.3248565196991 and batch: 700, loss is 4.797663288116455 and perplexity is 121.22681416085742
At time: 888.4914302825928 and batch: 750, loss is 4.813748369216919 and perplexity is 123.19252421890045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054118666537972 and perplexity of 156.66639414554686
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 891.91077876091 and batch: 50, loss is 4.897049598693847 and perplexity is 133.89415485654376
At time: 893.1331346035004 and batch: 100, loss is 4.893935832977295 and perplexity is 133.47788824218838
At time: 894.2998518943787 and batch: 150, loss is 4.873379344940186 and perplexity is 130.7620611971322
At time: 895.4664888381958 and batch: 200, loss is 4.88341778755188 and perplexity is 132.08131921370432
At time: 896.6276051998138 and batch: 250, loss is 4.90461989402771 and perplexity is 134.91161954720192
At time: 897.7964420318604 and batch: 300, loss is 4.92019793510437 and perplexity is 137.0297334969512
At time: 898.9580807685852 and batch: 350, loss is 4.867475528717041 and perplexity is 129.99234039965026
At time: 900.1190800666809 and batch: 400, loss is 4.878925256729126 and perplexity is 131.48927070987037
At time: 901.2801687717438 and batch: 450, loss is 4.868323898315429 and perplexity is 130.10266874225735
At time: 902.438357591629 and batch: 500, loss is 4.829797439575195 and perplexity is 125.1856004379596
At time: 903.6246681213379 and batch: 550, loss is 4.857082386016845 and perplexity is 128.64830791486946
At time: 904.8222076892853 and batch: 600, loss is 4.830547103881836 and perplexity is 125.27948280004445
At time: 906.013810634613 and batch: 650, loss is 4.811391592025757 and perplexity is 122.90252874928305
At time: 907.1944382190704 and batch: 700, loss is 4.7963502311706545 and perplexity is 121.06774090948012
At time: 908.4131791591644 and batch: 750, loss is 4.812517261505127 and perplexity is 123.04095427090895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054915317269259 and perplexity of 156.79125227056582
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 911.853185415268 and batch: 50, loss is 4.896833410263062 and perplexity is 133.86521161802335
At time: 913.0693352222443 and batch: 100, loss is 4.8937551879882815 and perplexity is 133.45377830826826
At time: 914.2283267974854 and batch: 150, loss is 4.873232507705689 and perplexity is 130.74286186731166
At time: 915.3885915279388 and batch: 200, loss is 4.883245515823364 and perplexity is 132.05856729635101
At time: 916.5501201152802 and batch: 250, loss is 4.9044402313232425 and perplexity is 134.88738313802457
At time: 917.7088050842285 and batch: 300, loss is 4.920040779113769 and perplexity is 137.00820014553366
At time: 918.8639183044434 and batch: 350, loss is 4.867219562530518 and perplexity is 129.9590710141015
At time: 920.0203890800476 and batch: 400, loss is 4.878724908828735 and perplexity is 131.46292974932592
At time: 921.1791622638702 and batch: 450, loss is 4.8681416416168215 and perplexity is 130.078958820084
At time: 922.3365602493286 and batch: 500, loss is 4.829467544555664 and perplexity is 125.14430914312882
At time: 923.4978332519531 and batch: 550, loss is 4.856738586425781 and perplexity is 128.60408628134255
At time: 924.6553158760071 and batch: 600, loss is 4.830210962295532 and perplexity is 125.23737823290922
At time: 925.815447807312 and batch: 650, loss is 4.811048126220703 and perplexity is 122.86032318180251
At time: 926.9722633361816 and batch: 700, loss is 4.79597149848938 and perplexity is 121.02189728113777
At time: 928.1387476921082 and batch: 750, loss is 4.81219217300415 and perplexity is 123.00096157246172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.055055130359738 and perplexity of 156.81317527263153
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 931.5253627300262 and batch: 50, loss is 4.896788530349731 and perplexity is 133.85920389374203
At time: 932.7489359378815 and batch: 100, loss is 4.8937093544006345 and perplexity is 133.44766178299557
At time: 933.9212951660156 and batch: 150, loss is 4.873186197280884 and perplexity is 130.73680725003535
At time: 935.0873203277588 and batch: 200, loss is 4.883197526931763 and perplexity is 132.0522301041387
At time: 936.2517371177673 and batch: 250, loss is 4.9043621063232425 and perplexity is 134.87684547284982
At time: 937.4742980003357 and batch: 300, loss is 4.919990520477295 and perplexity is 137.0013144732423
At time: 938.6414875984192 and batch: 350, loss is 4.867147512435913 and perplexity is 129.94970778805495
At time: 939.8058762550354 and batch: 400, loss is 4.878672552108765 and perplexity is 131.45604696170824
At time: 940.9712052345276 and batch: 450, loss is 4.868090381622315 and perplexity is 130.07229114426343
At time: 942.1407122612 and batch: 500, loss is 4.829379501342774 and perplexity is 125.1332915210975
At time: 943.3053977489471 and batch: 550, loss is 4.856651105880737 and perplexity is 128.5928364178591
At time: 944.469895362854 and batch: 600, loss is 4.8301254081726075 and perplexity is 125.22666411718203
At time: 945.6381912231445 and batch: 650, loss is 4.810959434509277 and perplexity is 122.84942697268205
At time: 946.8070874214172 and batch: 700, loss is 4.79586935043335 and perplexity is 121.00953576095674
At time: 947.9754536151886 and batch: 750, loss is 4.812107219696045 and perplexity is 122.99051267771635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.05508848678234 and perplexity of 156.8184060864156
Annealing...
Model not improving. Stopping early with 156.65733259523898loss at 47 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fccc82d30>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.466005873618206, 'wordvec_dim': 200, 'lr': 2.614572684018477, 'dropout': 0.0, 'batch_size': 80, 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.758310079574585 and batch: 50, loss is 7.362125997543335 and perplexity is 1575.1818383927537
At time: 2.9167656898498535 and batch: 100, loss is 6.445708703994751 and perplexity is 629.9929973426975
At time: 4.075948476791382 and batch: 150, loss is 6.2322812747955325 and perplexity is 508.9151355221326
At time: 5.234443664550781 and batch: 200, loss is 6.117236709594726 and perplexity is 453.609506276997
At time: 6.392869710922241 and batch: 250, loss is 6.059077205657959 and perplexity is 427.9803167652163
At time: 7.552477598190308 and batch: 300, loss is 5.950195951461792 and perplexity is 383.82854345683336
At time: 8.713050127029419 and batch: 350, loss is 5.782756319046021 and perplexity is 324.6528050178314
At time: 9.903077125549316 and batch: 400, loss is 5.753645811080933 and perplexity is 315.3382307178689
At time: 11.064804315567017 and batch: 450, loss is 5.65019157409668 and perplexity is 284.3459339218516
At time: 12.223761081695557 and batch: 500, loss is 5.599160299301148 and perplexity is 270.1994254946291
At time: 13.385255575180054 and batch: 550, loss is 5.578439111709595 and perplexity is 264.65818135924775
At time: 14.553250789642334 and batch: 600, loss is 5.510750818252563 and perplexity is 247.33676232541697
At time: 15.719568014144897 and batch: 650, loss is 5.465432529449463 and perplexity is 236.37807347902645
At time: 16.88776397705078 and batch: 700, loss is 5.4360308170318605 and perplexity is 229.52932911233185
At time: 18.072242975234985 and batch: 750, loss is 5.3950714683532714 and perplexity is 220.31789229080508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.110227717909702 and perplexity of 165.70808527642237
Finished 1 epochs...
Completing Train Step...
At time: 21.669180393218994 and batch: 50, loss is 5.411354579925537 and perplexity is 223.9347198019262
At time: 22.838292598724365 and batch: 100, loss is 5.360308437347412 and perplexity is 212.7905688874432
At time: 24.008299112319946 and batch: 150, loss is 5.3401605415344235 and perplexity is 208.5461879267567
At time: 25.23097801208496 and batch: 200, loss is 5.313877220153809 and perplexity is 203.13630768828452
At time: 26.40218448638916 and batch: 250, loss is 5.324806489944458 and perplexity is 205.36861572716802
At time: 27.573671102523804 and batch: 300, loss is 5.321037940979004 and perplexity is 204.59613053120916
At time: 28.740592002868652 and batch: 350, loss is 5.231917142868042 and perplexity is 187.15125550328088
At time: 29.903945922851562 and batch: 400, loss is 5.240626125335694 and perplexity is 188.7882705270369
At time: 31.073627471923828 and batch: 450, loss is 5.174057655334472 and perplexity is 176.63008940432107
At time: 32.23835039138794 and batch: 500, loss is 5.150492944717407 and perplexity is 172.51651046251487
At time: 33.40443539619446 and batch: 550, loss is 5.155557117462158 and perplexity is 173.392379777709
At time: 34.57056927680969 and batch: 600, loss is 5.105446882247925 and perplexity is 164.91775288287872
At time: 35.73970580101013 and batch: 650, loss is 5.077198905944824 and perplexity is 160.32434281472626
At time: 36.90424060821533 and batch: 700, loss is 5.066124067306519 and perplexity is 158.55857244711711
At time: 38.068111181259155 and batch: 750, loss is 5.057185277938843 and perplexity is 157.1475665032029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.887572532476381 and perplexity of 132.63122497237381
Finished 2 epochs...
Completing Train Step...
At time: 41.45852518081665 and batch: 50, loss is 5.105903673171997 and perplexity is 164.993103023934
At time: 42.65604209899902 and batch: 100, loss is 5.071087961196899 and perplexity is 159.34759707326953
At time: 43.82512640953064 and batch: 150, loss is 5.057168912887573 and perplexity is 157.1449947962633
At time: 44.993695974349976 and batch: 200, loss is 5.035855140686035 and perplexity is 153.83108360054618
At time: 46.16103148460388 and batch: 250, loss is 5.049930124282837 and perplexity is 156.0115626856855
At time: 47.32675242424011 and batch: 300, loss is 5.074056901931763 and perplexity is 159.8213936341502
At time: 48.544203996658325 and batch: 350, loss is 4.999050941467285 and perplexity is 148.2723731451875
At time: 49.71026659011841 and batch: 400, loss is 5.0169901943206785 and perplexity is 150.9562703166619
At time: 50.87473797798157 and batch: 450, loss is 4.959318599700928 and perplexity is 142.49666553790507
At time: 52.041841983795166 and batch: 500, loss is 4.9363414478302 and perplexity is 139.2598270831656
At time: 53.20773983001709 and batch: 550, loss is 4.9488632869720455 and perplexity is 141.01457967346798
At time: 54.370903730392456 and batch: 600, loss is 4.900847930908203 and perplexity is 134.40369642972513
At time: 55.53822135925293 and batch: 650, loss is 4.875393667221069 and perplexity is 131.0257235918633
At time: 56.70961403846741 and batch: 700, loss is 4.872341108322144 and perplexity is 130.6263696890818
At time: 57.87569761276245 and batch: 750, loss is 4.874496488571167 and perplexity is 130.9082228275389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.791482260060865 and perplexity of 120.47981879933447
Finished 3 epochs...
Completing Train Step...
At time: 61.238919496536255 and batch: 50, loss is 4.935000686645508 and perplexity is 139.07323802605984
At time: 62.43565845489502 and batch: 100, loss is 4.905940780639648 and perplexity is 135.0899402440403
At time: 63.60450601577759 and batch: 150, loss is 4.887775516510009 and perplexity is 132.6581497259589
At time: 64.7705295085907 and batch: 200, loss is 4.87330509185791 and perplexity is 130.7523520715143
At time: 65.93530607223511 and batch: 250, loss is 4.883334331512451 and perplexity is 132.07029668987462
At time: 67.1007890701294 and batch: 300, loss is 4.919437265396118 and perplexity is 136.92553876346307
At time: 68.26390337944031 and batch: 350, loss is 4.850767240524292 and perplexity is 127.83843505702758
At time: 69.42951607704163 and batch: 400, loss is 4.874497509002685 and perplexity is 130.9083564104836
At time: 70.59412050247192 and batch: 450, loss is 4.818571338653564 and perplexity is 123.78811309700872
At time: 71.76122736930847 and batch: 500, loss is 4.79580961227417 and perplexity is 121.00230708996313
At time: 72.93375945091248 and batch: 550, loss is 4.809021511077881 and perplexity is 122.61158472408701
At time: 74.10586667060852 and batch: 600, loss is 4.763134336471557 and perplexity is 117.11242097183833
At time: 75.27140069007874 and batch: 650, loss is 4.739939613342285 and perplexity is 114.42729158983167
At time: 76.44077587127686 and batch: 700, loss is 4.739261178970337 and perplexity is 114.34968651008161
At time: 77.67384386062622 and batch: 750, loss is 4.747787990570068 and perplexity is 115.32889356760244
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.7388795364734735 and perplexity of 114.30605413671988
Finished 4 epochs...
Completing Train Step...
At time: 81.05961012840271 and batch: 50, loss is 4.814002275466919 and perplexity is 123.2238075421005
At time: 82.27065443992615 and batch: 100, loss is 4.787332925796509 and perplexity is 119.98094347502553
At time: 83.44041061401367 and batch: 150, loss is 4.766031856536865 and perplexity is 117.45224865250741
At time: 84.605801820755 and batch: 200, loss is 4.756384725570679 and perplexity is 116.32461936660687
At time: 85.77367615699768 and batch: 250, loss is 4.760657043457031 and perplexity is 116.82265825122137
At time: 86.93786334991455 and batch: 300, loss is 4.805603113174438 and perplexity is 122.19316510974615
At time: 88.10200500488281 and batch: 350, loss is 4.740722017288208 and perplexity is 114.51685498710165
At time: 89.26525855064392 and batch: 400, loss is 4.768308830261231 and perplexity is 117.71998903979276
At time: 90.43365573883057 and batch: 450, loss is 4.711733484268189 and perplexity is 111.24483402837608
At time: 91.62579417228699 and batch: 500, loss is 4.690272912979126 and perplexity is 108.88289130714713
At time: 92.80340576171875 and batch: 550, loss is 4.701284952163697 and perplexity is 110.08854011514939
At time: 93.96014928817749 and batch: 600, loss is 4.659697484970093 and perplexity is 105.60413048728039
At time: 95.12178587913513 and batch: 650, loss is 4.63741662979126 and perplexity is 103.2771994203282
At time: 96.29718828201294 and batch: 700, loss is 4.637257766723633 and perplexity is 103.26079379077117
At time: 97.46057391166687 and batch: 750, loss is 4.65170958518982 and perplexity is 104.76393544096835
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.70121552223383 and perplexity of 110.08089694086566
Finished 5 epochs...
Completing Train Step...
At time: 100.83800601959229 and batch: 50, loss is 4.719066696166992 and perplexity is 112.06361444487531
At time: 102.02637314796448 and batch: 100, loss is 4.696182861328125 and perplexity is 109.52828882535206
At time: 103.18713068962097 and batch: 150, loss is 4.671237735748291 and perplexity is 106.82988780617752
At time: 104.35402846336365 and batch: 200, loss is 4.663754472732544 and perplexity is 106.03343540589948
At time: 105.51576471328735 and batch: 250, loss is 4.66533088684082 and perplexity is 106.20071982954218
At time: 106.73110628128052 and batch: 300, loss is 4.713711252212525 and perplexity is 111.46506820935367
At time: 107.88792848587036 and batch: 350, loss is 4.6511487865448 and perplexity is 104.7052004387203
At time: 109.04363346099854 and batch: 400, loss is 4.682303848266602 and perplexity is 108.01864469256505
At time: 110.20246481895447 and batch: 450, loss is 4.6273736476898195 and perplexity is 102.2451793090584
At time: 111.36134314537048 and batch: 500, loss is 4.603327751159668 and perplexity is 99.81592614127273
At time: 112.5209493637085 and batch: 550, loss is 4.61622311592102 and perplexity is 101.1114239237253
At time: 113.67809534072876 and batch: 600, loss is 4.5748517704010006 and perplexity is 97.01365777549843
At time: 114.83537149429321 and batch: 650, loss is 4.554816741943359 and perplexity is 95.08932775335212
At time: 115.99686670303345 and batch: 700, loss is 4.556312236785889 and perplexity is 95.23163973948408
At time: 117.15803813934326 and batch: 750, loss is 4.572426862716675 and perplexity is 96.77869360961569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.6757294410883 and perplexity of 107.31081546780591
Finished 6 epochs...
Completing Train Step...
At time: 120.51803135871887 and batch: 50, loss is 4.641886110305786 and perplexity is 103.73982793516657
At time: 121.71610736846924 and batch: 100, loss is 4.619592971801758 and perplexity is 101.45272960273238
At time: 122.88552570343018 and batch: 150, loss is 4.592141017913819 and perplexity is 98.70553440968139
At time: 124.05286526679993 and batch: 200, loss is 4.590355033874512 and perplexity is 98.52940522940403
At time: 125.22059798240662 and batch: 250, loss is 4.5869058322906495 and perplexity is 98.19014287730091
At time: 126.39845824241638 and batch: 300, loss is 4.636709976196289 and perplexity is 103.20424399622155
At time: 127.56385207176208 and batch: 350, loss is 4.576462488174439 and perplexity is 97.17004531262535
At time: 128.72957038879395 and batch: 400, loss is 4.609394006729126 and perplexity is 100.42327536444266
At time: 129.90307211875916 and batch: 450, loss is 4.556082649230957 and perplexity is 95.20977824982315
At time: 131.07652592658997 and batch: 500, loss is 4.531177997589111 and perplexity is 92.86789482823724
At time: 132.2456660270691 and batch: 550, loss is 4.544862899780274 and perplexity is 94.14751867904826
At time: 133.41460585594177 and batch: 600, loss is 4.503038711547852 and perplexity is 90.29108341683809
At time: 134.5847668647766 and batch: 650, loss is 4.485320339202881 and perplexity is 88.7053620670742
At time: 135.75268626213074 and batch: 700, loss is 4.489285802841186 and perplexity is 89.0578183192384
At time: 136.97917199134827 and batch: 750, loss is 4.504868831634521 and perplexity is 90.45647824227892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.655559096225472 and perplexity of 105.16800259809679
Finished 7 epochs...
Completing Train Step...
At time: 140.3533911705017 and batch: 50, loss is 4.573769578933716 and perplexity is 96.90872721056537
At time: 141.5478789806366 and batch: 100, loss is 4.554383163452148 and perplexity is 95.04810800273897
At time: 142.72439217567444 and batch: 150, loss is 4.524843549728393 and perplexity is 92.28148723548412
At time: 143.89220118522644 and batch: 200, loss is 4.5253503799438475 and perplexity is 92.32827013603446
At time: 145.0610477924347 and batch: 250, loss is 4.517561597824097 and perplexity is 91.6119386527339
At time: 146.23021054267883 and batch: 300, loss is 4.571528663635254 and perplexity is 96.69180610289523
At time: 147.40014028549194 and batch: 350, loss is 4.51290696144104 and perplexity is 91.18650926760543
At time: 148.56627893447876 and batch: 400, loss is 4.545099363327027 and perplexity is 94.16978376757027
At time: 149.7335877418518 and batch: 450, loss is 4.494123048782349 and perplexity is 89.48965650113196
At time: 150.90392923355103 and batch: 500, loss is 4.470265283584594 and perplexity is 87.37990039221357
At time: 152.07111191749573 and batch: 550, loss is 4.483750238418579 and perplexity is 88.56619499027887
At time: 153.23741149902344 and batch: 600, loss is 4.441172399520874 and perplexity is 84.87439006005755
At time: 154.40720653533936 and batch: 650, loss is 4.425021200180054 and perplexity is 83.51457772735858
At time: 155.5752158164978 and batch: 700, loss is 4.429056406021118 and perplexity is 83.85225708387452
At time: 156.74046659469604 and batch: 750, loss is 4.445451383590698 and perplexity is 85.23834434502989
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.643470231876817 and perplexity of 103.90429466762971
Finished 8 epochs...
Completing Train Step...
At time: 160.11062788963318 and batch: 50, loss is 4.514251461029053 and perplexity is 91.30919194667291
At time: 161.32412266731262 and batch: 100, loss is 4.497512426376343 and perplexity is 89.79348534224194
At time: 162.49522733688354 and batch: 150, loss is 4.465763235092163 and perplexity is 86.98739604314457
At time: 163.66509222984314 and batch: 200, loss is 4.468786087036133 and perplexity is 87.25074389262245
At time: 164.83219385147095 and batch: 250, loss is 4.458369235992432 and perplexity is 86.34658331896502
At time: 166.04157996177673 and batch: 300, loss is 4.513607559204101 and perplexity is 91.25041671611342
At time: 167.21096968650818 and batch: 350, loss is 4.457239694595337 and perplexity is 86.24910634111338
At time: 168.37387824058533 and batch: 400, loss is 4.489286556243896 and perplexity is 89.05788541566534
At time: 169.55165767669678 and batch: 450, loss is 4.439567012786865 and perplexity is 84.73824315365732
At time: 170.74177956581116 and batch: 500, loss is 4.414948759078979 and perplexity is 82.67760432097268
At time: 171.94181275367737 and batch: 550, loss is 4.428358240127563 and perplexity is 83.79373472941407
At time: 173.13540244102478 and batch: 600, loss is 4.3865026760101316 and perplexity is 80.35888590177757
At time: 174.31590867042542 and batch: 650, loss is 4.370801372528076 and perplexity is 79.10700048213732
At time: 175.49423098564148 and batch: 700, loss is 4.377013807296753 and perplexity is 79.59997726993792
At time: 176.6861448287964 and batch: 750, loss is 4.391959276199341 and perplexity is 80.7985707149627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.637160190316134 and perplexity of 103.25071846504264
Finished 9 epochs...
Completing Train Step...
At time: 180.11442613601685 and batch: 50, loss is 4.4629574966430665 and perplexity is 86.74367423119374
At time: 181.33846235275269 and batch: 100, loss is 4.446987085342407 and perplexity is 85.36934558342305
At time: 182.50963950157166 and batch: 150, loss is 4.414204015731811 and perplexity is 82.61605364776693
At time: 183.67926931381226 and batch: 200, loss is 4.418247537612915 and perplexity is 82.95078976875718
At time: 184.85137963294983 and batch: 250, loss is 4.406111993789673 and perplexity is 81.9502203310526
At time: 186.0241949558258 and batch: 300, loss is 4.462104988098145 and perplexity is 86.66975602012381
At time: 187.19790244102478 and batch: 350, loss is 4.407570781707764 and perplexity is 82.06985556235551
At time: 188.36515188217163 and batch: 400, loss is 4.4401478099822995 and perplexity is 84.78747318257447
At time: 189.53910279273987 and batch: 450, loss is 4.39065839767456 and perplexity is 80.69352992695255
At time: 190.70843744277954 and batch: 500, loss is 4.366762161254883 and perplexity is 78.78811505037403
At time: 191.87771344184875 and batch: 550, loss is 4.378583316802978 and perplexity is 79.72500828397462
At time: 193.04607915878296 and batch: 600, loss is 4.338908863067627 and perplexity is 76.62388655575201
At time: 194.2167718410492 and batch: 650, loss is 4.322578783035278 and perplexity is 75.38277367292487
At time: 195.38358116149902 and batch: 700, loss is 4.329583215713501 and perplexity is 75.91264077908802
At time: 196.60636949539185 and batch: 750, loss is 4.343282766342163 and perplexity is 76.95976604077909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.63498315145803 and perplexity of 103.02618213965225
Finished 10 epochs...
Completing Train Step...
At time: 200.00583386421204 and batch: 50, loss is 4.418195810317993 and perplexity is 82.94649905976512
At time: 201.22738337516785 and batch: 100, loss is 4.401518535614014 and perplexity is 81.57464866821397
At time: 202.3913929462433 and batch: 150, loss is 4.366695947647095 and perplexity is 78.78289837773501
At time: 203.55767941474915 and batch: 200, loss is 4.371979141235352 and perplexity is 79.20022511958571
At time: 204.72909259796143 and batch: 250, loss is 4.360592308044434 and perplexity is 78.30350048347714
At time: 205.90055060386658 and batch: 300, loss is 4.415942544937134 and perplexity is 82.75980899508876
At time: 207.0662100315094 and batch: 350, loss is 4.362076530456543 and perplexity is 78.41980658455117
At time: 208.23202681541443 and batch: 400, loss is 4.394370546340943 and perplexity is 80.99363297540268
At time: 209.39600038528442 and batch: 450, loss is 4.3464768409729 and perplexity is 77.20597427148886
At time: 210.56816744804382 and batch: 500, loss is 4.322058830261231 and perplexity is 75.34358837877224
At time: 211.73974418640137 and batch: 550, loss is 4.33400526046753 and perplexity is 76.2490731853584
At time: 212.9083709716797 and batch: 600, loss is 4.294415969848632 and perplexity is 73.28939872477439
At time: 214.07423758506775 and batch: 650, loss is 4.278471260070801 and perplexity is 72.13008753379111
At time: 215.24001097679138 and batch: 700, loss is 4.286309928894043 and perplexity is 72.69771321054358
At time: 216.41279911994934 and batch: 750, loss is 4.300709533691406 and perplexity is 73.75210474230434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.63056236089662 and perplexity of 102.57173022456692
Finished 11 epochs...
Completing Train Step...
At time: 219.7895107269287 and batch: 50, loss is 4.373525075912475 and perplexity is 79.32275818368687
At time: 221.00407433509827 and batch: 100, loss is 4.35983775138855 and perplexity is 78.24443834167671
At time: 222.17596340179443 and batch: 150, loss is 4.323748502731323 and perplexity is 75.47100197916656
At time: 223.34043407440186 and batch: 200, loss is 4.329635772705078 and perplexity is 75.91663062395627
At time: 224.50235247612 and batch: 250, loss is 4.316320810317993 and perplexity is 74.9125033361301
At time: 225.72103190422058 and batch: 300, loss is 4.374094934463501 and perplexity is 79.36797381776302
At time: 226.88256549835205 and batch: 350, loss is 4.32129825592041 and perplexity is 75.28630576531033
At time: 228.0501582622528 and batch: 400, loss is 4.353748207092285 and perplexity is 77.7694131797691
At time: 229.21536803245544 and batch: 450, loss is 4.305448017120361 and perplexity is 74.10240716413546
At time: 230.3800446987152 and batch: 500, loss is 4.281743812561035 and perplexity is 72.36652369523372
At time: 231.54833126068115 and batch: 550, loss is 4.292373552322387 and perplexity is 73.13986393055484
At time: 232.71636176109314 and batch: 600, loss is 4.253409252166748 and perplexity is 70.3448272566386
At time: 233.88452076911926 and batch: 650, loss is 4.238394432067871 and perplexity is 69.29650223120467
At time: 235.05654406547546 and batch: 700, loss is 4.246500301361084 and perplexity is 69.86049335183233
At time: 236.23098826408386 and batch: 750, loss is 4.2602746868133545 and perplexity is 70.8294366946373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.628880345544149 and perplexity of 102.3993480150028
Finished 12 epochs...
Completing Train Step...
At time: 239.60144448280334 and batch: 50, loss is 4.332767486572266 and perplexity is 76.15475245894022
At time: 240.81918954849243 and batch: 100, loss is 4.320669088363648 and perplexity is 75.23895296224009
At time: 241.98143339157104 and batch: 150, loss is 4.285291013717651 and perplexity is 72.62367813140243
At time: 243.14109683036804 and batch: 200, loss is 4.292408056259156 and perplexity is 73.14238758733293
At time: 244.2972764968872 and batch: 250, loss is 4.277635612487793 and perplexity is 72.069837377936
At time: 245.46584606170654 and batch: 300, loss is 4.335529251098633 and perplexity is 76.36536464956644
At time: 246.62516856193542 and batch: 350, loss is 4.281524248123169 and perplexity is 72.35063632435293
At time: 247.78375387191772 and batch: 400, loss is 4.313965654373169 and perplexity is 74.73628030636995
At time: 248.94548916816711 and batch: 450, loss is 4.2685842609405515 and perplexity is 71.42045128828266
At time: 250.11251187324524 and batch: 500, loss is 4.243322792053223 and perplexity is 69.6388632862285
At time: 251.27712655067444 and batch: 550, loss is 4.254651708602905 and perplexity is 70.43228195809198
At time: 252.4550383090973 and batch: 600, loss is 4.215700464248657 and perplexity is 67.74159982492215
At time: 253.64787983894348 and batch: 650, loss is 4.200260353088379 and perplexity is 66.70369529348463
At time: 254.8390235900879 and batch: 700, loss is 4.208622827529907 and perplexity is 67.26384208283929
At time: 256.1049497127533 and batch: 750, loss is 4.223604669570923 and perplexity is 68.279165053004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.6299591064453125 and perplexity of 102.50987203172161
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 259.4987964630127 and batch: 50, loss is 4.308398246765137 and perplexity is 74.32134908817449
At time: 260.7059464454651 and batch: 100, loss is 4.3062742233276365 and perplexity is 74.16365633164556
At time: 261.87452602386475 and batch: 150, loss is 4.2672925424575805 and perplexity is 71.32825572946444
At time: 263.04742646217346 and batch: 200, loss is 4.2680252838134765 and perplexity is 71.38054004538229
At time: 264.21620750427246 and batch: 250, loss is 4.244961266517639 and perplexity is 69.75305831272406
At time: 265.3864686489105 and batch: 300, loss is 4.292240695953369 and perplexity is 73.13014747926306
At time: 266.5561058521271 and batch: 350, loss is 4.233706092834472 and perplexity is 68.97237711899999
At time: 267.7252712249756 and batch: 400, loss is 4.249909725189209 and perplexity is 70.09908387950772
At time: 268.89194774627686 and batch: 450, loss is 4.196346426010132 and perplexity is 66.44313213947505
At time: 270.0603847503662 and batch: 500, loss is 4.161243786811829 and perplexity is 64.15126349481696
At time: 271.23195242881775 and batch: 550, loss is 4.165297231674194 and perplexity is 64.41182483299224
At time: 272.4007201194763 and batch: 600, loss is 4.118948407173157 and perplexity is 61.49454103247553
At time: 273.57334184646606 and batch: 650, loss is 4.09359384059906 and perplexity is 59.95497360587607
At time: 274.7424123287201 and batch: 700, loss is 4.08929491519928 and perplexity is 59.697784860700025
At time: 275.91339015960693 and batch: 750, loss is 4.098187546730042 and perplexity is 60.23102269447234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.593934170035428 and perplexity of 98.88268723163398
Finished 14 epochs...
Completing Train Step...
At time: 279.27979040145874 and batch: 50, loss is 4.261701765060425 and perplexity is 70.93058800124449
At time: 280.47698163986206 and batch: 100, loss is 4.25650876045227 and perplexity is 70.56319988061824
At time: 281.6462824344635 and batch: 150, loss is 4.218852171897888 and perplexity is 67.95543834450095
At time: 282.81589007377625 and batch: 200, loss is 4.2235562610626225 and perplexity is 68.27585984047661
At time: 283.99340057373047 and batch: 250, loss is 4.202880239486694 and perplexity is 66.87868051811763
At time: 285.2188460826874 and batch: 300, loss is 4.25408863067627 and perplexity is 70.39263425821868
At time: 286.388258934021 and batch: 350, loss is 4.199691371917725 and perplexity is 66.66575294211493
At time: 287.55639123916626 and batch: 400, loss is 4.219686069488525 and perplexity is 68.01212985497894
At time: 288.7255132198334 and batch: 450, loss is 4.171044702529907 and perplexity is 64.78309583157642
At time: 289.8944492340088 and batch: 500, loss is 4.140005965232849 and perplexity is 62.80319608377256
At time: 291.0654366016388 and batch: 550, loss is 4.148280363082886 and perplexity is 63.325010587633585
At time: 292.23137974739075 and batch: 600, loss is 4.105969967842102 and perplexity is 60.7015945963874
At time: 293.39709663391113 and batch: 650, loss is 4.085031809806824 and perplexity is 59.443828617874885
At time: 294.56678891181946 and batch: 700, loss is 4.084588465690612 and perplexity is 59.41748038731012
At time: 295.739217042923 and batch: 750, loss is 4.097576913833618 and perplexity is 60.194254877591554
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.592378128406613 and perplexity of 98.72894130248712
Finished 15 epochs...
Completing Train Step...
At time: 299.1166799068451 and batch: 50, loss is 4.238738136291504 and perplexity is 69.32032382526128
At time: 300.340167760849 and batch: 100, loss is 4.233992509841919 and perplexity is 68.9921348101852
At time: 301.5138876438141 and batch: 150, loss is 4.1962611103057865 and perplexity is 66.43746373866288
At time: 302.6822762489319 and batch: 200, loss is 4.2024569320678715 and perplexity is 66.85037626761552
At time: 303.8505744934082 and batch: 250, loss is 4.182563333511353 and perplexity is 65.53362262866972
At time: 305.0261778831482 and batch: 300, loss is 4.234684286117553 and perplexity is 69.03987844430497
At time: 306.19623255729675 and batch: 350, loss is 4.181535882949829 and perplexity is 65.46632464989453
At time: 307.36541652679443 and batch: 400, loss is 4.202833395004273 and perplexity is 66.87554769432208
At time: 308.53508710861206 and batch: 450, loss is 4.156194295883179 and perplexity is 63.82814874079554
At time: 309.7064187526703 and batch: 500, loss is 4.126216292381287 and perplexity is 61.943104376567746
At time: 310.8748881816864 and batch: 550, loss is 4.1361703062057495 and perplexity is 62.56276583677982
At time: 312.04520988464355 and batch: 600, loss is 4.0955373525619505 and perplexity is 60.071610119830304
At time: 313.21443724632263 and batch: 650, loss is 4.0763219499588015 and perplexity is 58.92832942353177
At time: 314.38208532333374 and batch: 700, loss is 4.076919937133789 and perplexity is 58.96357834692218
At time: 315.60276055336 and batch: 750, loss is 4.091737871170044 and perplexity is 59.84380220501874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.591882395189862 and perplexity of 98.68001021621326
Finished 16 epochs...
Completing Train Step...
At time: 318.99086475372314 and batch: 50, loss is 4.220864715576172 and perplexity is 68.09233934576066
At time: 320.2054080963135 and batch: 100, loss is 4.21676417350769 and perplexity is 67.813695529514
At time: 321.38279700279236 and batch: 150, loss is 4.179383935928345 and perplexity is 65.3255960620974
At time: 322.55496859550476 and batch: 200, loss is 4.186435289382935 and perplexity is 65.78785779964906
At time: 323.7293322086334 and batch: 250, loss is 4.166948709487915 and perplexity is 64.5182874187474
At time: 324.89719343185425 and batch: 300, loss is 4.219664812088013 and perplexity is 68.01068410926133
At time: 326.0665183067322 and batch: 350, loss is 4.167264499664307 and perplexity is 64.53866487744314
At time: 327.25342988967896 and batch: 400, loss is 4.189089393615722 and perplexity is 65.96269755032745
At time: 328.4308886528015 and batch: 450, loss is 4.1432591247558594 and perplexity is 63.00783758443382
At time: 329.6113483905792 and batch: 500, loss is 4.114668116569519 and perplexity is 61.23188904074667
At time: 330.7849669456482 and batch: 550, loss is 4.125156302452087 and perplexity is 61.87748009642775
At time: 331.9563019275665 and batch: 600, loss is 4.085330624580383 and perplexity is 59.46159396620488
At time: 333.127153635025 and batch: 650, loss is 4.0670324897766115 and perplexity is 58.38345178163169
At time: 334.2928500175476 and batch: 700, loss is 4.068212456703186 and perplexity is 58.45238298406497
At time: 335.46094036102295 and batch: 750, loss is 4.0841230249404905 and perplexity is 59.38983150562556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.592143214026163 and perplexity of 98.705751178362
Annealing...
Finished 17 epochs...
Completing Train Step...
At time: 338.8492925167084 and batch: 50, loss is 4.216416635513306 and perplexity is 67.79013178866265
At time: 340.0751748085022 and batch: 100, loss is 4.222612771987915 and perplexity is 68.21147269171288
At time: 341.24961709976196 and batch: 150, loss is 4.18805609703064 and perplexity is 65.89457372232663
At time: 342.4195740222931 and batch: 200, loss is 4.192322072982788 and perplexity is 66.17627883560391
At time: 343.58666729927063 and batch: 250, loss is 4.173498740196228 and perplexity is 64.9422712202041
At time: 344.7999849319458 and batch: 300, loss is 4.221154689788818 and perplexity is 68.11208723130008
At time: 345.96410369873047 and batch: 350, loss is 4.169091272354126 and perplexity is 64.65667009942078
At time: 347.1312074661255 and batch: 400, loss is 4.183518662452697 and perplexity is 65.59625870926457
At time: 348.2962968349457 and batch: 450, loss is 4.1315859508514405 and perplexity is 62.27661230213748
At time: 349.4662401676178 and batch: 500, loss is 4.09532564163208 and perplexity is 60.05889364954838
At time: 350.63145685195923 and batch: 550, loss is 4.101090803146362 and perplexity is 60.40614288397264
At time: 351.8010175228119 and batch: 600, loss is 4.058329334259033 and perplexity is 57.87753624529235
At time: 352.96943736076355 and batch: 650, loss is 4.035209050178528 and perplexity is 56.5547417814981
At time: 354.1370189189911 and batch: 700, loss is 4.02924578666687 and perplexity is 56.2184945153742
At time: 355.30604434013367 and batch: 750, loss is 4.04396388053894 and perplexity is 57.05204266793817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.57467225540516 and perplexity of 96.99624393219612
Finished 18 epochs...
Completing Train Step...
At time: 358.6513068675995 and batch: 50, loss is 4.207511348724365 and perplexity is 67.18912128097112
At time: 359.8470826148987 and batch: 100, loss is 4.206321649551391 and perplexity is 67.10923396930723
At time: 361.0180971622467 and batch: 150, loss is 4.169559950828552 and perplexity is 64.68698039126032
At time: 362.186151266098 and batch: 200, loss is 4.173165311813355 and perplexity is 64.92062123329166
At time: 363.3480157852173 and batch: 250, loss is 4.154903111457824 and perplexity is 63.745788012129644
At time: 364.5137917995453 and batch: 300, loss is 4.204248161315918 and perplexity is 66.97022792568129
At time: 365.6758358478546 and batch: 350, loss is 4.153249821662903 and perplexity is 63.64048482366941
At time: 366.8426196575165 and batch: 400, loss is 4.170232186317444 and perplexity is 64.73047989446532
At time: 368.00592255592346 and batch: 450, loss is 4.121197881698609 and perplexity is 61.63302713806923
At time: 369.1703197956085 and batch: 500, loss is 4.088010063171387 and perplexity is 59.621131295550626
At time: 370.33819818496704 and batch: 550, loss is 4.096387877464294 and perplexity is 60.12272425400179
At time: 371.50418853759766 and batch: 600, loss is 4.056331362724304 and perplexity is 57.76201401886333
At time: 372.6692955493927 and batch: 650, loss is 4.035971655845642 and perplexity is 56.59788719746119
At time: 373.8335747718811 and batch: 700, loss is 4.031873750686645 and perplexity is 56.36642899431184
At time: 375.0524549484253 and batch: 750, loss is 4.0479212713241575 and perplexity is 57.27826723012826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.573631109193314 and perplexity of 96.8953092132806
Finished 19 epochs...
Completing Train Step...
At time: 378.41033387184143 and batch: 50, loss is 4.200164260864258 and perplexity is 66.6972858949984
At time: 379.59978675842285 and batch: 100, loss is 4.197959480285644 and perplexity is 66.5503950050053
At time: 380.7580108642578 and batch: 150, loss is 4.160825228691101 and perplexity is 64.12441808110111
At time: 381.9241018295288 and batch: 200, loss is 4.164629096984863 and perplexity is 64.3688034320638
At time: 383.08238649368286 and batch: 250, loss is 4.146605610847473 and perplexity is 63.219045641876775
At time: 384.24398040771484 and batch: 300, loss is 4.196539440155029 and perplexity is 66.45595784154038
At time: 385.4078645706177 and batch: 350, loss is 4.146248302459717 and perplexity is 63.19646098168752
At time: 386.56888723373413 and batch: 400, loss is 4.164064054489136 and perplexity is 64.33244259640162
At time: 387.72788071632385 and batch: 450, loss is 4.116012334823608 and perplexity is 61.31425340916919
At time: 388.8892424106598 and batch: 500, loss is 4.0838547039031985 and perplexity is 59.37389810216064
At time: 390.04965233802795 and batch: 550, loss is 4.093294353485107 and perplexity is 59.93702055235201
At time: 391.21149015426636 and batch: 600, loss is 4.0542440366745 and perplexity is 57.641571607334065
At time: 392.3780200481415 and batch: 650, loss is 4.0348708534240725 and perplexity is 56.53561838529606
At time: 393.541615486145 and batch: 700, loss is 4.031425976753235 and perplexity is 56.3411952266221
At time: 394.7006275653839 and batch: 750, loss is 4.0480950784683225 and perplexity is 57.28822346738577
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.573471424191497 and perplexity of 96.87983772096833
Finished 20 epochs...
Completing Train Step...
At time: 398.0697023868561 and batch: 50, loss is 4.193818397521973 and perplexity is 66.27537414642082
At time: 399.2926630973816 and batch: 100, loss is 4.191425971984863 and perplexity is 66.11700476784243
At time: 400.46327686309814 and batch: 150, loss is 4.154160313606262 and perplexity is 63.69845535922084
At time: 401.6321268081665 and batch: 200, loss is 4.158291282653809 and perplexity is 63.962135959912686
At time: 402.81409788131714 and batch: 250, loss is 4.139890022277832 and perplexity is 62.795914917742245
At time: 404.0624432563782 and batch: 300, loss is 4.190844020843506 and perplexity is 66.07853909511132
At time: 405.25937724113464 and batch: 350, loss is 4.1409922790527345 and perplexity is 62.86517030199282
At time: 406.4509994983673 and batch: 400, loss is 4.159300446510315 and perplexity is 64.02671681656071
At time: 407.6399781703949 and batch: 450, loss is 4.111741585731506 and perplexity is 61.052953986385084
At time: 408.8370575904846 and batch: 500, loss is 4.080220527648926 and perplexity is 59.15851449959298
At time: 410.01487588882446 and batch: 550, loss is 4.090321202278137 and perplexity is 59.7590833754955
At time: 411.2091872692108 and batch: 600, loss is 4.051873278617859 and perplexity is 57.50507924616055
At time: 412.3816559314728 and batch: 650, loss is 4.033082051277161 and perplexity is 56.434577747548914
At time: 413.5576195716858 and batch: 700, loss is 4.029985194206238 and perplexity is 56.26007826584552
At time: 414.7312104701996 and batch: 750, loss is 4.047117385864258 and perplexity is 57.23224056649521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.573571493459302 and perplexity of 96.88953290048083
Annealing...
Finished 21 epochs...
Completing Train Step...
At time: 418.1308243274689 and batch: 50, loss is 4.194557867050171 and perplexity is 66.32440089072969
At time: 419.3324604034424 and batch: 100, loss is 4.198986253738403 and perplexity is 66.61876227671046
At time: 420.4994282722473 and batch: 150, loss is 4.163964929580689 and perplexity is 64.32606596496582
At time: 421.6732997894287 and batch: 200, loss is 4.166628894805908 and perplexity is 64.49765682233257
At time: 422.84262251853943 and batch: 250, loss is 4.148459854125977 and perplexity is 63.336377879970755
At time: 424.02141523361206 and batch: 300, loss is 4.19724684715271 and perplexity is 66.50298588317673
At time: 425.1940155029297 and batch: 350, loss is 4.148126792907715 and perplexity is 63.31528650134886
At time: 426.3666558265686 and batch: 400, loss is 4.165092687606812 and perplexity is 64.39865112370136
At time: 427.5386185646057 and batch: 450, loss is 4.115670084953308 and perplexity is 61.29327220450443
At time: 428.707227230072 and batch: 500, loss is 4.078745012283325 and perplexity is 59.0712895691304
At time: 429.8746919631958 and batch: 550, loss is 4.082630782127381 and perplexity is 59.301273547810844
At time: 431.0436215400696 and batch: 600, loss is 4.04133909702301 and perplexity is 56.90248976465277
At time: 432.2163860797882 and batch: 650, loss is 4.020778050422669 and perplexity is 55.74446095977991
At time: 433.3857262134552 and batch: 700, loss is 4.015441346168518 and perplexity is 55.4477616602138
At time: 434.6157736778259 and batch: 750, loss is 4.031744675636292 and perplexity is 56.35915396417367
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5649158566497094 and perplexity of 96.05451132808749
Finished 22 epochs...
Completing Train Step...
At time: 437.98965430259705 and batch: 50, loss is 4.191724815368652 and perplexity is 66.13676634993523
At time: 439.2208299636841 and batch: 100, loss is 4.192164630889892 and perplexity is 66.16586072391019
At time: 440.3917305469513 and batch: 150, loss is 4.155512599945069 and perplexity is 63.784652178461236
At time: 441.56302666664124 and batch: 200, loss is 4.157294645309448 and perplexity is 63.89842066238636
At time: 442.7337222099304 and batch: 250, loss is 4.138817968368531 and perplexity is 62.72863038443416
At time: 443.9043209552765 and batch: 300, loss is 4.189187469482422 and perplexity is 65.96916721631344
At time: 445.0745162963867 and batch: 350, loss is 4.140891742706299 and perplexity is 62.85885038514861
At time: 446.24644565582275 and batch: 400, loss is 4.1590125846862795 and perplexity is 64.00828862158505
At time: 447.4174768924713 and batch: 450, loss is 4.111003646850586 and perplexity is 61.00791725706404
At time: 448.58897519111633 and batch: 500, loss is 4.075759768486023 and perplexity is 58.89521031885386
At time: 449.75889825820923 and batch: 550, loss is 4.081440815925598 and perplexity is 59.23074900579899
At time: 450.9297151565552 and batch: 600, loss is 4.042017221450806 and perplexity is 56.941089819305375
At time: 452.1017723083496 and batch: 650, loss is 4.023086552619934 and perplexity is 55.87329582095561
At time: 453.2724642753601 and batch: 700, loss is 4.018506064414978 and perplexity is 55.61795408981043
At time: 454.4441018104553 and batch: 750, loss is 4.034938530921936 and perplexity is 56.53944470396493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.564167821130087 and perplexity of 95.9826860090882
Finished 23 epochs...
Completing Train Step...
At time: 457.8205680847168 and batch: 50, loss is 4.189827527999878 and perplexity is 66.01140485951935
At time: 459.0404624938965 and batch: 100, loss is 4.189005126953125 and perplexity is 65.9571393281387
At time: 460.2114772796631 and batch: 150, loss is 4.151822576522827 and perplexity is 63.549719038913054
At time: 461.379549741745 and batch: 200, loss is 4.153446063995362 and perplexity is 63.65297500636123
At time: 462.54793667793274 and batch: 250, loss is 4.1349460411071775 and perplexity is 62.486219292287956
At time: 463.75750184059143 and batch: 300, loss is 4.18574405670166 and perplexity is 65.74239879549158
At time: 464.9258954524994 and batch: 350, loss is 4.137807111740113 and perplexity is 62.66525277085564
At time: 466.09733748435974 and batch: 400, loss is 4.156401052474975 and perplexity is 63.84134699565596
At time: 467.2658483982086 and batch: 450, loss is 4.108934769630432 and perplexity is 60.88182984146664
At time: 468.4349699020386 and batch: 500, loss is 4.0743542528152465 and perplexity is 58.81249032355948
At time: 469.6055574417114 and batch: 550, loss is 4.080675411224365 and perplexity is 59.185430857626685
At time: 470.7744884490967 and batch: 600, loss is 4.041959090232849 and perplexity is 56.9377798606093
At time: 471.9432189464569 and batch: 650, loss is 4.023707242012024 and perplexity is 55.90798654794216
At time: 473.11724972724915 and batch: 700, loss is 4.019495811462402 and perplexity is 55.67302904629645
At time: 474.2881100177765 and batch: 750, loss is 4.036052885055542 and perplexity is 56.60248478584699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.563961295194404 and perplexity of 95.96286514188283
Finished 24 epochs...
Completing Train Step...
At time: 477.66240191459656 and batch: 50, loss is 4.187801675796509 and perplexity is 65.87781087606008
At time: 478.88411808013916 and batch: 100, loss is 4.186518955230713 and perplexity is 65.79336222680843
At time: 480.05150628089905 and batch: 150, loss is 4.149153537750244 and perplexity is 63.3803285302873
At time: 481.2219970226288 and batch: 200, loss is 4.150749025344848 and perplexity is 63.481531770988404
At time: 482.3841743469238 and batch: 250, loss is 4.132303619384766 and perplexity is 62.32132230871227
At time: 483.5566437244415 and batch: 300, loss is 4.183368730545044 and perplexity is 65.58642447431257
At time: 484.7392725944519 and batch: 350, loss is 4.1356957197189335 and perplexity is 62.533081438000266
At time: 485.9422745704651 and batch: 400, loss is 4.154562554359436 and perplexity is 63.72408262770091
At time: 487.14419531822205 and batch: 450, loss is 4.107421836853027 and perplexity is 60.78978936863167
At time: 488.3348834514618 and batch: 500, loss is 4.073217616081238 and perplexity is 58.74567986344374
At time: 489.5053880214691 and batch: 550, loss is 4.079919075965881 and perplexity is 59.14068375351987
At time: 490.6827929019928 and batch: 600, loss is 4.041581568717956 and perplexity is 56.916288680648336
At time: 491.855149269104 and batch: 650, loss is 4.023725619316101 and perplexity is 55.90901399545215
At time: 493.03071808815 and batch: 700, loss is 4.019745535850525 and perplexity is 55.68693369550273
At time: 494.280912399292 and batch: 750, loss is 4.036439232826233 and perplexity is 56.624357254577234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.563896001771439 and perplexity of 95.95659960249165
Finished 25 epochs...
Completing Train Step...
At time: 497.73129415512085 and batch: 50, loss is 4.185837831497192 and perplexity is 65.74856406456533
At time: 498.9452121257782 and batch: 100, loss is 4.184351282119751 and perplexity is 65.6508981881468
At time: 500.111483335495 and batch: 150, loss is 4.146910972595215 and perplexity is 63.23835326789975
At time: 501.2932035923004 and batch: 200, loss is 4.148533277511596 and perplexity is 63.34102842199476
At time: 502.4572319984436 and batch: 250, loss is 4.130145044326782 and perplexity is 62.18694214381031
At time: 503.6294949054718 and batch: 300, loss is 4.181412343978882 and perplexity is 65.45823750706451
At time: 504.8033285140991 and batch: 350, loss is 4.133949093818664 and perplexity is 62.423954867726756
At time: 505.9696502685547 and batch: 400, loss is 4.15301134109497 and perplexity is 63.62530961427336
At time: 507.1337707042694 and batch: 450, loss is 4.106103506088257 and perplexity is 60.709701122122134
At time: 508.3000464439392 and batch: 500, loss is 4.072152109146118 and perplexity is 58.68311926943306
At time: 509.4671039581299 and batch: 550, loss is 4.079126286506653 and perplexity is 59.09381622332523
At time: 510.6358766555786 and batch: 600, loss is 4.0410435771942135 and perplexity is 56.88567643507793
At time: 511.8043100833893 and batch: 650, loss is 4.023466687202454 and perplexity is 55.894539230358006
At time: 512.9674746990204 and batch: 700, loss is 4.019660506248474 and perplexity is 55.68219885899465
At time: 514.1349382400513 and batch: 750, loss is 4.036487512588501 and perplexity is 56.627091131078984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.563887130382449 and perplexity of 95.9557483379464
Finished 26 epochs...
Completing Train Step...
At time: 517.4993753433228 and batch: 50, loss is 4.183954725265503 and perplexity is 65.62486903584322
At time: 518.692581653595 and batch: 100, loss is 4.18239312171936 and perplexity is 65.52246898259429
At time: 519.8612926006317 and batch: 150, loss is 4.144910268783569 and perplexity is 63.111958534847766
At time: 521.0204646587372 and batch: 200, loss is 4.146571998596191 and perplexity is 63.21692074314038
At time: 522.1781015396118 and batch: 250, loss is 4.128245682716369 and perplexity is 62.068938754262916
At time: 523.3940057754517 and batch: 300, loss is 4.1796746349334715 and perplexity is 65.34458890834895
At time: 524.5544893741608 and batch: 350, loss is 4.132390818595886 and perplexity is 62.32675691579684
At time: 525.7123200893402 and batch: 400, loss is 4.151608929634095 and perplexity is 63.53614328942054
At time: 526.8716022968292 and batch: 450, loss is 4.104880781173706 and perplexity is 60.635515221722216
At time: 528.0286693572998 and batch: 500, loss is 4.071119565963745 and perplexity is 58.62255768631651
At time: 529.1910436153412 and batch: 550, loss is 4.078286080360413 and perplexity is 59.044186088421505
At time: 530.3535571098328 and batch: 600, loss is 4.040408062934875 and perplexity is 56.8495362615655
At time: 531.5151054859161 and batch: 650, loss is 4.0230515623092655 and perplexity is 55.87134083117982
At time: 532.6758379936218 and batch: 700, loss is 4.019392514228821 and perplexity is 55.667278473425135
At time: 533.8385653495789 and batch: 750, loss is 4.036356897354126 and perplexity is 56.6196952533166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.563907002293786 and perplexity of 95.9576551810159
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 537.1923124790192 and batch: 50, loss is 4.184447412490845 and perplexity is 65.65720953670353
At time: 538.4051823616028 and batch: 100, loss is 4.185788221359253 and perplexity is 65.74530235014052
At time: 539.5760912895203 and batch: 150, loss is 4.149485740661621 and perplexity is 63.401387157624065
At time: 540.7563772201538 and batch: 200, loss is 4.150963907241821 and perplexity is 63.4951742686685
At time: 541.9295058250427 and batch: 250, loss is 4.132283134460449 and perplexity is 62.320045674217376
At time: 543.0994238853455 and batch: 300, loss is 4.183479604721069 and perplexity is 65.59369671822917
At time: 544.2688868045807 and batch: 350, loss is 4.135874147415161 and perplexity is 62.54424006713391
At time: 545.4374811649323 and batch: 400, loss is 4.154458241462708 and perplexity is 63.71743573073522
At time: 546.6059262752533 and batch: 450, loss is 4.105808258056641 and perplexity is 60.69177934818026
At time: 547.7782158851624 and batch: 500, loss is 4.069966444969177 and perplexity is 58.55499774417314
At time: 548.9540073871613 and batch: 550, loss is 4.074636545181274 and perplexity is 58.8290949841791
At time: 550.1215116977692 and batch: 600, loss is 4.034932117462159 and perplexity is 56.53908209167328
At time: 551.2981255054474 and batch: 650, loss is 4.016177196502685 and perplexity is 55.4885779296513
At time: 552.4654123783112 and batch: 700, loss is 4.013115482330322 and perplexity is 55.318947576538804
At time: 553.6938335895538 and batch: 750, loss is 4.029667129516602 and perplexity is 56.242186766990855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.561382204987282 and perplexity of 95.71568714034261
Finished 28 epochs...
Completing Train Step...
At time: 557.0627641677856 and batch: 50, loss is 4.1833755397796635 and perplexity is 65.58687106918516
At time: 558.2597243785858 and batch: 100, loss is 4.182957863807678 and perplexity is 65.55948272918579
At time: 559.4251058101654 and batch: 150, loss is 4.146333351135254 and perplexity is 63.20183598555787
At time: 560.5942435264587 and batch: 200, loss is 4.147690172195435 and perplexity is 63.287647770118745
At time: 561.7646951675415 and batch: 250, loss is 4.128730068206787 and perplexity is 62.09901133037287
At time: 562.9318654537201 and batch: 300, loss is 4.18017578125 and perplexity is 65.37734431532569
At time: 564.0982055664062 and batch: 350, loss is 4.13315532207489 and perplexity is 62.37442415685813
At time: 565.2678418159485 and batch: 400, loss is 4.152308740615845 and perplexity is 63.580622141811965
At time: 566.4374554157257 and batch: 450, loss is 4.104519395828247 and perplexity is 60.61360639410982
At time: 567.60467004776 and batch: 500, loss is 4.069189925193786 and perplexity is 58.50954627974117
At time: 568.7713484764099 and batch: 550, loss is 4.0746745681762695 and perplexity is 58.831331885089824
At time: 569.9397728443146 and batch: 600, loss is 4.035433702468872 and perplexity is 56.567448360996764
At time: 571.1082043647766 and batch: 650, loss is 4.017273073196411 and perplexity is 55.54941990053235
At time: 572.2760124206543 and batch: 700, loss is 4.014538083076477 and perplexity is 55.39770035623766
At time: 573.4561479091644 and batch: 750, loss is 4.0311424350738525 and perplexity is 56.325222414095165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560970217682595 and perplexity of 95.67626161434708
Finished 29 epochs...
Completing Train Step...
At time: 576.8121371269226 and batch: 50, loss is 4.182737455368042 and perplexity is 65.54503445821305
At time: 578.026698589325 and batch: 100, loss is 4.181668024063111 and perplexity is 65.47497601450411
At time: 579.1959855556488 and batch: 150, loss is 4.144797973632812 and perplexity is 63.10487176586188
At time: 580.3626298904419 and batch: 200, loss is 4.14614893913269 and perplexity is 63.1901818830293
At time: 581.5300958156586 and batch: 250, loss is 4.1271093511581425 and perplexity is 61.998447918436554
At time: 582.7646112442017 and batch: 300, loss is 4.178715877532959 and perplexity is 65.28196932344157
At time: 583.9380462169647 and batch: 350, loss is 4.131944169998169 and perplexity is 62.2989249732319
At time: 585.1085665225983 and batch: 400, loss is 4.151330933570862 and perplexity is 63.51848294657973
At time: 586.2760407924652 and batch: 450, loss is 4.1038442230224605 and perplexity is 60.57269554791139
At time: 587.4550654888153 and batch: 500, loss is 4.06880823135376 and perplexity is 58.48721780793463
At time: 588.6257419586182 and batch: 550, loss is 4.0746464967727665 and perplexity is 58.829680430213294
At time: 589.7997536659241 and batch: 600, loss is 4.035676465034485 and perplexity is 56.58118248688945
At time: 590.9700322151184 and batch: 650, loss is 4.017793669700622 and perplexity is 55.57834626317186
At time: 592.1387565135956 and batch: 700, loss is 4.015211796760559 and perplexity is 55.4350351200928
At time: 593.3098976612091 and batch: 750, loss is 4.0318347311019895 and perplexity is 56.36422964257312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560816920080851 and perplexity of 95.66159579704379
Finished 30 epochs...
Completing Train Step...
At time: 596.678857088089 and batch: 50, loss is 4.182124233245849 and perplexity is 65.50485311438699
At time: 597.8809518814087 and batch: 100, loss is 4.180699038505554 and perplexity is 65.41156243674695
At time: 599.0548419952393 and batch: 150, loss is 4.143706259727478 and perplexity is 63.036016891611226
At time: 600.2219169139862 and batch: 200, loss is 4.145064573287964 and perplexity is 63.121697745713625
At time: 601.3886299133301 and batch: 250, loss is 4.126015601158142 and perplexity is 61.930674186541765
At time: 602.5589003562927 and batch: 300, loss is 4.177750091552735 and perplexity is 65.21895134854421
At time: 603.7280004024506 and batch: 350, loss is 4.131135559082031 and perplexity is 62.2485697440726
At time: 604.9017841815948 and batch: 400, loss is 4.150678358078003 and perplexity is 63.477045863148284
At time: 606.0726916790009 and batch: 450, loss is 4.103344674110413 and perplexity is 60.542444080423564
At time: 607.2460384368896 and batch: 500, loss is 4.068501558303833 and perplexity is 58.46928410450003
At time: 608.4131810665131 and batch: 550, loss is 4.074554095268249 and perplexity is 58.82424473036856
At time: 609.5858447551727 and batch: 600, loss is 4.035756144523621 and perplexity is 56.58569102622136
At time: 610.7544987201691 and batch: 650, loss is 4.01804560661316 and perplexity is 55.59235026412164
At time: 611.9246351718903 and batch: 700, loss is 4.015567321777343 and perplexity is 55.45474716573938
At time: 613.1536576747894 and batch: 750, loss is 4.032203364372253 and perplexity is 56.38501120302412
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560743110124455 and perplexity of 95.65453527940072
Finished 31 epochs...
Completing Train Step...
At time: 616.558607339859 and batch: 50, loss is 4.181524839401245 and perplexity is 65.46560167334975
At time: 617.7814557552338 and batch: 100, loss is 4.179875264167785 and perplexity is 65.35770025840428
At time: 618.9536361694336 and batch: 150, loss is 4.1428176164627075 and perplexity is 62.980025241748486
At time: 620.1228394508362 and batch: 200, loss is 4.144189167022705 and perplexity is 63.066464795195934
At time: 621.2921328544617 and batch: 250, loss is 4.125155649185181 and perplexity is 61.877439673930965
At time: 622.4613747596741 and batch: 300, loss is 4.1769923496246335 and perplexity is 65.16955093334725
At time: 623.633880853653 and batch: 350, loss is 4.130495533943177 and perplexity is 62.20874184136771
At time: 624.8060245513916 and batch: 400, loss is 4.150146989822388 and perplexity is 63.44332513587183
At time: 625.9773192405701 and batch: 450, loss is 4.102919120788574 and perplexity is 60.51668552344137
At time: 627.142662525177 and batch: 500, loss is 4.068213615417481 and perplexity is 58.452450713715905
At time: 628.3087687492371 and batch: 550, loss is 4.074416518211365 and perplexity is 58.816152420575946
At time: 629.4749507904053 and batch: 600, loss is 4.035742316246033 and perplexity is 56.58490854898852
At time: 630.6396582126617 and batch: 650, loss is 4.018154191970825 and perplexity is 55.59838710710898
At time: 631.8062343597412 and batch: 700, loss is 4.015758185386658 and perplexity is 55.46533246907953
At time: 632.97203540802 and batch: 750, loss is 4.032409715652466 and perplexity is 56.396647522814106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560705850290698 and perplexity of 95.65097127371564
Finished 32 epochs...
Completing Train Step...
At time: 636.329211473465 and batch: 50, loss is 4.180938835144043 and perplexity is 65.42724979034975
At time: 637.5206215381622 and batch: 100, loss is 4.179137134552002 and perplexity is 65.30947560443626
At time: 638.6839361190796 and batch: 150, loss is 4.142044520378112 and perplexity is 62.931354446850996
At time: 639.8497400283813 and batch: 200, loss is 4.143432655334473 and perplexity is 63.018772319675044
At time: 641.0127627849579 and batch: 250, loss is 4.124422736167908 and perplexity is 61.832105507947944
At time: 642.2195799350739 and batch: 300, loss is 4.176344385147095 and perplexity is 65.12733705734834
At time: 643.3829834461212 and batch: 350, loss is 4.129942770004273 and perplexity is 62.17436459431978
At time: 644.5457575321198 and batch: 400, loss is 4.149678235054016 and perplexity is 63.41359274383772
At time: 645.711503982544 and batch: 450, loss is 4.102531161308288 and perplexity is 60.49321205524878
At time: 646.8787260055542 and batch: 500, loss is 4.06793092250824 and perplexity is 58.43592895577334
At time: 648.0409162044525 and batch: 550, loss is 4.0742494058609005 and perplexity is 58.80632433631968
At time: 649.2015850543976 and batch: 600, loss is 4.035670771598816 and perplexity is 56.580860346483895
At time: 650.3791766166687 and batch: 650, loss is 4.01817699432373 and perplexity is 55.599654895606996
At time: 651.5427589416504 and batch: 700, loss is 4.015851531028748 and perplexity is 55.47051015780615
At time: 652.7188634872437 and batch: 750, loss is 4.032522397041321 and perplexity is 56.40300273343401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560686688090479 and perplexity of 95.64913840821391
Finished 33 epochs...
Completing Train Step...
At time: 656.1793439388275 and batch: 50, loss is 4.180365266799927 and perplexity is 65.38973355111956
At time: 657.3802247047424 and batch: 100, loss is 4.178455467224121 and perplexity is 65.26497143895676
At time: 658.5480091571808 and batch: 150, loss is 4.141345272064209 and perplexity is 62.88736518486337
At time: 659.7272527217865 and batch: 200, loss is 4.142751579284668 and perplexity is 62.975866355930705
At time: 660.8896849155426 and batch: 250, loss is 4.123767380714416 and perplexity is 61.79159677566159
At time: 662.0494723320007 and batch: 300, loss is 4.175761823654175 and perplexity is 65.08940742788577
At time: 663.2078993320465 and batch: 350, loss is 4.129441862106323 and perplexity is 62.14322876278748
At time: 664.375227689743 and batch: 400, loss is 4.149245719909668 and perplexity is 63.38617133513327
At time: 665.5375213623047 and batch: 450, loss is 4.102164030075073 and perplexity is 60.471007184006076
At time: 666.7009778022766 and batch: 500, loss is 4.067648525238037 and perplexity is 58.41942913881575
At time: 667.8646249771118 and batch: 550, loss is 4.074061293601989 and perplexity is 58.795263186212104
At time: 669.0286619663239 and batch: 600, loss is 4.035560383796692 and perplexity is 56.57461485438738
At time: 670.190945148468 and batch: 650, loss is 4.018143801689148 and perplexity is 55.597809427207295
At time: 671.3510038852692 and batch: 700, loss is 4.015882253646851 and perplexity is 55.4722143832847
At time: 672.5668745040894 and batch: 750, loss is 4.032575616836548 and perplexity is 56.4060045695675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.56067817155705 and perplexity of 95.64832381259792
Finished 34 epochs...
Completing Train Step...
At time: 675.9316062927246 and batch: 50, loss is 4.179803638458252 and perplexity is 65.35301913439622
At time: 677.133900642395 and batch: 100, loss is 4.177814531326294 and perplexity is 65.22315417841412
At time: 678.3055467605591 and batch: 150, loss is 4.140696582794189 and perplexity is 62.846584054419054
At time: 679.4748544692993 and batch: 200, loss is 4.1421228742599485 and perplexity is 62.93628555594435
At time: 680.6443903446198 and batch: 250, loss is 4.123163461685181 and perplexity is 61.75429092051367
At time: 681.8159482479095 and batch: 300, loss is 4.17522234916687 and perplexity is 65.05430282305541
At time: 682.9884407520294 and batch: 350, loss is 4.1289746189117436 and perplexity is 62.11419954443935
At time: 684.1594333648682 and batch: 400, loss is 4.148836770057678 and perplexity is 63.36025486937615
At time: 685.3319928646088 and batch: 450, loss is 4.101809802055359 and perplexity is 60.44959045230835
At time: 686.5029320716858 and batch: 500, loss is 4.067365155220032 and perplexity is 58.40287716940553
At time: 687.6727528572083 and batch: 550, loss is 4.073858141899109 and perplexity is 58.78332004155068
At time: 688.8462195396423 and batch: 600, loss is 4.035423159599304 and perplexity is 56.566851980910705
At time: 690.019280910492 and batch: 650, loss is 4.018071794509888 and perplexity is 55.5938061299122
At time: 691.1876828670502 and batch: 700, loss is 4.015871820449829 and perplexity is 55.47163563376191
At time: 692.3598351478577 and batch: 750, loss is 4.032588276863098 and perplexity is 56.40671867560326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560670364734738 and perplexity of 95.64757710604425
Finished 35 epochs...
Completing Train Step...
At time: 695.761287689209 and batch: 50, loss is 4.179252767562867 and perplexity is 65.31702797238148
At time: 696.9710025787354 and batch: 100, loss is 4.177204313278199 and perplexity is 65.18336597355575
At time: 698.1385192871094 and batch: 150, loss is 4.140084776878357 and perplexity is 62.80814590204821
At time: 699.3102405071259 and batch: 200, loss is 4.141531710624695 and perplexity is 62.899090907730674
At time: 700.4795913696289 and batch: 250, loss is 4.122596101760864 and perplexity is 61.719263948080595
At time: 701.7013761997223 and batch: 300, loss is 4.1747129535675045 and perplexity is 65.02117288631591
At time: 702.8692317008972 and batch: 350, loss is 4.128530693054199 and perplexity is 62.086631564663186
At time: 704.0387794971466 and batch: 400, loss is 4.148444452285767 and perplexity is 63.33540239071133
At time: 705.2079651355743 and batch: 450, loss is 4.101464443206787 and perplexity is 60.428717255932064
At time: 706.3761541843414 and batch: 500, loss is 4.0670806455612185 and perplexity is 58.38626335025641
At time: 707.5433442592621 and batch: 550, loss is 4.073644080162048 and perplexity is 58.77073812865351
At time: 708.7136671543121 and batch: 600, loss is 4.035266466140747 and perplexity is 56.55798901963631
At time: 709.886120557785 and batch: 650, loss is 4.0179721641540525 and perplexity is 55.58826757513389
At time: 711.0584576129913 and batch: 700, loss is 4.015831041336059 and perplexity is 55.4693735957437
At time: 712.2280223369598 and batch: 750, loss is 4.032572140693665 and perplexity is 56.40580849457696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560679590979288 and perplexity of 95.64845957805217
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 715.5975015163422 and batch: 50, loss is 4.179469633102417 and perplexity is 65.33119452095711
At time: 716.7969996929169 and batch: 100, loss is 4.1781239461898805 and perplexity is 65.24333831424171
At time: 717.9661440849304 and batch: 150, loss is 4.141421122550964 and perplexity is 62.89213540303281
At time: 719.1351211071014 and batch: 200, loss is 4.142797207832336 and perplexity is 62.97873991880843
At time: 720.3028402328491 and batch: 250, loss is 4.123701791763306 and perplexity is 61.7875440625497
At time: 721.4755506515503 and batch: 300, loss is 4.175514001846313 and perplexity is 65.07327885185335
At time: 722.645843744278 and batch: 350, loss is 4.1293028402328495 and perplexity is 62.13459009519697
At time: 723.8140242099762 and batch: 400, loss is 4.14913420677185 and perplexity is 63.37910333836799
At time: 724.9841468334198 and batch: 450, loss is 4.1013747549057005 and perplexity is 60.42329774998133
At time: 726.154292345047 and batch: 500, loss is 4.065605096817016 and perplexity is 58.300175102014165
At time: 727.3221623897552 and batch: 550, loss is 4.071408386230469 and perplexity is 58.63949151436218
At time: 728.4922556877136 and batch: 600, loss is 4.032743101119995 and perplexity is 56.41545247999113
At time: 729.6617064476013 and batch: 650, loss is 4.0146288347244266 and perplexity is 55.40272801696839
At time: 730.8340237140656 and batch: 700, loss is 4.012902474403381 and perplexity is 55.307165457081915
At time: 732.0645608901978 and batch: 750, loss is 4.029753203392029 and perplexity is 56.24702795831544
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560810177825218 and perplexity of 95.66095082428497
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 735.5049540996552 and batch: 50, loss is 4.179599866867066 and perplexity is 65.33970340242838
At time: 736.7528808116913 and batch: 100, loss is 4.177898397445679 and perplexity is 65.2286244206299
At time: 737.9429218769073 and batch: 150, loss is 4.1415034818649294 and perplexity is 62.89731536946472
At time: 739.1319398880005 and batch: 200, loss is 4.142698283195496 and perplexity is 62.97251007798074
At time: 740.3024752140045 and batch: 250, loss is 4.123621325492859 and perplexity is 61.78257244934519
At time: 741.4778306484222 and batch: 300, loss is 4.175261678695679 and perplexity is 65.05686142844647
At time: 742.6439456939697 and batch: 350, loss is 4.129017033576965 and perplexity is 62.11683415329118
At time: 743.8124027252197 and batch: 400, loss is 4.149066281318665 and perplexity is 63.374798430259304
At time: 744.9863080978394 and batch: 450, loss is 4.101291956901551 and perplexity is 60.41829502863401
At time: 746.1571850776672 and batch: 500, loss is 4.065105814933776 and perplexity is 58.27107414619056
At time: 747.3288271427155 and batch: 550, loss is 4.070734634399414 and perplexity is 58.599996356043896
At time: 748.4983787536621 and batch: 600, loss is 4.031994647979737 and perplexity is 56.373243954945316
At time: 749.6668977737427 and batch: 650, loss is 4.013589100837708 and perplexity is 55.345153859320526
At time: 750.8384003639221 and batch: 700, loss is 4.011981115341187 and perplexity is 55.25623116697184
At time: 752.0062434673309 and batch: 750, loss is 4.0289284324646 and perplexity is 56.200656170581006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560866245003634 and perplexity of 95.66631441424158
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 755.3763132095337 and batch: 50, loss is 4.179644651412964 and perplexity is 65.34262967689985
At time: 756.5898039340973 and batch: 100, loss is 4.177833504676819 and perplexity is 65.22439169192056
At time: 757.7570481300354 and batch: 150, loss is 4.14151430606842 and perplexity is 62.897996186489955
At time: 758.9250359535217 and batch: 200, loss is 4.142651162147522 and perplexity is 62.9695428172231
At time: 760.0900993347168 and batch: 250, loss is 4.123571515083313 and perplexity is 61.77949511075104
At time: 761.2930011749268 and batch: 300, loss is 4.175162620544434 and perplexity is 65.0504173352028
At time: 762.4643394947052 and batch: 350, loss is 4.128884334564209 and perplexity is 62.108591857607806
At time: 763.6320917606354 and batch: 400, loss is 4.149009366035461 and perplexity is 63.37119153830328
At time: 764.8002424240112 and batch: 450, loss is 4.101257481575012 and perplexity is 60.41621212408862
At time: 765.9680488109589 and batch: 500, loss is 4.064966473579407 and perplexity is 58.262955141468154
At time: 767.1387505531311 and batch: 550, loss is 4.070542397499085 and perplexity is 58.58873235709794
At time: 768.30553150177 and batch: 600, loss is 4.031778540611267 and perplexity is 56.361062597830035
At time: 769.4762954711914 and batch: 650, loss is 4.013289513587952 and perplexity is 55.328575640324026
At time: 770.6453824043274 and batch: 700, loss is 4.011709995269776 and perplexity is 55.24125212428325
At time: 771.8130996227264 and batch: 750, loss is 4.0286884069442745 and perplexity is 56.18716819763466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560879019803779 and perplexity of 95.66753654009501
Annealing...
Finished 39 epochs...
Completing Train Step...
At time: 775.2123482227325 and batch: 50, loss is 4.179656763076782 and perplexity is 65.34342108965612
At time: 776.4053530693054 and batch: 100, loss is 4.177814540863037 and perplexity is 65.22315480043058
At time: 777.5700697898865 and batch: 150, loss is 4.141516036987305 and perplexity is 62.89810505791358
At time: 778.7354946136475 and batch: 200, loss is 4.142635684013367 and perplexity is 62.96856817373456
At time: 779.900084733963 and batch: 250, loss is 4.123555335998535 and perplexity is 61.778495583147865
At time: 781.0675730705261 and batch: 300, loss is 4.17513261795044 and perplexity is 65.04846568321979
At time: 782.233784198761 and batch: 350, loss is 4.128843116760254 and perplexity is 62.106031930602356
At time: 783.4004981517792 and batch: 400, loss is 4.148989782333374 and perplexity is 63.36995050791934
At time: 784.5664982795715 and batch: 450, loss is 4.101245951652527 and perplexity is 60.4155155338618
At time: 785.7332406044006 and batch: 500, loss is 4.064926743507385 and perplexity is 58.260640396046995
At time: 786.9001622200012 and batch: 550, loss is 4.07048722743988 and perplexity is 58.58550010242773
At time: 788.0666599273682 and batch: 600, loss is 4.0317159938812255 and perplexity is 56.357537507905455
At time: 789.2355217933655 and batch: 650, loss is 4.013203148841858 and perplexity is 55.32379740827546
At time: 790.3995180130005 and batch: 700, loss is 4.011631188392639 and perplexity is 55.23689890524825
At time: 791.6182577610016 and batch: 750, loss is 4.028618860244751 and perplexity is 56.18326070140924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.560883987781613 and perplexity of 95.66801181547659
Annealing...
Model not improving. Stopping early with 95.64757710604425loss at 39 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9fccc82d30>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -147.79316595242122, 'params': {'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 2.575919372238502, 'wordvec_dim': 200, 'lr': 16.85910374052814, 'dropout': 0.4790889205209622, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -98.42915159806145, 'params': {'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.1690291668353847, 'wordvec_dim': 200, 'lr': 6.484969183534676, 'dropout': 0.00015306102629486507, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -146.01960202114392, 'params': {'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.986431260250071, 'wordvec_dim': 200, 'lr': 20.356485438410314, 'dropout': 0.17501787693936444, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -94.61852701543499, 'params': {'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 6.4920193942967295, 'wordvec_dim': 200, 'lr': 3.5614289339720253, 'dropout': 0.6602744902126817, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -156.65733259523898, 'params': {'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.972988016755215, 'wordvec_dim': 200, 'lr': 18.104274377656584, 'dropout': 0.8044130915304769, 'batch_size': 80, 'num_layers': 1}}, {'best_accuracy': -95.64757710604425, 'params': {'seq_len': 35, 'tune_wordvecs': True, 'data': 'wikitext', 'wordvec_source': '', 'anneal': 3.466005873618206, 'wordvec_dim': 200, 'lr': 2.614572684018477, 'dropout': 0.0, 'batch_size': 80, 'num_layers': 1}}]
