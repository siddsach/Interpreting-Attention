Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 18.976557961124893, 'dropout': 0.6137351798711002, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 7.8405872643857855}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.5111193656921387 and batch: 50, loss is 7.425650720596313 and perplexity is 1678.4914466815846
At time: 4.212341785430908 and batch: 100, loss is 6.533929986953735 and perplexity is 688.0971173798797
At time: 5.870999336242676 and batch: 150, loss is 6.400452423095703 and perplexity is 602.1173880713642
At time: 7.532093286514282 and batch: 200, loss is 6.411495428085328 and perplexity is 608.8034224018636
At time: 9.194287300109863 and batch: 250, loss is 6.48699613571167 and perplexity is 656.5482164230625
At time: 10.85554575920105 and batch: 300, loss is 6.521718454360962 and perplexity is 679.7454938491237
At time: 12.523579597473145 and batch: 350, loss is 6.476674299240113 and perplexity is 649.8062874985412
At time: 14.185476779937744 and batch: 400, loss is 6.54778974533081 and perplexity is 697.7003728409462
At time: 15.844113111495972 and batch: 450, loss is 6.561355371475219 and perplexity is 707.229604118955
At time: 17.50913166999817 and batch: 500, loss is 6.609404344558715 and perplexity is 742.0408863588078
At time: 19.19754981994629 and batch: 550, loss is 6.655459566116333 and perplexity is 777.0149307801812
At time: 20.887611865997314 and batch: 600, loss is 6.68615080833435 and perplexity is 801.2322125271414
At time: 22.574355363845825 and batch: 650, loss is 6.725649385452271 and perplexity is 833.5130721009016
At time: 24.253509044647217 and batch: 700, loss is 6.753202781677246 and perplexity is 856.7985113487417
At time: 25.937442541122437 and batch: 750, loss is 6.726111726760864 and perplexity is 833.8985287247802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.421657828397529 and perplexity of 615.0218701523273
Finished 1 epochs...
Completing Train Step...
At time: 30.53572678565979 and batch: 50, loss is 6.765562353134155 and perplexity is 867.4538860648385
At time: 32.2147421836853 and batch: 100, loss is 6.781396684646606 and perplexity is 881.298761369846
At time: 33.899181842803955 and batch: 150, loss is 6.822378644943237 and perplexity is 918.1664065487789
At time: 35.58049249649048 and batch: 200, loss is 6.671741380691528 and perplexity is 789.7696974077567
At time: 37.258633613586426 and batch: 250, loss is 6.707729606628418 and perplexity is 818.7097346022864
At time: 38.9354133605957 and batch: 300, loss is 6.738663244247436 and perplexity is 844.43118280971
At time: 40.6181538105011 and batch: 350, loss is 6.667728385925293 and perplexity is 786.606706523227
At time: 42.29388189315796 and batch: 400, loss is 6.742455759048462 and perplexity is 847.6397810513078
At time: 43.965901613235474 and batch: 450, loss is 6.755983781814575 and perplexity is 859.1845844234238
At time: 45.645877838134766 and batch: 500, loss is 6.757606496810913 and perplexity is 860.579927949437
At time: 47.32511806488037 and batch: 550, loss is 6.7637941265106205 and perplexity is 865.9213863115385
At time: 49.00621461868286 and batch: 600, loss is 6.760372467041016 and perplexity is 862.9635614206959
At time: 50.73533535003662 and batch: 650, loss is 6.790790185928345 and perplexity is 889.6162464078803
At time: 52.41481351852417 and batch: 700, loss is 6.785612211227417 and perplexity is 885.0217413712973
At time: 54.100515842437744 and batch: 750, loss is 6.755733165740967 and perplexity is 858.9692859361282
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.311024510583212 and perplexity of 550.608764451568
Finished 2 epochs...
Completing Train Step...
At time: 58.61392283439636 and batch: 50, loss is 6.689115085601807 and perplexity is 803.6108106311173
At time: 60.3221595287323 and batch: 100, loss is 6.715957098007202 and perplexity is 825.473447928145
At time: 62.00382137298584 and batch: 150, loss is 6.763582162857055 and perplexity is 865.7378619017351
At time: 63.686479330062866 and batch: 200, loss is 6.819814338684082 and perplexity is 915.814962884898
At time: 65.36608481407166 and batch: 250, loss is 6.878721704483032 and perplexity is 971.3838501928954
At time: 67.04583978652954 and batch: 300, loss is 6.890696582794189 and perplexity is 983.0859795419675
At time: 68.72836065292358 and batch: 350, loss is 6.7711571598052975 and perplexity is 872.3207246689617
At time: 70.40666365623474 and batch: 400, loss is 6.807549104690552 and perplexity is 904.6508830523162
At time: 72.08691000938416 and batch: 450, loss is 6.805546236038208 and perplexity is 902.8407994422892
At time: 73.7676568031311 and batch: 500, loss is 6.780104904174805 and perplexity is 880.1610518337743
At time: 75.44887661933899 and batch: 550, loss is 6.777406644821167 and perplexity is 877.7893502149461
At time: 77.12851214408875 and batch: 600, loss is 6.742444667816162 and perplexity is 847.6303797337258
At time: 78.80533385276794 and batch: 650, loss is 6.754974584579468 and perplexity is 858.3179350999827
At time: 80.48830914497375 and batch: 700, loss is 6.729338331222534 and perplexity is 836.5935349595775
At time: 82.16876745223999 and batch: 750, loss is 6.695148849487305 and perplexity is 808.47426623466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.367540226426235 and perplexity of 582.6229443472579
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 86.7150149345398 and batch: 50, loss is 6.681457653045654 and perplexity is 797.4807153965241
At time: 88.42897963523865 and batch: 100, loss is 6.5815599918365475 and perplexity is 721.6642419198619
At time: 90.11421132087708 and batch: 150, loss is 6.556724662780762 and perplexity is 703.9622008771848
At time: 91.79858565330505 and batch: 200, loss is 6.522360849380493 and perplexity is 680.182299254708
At time: 93.48229885101318 and batch: 250, loss is 6.5308000850677494 and perplexity is 685.9468077988779
At time: 95.16707634925842 and batch: 300, loss is 6.5058635711669925 and perplexity is 669.0531949996999
At time: 96.85086011886597 and batch: 350, loss is 6.413767385482788 and perplexity is 610.1881702894439
At time: 98.53634858131409 and batch: 400, loss is 6.418759727478028 and perplexity is 613.2420549926213
At time: 100.22189044952393 and batch: 450, loss is 6.380466833114624 and perplexity is 590.2031697400458
At time: 101.9059534072876 and batch: 500, loss is 6.342620029449463 and perplexity is 568.2832815114122
At time: 103.58970332145691 and batch: 550, loss is 6.298603782653808 and perplexity is 543.8120999322812
At time: 105.27042627334595 and batch: 600, loss is 6.226709260940551 and perplexity is 506.0873389132284
At time: 106.94517111778259 and batch: 650, loss is 6.1770540046691895 and perplexity is 481.57115816454456
At time: 108.61385202407837 and batch: 700, loss is 6.132925043106079 and perplexity is 460.7819986118282
At time: 110.29490804672241 and batch: 750, loss is 6.120925941467285 and perplexity is 455.28606763549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.885027508402979 and perplexity of 359.6126602305184
Finished 4 epochs...
Completing Train Step...
At time: 114.81778454780579 and batch: 50, loss is 6.320203866958618 and perplexity is 555.686266973738
At time: 116.53093433380127 and batch: 100, loss is 6.319250154495239 and perplexity is 555.1565546919501
At time: 118.2189552783966 and batch: 150, loss is 6.327742319107056 and perplexity is 559.8911103995329
At time: 119.90321636199951 and batch: 200, loss is 6.317254285812378 and perplexity is 554.0496401058828
At time: 121.58180499076843 and batch: 250, loss is 6.341276664733886 and perplexity is 567.520382343219
At time: 123.26723837852478 and batch: 300, loss is 6.320942583084107 and perplexity is 556.0969130366243
At time: 124.953773021698 and batch: 350, loss is 6.2384939384460445 and perplexity is 512.0866958033206
At time: 126.64035677909851 and batch: 400, loss is 6.24702091217041 and perplexity is 516.471915358513
At time: 128.33332586288452 and batch: 450, loss is 6.217995901107788 and perplexity is 501.6967738875127
At time: 130.07593417167664 and batch: 500, loss is 6.185992469787598 and perplexity is 485.89496045390655
At time: 131.7738001346588 and batch: 550, loss is 6.171270656585693 and perplexity is 478.79410260973725
At time: 133.46001625061035 and batch: 600, loss is 6.130398979187012 and perplexity is 459.6195027182028
At time: 135.1434245109558 and batch: 650, loss is 6.1199612712860105 and perplexity is 454.84707851599217
At time: 136.8383378982544 and batch: 700, loss is 6.092676906585694 and perplexity is 442.604638015867
At time: 138.53483748435974 and batch: 750, loss is 6.083881072998047 and perplexity is 438.7286326134539
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.827750982240189 and perplexity of 339.59406649373346
Finished 5 epochs...
Completing Train Step...
At time: 143.05731225013733 and batch: 50, loss is 6.2308104133605955 and perplexity is 508.1671421078235
At time: 144.7752993106842 and batch: 100, loss is 6.212134504318238 and perplexity is 498.7647313544919
At time: 146.46602630615234 and batch: 150, loss is 6.2139418506622315 and perplexity is 499.66698706689857
At time: 148.15346670150757 and batch: 200, loss is 6.205479249954224 and perplexity is 495.4563464823108
At time: 149.8415777683258 and batch: 250, loss is 6.22518401145935 and perplexity is 505.31601784017437
At time: 151.53075242042542 and batch: 300, loss is 6.203697471618653 and perplexity is 494.5743391020931
At time: 153.22250366210938 and batch: 350, loss is 6.126438932418823 and perplexity is 457.8029871091287
At time: 154.9084210395813 and batch: 400, loss is 6.132717409133911 and perplexity is 460.6863345470516
At time: 156.58105635643005 and batch: 450, loss is 6.1017372608184814 and perplexity is 446.63301452301357
At time: 158.2505669593811 and batch: 500, loss is 6.070961904525757 and perplexity is 433.09707931561985
At time: 159.9105999469757 and batch: 550, loss is 6.067835960388184 and perplexity is 431.7453558462604
At time: 161.5716097354889 and batch: 600, loss is 6.046494073867798 and perplexity is 422.6287245428104
At time: 163.2305827140808 and batch: 650, loss is 6.047188997268677 and perplexity is 422.9225212046431
At time: 164.8939995765686 and batch: 700, loss is 6.026506605148316 and perplexity is 414.26530634042484
At time: 166.5555489063263 and batch: 750, loss is 6.015418548583984 and perplexity is 409.69728125258644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.748454604037972 and perplexity of 313.70548629750266
Finished 6 epochs...
Completing Train Step...
At time: 170.95210814476013 and batch: 50, loss is 6.134761085510254 and perplexity is 461.62879103587744
At time: 172.6507978439331 and batch: 100, loss is 6.1165380859375 and perplexity is 453.29271461672823
At time: 174.3124942779541 and batch: 150, loss is 6.1132095909118656 and perplexity is 451.78644027494937
At time: 176.00201964378357 and batch: 200, loss is 6.1167982482910155 and perplexity is 453.41065965795883
At time: 177.68830275535583 and batch: 250, loss is 6.144913902282715 and perplexity is 466.3394965684593
At time: 179.3754072189331 and batch: 300, loss is 6.1304295539855955 and perplexity is 459.63355570675606
At time: 181.06521821022034 and batch: 350, loss is 6.056421003341675 and perplexity is 426.84502290908245
At time: 182.75246238708496 and batch: 400, loss is 6.072164163589478 and perplexity is 433.61808733524845
At time: 184.43979668617249 and batch: 450, loss is 6.045392818450928 and perplexity is 422.1635585508783
At time: 186.12472486495972 and batch: 500, loss is 6.026465148925781 and perplexity is 414.24813282167383
At time: 187.80443739891052 and batch: 550, loss is 6.039592485427857 and perplexity is 419.7219572169063
At time: 189.48133730888367 and batch: 600, loss is 6.026309375762939 and perplexity is 414.1836091054851
At time: 191.163592338562 and batch: 650, loss is 6.029806423187256 and perplexity is 415.63456437991886
At time: 192.84699940681458 and batch: 700, loss is 6.011980638504029 and perplexity is 408.29119721977446
At time: 194.5278356075287 and batch: 750, loss is 5.993832817077637 and perplexity is 400.9484306144445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.723714784134266 and perplexity of 306.0396853208408
Finished 7 epochs...
Completing Train Step...
At time: 199.04917883872986 and batch: 50, loss is 6.094624948501587 and perplexity is 443.4676907614219
At time: 200.7743682861328 and batch: 100, loss is 6.075741119384766 and perplexity is 435.17189736387786
At time: 202.4624469280243 and batch: 150, loss is 6.065526752471924 and perplexity is 430.7495162955401
At time: 204.14078783988953 and batch: 200, loss is 6.0797741508483885 and perplexity is 436.9305031906532
At time: 205.8130440711975 and batch: 250, loss is 6.1144643402099605 and perplexity is 452.35367478790573
At time: 207.4880862236023 and batch: 300, loss is 6.099978952407837 and perplexity is 445.84838594876663
At time: 209.1738498210907 and batch: 350, loss is 6.032853116989136 and perplexity is 416.90280662293145
At time: 210.86242699623108 and batch: 400, loss is 6.049037609100342 and perplexity is 423.70506386717915
At time: 212.54628944396973 and batch: 450, loss is 6.022998828887939 and perplexity is 412.8147020184605
At time: 214.27338862419128 and batch: 500, loss is 6.007088232040405 and perplexity is 406.2985491242745
At time: 215.96238017082214 and batch: 550, loss is 6.026593189239502 and perplexity is 414.30117667836237
At time: 217.64783453941345 and batch: 600, loss is 6.014238758087158 and perplexity is 409.2142093114645
At time: 219.33423900604248 and batch: 650, loss is 6.01867977142334 and perplexity is 411.0355764367116
At time: 221.02238488197327 and batch: 700, loss is 6.001268720626831 and perplexity is 403.94095675184195
At time: 222.70954537391663 and batch: 750, loss is 5.978686094284058 and perplexity is 394.92113797973576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.708902403365734 and perplexity of 301.5399173825628
Finished 8 epochs...
Completing Train Step...
At time: 227.21864938735962 and batch: 50, loss is 6.065805063247681 and perplexity is 430.86941521138345
At time: 228.93485116958618 and batch: 100, loss is 6.0494459438323975 and perplexity is 423.8781126894217
At time: 230.62231135368347 and batch: 150, loss is 6.038695249557495 and perplexity is 419.34553651564323
At time: 232.30803036689758 and batch: 200, loss is 6.057554063796997 and perplexity is 427.3289382259744
At time: 233.99543023109436 and batch: 250, loss is 6.09658917427063 and perplexity is 444.3396174774778
At time: 235.67858600616455 and batch: 300, loss is 6.084130077362061 and perplexity is 438.83789156000535
At time: 237.3435411453247 and batch: 350, loss is 6.017406339645386 and perplexity is 410.51248380388665
At time: 239.02688097953796 and batch: 400, loss is 6.032638034820557 and perplexity is 416.81314790553745
At time: 240.711767911911 and batch: 450, loss is 6.008127794265747 and perplexity is 406.72114136556326
At time: 242.3970170021057 and batch: 500, loss is 5.994893922805786 and perplexity is 401.3741050937354
At time: 244.08362817764282 and batch: 550, loss is 6.016206846237183 and perplexity is 410.02037198702266
At time: 245.76895713806152 and batch: 600, loss is 6.001800270080566 and perplexity is 404.15572842257
At time: 247.45515370368958 and batch: 650, loss is 6.004569396972657 and perplexity is 405.2764378959686
At time: 249.1415979862213 and batch: 700, loss is 5.988475112915039 and perplexity is 398.8060118864636
At time: 250.82885837554932 and batch: 750, loss is 5.967037982940674 and perplexity is 390.34774002617917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.692423266033794 and perplexity of 296.6115190791447
Finished 9 epochs...
Completing Train Step...
At time: 255.39684128761292 and batch: 50, loss is 6.042510280609131 and perplexity is 420.94840831758563
At time: 257.11216044425964 and batch: 100, loss is 6.028425140380859 and perplexity is 415.06085182325955
At time: 258.79942297935486 and batch: 150, loss is 6.021188735961914 and perplexity is 412.0681449192771
At time: 260.4878396987915 and batch: 200, loss is 6.039572286605835 and perplexity is 419.7134794134148
At time: 262.17085242271423 and batch: 250, loss is 6.081126070022583 and perplexity is 437.5215973814643
At time: 263.8576180934906 and batch: 300, loss is 6.072477254867554 and perplexity is 433.7538706315828
At time: 265.5454246997833 and batch: 350, loss is 6.002858867645264 and perplexity is 404.5837932266412
At time: 267.21171522140503 and batch: 400, loss is 6.020139627456665 and perplexity is 411.6360674114239
At time: 268.882150888443 and batch: 450, loss is 5.993903255462646 and perplexity is 400.9766737690584
At time: 270.54064869880676 and batch: 500, loss is 5.980035486221314 and perplexity is 395.4544010887311
At time: 272.2102506160736 and batch: 550, loss is 6.003910655975342 and perplexity is 405.009553604544
At time: 273.87470173835754 and batch: 600, loss is 5.991690263748169 and perplexity is 400.09029684644196
At time: 275.5354266166687 and batch: 650, loss is 5.995901899337769 and perplexity is 401.7788847421367
At time: 277.1946771144867 and batch: 700, loss is 5.978502817153931 and perplexity is 394.84876459933577
At time: 278.85942244529724 and batch: 750, loss is 5.957288799285888 and perplexity is 386.56065869087394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.68442198287609 and perplexity of 294.2477156683479
Finished 10 epochs...
Completing Train Step...
At time: 283.30037093162537 and batch: 50, loss is 6.02639386177063 and perplexity is 414.2186033033084
At time: 284.9982097148895 and batch: 100, loss is 6.012520580291748 and perplexity is 408.5117102254475
At time: 286.66003489494324 and batch: 150, loss is 6.008473834991455 and perplexity is 406.86190779853666
At time: 288.31768250465393 and batch: 200, loss is 6.028037595748901 and perplexity is 414.9000283833427
At time: 289.9763660430908 and batch: 250, loss is 6.06975154876709 and perplexity is 432.5731948788365
At time: 291.635728597641 and batch: 300, loss is 6.064154462814331 and perplexity is 430.15880859305065
At time: 293.29739236831665 and batch: 350, loss is 5.991282014846802 and perplexity is 399.92699375872786
At time: 294.97676515579224 and batch: 400, loss is 6.00901517868042 and perplexity is 407.08221955141175
At time: 296.66430592536926 and batch: 450, loss is 5.983770027160644 and perplexity is 396.9340028362438
At time: 298.41265630722046 and batch: 500, loss is 5.970797414779663 and perplexity is 391.8179876645786
At time: 300.10457587242126 and batch: 550, loss is 5.994710531234741 and perplexity is 401.30050321521395
At time: 301.79796981811523 and batch: 600, loss is 5.983706293106079 and perplexity is 396.90870542900984
At time: 303.49096298217773 and batch: 650, loss is 5.9863973808288575 and perplexity is 397.9782600604507
At time: 305.1850395202637 and batch: 700, loss is 5.971833877563476 and perplexity is 392.2243029557865
At time: 306.8791010379791 and batch: 750, loss is 5.94991473197937 and perplexity is 383.72061856850564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.678834072379178 and perplexity of 292.60807112919264
Finished 11 epochs...
Completing Train Step...
At time: 311.498042345047 and batch: 50, loss is 6.015439291000366 and perplexity is 409.705779452321
At time: 313.22171688079834 and batch: 100, loss is 5.999650173187256 and perplexity is 403.2876879664255
At time: 314.91625118255615 and batch: 150, loss is 5.998094596862793 and perplexity is 402.6608308755091
At time: 316.609610080719 and batch: 200, loss is 6.019086179733276 and perplexity is 411.2026586601571
At time: 318.3076536655426 and batch: 250, loss is 6.060602254867554 and perplexity is 428.6335057551
At time: 320.00021147727966 and batch: 300, loss is 6.0538107204437255 and perplexity is 425.7322895519419
At time: 321.6852185726166 and batch: 350, loss is 5.9817155170440675 and perplexity is 396.11933506994734
At time: 323.3736310005188 and batch: 400, loss is 6.002614965438843 and perplexity is 404.48512637981105
At time: 325.0590636730194 and batch: 450, loss is 5.975324802398681 and perplexity is 393.5959212311552
At time: 326.744108915329 and batch: 500, loss is 5.963386545181274 and perplexity is 388.9250086372811
At time: 328.4340031147003 and batch: 550, loss is 5.987864933013916 and perplexity is 398.56274270009516
At time: 330.12142157554626 and batch: 600, loss is 5.977066268920899 and perplexity is 394.2819525281514
At time: 331.81030201911926 and batch: 650, loss is 5.979506864547729 and perplexity is 395.24541056472816
At time: 333.4978578090668 and batch: 700, loss is 5.962645406723023 and perplexity is 388.6368681451935
At time: 335.1868624687195 and batch: 750, loss is 5.941276760101318 and perplexity is 380.4203250981779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.675629371820494 and perplexity of 291.67185083427717
Finished 12 epochs...
Completing Train Step...
At time: 339.71981501579285 and batch: 50, loss is 6.007087497711182 and perplexity is 406.2982507674861
At time: 341.44236612319946 and batch: 100, loss is 5.991811218261719 and perplexity is 400.1386925004504
At time: 343.1327271461487 and batch: 150, loss is 5.9882674884796145 and perplexity is 398.7232186086528
At time: 344.81974172592163 and batch: 200, loss is 6.007435331344604 and perplexity is 406.4395995458081
At time: 346.50670194625854 and batch: 250, loss is 6.0503003025054936 and perplexity is 424.2404113758083
At time: 348.19608998298645 and batch: 300, loss is 6.045046968460083 and perplexity is 422.0175785330673
At time: 349.8897466659546 and batch: 350, loss is 5.972086811065674 and perplexity is 392.32352216978296
At time: 351.5754225254059 and batch: 400, loss is 5.993041324615478 and perplexity is 400.6312085099117
At time: 353.2613379955292 and batch: 450, loss is 5.965702886581421 and perplexity is 389.8269359188217
At time: 354.94797015190125 and batch: 500, loss is 5.953111915588379 and perplexity is 384.94940712522674
At time: 356.6348259449005 and batch: 550, loss is 5.97914080619812 and perplexity is 395.10075416000393
At time: 358.3245837688446 and batch: 600, loss is 5.968125667572021 and perplexity is 390.77254624960614
At time: 360.0035820007324 and batch: 650, loss is 5.971203765869141 and perplexity is 391.97723568385476
At time: 361.6793420314789 and batch: 700, loss is 5.953885555267334 and perplexity is 385.24733449032493
At time: 363.36480045318604 and batch: 750, loss is 5.9329809474945066 and perplexity is 377.2774836056864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.669341331304506 and perplexity of 289.84356062444414
Finished 13 epochs...
Completing Train Step...
At time: 367.87537932395935 and batch: 50, loss is 5.9982326889038085 and perplexity is 402.716438970911
At time: 369.58297181129456 and batch: 100, loss is 5.982890768051147 and perplexity is 396.58514838758174
At time: 371.2506206035614 and batch: 150, loss is 5.979989147186279 and perplexity is 395.4360765379586
At time: 372.93057131767273 and batch: 200, loss is 5.998843126296997 and perplexity is 402.9623471922611
At time: 374.6059625148773 and batch: 250, loss is 6.0439423561096195 and perplexity is 421.5516700751993
At time: 376.28179121017456 and batch: 300, loss is 6.036062755584717 and perplexity is 418.24306368148
At time: 377.97311663627625 and batch: 350, loss is 5.963671541213989 and perplexity is 389.03586651804403
At time: 379.65575337409973 and batch: 400, loss is 5.984620609283447 and perplexity is 397.27177143258945
At time: 381.3310911655426 and batch: 450, loss is 5.956422338485718 and perplexity is 386.2258640973715
At time: 383.03267550468445 and batch: 500, loss is 5.941612510681153 and perplexity is 380.5480728874075
At time: 384.69588470458984 and batch: 550, loss is 5.968536291122437 and perplexity is 390.9330396088734
At time: 386.3571300506592 and batch: 600, loss is 5.95651593208313 and perplexity is 386.26201405708224
At time: 388.0271363258362 and batch: 650, loss is 5.95517391204834 and perplexity is 385.74399037236844
At time: 389.6898283958435 and batch: 700, loss is 5.935500154495239 and perplexity is 378.22912186719986
At time: 391.35840344429016 and batch: 750, loss is 5.915590295791626 and perplexity is 370.77310392215674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.650640798169513 and perplexity of 284.47369765550525
Finished 14 epochs...
Completing Train Step...
At time: 395.7827241420746 and batch: 50, loss is 5.972986221313477 and perplexity is 392.67654069651917
At time: 397.48570466041565 and batch: 100, loss is 5.960323295593262 and perplexity is 387.73545714154915
At time: 399.150354385376 and batch: 150, loss is 5.9565636348724365 and perplexity is 386.2804402720432
At time: 400.8290026187897 and batch: 200, loss is 5.973265304565429 and perplexity is 392.7861454361745
At time: 402.49744629859924 and batch: 250, loss is 6.017359771728516 and perplexity is 410.4933675377725
At time: 404.18693232536316 and batch: 300, loss is 6.008633222579956 and perplexity is 406.92676170519013
At time: 405.8735408782959 and batch: 350, loss is 5.937157182693482 and perplexity is 378.8563777343365
At time: 407.5536644458771 and batch: 400, loss is 5.95942066192627 and perplexity is 387.3856319697872
At time: 409.2117598056793 and batch: 450, loss is 5.931431970596313 and perplexity is 376.693541872256
At time: 410.8693606853485 and batch: 500, loss is 5.915812845230103 and perplexity is 370.8556284507901
At time: 412.53181529045105 and batch: 550, loss is 5.942711877822876 and perplexity is 380.9666649856258
At time: 414.1924271583557 and batch: 600, loss is 5.92960566520691 and perplexity is 376.0062122545164
At time: 415.85344910621643 and batch: 650, loss is 5.929441385269165 and perplexity is 375.9444470509064
At time: 417.51476407051086 and batch: 700, loss is 5.9197142124176025 and perplexity is 372.3052984396465
At time: 419.177143573761 and batch: 750, loss is 5.900434417724609 and perplexity is 365.19608106121336
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.640318404796512 and perplexity of 281.55235182396865
Finished 15 epochs...
Completing Train Step...
At time: 423.632399559021 and batch: 50, loss is 5.959517726898193 and perplexity is 387.42323537023475
At time: 425.3354330062866 and batch: 100, loss is 5.947872314453125 and perplexity is 382.93770064676204
At time: 427.0036759376526 and batch: 150, loss is 5.945813426971435 and perplexity is 382.1500860917332
At time: 428.68760323524475 and batch: 200, loss is 5.9600014400482175 and perplexity is 387.6106824154538
At time: 430.36052680015564 and batch: 250, loss is 6.0005976104736325 and perplexity is 403.6699588193764
At time: 432.038343667984 and batch: 300, loss is 5.994673624038696 and perplexity is 401.2856926121794
At time: 433.71294260025024 and batch: 350, loss is 5.926150732040405 and perplexity is 374.7093774497547
At time: 435.3936014175415 and batch: 400, loss is 5.9484144306182865 and perplexity is 383.14535364547044
At time: 437.055456161499 and batch: 450, loss is 5.922687845230103 and perplexity is 373.4140453774848
At time: 438.7248980998993 and batch: 500, loss is 5.907525911331176 and perplexity is 367.7950712044246
At time: 440.4099326133728 and batch: 550, loss is 5.933753509521484 and perplexity is 377.5690664815799
At time: 442.0892639160156 and batch: 600, loss is 5.921008710861206 and perplexity is 372.78755914454314
At time: 443.7621741294861 and batch: 650, loss is 5.921286745071411 and perplexity is 372.8912212492621
At time: 445.4412250518799 and batch: 700, loss is 5.911535787582397 and perplexity is 369.27284479026036
At time: 447.1248936653137 and batch: 750, loss is 5.89323471069336 and perplexity is 362.5762187061959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.63473120401072 and perplexity of 279.983648713067
Finished 16 epochs...
Completing Train Step...
At time: 451.6626100540161 and batch: 50, loss is 5.950350170135498 and perplexity is 383.88774155034423
At time: 453.378662109375 and batch: 100, loss is 5.938186540603637 and perplexity is 379.24655732634284
At time: 455.0669455528259 and batch: 150, loss is 5.935510625839234 and perplexity is 378.23308245518
At time: 456.7521171569824 and batch: 200, loss is 5.950882043838501 and perplexity is 384.09197565353975
At time: 458.43839383125305 and batch: 250, loss is 5.994366436004639 and perplexity is 401.16244138079156
At time: 460.1258170604706 and batch: 300, loss is 5.988643989562989 and perplexity is 398.87336659609167
At time: 461.81452465057373 and batch: 350, loss is 5.919439630508423 and perplexity is 372.2030841737396
At time: 463.4989812374115 and batch: 400, loss is 5.942374744415283 and perplexity is 380.83825004337984
At time: 465.18544840812683 and batch: 450, loss is 5.916270027160644 and perplexity is 371.025215706118
At time: 466.9040229320526 and batch: 500, loss is 5.89963318824768 and perplexity is 364.9035923871292
At time: 468.5908498764038 and batch: 550, loss is 5.9268552112579345 and perplexity is 374.97344542305984
At time: 470.2764928340912 and batch: 600, loss is 5.914831619262696 and perplexity is 370.4919137503318
At time: 471.9635720252991 and batch: 650, loss is 5.917163257598877 and perplexity is 371.3567747794218
At time: 473.65001153945923 and batch: 700, loss is 5.90534628868103 and perplexity is 366.9942897543347
At time: 475.3353183269501 and batch: 750, loss is 5.887446308135987 and perplexity is 360.48354405925744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.62997010696766 and perplexity of 278.65378770184964
Finished 17 epochs...
Completing Train Step...
At time: 479.827716588974 and batch: 50, loss is 5.9436295795440675 and perplexity is 381.31643921946034
At time: 481.5277624130249 and batch: 100, loss is 5.931464252471923 and perplexity is 376.7057024425997
At time: 483.1894419193268 and batch: 150, loss is 5.930215873718262 and perplexity is 376.2357244635854
At time: 484.8470311164856 and batch: 200, loss is 5.94513542175293 and perplexity is 381.89107415475536
At time: 486.50980401039124 and batch: 250, loss is 5.984946279525757 and perplexity is 397.40117209648383
At time: 488.1688289642334 and batch: 300, loss is 5.981912660598755 and perplexity is 396.19743514195335
At time: 489.83213686943054 and batch: 350, loss is 5.9128084564208985 and perplexity is 369.74310601265853
At time: 491.49009919166565 and batch: 400, loss is 5.936943197250367 and perplexity is 378.7753166577245
At time: 493.1543433666229 and batch: 450, loss is 5.909175262451172 and perplexity is 368.40219495953437
At time: 494.81323885917664 and batch: 500, loss is 5.8939027500152585 and perplexity is 362.81851480012523
At time: 496.4737069606781 and batch: 550, loss is 5.920827560424804 and perplexity is 372.72003463175116
At time: 498.1467390060425 and batch: 600, loss is 5.907818727493286 and perplexity is 367.90278331477117
At time: 499.836133480072 and batch: 650, loss is 5.909072551727295 and perplexity is 368.3643580465738
At time: 501.52471256256104 and batch: 700, loss is 5.897665185928345 and perplexity is 364.1861674494989
At time: 503.21559715270996 and batch: 750, loss is 5.881248931884766 and perplexity is 358.2564002566464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.628292349881904 and perplexity of 278.1866663027526
Finished 18 epochs...
Completing Train Step...
At time: 507.7265992164612 and batch: 50, loss is 5.935343341827393 and perplexity is 378.1698153996669
At time: 509.44491958618164 and batch: 100, loss is 5.923852462768554 and perplexity is 373.8491832592573
At time: 511.13792085647583 and batch: 150, loss is 5.921450357437134 and perplexity is 372.95223585536405
At time: 512.8202910423279 and batch: 200, loss is 5.9379956340789795 and perplexity is 379.1741635945329
At time: 514.5017273426056 and batch: 250, loss is 5.977050914764404 and perplexity is 394.27589870782504
At time: 516.1781108379364 and batch: 300, loss is 5.974761743545532 and perplexity is 393.374365943329
At time: 517.852489233017 and batch: 350, loss is 5.906654872894287 and perplexity is 367.4748470445293
At time: 519.5356633663177 and batch: 400, loss is 5.929269618988037 and perplexity is 375.87987801687615
At time: 521.2240524291992 and batch: 450, loss is 5.901028089523315 and perplexity is 365.412952044261
At time: 522.9127106666565 and batch: 500, loss is 5.886297445297242 and perplexity is 360.06963571899
At time: 524.6031396389008 and batch: 550, loss is 5.9117011451721195 and perplexity is 369.3339119066412
At time: 526.2902245521545 and batch: 600, loss is 5.900029840469361 and perplexity is 365.04836091723024
At time: 527.9756436347961 and batch: 650, loss is 5.9007830238342285 and perplexity is 365.3234128393087
At time: 529.6612522602081 and batch: 700, loss is 5.889359931945801 and perplexity is 361.17403441089215
At time: 531.3452928066254 and batch: 750, loss is 5.873878555297852 and perplexity is 355.62562249268154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.621462799781977 and perplexity of 276.2932494801402
Finished 19 epochs...
Completing Train Step...
At time: 535.8081369400024 and batch: 50, loss is 5.926245489120483 and perplexity is 374.74488549853277
At time: 537.5474846363068 and batch: 100, loss is 5.916795234680176 and perplexity is 371.2201321206456
At time: 539.2401268482208 and batch: 150, loss is 5.9136110496521 and perplexity is 370.03997844479386
At time: 540.9300236701965 and batch: 200, loss is 5.9322756481170655 and perplexity is 377.0114838471356
At time: 542.6190419197083 and batch: 250, loss is 5.968929595947266 and perplexity is 391.0868256999602
At time: 544.3145716190338 and batch: 300, loss is 5.966597585678101 and perplexity is 390.1758697983867
At time: 546.0065114498138 and batch: 350, loss is 5.8992582702636716 and perplexity is 364.7668091107712
At time: 547.693372964859 and batch: 400, loss is 5.92165542602539 and perplexity is 373.0287244862976
At time: 549.379031419754 and batch: 450, loss is 5.895198516845703 and perplexity is 363.28894771693723
At time: 551.0945465564728 and batch: 500, loss is 5.879203844070434 and perplexity is 357.52448313056954
At time: 552.7834768295288 and batch: 550, loss is 5.905255250930786 and perplexity is 366.96088094059775
At time: 554.4579107761383 and batch: 600, loss is 5.893824272155761 and perplexity is 362.7900426969273
At time: 556.1503937244415 and batch: 650, loss is 5.892905445098877 and perplexity is 362.45685448434233
At time: 557.8311104774475 and batch: 700, loss is 5.881426973342895 and perplexity is 358.3201904270112
At time: 559.4970188140869 and batch: 750, loss is 5.868037023544312 and perplexity is 353.5542799233308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.619570000227108 and perplexity of 275.770776364939
Finished 20 epochs...
Completing Train Step...
At time: 564.0236320495605 and batch: 50, loss is 5.918837356567383 and perplexity is 371.9789834471549
At time: 565.739893913269 and batch: 100, loss is 5.90631498336792 and perplexity is 367.3499674166564
At time: 567.4270882606506 and batch: 150, loss is 5.900544586181641 and perplexity is 365.23631636626794
At time: 569.1118783950806 and batch: 200, loss is 5.9135878467559815 and perplexity is 370.0313925452237
At time: 570.8038425445557 and batch: 250, loss is 5.950176429748535 and perplexity is 383.8210505392059
At time: 572.4947335720062 and batch: 300, loss is 5.946163454055786 and perplexity is 382.2838723851995
At time: 574.1877310276031 and batch: 350, loss is 5.868731107711792 and perplexity is 353.799761533965
At time: 575.8596866130829 and batch: 400, loss is 5.882264757156372 and perplexity is 358.6205110669126
At time: 577.5229113101959 and batch: 450, loss is 5.844227018356324 and perplexity is 345.23557785084193
At time: 579.1816954612732 and batch: 500, loss is 5.8187368202209475 and perplexity is 336.54666607286345
At time: 580.8445088863373 and batch: 550, loss is 5.841971015930175 and perplexity is 344.4576034356632
At time: 582.5041308403015 and batch: 600, loss is 5.827452116012573 and perplexity is 339.49258846109353
At time: 584.1728100776672 and batch: 650, loss is 5.82321325302124 and perplexity is 338.05657158162717
At time: 585.8541898727417 and batch: 700, loss is 5.812872114181519 and perplexity is 334.57869523368595
At time: 587.5356631278992 and batch: 750, loss is 5.794794120788574 and perplexity is 328.58452829416865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.553211655727653 and perplexity of 258.06504246526606
Finished 21 epochs...
Completing Train Step...
At time: 591.967832326889 and batch: 50, loss is 5.845392274856567 and perplexity is 345.63810032746795
At time: 593.6860256195068 and batch: 100, loss is 5.830952100753784 and perplexity is 340.68288914248495
At time: 595.3776745796204 and batch: 150, loss is 5.82761926651001 and perplexity is 339.54933955898593
At time: 597.0644073486328 and batch: 200, loss is 5.84507607460022 and perplexity is 345.5288267486183
At time: 598.7528052330017 and batch: 250, loss is 5.881947441101074 and perplexity is 358.50673307373853
At time: 600.4416496753693 and batch: 300, loss is 5.883835706710816 and perplexity is 359.18432854744435
At time: 602.1384620666504 and batch: 350, loss is 5.810264568328858 and perplexity is 333.7074024062213
At time: 603.8284945487976 and batch: 400, loss is 5.835790853500367 and perplexity is 342.33536414282224
At time: 605.5157537460327 and batch: 450, loss is 5.806594486236572 and perplexity is 332.4849135340388
At time: 607.2025420665741 and batch: 500, loss is 5.787876892089844 and perplexity is 326.3194769357412
At time: 608.8919229507446 and batch: 550, loss is 5.812587909698486 and perplexity is 334.4836199796199
At time: 610.5817613601685 and batch: 600, loss is 5.801228418350219 and perplexity is 330.70555526512914
At time: 612.2705206871033 and batch: 650, loss is 5.800429430007934 and perplexity is 330.441430911934
At time: 613.9586746692657 and batch: 700, loss is 5.7896761798858645 and perplexity is 326.9071481244489
At time: 615.6473736763 and batch: 750, loss is 5.772938966751099 and perplexity is 321.4811680696697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.534776554551235 and perplexity of 253.35117114584418
Finished 22 epochs...
Completing Train Step...
At time: 620.1969637870789 and batch: 50, loss is 5.824930620193482 and perplexity is 338.63763764910254
At time: 621.9126510620117 and batch: 100, loss is 5.813512716293335 and perplexity is 334.7930957177908
At time: 623.6010842323303 and batch: 150, loss is 5.811090688705445 and perplexity is 333.9831987959903
At time: 625.2874531745911 and batch: 200, loss is 5.828602056503296 and perplexity is 339.8832092871724
At time: 626.9720373153687 and batch: 250, loss is 5.8634302520751955 and perplexity is 351.92928202462895
At time: 628.659681558609 and batch: 300, loss is 5.8657457065582275 and perplexity is 352.7451023913757
At time: 630.3476958274841 and batch: 350, loss is 5.794807252883911 and perplexity is 328.58884332585296
At time: 632.031590461731 and batch: 400, loss is 5.821805963516235 and perplexity is 337.5811627137583
At time: 633.7130885124207 and batch: 450, loss is 5.79290431022644 and perplexity is 327.96415216336015
At time: 635.4278128147125 and batch: 500, loss is 5.772669095993042 and perplexity is 321.39442140886166
At time: 637.1144270896912 and batch: 550, loss is 5.798034410476685 and perplexity is 329.65096420042323
At time: 638.7992751598358 and batch: 600, loss is 5.786243839263916 and perplexity is 325.7870148794425
At time: 640.4892365932465 and batch: 650, loss is 5.7830759716033935 and perplexity is 324.75659770512874
At time: 642.1755278110504 and batch: 700, loss is 5.774367809295654 and perplexity is 321.9408423627688
At time: 643.8649883270264 and batch: 750, loss is 5.758027200698852 and perplexity is 316.7228815003053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.526536453601926 and perplexity of 251.27210952268683
Finished 23 epochs...
Completing Train Step...
At time: 648.3410491943359 and batch: 50, loss is 5.810821161270142 and perplexity is 333.8931932910059
At time: 650.0577869415283 and batch: 100, loss is 5.800192012786865 and perplexity is 330.3629877379293
At time: 651.7471706867218 and batch: 150, loss is 5.797051649093628 and perplexity is 329.3271551025199
At time: 653.4342174530029 and batch: 200, loss is 5.8123074150085445 and perplexity is 334.3898122572422
At time: 655.12140417099 and batch: 250, loss is 5.84895339012146 and perplexity is 346.87115165854914
At time: 656.8040552139282 and batch: 300, loss is 5.849734411239624 and perplexity is 347.1421711754827
At time: 658.4882044792175 and batch: 350, loss is 5.777395982742309 and perplexity is 322.91721263617745
At time: 660.1697039604187 and batch: 400, loss is 5.8023992538452145 and perplexity is 331.0929838309249
At time: 661.85324883461 and batch: 450, loss is 5.770183076858521 and perplexity is 320.5964210605681
At time: 663.5357329845428 and batch: 500, loss is 5.750166397094727 and perplexity is 314.2429450486518
At time: 665.2092864513397 and batch: 550, loss is 5.776802673339843 and perplexity is 322.72567964248395
At time: 666.8803715705872 and batch: 600, loss is 5.765301542282105 and perplexity is 319.03523214510034
At time: 668.5670938491821 and batch: 650, loss is 5.757727193832397 and perplexity is 316.6278767128486
At time: 670.2531199455261 and batch: 700, loss is 5.747671594619751 and perplexity is 313.459948089094
At time: 671.9401819705963 and batch: 750, loss is 5.734290437698364 and perplexity is 309.29342992145143
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.504870392555414 and perplexity of 245.88658488159587
Finished 24 epochs...
Completing Train Step...
At time: 676.4867296218872 and batch: 50, loss is 5.789244642257691 and perplexity is 326.76610582383296
At time: 678.2026109695435 and batch: 100, loss is 5.774763603210449 and perplexity is 322.0682898088979
At time: 679.8894007205963 and batch: 150, loss is 5.771971826553345 and perplexity is 321.1704010110891
At time: 681.5734572410583 and batch: 200, loss is 5.788478231430053 and perplexity is 326.5157646865577
At time: 683.2562403678894 and batch: 250, loss is 5.825764026641846 and perplexity is 338.91997807580213
At time: 684.9365422725677 and batch: 300, loss is 5.827708339691162 and perplexity is 339.57958564585607
At time: 686.6260251998901 and batch: 350, loss is 5.756699628829956 and perplexity is 316.3026880927775
At time: 688.3112094402313 and batch: 400, loss is 5.786802148818969 and perplexity is 325.9689556676762
At time: 690.0016989707947 and batch: 450, loss is 5.755506935119629 and perplexity is 315.92566074989395
At time: 691.6880300045013 and batch: 500, loss is 5.734397859573364 and perplexity is 309.32665658622204
At time: 693.3750848770142 and batch: 550, loss is 5.7617912769317625 and perplexity is 317.917297096425
At time: 695.0613920688629 and batch: 600, loss is 5.750581197738647 and perplexity is 314.3733202625973
At time: 696.7563247680664 and batch: 650, loss is 5.744752569198608 and perplexity is 312.5462846842193
At time: 698.4480185508728 and batch: 700, loss is 5.733628625869751 and perplexity is 309.08880359054893
At time: 700.1324005126953 and batch: 750, loss is 5.720689220428467 and perplexity is 305.11514209389077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.494206982989644 and perplexity of 243.27852563535123
Finished 25 epochs...
Completing Train Step...
At time: 704.647102355957 and batch: 50, loss is 5.774878330230713 and perplexity is 322.10524186376836
At time: 706.3629019260406 and batch: 100, loss is 5.759803457260132 and perplexity is 317.28596253682286
At time: 708.0511028766632 and batch: 150, loss is 5.756808614730835 and perplexity is 316.33716250476783
At time: 709.7380590438843 and batch: 200, loss is 5.771742782592773 and perplexity is 321.09684729426215
At time: 711.4245331287384 and batch: 250, loss is 5.80882308959961 and perplexity is 333.2267168161201
At time: 713.1113848686218 and batch: 300, loss is 5.809597606658936 and perplexity is 333.48490656619344
At time: 714.7979884147644 and batch: 350, loss is 5.739620532989502 and perplexity is 310.94639469234573
At time: 716.4817409515381 and batch: 400, loss is 5.771906318664551 and perplexity is 321.1493625052775
At time: 718.1676878929138 and batch: 450, loss is 5.739075422286987 and perplexity is 310.776940674486
At time: 719.8807344436646 and batch: 500, loss is 5.720186996459961 and perplexity is 304.9619444293667
At time: 721.5673518180847 and batch: 550, loss is 5.7471404647827145 and perplexity is 313.2935043634751
At time: 723.2526943683624 and batch: 600, loss is 5.733897504806518 and perplexity is 309.17192223335036
At time: 724.9507851600647 and batch: 650, loss is 5.729355430603027 and perplexity is 307.77082477401416
At time: 726.6421656608582 and batch: 700, loss is 5.716410713195801 and perplexity is 303.81249343465043
At time: 728.3358938694 and batch: 750, loss is 5.704971771240235 and perplexity is 300.3570012232985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.47963129087936 and perplexity of 239.75829000683748
Finished 26 epochs...
Completing Train Step...
At time: 732.8833892345428 and batch: 50, loss is 5.7579059982299805 and perplexity is 316.6844962313609
At time: 734.6022326946259 and batch: 100, loss is 5.7441246032714846 and perplexity is 312.35007787883046
At time: 736.2932240962982 and batch: 150, loss is 5.738977479934692 and perplexity is 310.7465039404243
At time: 737.9813752174377 and batch: 200, loss is 5.756309747695923 and perplexity is 316.17939167908065
At time: 739.6738648414612 and batch: 250, loss is 5.792371110916138 and perplexity is 327.7893285156888
At time: 741.3634955883026 and batch: 300, loss is 5.796715297698975 and perplexity is 329.21640408127814
At time: 743.0542960166931 and batch: 350, loss is 5.722889442443847 and perplexity is 305.7872022163713
At time: 744.7413170337677 and batch: 400, loss is 5.748645877838134 and perplexity is 313.76549567692655
At time: 746.4290354251862 and batch: 450, loss is 5.708478298187256 and perplexity is 301.41205985651885
At time: 748.1175136566162 and batch: 500, loss is 5.68461841583252 and perplexity is 294.30552129433846
At time: 749.8066577911377 and batch: 550, loss is 5.706514682769775 and perplexity is 300.820783198737
At time: 751.4951722621918 and batch: 600, loss is 5.692616147994995 and perplexity is 296.66873560850036
At time: 753.1819705963135 and batch: 650, loss is 5.683425378799439 and perplexity is 293.95461327314797
At time: 754.87291264534 and batch: 700, loss is 5.667497720718384 and perplexity is 289.30969423982987
At time: 756.5614197254181 and batch: 750, loss is 5.653257246017456 and perplexity is 285.21898282424775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.4358740518259445 and perplexity of 229.49334972002188
Finished 27 epochs...
Completing Train Step...
At time: 760.9711952209473 and batch: 50, loss is 5.708642740249633 and perplexity is 301.4616287527612
At time: 762.6696903705597 and batch: 100, loss is 5.694210186004638 and perplexity is 297.1420139609596
At time: 764.3308005332947 and batch: 150, loss is 5.685997390747071 and perplexity is 294.7116411756259
At time: 765.9891037940979 and batch: 200, loss is 5.696074075698853 and perplexity is 297.6963703676061
At time: 767.6492691040039 and batch: 250, loss is 5.731776008605957 and perplexity is 308.5167104354271
At time: 769.3159604072571 and batch: 300, loss is 5.736017036437988 and perplexity is 309.82791685718064
At time: 770.9936668872833 and batch: 350, loss is 5.666728200912476 and perplexity is 289.0871503370235
At time: 772.6784870624542 and batch: 400, loss is 5.69672046661377 and perplexity is 297.88886080214735
At time: 774.3636801242828 and batch: 450, loss is 5.665028257369995 and perplexity is 288.5961359691906
At time: 776.0503108501434 and batch: 500, loss is 5.647308988571167 and perplexity is 283.52746267192254
At time: 777.7391963005066 and batch: 550, loss is 5.6677280616760255 and perplexity is 289.3763417873914
At time: 779.4240927696228 and batch: 600, loss is 5.655532026290894 and perplexity is 285.86853185054446
At time: 781.1113376617432 and batch: 650, loss is 5.65003023147583 and perplexity is 284.3000605044128
At time: 782.8008439540863 and batch: 700, loss is 5.635511560440063 and perplexity is 280.20222102451385
At time: 784.4928596019745 and batch: 750, loss is 5.6273486614227295 and perplexity is 277.9242685865878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.409982725631359 and perplexity of 223.6277246194267
Finished 28 epochs...
Completing Train Step...
At time: 789.0333249568939 and batch: 50, loss is 5.680760765075684 and perplexity is 293.1723804134977
At time: 790.7500953674316 and batch: 100, loss is 5.666831617355347 and perplexity is 289.1170482477321
At time: 792.4378604888916 and batch: 150, loss is 5.659981269836425 and perplexity is 287.1432642653773
At time: 794.1225984096527 and batch: 200, loss is 5.67324007987976 and perplexity is 290.9757935049563
At time: 795.8105735778809 and batch: 250, loss is 5.709792947769165 and perplexity is 301.80857167493195
At time: 797.4982883930206 and batch: 300, loss is 5.715435514450073 and perplexity is 303.516360289921
At time: 799.186286687851 and batch: 350, loss is 5.645171775817871 and perplexity is 282.9221512328597
At time: 800.8729314804077 and batch: 400, loss is 5.6778608989715575 and perplexity is 292.32345125022607
At time: 802.5573801994324 and batch: 450, loss is 5.650203981399536 and perplexity is 284.3494619098561
At time: 804.2961337566376 and batch: 500, loss is 5.632329339981079 and perplexity is 279.3119730180865
At time: 805.974967956543 and batch: 550, loss is 5.654098052978515 and perplexity is 285.4588977773798
At time: 807.6338212490082 and batch: 600, loss is 5.639225482940674 and perplexity is 281.24480519792553
At time: 809.2930171489716 and batch: 650, loss is 5.6371884059906 and perplexity is 280.67247103021634
At time: 810.9522173404694 and batch: 700, loss is 5.62152585029602 and perplexity is 276.3106704607418
At time: 812.6124992370605 and batch: 750, loss is 5.6102276802062985 and perplexity is 273.2064346214155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.400670960892079 and perplexity of 221.55502109917683
Finished 29 epochs...
Completing Train Step...
At time: 817.0636639595032 and batch: 50, loss is 5.668680057525635 and perplexity is 289.65195803569463
At time: 818.7639944553375 and batch: 100, loss is 5.6535240745544435 and perplexity is 285.29509754249364
At time: 820.426903963089 and batch: 150, loss is 5.649758024215698 and perplexity is 284.22268249579326
At time: 822.086254119873 and batch: 200, loss is 5.664152040481567 and perplexity is 288.3433739142816
At time: 823.748129606247 and batch: 250, loss is 5.6983780097961425 and perplexity is 298.383033895953
At time: 825.4100525379181 and batch: 300, loss is 5.705568170547485 and perplexity is 300.53618735868804
At time: 827.0721650123596 and batch: 350, loss is 5.638221139907837 and perplexity is 280.96248073633825
At time: 828.729986667633 and batch: 400, loss is 5.669383716583252 and perplexity is 289.8558459849114
At time: 830.3895452022552 and batch: 450, loss is 5.64101019859314 and perplexity is 281.7471953823984
At time: 832.0534827709198 and batch: 500, loss is 5.623200550079345 and perplexity is 276.77379557021226
At time: 833.73970079422 and batch: 550, loss is 5.6449551773071285 and perplexity is 282.8608773524018
At time: 835.4282822608948 and batch: 600, loss is 5.631802072525025 and perplexity is 279.164739723701
At time: 837.1040992736816 and batch: 650, loss is 5.629906091690064 and perplexity is 278.63595017321995
At time: 838.7820856571198 and batch: 700, loss is 5.612014064788818 and perplexity is 273.69492256783246
At time: 840.4701206684113 and batch: 750, loss is 5.603049955368042 and perplexity is 271.25245496286163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.398533665856649 and perplexity of 221.0819983271834
Finished 30 epochs...
Completing Train Step...
At time: 845.013772726059 and batch: 50, loss is 5.660815439224243 and perplexity is 287.38289031680875
At time: 846.7296175956726 and batch: 100, loss is 5.645812129974365 and perplexity is 283.103379627351
At time: 848.4152517318726 and batch: 150, loss is 5.640084457397461 and perplexity is 281.48649108783724
At time: 850.100168466568 and batch: 200, loss is 5.653470602035522 and perplexity is 285.2798425028585
At time: 851.7786974906921 and batch: 250, loss is 5.691493244171142 and perplexity is 296.33579211761196
At time: 853.4559526443481 and batch: 300, loss is 5.699379396438599 and perplexity is 298.6819803359366
At time: 855.136378288269 and batch: 350, loss is 5.631711063385009 and perplexity is 279.13933433689493
At time: 856.7965581417084 and batch: 400, loss is 5.664189729690552 and perplexity is 288.35424155275547
At time: 858.4812569618225 and batch: 450, loss is 5.633694534301758 and perplexity is 279.69354854045275
At time: 860.1700563430786 and batch: 500, loss is 5.617927465438843 and perplexity is 275.31818507139974
At time: 861.8580713272095 and batch: 550, loss is 5.6391582107543945 and perplexity is 281.22588588137916
At time: 863.5470926761627 and batch: 600, loss is 5.62567135810852 and perplexity is 277.45849601992154
At time: 865.2328498363495 and batch: 650, loss is 5.6226655387878415 and perplexity is 276.62575806878834
At time: 866.9202754497528 and batch: 700, loss is 5.6072216796875 and perplexity is 272.38640905332784
At time: 868.60924077034 and batch: 750, loss is 5.597299289703369 and perplexity is 269.69704937884825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3915217199990915 and perplexity of 219.53720565556588
Finished 31 epochs...
Completing Train Step...
At time: 873.1662540435791 and batch: 50, loss is 5.65332968711853 and perplexity is 285.2396451498026
At time: 874.8843445777893 and batch: 100, loss is 5.638491859436035 and perplexity is 281.03855306323226
At time: 876.5742816925049 and batch: 150, loss is 5.633013734817505 and perplexity is 279.5031981194438
At time: 878.2611937522888 and batch: 200, loss is 5.648333044052124 and perplexity is 283.81795924098867
At time: 879.9520313739777 and batch: 250, loss is 5.684300394058227 and perplexity is 294.2119406114084
At time: 881.6375396251678 and batch: 300, loss is 5.692081594467163 and perplexity is 296.51019266790286
At time: 883.3271203041077 and batch: 350, loss is 5.625007486343383 and perplexity is 277.27436028643655
At time: 885.0119285583496 and batch: 400, loss is 5.655861339569092 and perplexity is 285.9626876564294
At time: 886.702490568161 and batch: 450, loss is 5.6277362251281735 and perplexity is 278.0320028215452
At time: 888.4186613559723 and batch: 500, loss is 5.610312271118164 and perplexity is 273.2295463803562
At time: 890.1082668304443 and batch: 550, loss is 5.632670574188232 and perplexity is 279.4073000812501
At time: 891.7998564243317 and batch: 600, loss is 5.618835611343384 and perplexity is 275.5683277194369
At time: 893.4901039600372 and batch: 650, loss is 5.618779811859131 and perplexity is 275.5529515778682
At time: 895.1802499294281 and batch: 700, loss is 5.601765823364258 and perplexity is 270.90435455561027
At time: 896.8698272705078 and batch: 750, loss is 5.590598649978638 and perplexity is 267.89594762203063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.39216791197311 and perplexity of 219.67911468115764
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 901.3966069221497 and batch: 50, loss is 5.649647808074951 and perplexity is 284.19135829486373
At time: 903.1151967048645 and batch: 100, loss is 5.633623418807983 and perplexity is 279.6736587028889
At time: 904.8037052154541 and batch: 150, loss is 5.6231835746765135 and perplexity is 276.7690972634171
At time: 906.491801738739 and batch: 200, loss is 5.635473852157593 and perplexity is 280.1916552792238
At time: 908.1724708080292 and batch: 250, loss is 5.665525693893432 and perplexity is 288.73972993922615
At time: 909.8622033596039 and batch: 300, loss is 5.660326290130615 and perplexity is 287.2423516114784
At time: 911.5525641441345 and batch: 350, loss is 5.592562856674195 and perplexity is 268.4226677603283
At time: 913.2403881549835 and batch: 400, loss is 5.615882616043091 and perplexity is 274.7557760638485
At time: 914.9286766052246 and batch: 450, loss is 5.577248630523681 and perplexity is 264.3432982420438
At time: 916.6179914474487 and batch: 500, loss is 5.539842844009399 and perplexity is 254.6379784301201
At time: 918.3084592819214 and batch: 550, loss is 5.553181085586548 and perplexity is 258.05715350108767
At time: 919.9960033893585 and batch: 600, loss is 5.532313594818115 and perplexity is 252.7279452182049
At time: 921.6837668418884 and batch: 650, loss is 5.514958848953247 and perplexity is 248.37975595021805
At time: 923.3742763996124 and batch: 700, loss is 5.497677040100098 and perplexity is 244.12418240312618
At time: 925.0717356204987 and batch: 750, loss is 5.506976337432861 and perplexity is 246.40495411239246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.339464764262354 and perplexity of 208.40113669654687
Finished 33 epochs...
Completing Train Step...
At time: 929.6026844978333 and batch: 50, loss is 5.610127630233765 and perplexity is 273.17910169248785
At time: 931.308581829071 and batch: 100, loss is 5.596862955093384 and perplexity is 269.5793968917859
At time: 932.9929187297821 and batch: 150, loss is 5.5911757755279545 and perplexity is 268.05060184111966
At time: 934.6803798675537 and batch: 200, loss is 5.603988933563232 and perplexity is 271.5072747198389
At time: 936.3710737228394 and batch: 250, loss is 5.634707660675049 and perplexity is 279.97705704163815
At time: 938.0553321838379 and batch: 300, loss is 5.633894681930542 and perplexity is 279.74953414348533
At time: 939.7408308982849 and batch: 350, loss is 5.571612615585327 and perplexity is 262.8576459760815
At time: 941.4257130622864 and batch: 400, loss is 5.596026973724365 and perplexity is 269.3541277120512
At time: 943.112065076828 and batch: 450, loss is 5.560045375823974 and perplexity is 259.8346262657235
At time: 944.7943613529205 and batch: 500, loss is 5.526262607574463 and perplexity is 251.20330907447644
At time: 946.4599132537842 and batch: 550, loss is 5.546133069992066 and perplexity is 256.24475706062015
At time: 948.121618270874 and batch: 600, loss is 5.529935731887817 and perplexity is 252.12770673106806
At time: 949.7851271629333 and batch: 650, loss is 5.519909200668335 and perplexity is 249.61237152418593
At time: 951.4532527923584 and batch: 700, loss is 5.506613254547119 and perplexity is 246.3155049303085
At time: 953.1210236549377 and batch: 750, loss is 5.512234411239624 and perplexity is 247.7039817461683
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.33378423646439 and perplexity of 207.22066427394802
Finished 34 epochs...
Completing Train Step...
At time: 957.6418099403381 and batch: 50, loss is 5.6007472515106205 and perplexity is 270.6285594874235
At time: 959.3624107837677 and batch: 100, loss is 5.5848886680603025 and perplexity is 266.3706255301799
At time: 961.051999092102 and batch: 150, loss is 5.579095773696899 and perplexity is 264.83202940002843
At time: 962.7154083251953 and batch: 200, loss is 5.592454032897949 and perplexity is 268.3934585813481
At time: 964.3776295185089 and batch: 250, loss is 5.623165340423584 and perplexity is 276.76405063170535
At time: 966.0537614822388 and batch: 300, loss is 5.623036813735962 and perplexity is 276.7284813508748
At time: 967.7483150959015 and batch: 350, loss is 5.563011531829834 and perplexity is 260.6064804570215
At time: 969.4371864795685 and batch: 400, loss is 5.588071937561035 and perplexity is 267.2199060455413
At time: 971.129531621933 and batch: 450, loss is 5.553647146224976 and perplexity is 258.177451813776
At time: 972.8745992183685 and batch: 500, loss is 5.522835559844971 and perplexity is 250.3438968090234
At time: 974.5651438236237 and batch: 550, loss is 5.54584077835083 and perplexity is 256.1698698050135
At time: 976.2527632713318 and batch: 600, loss is 5.531568689346313 and perplexity is 252.5397568888947
At time: 977.9405827522278 and batch: 650, loss is 5.524179201126099 and perplexity is 250.6804952864187
At time: 979.6301186084747 and batch: 700, loss is 5.511472549438476 and perplexity is 247.51533741406368
At time: 981.3214657306671 and batch: 750, loss is 5.514623079299927 and perplexity is 248.29637156542722
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.331781786541606 and perplexity of 206.80613045090112
Finished 35 epochs...
Completing Train Step...
At time: 985.855672121048 and batch: 50, loss is 5.595469207763672 and perplexity is 269.203933038873
At time: 987.576194524765 and batch: 100, loss is 5.577789916992187 and perplexity is 264.48642242451956
At time: 989.2648227214813 and batch: 150, loss is 5.5717942428588865 and perplexity is 262.9053924295523
At time: 990.9531109333038 and batch: 200, loss is 5.585603075027466 and perplexity is 266.5609905518587
At time: 992.6415612697601 and batch: 250, loss is 5.616746129989624 and perplexity is 274.9931339745543
At time: 994.3305914402008 and batch: 300, loss is 5.617223844528199 and perplexity is 275.1245335759131
At time: 996.019633769989 and batch: 350, loss is 5.5584645652771 and perplexity is 259.42420143549924
At time: 997.7101120948792 and batch: 400, loss is 5.5844482326507565 and perplexity is 266.25333230657407
At time: 999.3970379829407 and batch: 450, loss is 5.550908718109131 and perplexity is 257.47141857270213
At time: 1001.0894494056702 and batch: 500, loss is 5.521709079742432 and perplexity is 250.062048168723
At time: 1002.7782905101776 and batch: 550, loss is 5.546485681533813 and perplexity is 256.33512785142756
At time: 1004.4645195007324 and batch: 600, loss is 5.533054304122925 and perplexity is 252.9152125055626
At time: 1006.1504645347595 and batch: 650, loss is 5.526623373031616 and perplexity is 251.2939509003499
At time: 1007.8374824523926 and batch: 700, loss is 5.51399793624878 and perplexity is 248.1411993215965
At time: 1009.5146157741547 and batch: 750, loss is 5.515989713668823 and perplexity is 248.63593389640693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.33074135004088 and perplexity of 206.59107370005265
Finished 36 epochs...
Completing Train Step...
At time: 1013.9966833591461 and batch: 50, loss is 5.5917281150817875 and perplexity is 268.1986976867768
At time: 1015.7003252506256 and batch: 100, loss is 5.572677068710327 and perplexity is 263.1375945884253
At time: 1017.3756055831909 and batch: 150, loss is 5.566809225082397 and perplexity is 261.59806560578284
At time: 1019.060179233551 and batch: 200, loss is 5.581088962554932 and perplexity is 265.3604160625518
At time: 1020.744790315628 and batch: 250, loss is 5.612520227432251 and perplexity is 273.8334917796543
At time: 1022.4319241046906 and batch: 300, loss is 5.613579883575439 and perplexity is 274.12381491561035
At time: 1024.1184282302856 and batch: 350, loss is 5.555823192596436 and perplexity is 258.73986962267526
At time: 1025.801991224289 and batch: 400, loss is 5.582405271530152 and perplexity is 265.70994235171094
At time: 1027.486935377121 and batch: 450, loss is 5.549471778869629 and perplexity is 257.1017134738003
At time: 1029.1736714839935 and batch: 500, loss is 5.521178483963013 and perplexity is 249.92940149536633
At time: 1030.8579943180084 and batch: 550, loss is 5.546907405853272 and perplexity is 256.4432534067597
At time: 1032.542090177536 and batch: 600, loss is 5.534037256240845 and perplexity is 253.1639382720822
At time: 1034.2271316051483 and batch: 650, loss is 5.528030014038086 and perplexity is 251.64768000296934
At time: 1035.9121587276459 and batch: 700, loss is 5.5152614784240725 and perplexity is 248.45493435935228
At time: 1037.5970885753632 and batch: 750, loss is 5.516350135803223 and perplexity is 248.72556394174626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.329780046329942 and perplexity of 206.39257235958218
Finished 37 epochs...
Completing Train Step...
At time: 1042.0976316928864 and batch: 50, loss is 5.58863748550415 and perplexity is 267.37107445622735
At time: 1043.7982385158539 and batch: 100, loss is 5.568954524993896 and perplexity is 262.15987432140093
At time: 1045.4645464420319 and batch: 150, loss is 5.563202905654907 and perplexity is 260.65635848854805
At time: 1047.126317024231 and batch: 200, loss is 5.5779263782501225 and perplexity is 264.5225170371325
At time: 1048.7909097671509 and batch: 250, loss is 5.609542150497436 and perplexity is 273.01920767597915
At time: 1050.451980829239 and batch: 300, loss is 5.611144371032715 and perplexity is 273.4569952795334
At time: 1052.111798286438 and batch: 350, loss is 5.554130725860595 and perplexity is 258.3023313640193
At time: 1053.7708477973938 and batch: 400, loss is 5.580992221832275 and perplexity is 265.3347461458206
At time: 1055.4359896183014 and batch: 450, loss is 5.5484818267822265 and perplexity is 256.8473210348107
At time: 1057.1609225273132 and batch: 500, loss is 5.520749416351318 and perplexity is 249.82218788656286
At time: 1058.8479945659637 and batch: 550, loss is 5.5471395492553714 and perplexity is 256.5027919265209
At time: 1060.53257894516 and batch: 600, loss is 5.53450288772583 and perplexity is 253.2818468214407
At time: 1062.2033042907715 and batch: 650, loss is 5.528682994842529 and perplexity is 251.81205476843675
At time: 1063.8773651123047 and batch: 700, loss is 5.515734624862671 and perplexity is 248.57251774158232
At time: 1065.546134710312 and batch: 750, loss is 5.516242723464966 and perplexity is 248.69884918211216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3289674270984735 and perplexity of 206.2249219132646
Finished 38 epochs...
Completing Train Step...
At time: 1070.124639749527 and batch: 50, loss is 5.586174383163452 and perplexity is 266.71332252460184
At time: 1071.884325504303 and batch: 100, loss is 5.565969247817993 and perplexity is 261.37842143922927
At time: 1073.589571237564 and batch: 150, loss is 5.56034743309021 and perplexity is 259.913123057273
At time: 1075.2743349075317 and batch: 200, loss is 5.57557333946228 and perplexity is 263.9008170232618
At time: 1076.9596469402313 and batch: 250, loss is 5.607255353927612 and perplexity is 272.39558161310816
At time: 1078.6446495056152 and batch: 300, loss is 5.609311456680298 and perplexity is 272.9562310972413
At time: 1080.3276071548462 and batch: 350, loss is 5.552810449600219 and perplexity is 257.96152595607003
At time: 1082.006049156189 and batch: 400, loss is 5.579904794692993 and perplexity is 265.0463707638923
At time: 1083.686384677887 and batch: 450, loss is 5.547713584899903 and perplexity is 256.65007594110125
At time: 1085.3490889072418 and batch: 500, loss is 5.520364675521851 and perplexity is 249.7260895784328
At time: 1087.0125668048859 and batch: 550, loss is 5.547107009887696 and perplexity is 256.49444562365693
At time: 1088.6849036216736 and batch: 600, loss is 5.5346316051483155 and perplexity is 253.31445070622482
At time: 1090.3735735416412 and batch: 650, loss is 5.529027643203736 and perplexity is 251.89885633759485
At time: 1092.0647294521332 and batch: 700, loss is 5.5158975791931155 and perplexity is 248.61302701026852
At time: 1093.7576599121094 and batch: 750, loss is 5.51599573135376 and perplexity is 248.63743011362303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.328278297601744 and perplexity of 206.08285519341933
Finished 39 epochs...
Completing Train Step...
At time: 1098.3815410137177 and batch: 50, loss is 5.5840800094604495 and perplexity is 266.15530970332765
At time: 1100.1078464984894 and batch: 100, loss is 5.563645257949829 and perplexity is 260.7716859327106
At time: 1101.800531387329 and batch: 150, loss is 5.558212518692017 and perplexity is 259.35882269103024
At time: 1103.4764833450317 and batch: 200, loss is 5.573749761581421 and perplexity is 263.4200118566717
At time: 1105.1385869979858 and batch: 250, loss is 5.605466718673706 and perplexity is 271.9088007393191
At time: 1106.7999119758606 and batch: 300, loss is 5.607871103286743 and perplexity is 272.56336066753806
At time: 1108.4594333171844 and batch: 350, loss is 5.551762857437134 and perplexity is 257.69142898353823
At time: 1110.1336834430695 and batch: 400, loss is 5.579089508056641 and perplexity is 264.8303700630018
At time: 1111.8250291347504 and batch: 450, loss is 5.547046318054199 and perplexity is 256.47887897785927
At time: 1113.5170834064484 and batch: 500, loss is 5.51995732307434 and perplexity is 249.62438376109904
At time: 1115.20832157135 and batch: 550, loss is 5.547029552459716 and perplexity is 256.474578993027
At time: 1116.897879600525 and batch: 600, loss is 5.5346608543396 and perplexity is 253.32186005740678
At time: 1118.58766746521 and batch: 650, loss is 5.529171266555786 and perplexity is 251.935037493887
At time: 1120.2788996696472 and batch: 700, loss is 5.515968675613403 and perplexity is 248.63070313487304
At time: 1121.970802783966 and batch: 750, loss is 5.515661401748657 and perplexity is 248.554317154135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.327895763308503 and perplexity of 206.0040365104446
Finished 40 epochs...
Completing Train Step...
At time: 1126.485172510147 and batch: 50, loss is 5.58229850769043 and perplexity is 265.681575652309
At time: 1128.2235991954803 and batch: 100, loss is 5.561587257385254 and perplexity is 260.23556950900803
At time: 1129.9103209972382 and batch: 150, loss is 5.556239013671875 and perplexity is 258.8474814856104
At time: 1131.5965933799744 and batch: 200, loss is 5.572241840362548 and perplexity is 263.0230945665293
At time: 1133.2798900604248 and batch: 250, loss is 5.603925065994263 and perplexity is 271.48993476398135
At time: 1134.9575550556183 and batch: 300, loss is 5.606632661819458 and perplexity is 272.2260158332572
At time: 1136.630137681961 and batch: 350, loss is 5.550753812789917 and perplexity is 257.4315379693583
At time: 1138.3118026256561 and batch: 400, loss is 5.578303365707398 and perplexity is 264.6222575074943
At time: 1140.0008535385132 and batch: 450, loss is 5.546386175155639 and perplexity is 256.3096221402679
At time: 1141.7426598072052 and batch: 500, loss is 5.519480209350586 and perplexity is 249.50531294924093
At time: 1143.4144887924194 and batch: 550, loss is 5.546772422790528 and perplexity is 256.4086402471425
At time: 1145.083915233612 and batch: 600, loss is 5.534576053619385 and perplexity is 253.3003790920413
At time: 1146.7471690177917 and batch: 650, loss is 5.529091987609863 and perplexity is 251.91506514137745
At time: 1148.4102034568787 and batch: 700, loss is 5.515860233306885 and perplexity is 248.60374250981639
At time: 1150.0719199180603 and batch: 750, loss is 5.515265235900879 and perplexity is 248.45586792475945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.327437999636628 and perplexity of 205.90975692680104
Finished 41 epochs...
Completing Train Step...
At time: 1154.470796585083 and batch: 50, loss is 5.580692319869995 and perplexity is 265.2551836658586
At time: 1156.1897876262665 and batch: 100, loss is 5.559809350967408 and perplexity is 259.77330607215765
At time: 1157.8490295410156 and batch: 150, loss is 5.554683094024658 and perplexity is 258.4450487612126
At time: 1159.508347272873 and batch: 200, loss is 5.570830402374267 and perplexity is 262.65211564751974
At time: 1161.2015235424042 and batch: 250, loss is 5.602619733810425 and perplexity is 271.1357814087187
At time: 1162.8953697681427 and batch: 300, loss is 5.605525236129761 and perplexity is 271.9247126161741
At time: 1164.5848684310913 and batch: 350, loss is 5.549821681976319 and perplexity is 257.1916899026781
At time: 1166.2735896110535 and batch: 400, loss is 5.577565269470215 and perplexity is 264.42701287848007
At time: 1167.960985660553 and batch: 450, loss is 5.545785903930664 and perplexity is 256.1558130176265
At time: 1169.6486899852753 and batch: 500, loss is 5.519011201858521 and perplexity is 249.3883205254638
At time: 1171.3347532749176 and batch: 550, loss is 5.546537790298462 and perplexity is 256.34848550629846
At time: 1173.0227320194244 and batch: 600, loss is 5.534450531005859 and perplexity is 253.26858616185825
At time: 1174.7054421901703 and batch: 650, loss is 5.528942317962646 and perplexity is 251.8773639238834
At time: 1176.3963539600372 and batch: 700, loss is 5.515627403259277 and perplexity is 248.5458668264728
At time: 1178.0868790149689 and batch: 750, loss is 5.514718894958496 and perplexity is 248.32016338558535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.327051561932231 and perplexity of 205.8302010057151
Finished 42 epochs...
Completing Train Step...
At time: 1182.64976978302 and batch: 50, loss is 5.579137372970581 and perplexity is 264.84304644924816
At time: 1184.3498077392578 and batch: 100, loss is 5.558178768157959 and perplexity is 259.35006933996794
At time: 1186.0295791625977 and batch: 150, loss is 5.553259706497192 and perplexity is 258.07744298707263
At time: 1187.7159667015076 and batch: 200, loss is 5.569616422653199 and perplexity is 262.33345476848586
At time: 1189.403221130371 and batch: 250, loss is 5.601423015594483 and perplexity is 270.81150235414
At time: 1191.0907595157623 and batch: 300, loss is 5.60452428817749 and perplexity is 271.6526663067208
At time: 1192.7763826847076 and batch: 350, loss is 5.548914518356323 and perplexity is 256.9584807536543
At time: 1194.462436914444 and batch: 400, loss is 5.576753959655762 and perplexity is 264.2125676502472
At time: 1196.1481521129608 and batch: 450, loss is 5.545148029327392 and perplexity is 255.9924698317955
At time: 1197.8404920101166 and batch: 500, loss is 5.518441696166992 and perplexity is 249.24633289274377
At time: 1199.5298619270325 and batch: 550, loss is 5.546186256408691 and perplexity is 256.2583861634656
At time: 1201.2192006111145 and batch: 600, loss is 5.534207468032837 and perplexity is 253.20703342723093
At time: 1202.9070518016815 and batch: 650, loss is 5.528672332763672 and perplexity is 251.80936994276456
At time: 1204.600557088852 and batch: 700, loss is 5.515293912887573 and perplexity is 248.46299299254036
At time: 1206.2894024848938 and batch: 750, loss is 5.514079875946045 and perplexity is 248.16153276939494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.326653059138808 and perplexity of 205.74819343685132
Finished 43 epochs...
Completing Train Step...
At time: 1210.8532881736755 and batch: 50, loss is 5.577707090377808 and perplexity is 264.46451681679696
At time: 1212.5713665485382 and batch: 100, loss is 5.556711740493775 and perplexity is 258.9698745598551
At time: 1214.2575166225433 and batch: 150, loss is 5.551960973739624 and perplexity is 257.74248691416915
At time: 1215.9463617801666 and batch: 200, loss is 5.5685998058319095 and perplexity is 262.0668976817479
At time: 1217.6298291683197 and batch: 250, loss is 5.600362033843994 and perplexity is 270.5243286622818
At time: 1219.31436252594 and batch: 300, loss is 5.603584270477295 and perplexity is 271.3974279751568
At time: 1221.0021631717682 and batch: 350, loss is 5.548151168823242 and perplexity is 256.7624064634799
At time: 1222.6893787384033 and batch: 400, loss is 5.576095514297485 and perplexity is 264.0386553736284
At time: 1224.3779311180115 and batch: 450, loss is 5.544546499252319 and perplexity is 255.83852896687122
At time: 1226.0987150669098 and batch: 500, loss is 5.517920246124268 and perplexity is 249.11639726222234
At time: 1227.7844173908234 and batch: 550, loss is 5.545863695144654 and perplexity is 256.1757404643717
At time: 1229.4709720611572 and batch: 600, loss is 5.533935670852661 and perplexity is 253.13822182136855
At time: 1231.1586484909058 and batch: 650, loss is 5.528428611755371 and perplexity is 251.74800618734434
At time: 1232.8486549854279 and batch: 700, loss is 5.5149435043334964 and perplexity is 248.37594468655047
At time: 1234.5357966423035 and batch: 750, loss is 5.513458290100098 and perplexity is 248.00732700414238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.326258104900981 and perplexity of 205.6669483610279
Finished 44 epochs...
Completing Train Step...
At time: 1239.0711686611176 and batch: 50, loss is 5.576387042999268 and perplexity is 264.1156414413289
At time: 1240.7889778614044 and batch: 100, loss is 5.555340337753296 and perplexity is 258.61496598110534
At time: 1242.473182439804 and batch: 150, loss is 5.5508521366119385 and perplexity is 257.45685086649024
At time: 1244.1554436683655 and batch: 200, loss is 5.567477884292603 and perplexity is 261.7730440556787
At time: 1245.840003490448 and batch: 250, loss is 5.599330224990845 and perplexity is 270.245343219552
At time: 1247.5245370864868 and batch: 300, loss is 5.602751836776734 and perplexity is 271.171601615641
At time: 1249.210648059845 and batch: 350, loss is 5.547372055053711 and perplexity is 256.5624372466068
At time: 1250.8976092338562 and batch: 400, loss is 5.575438814163208 and perplexity is 263.86531807473335
At time: 1252.585916519165 and batch: 450, loss is 5.543923559188843 and perplexity is 255.67920652671467
At time: 1254.274535894394 and batch: 500, loss is 5.5173336696624755 and perplexity is 248.97031429594514
At time: 1255.9617948532104 and batch: 550, loss is 5.54541163444519 and perplexity is 256.05995965190107
At time: 1257.6468970775604 and batch: 600, loss is 5.533541812896728 and perplexity is 253.0385409500942
At time: 1259.3306822776794 and batch: 650, loss is 5.5280390548706055 and perplexity is 251.64995511778272
At time: 1261.0124144554138 and batch: 700, loss is 5.51441909790039 and perplexity is 248.245728889313
At time: 1262.6900124549866 and batch: 750, loss is 5.512718572616577 and perplexity is 247.82393948415574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3257329186727835 and perplexity of 205.55896327077636
Finished 45 epochs...
Completing Train Step...
At time: 1267.1355187892914 and batch: 50, loss is 5.575129251480103 and perplexity is 263.78364786056926
At time: 1268.8519093990326 and batch: 100, loss is 5.554168682098389 and perplexity is 258.3121357347986
At time: 1270.541015625 and batch: 150, loss is 5.5498114109039305 and perplexity is 257.1890482817796
At time: 1272.2307841777802 and batch: 200, loss is 5.566559658050537 and perplexity is 261.5327874989798
At time: 1273.9196336269379 and batch: 250, loss is 5.598497514724731 and perplexity is 270.0204008168058
At time: 1275.6064929962158 and batch: 300, loss is 5.601976804733276 and perplexity is 270.9615163570329
At time: 1277.2923321723938 and batch: 350, loss is 5.546722888946533 and perplexity is 256.39593965611476
At time: 1278.9831628799438 and batch: 400, loss is 5.5747903537750245 and perplexity is 263.6942673339552
At time: 1280.6776781082153 and batch: 450, loss is 5.543323583602906 and perplexity is 255.52585125427674
At time: 1282.3733007907867 and batch: 500, loss is 5.5168129825592045 and perplexity is 248.8407124081378
At time: 1284.063274860382 and batch: 550, loss is 5.545017461776734 and perplexity is 255.95904770399272
At time: 1285.7535691261292 and batch: 600, loss is 5.533171138763428 and perplexity is 252.94476348974905
At time: 1287.4459340572357 and batch: 650, loss is 5.52766710281372 and perplexity is 251.55637080487938
At time: 1289.1381006240845 and batch: 700, loss is 5.513938827514648 and perplexity is 248.12653244289336
At time: 1290.8339982032776 and batch: 750, loss is 5.512022695541382 and perplexity is 247.65154447580807
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.325337609579397 and perplexity of 205.47772000252766
Finished 46 epochs...
Completing Train Step...
At time: 1295.340945482254 and batch: 50, loss is 5.574012899398804 and perplexity is 263.4893367442464
At time: 1297.0535004138947 and batch: 100, loss is 5.55306074142456 and perplexity is 258.02609969781406
At time: 1298.7337067127228 and batch: 150, loss is 5.54884334564209 and perplexity is 256.9401929719371
At time: 1300.4187965393066 and batch: 200, loss is 5.56565486907959 and perplexity is 261.2962625360343
At time: 1302.1094403266907 and batch: 250, loss is 5.597665739059448 and perplexity is 269.7958977992934
At time: 1303.7959923744202 and batch: 300, loss is 5.601246194839478 and perplexity is 270.76362149311944
At time: 1305.4835724830627 and batch: 350, loss is 5.546022415161133 and perplexity is 256.21640390908567
At time: 1307.1723997592926 and batch: 400, loss is 5.574101066589355 and perplexity is 263.5125688829484
At time: 1308.8665027618408 and batch: 450, loss is 5.542752380371094 and perplexity is 255.37993573990292
At time: 1310.5859315395355 and batch: 500, loss is 5.516251268386841 and perplexity is 248.7009743034283
At time: 1312.274290561676 and batch: 550, loss is 5.544513816833496 and perplexity is 255.83016768155076
At time: 1313.962541103363 and batch: 600, loss is 5.532664613723755 and perplexity is 252.81667307662684
At time: 1315.6533567905426 and batch: 650, loss is 5.527134819030762 and perplexity is 251.42250705811134
At time: 1317.3435893058777 and batch: 700, loss is 5.51335223197937 and perplexity is 247.98102520789786
At time: 1319.031620502472 and batch: 750, loss is 5.511158113479614 and perplexity is 247.43752192624567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.324949397597202 and perplexity of 205.39796657116912
Finished 47 epochs...
Completing Train Step...
At time: 1323.5497119426727 and batch: 50, loss is 5.572862396240234 and perplexity is 263.1863657480358
At time: 1325.2668323516846 and batch: 100, loss is 5.551901731491089 and perplexity is 257.72721812198455
At time: 1326.9542939662933 and batch: 150, loss is 5.54786075592041 and perplexity is 256.68785017425876
At time: 1328.6428492069244 and batch: 200, loss is 5.564625082015991 and perplexity is 261.0273215248618
At time: 1330.3329629898071 and batch: 250, loss is 5.5966877746582036 and perplexity is 269.53217599193243
At time: 1332.0240323543549 and batch: 300, loss is 5.600343770980835 and perplexity is 270.51938815860035
At time: 1333.7097337245941 and batch: 350, loss is 5.545274295806885 and perplexity is 256.0247951404963
At time: 1335.3976440429688 and batch: 400, loss is 5.573332948684692 and perplexity is 263.31023787766765
At time: 1337.0852553844452 and batch: 450, loss is 5.542015399932861 and perplexity is 255.1917950594616
At time: 1338.7766580581665 and batch: 500, loss is 5.5155361557006835 and perplexity is 248.5231886576059
At time: 1340.4629499912262 and batch: 550, loss is 5.543693132400513 and perplexity is 255.62029797562667
At time: 1342.1472625732422 and batch: 600, loss is 5.531861200332641 and perplexity is 252.6136383473116
At time: 1343.8309588432312 and batch: 650, loss is 5.526378736495972 and perplexity is 251.23248273775903
At time: 1345.5080065727234 and batch: 700, loss is 5.512551107406616 and perplexity is 247.78244107096398
At time: 1347.1876673698425 and batch: 750, loss is 5.51000226020813 and perplexity is 247.1516856812417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.324634995571403 and perplexity of 205.33339918497438
Finished 48 epochs...
Completing Train Step...
At time: 1351.6162176132202 and batch: 50, loss is 5.571584234237671 and perplexity is 262.85018582771204
At time: 1353.3279061317444 and batch: 100, loss is 5.5510095119476315 and perplexity is 257.4973714132055
At time: 1355.00536775589 and batch: 150, loss is 5.546927156448365 and perplexity is 256.44831836363977
At time: 1356.6922619342804 and batch: 200, loss is 5.5638860321044925 and perplexity is 260.8344805743113
At time: 1358.3816378116608 and batch: 250, loss is 5.595965700149536 and perplexity is 269.3376239273794
At time: 1360.0735032558441 and batch: 300, loss is 5.599721956253052 and perplexity is 270.35122750669564
At time: 1361.7596638202667 and batch: 350, loss is 5.54469051361084 and perplexity is 255.8753760416956
At time: 1363.447440624237 and batch: 400, loss is 5.572671298980713 and perplexity is 263.13607636003314
At time: 1365.1387813091278 and batch: 450, loss is 5.541400880813598 and perplexity is 255.035022996967
At time: 1366.8290090560913 and batch: 500, loss is 5.514905271530151 and perplexity is 248.36644875943063
At time: 1368.5149624347687 and batch: 550, loss is 5.54296464920044 and perplexity is 255.43415069376965
At time: 1370.2033586502075 and batch: 600, loss is 5.531257934570313 and perplexity is 252.4612911457451
At time: 1371.8689522743225 and batch: 650, loss is 5.525701913833618 and perplexity is 251.06250043035058
At time: 1373.5323746204376 and batch: 700, loss is 5.511804685592652 and perplexity is 247.59755986009944
At time: 1375.216181755066 and batch: 750, loss is 5.509178133010864 and perplexity is 246.94808516310744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.324364240779433 and perplexity of 205.27781170882096
Finished 49 epochs...
Completing Train Step...
At time: 1379.7335650920868 and batch: 50, loss is 5.570648927688598 and perplexity is 262.60445526209986
At time: 1381.4502301216125 and batch: 100, loss is 5.5500940990448 and perplexity is 257.26176285297953
At time: 1383.1364166736603 and batch: 150, loss is 5.546180515289307 and perplexity is 256.25691495770064
At time: 1384.8239200115204 and batch: 200, loss is 5.563148574829102 and perplexity is 260.6421971980407
At time: 1386.5132689476013 and batch: 250, loss is 5.5953333282470705 and perplexity is 269.1673562236608
At time: 1388.2027666568756 and batch: 300, loss is 5.5991068363189695 and perplexity is 270.18498021370664
At time: 1389.888040304184 and batch: 350, loss is 5.5441914081573485 and perplexity is 255.7476991108756
At time: 1391.574553012848 and batch: 400, loss is 5.57207070350647 and perplexity is 262.97808547251645
At time: 1393.261570930481 and batch: 450, loss is 5.540822257995606 and perplexity is 254.8874965984599
At time: 1394.9848506450653 and batch: 500, loss is 5.51435188293457 and perplexity is 248.22904362188692
At time: 1396.6701514720917 and batch: 550, loss is 5.542350091934204 and perplexity is 255.27722000680222
At time: 1398.3601779937744 and batch: 600, loss is 5.53064377784729 and perplexity is 252.30628794948458
At time: 1400.0534780025482 and batch: 650, loss is 5.525106563568115 and perplexity is 250.9130747887727
At time: 1401.7433853149414 and batch: 700, loss is 5.511108932495117 and perplexity is 247.425353004558
At time: 1403.4306337833405 and batch: 750, loss is 5.508495273590088 and perplexity is 246.7795118991824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.324094195698583 and perplexity of 205.22238492976274
Finished Training.
Improved accuracyfrom -10000000 to -205.22238492976274
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23136e5898>
SETTINGS FOR THIS RUN
{'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 15.437431618724457, 'dropout': 0.47422080821405554, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 2.068774202983712}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2200748920440674 and batch: 50, loss is 7.457994356155395 and perplexity is 1733.6674486379427
At time: 3.893247127532959 and batch: 100, loss is 6.4099517345428465 and perplexity is 607.8643415031061
At time: 5.568136930465698 and batch: 150, loss is 6.196619300842285 and perplexity is 491.0860175002064
At time: 7.256197452545166 and batch: 200, loss is 6.171351537704468 and perplexity is 478.83282957853834
At time: 8.949371337890625 and batch: 250, loss is 6.209658918380737 and perplexity is 497.53152348527107
At time: 10.64199447631836 and batch: 300, loss is 6.202567377090454 and perplexity is 494.01573904257145
At time: 12.33145785331726 and batch: 350, loss is 6.152843608856201 and perplexity is 470.05213254255034
At time: 14.022984743118286 and batch: 400, loss is 6.216585273742676 and perplexity is 500.989565610224
At time: 15.713029146194458 and batch: 450, loss is 6.217702779769898 and perplexity is 501.54973740875306
At time: 17.403716564178467 and batch: 500, loss is 6.252723436355591 and perplexity is 519.4255224507791
At time: 19.12180519104004 and batch: 550, loss is 6.256330461502075 and perplexity is 521.3024864651439
At time: 20.81428599357605 and batch: 600, loss is 6.252469339370728 and perplexity is 519.2935547586695
At time: 22.50664520263672 and batch: 650, loss is 6.265958271026611 and perplexity is 526.3457262286661
At time: 24.199575424194336 and batch: 700, loss is 6.275976943969726 and perplexity is 531.6455160150937
At time: 25.889973163604736 and batch: 750, loss is 6.27527271270752 and perplexity is 531.2712464239452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.768210122751635 and perplexity of 319.9645225932965
Finished 1 epochs...
Completing Train Step...
At time: 30.448076486587524 and batch: 50, loss is 6.055935945510864 and perplexity is 426.638028594343
At time: 32.12916660308838 and batch: 100, loss is 5.971177549362182 and perplexity is 391.9669595446308
At time: 33.80919075012207 and batch: 150, loss is 5.95379298210144 and perplexity is 385.2116725756126
At time: 35.49061441421509 and batch: 200, loss is 5.922534513473511 and perplexity is 373.35679353534584
At time: 37.174657106399536 and batch: 250, loss is 5.958775634765625 and perplexity is 387.13583828603913
At time: 38.8557653427124 and batch: 300, loss is 5.96572172164917 and perplexity is 389.834278404718
At time: 40.53619074821472 and batch: 350, loss is 5.895509386062622 and perplexity is 363.4019006235107
At time: 42.22150945663452 and batch: 400, loss is 5.933365068435669 and perplexity is 377.4224316247709
At time: 43.9538049697876 and batch: 450, loss is 5.8928373908996585 and perplexity is 362.43218861267684
At time: 45.63904428482056 and batch: 500, loss is 5.893683528900146 and perplexity is 362.7389860382377
At time: 47.32484269142151 and batch: 550, loss is 5.890943727493286 and perplexity is 361.7465134636791
At time: 49.00767993927002 and batch: 600, loss is 5.855217819213867 and perplexity is 349.05092177156934
At time: 50.69255328178406 and batch: 650, loss is 5.848324565887451 and perplexity is 346.65309923781604
At time: 52.377755641937256 and batch: 700, loss is 5.857101774215698 and perplexity is 349.70913783157073
At time: 54.060870885849 and batch: 750, loss is 5.84725399017334 and perplexity is 346.28217943284506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.580259456190952 and perplexity of 265.1403891781464
Finished 2 epochs...
Completing Train Step...
At time: 58.60371661186218 and batch: 50, loss is 5.8333485984802245 and perplexity is 341.50031399893754
At time: 60.28725337982178 and batch: 100, loss is 5.826476535797119 and perplexity is 339.1615477131556
At time: 61.970627307891846 and batch: 150, loss is 5.825880012512207 and perplexity is 338.9592902842284
At time: 63.631380558013916 and batch: 200, loss is 5.809699373245239 and perplexity is 333.51884591363495
At time: 65.31760764122009 and batch: 250, loss is 5.853539524078369 and perplexity is 348.4656026140541
At time: 66.98261308670044 and batch: 300, loss is 5.872450113296509 and perplexity is 355.1179945616039
At time: 68.63710021972656 and batch: 350, loss is 5.777580595016479 and perplexity is 322.97683262029193
At time: 70.2942943572998 and batch: 400, loss is 5.82347601890564 and perplexity is 338.14541298737004
At time: 71.95939016342163 and batch: 450, loss is 5.818993616104126 and perplexity is 336.63310096879604
At time: 73.62685704231262 and batch: 500, loss is 5.818651342391968 and perplexity is 336.5179000239445
At time: 75.29173159599304 and batch: 550, loss is 5.8174170398712155 and perplexity is 336.10279136919763
At time: 76.95728015899658 and batch: 600, loss is 5.77985918045044 and perplexity is 323.7136020038274
At time: 78.66790246963501 and batch: 650, loss is 5.770166749954224 and perplexity is 320.5911867562136
At time: 80.34469413757324 and batch: 700, loss is 5.774483165740967 and perplexity is 321.97798245607805
At time: 82.03195476531982 and batch: 750, loss is 5.779348993301392 and perplexity is 323.54848960680613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.571934544762899 and perplexity of 262.9422811444034
Finished 3 epochs...
Completing Train Step...
At time: 86.60234999656677 and batch: 50, loss is 5.806147985458374 and perplexity is 332.33649189906015
At time: 88.2945921421051 and batch: 100, loss is 5.797749719619751 and perplexity is 329.5571289425896
At time: 89.98711323738098 and batch: 150, loss is 5.787637815475464 and perplexity is 326.2414709050698
At time: 91.67671012878418 and batch: 200, loss is 5.770942735671997 and perplexity is 320.8400574859581
At time: 93.3672947883606 and batch: 250, loss is 5.826470127105713 and perplexity is 339.1593741384244
At time: 95.05752444267273 and batch: 300, loss is 5.8443575191497805 and perplexity is 345.28063430756873
At time: 96.75181365013123 and batch: 350, loss is 5.76316668510437 and perplexity is 318.35486399262663
At time: 98.44138526916504 and batch: 400, loss is 5.799250383377075 and perplexity is 330.0520546476148
At time: 100.13238096237183 and batch: 450, loss is 5.7766725540161135 and perplexity is 322.6836895272205
At time: 101.82213854789734 and batch: 500, loss is 5.770893898010254 and perplexity is 320.8243887903713
At time: 103.5103087425232 and batch: 550, loss is 5.781684875488281 and perplexity is 324.305144144154
At time: 105.19764423370361 and batch: 600, loss is 5.738749647140503 and perplexity is 310.67571376061557
At time: 106.88757824897766 and batch: 650, loss is 5.731079587936401 and perplexity is 308.30192781958397
At time: 108.57499718666077 and batch: 700, loss is 5.732326974868775 and perplexity is 308.68673957024987
At time: 110.25264549255371 and batch: 750, loss is 5.736556329727173 and perplexity is 309.9950500363941
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.5792016317678055 and perplexity of 264.86006549167206
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 114.80645227432251 and batch: 50, loss is 5.683667936325073 and perplexity is 294.02592282477696
At time: 116.49224090576172 and batch: 100, loss is 5.621410713195801 and perplexity is 276.2788586827784
At time: 118.18131232261658 and batch: 150, loss is 5.581595582962036 and perplexity is 265.4948871245826
At time: 119.87157917022705 and batch: 200, loss is 5.567540731430054 and perplexity is 261.7894962591409
At time: 121.6192455291748 and batch: 250, loss is 5.584944286346436 and perplexity is 266.3854410198503
At time: 123.31126880645752 and batch: 300, loss is 5.6045564270019534 and perplexity is 271.66139704437524
At time: 125.0044424533844 and batch: 350, loss is 5.520887422561645 and perplexity is 249.85666727909913
At time: 126.69062566757202 and batch: 400, loss is 5.518011569976807 and perplexity is 249.13914857020364
At time: 128.38020730018616 and batch: 450, loss is 5.499928321838379 and perplexity is 244.6743938249212
At time: 130.06846618652344 and batch: 500, loss is 5.48063645362854 and perplexity is 239.99940726950354
At time: 131.76061391830444 and batch: 550, loss is 5.470198421478272 and perplexity is 237.5073146384868
At time: 133.4537365436554 and batch: 600, loss is 5.42772292137146 and perplexity is 227.6303226912854
At time: 135.1510443687439 and batch: 650, loss is 5.414661569595337 and perplexity is 224.6764954532128
At time: 136.84744787216187 and batch: 700, loss is 5.4222800827026365 and perplexity is 226.39473317624794
At time: 138.54196572303772 and batch: 750, loss is 5.433673524856568 and perplexity is 228.98889864732487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.317652857580851 and perplexity of 203.90472645686935
Finished 5 epochs...
Completing Train Step...
At time: 143.1592583656311 and batch: 50, loss is 5.481476449966431 and perplexity is 240.2010905874714
At time: 144.81836247444153 and batch: 100, loss is 5.469418468475342 and perplexity is 237.32214231745783
At time: 146.47847986221313 and batch: 150, loss is 5.45951940536499 and perplexity is 234.9844649504025
At time: 148.1380066871643 and batch: 200, loss is 5.460316562652588 and perplexity is 235.17185921053672
At time: 149.80010676383972 and batch: 250, loss is 5.484824333190918 and perplexity is 241.00660341822885
At time: 151.45741391181946 and batch: 300, loss is 5.497583036422729 and perplexity is 244.10123491083695
At time: 153.1134877204895 and batch: 350, loss is 5.425701007843018 and perplexity is 227.1705388405843
At time: 154.77065300941467 and batch: 400, loss is 5.4517104530334475 and perplexity is 233.15662849722418
At time: 156.43201112747192 and batch: 450, loss is 5.423524522781372 and perplexity is 226.6766432295304
At time: 158.09055495262146 and batch: 500, loss is 5.412271757125854 and perplexity is 224.1402018386029
At time: 159.74835538864136 and batch: 550, loss is 5.424825658798218 and perplexity is 226.97177233413115
At time: 161.4195568561554 and batch: 600, loss is 5.388592920303345 and perplexity is 218.89516581642386
At time: 163.10913252830505 and batch: 650, loss is 5.3883022212982175 and perplexity is 218.83154245756546
At time: 164.83569860458374 and batch: 700, loss is 5.388605413436889 and perplexity is 218.89790052004514
At time: 166.52937722206116 and batch: 750, loss is 5.394629030227661 and perplexity is 220.2204368161001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.29737250749455 and perplexity of 199.81111743705463
Finished 6 epochs...
Completing Train Step...
At time: 171.1348798274994 and batch: 50, loss is 5.448563852310181 and perplexity is 232.42413072474818
At time: 172.82220315933228 and batch: 100, loss is 5.4443982887268065 and perplexity is 231.45796693531818
At time: 174.51004123687744 and batch: 150, loss is 5.431698846817016 and perplexity is 228.5371654584578
At time: 176.19775867462158 and batch: 200, loss is 5.435660886764526 and perplexity is 229.44443496967682
At time: 177.8828318119049 and batch: 250, loss is 5.458614807128907 and perplexity is 234.77199453257268
At time: 179.56531310081482 and batch: 300, loss is 5.472563161849975 and perplexity is 238.06962236784656
At time: 181.25284123420715 and batch: 350, loss is 5.398659000396728 and perplexity is 221.10970927429662
At time: 182.93920731544495 and batch: 400, loss is 5.424149398803711 and perplexity is 226.8183322931475
At time: 184.6253147125244 and batch: 450, loss is 5.401997785568238 and perplexity is 221.8491808743676
At time: 186.3119022846222 and batch: 500, loss is 5.3871525859832765 and perplexity is 218.5801105435294
At time: 187.99871182441711 and batch: 550, loss is 5.402819709777832 and perplexity is 222.0315990436922
At time: 189.6901993751526 and batch: 600, loss is 5.364820909500122 and perplexity is 213.75295013015358
At time: 191.37944078445435 and batch: 650, loss is 5.3625187587738035 and perplexity is 213.26142462074915
At time: 193.06799030303955 and batch: 700, loss is 5.362065649032592 and perplexity is 213.16481568070773
At time: 194.75506448745728 and batch: 750, loss is 5.366231193542481 and perplexity is 214.0546151714276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.298033958257631 and perplexity of 199.943326373186
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 199.30470371246338 and batch: 50, loss is 5.403972883224487 and perplexity is 222.28778767460076
At time: 201.0227644443512 and batch: 100, loss is 5.3610873126983645 and perplexity is 212.95637077758545
At time: 202.71032643318176 and batch: 150, loss is 5.340364141464233 and perplexity is 208.58865223869944
At time: 204.40487575531006 and batch: 200, loss is 5.3365516853332515 and perplexity is 207.79493112745988
At time: 206.09638690948486 and batch: 250, loss is 5.351912956237793 and perplexity is 211.01156791981413
At time: 207.83744764328003 and batch: 300, loss is 5.358798284530639 and perplexity is 212.46946512935216
At time: 209.52898597717285 and batch: 350, loss is 5.281135816574096 and perplexity is 196.59304222520103
At time: 211.21722078323364 and batch: 400, loss is 5.3000969982147215 and perplexity is 200.35624323018254
At time: 212.9075574874878 and batch: 450, loss is 5.267013454437256 and perplexity is 193.83619649507108
At time: 214.59462118148804 and batch: 500, loss is 5.238290386199951 and perplexity is 188.34782495841773
At time: 216.285138130188 and batch: 550, loss is 5.257733526229859 and perplexity is 192.0457310521038
At time: 217.97436475753784 and batch: 600, loss is 5.226659059524536 and perplexity is 186.16978120211039
At time: 219.66706490516663 and batch: 650, loss is 5.223546085357666 and perplexity is 185.5911405964864
At time: 221.3556079864502 and batch: 700, loss is 5.218486461639404 and perplexity is 184.65449080554342
At time: 223.04183506965637 and batch: 750, loss is 5.242955837249756 and perplexity is 189.22860553781743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.165728546852289 and perplexity of 175.1650280257923
Finished 8 epochs...
Completing Train Step...
At time: 227.59958624839783 and batch: 50, loss is 5.290341472625732 and perplexity is 198.41116582474294
At time: 229.31740593910217 and batch: 100, loss is 5.28172682762146 and perplexity is 196.70926522614675
At time: 230.99620866775513 and batch: 150, loss is 5.266283655166626 and perplexity is 193.69478658694754
At time: 232.67712092399597 and batch: 200, loss is 5.269455680847168 and perplexity is 194.3101669092979
At time: 234.35045266151428 and batch: 250, loss is 5.290551815032959 and perplexity is 198.4529044965357
At time: 236.0209493637085 and batch: 300, loss is 5.30547119140625 and perplexity is 201.4358949182699
At time: 237.70584106445312 and batch: 350, loss is 5.239643144607544 and perplexity is 188.6027864739682
At time: 239.39280557632446 and batch: 400, loss is 5.263168354034423 and perplexity is 193.09230793746045
At time: 241.08630776405334 and batch: 450, loss is 5.2305616283416745 and perplexity is 186.897741117852
At time: 242.77349877357483 and batch: 500, loss is 5.213309316635132 and perplexity is 183.70097809746528
At time: 244.46563529968262 and batch: 550, loss is 5.240667524337769 and perplexity is 188.79608633482246
At time: 246.1551251411438 and batch: 600, loss is 5.20749361038208 and perplexity is 182.63572776440898
At time: 247.84419989585876 and batch: 650, loss is 5.208971395492553 and perplexity is 182.90582364622188
At time: 249.58224368095398 and batch: 700, loss is 5.198368215560913 and perplexity is 180.97668585905686
At time: 251.270281791687 and batch: 750, loss is 5.215834102630615 and perplexity is 184.1653697524053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.151997765829397 and perplexity of 172.77631237830306
Finished 9 epochs...
Completing Train Step...
At time: 255.806410074234 and batch: 50, loss is 5.268198976516723 and perplexity is 194.06612985442285
At time: 257.5247926712036 and batch: 100, loss is 5.253849821090698 and perplexity is 191.3013285151042
At time: 259.2142174243927 and batch: 150, loss is 5.242465667724609 and perplexity is 189.13587417099845
At time: 260.9049482345581 and batch: 200, loss is 5.247102432250976 and perplexity is 190.0148890005372
At time: 262.59590220451355 and batch: 250, loss is 5.265270719528198 and perplexity is 193.49868557025738
At time: 264.2832908630371 and batch: 300, loss is 5.283954811096192 and perplexity is 197.14801880476406
At time: 265.972460269928 and batch: 350, loss is 5.215694437026977 and perplexity is 184.13964998099516
At time: 267.6639144420624 and batch: 400, loss is 5.240403642654419 and perplexity is 188.74627307844258
At time: 269.3568046092987 and batch: 450, loss is 5.210868349075318 and perplexity is 183.25311679904868
At time: 271.04741072654724 and batch: 500, loss is 5.197157258987427 and perplexity is 180.75766359166005
At time: 272.7357552051544 and batch: 550, loss is 5.226226902008056 and perplexity is 186.0893439138637
At time: 274.4250593185425 and batch: 600, loss is 5.185098905563354 and perplexity is 178.5911125709365
At time: 276.1160490512848 and batch: 650, loss is 5.18147198677063 and perplexity is 177.94455033167645
At time: 277.80469822883606 and batch: 700, loss is 5.176249895095825 and perplexity is 177.01772965430195
At time: 279.49266290664673 and batch: 750, loss is 5.190482091903687 and perplexity is 179.55509412689585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.130673430686773 and perplexity of 169.13097769275598
Finished 10 epochs...
Completing Train Step...
At time: 284.03338599205017 and batch: 50, loss is 5.240072755813599 and perplexity is 188.68382975183434
At time: 285.751948595047 and batch: 100, loss is 5.234111385345459 and perplexity is 187.56236160609885
At time: 287.43380188941956 and batch: 150, loss is 5.217222604751587 and perplexity is 184.42126137083162
At time: 289.11934995651245 and batch: 200, loss is 5.224666557312012 and perplexity is 185.79920680890953
At time: 290.8097138404846 and batch: 250, loss is 5.240503492355347 and perplexity is 188.76512027828872
At time: 292.52607440948486 and batch: 300, loss is 5.257573595046997 and perplexity is 192.01501940711344
At time: 294.2151417732239 and batch: 350, loss is 5.190235481262207 and perplexity is 179.5108193894874
At time: 295.9095060825348 and batch: 400, loss is 5.212835350036621 and perplexity is 183.61393060016593
At time: 297.6018605232239 and batch: 450, loss is 5.187058544158935 and perplexity is 178.94142974337734
At time: 299.2904076576233 and batch: 500, loss is 5.175456743240357 and perplexity is 176.87738337888774
At time: 300.9791786670685 and batch: 550, loss is 5.200631589889526 and perplexity is 181.38676775325752
At time: 302.66924118995667 and batch: 600, loss is 5.162985849380493 and perplexity is 174.68526157414496
At time: 304.35864758491516 and batch: 650, loss is 5.159769735336304 and perplexity is 174.12435630221646
At time: 306.0466721057892 and batch: 700, loss is 5.149568681716919 and perplexity is 172.35713349938186
At time: 307.7338972091675 and batch: 750, loss is 5.165513477325439 and perplexity is 175.1273594169243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.120769412018532 and perplexity of 167.46416900916063
Finished 11 epochs...
Completing Train Step...
At time: 312.2609667778015 and batch: 50, loss is 5.219800262451172 and perplexity is 184.89724945879988
At time: 313.9745593070984 and batch: 100, loss is 5.2111642551422115 and perplexity is 183.3073505317363
At time: 315.65613651275635 and batch: 150, loss is 5.1981877422332765 and perplexity is 180.94402734141948
At time: 317.3361587524414 and batch: 200, loss is 5.203784732818604 and perplexity is 181.9596088076735
At time: 319.0101764202118 and batch: 250, loss is 5.214829072952271 and perplexity is 183.98037107023592
At time: 320.68479204177856 and batch: 300, loss is 5.2378295803070065 and perplexity is 188.26105316476594
At time: 322.3711214065552 and batch: 350, loss is 5.173308353424073 and perplexity is 176.49778971329198
At time: 324.0601718425751 and batch: 400, loss is 5.195656490325928 and perplexity is 180.48659161385868
At time: 325.7508487701416 and batch: 450, loss is 5.169333009719849 and perplexity is 175.79754311877832
At time: 327.43948769569397 and batch: 500, loss is 5.157191190719605 and perplexity is 173.67594725046234
At time: 329.1293134689331 and batch: 550, loss is 5.180293197631836 and perplexity is 177.73491481075035
At time: 330.82741141319275 and batch: 600, loss is 5.140280084609985 and perplexity is 170.76358987743984
At time: 332.52603006362915 and batch: 650, loss is 5.138726606369018 and perplexity is 170.49851830098692
At time: 334.2679617404938 and batch: 700, loss is 5.12976469039917 and perplexity is 168.97735137322985
At time: 335.9638319015503 and batch: 750, loss is 5.146072807312012 and perplexity is 171.7556465818094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.108156426008358 and perplexity of 165.36521068067344
Finished 12 epochs...
Completing Train Step...
At time: 340.49114179611206 and batch: 50, loss is 5.1991306591033934 and perplexity is 181.11472298059851
At time: 342.20768880844116 and batch: 100, loss is 5.189149513244629 and perplexity is 179.31598219345088
At time: 343.8977496623993 and batch: 150, loss is 5.178316459655762 and perplexity is 177.383926475246
At time: 345.59362292289734 and batch: 200, loss is 5.181725101470947 and perplexity is 177.9895964138791
At time: 347.2827661037445 and batch: 250, loss is 5.1954449272155765 and perplexity is 180.4484113480704
At time: 348.97153186798096 and batch: 300, loss is 5.213598318099976 and perplexity is 183.75407562149022
At time: 350.6609733104706 and batch: 350, loss is 5.152050151824951 and perplexity is 172.78536367451363
At time: 352.35322976112366 and batch: 400, loss is 5.174620532989502 and perplexity is 176.7295385210698
At time: 354.05015659332275 and batch: 450, loss is 5.145945692062378 and perplexity is 171.73381520749783
At time: 355.73495864868164 and batch: 500, loss is 5.133920726776123 and perplexity is 169.68108875855498
At time: 357.4220657348633 and batch: 550, loss is 5.156152849197388 and perplexity is 173.49570589525737
At time: 359.1101939678192 and batch: 600, loss is 5.120057039260864 and perplexity is 167.34491457911975
At time: 360.7983739376068 and batch: 650, loss is 5.11678466796875 and perplexity is 166.79819490752377
At time: 362.48219776153564 and batch: 700, loss is 5.103725175857544 and perplexity is 164.6340572242465
At time: 364.1694829463959 and batch: 750, loss is 5.120074510574341 and perplexity is 167.34783834012202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.093569378520167 and perplexity of 162.9705286319053
Finished 13 epochs...
Completing Train Step...
At time: 368.71702218055725 and batch: 50, loss is 5.173404798507691 and perplexity is 176.51481287826638
At time: 370.45298862457275 and batch: 100, loss is 5.163399229049682 and perplexity is 174.75748783719257
At time: 372.14332485198975 and batch: 150, loss is 5.153138990402222 and perplexity is 172.97360150577902
At time: 373.8328003883362 and batch: 200, loss is 5.157857103347778 and perplexity is 173.79163877289415
At time: 375.5248963832855 and batch: 250, loss is 5.172348289489746 and perplexity is 176.32842186589008
At time: 377.2412807941437 and batch: 300, loss is 5.192166767120361 and perplexity is 179.85784108754692
At time: 378.93306279182434 and batch: 350, loss is 5.129583559036255 and perplexity is 168.94674704705923
At time: 380.6262652873993 and batch: 400, loss is 5.157000169754029 and perplexity is 173.6427746717511
At time: 382.3161928653717 and batch: 450, loss is 5.129325857162476 and perplexity is 168.9032147631928
At time: 384.0072057247162 and batch: 500, loss is 5.1197913646697994 and perplexity is 167.3004611927013
At time: 385.69480538368225 and batch: 550, loss is 5.144818964004517 and perplexity is 171.54042686791323
At time: 387.38364601135254 and batch: 600, loss is 5.108318195343018 and perplexity is 165.39196386464525
At time: 389.0717062950134 and batch: 650, loss is 5.102791328430175 and perplexity is 164.48038589740463
At time: 390.75745129585266 and batch: 700, loss is 5.087641801834106 and perplexity is 162.0073657569257
At time: 392.44756388664246 and batch: 750, loss is 5.102830142974853 and perplexity is 164.4867702525938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.089173782703488 and perplexity of 162.25574815192428
Finished 14 epochs...
Completing Train Step...
At time: 397.00456166267395 and batch: 50, loss is 5.1578453540802 and perplexity is 173.78959686042282
At time: 398.7226662635803 and batch: 100, loss is 5.155261363983154 and perplexity is 173.3411059607356
At time: 400.4077157974243 and batch: 150, loss is 5.140797309875488 and perplexity is 170.85193596599737
At time: 402.08966064453125 and batch: 200, loss is 5.147942581176758 and perplexity is 172.07709122165485
At time: 403.76260781288147 and batch: 250, loss is 5.164176063537598 and perplexity is 174.89329822503268
At time: 405.43770694732666 and batch: 300, loss is 5.181173353195191 and perplexity is 177.89141804831775
At time: 407.11932468414307 and batch: 350, loss is 5.118523960113525 and perplexity is 167.08855813773283
At time: 408.8080823421478 and batch: 400, loss is 5.14517822265625 and perplexity is 171.60206532179853
At time: 410.497394323349 and batch: 450, loss is 5.118200435638427 and perplexity is 167.0345096431433
At time: 412.18512868881226 and batch: 500, loss is 5.109091520309448 and perplexity is 165.51991506710183
At time: 413.87617921829224 and batch: 550, loss is 5.131948366165161 and perplexity is 169.3467462930057
At time: 415.56811022758484 and batch: 600, loss is 5.093168420791626 and perplexity is 162.90519743732492
At time: 417.2556722164154 and batch: 650, loss is 5.085081081390381 and perplexity is 161.59304089487418
At time: 418.97284483909607 and batch: 700, loss is 5.072616720199585 and perplexity is 159.59138744773625
At time: 420.6678841114044 and batch: 750, loss is 5.087931146621704 and perplexity is 162.0542485260753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0865631103515625 and perplexity of 161.83270401137898
Finished 15 epochs...
Completing Train Step...
At time: 425.18799901008606 and batch: 50, loss is 5.1435239887237545 and perplexity is 171.31843002670206
At time: 426.9013934135437 and batch: 100, loss is 5.136883306503296 and perplexity is 170.18452788324944
At time: 428.5910050868988 and batch: 150, loss is 5.125950193405151 and perplexity is 168.33401555569577
At time: 430.27867603302 and batch: 200, loss is 5.1331058120727535 and perplexity is 169.5428694705561
At time: 431.96534609794617 and batch: 250, loss is 5.1500632953643795 and perplexity is 172.4424047762781
At time: 433.65485095977783 and batch: 300, loss is 5.167368745803833 and perplexity is 175.4525692690669
At time: 435.34227895736694 and batch: 350, loss is 5.105239915847778 and perplexity is 164.883623981134
At time: 437.0338046550751 and batch: 400, loss is 5.130702753067016 and perplexity is 169.13593708830504
At time: 438.7222697734833 and batch: 450, loss is 5.1033624744415285 and perplexity is 164.57435504625357
At time: 440.41166853904724 and batch: 500, loss is 5.093060178756714 and perplexity is 162.88756520154828
At time: 442.09835863113403 and batch: 550, loss is 5.119059839248657 and perplexity is 167.17812140515798
At time: 443.78537011146545 and batch: 600, loss is 5.080762557983398 and perplexity is 160.896702224992
At time: 445.47112941741943 and batch: 650, loss is 5.071009511947632 and perplexity is 159.33509686422897
At time: 447.15650844573975 and batch: 700, loss is 5.0596491146087645 and perplexity is 157.53522981357338
At time: 448.8431348800659 and batch: 750, loss is 5.074626178741455 and perplexity is 159.91240214929957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0796015539834665 and perplexity of 160.71000890694827
Finished 16 epochs...
Completing Train Step...
At time: 453.3599925041199 and batch: 50, loss is 5.12941351890564 and perplexity is 168.91802176241916
At time: 455.0615141391754 and batch: 100, loss is 5.130624170303345 and perplexity is 169.12264644114725
At time: 456.7233715057373 and batch: 150, loss is 5.112968091964722 and perplexity is 166.1628101876265
At time: 458.38812828063965 and batch: 200, loss is 5.120859651565552 and perplexity is 167.47928158177965
At time: 460.0508785247803 and batch: 250, loss is 5.13786361694336 and perplexity is 170.3514433537927
At time: 461.7702798843384 and batch: 300, loss is 5.155308322906494 and perplexity is 173.34924606356583
At time: 463.4597957134247 and batch: 350, loss is 5.09450138092041 and perplexity is 163.12248835817425
At time: 465.1511278152466 and batch: 400, loss is 5.118035984039307 and perplexity is 167.00704280946843
At time: 466.84538769721985 and batch: 450, loss is 5.094119729995728 and perplexity is 163.0602443881458
At time: 468.53417897224426 and batch: 500, loss is 5.082385158538818 and perplexity is 161.15798522503178
At time: 470.2232336997986 and batch: 550, loss is 5.106061439514161 and perplexity is 165.0191354357455
At time: 471.9152641296387 and batch: 600, loss is 5.070803365707397 and perplexity is 159.30225391841424
At time: 473.60752987861633 and batch: 650, loss is 5.058669013977051 and perplexity is 157.38090507455667
At time: 475.2964689731598 and batch: 700, loss is 5.046494693756103 and perplexity is 155.47651538585225
At time: 476.98759484291077 and batch: 750, loss is 5.062311325073242 and perplexity is 157.95518050143627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0780522546102835 and perplexity of 160.46121376972542
Finished 17 epochs...
Completing Train Step...
At time: 481.54350090026855 and batch: 50, loss is 5.117599687576294 and perplexity is 166.9341941203607
At time: 483.259113073349 and batch: 100, loss is 5.113884534835815 and perplexity is 166.31515870915936
At time: 484.9441032409668 and batch: 150, loss is 5.102999811172485 and perplexity is 164.51468079413553
At time: 486.6275565624237 and batch: 200, loss is 5.112038764953613 and perplexity is 166.00846233080225
At time: 488.29522252082825 and batch: 250, loss is 5.125585250854492 and perplexity is 168.2725945189528
At time: 489.97128033638 and batch: 300, loss is 5.144085836410523 and perplexity is 171.41471193566443
At time: 491.6606855392456 and batch: 350, loss is 5.084959058761597 and perplexity is 161.57332409020356
At time: 493.3547134399414 and batch: 400, loss is 5.108817510604858 and perplexity is 165.47456721722972
At time: 495.0445628166199 and batch: 450, loss is 5.083417062759399 and perplexity is 161.32437066231907
At time: 496.73316645622253 and batch: 500, loss is 5.069110555648804 and perplexity is 159.03281358062395
At time: 498.4225606918335 and batch: 550, loss is 5.0935767650604244 and perplexity is 162.97173242472178
At time: 500.11341667175293 and batch: 600, loss is 5.058285636901855 and perplexity is 157.32058040776363
At time: 501.80524373054504 and batch: 650, loss is 5.047284994125366 and perplexity is 155.59943709951347
At time: 503.5284032821655 and batch: 700, loss is 5.037266778945923 and perplexity is 154.04839078720002
At time: 505.2189612388611 and batch: 750, loss is 5.052530946731568 and perplexity is 156.41784917066585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.075115115143532 and perplexity of 159.9906082612035
Finished 18 epochs...
Completing Train Step...
At time: 509.74034333229065 and batch: 50, loss is 5.105509414672851 and perplexity is 164.92806591232585
At time: 511.4595630168915 and batch: 100, loss is 5.102824811935425 and perplexity is 164.48589336947344
At time: 513.1498222351074 and batch: 150, loss is 5.091496829986572 and perplexity is 162.6331140765702
At time: 514.8447241783142 and batch: 200, loss is 5.100734930038453 and perplexity is 164.14249623332773
At time: 516.5346422195435 and batch: 250, loss is 5.113238096237183 and perplexity is 166.20768091367253
At time: 518.2171466350555 and batch: 300, loss is 5.131085300445557 and perplexity is 169.20065197512116
At time: 519.8809485435486 and batch: 350, loss is 5.072661457061767 and perplexity is 159.59852722534634
At time: 521.5466542243958 and batch: 400, loss is 5.098562784194947 and perplexity is 163.78634174217513
At time: 523.2135925292969 and batch: 450, loss is 5.074370126724244 and perplexity is 159.87146149784402
At time: 524.8747253417969 and batch: 500, loss is 5.059873304367065 and perplexity is 157.57055155790786
At time: 526.5523660182953 and batch: 550, loss is 5.09301194190979 and perplexity is 162.87970820849966
At time: 528.2416212558746 and batch: 600, loss is 5.045777177810669 and perplexity is 155.3649985192716
At time: 529.9285583496094 and batch: 650, loss is 5.03572268486023 and perplexity is 153.810709126722
At time: 531.615638256073 and batch: 700, loss is 5.027180080413818 and perplexity is 152.5023613841509
At time: 533.3046140670776 and batch: 750, loss is 5.039769458770752 and perplexity is 154.43440742341272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.074760969295058 and perplexity of 159.93395828326226
Finished 19 epochs...
Completing Train Step...
At time: 537.8230137825012 and batch: 50, loss is 5.096287307739257 and perplexity is 163.41407348223504
At time: 539.5388877391815 and batch: 100, loss is 5.094307804107666 and perplexity is 163.09091468285538
At time: 541.2310779094696 and batch: 150, loss is 5.083599720001221 and perplexity is 161.35384041825307
At time: 542.9209082126617 and batch: 200, loss is 5.094985313415528 and perplexity is 163.20144773493837
At time: 544.6066422462463 and batch: 250, loss is 5.108059186935424 and perplexity is 165.3491315026663
At time: 546.3230037689209 and batch: 300, loss is 5.122015972137451 and perplexity is 167.6730533299782
At time: 548.0123827457428 and batch: 350, loss is 5.062597274780273 and perplexity is 158.00035419741945
At time: 549.6993706226349 and batch: 400, loss is 5.088821439743042 and perplexity is 162.19858855175374
At time: 551.3858633041382 and batch: 450, loss is 5.066390829086304 and perplexity is 158.6008754562654
At time: 553.0725874900818 and batch: 500, loss is 5.050690021514892 and perplexity is 156.13016049570578
At time: 554.7589921951294 and batch: 550, loss is 5.075797061920166 and perplexity is 160.09975055118568
At time: 556.4469187259674 and batch: 600, loss is 5.035846509933472 and perplexity is 153.82975592825647
At time: 558.1333713531494 and batch: 650, loss is 5.027647361755371 and perplexity is 152.57363954434868
At time: 559.8166959285736 and batch: 700, loss is 5.016691732406616 and perplexity is 150.9112223421702
At time: 561.5032720565796 and batch: 750, loss is 5.0292826271057125 and perplexity is 152.82334203974634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.072087132653524 and perplexity of 159.50689221227236
Finished 20 epochs...
Completing Train Step...
At time: 565.9581968784332 and batch: 50, loss is 5.087654523849487 and perplexity is 162.00942683023513
At time: 567.6752042770386 and batch: 100, loss is 5.0893949604034425 and perplexity is 162.29163947413858
At time: 569.3583045005798 and batch: 150, loss is 5.075258569717407 and perplexity is 160.0135612920549
At time: 571.0373985767365 and batch: 200, loss is 5.08408218383789 and perplexity is 161.431706593459
At time: 572.7155065536499 and batch: 250, loss is 5.09769118309021 and perplexity is 163.64364758099973
At time: 574.4079556465149 and batch: 300, loss is 5.112466764450073 and perplexity is 166.0795290762687
At time: 576.10009765625 and batch: 350, loss is 5.054238004684448 and perplexity is 156.68509153827316
At time: 577.7922642230988 and batch: 400, loss is 5.082382783889771 and perplexity is 161.1576025318301
At time: 579.4805874824524 and batch: 450, loss is 5.057995100021362 and perplexity is 157.27487961629868
At time: 581.169759273529 and batch: 500, loss is 5.042804079055786 and perplexity is 154.90376901579899
At time: 582.8583917617798 and batch: 550, loss is 5.068389625549316 and perplexity is 158.9182033564516
At time: 584.5506353378296 and batch: 600, loss is 5.030580615997314 and perplexity is 153.02183383229442
At time: 586.2399690151215 and batch: 650, loss is 5.02117657661438 and perplexity is 151.58955563616075
At time: 587.9577805995941 and batch: 700, loss is 5.008864126205444 and perplexity is 149.73455995521167
At time: 589.6458625793457 and batch: 750, loss is 5.02365969657898 and perplexity is 151.96643841711327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.068875689839208 and perplexity of 158.99546659604667
Finished 21 epochs...
Completing Train Step...
At time: 594.1924276351929 and batch: 50, loss is 5.079613170623779 and perplexity is 160.71187582816006
At time: 595.9135904312134 and batch: 100, loss is 5.080906352996826 and perplexity is 160.91984003195972
At time: 597.6009600162506 and batch: 150, loss is 5.068639211654663 and perplexity is 158.9578720820715
At time: 599.2927169799805 and batch: 200, loss is 5.076442766189575 and perplexity is 160.20316102634754
At time: 600.985942363739 and batch: 250, loss is 5.086892232894898 and perplexity is 161.88597556847233
At time: 602.6735577583313 and batch: 300, loss is 5.104317350387573 and perplexity is 164.7315781915767
At time: 604.363299369812 and batch: 350, loss is 5.045995397567749 and perplexity is 155.3989059310063
At time: 606.0523145198822 and batch: 400, loss is 5.074078321456909 and perplexity is 159.82481696917307
At time: 607.7362344264984 and batch: 450, loss is 5.04945463180542 and perplexity is 155.93739799501202
At time: 609.4239206314087 and batch: 500, loss is 5.034966478347778 and perplexity is 153.6944404339076
At time: 611.1117560863495 and batch: 550, loss is 5.0609707927703855 and perplexity is 157.74357834102412
At time: 612.8035995960236 and batch: 600, loss is 5.020727672576904 and perplexity is 151.52152174408346
At time: 614.4768917560577 and batch: 650, loss is 5.009071750640869 and perplexity is 149.76565173628097
At time: 616.1397843360901 and batch: 700, loss is 5.001820468902588 and perplexity is 148.68358672130736
At time: 617.802666425705 and batch: 750, loss is 5.012979784011841 and perplexity is 150.35208605716903
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0675304324127906 and perplexity of 158.78172056779252
Finished 22 epochs...
Completing Train Step...
At time: 622.2016353607178 and batch: 50, loss is 5.069328413009644 and perplexity is 159.06746382394587
At time: 623.9196767807007 and batch: 100, loss is 5.077115163803101 and perplexity is 160.31091747302986
At time: 625.6086587905884 and batch: 150, loss is 5.058818683624268 and perplexity is 157.40446198193092
At time: 627.2987701892853 and batch: 200, loss is 5.067517776489257 and perplexity is 158.77971105119474
At time: 628.98548579216 and batch: 250, loss is 5.0803654003143315 and perplexity is 160.8328135535535
At time: 630.7035541534424 and batch: 300, loss is 5.097637443542481 and perplexity is 163.6348536816822
At time: 632.3962955474854 and batch: 350, loss is 5.039098453521729 and perplexity is 154.33081588452183
At time: 634.0869975090027 and batch: 400, loss is 5.0692910480499265 and perplexity is 159.0615203856066
At time: 635.7739896774292 and batch: 450, loss is 5.040336093902588 and perplexity is 154.521940181449
At time: 637.4612941741943 and batch: 500, loss is 5.026887321472168 and perplexity is 152.45772148891604
At time: 639.1501994132996 and batch: 550, loss is 5.053475761413575 and perplexity is 156.56570488821768
At time: 640.8409078121185 and batch: 600, loss is 5.013346281051636 and perplexity is 150.40719975052176
At time: 642.5293254852295 and batch: 650, loss is 5.0004936218261715 and perplexity is 148.48643716153742
At time: 644.2169623374939 and batch: 700, loss is 4.992244625091553 and perplexity is 147.2666111138094
At time: 645.8988847732544 and batch: 750, loss is 5.003821697235107 and perplexity is 148.98143445889565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062327185342478 and perplexity of 157.95768573299299
Finished 23 epochs...
Completing Train Step...
At time: 650.3960406780243 and batch: 50, loss is 5.059147939682007 and perplexity is 157.45629688756372
At time: 652.109566450119 and batch: 100, loss is 5.06360200881958 and perplexity is 158.15918230814745
At time: 653.7994339466095 and batch: 150, loss is 5.052461280822754 and perplexity is 156.4069525586142
At time: 655.4885582923889 and batch: 200, loss is 5.060214939117432 and perplexity is 157.62439233037026
At time: 657.1724264621735 and batch: 250, loss is 5.071491308212281 and perplexity is 159.41188241473597
At time: 658.8605353832245 and batch: 300, loss is 5.087981090545655 and perplexity is 162.06234235325658
At time: 660.5534129142761 and batch: 350, loss is 5.031400604248047 and perplexity is 153.14736139667292
At time: 662.2357156276703 and batch: 400, loss is 5.0584117126464845 and perplexity is 157.34041596744922
At time: 663.9235281944275 and batch: 450, loss is 5.032698125839233 and perplexity is 153.34620237705417
At time: 665.6127042770386 and batch: 500, loss is 5.0179833316802975 and perplexity is 151.10626509876442
At time: 667.3061304092407 and batch: 550, loss is 5.043227062225342 and perplexity is 154.9693045622329
At time: 668.9985613822937 and batch: 600, loss is 5.0058454990386965 and perplexity is 149.28324865770128
At time: 670.687215089798 and batch: 650, loss is 4.993763122558594 and perplexity is 147.49040496195855
At time: 672.405220746994 and batch: 700, loss is 4.987062044143677 and perplexity is 146.5053642962884
At time: 674.0960111618042 and batch: 750, loss is 4.996446056365967 and perplexity is 147.88664325857366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.06502870071766 and perplexity of 158.38498777126446
Annealing...
Finished 24 epochs...
Completing Train Step...
At time: 678.6202774047852 and batch: 50, loss is 5.048039236068726 and perplexity is 155.716840991241
At time: 680.3401803970337 and batch: 100, loss is 5.044668216705322 and perplexity is 155.1928002770054
At time: 682.0314152240753 and batch: 150, loss is 5.0189750862121585 and perplexity is 151.2561997588687
At time: 683.7066259384155 and batch: 200, loss is 5.018005352020264 and perplexity is 151.1095925467286
At time: 685.3746917247772 and batch: 250, loss is 5.030856809616089 and perplexity is 153.06410332334525
At time: 687.0641543865204 and batch: 300, loss is 5.038455047607422 and perplexity is 154.23155046222627
At time: 688.7551636695862 and batch: 350, loss is 4.985979146957398 and perplexity is 146.34679991947115
At time: 690.447235584259 and batch: 400, loss is 4.995739574432373 and perplexity is 147.78220091455034
At time: 692.1337773799896 and batch: 450, loss is 4.972193660736084 and perplexity is 144.3431802915288
At time: 693.8206701278687 and batch: 500, loss is 4.943809499740601 and perplexity is 140.30371977160587
At time: 695.5102150440216 and batch: 550, loss is 4.9671058845520015 and perplexity is 143.61065952749172
At time: 697.2029371261597 and batch: 600, loss is 4.929424810409546 and perplexity is 138.2999407714933
At time: 698.8901996612549 and batch: 650, loss is 4.914311637878418 and perplexity is 136.2255050427621
At time: 700.5781908035278 and batch: 700, loss is 4.913800134658813 and perplexity is 136.1558430760194
At time: 702.26993060112 and batch: 750, loss is 4.930853414535522 and perplexity is 138.49765783358023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.013129655704942 and perplexity of 150.37462126752058
Finished 25 epochs...
Completing Train Step...
At time: 706.6893999576569 and batch: 50, loss is 5.007041997909546 and perplexity is 149.46197279645017
At time: 708.4103107452393 and batch: 100, loss is 5.006532363891601 and perplexity is 149.38582129706947
At time: 710.0778687000275 and batch: 150, loss is 4.988428363800049 and perplexity is 146.70567426785402
At time: 711.7712881565094 and batch: 200, loss is 4.994601755142212 and perplexity is 147.6141471011733
At time: 713.462249994278 and batch: 250, loss is 5.011257362365723 and perplexity is 150.09333926910892
At time: 715.1890139579773 and batch: 300, loss is 5.02130166053772 and perplexity is 151.6085182384507
At time: 716.8798863887787 and batch: 350, loss is 4.968574209213257 and perplexity is 143.8216814869554
At time: 718.5704762935638 and batch: 400, loss is 4.983425884246826 and perplexity is 145.97361471509606
At time: 720.2598657608032 and batch: 450, loss is 4.959897422790528 and perplexity is 142.5791697734842
At time: 721.9513468742371 and batch: 500, loss is 4.935482978820801 and perplexity is 139.14032813776993
At time: 723.643129825592 and batch: 550, loss is 4.963044481277466 and perplexity is 143.02858155144182
At time: 725.335944890976 and batch: 600, loss is 4.927595844268799 and perplexity is 138.0472260363486
At time: 727.0250897407532 and batch: 650, loss is 4.916619186401367 and perplexity is 136.54021497027895
At time: 728.716705083847 and batch: 700, loss is 4.914892911911011 and perplexity is 136.3047124097829
At time: 730.4120450019836 and batch: 750, loss is 4.9268333625793455 and perplexity is 137.94200767285145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.010941261468932 and perplexity of 150.04590212782003
Finished 26 epochs...
Completing Train Step...
At time: 734.9346122741699 and batch: 50, loss is 4.995959396362305 and perplexity is 147.81469025396873
At time: 736.655277967453 and batch: 100, loss is 4.992918996810913 and perplexity is 147.36595704582905
At time: 738.3477277755737 and batch: 150, loss is 4.979165172576904 and perplexity is 145.3529863295708
At time: 740.0330529212952 and batch: 200, loss is 4.98662501335144 and perplexity is 146.44135092978803
At time: 741.7177329063416 and batch: 250, loss is 5.002204704284668 and perplexity is 148.7407271930527
At time: 743.3990564346313 and batch: 300, loss is 5.013864736557007 and perplexity is 150.48519940920903
At time: 745.0794064998627 and batch: 350, loss is 4.963671035766602 and perplexity is 143.1182248315548
At time: 746.7620072364807 and batch: 400, loss is 4.979160423278809 and perplexity is 145.35229600654895
At time: 748.4426321983337 and batch: 450, loss is 4.954938011169434 and perplexity is 141.87381150870618
At time: 750.1321067810059 and batch: 500, loss is 4.93078164100647 and perplexity is 138.48771772463462
At time: 751.8229796886444 and batch: 550, loss is 4.958716926574707 and perplexity is 142.41095491115087
At time: 753.514716386795 and batch: 600, loss is 4.924387454986572 and perplexity is 137.60502654926455
At time: 755.2029466629028 and batch: 650, loss is 4.914371109008789 and perplexity is 136.2336067684393
At time: 756.9273416996002 and batch: 700, loss is 4.911698665618896 and perplexity is 135.87001622052355
At time: 758.6173372268677 and batch: 750, loss is 4.920724925994873 and perplexity is 137.10196594948266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.008361106695131 and perplexity of 149.65925949057515
Finished 27 epochs...
Completing Train Step...
At time: 763.177526473999 and batch: 50, loss is 4.988167142868042 and perplexity is 146.667356679787
At time: 764.895754814148 and batch: 100, loss is 4.984112253189087 and perplexity is 146.0738408627488
At time: 766.588455915451 and batch: 150, loss is 4.972107162475586 and perplexity is 144.3306953974874
At time: 768.2784729003906 and batch: 200, loss is 4.979450244903564 and perplexity is 145.3944283502767
At time: 769.9660904407501 and batch: 250, loss is 4.994959287643432 and perplexity is 147.6669333922469
At time: 771.6549279689789 and batch: 300, loss is 5.008293781280518 and perplexity is 149.6491839580528
At time: 773.3437693119049 and batch: 350, loss is 4.9588600730896 and perplexity is 142.43134200216468
At time: 775.0331299304962 and batch: 400, loss is 4.974401741027832 and perplexity is 144.6622537634612
At time: 776.7195258140564 and batch: 450, loss is 4.949731998443603 and perplexity is 141.13713388090295
At time: 778.4084248542786 and batch: 500, loss is 4.925024032592773 and perplexity is 137.6926507144982
At time: 780.099659204483 and batch: 550, loss is 4.954526882171631 and perplexity is 141.8154950593793
At time: 781.7914533615112 and batch: 600, loss is 4.920943069458008 and perplexity is 137.13187710948088
At time: 783.4784893989563 and batch: 650, loss is 4.910498237609863 and perplexity is 135.70701190451234
At time: 785.1678957939148 and batch: 700, loss is 4.908086090087891 and perplexity is 135.38006105739956
At time: 786.8603553771973 and batch: 750, loss is 4.916731243133545 and perplexity is 136.5555160778594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.006918263989825 and perplexity of 149.44348042479726
Finished 28 epochs...
Completing Train Step...
At time: 791.4153625965118 and batch: 50, loss is 4.983582210540772 and perplexity is 145.99643601303626
At time: 793.135448217392 and batch: 100, loss is 4.978778657913208 and perplexity is 145.2968161249455
At time: 794.8277850151062 and batch: 150, loss is 4.966598529815673 and perplexity is 143.53781645939358
At time: 796.514927148819 and batch: 200, loss is 4.974698390960693 and perplexity is 144.70517417718182
At time: 798.2031745910645 and batch: 250, loss is 4.990377521514892 and perplexity is 146.9919056290763
At time: 799.9202754497528 and batch: 300, loss is 5.00501145362854 and perplexity is 149.1587915579961
At time: 801.6095306873322 and batch: 350, loss is 4.955373277664185 and perplexity is 141.93557786677647
At time: 803.2978870868683 and batch: 400, loss is 4.970334358215332 and perplexity is 144.07505199561106
At time: 804.9852604866028 and batch: 450, loss is 4.945134859085083 and perplexity is 140.48979589930008
At time: 806.67502784729 and batch: 500, loss is 4.9200496578216555 and perplexity is 137.00941660672106
At time: 808.3668308258057 and batch: 550, loss is 4.950271577835083 and perplexity is 141.21330911916374
At time: 810.0577397346497 and batch: 600, loss is 4.916703357696533 and perplexity is 136.55170822070932
At time: 811.7458860874176 and batch: 650, loss is 4.9068146991729735 and perplexity is 135.2080494478795
At time: 813.4337329864502 and batch: 700, loss is 4.9045943260192875 and perplexity is 134.90817016987407
At time: 815.1227312088013 and batch: 750, loss is 4.911493978500366 and perplexity is 135.8422082244751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.007343735805778 and perplexity of 149.50707794231207
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 819.6479375362396 and batch: 50, loss is 4.976694459915161 and perplexity is 144.9943041485895
At time: 821.3671612739563 and batch: 100, loss is 4.968354845046997 and perplexity is 143.7901356238485
At time: 823.057124376297 and batch: 150, loss is 4.953781290054321 and perplexity is 141.70979795251193
At time: 824.7448735237122 and batch: 200, loss is 4.957730655670166 and perplexity is 142.27056837079405
At time: 826.4341628551483 and batch: 250, loss is 4.967502546310425 and perplexity is 143.66763568361233
At time: 828.1032497882843 and batch: 300, loss is 4.981204671859741 and perplexity is 145.64973614860742
At time: 829.7945003509521 and batch: 350, loss is 4.9311753845214845 and perplexity is 138.54225710195632
At time: 831.4752078056335 and batch: 400, loss is 4.940754699707031 and perplexity is 139.8757739409986
At time: 833.1491329669952 and batch: 450, loss is 4.913060874938965 and perplexity is 136.05522574136492
At time: 834.8331561088562 and batch: 500, loss is 4.881741590499878 and perplexity is 131.86011034243575
At time: 836.5228054523468 and batch: 550, loss is 4.909138736724853 and perplexity is 135.52264345465795
At time: 838.1968598365784 and batch: 600, loss is 4.878419342041016 and perplexity is 131.4227651809699
At time: 839.8566496372223 and batch: 650, loss is 4.8633487510681155 and perplexity is 129.45699629755043
At time: 841.5757827758789 and batch: 700, loss is 4.861848659515381 and perplexity is 129.2629445350561
At time: 843.2388768196106 and batch: 750, loss is 4.878006916046143 and perplexity is 131.36857419194556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980228867641715 and perplexity of 145.50767984256268
Finished 30 epochs...
Completing Train Step...
At time: 847.7177641391754 and batch: 50, loss is 4.958017463684082 and perplexity is 142.3113785620124
At time: 849.4361660480499 and batch: 100, loss is 4.951579580307007 and perplexity is 141.3981373280858
At time: 851.1269955635071 and batch: 150, loss is 4.938598833084106 and perplexity is 139.57454524971408
At time: 852.8217871189117 and batch: 200, loss is 4.94506718635559 and perplexity is 140.4802888930318
At time: 854.5118408203125 and batch: 250, loss is 4.9561245822906494 and perplexity is 142.04225479150335
At time: 856.2047033309937 and batch: 300, loss is 4.971861743927002 and perplexity is 144.29527831388924
At time: 857.8964140415192 and batch: 350, loss is 4.924835300445556 and perplexity is 137.6666661370125
At time: 859.5875430107117 and batch: 400, loss is 4.935515127182007 and perplexity is 139.14480134319996
At time: 861.2762105464935 and batch: 450, loss is 4.909040937423706 and perplexity is 135.5093900829343
At time: 862.9659821987152 and batch: 500, loss is 4.87956901550293 and perplexity is 131.57394533364265
At time: 864.6562547683716 and batch: 550, loss is 4.909168882369995 and perplexity is 135.52672893375563
At time: 866.3454246520996 and batch: 600, loss is 4.8787033748626705 and perplexity is 131.4600988615382
At time: 868.0344743728638 and batch: 650, loss is 4.865392713546753 and perplexity is 129.72187214651203
At time: 869.7236354351044 and batch: 700, loss is 4.863261432647705 and perplexity is 129.4456928106311
At time: 871.4157967567444 and batch: 750, loss is 4.878456726074218 and perplexity is 131.4276783858241
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978621726812318 and perplexity of 145.27401632468099
Finished 31 epochs...
Completing Train Step...
At time: 875.954913854599 and batch: 50, loss is 4.951421661376953 and perplexity is 141.37580964855542
At time: 877.6768736839294 and batch: 100, loss is 4.945420083999633 and perplexity is 140.5298728045248
At time: 879.3692970275879 and batch: 150, loss is 4.932867937088012 and perplexity is 138.7769457101632
At time: 881.0631484985352 and batch: 200, loss is 4.940533370971679 and perplexity is 139.8448188385991
At time: 882.7553281784058 and batch: 250, loss is 4.951735496520996 and perplexity is 141.42018530909735
At time: 884.4954378604889 and batch: 300, loss is 4.968491115570068 and perplexity is 143.80973131596969
At time: 886.1890754699707 and batch: 350, loss is 4.9223206424713135 and perplexity is 137.32091646051
At time: 887.8791918754578 and batch: 400, loss is 4.932885160446167 and perplexity is 138.77933593578658
At time: 889.5680599212646 and batch: 450, loss is 4.907076568603515 and perplexity is 135.24346093918425
At time: 891.2637701034546 and batch: 500, loss is 4.878140983581543 and perplexity is 131.38618763358542
At time: 892.9558789730072 and batch: 550, loss is 4.908724184036255 and perplexity is 135.46647382188846
At time: 894.6451921463013 and batch: 600, loss is 4.877376651763916 and perplexity is 131.2858033583152
At time: 896.3349094390869 and batch: 650, loss is 4.865263032913208 and perplexity is 129.7050508226709
At time: 898.0258824825287 and batch: 700, loss is 4.862494449615479 and perplexity is 129.34644822497242
At time: 899.7175691127777 and batch: 750, loss is 4.877529039382934 and perplexity is 131.30581121373595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978248418763626 and perplexity of 145.21979448647517
Finished 32 epochs...
Completing Train Step...
At time: 904.2679877281189 and batch: 50, loss is 4.947629985809326 and perplexity is 140.84077342814888
At time: 905.9897835254669 and batch: 100, loss is 4.941894903182983 and perplexity is 140.0353517429414
At time: 907.6821177005768 and batch: 150, loss is 4.92958701133728 and perplexity is 138.32237496956677
At time: 909.3713014125824 and batch: 200, loss is 4.93758189201355 and perplexity is 139.43267830963055
At time: 911.0622084140778 and batch: 250, loss is 4.948262338638306 and perplexity is 140.9298626546054
At time: 912.7531976699829 and batch: 300, loss is 4.966122436523437 and perplexity is 143.46949533270063
At time: 914.4431982040405 and batch: 350, loss is 4.920286703109741 and perplexity is 137.04189789296686
At time: 916.130658864975 and batch: 400, loss is 4.931203346252442 and perplexity is 138.54613103743637
At time: 917.8076269626617 and batch: 450, loss is 4.904849748611451 and perplexity is 134.94263316552934
At time: 919.4898452758789 and batch: 500, loss is 4.876518869400025 and perplexity is 131.173236997187
At time: 921.1795828342438 and batch: 550, loss is 4.907148656845092 and perplexity is 135.2532107538874
At time: 922.8699142932892 and batch: 600, loss is 4.876139945983887 and perplexity is 131.12354180204935
At time: 924.5595002174377 and batch: 650, loss is 4.864234876632691 and perplexity is 129.57176229251414
At time: 926.2756564617157 and batch: 700, loss is 4.86133192062378 and perplexity is 129.1961665992361
At time: 927.9662086963654 and batch: 750, loss is 4.876626033782959 and perplexity is 131.1872948494243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9774052819540335 and perplexity of 145.09740593465898
Finished 33 epochs...
Completing Train Step...
At time: 932.3942310810089 and batch: 50, loss is 4.944295616149902 and perplexity is 140.37194029228672
At time: 934.1271162033081 and batch: 100, loss is 4.9387100887298585 and perplexity is 139.59007456972262
At time: 935.8200242519379 and batch: 150, loss is 4.925545177459717 and perplexity is 137.76442723399174
At time: 937.5095298290253 and batch: 200, loss is 4.93440260887146 and perplexity is 138.99008628151535
At time: 939.1947703361511 and batch: 250, loss is 4.944853916168213 and perplexity is 140.45033183008445
At time: 940.8794982433319 and batch: 300, loss is 4.963994998931884 and perplexity is 143.16459737577338
At time: 942.5713622570038 and batch: 350, loss is 4.918318099975586 and perplexity is 136.77238215506748
At time: 944.2573380470276 and batch: 400, loss is 4.929206447601318 and perplexity is 138.26974450503954
At time: 945.9455177783966 and batch: 450, loss is 4.902666921615601 and perplexity is 134.6483979920338
At time: 947.6378569602966 and batch: 500, loss is 4.874756822586059 and perplexity is 130.9423071272288
At time: 949.3281462192535 and batch: 550, loss is 4.905468816757202 and perplexity is 135.0261977146324
At time: 951.0140700340271 and batch: 600, loss is 4.874127941131592 and perplexity is 130.85998582655407
At time: 952.700511932373 and batch: 650, loss is 4.862211408615113 and perplexity is 129.309843057499
At time: 954.3897309303284 and batch: 700, loss is 4.859827098846435 and perplexity is 129.00189560246184
At time: 956.0834393501282 and batch: 750, loss is 4.87526873588562 and perplexity is 131.00935539570838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.977188820062682 and perplexity of 145.06600127481877
Finished 34 epochs...
Completing Train Step...
At time: 960.6513464450836 and batch: 50, loss is 4.941311893463134 and perplexity is 139.95373356615985
At time: 962.3766515254974 and batch: 100, loss is 4.935260496139526 and perplexity is 139.10937526785887
At time: 964.0657143592834 and batch: 150, loss is 4.922604866027832 and perplexity is 137.35995184689622
At time: 965.7533853054047 and batch: 200, loss is 4.93179518699646 and perplexity is 138.6281525521534
At time: 967.4413392543793 and batch: 250, loss is 4.94210391998291 and perplexity is 140.06462454318626
At time: 969.1620225906372 and batch: 300, loss is 4.961895742416382 and perplexity is 142.86437339577026
At time: 970.8501577377319 and batch: 350, loss is 4.916315746307373 and perplexity is 136.49878948000963
At time: 972.5337219238281 and batch: 400, loss is 4.926440172195434 and perplexity is 137.88778086332573
At time: 974.2082004547119 and batch: 450, loss is 4.900870199203491 and perplexity is 134.4066894042492
At time: 975.8744924068451 and batch: 500, loss is 4.872762851715088 and perplexity is 130.68147211616795
At time: 977.5396978855133 and batch: 550, loss is 4.903926296234131 and perplexity is 134.81807758955358
At time: 979.2144241333008 and batch: 600, loss is 4.872692613601685 and perplexity is 130.67229361845375
At time: 980.8899910449982 and batch: 650, loss is 4.860601463317871 and perplexity is 129.10182877451962
At time: 982.5779891014099 and batch: 700, loss is 4.858437900543213 and perplexity is 128.822810808925
At time: 984.2666094303131 and batch: 750, loss is 4.874036808013916 and perplexity is 130.84806069146222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.976272228152253 and perplexity of 144.93309587088606
Finished 35 epochs...
Completing Train Step...
At time: 988.7773740291595 and batch: 50, loss is 4.938079147338867 and perplexity is 139.50202919256225
At time: 990.4884927272797 and batch: 100, loss is 4.933102207183838 and perplexity is 138.8094608070418
At time: 992.175971031189 and batch: 150, loss is 4.919914846420288 and perplexity is 136.9909474202242
At time: 993.857759475708 and batch: 200, loss is 4.929406051635742 and perplexity is 138.29734645852048
At time: 995.5402245521545 and batch: 250, loss is 4.9395750141143795 and perplexity is 139.71086179708465
At time: 997.2240042686462 and batch: 300, loss is 4.959621753692627 and perplexity is 142.53987051941712
At time: 998.9116017818451 and batch: 350, loss is 4.914322347640991 and perplexity is 136.22696399338997
At time: 1000.5827367305756 and batch: 400, loss is 4.92465950012207 and perplexity is 137.64246641979375
At time: 1002.246701002121 and batch: 450, loss is 4.898706588745117 and perplexity is 134.1162000515996
At time: 1003.9091691970825 and batch: 500, loss is 4.870597696304321 and perplexity is 130.398832509481
At time: 1005.5729451179504 and batch: 550, loss is 4.902012596130371 and perplexity is 134.56032293170293
At time: 1007.2450385093689 and batch: 600, loss is 4.870942535400391 and perplexity is 130.44380687902694
At time: 1008.9307162761688 and batch: 650, loss is 4.858672456741333 and perplexity is 128.85303054163376
At time: 1010.6569945812225 and batch: 700, loss is 4.856609973907471 and perplexity is 128.587547249527
At time: 1012.3478746414185 and batch: 750, loss is 4.8723196601867675 and perplexity is 130.6235680270663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.975758042446403 and perplexity of 144.8585925006101
Finished 36 epochs...
Completing Train Step...
At time: 1016.8576600551605 and batch: 50, loss is 4.935167064666748 and perplexity is 139.09637868120492
At time: 1018.5754165649414 and batch: 100, loss is 4.930228681564331 and perplexity is 138.41116080188584
At time: 1020.2662873268127 and batch: 150, loss is 4.917239265441895 and perplexity is 136.62490695091853
At time: 1021.9531962871552 and batch: 200, loss is 4.926791887283326 and perplexity is 137.93628660589192
At time: 1023.6419548988342 and batch: 250, loss is 4.937082672119141 and perplexity is 139.36308811453785
At time: 1025.3328030109406 and batch: 300, loss is 4.95759183883667 and perplexity is 142.25082019171475
At time: 1027.0234849452972 and batch: 350, loss is 4.912387857437134 and perplexity is 135.96368899959518
At time: 1028.711735010147 and batch: 400, loss is 4.92249608039856 and perplexity is 137.34500987084854
At time: 1030.399483203888 and batch: 450, loss is 4.8965938282012935 and perplexity is 133.83314375622047
At time: 1032.0891659259796 and batch: 500, loss is 4.868906564712525 and perplexity is 130.1784972847324
At time: 1033.779706954956 and batch: 550, loss is 4.900377168655395 and perplexity is 134.34043913355006
At time: 1035.4673039913177 and batch: 600, loss is 4.869002847671509 and perplexity is 130.1910318590704
At time: 1037.156159877777 and batch: 650, loss is 4.856876211166382 and perplexity is 128.62178660333043
At time: 1038.8457527160645 and batch: 700, loss is 4.85503116607666 and perplexity is 128.3846923995345
At time: 1040.5368225574493 and batch: 750, loss is 4.870316286087036 and perplexity is 130.36214210845975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.976369813431141 and perplexity of 144.94723989757986
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1045.0460953712463 and batch: 50, loss is 4.933441247940063 and perplexity is 138.85653085048574
At time: 1046.7654044628143 and batch: 100, loss is 4.9285960865020755 and perplexity is 138.18537578210388
At time: 1048.45716714859 and batch: 150, loss is 4.913879680633545 and perplexity is 136.166674156051
At time: 1050.145602464676 and batch: 200, loss is 4.921324186325073 and perplexity is 137.1841503413321
At time: 1051.8355803489685 and batch: 250, loss is 4.927248973846435 and perplexity is 137.99934984064535
At time: 1053.5556013584137 and batch: 300, loss is 4.943722820281982 and perplexity is 140.29155884819264
At time: 1055.247431755066 and batch: 350, loss is 4.898728666305542 and perplexity is 134.11916104279572
At time: 1056.9360764026642 and batch: 400, loss is 4.907700252532959 and perplexity is 135.32783642141547
At time: 1058.6249492168427 and batch: 450, loss is 4.876641111373901 and perplexity is 131.18927285270465
At time: 1060.3149135112762 and batch: 500, loss is 4.846136026382446 and perplexity is 127.24775672339179
At time: 1062.0097444057465 and batch: 550, loss is 4.875272541046143 and perplexity is 131.00985390828407
At time: 1063.7066237926483 and batch: 600, loss is 4.845945978164673 and perplexity is 127.22357581185328
At time: 1065.406934261322 and batch: 650, loss is 4.831817255020142 and perplexity is 125.43870777621545
At time: 1067.1042540073395 and batch: 700, loss is 4.830774049758912 and perplexity is 125.30791768861644
At time: 1068.801577091217 and batch: 750, loss is 4.853613252639771 and perplexity is 128.20278301544732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.962916529455851 and perplexity of 143.0102819546476
Finished 38 epochs...
Completing Train Step...
At time: 1073.320039987564 and batch: 50, loss is 4.924189147949218 and perplexity is 137.5777412096514
At time: 1075.0438454151154 and batch: 100, loss is 4.920725288391114 and perplexity is 137.10201563472876
At time: 1076.7314286231995 and batch: 150, loss is 4.9063185119628905 and perplexity is 135.14097758451965
At time: 1078.414739370346 and batch: 200, loss is 4.914025735855103 and perplexity is 136.1865634622468
At time: 1080.097805261612 and batch: 250, loss is 4.920788202285767 and perplexity is 137.11064152783854
At time: 1081.7788054943085 and batch: 300, loss is 4.938233346939087 and perplexity is 139.52354200828466
At time: 1083.469617843628 and batch: 350, loss is 4.895350818634033 and perplexity is 133.66689122626363
At time: 1085.1586511135101 and batch: 400, loss is 4.9046395397186275 and perplexity is 134.91427000521568
At time: 1086.8360214233398 and batch: 450, loss is 4.875103340148926 and perplexity is 130.98768879868967
At time: 1088.5282137393951 and batch: 500, loss is 4.846050472259521 and perplexity is 127.23687061885337
At time: 1090.2263700962067 and batch: 550, loss is 4.876332817077636 and perplexity is 131.14883418197905
At time: 1091.9252290725708 and batch: 600, loss is 4.8477714729309085 and perplexity is 127.45603389459204
At time: 1093.6248226165771 and batch: 650, loss is 4.833641452789307 and perplexity is 125.66774162511288
At time: 1095.3548831939697 and batch: 700, loss is 4.832363195419312 and perplexity is 125.50720853137356
At time: 1097.0516760349274 and batch: 750, loss is 4.8539278602600096 and perplexity is 128.24312293320986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.961833510287972 and perplexity of 142.85548291837947
Finished 39 epochs...
Completing Train Step...
At time: 1101.566344499588 and batch: 50, loss is 4.9213565063476565 and perplexity is 137.18858420782018
At time: 1103.2951860427856 and batch: 100, loss is 4.917705917358399 and perplexity is 136.6886781038975
At time: 1104.992381811142 and batch: 150, loss is 4.903185482025147 and perplexity is 134.71823942737785
At time: 1106.6851644515991 and batch: 200, loss is 4.91103343963623 and perplexity is 135.77966201178404
At time: 1108.3777301311493 and batch: 250, loss is 4.917776851654053 and perplexity is 136.6983743628973
At time: 1110.0728733539581 and batch: 300, loss is 4.935566396713257 and perplexity is 139.15193541481935
At time: 1111.7642259597778 and batch: 350, loss is 4.893817939758301 and perplexity is 133.46215303183448
At time: 1113.4584867954254 and batch: 400, loss is 4.903399953842163 and perplexity is 134.74713579159018
At time: 1115.154141664505 and batch: 450, loss is 4.874215993881226 and perplexity is 130.87150891543553
At time: 1116.8546488285065 and batch: 500, loss is 4.845965681076049 and perplexity is 127.22608251138702
At time: 1118.5492634773254 and batch: 550, loss is 4.87651273727417 and perplexity is 131.17243262885515
At time: 1120.2463898658752 and batch: 600, loss is 4.848337659835815 and perplexity is 127.5282182649288
At time: 1121.935584306717 and batch: 650, loss is 4.834001808166504 and perplexity is 125.71303483188295
At time: 1123.634015083313 and batch: 700, loss is 4.83272871017456 and perplexity is 125.55309165294999
At time: 1125.333826303482 and batch: 750, loss is 4.853611831665039 and perplexity is 128.20260084266155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.961440330327943 and perplexity of 142.79932604591758
Finished 40 epochs...
Completing Train Step...
At time: 1129.8699293136597 and batch: 50, loss is 4.919485282897949 and perplexity is 136.9321137436272
At time: 1131.5939214229584 and batch: 100, loss is 4.915788135528564 and perplexity is 136.42679024284263
At time: 1133.2818791866302 and batch: 150, loss is 4.9011487293243405 and perplexity is 134.44413092974617
At time: 1134.9744455814362 and batch: 200, loss is 4.909039134979248 and perplexity is 135.50914583500528
At time: 1136.664466381073 and batch: 250, loss is 4.915634860992432 and perplexity is 136.40588109231322
At time: 1138.3807883262634 and batch: 300, loss is 4.9334093284606935 and perplexity is 138.85209869305035
At time: 1140.0712497234344 and batch: 350, loss is 4.892487182617187 and perplexity is 133.28466544125124
At time: 1141.7599499225616 and batch: 400, loss is 4.902120742797852 and perplexity is 134.5748759691202
At time: 1143.4492316246033 and batch: 450, loss is 4.872888288497925 and perplexity is 130.697865407746
At time: 1145.1408078670502 and batch: 500, loss is 4.845283699035645 and perplexity is 127.13934618771246
At time: 1146.8297543525696 and batch: 550, loss is 4.875779981613159 and perplexity is 131.0763504929415
At time: 1148.5169212818146 and batch: 600, loss is 4.8479392719268795 and perplexity is 127.47742268356848
At time: 1150.206971168518 and batch: 650, loss is 4.83392728805542 and perplexity is 125.70366703161159
At time: 1151.8983674049377 and batch: 700, loss is 4.832714548110962 and perplexity is 125.5513135746717
At time: 1153.5894074440002 and batch: 750, loss is 4.852987117767334 and perplexity is 128.12253590763598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.961069861123728 and perplexity of 142.7464330914452
Finished 41 epochs...
Completing Train Step...
At time: 1158.0993120670319 and batch: 50, loss is 4.918104162216187 and perplexity is 136.74312450785084
At time: 1159.8169856071472 and batch: 100, loss is 4.91444884300232 and perplexity is 136.244197162358
At time: 1161.5036766529083 and batch: 150, loss is 4.899385061264038 and perplexity is 134.20722508317613
At time: 1163.191636800766 and batch: 200, loss is 4.906922540664673 and perplexity is 135.22263127187037
At time: 1164.878315448761 and batch: 250, loss is 4.913574905395508 and perplexity is 136.12518024900123
At time: 1166.5686893463135 and batch: 300, loss is 4.931552000045777 and perplexity is 138.59444409335012
At time: 1168.25231051445 and batch: 350, loss is 4.891232566833496 and perplexity is 133.11754925149026
At time: 1169.9374723434448 and batch: 400, loss is 4.900705823898315 and perplexity is 134.38459807934453
At time: 1171.6143639087677 and batch: 450, loss is 4.871761064529419 and perplexity is 130.55062264460275
At time: 1173.29070520401 and batch: 500, loss is 4.844416904449463 and perplexity is 127.02919023893323
At time: 1174.9773416519165 and batch: 550, loss is 4.875001583099365 and perplexity is 130.97436055608156
At time: 1176.662263393402 and batch: 600, loss is 4.847099018096924 and perplexity is 127.37035427954345
At time: 1178.3472034931183 and batch: 650, loss is 4.833529367446899 and perplexity is 125.65365690262348
At time: 1180.0650026798248 and batch: 700, loss is 4.832301902770996 and perplexity is 125.49951609792716
At time: 1181.754001379013 and batch: 750, loss is 4.852489995956421 and perplexity is 128.0588592294109
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.960781718409339 and perplexity of 142.70530767202442
Finished 42 epochs...
Completing Train Step...
At time: 1186.2652287483215 and batch: 50, loss is 4.91676661491394 and perplexity is 136.5603463750135
At time: 1187.9862813949585 and batch: 100, loss is 4.913102550506592 and perplexity is 136.06089603828192
At time: 1189.6772270202637 and batch: 150, loss is 4.897848386764526 and perplexity is 134.0011506379645
At time: 1191.3646767139435 and batch: 200, loss is 4.90537992477417 and perplexity is 135.01419550161438
At time: 1193.053288936615 and batch: 250, loss is 4.9117630672454835 and perplexity is 135.87876675234378
At time: 1194.7246551513672 and batch: 300, loss is 4.929927320480346 and perplexity is 138.36945534894696
At time: 1196.3940818309784 and batch: 350, loss is 4.889953756332398 and perplexity is 132.9474259325547
At time: 1198.0568463802338 and batch: 400, loss is 4.899530715942383 and perplexity is 134.22677441706864
At time: 1199.7380058765411 and batch: 450, loss is 4.870719270706177 and perplexity is 130.41468663325432
At time: 1201.428108215332 and batch: 500, loss is 4.843548574447632 and perplexity is 126.91893485789578
At time: 1203.1178216934204 and batch: 550, loss is 4.87426342010498 and perplexity is 130.8777158040839
At time: 1204.8051226139069 and batch: 600, loss is 4.846437406539917 and perplexity is 127.28611245188837
At time: 1206.493206501007 and batch: 650, loss is 4.83298994064331 and perplexity is 125.58589423026527
At time: 1208.180674314499 and batch: 700, loss is 4.831784181594848 and perplexity is 125.4345591570898
At time: 1209.869870185852 and batch: 750, loss is 4.851876230239868 and perplexity is 127.9802852074021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9602568870367 and perplexity of 142.6304311000218
Finished 43 epochs...
Completing Train Step...
At time: 1214.3815364837646 and batch: 50, loss is 4.915802745819092 and perplexity is 136.42878349244478
At time: 1216.1000645160675 and batch: 100, loss is 4.9118798732757565 and perplexity is 135.89463913866368
At time: 1217.7882115840912 and batch: 150, loss is 4.896561517715454 and perplexity is 133.82881961218223
At time: 1219.4760401248932 and batch: 200, loss is 4.903863449096679 and perplexity is 134.80960492554448
At time: 1221.1651737689972 and batch: 250, loss is 4.910336856842041 and perplexity is 135.6851131697972
At time: 1222.8833920955658 and batch: 300, loss is 4.928287057876587 and perplexity is 138.14267914294498
At time: 1224.5561389923096 and batch: 350, loss is 4.8886729335784915 and perplexity is 132.77725284839036
At time: 1226.2353513240814 and batch: 400, loss is 4.89869462966919 and perplexity is 134.1145961553706
At time: 1227.9228160381317 and batch: 450, loss is 4.869010801315308 and perplexity is 130.19206735628154
At time: 1229.6107234954834 and batch: 500, loss is 4.842288513183593 and perplexity is 126.75910994014914
At time: 1231.2981231212616 and batch: 550, loss is 4.873069458007812 and perplexity is 130.72154602100304
At time: 1232.9835307598114 and batch: 600, loss is 4.845497751235962 and perplexity is 127.16656355737588
At time: 1234.669944524765 and batch: 650, loss is 4.832011384963989 and perplexity is 125.46306154933414
At time: 1236.3604452610016 and batch: 700, loss is 4.831171617507935 and perplexity is 125.35774597979116
At time: 1238.0494225025177 and batch: 750, loss is 4.850973863601684 and perplexity is 127.86485215698528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.960157172624455 and perplexity of 142.61620949947743
Finished 44 epochs...
Completing Train Step...
At time: 1242.5631411075592 and batch: 50, loss is 4.9145438098907475 and perplexity is 136.25713646422176
At time: 1244.285980939865 and batch: 100, loss is 4.91094970703125 and perplexity is 135.7682933029529
At time: 1245.9763162136078 and batch: 150, loss is 4.8954931735992435 and perplexity is 133.68592072635371
At time: 1247.669671535492 and batch: 200, loss is 4.903382987976074 and perplexity is 134.7448497091212
At time: 1249.360918521881 and batch: 250, loss is 4.908475685119629 and perplexity is 135.43281473220142
At time: 1251.0521149635315 and batch: 300, loss is 4.92692889213562 and perplexity is 137.95518584107828
At time: 1252.7404704093933 and batch: 350, loss is 4.887456588745117 and perplexity is 132.61584810471135
At time: 1254.4323365688324 and batch: 400, loss is 4.897051019668579 and perplexity is 133.89434511688972
At time: 1256.119470357895 and batch: 450, loss is 4.868034286499023 and perplexity is 130.0649949276979
At time: 1257.8070781230927 and batch: 500, loss is 4.841264305114746 and perplexity is 126.62934869954664
At time: 1259.4936878681183 and batch: 550, loss is 4.87213680267334 and perplexity is 130.59968470991035
At time: 1261.172387599945 and batch: 600, loss is 4.844981145858765 and perplexity is 127.10088559309786
At time: 1262.848239183426 and batch: 650, loss is 4.8319633674621585 and perplexity is 125.45703727118307
At time: 1264.5630795955658 and batch: 700, loss is 4.83088680267334 and perplexity is 125.32204731810025
At time: 1266.2505340576172 and batch: 750, loss is 4.850253496170044 and perplexity is 127.77277565028001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.959387136060138 and perplexity of 142.50643207523825
Finished 45 epochs...
Completing Train Step...
At time: 1270.7040739059448 and batch: 50, loss is 4.913361091613769 and perplexity is 136.09607792077264
At time: 1272.4248178005219 and batch: 100, loss is 4.909871129989624 and perplexity is 135.62193568198904
At time: 1274.0983991622925 and batch: 150, loss is 4.894171962738037 and perplexity is 133.50941006547717
At time: 1275.7901933193207 and batch: 200, loss is 4.901758899688721 and perplexity is 134.52618978649969
At time: 1277.4858167171478 and batch: 250, loss is 4.907008028030395 and perplexity is 135.2341915925275
At time: 1279.1777784824371 and batch: 300, loss is 4.9253248119354245 and perplexity is 137.7340720484965
At time: 1280.8716475963593 and batch: 350, loss is 4.886291723251343 and perplexity is 132.46145841839348
At time: 1282.5632572174072 and batch: 400, loss is 4.8958792400360105 and perplexity is 133.737542337459
At time: 1284.252810716629 and batch: 450, loss is 4.866878690719605 and perplexity is 129.91477917956817
At time: 1285.9448747634888 and batch: 500, loss is 4.8401973438262935 and perplexity is 126.49431213870433
At time: 1287.6383910179138 and batch: 550, loss is 4.870988159179688 and perplexity is 130.44975835424597
At time: 1289.3284442424774 and batch: 600, loss is 4.844138994216919 and perplexity is 126.99389243217206
At time: 1291.0185725688934 and batch: 650, loss is 4.830946702957153 and perplexity is 125.32955436913764
At time: 1292.7118179798126 and batch: 700, loss is 4.829802255630494 and perplexity is 125.18620334018568
At time: 1294.4043235778809 and batch: 750, loss is 4.849474630355835 and perplexity is 127.673296548804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.958946050599564 and perplexity of 142.4435884207416
Finished 46 epochs...
Completing Train Step...
At time: 1298.9285175800323 and batch: 50, loss is 4.912297220230102 and perplexity is 135.9513661890276
At time: 1300.647353887558 and batch: 100, loss is 4.908477792739868 and perplexity is 135.4331001734436
At time: 1302.3371894359589 and batch: 150, loss is 4.8927405834197994 and perplexity is 133.3184441620467
At time: 1304.0279200077057 and batch: 200, loss is 4.899873676300049 and perplexity is 134.27281677453146
At time: 1305.7200727462769 and batch: 250, loss is 4.905690135955811 and perplexity is 135.05608491168502
At time: 1307.44242978096 and batch: 300, loss is 4.9238902187347415 and perplexity is 137.536621349812
At time: 1309.1310069561005 and batch: 350, loss is 4.884900808334351 and perplexity is 132.27734387346925
At time: 1310.821217060089 and batch: 400, loss is 4.894787044525146 and perplexity is 133.59155453220907
At time: 1312.5121273994446 and batch: 450, loss is 4.865467348098755 and perplexity is 129.73155424162942
At time: 1314.2034230232239 and batch: 500, loss is 4.838967914581299 and perplexity is 126.33889189071103
At time: 1315.8924572467804 and batch: 550, loss is 4.869831438064575 and perplexity is 130.29895160178873
At time: 1317.5811743736267 and batch: 600, loss is 4.843277540206909 and perplexity is 126.88454014203661
At time: 1319.271695137024 and batch: 650, loss is 4.829902906417846 and perplexity is 125.19880406424333
At time: 1320.96639418602 and batch: 700, loss is 4.82909122467041 and perplexity is 125.09722371121978
At time: 1322.6657807826996 and batch: 750, loss is 4.8487864685058595 and perplexity is 127.58546688083801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.958876498909884 and perplexity of 142.4336815730059
Finished 47 epochs...
Completing Train Step...
At time: 1327.1963438987732 and batch: 50, loss is 4.912093963623047 and perplexity is 135.92373598371748
At time: 1328.919713973999 and batch: 100, loss is 4.907904176712036 and perplexity is 135.35543585336413
At time: 1330.6091272830963 and batch: 150, loss is 4.891822547912597 and perplexity is 133.19610925902523
At time: 1332.2968003749847 and batch: 200, loss is 4.8990716457366945 and perplexity is 134.16516904580521
At time: 1333.9887299537659 and batch: 250, loss is 4.904312353134156 and perplexity is 134.87013508658654
At time: 1335.6870920658112 and batch: 300, loss is 4.922488384246826 and perplexity is 137.34395284688023
At time: 1337.3831524848938 and batch: 350, loss is 4.8838459014892575 and perplexity is 132.13787717308838
At time: 1339.0751259326935 and batch: 400, loss is 4.89349401473999 and perplexity is 133.41892830263342
At time: 1340.7696759700775 and batch: 450, loss is 4.864564151763916 and perplexity is 129.61443407653917
At time: 1342.4629788398743 and batch: 500, loss is 4.838007116317749 and perplexity is 126.21756399791151
At time: 1344.1377127170563 and batch: 550, loss is 4.868885583877564 and perplexity is 130.1757660598172
At time: 1345.8030054569244 and batch: 600, loss is 4.8423786544799805 and perplexity is 126.77053668565112
At time: 1347.4684298038483 and batch: 650, loss is 4.828906230926513 and perplexity is 125.0740836479038
At time: 1349.1757113933563 and batch: 700, loss is 4.8282326984405515 and perplexity is 124.98987055272627
At time: 1350.8388333320618 and batch: 750, loss is 4.847960824966431 and perplexity is 127.48017023911045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.958709716796875 and perplexity of 142.40992816350646
Finished 48 epochs...
Completing Train Step...
At time: 1355.2528865337372 and batch: 50, loss is 4.910762996673584 and perplexity is 135.74294632269607
At time: 1356.9692981243134 and batch: 100, loss is 4.907029294967652 and perplexity is 135.2370676401773
At time: 1358.665904045105 and batch: 150, loss is 4.890766115188598 and perplexity is 133.05547083109337
At time: 1360.3542656898499 and batch: 200, loss is 4.897726001739502 and perplexity is 133.98475190729047
At time: 1362.0430099964142 and batch: 250, loss is 4.902950687408447 and perplexity is 134.6866120231065
At time: 1363.732920408249 and batch: 300, loss is 4.921186733245849 and perplexity is 137.16529525332166
At time: 1365.4194118976593 and batch: 350, loss is 4.882660531997681 and perplexity is 131.98133776165648
At time: 1367.082013130188 and batch: 400, loss is 4.892406072616577 and perplexity is 133.27385516037282
At time: 1368.7449743747711 and batch: 450, loss is 4.863514213562012 and perplexity is 129.47841834723417
At time: 1370.4268882274628 and batch: 500, loss is 4.837031478881836 and perplexity is 126.09448146912995
At time: 1372.0929815769196 and batch: 550, loss is 4.8679932117462155 and perplexity is 130.05965264989942
At time: 1373.7594368457794 and batch: 600, loss is 4.841469573974609 and perplexity is 126.65534442954511
At time: 1375.4228072166443 and batch: 650, loss is 4.828081712722779 and perplexity is 124.97100029201232
At time: 1377.085163116455 and batch: 700, loss is 4.827341375350952 and perplexity is 124.878513829918
At time: 1378.7570049762726 and batch: 750, loss is 4.847169437408447 and perplexity is 127.37932392801483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.958394250204397 and perplexity of 142.36500967424757
Finished 49 epochs...
Completing Train Step...
At time: 1383.3504176139832 and batch: 50, loss is 4.909663858413697 and perplexity is 135.59382802270994
At time: 1385.0837111473083 and batch: 100, loss is 4.905805025100708 and perplexity is 135.07160228116527
At time: 1386.770111322403 and batch: 150, loss is 4.8895580863952635 and perplexity is 132.8948330382935
At time: 1388.4604671001434 and batch: 200, loss is 4.896430263519287 and perplexity is 133.8112551707686
At time: 1390.1525177955627 and batch: 250, loss is 4.9016252136230465 and perplexity is 134.50820671152704
At time: 1391.8739256858826 and batch: 300, loss is 4.919931287765503 and perplexity is 136.99319975419772
At time: 1393.560084104538 and batch: 350, loss is 4.881645755767822 and perplexity is 131.84747416959402
At time: 1395.2479012012482 and batch: 400, loss is 4.891233425140381 and perplexity is 133.11766350724835
At time: 1396.9379942417145 and batch: 450, loss is 4.86267858505249 and perplexity is 129.37026768267796
At time: 1398.6305339336395 and batch: 500, loss is 4.836147260665894 and perplexity is 125.98303571013132
At time: 1400.3237075805664 and batch: 550, loss is 4.867117280960083 and perplexity is 129.94577927598746
At time: 1402.0119915008545 and batch: 600, loss is 4.840667228698731 and perplexity is 126.55376386906818
At time: 1403.7045397758484 and batch: 650, loss is 4.827225151062012 and perplexity is 124.86400075684642
At time: 1405.3959860801697 and batch: 700, loss is 4.826358547210694 and perplexity is 124.75584000593022
At time: 1407.0796077251434 and batch: 750, loss is 4.846319608688354 and perplexity is 127.27111930440535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.957617116528888 and perplexity of 142.25441600961204
Finished Training.
Improved accuracyfrom -205.22238492976274 to -142.25441600961204
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f231118f208>
SETTINGS FOR THIS RUN
{'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 20.848620798797626, 'dropout': 0.30043171347418685, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 3.5148099363181053}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2630186080932617 and batch: 50, loss is 7.64457447052002 and perplexity is 2089.2793366991496
At time: 3.9247448444366455 and batch: 100, loss is 6.52935019493103 and perplexity is 684.9529809319228
At time: 5.613617181777954 and batch: 150, loss is 6.2304565048217775 and perplexity is 507.9873292376167
At time: 7.2936341762542725 and batch: 200, loss is 6.190553150177002 and perplexity is 488.11603302259084
At time: 8.97413182258606 and batch: 250, loss is 6.2689995765686035 and perplexity is 527.9489411002364
At time: 10.670179605484009 and batch: 300, loss is 6.30572057723999 and perplexity is 547.6961033749985
At time: 12.364463806152344 and batch: 350, loss is 6.2802557945251465 and perplexity is 533.9252215079622
At time: 14.06113862991333 and batch: 400, loss is 6.348265390396119 and perplexity is 571.5005184428913
At time: 15.758752346038818 and batch: 450, loss is 6.323663301467896 and perplexity is 557.611956199416
At time: 17.52701759338379 and batch: 500, loss is 6.365442485809326 and perplexity is 581.402033557354
At time: 19.224612712860107 and batch: 550, loss is 6.400831518173217 and perplexity is 602.3456910808072
At time: 20.928563117980957 and batch: 600, loss is 6.3636589622497555 and perplexity is 580.3660134906487
At time: 22.626524925231934 and batch: 650, loss is 6.312459154129028 and perplexity is 551.3992586648859
At time: 24.331547021865845 and batch: 700, loss is 6.371730079650879 and perplexity is 585.0691700541191
At time: 26.03356122970581 and batch: 750, loss is 6.386948289871216 and perplexity is 594.0409698949323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.861118050508721 and perplexity of 351.11649061834197
Finished 1 epochs...
Completing Train Step...
At time: 30.78238868713379 and batch: 50, loss is 6.261884727478027 and perplexity is 524.2059950940894
At time: 32.44358825683594 and batch: 100, loss is 6.237886638641357 and perplexity is 511.77580006599175
At time: 34.09755206108093 and batch: 150, loss is 6.243143253326416 and perplexity is 514.4730913516078
At time: 35.74948239326477 and batch: 200, loss is 6.231845998764038 and perplexity is 508.69366516544096
At time: 37.403950929641724 and batch: 250, loss is 6.261047410964966 and perplexity is 523.767252467008
At time: 39.05814218521118 and batch: 300, loss is 6.289194250106812 and perplexity is 538.7190813183606
At time: 40.71327996253967 and batch: 350, loss is 6.202250413894653 and perplexity is 493.8591790483373
At time: 42.432273149490356 and batch: 400, loss is 6.24963210105896 and perplexity is 517.8222833508252
At time: 44.08624029159546 and batch: 450, loss is 6.227953634262085 and perplexity is 506.7174924879721
At time: 45.76202058792114 and batch: 500, loss is 6.237409162521362 and perplexity is 511.53149767158993
At time: 47.45571947097778 and batch: 550, loss is 6.235127248764038 and perplexity is 510.36555770290937
At time: 49.142239570617676 and batch: 600, loss is 6.230639457702637 and perplexity is 508.0802754850733
At time: 50.83657622337341 and batch: 650, loss is 6.2350534820556645 and perplexity is 510.32791110420027
At time: 52.52546834945679 and batch: 700, loss is 6.262846193313599 and perplexity is 524.7102436191941
At time: 54.217493772506714 and batch: 750, loss is 6.208780040740967 and perplexity is 497.09444625103737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.884336604628452 and perplexity of 359.3642882966307
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 58.83078575134277 and batch: 50, loss is 6.131402082443238 and perplexity is 460.0807798536238
At time: 60.52915406227112 and batch: 100, loss is 6.049844264984131 and perplexity is 424.0469859381234
At time: 62.23240828514099 and batch: 150, loss is 5.990017032623291 and perplexity is 399.421413063635
At time: 63.92148804664612 and batch: 200, loss is 5.932183198928833 and perplexity is 376.97663105258056
At time: 65.60785412788391 and batch: 250, loss is 5.962303094863891 and perplexity is 388.5038559034645
At time: 67.29298424720764 and batch: 300, loss is 5.95812084197998 and perplexity is 386.88242750689403
At time: 68.97942543029785 and batch: 350, loss is 5.854588222503662 and perplexity is 348.83122962551124
At time: 70.66221070289612 and batch: 400, loss is 5.84064359664917 and perplexity is 344.0006671115123
At time: 72.3464469909668 and batch: 450, loss is 5.829077405929565 and perplexity is 340.0448109812989
At time: 74.03208756446838 and batch: 500, loss is 5.812519073486328 and perplexity is 334.46059618663605
At time: 75.76559209823608 and batch: 550, loss is 5.772004766464233 and perplexity is 321.1809805097214
At time: 77.44439148902893 and batch: 600, loss is 5.712037763595581 and perplexity is 302.48683733909843
At time: 79.11976742744446 and batch: 650, loss is 5.673682737350464 and perplexity is 291.10462462567045
At time: 80.80133724212646 and batch: 700, loss is 5.670463809967041 and perplexity is 290.1690865004641
At time: 82.4859082698822 and batch: 750, loss is 5.672122392654419 and perplexity is 290.6507552570581
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.497331131336301 and perplexity of 244.03975231237422
Finished 3 epochs...
Completing Train Step...
At time: 87.02809858322144 and batch: 50, loss is 5.77433837890625 and perplexity is 321.9313676578356
At time: 88.74389719963074 and batch: 100, loss is 5.7772443485260006 and perplexity is 322.86825104992647
At time: 90.42900371551514 and batch: 150, loss is 5.748019256591797 and perplexity is 313.56894513877074
At time: 92.11303734779358 and batch: 200, loss is 5.724230680465698 and perplexity is 306.19761080485677
At time: 93.79692769050598 and batch: 250, loss is 5.752512559890747 and perplexity is 314.9810757039925
At time: 95.48026657104492 and batch: 300, loss is 5.75896656036377 and perplexity is 317.02053798150166
At time: 97.16237330436707 and batch: 350, loss is 5.680643663406372 and perplexity is 293.13805144838403
At time: 98.83321046829224 and batch: 400, loss is 5.704891242980957 and perplexity is 300.3328149706796
At time: 100.51725459098816 and batch: 450, loss is 5.689453496932983 and perplexity is 295.7319580478681
At time: 102.2123532295227 and batch: 500, loss is 5.680072965621949 and perplexity is 292.97080593975204
At time: 103.90659141540527 and batch: 550, loss is 5.68230712890625 and perplexity is 293.6260822823608
At time: 105.59698915481567 and batch: 600, loss is 5.654845914840698 and perplexity is 285.6724614483568
At time: 107.29276132583618 and batch: 650, loss is 5.646333065032959 and perplexity is 283.25089652303063
At time: 108.98843288421631 and batch: 700, loss is 5.638742074966431 and perplexity is 281.1088820721868
At time: 110.67714881896973 and batch: 750, loss is 5.63965223312378 and perplexity is 281.36485208323455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.4532012939453125 and perplexity of 233.5044871744047
Finished 4 epochs...
Completing Train Step...
At time: 115.2154803276062 and batch: 50, loss is 5.692499141693116 and perplexity is 296.6340255275523
At time: 116.94158625602722 and batch: 100, loss is 5.680043363571167 and perplexity is 292.9621335314383
At time: 118.62740182876587 and batch: 150, loss is 5.668749914169312 and perplexity is 289.6721928560775
At time: 120.31382751464844 and batch: 200, loss is 5.66566388130188 and perplexity is 288.7796328912016
At time: 121.99921774864197 and batch: 250, loss is 5.694798402786255 and perplexity is 297.31684929559253
At time: 123.68684601783752 and batch: 300, loss is 5.705263195037841 and perplexity is 300.44454515680593
At time: 125.37241792678833 and batch: 350, loss is 5.634879989624023 and perplexity is 280.02530935113066
At time: 127.05835771560669 and batch: 400, loss is 5.663675289154053 and perplexity is 288.2059385918275
At time: 128.74430298805237 and batch: 450, loss is 5.643493175506592 and perplexity is 282.44763639353096
At time: 130.43158769607544 and batch: 500, loss is 5.636020612716675 and perplexity is 280.3448949141912
At time: 132.11862993240356 and batch: 550, loss is 5.652968168258667 and perplexity is 285.13654427607827
At time: 133.80427646636963 and batch: 600, loss is 5.622970342636108 and perplexity is 276.71008751569457
At time: 135.49316883087158 and batch: 650, loss is 5.6142024803161625 and perplexity is 274.2945366492142
At time: 137.1813039779663 and batch: 700, loss is 5.603645715713501 and perplexity is 271.41410456656763
At time: 138.8694612979889 and batch: 750, loss is 5.60489706993103 and perplexity is 271.7539523416403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.432038063226744 and perplexity of 228.61470216533212
Finished 5 epochs...
Completing Train Step...
At time: 143.40791630744934 and batch: 50, loss is 5.655539093017578 and perplexity is 285.8705520124645
At time: 145.12331175804138 and batch: 100, loss is 5.646820669174194 and perplexity is 283.3890445112123
At time: 146.81119847297668 and batch: 150, loss is 5.635417699813843 and perplexity is 280.1759223028066
At time: 148.49775862693787 and batch: 200, loss is 5.636454305648804 and perplexity is 280.46650488245956
At time: 150.1850254535675 and batch: 250, loss is 5.665185480117798 and perplexity is 288.6415134137828
At time: 151.87101483345032 and batch: 300, loss is 5.677979202270508 and perplexity is 292.3580361245815
At time: 153.56301355361938 and batch: 350, loss is 5.599392881393433 and perplexity is 270.26227635105334
At time: 155.25262832641602 and batch: 400, loss is 5.628737287521362 and perplexity is 278.3104695617086
At time: 156.93837666511536 and batch: 450, loss is 5.611654424667359 and perplexity is 273.5965085905055
At time: 158.62478065490723 and batch: 500, loss is 5.602587175369263 and perplexity is 271.12695379404033
At time: 160.36037468910217 and batch: 550, loss is 5.620904970169067 and perplexity is 276.13916790336185
At time: 162.03948593139648 and batch: 600, loss is 5.592810640335083 and perplexity is 268.4891867524306
At time: 163.7259521484375 and batch: 650, loss is 5.58289587020874 and perplexity is 265.84033128001926
At time: 165.4042296409607 and batch: 700, loss is 5.572350778579712 and perplexity is 263.0517493942988
At time: 167.0812864303589 and batch: 750, loss is 5.568542575836181 and perplexity is 262.0519000234751
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.404253582621729 and perplexity of 222.35019248070137
Finished 6 epochs...
Completing Train Step...
At time: 171.65047883987427 and batch: 50, loss is 5.612939434051514 and perplexity is 273.9483086563002
At time: 173.3551425933838 and batch: 100, loss is 5.607234802246094 and perplexity is 272.3899834833935
At time: 175.0362091064453 and batch: 150, loss is 5.596308727264404 and perplexity is 269.4300298834087
At time: 176.71998858451843 and batch: 200, loss is 5.594872493743896 and perplexity is 269.04334319572047
At time: 178.4054892063141 and batch: 250, loss is 5.624829797744751 and perplexity is 277.22509617088605
At time: 180.09272694587708 and batch: 300, loss is 5.637921724319458 and perplexity is 280.8783687826996
At time: 181.7775433063507 and batch: 350, loss is 5.55765058517456 and perplexity is 259.21312121661714
At time: 183.46258163452148 and batch: 400, loss is 5.588170108795166 and perplexity is 267.2461406412222
At time: 185.14694142341614 and batch: 450, loss is 5.566233720779419 and perplexity is 261.44755810638674
At time: 186.83310794830322 and batch: 500, loss is 5.555119161605835 and perplexity is 258.5577728443659
At time: 188.52007126808167 and batch: 550, loss is 5.570196418762207 and perplexity is 262.48565128394125
At time: 190.20397281646729 and batch: 600, loss is 5.5383468341827395 and perplexity is 254.25732231578547
At time: 191.89166593551636 and batch: 650, loss is 5.531090297698975 and perplexity is 252.41897287191404
At time: 193.57844281196594 and batch: 700, loss is 5.5286823081970216 and perplexity is 251.81188186287997
At time: 195.26507210731506 and batch: 750, loss is 5.524948844909668 and perplexity is 250.87350423581944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3854859817859735 and perplexity of 218.216127406012
Finished 7 epochs...
Completing Train Step...
At time: 199.76591515541077 and batch: 50, loss is 5.569121599197388 and perplexity is 262.2036781327322
At time: 201.49013757705688 and batch: 100, loss is 5.558272733688354 and perplexity is 259.3744404517957
At time: 203.17429900169373 and batch: 150, loss is 5.553897523880005 and perplexity is 258.2421017718572
At time: 204.85791420936584 and batch: 200, loss is 5.555780639648438 and perplexity is 258.7288597127119
At time: 206.54370260238647 and batch: 250, loss is 5.586771984100341 and perplexity is 266.87275829076015
At time: 208.230530500412 and batch: 300, loss is 5.6024959278106685 and perplexity is 271.10221525012054
At time: 209.9149308204651 and batch: 350, loss is 5.5251432800292966 and perplexity is 250.92228759807298
At time: 211.59980463981628 and batch: 400, loss is 5.5528279972076415 and perplexity is 257.9660526033734
At time: 213.28589296340942 and batch: 450, loss is 5.536025695800781 and perplexity is 253.66784028540923
At time: 214.96909928321838 and batch: 500, loss is 5.527685689926147 and perplexity is 251.56104655487945
At time: 216.64307236671448 and batch: 550, loss is 5.541990518569946 and perplexity is 255.18544561878753
At time: 218.33039355278015 and batch: 600, loss is 5.507772645950317 and perplexity is 246.60124662042108
At time: 220.01800322532654 and batch: 650, loss is 5.502566719055176 and perplexity is 245.32079442040276
At time: 221.7049515247345 and batch: 700, loss is 5.4980107688903805 and perplexity is 244.20566726736465
At time: 223.39234924316406 and batch: 750, loss is 5.5038055896759035 and perplexity is 245.62490348225927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.366492692814317 and perplexity of 214.11059761679348
Finished 8 epochs...
Completing Train Step...
At time: 227.92808198928833 and batch: 50, loss is 5.5429355430603025 and perplexity is 255.4267160997804
At time: 229.64441895484924 and batch: 100, loss is 5.532953510284424 and perplexity is 252.88972149516877
At time: 231.3287889957428 and batch: 150, loss is 5.531409568786621 and perplexity is 252.49957581833604
At time: 233.01546168327332 and batch: 200, loss is 5.533941688537598 and perplexity is 253.1397451320164
At time: 234.7012357711792 and batch: 250, loss is 5.56389741897583 and perplexity is 260.8374506798921
At time: 236.38972759246826 and batch: 300, loss is 5.581562309265137 and perplexity is 265.4860532751484
At time: 238.0743851661682 and batch: 350, loss is 5.504315385818481 and perplexity is 245.75015403398697
At time: 239.7578580379486 and batch: 400, loss is 5.533575983047485 and perplexity is 253.04718746291124
At time: 241.4414188861847 and batch: 450, loss is 5.510045347213745 and perplexity is 247.162334936731
At time: 243.12145900726318 and batch: 500, loss is 5.504842224121094 and perplexity is 245.87965873902917
At time: 244.82682299613953 and batch: 550, loss is 5.518943853378296 and perplexity is 249.3715251666679
At time: 246.50553154945374 and batch: 600, loss is 5.482156438827515 and perplexity is 240.3644801987531
At time: 248.19351768493652 and batch: 650, loss is 5.478660440444946 and perplexity is 239.52563352242865
At time: 249.8812620639801 and batch: 700, loss is 5.47598711013794 and perplexity is 238.88615753314454
At time: 251.56914401054382 and batch: 750, loss is 5.4767043399810795 and perplexity is 239.05755527287982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.353991486305414 and perplexity of 211.45061793961648
Finished 9 epochs...
Completing Train Step...
At time: 256.0668773651123 and batch: 50, loss is 5.52259729385376 and perplexity is 250.28425547783922
At time: 257.7817974090576 and batch: 100, loss is 5.511599950790405 and perplexity is 247.54687321148225
At time: 259.45607590675354 and batch: 150, loss is 5.510559329986572 and perplexity is 247.289404772036
At time: 261.1337788105011 and batch: 200, loss is 5.511051120758057 and perplexity is 247.41104932857232
At time: 262.8192398548126 and batch: 250, loss is 5.543023386001587 and perplexity is 255.44915451931922
At time: 264.5043294429779 and batch: 300, loss is 5.558080997467041 and perplexity is 259.3247137440361
At time: 266.1859841346741 and batch: 350, loss is 5.481677675247193 and perplexity is 240.24942998275523
At time: 267.8708770275116 and batch: 400, loss is 5.5067675495147705 and perplexity is 246.3535131053334
At time: 269.55394768714905 and batch: 450, loss is 5.485033912658691 and perplexity is 241.05711874720603
At time: 271.2395565509796 and batch: 500, loss is 5.483091115951538 and perplexity is 240.58924840631008
At time: 272.9246587753296 and batch: 550, loss is 5.4897800636291505 and perplexity is 242.20393154586975
At time: 274.60943388938904 and batch: 600, loss is 5.457233333587647 and perplexity is 234.4478871583239
At time: 276.2945740222931 and batch: 650, loss is 5.454537744522095 and perplexity is 233.81676300509756
At time: 277.9849925041199 and batch: 700, loss is 5.445068435668945 and perplexity is 231.61312976923597
At time: 279.6732609272003 and batch: 750, loss is 5.451752624511719 and perplexity is 233.16646126424635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.343017223269441 and perplexity of 209.14278975777086
Finished 10 epochs...
Completing Train Step...
At time: 284.21656465530396 and batch: 50, loss is 5.495604496002198 and perplexity is 243.61874821797235
At time: 285.92939162254333 and batch: 100, loss is 5.483024225234986 and perplexity is 240.57315575731977
At time: 287.58591532707214 and batch: 150, loss is 5.486195259094238 and perplexity is 241.33723219572073
At time: 289.24331855773926 and batch: 200, loss is 5.486024608612061 and perplexity is 241.29605139454094
At time: 290.90640807151794 and batch: 250, loss is 5.518601818084717 and perplexity is 249.28624588893993
At time: 292.56737780570984 and batch: 300, loss is 5.533239669799805 and perplexity is 252.9620986505287
At time: 294.2242867946625 and batch: 350, loss is 5.457848272323608 and perplexity is 234.5921025829828
At time: 295.8820195198059 and batch: 400, loss is 5.488797073364258 and perplexity is 241.9659644178882
At time: 297.5747449398041 and batch: 450, loss is 5.465550260543823 and perplexity is 236.4059041665365
At time: 299.2668294906616 and batch: 500, loss is 5.462704133987427 and perplexity is 235.73401963235344
At time: 300.9548649787903 and batch: 550, loss is 5.475252227783203 and perplexity is 238.7106688008668
At time: 302.64213275909424 and batch: 600, loss is 5.437836742401123 and perplexity is 229.94421646580574
At time: 304.33490228652954 and batch: 650, loss is 5.436102199554443 and perplexity is 229.54571407964377
At time: 306.03308963775635 and batch: 700, loss is 5.426880044937134 and perplexity is 227.43853929275855
At time: 307.7311375141144 and batch: 750, loss is 5.435432500839234 and perplexity is 229.3920390735634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3293371865915695 and perplexity of 206.30118963534318
Finished 11 epochs...
Completing Train Step...
At time: 312.21044874191284 and batch: 50, loss is 5.476712188720703 and perplexity is 239.05943158074953
At time: 313.9306674003601 and batch: 100, loss is 5.469664707183838 and perplexity is 237.38058741070626
At time: 315.6172580718994 and batch: 150, loss is 5.466785612106324 and perplexity is 236.69812903269187
At time: 317.2928936481476 and batch: 200, loss is 5.469142093658447 and perplexity is 237.25656151668798
At time: 318.9767265319824 and batch: 250, loss is 5.500697116851807 and perplexity is 244.86257060422375
At time: 320.66440200805664 and batch: 300, loss is 5.520571966171264 and perplexity is 249.77786082737953
At time: 322.34864044189453 and batch: 350, loss is 5.442019414901734 and perplexity is 230.90801203218456
At time: 324.0352430343628 and batch: 400, loss is 5.4738173675537105 and perplexity is 238.36839796997327
At time: 325.72313594818115 and batch: 450, loss is 5.452457456588745 and perplexity is 233.3308623962218
At time: 327.4100959300995 and batch: 500, loss is 5.449389266967773 and perplexity is 232.61605620718305
At time: 329.1275351047516 and batch: 550, loss is 5.458884677886963 and perplexity is 234.83536117872538
At time: 330.8143973350525 and batch: 600, loss is 5.422014017105102 and perplexity is 226.33450533891983
At time: 332.501225233078 and batch: 650, loss is 5.419658546447754 and perplexity is 225.80200843946125
At time: 334.18661093711853 and batch: 700, loss is 5.410969762802124 and perplexity is 223.8485624656921
At time: 335.8724617958069 and batch: 750, loss is 5.42067666053772 and perplexity is 226.03201771376948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.324396532635356 and perplexity of 205.28444061737025
Finished 12 epochs...
Completing Train Step...
At time: 340.3541827201843 and batch: 50, loss is 5.459696092605591 and perplexity is 235.025987375232
At time: 342.07122683525085 and batch: 100, loss is 5.449299335479736 and perplexity is 232.5951376397401
At time: 343.75552320480347 and batch: 150, loss is 5.45247486114502 and perplexity is 233.33492345168725
At time: 345.4397060871124 and batch: 200, loss is 5.451319446563721 and perplexity is 233.06548056789845
At time: 347.12349557876587 and batch: 250, loss is 5.484508104324341 and perplexity is 240.93040222233623
At time: 348.8092770576477 and batch: 300, loss is 5.508470697402954 and perplexity is 246.7734470742425
At time: 350.4963617324829 and batch: 350, loss is 5.42998722076416 and perplexity is 228.1463298696194
At time: 352.1842043399811 and batch: 400, loss is 5.4595307445526124 and perplexity is 234.98712949844585
At time: 353.868275642395 and batch: 450, loss is 5.435498476028442 and perplexity is 229.40717375599556
At time: 355.55075454711914 and batch: 500, loss is 5.4287216758728025 and perplexity is 227.8577830703462
At time: 357.23752784729004 and batch: 550, loss is 5.4407222461700435 and perplexity is 230.6086795634117
At time: 358.923850774765 and batch: 600, loss is 5.404308948516846 and perplexity is 222.36250343893786
At time: 360.60898089408875 and batch: 650, loss is 5.406266069412231 and perplexity is 222.7981198787408
At time: 362.29532957077026 and batch: 700, loss is 5.396949224472046 and perplexity is 220.73198422128476
At time: 363.98086047172546 and batch: 750, loss is 5.405581769943237 and perplexity is 222.64571139608407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.324320948401163 and perplexity of 205.2689249365122
Finished 13 epochs...
Completing Train Step...
At time: 368.4881706237793 and batch: 50, loss is 5.445902538299561 and perplexity is 231.8063994822443
At time: 370.2043948173523 and batch: 100, loss is 5.437304372787476 and perplexity is 229.82183373144042
At time: 371.8905863761902 and batch: 150, loss is 5.4414777755737305 and perplexity is 230.7829770367092
At time: 373.5761299133301 and batch: 200, loss is 5.439095792770385 and perplexity is 230.23391014765664
At time: 375.2600648403168 and batch: 250, loss is 5.471278581619263 and perplexity is 237.7639991782003
At time: 376.9465482234955 and batch: 300, loss is 5.495268478393554 and perplexity is 243.53690178046767
At time: 378.63313126564026 and batch: 350, loss is 5.418884181976319 and perplexity is 225.62722306907153
At time: 380.31884694099426 and batch: 400, loss is 5.448509111404419 and perplexity is 232.4114079655422
At time: 382.0038151741028 and batch: 450, loss is 5.426041212081909 and perplexity is 227.24783636857686
At time: 383.68794798851013 and batch: 500, loss is 5.4196266746521 and perplexity is 225.79481183867486
At time: 385.3728451728821 and batch: 550, loss is 5.427649230957031 and perplexity is 227.61354913650231
At time: 387.0539515018463 and batch: 600, loss is 5.390308227539062 and perplexity is 219.27096048773075
At time: 388.7387864589691 and batch: 650, loss is 5.392877674102783 and perplexity is 219.8350899435577
At time: 390.4260005950928 and batch: 700, loss is 5.386241474151611 and perplexity is 218.38105031548142
At time: 392.1147243976593 and batch: 750, loss is 5.39395489692688 and perplexity is 220.07202891510428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.319490654523983 and perplexity of 204.27980649474617
Finished 14 epochs...
Completing Train Step...
At time: 396.62442326545715 and batch: 50, loss is 5.430228443145752 and perplexity is 228.20137050891202
At time: 398.33900785446167 and batch: 100, loss is 5.431365432739258 and perplexity is 228.4609806514523
At time: 400.02320647239685 and batch: 150, loss is 5.427429885864258 and perplexity is 227.56362869655237
At time: 401.6995005607605 and batch: 200, loss is 5.428006401062012 and perplexity is 227.69486041185644
At time: 403.37743949890137 and batch: 250, loss is 5.461876649856567 and perplexity is 235.53903415682322
At time: 405.06321930885315 and batch: 300, loss is 5.482808542251587 and perplexity is 240.52127381658744
At time: 406.74871706962585 and batch: 350, loss is 5.405729694366455 and perplexity is 222.67864857057052
At time: 408.4290339946747 and batch: 400, loss is 5.432470064163208 and perplexity is 228.71348526641685
At time: 410.1045916080475 and batch: 450, loss is 5.410120916366577 and perplexity is 223.6586300344789
At time: 411.7888090610504 and batch: 500, loss is 5.403460359573364 and perplexity is 222.17388911640742
At time: 413.520943403244 and batch: 550, loss is 5.412538175582886 and perplexity is 224.19992488064258
At time: 415.2094073295593 and batch: 600, loss is 5.3741637134552 and perplexity is 215.75936019136367
At time: 416.8980848789215 and batch: 650, loss is 5.3734061145782475 and perplexity is 215.5959630449532
At time: 418.5862646102905 and batch: 700, loss is 5.370036706924439 and perplexity is 214.87075480413964
At time: 420.2770187854767 and batch: 750, loss is 5.377917108535766 and perplexity is 216.5707120219517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.310770079146984 and perplexity of 202.50611409387872
Finished 15 epochs...
Completing Train Step...
At time: 424.7994484901428 and batch: 50, loss is 5.416759271621704 and perplexity is 225.14829446686863
At time: 426.51664638519287 and batch: 100, loss is 5.410548467636108 and perplexity is 223.75427601101399
At time: 428.2060935497284 and batch: 150, loss is 5.4162046909332275 and perplexity is 225.02346618758983
At time: 429.89364409446716 and batch: 200, loss is 5.4158940029144285 and perplexity is 224.95356495199718
At time: 431.5820610523224 and batch: 250, loss is 5.44959774017334 and perplexity is 232.66455547731042
At time: 433.26930689811707 and batch: 300, loss is 5.471218528747559 and perplexity is 237.74972119598308
At time: 434.9545383453369 and batch: 350, loss is 5.390086812973022 and perplexity is 219.22241607758727
At time: 436.6405670642853 and batch: 400, loss is 5.425801830291748 and perplexity is 227.19344388524152
At time: 438.32786536216736 and batch: 450, loss is 5.399342126846314 and perplexity is 221.26080676843344
At time: 440.01175928115845 and batch: 500, loss is 5.389916801452637 and perplexity is 219.1851489093411
At time: 441.697678565979 and batch: 550, loss is 5.403353071212768 and perplexity is 222.15005372272998
At time: 443.3808147907257 and batch: 600, loss is 5.362112779617309 and perplexity is 213.17486249986618
At time: 445.06596183776855 and batch: 650, loss is 5.362880268096924 and perplexity is 213.33853455115107
At time: 446.7523319721222 and batch: 700, loss is 5.359134397506714 and perplexity is 212.54089087648958
At time: 448.43878746032715 and batch: 750, loss is 5.37166464805603 and perplexity is 215.22083662284209
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.314150433207667 and perplexity of 203.1918147615407
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 452.9310019016266 and batch: 50, loss is 5.393392095565796 and perplexity is 219.94820692455497
At time: 454.6481099128723 and batch: 100, loss is 5.37023546218872 and perplexity is 214.91346574216837
At time: 456.3332359790802 and batch: 150, loss is 5.35349142074585 and perplexity is 211.34490520188882
At time: 458.01826095581055 and batch: 200, loss is 5.33973069190979 and perplexity is 208.45656369000992
At time: 459.6966197490692 and batch: 250, loss is 5.362575092315674 and perplexity is 213.27343873053786
At time: 461.3696141242981 and batch: 300, loss is 5.370719881057739 and perplexity is 215.0175991002255
At time: 463.05116987228394 and batch: 350, loss is 5.292602472305298 and perplexity is 198.8602809403336
At time: 464.7337441444397 and batch: 400, loss is 5.31366286277771 and perplexity is 203.09276858900824
At time: 466.4201579093933 and batch: 450, loss is 5.2775034332275395 and perplexity is 195.88023630848622
At time: 468.1084430217743 and batch: 500, loss is 5.242675657272339 and perplexity is 189.17559489799788
At time: 469.79687666893005 and batch: 550, loss is 5.249285116195678 and perplexity is 190.4300844033465
At time: 471.48259234428406 and batch: 600, loss is 5.212402935028076 and perplexity is 183.53455034464034
At time: 473.16648745536804 and batch: 650, loss is 5.20401294708252 and perplexity is 182.0011393246079
At time: 474.8569395542145 and batch: 700, loss is 5.1986207675933835 and perplexity is 181.02239766096153
At time: 476.54643058776855 and batch: 750, loss is 5.221527795791626 and perplexity is 185.2169416817785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.180415308752726 and perplexity of 177.75661954558686
Finished 17 epochs...
Completing Train Step...
At time: 481.0902738571167 and batch: 50, loss is 5.302033815383911 and perplexity is 200.74467267901287
At time: 482.8109612464905 and batch: 100, loss is 5.294463033676148 and perplexity is 199.23061710699818
At time: 484.4987459182739 and batch: 150, loss is 5.2910551071167 and perplexity is 198.55280941093153
At time: 486.17906284332275 and batch: 200, loss is 5.285750770568848 and perplexity is 197.50240679462996
At time: 487.86456966400146 and batch: 250, loss is 5.313797187805176 and perplexity is 203.12005086303228
At time: 489.53936982154846 and batch: 300, loss is 5.324383716583252 and perplexity is 205.28180969814218
At time: 491.21613812446594 and batch: 350, loss is 5.2515531921386716 and perplexity is 190.86248446933607
At time: 492.8955674171448 and batch: 400, loss is 5.275789384841919 and perplexity is 195.54477568566702
At time: 494.58092737197876 and batch: 450, loss is 5.239560403823853 and perplexity is 188.58718197718204
At time: 496.26727175712585 and batch: 500, loss is 5.218248357772827 and perplexity is 184.61052909123657
At time: 497.9877345561981 and batch: 550, loss is 5.236762866973877 and perplexity is 188.06033966016415
At time: 499.6749939918518 and batch: 600, loss is 5.207541589736938 and perplexity is 182.64449071901993
At time: 501.3643181324005 and batch: 650, loss is 5.209303073883056 and perplexity is 182.96649961732044
At time: 503.05140495300293 and batch: 700, loss is 5.206129903793335 and perplexity is 182.3868359653814
At time: 504.71794056892395 and batch: 750, loss is 5.221175737380982 and perplexity is 185.1517459766871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.173547434252362 and perplexity of 176.5399919957252
Finished 18 epochs...
Completing Train Step...
At time: 509.12045907974243 and batch: 50, loss is 5.28410418510437 and perplexity is 197.17746979408878
At time: 510.835818529129 and batch: 100, loss is 5.274057264328003 and perplexity is 195.20636173979202
At time: 512.523220539093 and batch: 150, loss is 5.2737728786468505 and perplexity is 195.15085573857343
At time: 514.2109260559082 and batch: 200, loss is 5.270543937683105 and perplexity is 194.52174137953017
At time: 515.8986315727234 and batch: 250, loss is 5.300635423660278 and perplexity is 200.46414917676066
At time: 517.5894870758057 and batch: 300, loss is 5.313316984176636 and perplexity is 203.0225352931177
At time: 519.2776870727539 and batch: 350, loss is 5.241159839630127 and perplexity is 188.88905641868163
At time: 520.9640052318573 and batch: 400, loss is 5.266513233184814 and perplexity is 193.7392597570213
At time: 522.6522853374481 and batch: 450, loss is 5.233229694366455 and perplexity is 187.3970624459675
At time: 524.3426876068115 and batch: 500, loss is 5.215480623245239 and perplexity is 184.10028259485955
At time: 526.0335586071014 and batch: 550, loss is 5.236192655563355 and perplexity is 187.95313607588136
At time: 527.7207987308502 and batch: 600, loss is 5.208046245574951 and perplexity is 182.7366865891775
At time: 529.4093825817108 and batch: 650, loss is 5.209721345901489 and perplexity is 183.0430453917827
At time: 531.1010558605194 and batch: 700, loss is 5.204337673187256 and perplexity is 182.0602494424181
At time: 532.7893748283386 and batch: 750, loss is 5.216777448654175 and perplexity is 184.3391833919517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.170302635015443 and perplexity of 175.96808353027663
Finished 19 epochs...
Completing Train Step...
At time: 537.3455102443695 and batch: 50, loss is 5.274647560119629 and perplexity is 195.32162525006117
At time: 539.0635340213776 and batch: 100, loss is 5.26502329826355 and perplexity is 193.45081580300925
At time: 540.750078201294 and batch: 150, loss is 5.264851789474488 and perplexity is 193.41764013288858
At time: 542.4373369216919 and batch: 200, loss is 5.263403234481811 and perplexity is 193.13766687189081
At time: 544.1267740726471 and batch: 250, loss is 5.294072856903076 and perplexity is 199.15289711097302
At time: 545.8148384094238 and batch: 300, loss is 5.307283124923706 and perplexity is 201.80121423510008
At time: 547.5020966529846 and batch: 350, loss is 5.2360790252685545 and perplexity is 187.93178011898527
At time: 549.1870260238647 and batch: 400, loss is 5.261916265487671 and perplexity is 192.85069056496422
At time: 550.872994184494 and batch: 450, loss is 5.230481557846069 and perplexity is 186.88277672220443
At time: 552.5606153011322 and batch: 500, loss is 5.2129576206207275 and perplexity is 183.63638255528937
At time: 554.2476150989532 and batch: 550, loss is 5.234318008422852 and perplexity is 187.60112032254168
At time: 555.9335238933563 and batch: 600, loss is 5.206488885879517 and perplexity is 182.45232132558067
At time: 557.6203875541687 and batch: 650, loss is 5.208270282745361 and perplexity is 182.77763098573317
At time: 559.3077731132507 and batch: 700, loss is 5.201737813949585 and perplexity is 181.5875331852245
At time: 560.9948592185974 and batch: 750, loss is 5.2138736534118655 and perplexity is 183.80467657300716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.168937505677689 and perplexity of 175.7280282274894
Finished 20 epochs...
Completing Train Step...
At time: 565.5079085826874 and batch: 50, loss is 5.267098226547241 and perplexity is 193.85262909494267
At time: 567.2270271778107 and batch: 100, loss is 5.257179050445557 and perplexity is 191.93927586089708
At time: 568.9158630371094 and batch: 150, loss is 5.259307069778442 and perplexity is 192.34816125437825
At time: 570.6029601097107 and batch: 200, loss is 5.257081298828125 and perplexity is 191.92051440322936
At time: 572.2917096614838 and batch: 250, loss is 5.286906824111939 and perplexity is 197.73086217965906
At time: 573.9726762771606 and batch: 300, loss is 5.300502033233642 and perplexity is 200.43741096172738
At time: 575.6525709629059 and batch: 350, loss is 5.23067946434021 and perplexity is 186.91976569741936
At time: 577.337789773941 and batch: 400, loss is 5.256224775314331 and perplexity is 191.7562003493345
At time: 579.0188210010529 and batch: 450, loss is 5.224518795013427 and perplexity is 185.77175471927816
At time: 580.689519405365 and batch: 500, loss is 5.208551931381225 and perplexity is 182.82911730635246
At time: 582.3944311141968 and batch: 550, loss is 5.230441646575928 and perplexity is 186.87531814205963
At time: 584.0805289745331 and batch: 600, loss is 5.2023179149627685 and perplexity is 181.69290285677852
At time: 585.7589197158813 and batch: 650, loss is 5.204700651168824 and perplexity is 182.12634529922906
At time: 587.4347057342529 and batch: 700, loss is 5.1976477241516115 and perplexity is 180.84634067354327
At time: 589.1195793151855 and batch: 750, loss is 5.208583459854126 and perplexity is 182.83488172009405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.165202650912972 and perplexity of 175.07293366699037
Finished 21 epochs...
Completing Train Step...
At time: 593.6421666145325 and batch: 50, loss is 5.260220880508423 and perplexity is 192.52401140266377
At time: 595.3616468906403 and batch: 100, loss is 5.250379648208618 and perplexity is 190.6386303362328
At time: 597.0487415790558 and batch: 150, loss is 5.253295745849609 and perplexity is 191.19536254465777
At time: 598.7354753017426 and batch: 200, loss is 5.250772819519043 and perplexity is 190.71359871308152
At time: 600.4232518672943 and batch: 250, loss is 5.280633668899537 and perplexity is 196.49434826775746
At time: 602.1114773750305 and batch: 300, loss is 5.294204654693604 and perplexity is 199.17914675257376
At time: 603.7971382141113 and batch: 350, loss is 5.224666299819947 and perplexity is 185.79915896709423
At time: 605.4842412471771 and batch: 400, loss is 5.250513906478882 and perplexity is 190.66422686722177
At time: 607.1727569103241 and batch: 450, loss is 5.220278244018555 and perplexity is 184.98564806066912
At time: 608.8627021312714 and batch: 500, loss is 5.203039979934692 and perplexity is 181.82414431430595
At time: 610.5504250526428 and batch: 550, loss is 5.225190382003785 and perplexity is 185.89655851653592
At time: 612.2365081310272 and batch: 600, loss is 5.196293678283691 and perplexity is 180.60163214390067
At time: 613.9241714477539 and batch: 650, loss is 5.198420295715332 and perplexity is 180.98611139824217
At time: 615.612410068512 and batch: 700, loss is 5.191366224288941 and perplexity is 179.71391479945507
At time: 617.3001062870026 and batch: 750, loss is 5.199610958099365 and perplexity is 181.20173309396415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.156373489734738 and perplexity of 173.53399030442515
Finished 22 epochs...
Completing Train Step...
At time: 621.8265490531921 and batch: 50, loss is 5.250018978118897 and perplexity is 190.5698850822488
At time: 623.546088218689 and batch: 100, loss is 5.239671516418457 and perplexity is 188.60813755247327
At time: 625.2331080436707 and batch: 150, loss is 5.242426013946533 and perplexity is 189.12837436771656
At time: 626.9220633506775 and batch: 200, loss is 5.239419746398926 and perplexity is 188.56065765525605
At time: 628.6105115413666 and batch: 250, loss is 5.268319988250733 and perplexity is 194.089615554303
At time: 630.3000347614288 and batch: 300, loss is 5.283570823669433 and perplexity is 197.07233097684983
At time: 631.9863946437836 and batch: 350, loss is 5.212860116958618 and perplexity is 183.61847820837755
At time: 633.6660196781158 and batch: 400, loss is 5.239498138427734 and perplexity is 188.57543988716017
At time: 635.3535709381104 and batch: 450, loss is 5.207598924636841 and perplexity is 182.65496292282154
At time: 637.0473670959473 and batch: 500, loss is 5.191304197311402 and perplexity is 179.702768034202
At time: 638.7402482032776 and batch: 550, loss is 5.214136877059937 and perplexity is 183.85306467867468
At time: 640.4347863197327 and batch: 600, loss is 5.1860819339752195 and perplexity is 178.76675902728434
At time: 642.1253390312195 and batch: 650, loss is 5.188897800445557 and perplexity is 179.2708517458486
At time: 643.8139650821686 and batch: 700, loss is 5.181798267364502 and perplexity is 178.00261965816696
At time: 645.5020604133606 and batch: 750, loss is 5.190573663711548 and perplexity is 179.57153706431893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.151702526003816 and perplexity of 172.72530945938885
Finished 23 epochs...
Completing Train Step...
At time: 650.0136229991913 and batch: 50, loss is 5.240528774261475 and perplexity is 188.76989268066723
At time: 651.7369184494019 and batch: 100, loss is 5.229976739883423 and perplexity is 186.78845874831256
At time: 653.4219751358032 and batch: 150, loss is 5.233898916244507 and perplexity is 187.52251463303108
At time: 655.1058912277222 and batch: 200, loss is 5.2331454467773435 and perplexity is 187.3812753602712
At time: 656.7904381752014 and batch: 250, loss is 5.262764463424682 and perplexity is 193.01433551469594
At time: 658.4748570919037 and batch: 300, loss is 5.278256149291992 and perplexity is 196.02773401404644
At time: 660.1590406894684 and batch: 350, loss is 5.207793674468994 and perplexity is 182.69053841024038
At time: 661.8503551483154 and batch: 400, loss is 5.234397354125977 and perplexity is 187.61600625590034
At time: 663.5412220954895 and batch: 450, loss is 5.203168039321899 and perplexity is 181.84743009375583
At time: 665.2350571155548 and batch: 500, loss is 5.187352991104126 and perplexity is 178.9941262585165
At time: 666.9755966663361 and batch: 550, loss is 5.209938793182373 and perplexity is 183.08285193204307
At time: 668.6642823219299 and batch: 600, loss is 5.181720638275147 and perplexity is 177.98880201323263
At time: 670.351854801178 and batch: 650, loss is 5.184078435897828 and perplexity is 178.4089587150747
At time: 672.0424430370331 and batch: 700, loss is 5.177243795394897 and perplexity is 177.19375509012266
At time: 673.7151956558228 and batch: 750, loss is 5.184406023025513 and perplexity is 178.46741276728832
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1494090945221656 and perplexity of 172.32962970263625
Finished 24 epochs...
Completing Train Step...
At time: 678.1432943344116 and batch: 50, loss is 5.235034046173095 and perplexity is 187.73549791065213
At time: 679.8500900268555 and batch: 100, loss is 5.224876747131348 and perplexity is 185.83826401519127
At time: 681.51060962677 and batch: 150, loss is 5.228801956176758 and perplexity is 186.5691515548633
At time: 683.1705830097198 and batch: 200, loss is 5.227801189422608 and perplexity is 186.3825327471618
At time: 684.8314878940582 and batch: 250, loss is 5.256042518615723 and perplexity is 191.7212546819588
At time: 686.4941339492798 and batch: 300, loss is 5.273273315429687 and perplexity is 195.05338989645034
At time: 688.1538908481598 and batch: 350, loss is 5.203393354415893 and perplexity is 181.88840768082105
At time: 689.8133747577667 and batch: 400, loss is 5.229835834503174 and perplexity is 186.7621411036989
At time: 691.5073735713959 and batch: 450, loss is 5.198310298919678 and perplexity is 180.96620460079248
At time: 693.1982889175415 and batch: 500, loss is 5.182800168991089 and perplexity is 178.1810501423096
At time: 694.8876068592072 and batch: 550, loss is 5.205254573822021 and perplexity is 182.22725715373522
At time: 696.5797543525696 and batch: 600, loss is 5.176773405075073 and perplexity is 177.1104244634915
At time: 698.2703788280487 and batch: 650, loss is 5.178720989227295 and perplexity is 177.45569803490025
At time: 699.9612321853638 and batch: 700, loss is 5.172567911148072 and perplexity is 176.36715165910516
At time: 701.6505987644196 and batch: 750, loss is 5.178000993728638 and perplexity is 177.32797671600892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.146300559820131 and perplexity of 171.7947688160263
Finished 25 epochs...
Completing Train Step...
At time: 706.1938583850861 and batch: 50, loss is 5.229873714447021 and perplexity is 186.76921577711
At time: 707.9156606197357 and batch: 100, loss is 5.219773263931274 and perplexity is 184.89225757411833
At time: 709.6019871234894 and batch: 150, loss is 5.224636526107788 and perplexity is 185.79362711876803
At time: 711.2905323505402 and batch: 200, loss is 5.223340864181519 and perplexity is 185.5530572722167
At time: 712.9800009727478 and batch: 250, loss is 5.251274747848511 and perplexity is 190.80934729854388
At time: 714.671023607254 and batch: 300, loss is 5.269692287445069 and perplexity is 194.356147416259
At time: 716.3589391708374 and batch: 350, loss is 5.199254322052002 and perplexity is 181.13712154618648
At time: 718.0465769767761 and batch: 400, loss is 5.225534191131592 and perplexity is 185.9604824383661
At time: 719.7349820137024 and batch: 450, loss is 5.194625797271729 and perplexity is 180.30066117257334
At time: 721.4291381835938 and batch: 500, loss is 5.179502763748169 and perplexity is 177.59448262026578
At time: 723.1226167678833 and batch: 550, loss is 5.202463331222535 and perplexity is 181.71932588026004
At time: 724.8140096664429 and batch: 600, loss is 5.1740553760528565 and perplexity is 176.6296868150643
At time: 726.5030069351196 and batch: 650, loss is 5.175297107696533 and perplexity is 176.84914971521
At time: 728.1925566196442 and batch: 700, loss is 5.167790060043335 and perplexity is 175.52650550895888
At time: 729.88059258461 and batch: 750, loss is 5.174462757110596 and perplexity is 176.70165706236955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1457707604696585 and perplexity of 171.7037761651451
Finished 26 epochs...
Completing Train Step...
At time: 734.421728849411 and batch: 50, loss is 5.2259000873565675 and perplexity is 186.02853712660243
At time: 736.1440925598145 and batch: 100, loss is 5.214382677078247 and perplexity is 183.89826131977924
At time: 737.8309600353241 and batch: 150, loss is 5.219120693206787 and perplexity is 184.77164165912905
At time: 739.5170953273773 and batch: 200, loss is 5.2182847595214845 and perplexity is 184.61724935963002
At time: 741.204460144043 and batch: 250, loss is 5.244006118774414 and perplexity is 189.4274532509113
At time: 742.884416103363 and batch: 300, loss is 5.263577680587769 and perplexity is 193.17136192469033
At time: 744.545168876648 and batch: 350, loss is 5.192510385513305 and perplexity is 179.91965416930748
At time: 746.2333345413208 and batch: 400, loss is 5.217880420684814 and perplexity is 184.5426165252882
At time: 747.9218380451202 and batch: 450, loss is 5.188779392242432 and perplexity is 179.24962586310477
At time: 749.6001365184784 and batch: 500, loss is 5.174756946563721 and perplexity is 176.75364847351452
At time: 751.3008010387421 and batch: 550, loss is 5.197509384155273 and perplexity is 180.8213241218714
At time: 752.9823708534241 and batch: 600, loss is 5.168778142929077 and perplexity is 175.7000259572216
At time: 754.6690566539764 and batch: 650, loss is 5.170502452850342 and perplexity is 176.00324860492645
At time: 756.3459689617157 and batch: 700, loss is 5.16479923248291 and perplexity is 175.0023202632759
At time: 758.0286808013916 and batch: 750, loss is 5.1692098903656 and perplexity is 175.77590037113893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.142679613690044 and perplexity of 171.1738340767691
Finished 27 epochs...
Completing Train Step...
At time: 762.5528628826141 and batch: 50, loss is 5.220401725769043 and perplexity is 185.00849182267163
At time: 764.2738273143768 and batch: 100, loss is 5.20889181137085 and perplexity is 182.89126782610631
At time: 765.9610986709595 and batch: 150, loss is 5.213194389343261 and perplexity is 183.67986705467925
At time: 767.6507277488708 and batch: 200, loss is 5.212908306121826 and perplexity is 183.6273268423944
At time: 769.3386499881744 and batch: 250, loss is 5.23821044921875 and perplexity is 188.33276960362252
At time: 771.0318734645844 and batch: 300, loss is 5.258268890380859 and perplexity is 192.14857297834564
At time: 772.7221777439117 and batch: 350, loss is 5.187213010787964 and perplexity is 178.9690723576992
At time: 774.4111711978912 and batch: 400, loss is 5.212305307388306 and perplexity is 183.5166331742936
At time: 776.1012074947357 and batch: 450, loss is 5.1832544708251955 and perplexity is 178.26201651038448
At time: 777.790689945221 and batch: 500, loss is 5.16946852684021 and perplexity is 175.8213683099125
At time: 779.4780471324921 and batch: 550, loss is 5.1911502456665035 and perplexity is 179.67510462693855
At time: 781.1650214195251 and batch: 600, loss is 5.162987728118896 and perplexity is 174.6855897623626
At time: 782.8551332950592 and batch: 650, loss is 5.16393159866333 and perplexity is 174.85054818252667
At time: 784.5504622459412 and batch: 700, loss is 5.157688417434692 and perplexity is 173.7623250440971
At time: 786.2402429580688 and batch: 750, loss is 5.161085023880005 and perplexity is 174.35353075546746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.137984519781068 and perplexity of 170.37204057180787
Finished 28 epochs...
Completing Train Step...
At time: 790.7719120979309 and batch: 50, loss is 5.212985696792603 and perplexity is 183.64153843430688
At time: 792.490927696228 and batch: 100, loss is 5.200948553085327 and perplexity is 181.44426979537474
At time: 794.1785802841187 and batch: 150, loss is 5.2044942092895505 and perplexity is 182.0887506749263
At time: 795.8659689426422 and batch: 200, loss is 5.2056443309783935 and perplexity is 182.29829537422515
At time: 797.5550918579102 and batch: 250, loss is 5.230172576904297 and perplexity is 186.82504242570985
At time: 799.2462072372437 and batch: 300, loss is 5.250118312835693 and perplexity is 190.58881622805774
At time: 800.9344222545624 and batch: 350, loss is 5.179773235321045 and perplexity is 177.64252337585071
At time: 802.6241524219513 and batch: 400, loss is 5.204737367630005 and perplexity is 182.13303245688002
At time: 804.3144981861115 and batch: 450, loss is 5.175199060440064 and perplexity is 176.83181099129254
At time: 806.0095443725586 and batch: 500, loss is 5.161655435562134 and perplexity is 174.4530124162842
At time: 807.6986420154572 and batch: 550, loss is 5.184092397689819 and perplexity is 178.4114496412346
At time: 809.3873519897461 and batch: 600, loss is 5.154282932281494 and perplexity is 173.17158647260723
At time: 811.0783808231354 and batch: 650, loss is 5.154221916198731 and perplexity is 173.16102054310386
At time: 812.7689502239227 and batch: 700, loss is 5.147303457260132 and perplexity is 171.96714777463737
At time: 814.4589285850525 and batch: 750, loss is 5.156054553985595 and perplexity is 173.47865293622718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.132379221361737 and perplexity of 169.41972593937032
Finished 29 epochs...
Completing Train Step...
At time: 819.0178537368774 and batch: 50, loss is 5.203896570205688 and perplexity is 181.979959832859
At time: 820.7373659610748 and batch: 100, loss is 5.192117490768433 and perplexity is 179.84897856763047
At time: 822.4265463352203 and batch: 150, loss is 5.19625786781311 and perplexity is 180.5951648302654
At time: 824.1165218353271 and batch: 200, loss is 5.196220626831055 and perplexity is 180.58843941420398
At time: 825.8060455322266 and batch: 250, loss is 5.220068292617798 and perplexity is 184.94681414149935
At time: 827.4985451698303 and batch: 300, loss is 5.2399779796600345 and perplexity is 188.66594787161196
At time: 829.1848466396332 and batch: 350, loss is 5.1695664119720455 and perplexity is 175.83857945007264
At time: 830.8694462776184 and batch: 400, loss is 5.194663610458374 and perplexity is 180.30747904402838
At time: 832.5436091423035 and batch: 450, loss is 5.162201461791992 and perplexity is 174.5482943477909
At time: 834.2087972164154 and batch: 500, loss is 5.1506396484375 and perplexity is 172.5418211329165
At time: 835.9161894321442 and batch: 550, loss is 5.173611764907837 and perplexity is 176.55134929443508
At time: 837.580029964447 and batch: 600, loss is 5.143747730255127 and perplexity is 171.35676536303205
At time: 839.2571785449982 and batch: 650, loss is 5.1418438816070555 and perplexity is 171.03083837323453
At time: 840.9410662651062 and batch: 700, loss is 5.137051458358765 and perplexity is 170.2131471335077
At time: 842.625515460968 and batch: 750, loss is 5.142529182434082 and perplexity is 171.1480861186185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.122926933820858 and perplexity of 167.8258666498735
Finished 30 epochs...
Completing Train Step...
At time: 847.1489546298981 and batch: 50, loss is 5.192431325912476 and perplexity is 179.9054303555394
At time: 848.8555288314819 and batch: 100, loss is 5.177573509216309 and perplexity is 177.25218795277556
At time: 850.5352046489716 and batch: 150, loss is 5.181028022766113 and perplexity is 177.86556689072896
At time: 852.2218446731567 and batch: 200, loss is 5.181612329483032 and perplexity is 177.969525305014
At time: 853.9094908237457 and batch: 250, loss is 5.2057962894439695 and perplexity is 182.32599924833312
At time: 855.5924813747406 and batch: 300, loss is 5.229739513397217 and perplexity is 186.74415283405582
At time: 857.2631771564484 and batch: 350, loss is 5.156652841567993 and perplexity is 173.58247411445404
At time: 858.9459872245789 and batch: 400, loss is 5.177778787612915 and perplexity is 177.28857773260353
At time: 860.6268165111542 and batch: 450, loss is 5.146894178390503 and perplexity is 171.89677965585716
At time: 862.3030498027802 and batch: 500, loss is 5.139618463516236 and perplexity is 170.65064645132904
At time: 863.9754190444946 and batch: 550, loss is 5.160768823623657 and perplexity is 174.29840883958877
At time: 865.6582727432251 and batch: 600, loss is 5.131225814819336 and perplexity is 169.224428769228
At time: 867.3383209705353 and batch: 650, loss is 5.125337219238281 and perplexity is 168.23086277098062
At time: 869.0117907524109 and batch: 700, loss is 5.11713791847229 and perplexity is 166.85712686212293
At time: 870.688544511795 and batch: 750, loss is 5.121584119796753 and perplexity is 167.6006589623894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.11598382994186 and perplexity of 166.66467004325594
Finished 31 epochs...
Completing Train Step...
At time: 875.178982257843 and batch: 50, loss is 5.180341892242431 and perplexity is 177.74356975393908
At time: 876.8816363811493 and batch: 100, loss is 5.165904569625854 and perplexity is 175.1958637736743
At time: 878.5642111301422 and batch: 150, loss is 5.169792528152466 and perplexity is 175.87834389355106
At time: 880.2407717704773 and batch: 200, loss is 5.169312744140625 and perplexity is 175.79398051584016
At time: 881.9197402000427 and batch: 250, loss is 5.194115629196167 and perplexity is 180.20870099080946
At time: 883.6066513061523 and batch: 300, loss is 5.218745288848877 and perplexity is 184.7022905977917
At time: 885.2846405506134 and batch: 350, loss is 5.146780462265014 and perplexity is 171.87723333147795
At time: 886.9586083889008 and batch: 400, loss is 5.166791400909424 and perplexity is 175.35130185991227
At time: 888.6472642421722 and batch: 450, loss is 5.135590219497681 and perplexity is 169.9646067009422
At time: 890.3321390151978 and batch: 500, loss is 5.12880295753479 and perplexity is 168.81491842215206
At time: 892.0067098140717 and batch: 550, loss is 5.149559364318848 and perplexity is 172.35552758684017
At time: 893.6803123950958 and batch: 600, loss is 5.118564920425415 and perplexity is 167.09540227735548
At time: 895.3616480827332 and batch: 650, loss is 5.1109014415740965 and perplexity is 165.81976435104792
At time: 897.0437521934509 and batch: 700, loss is 5.107637147903443 and perplexity is 165.27936243893754
At time: 898.7175238132477 and batch: 750, loss is 5.110169887542725 and perplexity is 165.69850259412732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.111213506654251 and perplexity of 165.8715189840882
Finished 32 epochs...
Completing Train Step...
At time: 903.158285856247 and batch: 50, loss is 5.171239700317383 and perplexity is 176.13305439776786
At time: 904.8642547130585 and batch: 100, loss is 5.157512378692627 and perplexity is 173.73173883523586
At time: 906.5400021076202 and batch: 150, loss is 5.161618251800537 and perplexity is 174.44652571766133
At time: 908.2138671875 and batch: 200, loss is 5.161064453125 and perplexity is 174.34994420859127
At time: 909.8792214393616 and batch: 250, loss is 5.185791177749634 and perplexity is 178.7147890348349
At time: 911.5490951538086 and batch: 300, loss is 5.210625801086426 and perplexity is 183.20867451402225
At time: 913.2305195331573 and batch: 350, loss is 5.138659896850586 and perplexity is 170.4871448063021
At time: 914.9128382205963 and batch: 400, loss is 5.1573232078552245 and perplexity is 173.69887696506873
At time: 916.5883204936981 and batch: 450, loss is 5.126262445449829 and perplexity is 168.38658640349206
At time: 918.2632641792297 and batch: 500, loss is 5.121423206329346 and perplexity is 167.57369192895356
At time: 919.9902899265289 and batch: 550, loss is 5.1431104183197025 and perplexity is 171.24759244354797
At time: 921.6747453212738 and batch: 600, loss is 5.110831279754638 and perplexity is 165.80813054280827
At time: 923.3632164001465 and batch: 650, loss is 5.101972389221191 and perplexity is 164.34574160055644
At time: 925.0524611473083 and batch: 700, loss is 5.096155862808228 and perplexity is 163.39259494227045
At time: 926.7402312755585 and batch: 750, loss is 5.10215425491333 and perplexity is 164.37563317064584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.10874761537064 and perplexity of 165.46300173778755
Finished 33 epochs...
Completing Train Step...
At time: 931.2309775352478 and batch: 50, loss is 5.162825965881348 and perplexity is 174.6573345158728
At time: 932.9351086616516 and batch: 100, loss is 5.149369850158691 and perplexity is 172.3228668687132
At time: 934.6088018417358 and batch: 150, loss is 5.153711214065551 and perplexity is 173.07260941832558
At time: 936.3006207942963 and batch: 200, loss is 5.153835124969483 and perplexity is 173.09405633053004
At time: 937.990204334259 and batch: 250, loss is 5.1762834072113035 and perplexity is 177.02366199230187
At time: 939.6804518699646 and batch: 300, loss is 5.201670598983765 and perplexity is 181.5753281955717
At time: 941.3691740036011 and batch: 350, loss is 5.130681781768799 and perplexity is 169.13239012532148
At time: 943.0532438755035 and batch: 400, loss is 5.14941102027893 and perplexity is 172.32996156790594
At time: 944.7392637729645 and batch: 450, loss is 5.117447862625122 and perplexity is 166.90885126837676
At time: 946.4257998466492 and batch: 500, loss is 5.113151178359986 and perplexity is 166.19323512268153
At time: 948.1118652820587 and batch: 550, loss is 5.135563259124756 and perplexity is 169.96002445353128
At time: 949.795844078064 and batch: 600, loss is 5.103658180236817 and perplexity is 164.62302783285446
At time: 951.4888970851898 and batch: 650, loss is 5.095499086380005 and perplexity is 163.28531776978141
At time: 953.178172826767 and batch: 700, loss is 5.088117475509644 and perplexity is 162.08444672730027
At time: 954.8686671257019 and batch: 750, loss is 5.094850187301636 and perplexity is 163.17939644741034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.105160735374273 and perplexity of 164.87056893457319
Finished 34 epochs...
Completing Train Step...
At time: 959.3590593338013 and batch: 50, loss is 5.157753524780273 and perplexity is 173.773638616137
At time: 961.0776476860046 and batch: 100, loss is 5.143603639602661 and perplexity is 171.33207623368557
At time: 962.7499015331268 and batch: 150, loss is 5.1472976016998295 and perplexity is 171.96614081358172
At time: 964.4343709945679 and batch: 200, loss is 5.146628427505493 and perplexity is 171.85110400397454
At time: 966.1219506263733 and batch: 250, loss is 5.168317165374756 and perplexity is 175.61905085424806
At time: 967.810378074646 and batch: 300, loss is 5.194553232192993 and perplexity is 180.28757811559146
At time: 969.4980115890503 and batch: 350, loss is 5.122137584686279 and perplexity is 167.69344571732347
At time: 971.1724853515625 and batch: 400, loss is 5.141922492980957 and perplexity is 171.0442838708961
At time: 972.8508157730103 and batch: 450, loss is 5.110701570510864 and perplexity is 165.78662509034208
At time: 974.5229403972626 and batch: 500, loss is 5.106715393066406 and perplexity is 165.12708557889493
At time: 976.1968069076538 and batch: 550, loss is 5.131155643463135 and perplexity is 169.21255447818132
At time: 977.8780829906464 and batch: 600, loss is 5.09907301902771 and perplexity is 163.86993256252347
At time: 979.5578682422638 and batch: 650, loss is 5.088898916244506 and perplexity is 162.21115561775702
At time: 981.2328400611877 and batch: 700, loss is 5.081420783996582 and perplexity is 161.0026434826528
At time: 982.9169635772705 and batch: 750, loss is 5.090605144500732 and perplexity is 162.48816112505705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.102320116619731 and perplexity of 164.40289905477485
Finished 35 epochs...
Completing Train Step...
At time: 987.4569511413574 and batch: 50, loss is 5.150537033081054 and perplexity is 172.5241166008331
At time: 989.1847832202911 and batch: 100, loss is 5.136720457077026 and perplexity is 170.1568156870426
At time: 990.8603029251099 and batch: 150, loss is 5.141117792129517 and perplexity is 170.90669975448344
At time: 992.5404922962189 and batch: 200, loss is 5.141160707473755 and perplexity is 170.91403443172
At time: 994.2160928249359 and batch: 250, loss is 5.162056760787964 and perplexity is 174.52303886163858
At time: 995.8889739513397 and batch: 300, loss is 5.18624363899231 and perplexity is 178.79566884647642
At time: 997.5656278133392 and batch: 350, loss is 5.116938419342041 and perplexity is 166.8238423306661
At time: 999.2332048416138 and batch: 400, loss is 5.137098426818848 and perplexity is 170.2211419706656
At time: 1000.9070279598236 and batch: 450, loss is 5.105118188858032 and perplexity is 164.8635544154571
At time: 1002.5725548267365 and batch: 500, loss is 5.100428123474121 and perplexity is 164.09214396259608
At time: 1004.2848792076111 and batch: 550, loss is 5.12546859741211 and perplexity is 168.25296608642793
At time: 1005.9650013446808 and batch: 600, loss is 5.092422256469726 and perplexity is 162.78368872951407
At time: 1007.6505417823792 and batch: 650, loss is 5.082383537292481 and perplexity is 161.15772394845033
At time: 1009.3340213298798 and batch: 700, loss is 5.07591986656189 and perplexity is 160.11941275097206
At time: 1011.0200252532959 and batch: 750, loss is 5.083844985961914 and perplexity is 161.39341987648328
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.102646583734557 and perplexity of 164.45657995694603
Annealing...
Finished 36 epochs...
Completing Train Step...
At time: 1015.4667694568634 and batch: 50, loss is 5.14597222328186 and perplexity is 171.73837157548428
At time: 1017.2046904563904 and batch: 100, loss is 5.129219179153442 and perplexity is 168.88519746556432
At time: 1018.889999628067 and batch: 150, loss is 5.126809797286987 and perplexity is 168.47877833932392
At time: 1020.5774757862091 and batch: 200, loss is 5.125224494934082 and perplexity is 168.2119001328252
At time: 1022.2646334171295 and batch: 250, loss is 5.137685298919678 and perplexity is 170.32106932928096
At time: 1023.9531667232513 and batch: 300, loss is 5.156994495391846 and perplexity is 173.64178936255271
At time: 1025.6389780044556 and batch: 350, loss is 5.088716058731079 and perplexity is 162.181496800942
At time: 1027.3245573043823 and batch: 400, loss is 5.103187341690063 and perplexity is 164.54553521035592
At time: 1029.0100235939026 and batch: 450, loss is 5.061948251724243 and perplexity is 157.8978415948042
At time: 1030.6980578899384 and batch: 500, loss is 5.048847303390503 and perplexity is 155.84272153502368
At time: 1032.3842754364014 and batch: 550, loss is 5.067412166595459 and perplexity is 158.7629432282129
At time: 1034.069341659546 and batch: 600, loss is 5.036334161758423 and perplexity is 153.9047895831291
At time: 1035.7561013698578 and batch: 650, loss is 5.018243646621704 and perplexity is 151.14560543752657
At time: 1037.444179058075 and batch: 700, loss is 5.012529830932618 and perplexity is 150.28444989077175
At time: 1039.1303491592407 and batch: 750, loss is 5.034339160919189 and perplexity is 153.59805546789156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.06643250931141 and perplexity of 158.60748611422233
Finished 37 epochs...
Completing Train Step...
At time: 1043.6199374198914 and batch: 50, loss is 5.1204282093048095 and perplexity is 167.40703952716476
At time: 1045.3329451084137 and batch: 100, loss is 5.10374306678772 and perplexity is 164.63700270701744
At time: 1047.0171329975128 and batch: 150, loss is 5.104997940063477 and perplexity is 164.8437309636695
At time: 1048.7010729312897 and batch: 200, loss is 5.106084127426147 and perplexity is 165.02287941783777
At time: 1050.386041879654 and batch: 250, loss is 5.1202734851837155 and perplexity is 167.38113962383053
At time: 1052.0723361968994 and batch: 300, loss is 5.142909698486328 and perplexity is 171.21322310474628
At time: 1053.756576538086 and batch: 350, loss is 5.078101367950439 and perplexity is 160.4690947494281
At time: 1055.440616607666 and batch: 400, loss is 5.094300603866577 and perplexity is 163.08974039317772
At time: 1057.1253769397736 and batch: 450, loss is 5.055704841613769 and perplexity is 156.9150916619139
At time: 1058.811348438263 and batch: 500, loss is 5.046168556213379 and perplexity is 155.42581692495813
At time: 1060.4960424900055 and batch: 550, loss is 5.06723913192749 and perplexity is 158.73547411166888
At time: 1062.1792385578156 and batch: 600, loss is 5.037015733718872 and perplexity is 154.00972252790206
At time: 1063.864619255066 and batch: 650, loss is 5.021978063583374 and perplexity is 151.7111013918018
At time: 1065.5509476661682 and batch: 700, loss is 5.016390781402588 and perplexity is 150.86581229173012
At time: 1067.2373151779175 and batch: 750, loss is 5.0359398555755615 and perplexity is 153.84411593580776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.063860871071039 and perplexity of 158.2001290497229
Finished 38 epochs...
Completing Train Step...
At time: 1071.757129907608 and batch: 50, loss is 5.112871713638306 and perplexity is 166.14679646576832
At time: 1073.4738018512726 and batch: 100, loss is 5.095136241912842 and perplexity is 163.2260813431145
At time: 1075.1578335762024 and batch: 150, loss is 5.097067632675171 and perplexity is 163.54163932358293
At time: 1076.8314201831818 and batch: 200, loss is 5.099155588150024 and perplexity is 163.88346371764902
At time: 1078.5040292739868 and batch: 250, loss is 5.113635492324829 and perplexity is 166.27374432160497
At time: 1080.1819207668304 and batch: 300, loss is 5.1375533294677735 and perplexity is 170.29859363419902
At time: 1081.8593564033508 and batch: 350, loss is 5.074530506134034 and perplexity is 159.89710364466254
At time: 1083.5259366035461 and batch: 400, loss is 5.091431570053101 and perplexity is 162.62250099667352
At time: 1085.190459728241 and batch: 450, loss is 5.054021596908569 and perplexity is 156.65118733479895
At time: 1086.874153137207 and batch: 500, loss is 5.045635318756103 and perplexity is 155.34296015067713
At time: 1088.582456111908 and batch: 550, loss is 5.067031841278077 and perplexity is 158.70257314230477
At time: 1090.2593870162964 and batch: 600, loss is 5.036754083633423 and perplexity is 153.96943114219482
At time: 1091.9304428100586 and batch: 650, loss is 5.022857007980346 and perplexity is 151.8445056331955
At time: 1093.6036162376404 and batch: 700, loss is 5.0171450328826905 and perplexity is 150.9796459781646
At time: 1095.280833721161 and batch: 750, loss is 5.035593938827515 and perplexity is 153.79090788282602
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062281054119731 and perplexity of 157.95039911987936
Finished 39 epochs...
Completing Train Step...
At time: 1099.7143740653992 and batch: 50, loss is 5.108048286437988 and perplexity is 165.34732912470568
At time: 1101.4170064926147 and batch: 100, loss is 5.090072479248047 and perplexity is 162.40163237512812
At time: 1103.0946829319 and batch: 150, loss is 5.092438697814941 and perplexity is 162.78636513433764
At time: 1104.7723553180695 and batch: 200, loss is 5.094645643234253 and perplexity is 163.14602248328754
At time: 1106.454833984375 and batch: 250, loss is 5.109425096511841 and perplexity is 165.57513778176994
At time: 1108.1367619037628 and batch: 300, loss is 5.1343645572662355 and perplexity is 169.75641511417172
At time: 1109.8182580471039 and batch: 350, loss is 5.072491683959961 and perplexity is 159.57143398825212
At time: 1111.4935970306396 and batch: 400, loss is 5.08965690612793 and perplexity is 162.33415664357113
At time: 1113.1723136901855 and batch: 450, loss is 5.052920351028442 and perplexity is 156.4787708140458
At time: 1114.8471200466156 and batch: 500, loss is 5.044489650726319 and perplexity is 155.1650905967664
At time: 1116.5256731510162 and batch: 550, loss is 5.066094999313354 and perplexity is 158.5539635346033
At time: 1118.197493314743 and batch: 600, loss is 5.036277570724487 and perplexity is 153.89608019839784
At time: 1119.8757071495056 and batch: 650, loss is 5.022524728775024 and perplexity is 151.79405924313852
At time: 1121.5565900802612 and batch: 700, loss is 5.016718978881836 and perplexity is 150.91533419706667
At time: 1123.2302582263947 and batch: 750, loss is 5.0345782279968265 and perplexity is 153.63478009579285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.060928699582122 and perplexity of 157.73693855062953
Finished 40 epochs...
Completing Train Step...
At time: 1127.6882100105286 and batch: 50, loss is 5.104013795852661 and perplexity is 164.68158076283046
At time: 1129.3954949378967 and batch: 100, loss is 5.085918989181518 and perplexity is 161.72849770506662
At time: 1131.0789742469788 and batch: 150, loss is 5.088647003173828 and perplexity is 162.17029765399073
At time: 1132.7531089782715 and batch: 200, loss is 5.091409597396851 and perplexity is 162.6189277876172
At time: 1134.4270734786987 and batch: 250, loss is 5.106352844238281 and perplexity is 165.06722979850375
At time: 1136.0972859859467 and batch: 300, loss is 5.131849937438965 and perplexity is 169.33007852878947
At time: 1137.7669775485992 and batch: 350, loss is 5.070334682464599 and perplexity is 159.2276091152133
At time: 1139.437302350998 and batch: 400, loss is 5.087624216079712 and perplexity is 162.00451676023243
At time: 1141.1127018928528 and batch: 450, loss is 5.051665544509888 and perplexity is 156.282543371897
At time: 1142.797859430313 and batch: 500, loss is 5.043377771377563 and perplexity is 154.99266161476035
At time: 1144.48317360878 and batch: 550, loss is 5.064964294433594 and perplexity is 158.37478711124712
At time: 1146.1690883636475 and batch: 600, loss is 5.035757637023925 and perplexity is 153.81608523775836
At time: 1147.8572068214417 and batch: 650, loss is 5.021936769485474 and perplexity is 151.70483674807568
At time: 1149.5430707931519 and batch: 700, loss is 5.015568475723267 and perplexity is 150.74180547021166
At time: 1151.230316400528 and batch: 750, loss is 5.0331806945800786 and perplexity is 153.42022031878477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.060042625249818 and perplexity of 157.5972338016603
Finished 41 epochs...
Completing Train Step...
At time: 1155.7335240840912 and batch: 50, loss is 5.100676774978638 and perplexity is 164.1329507942016
At time: 1157.4522213935852 and batch: 100, loss is 5.082522592544556 and perplexity is 161.18013533455215
At time: 1159.1138586997986 and batch: 150, loss is 5.085046911239624 and perplexity is 161.5875193306424
At time: 1160.7747836112976 and batch: 200, loss is 5.088854608535766 and perplexity is 162.20396857234144
At time: 1162.444328069687 and batch: 250, loss is 5.1032083797454835 and perplexity is 164.54899696485901
At time: 1164.117134809494 and batch: 300, loss is 5.1291654014587404 and perplexity is 168.8761154531824
At time: 1165.7916283607483 and batch: 350, loss is 5.068182334899903 and perplexity is 158.8852645129493
At time: 1167.4683649539948 and batch: 400, loss is 5.085545492172241 and perplexity is 161.6681038740186
At time: 1169.1524336338043 and batch: 450, loss is 5.050115156173706 and perplexity is 156.04043247095973
At time: 1170.8286354541779 and batch: 500, loss is 5.042010850906372 and perplexity is 154.780943706452
At time: 1172.5653495788574 and batch: 550, loss is 5.063507814407348 and perplexity is 158.1442852985494
At time: 1174.2473261356354 and batch: 600, loss is 5.034552221298218 and perplexity is 153.63078461432593
At time: 1175.929586648941 and batch: 650, loss is 5.020926942825318 and perplexity is 151.55171848391734
At time: 1177.6185777187347 and batch: 700, loss is 5.014515151977539 and perplexity is 150.58310914101466
At time: 1179.3004789352417 and batch: 750, loss is 5.0315900230407715 and perplexity is 153.1763731325749
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.059476275776708 and perplexity of 157.5080039613547
Finished 42 epochs...
Completing Train Step...
At time: 1183.8139715194702 and batch: 50, loss is 5.097541704177856 and perplexity is 163.619188134677
At time: 1185.5555527210236 and batch: 100, loss is 5.079280891418457 and perplexity is 160.65848348484798
At time: 1187.2387719154358 and batch: 150, loss is 5.081922245025635 and perplexity is 161.08340028043503
At time: 1188.929292678833 and batch: 200, loss is 5.086463603973389 and perplexity is 161.816601426301
At time: 1190.6171264648438 and batch: 250, loss is 5.1002809429168705 and perplexity is 164.06799456661167
At time: 1192.3009390830994 and batch: 300, loss is 5.127021579742432 and perplexity is 168.5144629672466
At time: 1193.9800863265991 and batch: 350, loss is 5.066875066757202 and perplexity is 158.67769457265268
At time: 1195.6603770256042 and batch: 400, loss is 5.08399169921875 and perplexity is 161.41710016780908
At time: 1197.3416135311127 and batch: 450, loss is 5.048921403884887 and perplexity is 155.85426998560325
At time: 1199.0292727947235 and batch: 500, loss is 5.040877494812012 and perplexity is 154.60562115082166
At time: 1200.716554403305 and batch: 550, loss is 5.062376680374146 and perplexity is 157.96550404713275
At time: 1202.4018397331238 and batch: 600, loss is 5.033522291183472 and perplexity is 153.4726370971245
At time: 1204.0905809402466 and batch: 650, loss is 5.019132375717163 and perplexity is 151.27999264279404
At time: 1205.7794086933136 and batch: 700, loss is 5.0127866744995115 and perplexity is 150.32305444238352
At time: 1207.466724395752 and batch: 750, loss is 5.029964418411255 and perplexity is 152.92757119286063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.058692399845567 and perplexity of 157.3845856067458
Finished 43 epochs...
Completing Train Step...
At time: 1211.995371580124 and batch: 50, loss is 5.094822463989257 and perplexity is 163.17487263673675
At time: 1213.7298457622528 and batch: 100, loss is 5.076684484481811 and perplexity is 160.24188974137255
At time: 1215.4183871746063 and batch: 150, loss is 5.079407196044922 and perplexity is 160.678776676128
At time: 1217.1060271263123 and batch: 200, loss is 5.084441642761231 and perplexity is 161.48974509150722
At time: 1218.7992701530457 and batch: 250, loss is 5.098010120391845 and perplexity is 163.69584796827183
At time: 1220.4894349575043 and batch: 300, loss is 5.125174674987793 and perplexity is 168.20352003374515
At time: 1222.1806836128235 and batch: 350, loss is 5.065613145828247 and perplexity is 158.4775821584906
At time: 1223.8660488128662 and batch: 400, loss is 5.082570018768311 and perplexity is 161.18777968098524
At time: 1225.5528931617737 and batch: 450, loss is 5.047718267440796 and perplexity is 155.6668687906259
At time: 1227.2408044338226 and batch: 500, loss is 5.039520959854126 and perplexity is 154.39603540837786
At time: 1228.9265580177307 and batch: 550, loss is 5.061217641830444 and perplexity is 157.7825220014731
At time: 1230.6165306568146 and batch: 600, loss is 5.032208185195923 and perplexity is 153.27109024175888
At time: 1232.3040673732758 and batch: 650, loss is 5.017421169281006 and perplexity is 151.02134271055172
At time: 1233.9917726516724 and batch: 700, loss is 5.011094388961792 and perplexity is 150.06888003985688
At time: 1235.679214477539 and batch: 750, loss is 5.028183040618896 and perplexity is 152.6553919127366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.057810583779978 and perplexity of 157.24586252379254
Finished 44 epochs...
Completing Train Step...
At time: 1240.1811046600342 and batch: 50, loss is 5.092357311248779 and perplexity is 162.77311705017672
At time: 1241.9014775753021 and batch: 100, loss is 5.074061660766602 and perplexity is 159.82215419957598
At time: 1243.584776878357 and batch: 150, loss is 5.076883869171143 and perplexity is 160.2738427061366
At time: 1245.2677283287048 and batch: 200, loss is 5.082166128158569 and perplexity is 161.12269059568695
At time: 1246.954355955124 and batch: 250, loss is 5.096166954040528 and perplexity is 163.39440717754692
At time: 1248.6416897773743 and batch: 300, loss is 5.123492002487183 and perplexity is 167.92072658718124
At time: 1250.3273558616638 and batch: 350, loss is 5.064406051635742 and perplexity is 158.28640019995464
At time: 1252.0143654346466 and batch: 400, loss is 5.081152400970459 and perplexity is 160.95943890392823
At time: 1253.704018354416 and batch: 450, loss is 5.0465219116210935 and perplexity is 155.48074718224717
At time: 1255.3904280662537 and batch: 500, loss is 5.0383625316619876 and perplexity is 154.21728224454844
At time: 1257.100816488266 and batch: 550, loss is 5.060164222717285 and perplexity is 157.6163983913295
At time: 1258.7890539169312 and batch: 600, loss is 5.0309908485412596 and perplexity is 153.0846212463064
At time: 1260.4730036258698 and batch: 650, loss is 5.015783567428588 and perplexity is 150.77423226945623
At time: 1262.1487398147583 and batch: 700, loss is 5.0097371768951415 and perplexity is 149.8653428978141
At time: 1263.8352212905884 and batch: 750, loss is 5.02682523727417 and perplexity is 152.44825656736276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0562896728515625 and perplexity of 157.00688734907524
Finished 45 epochs...
Completing Train Step...
At time: 1268.3285236358643 and batch: 50, loss is 5.090149745941162 and perplexity is 162.4141810970112
At time: 1270.051985502243 and batch: 100, loss is 5.071717042922973 and perplexity is 159.44787127170812
At time: 1271.738559961319 and batch: 150, loss is 5.074403133392334 and perplexity is 159.87673840919695
At time: 1273.4234857559204 and batch: 200, loss is 5.080161352157592 and perplexity is 160.7999992623664
At time: 1275.1100797653198 and batch: 250, loss is 5.093870964050293 and perplexity is 163.01968559731984
At time: 1276.7983746528625 and batch: 300, loss is 5.121829719543457 and perplexity is 167.64182669696356
At time: 1278.4830067157745 and batch: 350, loss is 5.0630960941314695 and perplexity is 158.07918749174073
At time: 1280.1639323234558 and batch: 400, loss is 5.079599885940552 and perplexity is 160.7097408359802
At time: 1281.83456158638 and batch: 450, loss is 5.045392808914184 and perplexity is 155.30529252153778
At time: 1283.5004060268402 and batch: 500, loss is 5.037227067947388 and perplexity is 154.0422734932419
At time: 1285.1622807979584 and batch: 550, loss is 5.058928956985474 and perplexity is 157.42182045809366
At time: 1286.823994398117 and batch: 600, loss is 5.029716548919677 and perplexity is 152.8896698110331
At time: 1288.4930951595306 and batch: 650, loss is 5.014447803497315 and perplexity is 150.5729679389676
At time: 1290.1620318889618 and batch: 700, loss is 5.0085262966156 and perplexity is 149.6839837337896
At time: 1291.8553686141968 and batch: 750, loss is 5.02588996887207 and perplexity is 152.30574318456038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.055618640988372 and perplexity of 156.9015660658441
Finished 46 epochs...
Completing Train Step...
At time: 1296.3217170238495 and batch: 50, loss is 5.088025856018066 and perplexity is 162.0695973129568
At time: 1298.026578426361 and batch: 100, loss is 5.069445724487305 and perplexity is 159.08612535775947
At time: 1299.6872057914734 and batch: 150, loss is 5.072291393280029 and perplexity is 159.53947651773944
At time: 1301.3466820716858 and batch: 200, loss is 5.078140773773193 and perplexity is 160.4754182907247
At time: 1303.0079383850098 and batch: 250, loss is 5.0920061588287355 and perplexity is 162.71596891064743
At time: 1304.6707572937012 and batch: 300, loss is 5.120347185134888 and perplexity is 167.39347606023975
At time: 1306.3308010101318 and batch: 350, loss is 5.0617577075958256 and perplexity is 157.86775795442
At time: 1307.99871301651 and batch: 400, loss is 5.078128223419189 and perplexity is 160.4734042800545
At time: 1309.6645834445953 and batch: 450, loss is 5.044355964660644 and perplexity is 155.14434857276518
At time: 1311.3302359580994 and batch: 500, loss is 5.03627046585083 and perplexity is 153.89498679007593
At time: 1312.9905560016632 and batch: 550, loss is 5.057736511230469 and perplexity is 157.23421535322876
At time: 1314.6640455722809 and batch: 600, loss is 5.028519897460938 and perplexity is 152.7068235880465
At time: 1316.3248159885406 and batch: 650, loss is 5.013087005615234 and perplexity is 150.36820791319977
At time: 1317.9897871017456 and batch: 700, loss is 5.007302446365356 and perplexity is 149.50090500616423
At time: 1319.651338338852 and batch: 750, loss is 5.024890489578247 and perplexity is 152.15359279616453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054946189702943 and perplexity of 156.796092872824
Finished 47 epochs...
Completing Train Step...
At time: 1324.1455078125 and batch: 50, loss is 5.086082983016968 and perplexity is 161.75502235658692
At time: 1325.834823846817 and batch: 100, loss is 5.067407855987549 and perplexity is 158.7622588648889
At time: 1327.4974176883698 and batch: 150, loss is 5.070291662216187 and perplexity is 159.22075925125753
At time: 1329.1567940711975 and batch: 200, loss is 5.076210451126099 and perplexity is 160.1659477415953
At time: 1330.8160426616669 and batch: 250, loss is 5.090072412490844 and perplexity is 162.40162153364983
At time: 1332.4776422977448 and batch: 300, loss is 5.118895149230957 and perplexity is 167.1505911044297
At time: 1334.136818408966 and batch: 350, loss is 5.060567598342896 and perplexity is 157.67998982939295
At time: 1335.7953524589539 and batch: 400, loss is 5.076715679168701 and perplexity is 160.2468885149168
At time: 1337.4548604488373 and batch: 450, loss is 5.043220310211182 and perplexity is 154.9682582108266
At time: 1339.1147999763489 and batch: 500, loss is 5.035084743499755 and perplexity is 153.71261820514084
At time: 1340.8142261505127 and batch: 550, loss is 5.056549167633056 and perplexity is 157.0476351036871
At time: 1342.471756696701 and batch: 600, loss is 5.02710883140564 and perplexity is 152.49149612923128
At time: 1344.1310439109802 and batch: 650, loss is 5.011593418121338 and perplexity is 150.14378747588015
At time: 1345.791133403778 and batch: 700, loss is 5.005931978225708 and perplexity is 149.29615910991427
At time: 1347.451683998108 and batch: 750, loss is 5.023698120117188 and perplexity is 151.97227761754678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.054332644440407 and perplexity of 156.6999208788103
Finished 48 epochs...
Completing Train Step...
At time: 1351.8302154541016 and batch: 50, loss is 5.0840191078186034 and perplexity is 161.4215244451485
At time: 1353.5169339179993 and batch: 100, loss is 5.065350933074951 and perplexity is 158.43603276296588
At time: 1355.1759986877441 and batch: 150, loss is 5.0682516860961915 and perplexity is 158.89628377821114
At time: 1356.83580493927 and batch: 200, loss is 5.074483041763306 and perplexity is 159.88951440936737
At time: 1358.4963388442993 and batch: 250, loss is 5.088398370742798 and perplexity is 162.1299818707574
At time: 1360.1600816249847 and batch: 300, loss is 5.117524309158325 and perplexity is 166.9216113591433
At time: 1361.8227105140686 and batch: 350, loss is 5.059556751251221 and perplexity is 157.52068000276142
At time: 1363.4810018539429 and batch: 400, loss is 5.075501499176025 and perplexity is 160.05243802183134
At time: 1365.1400196552277 and batch: 450, loss is 5.042093925476074 and perplexity is 154.7938026008646
At time: 1366.8141729831696 and batch: 500, loss is 5.034229516983032 and perplexity is 153.58121529573225
At time: 1368.47997879982 and batch: 550, loss is 5.055192260742188 and perplexity is 156.8346805978128
At time: 1370.1386597156525 and batch: 600, loss is 5.025769290924072 and perplexity is 152.28736434898693
At time: 1371.7996096611023 and batch: 650, loss is 5.009969348907471 and perplexity is 149.9001414755247
At time: 1373.4650604724884 and batch: 700, loss is 5.004730253219605 and perplexity is 149.11685394152755
At time: 1375.1270484924316 and batch: 750, loss is 5.022520122528076 and perplexity is 151.7933600438267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.053889429846476 and perplexity of 156.63048457573697
Finished 49 epochs...
Completing Train Step...
At time: 1379.479697227478 and batch: 50, loss is 5.081978960037231 and perplexity is 161.09253638642465
At time: 1381.1787676811218 and batch: 100, loss is 5.063403873443604 and perplexity is 158.1278484833688
At time: 1382.846465587616 and batch: 150, loss is 5.066302976608276 and perplexity is 158.5869425883667
At time: 1384.5112476348877 and batch: 200, loss is 5.072683982849121 and perplexity is 159.60212234832392
At time: 1386.1729555130005 and batch: 250, loss is 5.086693859100341 and perplexity is 161.85386481828434
At time: 1387.8340725898743 and batch: 300, loss is 5.116141357421875 and perplexity is 166.6909263767281
At time: 1389.4946720600128 and batch: 350, loss is 5.0584267902374265 and perplexity is 157.3427882997643
At time: 1391.1548273563385 and batch: 400, loss is 5.074403715133667 and perplexity is 159.87683141613095
At time: 1392.8175060749054 and batch: 450, loss is 5.040931558609008 and perplexity is 154.61397994369008
At time: 1394.4794597625732 and batch: 500, loss is 5.032723894119263 and perplexity is 153.35015389585024
At time: 1396.1441802978516 and batch: 550, loss is 5.053689460754395 and perplexity is 156.59916645137702
At time: 1397.8366832733154 and batch: 600, loss is 5.0245552825927735 and perplexity is 152.10259839631132
At time: 1399.528005361557 and batch: 650, loss is 5.008788404464721 and perplexity is 149.72322222294727
At time: 1401.2208971977234 and batch: 700, loss is 5.0035654640197755 and perplexity is 148.9432653572242
At time: 1402.9148671627045 and batch: 750, loss is 5.021522016525268 and perplexity is 151.64192976428757
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0535533816315406 and perplexity of 156.5778580240155
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f231118f208>
SETTINGS FOR THIS RUN
{'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 13.552501739861292, 'dropout': 0.7242406468608337, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 6.889933187403387}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2113425731658936 and batch: 50, loss is 7.492274398803711 and perplexity is 1794.1280175632037
At time: 3.858642578125 and batch: 100, loss is 6.603511562347412 and perplexity is 737.6810593952
At time: 5.510752201080322 and batch: 150, loss is 6.439464321136475 and perplexity is 626.0713367871091
At time: 7.170490264892578 and batch: 200, loss is 6.409887714385986 and perplexity is 607.8254271782772
At time: 8.832437992095947 and batch: 250, loss is 6.437303791046142 and perplexity is 624.7201509896713
At time: 10.494361400604248 and batch: 300, loss is 6.427099332809449 and perplexity is 618.3776362999967
At time: 12.157808065414429 and batch: 350, loss is 6.36201663017273 and perplexity is 579.4136420394349
At time: 13.925822496414185 and batch: 400, loss is 6.404477319717407 and perplexity is 604.5457319507989
At time: 15.587410688400269 and batch: 450, loss is 6.39527009010315 and perplexity is 599.0050867222308
At time: 17.250954151153564 and batch: 500, loss is 6.431165399551392 and perplexity is 620.8971197636807
At time: 18.91405487060547 and batch: 550, loss is 6.473336009979248 and perplexity is 647.6406628991888
At time: 20.57623028755188 and batch: 600, loss is 6.48939115524292 and perplexity is 658.1225467479964
At time: 22.236104488372803 and batch: 650, loss is 6.5002233505249025 and perplexity is 665.2902093689192
At time: 23.898053407669067 and batch: 700, loss is 6.506561489105224 and perplexity is 669.5203022084114
At time: 25.561889171600342 and batch: 750, loss is 6.49422064781189 and perplexity is 661.3086320920487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.071914317996003 and perplexity of 433.50976329958183
Finished 1 epochs...
Completing Train Step...
At time: 29.963013410568237 and batch: 50, loss is 6.441054010391236 and perplexity is 627.0673871594395
At time: 31.617419481277466 and batch: 100, loss is 6.391124629974366 and perplexity is 596.5270748167467
At time: 33.273035287857056 and batch: 150, loss is 6.394842872619629 and perplexity is 598.7492359323217
At time: 34.956053495407104 and batch: 200, loss is 6.408687629699707 and perplexity is 607.0964227122303
At time: 36.610424518585205 and batch: 250, loss is 6.460329284667969 and perplexity is 639.2715242273619
At time: 38.265146017074585 and batch: 300, loss is 6.429897632598877 and perplexity is 620.1104656667275
At time: 39.91899251937866 and batch: 350, loss is 6.346769990921021 and perplexity is 570.6465355495202
At time: 41.571094274520874 and batch: 400, loss is 6.380983839035034 and perplexity is 590.5083871658558
At time: 43.22308588027954 and batch: 450, loss is 6.333740406036377 and perplexity is 563.2594777320452
At time: 44.877805948257446 and batch: 500, loss is 6.293060903549194 and perplexity is 540.8061537011048
At time: 46.535080909729004 and batch: 550, loss is 6.287674045562744 and perplexity is 537.9007403034293
At time: 48.188292264938354 and batch: 600, loss is 6.234086236953735 and perplexity is 509.83453757683606
At time: 49.84219694137573 and batch: 650, loss is 6.246973438262939 and perplexity is 516.4473970005877
At time: 51.5023627281189 and batch: 700, loss is 6.238389263153076 and perplexity is 512.0330957837599
At time: 53.158730268478394 and batch: 750, loss is 6.225655517578125 and perplexity is 505.55433361375833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.8700408935546875 and perplexity of 354.26346706401927
Finished 2 epochs...
Completing Train Step...
At time: 57.56033968925476 and batch: 50, loss is 6.270034198760986 and perplexity is 528.4954514582654
At time: 59.2396924495697 and batch: 100, loss is 6.263486042022705 and perplexity is 525.0460862239992
At time: 60.894187927246094 and batch: 150, loss is 6.256287660598755 and perplexity is 521.2801747253048
At time: 62.54788517951965 and batch: 200, loss is 6.225920276641846 and perplexity is 505.6882014263623
At time: 64.2013099193573 and batch: 250, loss is 6.262551794052124 and perplexity is 524.5557920473143
At time: 65.85686612129211 and batch: 300, loss is 6.244759349822998 and perplexity is 515.305201716343
At time: 67.53816628456116 and batch: 350, loss is 6.144598836898804 and perplexity is 466.19259227938335
At time: 69.19348478317261 and batch: 400, loss is 6.202916345596313 and perplexity is 494.18816506077127
At time: 70.84563541412354 and batch: 450, loss is 6.198281784057617 and perplexity is 491.903118781972
At time: 72.51563262939453 and batch: 500, loss is 6.173843364715577 and perplexity is 480.0274859774744
At time: 74.18121242523193 and batch: 550, loss is 6.180910253524781 and perplexity is 483.4318016384874
At time: 75.83723402023315 and batch: 600, loss is 6.1447549533844 and perplexity is 466.26537830990264
At time: 77.49458932876587 and batch: 650, loss is 6.142927465438842 and perplexity is 465.4140620736197
At time: 79.15427947044373 and batch: 700, loss is 6.129266738891602 and perplexity is 459.09939749427815
At time: 80.81437492370605 and batch: 750, loss is 6.125640630722046 and perplexity is 457.4376680445453
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.833387507948765 and perplexity of 341.5136018531716
Finished 3 epochs...
Completing Train Step...
At time: 85.14710998535156 and batch: 50, loss is 6.175841550827027 and perplexity is 480.98762918601363
At time: 86.83206129074097 and batch: 100, loss is 6.162758703231812 and perplexity is 474.73592555146774
At time: 88.49456310272217 and batch: 150, loss is 6.160593042373657 and perplexity is 473.7089210127015
At time: 90.14890336990356 and batch: 200, loss is 6.155842418670654 and perplexity is 471.4638451618083
At time: 91.8021297454834 and batch: 250, loss is 6.191625480651855 and perplexity is 488.63973546092427
At time: 93.46235203742981 and batch: 300, loss is 6.199639749526978 and perplexity is 492.5715599888665
At time: 95.12106418609619 and batch: 350, loss is 6.111792049407959 and perplexity is 451.146467945791
At time: 96.77866840362549 and batch: 400, loss is 6.156126804351807 and perplexity is 471.5979417952309
At time: 98.4332480430603 and batch: 450, loss is 6.156289958953858 and perplexity is 471.67489144692667
At time: 100.08957624435425 and batch: 500, loss is 6.136139135360718 and perplexity is 462.265377045221
At time: 101.74731135368347 and batch: 550, loss is 6.154190425872803 and perplexity is 470.6856332623636
At time: 103.40388345718384 and batch: 600, loss is 6.1100183200836184 and perplexity is 450.34696548611237
At time: 105.05680704116821 and batch: 650, loss is 6.105623617172241 and perplexity is 448.37216687071356
At time: 106.71331906318665 and batch: 700, loss is 6.108365688323975 and perplexity is 449.6033224414002
At time: 108.41084289550781 and batch: 750, loss is 6.089693002700805 and perplexity is 441.285916764211
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.789470583893532 and perplexity of 326.83994423359184
Finished 4 epochs...
Completing Train Step...
At time: 112.74818968772888 and batch: 50, loss is 6.145096111297607 and perplexity is 466.4244755704694
At time: 114.42974925041199 and batch: 100, loss is 6.1341471767425535 and perplexity is 461.34548004608575
At time: 116.090580701828 and batch: 150, loss is 6.132032527923584 and perplexity is 460.3709271532933
At time: 117.74972748756409 and batch: 200, loss is 6.120833101272583 and perplexity is 455.243800750389
At time: 119.40360379219055 and batch: 250, loss is 6.171903009414673 and perplexity is 479.09696516292536
At time: 121.05931973457336 and batch: 300, loss is 6.165642576217651 and perplexity is 476.10697968533964
At time: 122.71888613700867 and batch: 350, loss is 6.067790298461914 and perplexity is 431.7256419717448
At time: 124.37599682807922 and batch: 400, loss is 6.119715805053711 and perplexity is 454.73544261933847
At time: 126.03009724617004 and batch: 450, loss is 6.113910140991211 and perplexity is 452.10305018913306
At time: 127.69881987571716 and batch: 500, loss is 6.088096675872802 and perplexity is 440.58204217287846
At time: 129.35688877105713 and batch: 550, loss is 6.095305652618408 and perplexity is 443.76966380968867
At time: 131.0200743675232 and batch: 600, loss is 6.066416158676147 and perplexity is 431.1327980091004
At time: 132.67961168289185 and batch: 650, loss is 6.063641338348389 and perplexity is 429.93814020413737
At time: 134.33384919166565 and batch: 700, loss is 6.066349115371704 and perplexity is 431.1038944105752
At time: 136.02458691596985 and batch: 750, loss is 6.041400747299194 and perplexity is 420.4816110482404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.753951671511628 and perplexity of 315.4346949564758
Finished 5 epochs...
Completing Train Step...
At time: 140.60890531539917 and batch: 50, loss is 6.088572225570679 and perplexity is 440.7916106560885
At time: 142.2942750453949 and batch: 100, loss is 6.083331155776977 and perplexity is 438.4874345085778
At time: 143.95837497711182 and batch: 150, loss is 6.081789617538452 and perplexity is 437.8120100912064
At time: 145.62424230575562 and batch: 200, loss is 6.071797771453857 and perplexity is 433.45924217977085
At time: 147.27950882911682 and batch: 250, loss is 6.105268707275391 and perplexity is 448.2130633865897
At time: 148.96073698997498 and batch: 300, loss is 6.102768363952637 and perplexity is 447.093776729961
At time: 150.61819052696228 and batch: 350, loss is 6.021822624206543 and perplexity is 412.32943287726073
At time: 152.28147435188293 and batch: 400, loss is 6.067163133621216 and perplexity is 431.45496371708674
At time: 153.94924807548523 and batch: 450, loss is 6.062405385971069 and perplexity is 429.40708538448746
At time: 155.6068992614746 and batch: 500, loss is 6.0493559074401855 and perplexity is 423.8399499514609
At time: 157.26631808280945 and batch: 550, loss is 6.059456872940063 and perplexity is 428.14283773887246
At time: 158.9239604473114 and batch: 600, loss is 6.021822090148926 and perplexity is 412.3292126696453
At time: 160.58194947242737 and batch: 650, loss is 6.016068925857544 and perplexity is 409.96382572118944
At time: 162.23671174049377 and batch: 700, loss is 6.015716018676758 and perplexity is 409.81917206938743
At time: 163.89317202568054 and batch: 750, loss is 5.9984313488006595 and perplexity is 402.79645052441754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.7358277786609735 and perplexity of 309.76928506281575
Finished 6 epochs...
Completing Train Step...
At time: 168.24685049057007 and batch: 50, loss is 6.057461090087891 and perplexity is 427.28920971646136
At time: 169.92756175994873 and batch: 100, loss is 6.061285572052002 and perplexity is 428.9264984874967
At time: 171.58269357681274 and batch: 150, loss is 6.046972427368164 and perplexity is 422.8309388336517
At time: 173.24781775474548 and batch: 200, loss is 6.051343936920166 and perplexity is 424.6833943851645
At time: 174.910058259964 and batch: 250, loss is 6.092582302093506 and perplexity is 442.5627676094431
At time: 176.56518268585205 and batch: 300, loss is 6.086916608810425 and perplexity is 440.0624324640854
At time: 178.22484850883484 and batch: 350, loss is 5.983425426483154 and perplexity is 396.797242675123
At time: 179.8847415447235 and batch: 400, loss is 6.027054262161255 and perplexity is 414.49224377692684
At time: 181.54091954231262 and batch: 450, loss is 6.028705139160156 and perplexity is 415.177084626866
At time: 183.19745635986328 and batch: 500, loss is 6.01427812576294 and perplexity is 409.23031944088893
At time: 184.85460591316223 and batch: 550, loss is 6.023917074203491 and perplexity is 413.1939412754686
At time: 186.5178325176239 and batch: 600, loss is 5.9967947101593015 and perplexity is 402.13775745718124
At time: 188.17620182037354 and batch: 650, loss is 5.991085357666016 and perplexity is 399.84835297650574
At time: 189.83451533317566 and batch: 700, loss is 5.9946062374114994 and perplexity is 401.2586522339022
At time: 191.5215904712677 and batch: 750, loss is 5.97676721572876 and perplexity is 394.16405888075957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.7216094260992 and perplexity of 305.39603999997377
Finished 7 epochs...
Completing Train Step...
At time: 195.90561771392822 and batch: 50, loss is 5.960385580062866 and perplexity is 387.75960779094135
At time: 197.58827829360962 and batch: 100, loss is 5.9557890033721925 and perplexity is 385.98133113971517
At time: 199.24572920799255 and batch: 150, loss is 5.985718231201172 and perplexity is 397.7080650351147
At time: 200.90966391563416 and batch: 200, loss is 6.0008502960205075 and perplexity is 403.77197327192374
At time: 202.5693657398224 and batch: 250, loss is 6.046011209487915 and perplexity is 422.4247014475201
At time: 204.22740006446838 and batch: 300, loss is 6.04775029182434 and perplexity is 423.1599719469331
At time: 205.88294339179993 and batch: 350, loss is 5.955594568252564 and perplexity is 385.9062901089628
At time: 207.54112935066223 and batch: 400, loss is 6.00445065498352 and perplexity is 405.2283174225997
At time: 209.2007040977478 and batch: 450, loss is 6.00703797340393 and perplexity is 406.2781296263262
At time: 210.8546280860901 and batch: 500, loss is 5.994340314865112 and perplexity is 401.15196269754546
At time: 212.51201033592224 and batch: 550, loss is 5.991882333755493 and perplexity is 400.167149573003
At time: 214.16996479034424 and batch: 600, loss is 5.956822376251221 and perplexity is 386.38039993704984
At time: 215.82967400550842 and batch: 650, loss is 5.963944129943847 and perplexity is 389.14192776566125
At time: 217.48629665374756 and batch: 700, loss is 5.954567527770996 and perplexity is 385.5101521865914
At time: 219.15045976638794 and batch: 750, loss is 5.940874767303467 and perplexity is 380.2674296008356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.730597562568132 and perplexity of 308.1533542802729
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 223.52107501029968 and batch: 50, loss is 5.94700421333313 and perplexity is 382.60541624903846
At time: 225.2066400051117 and batch: 100, loss is 5.907329845428467 and perplexity is 367.72296620067397
At time: 226.8684787750244 and batch: 150, loss is 5.869985408782959 and perplexity is 354.2438113817181
At time: 228.52843260765076 and batch: 200, loss is 5.851716070175171 and perplexity is 347.83077062017645
At time: 230.18882846832275 and batch: 250, loss is 5.869069290161133 and perplexity is 353.91943063775165
At time: 231.8729350566864 and batch: 300, loss is 5.860171518325806 and perplexity is 350.78430479710573
At time: 233.53223299980164 and batch: 350, loss is 5.759911403656006 and perplexity is 317.3202142615821
At time: 235.19306993484497 and batch: 400, loss is 5.7710476493835445 and perplexity is 320.873719772989
At time: 236.86392045021057 and batch: 450, loss is 5.7423789691925045 and perplexity is 311.8053045640827
At time: 238.52216863632202 and batch: 500, loss is 5.6956933498382565 and perplexity is 297.5830512339327
At time: 240.18093824386597 and batch: 550, loss is 5.666909379959106 and perplexity is 289.13953161636647
At time: 241.84210085868835 and batch: 600, loss is 5.600488014221192 and perplexity is 270.5584115660913
At time: 243.50365209579468 and batch: 650, loss is 5.541959524154663 and perplexity is 255.17753641768294
At time: 245.1618902683258 and batch: 700, loss is 5.522823362350464 and perplexity is 250.34084325934012
At time: 246.82147121429443 and batch: 750, loss is 5.538120183944702 and perplexity is 254.19970136330784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.408112636832303 and perplexity of 223.20991171184264
Finished 9 epochs...
Completing Train Step...
At time: 251.16357851028442 and batch: 50, loss is 5.687785701751709 and perplexity is 295.239148779979
At time: 252.8482584953308 and batch: 100, loss is 5.711531457901001 and perplexity is 302.3337252948388
At time: 254.51263236999512 and batch: 150, loss is 5.694092416763306 and perplexity is 297.10702183194616
At time: 256.1802167892456 and batch: 200, loss is 5.690343999862671 and perplexity is 295.995425514777
At time: 257.840448141098 and batch: 250, loss is 5.7200061702728275 and perplexity is 304.9068043092739
At time: 259.50208854675293 and batch: 300, loss is 5.718024702072143 and perplexity is 304.30323934223463
At time: 261.16001749038696 and batch: 350, loss is 5.632687578201294 and perplexity is 279.41205116702383
At time: 262.8222436904907 and batch: 400, loss is 5.644894227981568 and perplexity is 282.84363769807766
At time: 264.4899196624756 and batch: 450, loss is 5.620421457290649 and perplexity is 276.00568333279995
At time: 266.1490797996521 and batch: 500, loss is 5.590514583587646 and perplexity is 267.8734275231578
At time: 267.80698108673096 and batch: 550, loss is 5.582185745239258 and perplexity is 265.6516184356635
At time: 269.4690771102905 and batch: 600, loss is 5.538745937347412 and perplexity is 254.35881746993417
At time: 271.14186120033264 and batch: 650, loss is 5.51056471824646 and perplexity is 247.2907372352063
At time: 272.80792474746704 and batch: 700, loss is 5.504404630661011 and perplexity is 245.7720869464704
At time: 274.5106563568115 and batch: 750, loss is 5.518969202041626 and perplexity is 249.3778464816216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.370697731195494 and perplexity of 215.01283654277043
Finished 10 epochs...
Completing Train Step...
At time: 278.8675673007965 and batch: 50, loss is 5.6254470252990725 and perplexity is 277.39625995706064
At time: 280.5517599582672 and batch: 100, loss is 5.6408508777618405 and perplexity is 281.7023107606364
At time: 282.21170020103455 and batch: 150, loss is 5.627216386795044 and perplexity is 277.8875086886902
At time: 283.87628722190857 and batch: 200, loss is 5.625745391845703 and perplexity is 277.4790380696879
At time: 285.5348324775696 and batch: 250, loss is 5.65482123374939 and perplexity is 285.66541082726036
At time: 287.19196105003357 and batch: 300, loss is 5.652812967300415 and perplexity is 285.09229424508635
At time: 288.84888195991516 and batch: 350, loss is 5.572538919448853 and perplexity is 263.1012448349705
At time: 290.5084972381592 and batch: 400, loss is 5.589282789230347 and perplexity is 267.5436656877444
At time: 292.16976261138916 and batch: 450, loss is 5.566045141220092 and perplexity is 261.3982590896316
At time: 293.8307282924652 and batch: 500, loss is 5.542648515701294 and perplexity is 255.35341216465878
At time: 295.49384474754333 and batch: 550, loss is 5.545960378646851 and perplexity is 256.20050962950324
At time: 297.15949296951294 and batch: 600, loss is 5.5146761226654055 and perplexity is 248.30954238992052
At time: 298.82515048980713 and batch: 650, loss is 5.499851942062378 and perplexity is 244.65570636320885
At time: 300.4837749004364 and batch: 700, loss is 5.495881099700927 and perplexity is 243.6861433852559
At time: 302.1408922672272 and batch: 750, loss is 5.508286476135254 and perplexity is 246.72799034416454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.359564138013263 and perplexity of 212.63224793510557
Finished 11 epochs...
Completing Train Step...
At time: 306.4829430580139 and batch: 50, loss is 5.593185892105103 and perplexity is 268.589956700859
At time: 308.16836166381836 and batch: 100, loss is 5.600890264511109 and perplexity is 270.6672656574104
At time: 309.8260977268219 and batch: 150, loss is 5.587963390350342 and perplexity is 267.19090164430014
At time: 311.48677825927734 and batch: 200, loss is 5.591508131027222 and perplexity is 268.1397047388224
At time: 313.14895606040955 and batch: 250, loss is 5.6221232414245605 and perplexity is 276.4757853181611
At time: 314.83673763275146 and batch: 300, loss is 5.620406370162964 and perplexity is 276.001519231226
At time: 316.5019419193268 and batch: 350, loss is 5.546022119522095 and perplexity is 256.2163281615258
At time: 318.1581356525421 and batch: 400, loss is 5.5651273441314695 and perplexity is 261.1584585894015
At time: 319.8171215057373 and batch: 450, loss is 5.539898290634155 and perplexity is 254.65209763798623
At time: 321.4767563343048 and batch: 500, loss is 5.523037090301513 and perplexity is 250.39435381298065
At time: 323.1342372894287 and batch: 550, loss is 5.535402736663818 and perplexity is 253.50986479779547
At time: 324.7920444011688 and batch: 600, loss is 5.509581136703491 and perplexity is 247.04762620967279
At time: 326.4520049095154 and batch: 650, loss is 5.497680397033691 and perplexity is 244.12500191317056
At time: 328.1182613372803 and batch: 700, loss is 5.49336685180664 and perplexity is 243.07422559124535
At time: 329.78060722351074 and batch: 750, loss is 5.501686582565307 and perplexity is 245.10497362731937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.353418394576671 and perplexity of 211.32947205660514
Finished 12 epochs...
Completing Train Step...
At time: 334.15913224220276 and batch: 50, loss is 5.575475711822509 and perplexity is 263.87505426696134
At time: 335.843727350235 and batch: 100, loss is 5.575941009521484 and perplexity is 263.99786329169126
At time: 337.5015482902527 and batch: 150, loss is 5.5650571346282955 and perplexity is 261.14012342743314
At time: 339.15835213661194 and batch: 200, loss is 5.571401214599609 and perplexity is 262.8020834838085
At time: 340.8478400707245 and batch: 250, loss is 5.604117403030395 and perplexity is 271.5421573553819
At time: 342.54271721839905 and batch: 300, loss is 5.60602988243103 and perplexity is 272.06197304769097
At time: 344.23203110694885 and batch: 350, loss is 5.533452730178833 and perplexity is 253.01600059312727
At time: 345.9229552745819 and batch: 400, loss is 5.552571382522583 and perplexity is 257.89986321897595
At time: 347.6172833442688 and batch: 450, loss is 5.527870483398438 and perplexity is 251.60753768966217
At time: 349.3102345466614 and batch: 500, loss is 5.513790216445923 and perplexity is 248.08966083356034
At time: 351.00101685523987 and batch: 550, loss is 5.53028920173645 and perplexity is 252.21684202579408
At time: 352.6827826499939 and batch: 600, loss is 5.5043948364257815 and perplexity is 245.76967980862617
At time: 354.34269070625305 and batch: 650, loss is 5.491501274108887 and perplexity is 242.6211744695172
At time: 356.00235056877136 and batch: 700, loss is 5.485660333633422 and perplexity is 241.2081692881717
At time: 357.68657517433167 and batch: 750, loss is 5.492259349822998 and perplexity is 242.80516942185253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.34750756552053 and perplexity of 210.08402411380732
Finished 13 epochs...
Completing Train Step...
At time: 362.0300941467285 and batch: 50, loss is 5.561915416717529 and perplexity is 260.32098225346044
At time: 363.71684408187866 and batch: 100, loss is 5.560114488601685 and perplexity is 259.85258477906433
At time: 365.374666929245 and batch: 150, loss is 5.552076597213745 and perplexity is 257.7722897188533
At time: 367.0349586009979 and batch: 200, loss is 5.557627573013305 and perplexity is 259.20715623110624
At time: 368.6937208175659 and batch: 250, loss is 5.591413068771362 and perplexity is 268.11421598513266
At time: 370.35274958610535 and batch: 300, loss is 5.596619691848755 and perplexity is 269.51382610881694
At time: 372.007746219635 and batch: 350, loss is 5.5232774448394775 and perplexity is 250.45454446545867
At time: 373.6703839302063 and batch: 400, loss is 5.543472356796265 and perplexity is 255.56386947912995
At time: 375.33034563064575 and batch: 450, loss is 5.516767807006836 and perplexity is 248.82947114542006
At time: 376.98930311203003 and batch: 500, loss is 5.503844327926636 and perplexity is 245.63441874565765
At time: 378.64465165138245 and batch: 550, loss is 5.520934925079346 and perplexity is 249.86853638176308
At time: 380.30505418777466 and batch: 600, loss is 5.496578512191772 and perplexity is 243.85615242182058
At time: 381.9640507698059 and batch: 650, loss is 5.483392648696899 and perplexity is 240.6618048814132
At time: 383.6236162185669 and batch: 700, loss is 5.476808671951294 and perplexity is 239.08249791975118
At time: 385.2822184562683 and batch: 750, loss is 5.481382474899292 and perplexity is 240.17851873446895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.340275609216024 and perplexity of 208.5701862337964
Finished 14 epochs...
Completing Train Step...
At time: 389.6453788280487 and batch: 50, loss is 5.550019454956055 and perplexity is 257.2425604998025
At time: 391.33634901046753 and batch: 100, loss is 5.5479660320281985 and perplexity is 256.71487469453496
At time: 393.00083446502686 and batch: 150, loss is 5.538777437210083 and perplexity is 254.36682986394766
At time: 394.6596612930298 and batch: 200, loss is 5.5467500972747805 and perplexity is 256.40291585590717
At time: 396.3183562755585 and batch: 250, loss is 5.581090774536133 and perplexity is 265.3608968910727
At time: 398.0048875808716 and batch: 300, loss is 5.586304731369019 and perplexity is 266.74809039350913
At time: 399.66012835502625 and batch: 350, loss is 5.513607730865479 and perplexity is 248.044392178365
At time: 401.3175280094147 and batch: 400, loss is 5.53500129699707 and perplexity is 253.40811630646024
At time: 402.97768902778625 and batch: 450, loss is 5.510217599868774 and perplexity is 247.2049129721317
At time: 404.6457750797272 and batch: 500, loss is 5.496665887832641 and perplexity is 243.87746044030567
At time: 406.30549001693726 and batch: 550, loss is 5.512922248840332 and perplexity is 247.87442046901967
At time: 407.9691388607025 and batch: 600, loss is 5.48996829032898 and perplexity is 242.24952508341624
At time: 409.63177967071533 and batch: 650, loss is 5.476259307861328 and perplexity is 238.95119065190602
At time: 411.3045198917389 and batch: 700, loss is 5.471026086807251 and perplexity is 237.70397258044588
At time: 412.97367668151855 and batch: 750, loss is 5.47452820777893 and perplexity is 238.53790005298362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.334670665652253 and perplexity of 207.4044321556629
Finished 15 epochs...
Completing Train Step...
At time: 417.46811628341675 and batch: 50, loss is 5.540033903121948 and perplexity is 254.68663398419565
At time: 419.16106486320496 and batch: 100, loss is 5.537877321243286 and perplexity is 254.1379732331692
At time: 420.81892013549805 and batch: 150, loss is 5.53128867149353 and perplexity is 252.46905114832506
At time: 422.4777398109436 and batch: 200, loss is 5.539431638717652 and perplexity is 254.53329147130054
At time: 424.1416120529175 and batch: 250, loss is 5.573772630691528 and perplexity is 263.4260361068116
At time: 425.82196283340454 and batch: 300, loss is 5.5785746669769285 and perplexity is 264.6940596014614
At time: 427.491087436676 and batch: 350, loss is 5.50657958984375 and perplexity is 246.30721293147445
At time: 429.1540403366089 and batch: 400, loss is 5.52827374458313 and perplexity is 251.70902170429505
At time: 430.82047605514526 and batch: 450, loss is 5.502812147140503 and perplexity is 245.381010422315
At time: 432.48299264907837 and batch: 500, loss is 5.489411563873291 and perplexity is 242.1146958988937
At time: 434.1440303325653 and batch: 550, loss is 5.50596568107605 and perplexity is 246.156049179032
At time: 435.8043191432953 and batch: 600, loss is 5.483404369354248 and perplexity is 240.66462561249548
At time: 437.46613240242004 and batch: 650, loss is 5.469864530563354 and perplexity is 237.42802634146025
At time: 439.1265797615051 and batch: 700, loss is 5.464044160842896 and perplexity is 236.0501212944653
At time: 440.83093428611755 and batch: 750, loss is 5.466205444335937 and perplexity is 236.56084423487468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.332612148551053 and perplexity of 206.97792572121014
Finished 16 epochs...
Completing Train Step...
At time: 445.21512508392334 and batch: 50, loss is 5.530668020248413 and perplexity is 252.31240453386297
At time: 446.9032213687897 and batch: 100, loss is 5.528800687789917 and perplexity is 251.84169301542235
At time: 448.56279277801514 and batch: 150, loss is 5.522804327011109 and perplexity is 250.33607798178863
At time: 450.22146368026733 and batch: 200, loss is 5.53080186843872 and perplexity is 252.34617835283373
At time: 451.8811719417572 and batch: 250, loss is 5.565660715103149 and perplexity is 261.29779008460616
At time: 453.5425362586975 and batch: 300, loss is 5.570273180007934 and perplexity is 262.5058007828597
At time: 455.2001299858093 and batch: 350, loss is 5.499229927062988 and perplexity is 244.5035741633247
At time: 456.85798239707947 and batch: 400, loss is 5.521306753158569 and perplexity is 249.96146179477256
At time: 458.5182800292969 and batch: 450, loss is 5.496691827774048 and perplexity is 243.88378668939075
At time: 460.17974519729614 and batch: 500, loss is 5.483062705993652 and perplexity is 240.58241337298705
At time: 461.83936309814453 and batch: 550, loss is 5.499390335083008 and perplexity is 244.54279764334026
At time: 463.4980938434601 and batch: 600, loss is 5.477387762069702 and perplexity is 239.22098832712382
At time: 465.16084814071655 and batch: 650, loss is 5.463115873336792 and perplexity is 235.83110058887357
At time: 466.82100534439087 and batch: 700, loss is 5.456093606948852 and perplexity is 234.18083286927217
At time: 468.4826250076294 and batch: 750, loss is 5.458136281967163 and perplexity is 234.65967710141783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.330573148505632 and perplexity of 206.5563276865362
Finished 17 epochs...
Completing Train Step...
At time: 472.82958793640137 and batch: 50, loss is 5.524841184616089 and perplexity is 250.84649657455472
At time: 474.525488615036 and batch: 100, loss is 5.521332883834839 and perplexity is 249.96799354214977
At time: 476.18448853492737 and batch: 150, loss is 5.515116176605225 and perplexity is 248.41883602812823
At time: 477.8447036743164 and batch: 200, loss is 5.524768686294555 and perplexity is 250.82831128379996
At time: 479.503390789032 and batch: 250, loss is 5.559376707077027 and perplexity is 259.66094104717496
At time: 481.1976225376129 and batch: 300, loss is 5.564801445007324 and perplexity is 261.07336114382883
At time: 482.8905198574066 and batch: 350, loss is 5.493047361373901 and perplexity is 242.99657810619829
At time: 484.5833840370178 and batch: 400, loss is 5.512404079437256 and perplexity is 247.74601279986317
At time: 486.25097918510437 and batch: 450, loss is 5.484591722488403 and perplexity is 240.9505492225527
At time: 487.94587564468384 and batch: 500, loss is 5.4701431941986085 and perplexity is 237.49419811779742
At time: 489.6077284812927 and batch: 550, loss is 5.482676935195923 and perplexity is 240.48962160278887
At time: 491.2945303916931 and batch: 600, loss is 5.458946981430054 and perplexity is 234.84999270956322
At time: 492.96906423568726 and batch: 650, loss is 5.4456326770782475 and perplexity is 231.74385236407224
At time: 494.62844371795654 and batch: 700, loss is 5.4381920433044435 and perplexity is 230.02593036928167
At time: 496.2896490097046 and batch: 750, loss is 5.439689226150513 and perplexity is 230.3705791831486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.315079799918241 and perplexity of 203.38074224790108
Finished 18 epochs...
Completing Train Step...
At time: 500.6332802772522 and batch: 50, loss is 5.505250387191772 and perplexity is 245.98003821976232
At time: 502.33655405044556 and batch: 100, loss is 5.502863187789917 and perplexity is 245.39353514807306
At time: 503.9932837486267 and batch: 150, loss is 5.495967893600464 and perplexity is 243.70729477379587
At time: 505.6495990753174 and batch: 200, loss is 5.50441689491272 and perplexity is 245.77510117569145
At time: 507.3054931163788 and batch: 250, loss is 5.542534456253052 and perplexity is 255.32428835631495
At time: 508.96804547309875 and batch: 300, loss is 5.547596111297607 and perplexity is 256.61992810297323
At time: 510.62657713890076 and batch: 350, loss is 5.477568197250366 and perplexity is 239.26415610374698
At time: 512.2827920913696 and batch: 400, loss is 5.499303340911865 and perplexity is 244.52152477067185
At time: 513.941810131073 and batch: 450, loss is 5.475133047103882 and perplexity is 238.68222079645787
At time: 515.6021304130554 and batch: 500, loss is 5.46091962814331 and perplexity is 235.3137260164034
At time: 517.2624983787537 and batch: 550, loss is 5.476359596252442 and perplexity is 238.97515588406833
At time: 518.9190032482147 and batch: 600, loss is 5.4528746414184575 and perplexity is 233.42822479996394
At time: 520.5761466026306 and batch: 650, loss is 5.43983564376831 and perplexity is 230.4043119640396
At time: 522.2383854389191 and batch: 700, loss is 5.43108720779419 and perplexity is 228.39742594932198
At time: 523.9418451786041 and batch: 750, loss is 5.431624250411987 and perplexity is 228.5201180433449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.311150484306868 and perplexity of 202.58316311858226
Finished 19 epochs...
Completing Train Step...
At time: 528.2886073589325 and batch: 50, loss is 5.499139251708985 and perplexity is 244.48140472030866
At time: 529.9791393280029 and batch: 100, loss is 5.49691104888916 and perplexity is 243.9372570257659
At time: 531.6403667926788 and batch: 150, loss is 5.488930530548096 and perplexity is 241.9982586689861
At time: 533.2971267700195 and batch: 200, loss is 5.499544630050659 and perplexity is 244.58053227744924
At time: 534.954925775528 and batch: 250, loss is 5.537170619964599 and perplexity is 253.95843704921992
At time: 536.622058391571 and batch: 300, loss is 5.541836404800415 and perplexity is 255.14612105813976
At time: 538.280645608902 and batch: 350, loss is 5.4707957363128665 and perplexity is 237.64922365880741
At time: 539.937735080719 and batch: 400, loss is 5.494516410827637 and perplexity is 243.35381443119266
At time: 541.5933768749237 and batch: 450, loss is 5.470107440948486 and perplexity is 237.48570708012153
At time: 543.2502136230469 and batch: 500, loss is 5.455408353805542 and perplexity is 234.02041468725446
At time: 544.9095158576965 and batch: 550, loss is 5.4709700584411625 and perplexity is 237.6906547883399
At time: 546.5664451122284 and batch: 600, loss is 5.44656512260437 and perplexity is 231.96004165912206
At time: 548.2217648029327 and batch: 650, loss is 5.433946981430053 and perplexity is 229.0515257294186
At time: 549.8781368732452 and batch: 700, loss is 5.425226554870606 and perplexity is 227.06278266782653
At time: 551.5378127098083 and batch: 750, loss is 5.425564193725586 and perplexity is 227.13946082981016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.309055062227471 and perplexity of 202.15911032562278
Finished 20 epochs...
Completing Train Step...
At time: 555.8803052902222 and batch: 50, loss is 5.492266855239868 and perplexity is 242.80699178270604
At time: 557.5652506351471 and batch: 100, loss is 5.490346240997314 and perplexity is 242.3411007577788
At time: 559.2251222133636 and batch: 150, loss is 5.4825451469421385 and perplexity is 240.45792998384195
At time: 560.8806159496307 and batch: 200, loss is 5.493313341140747 and perplexity is 243.06121887557467
At time: 562.5364999771118 and batch: 250, loss is 5.530021667480469 and perplexity is 252.1493744059688
At time: 564.2199647426605 and batch: 300, loss is 5.535356407165527 and perplexity is 253.49812008501297
At time: 565.8830575942993 and batch: 350, loss is 5.465825567245483 and perplexity is 236.47099725613626
At time: 567.5411577224731 and batch: 400, loss is 5.489445915222168 and perplexity is 242.123013008132
At time: 569.1991493701935 and batch: 450, loss is 5.465828514099121 and perplexity is 236.47169410258144
At time: 570.8571197986603 and batch: 500, loss is 5.450746517181397 and perplexity is 232.93198875041944
At time: 572.5161201953888 and batch: 550, loss is 5.46579026222229 and perplexity is 236.46264878946593
At time: 574.1817631721497 and batch: 600, loss is 5.4425034332275395 and perplexity is 231.01980279378972
At time: 575.8421008586884 and batch: 650, loss is 5.429031438827515 and perplexity is 227.92837590344212
At time: 577.5011293888092 and batch: 700, loss is 5.418884029388428 and perplexity is 225.62718864109212
At time: 579.1572985649109 and batch: 750, loss is 5.419942083358765 and perplexity is 225.86604072076295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.307271913040516 and perplexity of 201.79895167614228
Finished 21 epochs...
Completing Train Step...
At time: 583.5043532848358 and batch: 50, loss is 5.486241073608398 and perplexity is 241.3482891970462
At time: 585.1898958683014 and batch: 100, loss is 5.483586702346802 and perplexity is 240.70851071461476
At time: 586.8564457893372 and batch: 150, loss is 5.4759206199646 and perplexity is 238.8702744791611
At time: 588.514595746994 and batch: 200, loss is 5.486548242568969 and perplexity is 241.42243528728022
At time: 590.1719796657562 and batch: 250, loss is 5.523187866210938 and perplexity is 250.43211009568896
At time: 591.8283867835999 and batch: 300, loss is 5.528205413818359 and perplexity is 251.69182282195536
At time: 593.4858283996582 and batch: 350, loss is 5.460011644363403 and perplexity is 235.10016194102167
At time: 595.1477320194244 and batch: 400, loss is 5.483558950424194 and perplexity is 240.7018306833466
At time: 596.8062334060669 and batch: 450, loss is 5.45913516998291 and perplexity is 234.89419294868867
At time: 598.4650785923004 and batch: 500, loss is 5.443804149627685 and perplexity is 231.3204895517474
At time: 600.1246826648712 and batch: 550, loss is 5.458762292861938 and perplexity is 234.80662260579226
At time: 601.7865498065948 and batch: 600, loss is 5.435961561203003 and perplexity is 229.51343341883813
At time: 603.4421727657318 and batch: 650, loss is 5.4215936279296875 and perplexity is 226.23937675976597
At time: 605.0979778766632 and batch: 700, loss is 5.413605136871338 and perplexity is 224.43926518207678
At time: 606.7814936637878 and batch: 750, loss is 5.413193054199219 and perplexity is 224.34679670358955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.304143861282704 and perplexity of 201.1687003539072
Finished 22 epochs...
Completing Train Step...
At time: 611.105975151062 and batch: 50, loss is 5.480167760848999 and perplexity is 239.88694763678984
At time: 612.7924416065216 and batch: 100, loss is 5.476999053955078 and perplexity is 239.12801925785914
At time: 614.4589650630951 and batch: 150, loss is 5.468565111160278 and perplexity is 237.11970811792176
At time: 616.1193640232086 and batch: 200, loss is 5.478863172531128 and perplexity is 239.57419797642683
At time: 617.7768082618713 and batch: 250, loss is 5.51328254699707 and perplexity is 247.96374525662904
At time: 619.447053194046 and batch: 300, loss is 5.519300174713135 and perplexity is 249.46039739398108
At time: 621.1158332824707 and batch: 350, loss is 5.450523519515992 and perplexity is 232.88005125191555
At time: 622.7766737937927 and batch: 400, loss is 5.472940196990967 and perplexity is 238.15939990506155
At time: 624.4348442554474 and batch: 450, loss is 5.44922345161438 and perplexity is 232.57748809129927
At time: 626.0950591564178 and batch: 500, loss is 5.431944093704224 and perplexity is 228.59322036026177
At time: 627.7606699466705 and batch: 550, loss is 5.44796724319458 and perplexity is 232.28550572625304
At time: 629.4302866458893 and batch: 600, loss is 5.42211916923523 and perplexity is 226.35830614560842
At time: 631.08748960495 and batch: 650, loss is 5.407720317840576 and perplexity is 223.12235939967675
At time: 632.7464933395386 and batch: 700, loss is 5.398071451187134 and perplexity is 220.97983459699702
At time: 634.4070596694946 and batch: 750, loss is 5.396436634063721 and perplexity is 220.6188681169543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.284047326376272 and perplexity of 197.1662588526696
Finished 23 epochs...
Completing Train Step...
At time: 638.7902028560638 and batch: 50, loss is 5.460051307678222 and perplexity is 235.1094869776885
At time: 640.4767918586731 and batch: 100, loss is 5.456488132476807 and perplexity is 234.27324141356297
At time: 642.1388969421387 and batch: 150, loss is 5.445446901321411 and perplexity is 231.70080397330455
At time: 643.8001682758331 and batch: 200, loss is 5.454525518417358 and perplexity is 233.813904354339
At time: 645.4591722488403 and batch: 250, loss is 5.483057518005371 and perplexity is 240.58116523748333
At time: 647.1553521156311 and batch: 300, loss is 5.490429239273071 and perplexity is 242.36121548601912
At time: 648.8243029117584 and batch: 350, loss is 5.424355974197388 and perplexity is 226.86519221932136
At time: 650.4928026199341 and batch: 400, loss is 5.447894840240479 and perplexity is 232.2686881782709
At time: 652.1581699848175 and batch: 450, loss is 5.42150053024292 and perplexity is 226.21831537753206
At time: 653.81764793396 and batch: 500, loss is 5.404037551879883 and perplexity is 222.30216319175832
At time: 655.4784600734711 and batch: 550, loss is 5.4175843334198 and perplexity is 225.3341323768778
At time: 657.1497752666473 and batch: 600, loss is 5.392148094177246 and perplexity is 219.674761168446
At time: 658.816680431366 and batch: 650, loss is 5.377796983718872 and perplexity is 216.5446980673174
At time: 660.4836378097534 and batch: 700, loss is 5.373622407913208 and perplexity is 215.64260005826003
At time: 662.1528513431549 and batch: 750, loss is 5.373173885345459 and perplexity is 215.54590117300296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.267792812613553 and perplexity of 193.98732320294005
Finished 24 epochs...
Completing Train Step...
At time: 666.507152557373 and batch: 50, loss is 5.436736087799073 and perplexity is 229.6912665365259
At time: 668.1935541629791 and batch: 100, loss is 5.434427242279053 and perplexity is 229.1615566292819
At time: 669.8548657894135 and batch: 150, loss is 5.424853458404541 and perplexity is 226.97808214775316
At time: 671.5208320617676 and batch: 200, loss is 5.4287647533416745 and perplexity is 227.86759881832091
At time: 673.1865246295929 and batch: 250, loss is 5.450148181915283 and perplexity is 232.79265901404824
At time: 674.8446130752563 and batch: 300, loss is 5.46213581085205 and perplexity is 235.60008459799536
At time: 676.504075050354 and batch: 350, loss is 5.399471321105957 and perplexity is 221.2893942411809
At time: 678.1644992828369 and batch: 400, loss is 5.422350492477417 and perplexity is 226.41067413961557
At time: 679.8226253986359 and batch: 450, loss is 5.395058755874634 and perplexity is 220.3150915221082
At time: 681.4795978069305 and batch: 500, loss is 5.37694016456604 and perplexity is 216.3592378868327
At time: 683.139054775238 and batch: 550, loss is 5.390234212875367 and perplexity is 219.25473182191902
At time: 684.7987368106842 and batch: 600, loss is 5.365322551727295 and perplexity is 213.86020453554184
At time: 686.4557404518127 and batch: 650, loss is 5.351615352630615 and perplexity is 210.94877945954443
At time: 688.1121153831482 and batch: 700, loss is 5.34734561920166 and perplexity is 210.05000453419592
At time: 689.8009040355682 and batch: 750, loss is 5.349291706085205 and perplexity is 210.45917810743347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.248600094817405 and perplexity of 190.29968039433024
Finished 25 epochs...
Completing Train Step...
At time: 694.1502044200897 and batch: 50, loss is 5.411124105453491 and perplexity is 223.88311451268646
At time: 695.8334631919861 and batch: 100, loss is 5.410959281921387 and perplexity is 223.84621634790037
At time: 697.4923658370972 and batch: 150, loss is 5.398129024505615 and perplexity is 220.99255750563879
At time: 699.1514055728912 and batch: 200, loss is 5.403186922073364 and perplexity is 222.11314674860128
At time: 700.8133640289307 and batch: 250, loss is 5.428359413146973 and perplexity is 227.77525363832072
At time: 702.4705545902252 and batch: 300, loss is 5.443004636764527 and perplexity is 231.1356197575755
At time: 704.1293842792511 and batch: 350, loss is 5.38153787612915 and perplexity is 217.3562855691016
At time: 705.7887871265411 and batch: 400, loss is 5.404141969680786 and perplexity is 222.3253767067059
At time: 707.44677901268 and batch: 450, loss is 5.377393569946289 and perplexity is 216.45735857189942
At time: 709.1036334037781 and batch: 500, loss is 5.361443948745728 and perplexity is 213.03233224041318
At time: 710.7596628665924 and batch: 550, loss is 5.374206619262695 and perplexity is 215.7686177195368
At time: 712.4182035923004 and batch: 600, loss is 5.344710941314697 and perplexity is 209.4973188262067
At time: 714.0754823684692 and batch: 650, loss is 5.333605642318726 and perplexity is 207.18365918099286
At time: 715.7318382263184 and batch: 700, loss is 5.333598794937134 and perplexity is 207.182240520276
At time: 717.3930375576019 and batch: 750, loss is 5.334466905593872 and perplexity is 207.3621757216912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.23680718000545 and perplexity of 188.06867336857775
Finished 26 epochs...
Completing Train Step...
At time: 721.712620973587 and batch: 50, loss is 5.394855403900147 and perplexity is 220.2702945681668
At time: 723.3967444896698 and batch: 100, loss is 5.396222009658813 and perplexity is 220.57152300456326
At time: 725.054634809494 and batch: 150, loss is 5.3834912300109865 and perplexity is 217.78127425477703
At time: 726.7135584354401 and batch: 200, loss is 5.388873949050903 and perplexity is 218.9566902953885
At time: 728.3754873275757 and batch: 250, loss is 5.411990642547607 and perplexity is 224.07720161585036
At time: 730.0578513145447 and batch: 300, loss is 5.4277294254302975 and perplexity is 227.631803217112
At time: 731.7166140079498 and batch: 350, loss is 5.3686329364776615 and perplexity is 214.56933719872924
At time: 733.3775899410248 and batch: 400, loss is 5.392125482559204 and perplexity is 219.6697940228108
At time: 735.0379672050476 and batch: 450, loss is 5.364330654144287 and perplexity is 213.64818228512368
At time: 736.7014045715332 and batch: 500, loss is 5.349532432556153 and perplexity is 210.5098473011211
At time: 738.3577609062195 and batch: 550, loss is 5.362745876312256 and perplexity is 213.30986553123785
At time: 740.018084526062 and batch: 600, loss is 5.331358432769775 and perplexity is 206.71859682568956
At time: 741.6777811050415 and batch: 650, loss is 5.321219234466553 and perplexity is 204.6332258397196
At time: 743.3345768451691 and batch: 700, loss is 5.320839376449585 and perplexity is 204.5555090299574
At time: 744.9913990497589 and batch: 750, loss is 5.323232517242432 and perplexity is 205.04562538783023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.2279169836709665 and perplexity of 186.4041160220339
Finished 27 epochs...
Completing Train Step...
At time: 749.3271758556366 and batch: 50, loss is 5.3843432140350345 and perplexity is 217.9668994848157
At time: 751.0255749225616 and batch: 100, loss is 5.384951953887939 and perplexity is 218.099625016708
At time: 752.6847183704376 and batch: 150, loss is 5.3725346660614015 and perplexity is 215.40816410315927
At time: 754.3455142974854 and batch: 200, loss is 5.379476842880249 and perplexity is 216.90876837001508
At time: 756.0045382976532 and batch: 250, loss is 5.401708488464355 and perplexity is 221.78500983153793
At time: 757.6620802879333 and batch: 300, loss is 5.419112997055054 and perplexity is 225.67885588684123
At time: 759.3208630084991 and batch: 350, loss is 5.358597202301025 and perplexity is 212.42674559079308
At time: 760.980565071106 and batch: 400, loss is 5.382917537689209 and perplexity is 217.65637064145776
At time: 762.6406733989716 and batch: 450, loss is 5.354988403320313 and perplexity is 211.66152176779156
At time: 764.297381401062 and batch: 500, loss is 5.3394691753387455 and perplexity is 208.40205597190837
At time: 765.9542105197906 and batch: 550, loss is 5.352604084014892 and perplexity is 211.15745428294466
At time: 767.6125204563141 and batch: 600, loss is 5.321318206787109 and perplexity is 204.6534798672213
At time: 769.2723727226257 and batch: 650, loss is 5.311440887451172 and perplexity is 202.6420024492848
At time: 770.9392187595367 and batch: 700, loss is 5.312361602783203 and perplexity is 202.82866396572697
At time: 772.6311283111572 and batch: 750, loss is 5.313569602966308 and perplexity is 203.07382907887396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.224500789198765 and perplexity of 185.76840977760747
Finished 28 epochs...
Completing Train Step...
At time: 776.9788854122162 and batch: 50, loss is 5.373687896728516 and perplexity is 215.65672269910016
At time: 778.6707792282104 and batch: 100, loss is 5.374645547866821 and perplexity is 215.8633455255776
At time: 780.337724685669 and batch: 150, loss is 5.3626462173461915 and perplexity is 213.2886083498396
At time: 782.0111107826233 and batch: 200, loss is 5.368273620605469 and perplexity is 214.49225287982682
At time: 783.6777610778809 and batch: 250, loss is 5.39247275352478 and perplexity is 219.74609221159503
At time: 785.3429939746857 and batch: 300, loss is 5.4089484214782715 and perplexity is 223.3965451106797
At time: 787.0158863067627 and batch: 350, loss is 5.350124568939209 and perplexity is 210.63453475304487
At time: 788.6890733242035 and batch: 400, loss is 5.372387905120849 and perplexity is 215.37655291809318
At time: 790.386088848114 and batch: 450, loss is 5.346094474792481 and perplexity is 209.78736597901516
At time: 792.0917780399323 and batch: 500, loss is 5.331078052520752 and perplexity is 206.66064513866766
At time: 793.7974817752838 and batch: 550, loss is 5.343801794052124 and perplexity is 209.30694146592032
At time: 795.5045621395111 and batch: 600, loss is 5.3101126575469975 and perplexity is 202.37302595264688
At time: 797.2117068767548 and batch: 650, loss is 5.301015148162842 and perplexity is 200.5402847804509
At time: 798.9165086746216 and batch: 700, loss is 5.30179518699646 and perplexity is 200.69677501658037
At time: 800.6172559261322 and batch: 750, loss is 5.302613401412964 and perplexity is 200.86105521029108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.220553021098292 and perplexity of 185.03648486091035
Finished 29 epochs...
Completing Train Step...
At time: 805.176301240921 and batch: 50, loss is 5.3626398181915285 and perplexity is 213.2872434874139
At time: 806.8666803836823 and batch: 100, loss is 5.363576850891113 and perplexity is 213.48719427454859
At time: 808.533441066742 and batch: 150, loss is 5.35248064994812 and perplexity is 211.13139186816127
At time: 810.2030832767487 and batch: 200, loss is 5.35778468132019 and perplexity is 212.2542145051654
At time: 811.8727369308472 and batch: 250, loss is 5.381516675949097 and perplexity is 217.35167762555662
At time: 813.582902431488 and batch: 300, loss is 5.397051906585693 and perplexity is 220.75465061167068
At time: 815.2496991157532 and batch: 350, loss is 5.338347301483155 and perplexity is 208.16838625234476
At time: 816.9178833961487 and batch: 400, loss is 5.3616813278198245 and perplexity is 213.08290766072884
At time: 818.5864479541779 and batch: 450, loss is 5.336108303070068 and perplexity is 207.7028189625776
At time: 820.2518594264984 and batch: 500, loss is 5.323687324523926 and perplexity is 205.1389028413197
At time: 821.916672706604 and batch: 550, loss is 5.332273921966553 and perplexity is 206.90793212184172
At time: 823.5825490951538 and batch: 600, loss is 5.299245090484619 and perplexity is 200.18563088110838
At time: 825.2501764297485 and batch: 650, loss is 5.288209018707275 and perplexity is 197.9885139596897
At time: 826.9164483547211 and batch: 700, loss is 5.290500869750977 and perplexity is 198.44279451488603
At time: 828.5801753997803 and batch: 750, loss is 5.291108551025391 and perplexity is 198.56342113271154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.213052439135175 and perplexity of 183.6537955098039
Finished 30 epochs...
Completing Train Step...
At time: 832.9483306407928 and batch: 50, loss is 5.350924730300903 and perplexity is 210.80314381742593
At time: 834.644793510437 and batch: 100, loss is 5.3521186637878415 and perplexity is 211.05497905732904
At time: 836.31263256073 and batch: 150, loss is 5.3401062870025635 and perplexity is 208.53487365788746
At time: 837.9841315746307 and batch: 200, loss is 5.347302646636963 and perplexity is 210.04097834072704
At time: 839.6525630950928 and batch: 250, loss is 5.36672869682312 and perplexity is 214.1611345393819
At time: 841.3193907737732 and batch: 300, loss is 5.3831272792816165 and perplexity is 217.7020270230875
At time: 842.9897089004517 and batch: 350, loss is 5.324563837051391 and perplexity is 205.31878848402354
At time: 844.6622948646545 and batch: 400, loss is 5.3481599140167235 and perplexity is 210.22111682226154
At time: 846.3327693939209 and batch: 450, loss is 5.323031616210938 and perplexity is 205.0044356478556
At time: 847.9990713596344 and batch: 500, loss is 5.309737462997436 and perplexity is 202.2971109386715
At time: 849.6648936271667 and batch: 550, loss is 5.320952835083008 and perplexity is 204.57871893512836
At time: 851.3326354026794 and batch: 600, loss is 5.286734027862549 and perplexity is 197.69669798009352
At time: 853.0081667900085 and batch: 650, loss is 5.2792791080474855 and perplexity is 196.2283649019416
At time: 854.685045003891 and batch: 700, loss is 5.280421466827392 and perplexity is 196.45265618362018
At time: 856.395783662796 and batch: 750, loss is 5.280507888793945 and perplexity is 196.46963474215175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.203734996706941 and perplexity of 181.95055906930298
Finished 31 epochs...
Completing Train Step...
At time: 860.7575972080231 and batch: 50, loss is 5.339777307510376 and perplexity is 208.46628124441543
At time: 862.450183391571 and batch: 100, loss is 5.340369329452515 and perplexity is 208.58973439699005
At time: 864.1171271800995 and batch: 150, loss is 5.330809535980225 and perplexity is 206.60516078673842
At time: 865.7844235897064 and batch: 200, loss is 5.337103481292725 and perplexity is 207.90962317124809
At time: 867.4534804821014 and batch: 250, loss is 5.356316890716553 and perplexity is 211.94289829292967
At time: 869.1198844909668 and batch: 300, loss is 5.374556570053101 and perplexity is 215.8441393315056
At time: 870.7862331867218 and batch: 350, loss is 5.3150018692016605 and perplexity is 203.36489325848862
At time: 872.4517357349396 and batch: 400, loss is 5.338654680252075 and perplexity is 208.23238262969915
At time: 874.1198647022247 and batch: 450, loss is 5.314335479736328 and perplexity is 203.22941818059843
At time: 875.790417432785 and batch: 500, loss is 5.301970691680908 and perplexity is 200.73200133185057
At time: 877.4580624103546 and batch: 550, loss is 5.3135374069213865 and perplexity is 203.06729101000107
At time: 879.1235916614532 and batch: 600, loss is 5.277943000793457 and perplexity is 195.9663578338997
At time: 880.7909343242645 and batch: 650, loss is 5.268778429031372 and perplexity is 194.1786145479971
At time: 882.4593937397003 and batch: 700, loss is 5.269534969329834 and perplexity is 194.32557407839627
At time: 884.1249995231628 and batch: 750, loss is 5.27087140083313 and perplexity is 194.58545051233793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.200012561886809 and perplexity of 181.2745190108378
Finished 32 epochs...
Completing Train Step...
At time: 888.485954284668 and batch: 50, loss is 5.329576187133789 and perplexity is 206.3505016240878
At time: 890.1769914627075 and batch: 100, loss is 5.331953802108765 and perplexity is 206.84170738452795
At time: 891.8426690101624 and batch: 150, loss is 5.316798686981201 and perplexity is 203.73063139842566
At time: 893.5095191001892 and batch: 200, loss is 5.3251947498321535 and perplexity is 205.44836760407196
At time: 895.1761209964752 and batch: 250, loss is 5.346339569091797 and perplexity is 209.83878996809108
At time: 896.8773880004883 and batch: 300, loss is 5.363434953689575 and perplexity is 213.4569031882778
At time: 898.5407471656799 and batch: 350, loss is 5.303588342666626 and perplexity is 201.05697843056575
At time: 900.2071986198425 and batch: 400, loss is 5.326900806427002 and perplexity is 205.79917330862781
At time: 901.8740878105164 and batch: 450, loss is 5.30286732673645 and perplexity is 200.91206539482585
At time: 903.5404906272888 and batch: 500, loss is 5.287185373306275 and perplexity is 197.7859476236614
At time: 905.2035853862762 and batch: 550, loss is 5.299501991271972 and perplexity is 200.2370653337914
At time: 906.8710470199585 and batch: 600, loss is 5.26670973777771 and perplexity is 193.77733415216193
At time: 908.5375068187714 and batch: 650, loss is 5.256929035186768 and perplexity is 191.89129411150557
At time: 910.2053158283234 and batch: 700, loss is 5.258023357391357 and perplexity is 192.1013999562973
At time: 911.8710856437683 and batch: 750, loss is 5.260304117202759 and perplexity is 192.54003713190838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.18763094170149 and perplexity of 179.0438846977083
Finished 33 epochs...
Completing Train Step...
At time: 916.2385022640228 and batch: 50, loss is 5.31747296333313 and perplexity is 203.86804846867523
At time: 917.9446115493774 and batch: 100, loss is 5.319419431686401 and perplexity is 204.26525762537906
At time: 919.6112198829651 and batch: 150, loss is 5.30489486694336 and perplexity is 201.31983593135342
At time: 921.2762937545776 and batch: 200, loss is 5.311692810058593 and perplexity is 202.69305898179223
At time: 922.9444468021393 and batch: 250, loss is 5.333713693618774 and perplexity is 207.20604685420304
At time: 924.6128861904144 and batch: 300, loss is 5.349654216766357 and perplexity is 210.53548563775553
At time: 926.2787253856659 and batch: 350, loss is 5.290007467269898 and perplexity is 198.34490649880095
At time: 927.9444990158081 and batch: 400, loss is 5.314131450653076 and perplexity is 203.18795769843325
At time: 929.6110112667084 and batch: 450, loss is 5.290675649642944 and perplexity is 198.47748135626594
At time: 931.2772541046143 and batch: 500, loss is 5.2779405975341795 and perplexity is 195.96588687649802
At time: 932.940098285675 and batch: 550, loss is 5.290581951141357 and perplexity is 198.4588851848944
At time: 934.6054532527924 and batch: 600, loss is 5.256766891479492 and perplexity is 191.86018266801494
At time: 936.278350353241 and batch: 650, loss is 5.245427255630493 and perplexity is 189.69684696360667
At time: 937.9465489387512 and batch: 700, loss is 5.2466569423675535 and perplexity is 189.93025814228304
At time: 939.6377322673798 and batch: 750, loss is 5.249462604522705 and perplexity is 190.46388652009466
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.185681720112646 and perplexity of 178.6952284068819
Finished 34 epochs...
Completing Train Step...
At time: 943.9909002780914 and batch: 50, loss is 5.3055996799469 and perplexity is 201.46177878529699
At time: 945.6835649013519 and batch: 100, loss is 5.306654024124145 and perplexity is 201.67430085471267
At time: 947.3486006259918 and batch: 150, loss is 5.2927313327789305 and perplexity is 198.8859078214329
At time: 949.014951467514 and batch: 200, loss is 5.298452196121215 and perplexity is 200.02696773261883
At time: 950.681741476059 and batch: 250, loss is 5.318603277206421 and perplexity is 204.09861363313445
At time: 952.3493564128876 and batch: 300, loss is 5.336757326126099 and perplexity is 207.8376666357779
At time: 954.01260638237 and batch: 350, loss is 5.2780234909057615 and perplexity is 195.98213182286614
At time: 955.6791830062866 and batch: 400, loss is 5.302330341339111 and perplexity is 200.80420751120556
At time: 957.3481774330139 and batch: 450, loss is 5.278916387557984 and perplexity is 196.15720176031525
At time: 959.017413854599 and batch: 500, loss is 5.264909954071045 and perplexity is 193.4288905190778
At time: 960.6819286346436 and batch: 550, loss is 5.277828922271729 and perplexity is 195.94400355658516
At time: 962.3467893600464 and batch: 600, loss is 5.241674222946167 and perplexity is 188.9862427912674
At time: 964.013513803482 and batch: 650, loss is 5.232781896591186 and perplexity is 187.31316524420444
At time: 965.6823401451111 and batch: 700, loss is 5.233673582077026 and perplexity is 187.4802641637283
At time: 967.3481440544128 and batch: 750, loss is 5.235046234130859 and perplexity is 187.73778603691522
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.177240061205487 and perplexity of 177.19309341631427
Finished 35 epochs...
Completing Train Step...
At time: 971.7176067829132 and batch: 50, loss is 5.291961421966553 and perplexity is 198.73284234151663
At time: 973.4127838611603 and batch: 100, loss is 5.292993946075439 and perplexity is 198.93814476407272
At time: 975.0847175121307 and batch: 150, loss is 5.277838687896729 and perplexity is 195.9459170815883
At time: 976.751711845398 and batch: 200, loss is 5.2845814990997315 and perplexity is 197.27160782490398
At time: 978.4180293083191 and batch: 250, loss is 5.3077311515808105 and perplexity is 201.89164681510402
At time: 980.1120247840881 and batch: 300, loss is 5.325263185501099 and perplexity is 205.4624280816562
At time: 981.7754390239716 and batch: 350, loss is 5.269158878326416 and perplexity is 194.25250371966484
At time: 983.4421243667603 and batch: 400, loss is 5.294104776382446 and perplexity is 199.15925406921866
At time: 985.109356880188 and batch: 450, loss is 5.268484191894531 and perplexity is 194.121488393147
At time: 986.7763900756836 and batch: 500, loss is 5.253964853286743 and perplexity is 191.32333559276657
At time: 988.4409921169281 and batch: 550, loss is 5.270111999511719 and perplexity is 194.43773815766883
At time: 990.1068685054779 and batch: 600, loss is 5.233110694885254 and perplexity is 187.37476361955862
At time: 991.7727944850922 and batch: 650, loss is 5.224412975311279 and perplexity is 185.75209744760784
At time: 993.4404804706573 and batch: 700, loss is 5.2239346408844 and perplexity is 185.66326707154084
At time: 995.1063992977142 and batch: 750, loss is 5.225115032196045 and perplexity is 185.88255177430148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.171010571856831 and perplexity of 176.09270192528797
Finished 36 epochs...
Completing Train Step...
At time: 999.450537443161 and batch: 50, loss is 5.281817903518677 and perplexity is 196.72718151482673
At time: 1001.1444180011749 and batch: 100, loss is 5.284038753509521 and perplexity is 197.16456857985
At time: 1002.8113303184509 and batch: 150, loss is 5.267871475219726 and perplexity is 194.00258335154135
At time: 1004.4772667884827 and batch: 200, loss is 5.277812786102295 and perplexity is 195.9408417964537
At time: 1006.1442315578461 and batch: 250, loss is 5.301639261245728 and perplexity is 200.6654836608938
At time: 1007.8133080005646 and batch: 300, loss is 5.320002088546753 and perplexity is 204.38430885870682
At time: 1009.479484796524 and batch: 350, loss is 5.263844718933106 and perplexity is 193.22295297363272
At time: 1011.1451621055603 and batch: 400, loss is 5.2895675373077395 and perplexity is 198.2576678224515
At time: 1012.8165469169617 and batch: 450, loss is 5.262370557785034 and perplexity is 192.9383210516494
At time: 1014.4868972301483 and batch: 500, loss is 5.248188524246216 and perplexity is 190.2213747614256
At time: 1016.1545126438141 and batch: 550, loss is 5.261166162490845 and perplexity is 192.70608692462358
At time: 1017.8239574432373 and batch: 600, loss is 5.225292329788208 and perplexity is 185.9155112248855
At time: 1019.4898371696472 and batch: 650, loss is 5.2160092544555665 and perplexity is 184.1976294781027
At time: 1021.1582152843475 and batch: 700, loss is 5.216412019729614 and perplexity is 184.27183282908183
At time: 1022.8697719573975 and batch: 750, loss is 5.219692153930664 and perplexity is 184.8772615711648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.169230261514353 and perplexity of 175.77948116462485
Finished 37 epochs...
Completing Train Step...
At time: 1027.2179787158966 and batch: 50, loss is 5.276307010650635 and perplexity is 195.64602090963143
At time: 1028.917311668396 and batch: 100, loss is 5.277983446121215 and perplexity is 195.9742839177573
At time: 1030.584034204483 and batch: 150, loss is 5.2634924793243405 and perplexity is 193.15490418171635
At time: 1032.2568669319153 and batch: 200, loss is 5.273047580718994 and perplexity is 195.00936454512419
At time: 1033.9240806102753 and batch: 250, loss is 5.2938221836090085 and perplexity is 199.10298105480354
At time: 1035.5921442508698 and batch: 300, loss is 5.31215458869934 and perplexity is 202.78667992146922
At time: 1037.2656638622284 and batch: 350, loss is 5.255378408432007 and perplexity is 191.5939729135271
At time: 1038.9315190315247 and batch: 400, loss is 5.282364006042481 and perplexity is 196.83464406526707
At time: 1040.598905801773 and batch: 450, loss is 5.254817018508911 and perplexity is 191.48644417340716
At time: 1042.2672038078308 and batch: 500, loss is 5.240411720275879 and perplexity is 188.74779770554613
At time: 1043.9346640110016 and batch: 550, loss is 5.254867715835571 and perplexity is 191.49615227030364
At time: 1045.5987524986267 and batch: 600, loss is 5.219287166595459 and perplexity is 184.80240378091273
At time: 1047.2656314373016 and batch: 650, loss is 5.209643669128418 and perplexity is 183.02882775088105
At time: 1048.9345829486847 and batch: 700, loss is 5.212186336517334 and perplexity is 183.49480133930902
At time: 1050.6028866767883 and batch: 750, loss is 5.214130544662476 and perplexity is 183.85190045168096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.163571025049964 and perplexity of 174.78751305365446
Finished 38 epochs...
Completing Train Step...
At time: 1054.9504036903381 and batch: 50, loss is 5.271251535415649 and perplexity is 194.65943323213702
At time: 1056.6458230018616 and batch: 100, loss is 5.272149114608765 and perplexity is 194.8342339261472
At time: 1058.3135480880737 and batch: 150, loss is 5.256583595275879 and perplexity is 191.82501864772163
At time: 1059.9799664020538 and batch: 200, loss is 5.2641850852966305 and perplexity is 193.28873076112401
At time: 1061.6467561721802 and batch: 250, loss is 5.287433347702026 and perplexity is 197.8349995560718
At time: 1063.3417706489563 and batch: 300, loss is 5.30551290512085 and perplexity is 201.44429773295576
At time: 1065.008781671524 and batch: 350, loss is 5.251747188568115 and perplexity is 190.8995147015881
At time: 1066.6730823516846 and batch: 400, loss is 5.277946481704712 and perplexity is 195.9670399765874
At time: 1068.346593618393 and batch: 450, loss is 5.251345329284668 and perplexity is 190.82281537160077
At time: 1070.0145099163055 and batch: 500, loss is 5.236026821136474 and perplexity is 187.92196955959196
At time: 1071.683256149292 and batch: 550, loss is 5.250377426147461 and perplexity is 190.6382067260079
At time: 1073.3483262062073 and batch: 600, loss is 5.2137211990356445 and perplexity is 183.77665688161028
At time: 1075.0148329734802 and batch: 650, loss is 5.204737300872803 and perplexity is 182.1330202981887
At time: 1076.6830945014954 and batch: 700, loss is 5.208201179504394 and perplexity is 182.76500089545092
At time: 1078.358546257019 and batch: 750, loss is 5.209012365341186 and perplexity is 182.9133174236391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.160909963208575 and perplexity of 174.32301098079142
Finished 39 epochs...
Completing Train Step...
At time: 1082.724068403244 and batch: 50, loss is 5.264177131652832 and perplexity is 193.287193417523
At time: 1084.4186577796936 and batch: 100, loss is 5.26590666770935 and perplexity is 193.6217798440423
At time: 1086.084538936615 and batch: 150, loss is 5.249950866699219 and perplexity is 190.5569055388606
At time: 1087.7497153282166 and batch: 200, loss is 5.258896780014038 and perplexity is 192.26925896012236
At time: 1089.416337966919 and batch: 250, loss is 5.2833531665802 and perplexity is 197.02944145469465
At time: 1091.0843560695648 and batch: 300, loss is 5.303647012710571 and perplexity is 201.06877479836902
At time: 1092.7529609203339 and batch: 350, loss is 5.243870115280151 and perplexity is 189.4016922071955
At time: 1094.4183609485626 and batch: 400, loss is 5.272070016860962 and perplexity is 194.81882358651828
At time: 1096.0837733745575 and batch: 450, loss is 5.242585945129394 and perplexity is 189.158624311232
At time: 1097.7502117156982 and batch: 500, loss is 5.229595813751221 and perplexity is 186.71731969340436
At time: 1099.4176189899445 and batch: 550, loss is 5.2401590919494625 and perplexity is 188.70012068783316
At time: 1101.0813586711884 and batch: 600, loss is 5.2053078937530515 and perplexity is 182.23697375756055
At time: 1102.7460205554962 and batch: 650, loss is 5.196319589614868 and perplexity is 180.60631183323045
At time: 1104.4115662574768 and batch: 700, loss is 5.1967177581787105 and perplexity is 180.67823790743455
At time: 1106.1043779850006 and batch: 750, loss is 5.19892912864685 and perplexity is 181.07822652548668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.155569032181141 and perplexity of 173.39444571149534
Finished 40 epochs...
Completing Train Step...
At time: 1110.4527928829193 and batch: 50, loss is 5.256274185180664 and perplexity is 191.76567523163664
At time: 1112.1570534706116 and batch: 100, loss is 5.257877445220947 and perplexity is 192.0733720689455
At time: 1113.8265063762665 and batch: 150, loss is 5.241947803497315 and perplexity is 189.03795282483674
At time: 1115.491604566574 and batch: 200, loss is 5.249682340621948 and perplexity is 190.50574291007683
At time: 1117.1576344966888 and batch: 250, loss is 5.276451654434204 and perplexity is 195.67432193707057
At time: 1118.825236082077 and batch: 300, loss is 5.296786117553711 and perplexity is 199.69398455385036
At time: 1120.4925866127014 and batch: 350, loss is 5.238414134979248 and perplexity is 188.37113421405573
At time: 1122.1573412418365 and batch: 400, loss is 5.263190956115722 and perplexity is 193.09667227482404
At time: 1123.8231167793274 and batch: 450, loss is 5.237451820373535 and perplexity is 188.18994911282374
At time: 1125.4904525279999 and batch: 500, loss is 5.223441534042358 and perplexity is 185.57173781293858
At time: 1127.1564779281616 and batch: 550, loss is 5.237817087173462 and perplexity is 188.2587012089792
At time: 1128.8203735351562 and batch: 600, loss is 5.202446851730347 and perplexity is 181.7163312627238
At time: 1130.4851059913635 and batch: 650, loss is 5.191365299224853 and perplexity is 179.71374855264335
At time: 1132.1515126228333 and batch: 700, loss is 5.192303676605224 and perplexity is 179.882467017642
At time: 1133.8190474510193 and batch: 750, loss is 5.195038690567016 and perplexity is 180.37512147772992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.155026812886083 and perplexity of 173.30045338190487
Finished 41 epochs...
Completing Train Step...
At time: 1138.2233781814575 and batch: 50, loss is 5.253410062789917 and perplexity is 191.2172206628577
At time: 1139.9175817966461 and batch: 100, loss is 5.252438220977783 and perplexity is 191.0314780434586
At time: 1141.5858743190765 and batch: 150, loss is 5.239502611160279 and perplexity is 188.57628333655347
At time: 1143.2508075237274 and batch: 200, loss is 5.246167888641358 and perplexity is 189.83739475126578
At time: 1144.9164516925812 and batch: 250, loss is 5.2712728309631345 and perplexity is 194.66357865548025
At time: 1146.6087701320648 and batch: 300, loss is 5.2899244689941405 and perplexity is 198.3284448967081
At time: 1148.2762157917023 and batch: 350, loss is 5.23373631477356 and perplexity is 187.4920256751579
At time: 1149.9427213668823 and batch: 400, loss is 5.261528129577637 and perplexity is 192.77585281123038
At time: 1151.612455368042 and batch: 450, loss is 5.234341421127319 and perplexity is 187.60551262354718
At time: 1153.2858593463898 and batch: 500, loss is 5.22036075592041 and perplexity is 185.00091220803478
At time: 1154.9528396129608 and batch: 550, loss is 5.234579257965088 and perplexity is 187.65013743191878
At time: 1156.6169953346252 and batch: 600, loss is 5.1981579685211186 and perplexity is 180.93864004623302
At time: 1158.2813148498535 and batch: 650, loss is 5.187987384796142 and perplexity is 179.10771502931323
At time: 1159.9476552009583 and batch: 700, loss is 5.190417413711548 and perplexity is 179.54348120357386
At time: 1161.614785194397 and batch: 750, loss is 5.193628873825073 and perplexity is 180.12100478271594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.152372493300327 and perplexity of 172.84106854107605
Finished 42 epochs...
Completing Train Step...
At time: 1165.9614524841309 and batch: 50, loss is 5.248778400421142 and perplexity is 190.33361491900195
At time: 1167.655179977417 and batch: 100, loss is 5.247752952575683 and perplexity is 190.13853776148466
At time: 1169.3238480091095 and batch: 150, loss is 5.232810192108154 and perplexity is 187.31846544203546
At time: 1170.9889628887177 and batch: 200, loss is 5.2405080986022945 and perplexity is 188.76598977905033
At time: 1172.6535131931305 and batch: 250, loss is 5.265639190673828 and perplexity is 193.56999738997447
At time: 1174.3194394111633 and batch: 300, loss is 5.286603507995605 and perplexity is 197.6708963172294
At time: 1175.9864754676819 and batch: 350, loss is 5.230194520950318 and perplexity is 186.8291421680209
At time: 1177.6585609912872 and batch: 400, loss is 5.25730679512024 and perplexity is 191.96379664741738
At time: 1179.32381606102 and batch: 450, loss is 5.229699354171753 and perplexity is 186.73665348410324
At time: 1180.9904503822327 and batch: 500, loss is 5.216166963577271 and perplexity is 184.22668141528524
At time: 1182.6566772460938 and batch: 550, loss is 5.2290214157104495 and perplexity is 186.6101004270106
At time: 1184.321834564209 and batch: 600, loss is 5.192438411712646 and perplexity is 179.90670513398493
At time: 1185.987133026123 and batch: 650, loss is 5.18530306816101 and perplexity is 178.62757791870072
At time: 1187.6671123504639 and batch: 700, loss is 5.185670347213745 and perplexity is 178.6931961356716
At time: 1189.3656480312347 and batch: 750, loss is 5.189464273452759 and perplexity is 179.37243261303408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.148315074831941 and perplexity of 172.14120078579893
Finished 43 epochs...
Completing Train Step...
At time: 1193.7177846431732 and batch: 50, loss is 5.246225643157959 and perplexity is 189.84835903484787
At time: 1195.428332567215 and batch: 100, loss is 5.243420886993408 and perplexity is 189.31662671784164
At time: 1197.1004884243011 and batch: 150, loss is 5.228770313262939 and perplexity is 186.5632480566818
At time: 1198.766744852066 and batch: 200, loss is 5.234442281723022 and perplexity is 187.62443558158193
At time: 1200.4324481487274 and batch: 250, loss is 5.2593026447296145 and perplexity is 192.34731010625597
At time: 1202.0997822284698 and batch: 300, loss is 5.2793936347961425 and perplexity is 196.25083958551977
At time: 1203.7683987617493 and batch: 350, loss is 5.223165121078491 and perplexity is 185.52045046744988
At time: 1205.43612742424 and batch: 400, loss is 5.252585563659668 and perplexity is 191.05962720749366
At time: 1207.1008253097534 and batch: 450, loss is 5.224480276107788 and perplexity is 185.76459913240126
At time: 1208.767518043518 and batch: 500, loss is 5.209818620681762 and perplexity is 183.06085172984353
At time: 1210.4373412132263 and batch: 550, loss is 5.221656618118286 and perplexity is 185.240803296064
At time: 1212.1056847572327 and batch: 600, loss is 5.1851914024353025 and perplexity is 178.6076324542141
At time: 1213.7695889472961 and batch: 650, loss is 5.174445867538452 and perplexity is 176.6986726721873
At time: 1215.4364202022552 and batch: 700, loss is 5.1790214538574215 and perplexity is 177.50902520663743
At time: 1217.1039018630981 and batch: 750, loss is 5.181478137969971 and perplexity is 177.94564490744366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.147004948105923 and perplexity of 171.91582166784087
Finished 44 epochs...
Completing Train Step...
At time: 1221.4515187740326 and batch: 50, loss is 5.237643098831176 and perplexity is 188.22594923894812
At time: 1223.1449358463287 and batch: 100, loss is 5.2370819568634035 and perplexity is 188.12035738818645
At time: 1224.8147246837616 and batch: 150, loss is 5.221372833251953 and perplexity is 185.18824221783336
At time: 1226.4838137626648 and batch: 200, loss is 5.228967399597168 and perplexity is 186.60002074692142
At time: 1228.1487786769867 and batch: 250, loss is 5.250708150863647 and perplexity is 190.70126591986389
At time: 1229.8442313671112 and batch: 300, loss is 5.272277202606201 and perplexity is 194.85919145134838
At time: 1231.5125529766083 and batch: 350, loss is 5.216961469650268 and perplexity is 184.37310879349593
At time: 1233.182201385498 and batch: 400, loss is 5.2444917106628415 and perplexity is 189.5194600227185
At time: 1234.8489546775818 and batch: 450, loss is 5.2174920558929445 and perplexity is 184.47096058565293
At time: 1236.5163102149963 and batch: 500, loss is 5.20298204421997 and perplexity is 181.8136105076962
At time: 1238.1862182617188 and batch: 550, loss is 5.215776805877685 and perplexity is 184.1548179770105
At time: 1239.8565542697906 and batch: 600, loss is 5.180727338790893 and perplexity is 177.81209360470453
At time: 1241.5202505588531 and batch: 650, loss is 5.171807861328125 and perplexity is 176.23315476584997
At time: 1243.1946828365326 and batch: 700, loss is 5.170439672470093 and perplexity is 175.9921994008942
At time: 1244.8628706932068 and batch: 750, loss is 5.173133525848389 and perplexity is 176.466935729745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.14018888251726 and perplexity of 170.7480165908507
Finished 45 epochs...
Completing Train Step...
At time: 1249.2503254413605 and batch: 50, loss is 5.231047058105469 and perplexity is 186.9884888682241
At time: 1250.9443500041962 and batch: 100, loss is 5.230033092498779 and perplexity is 186.79898506307129
At time: 1252.614090204239 and batch: 150, loss is 5.217212591171265 and perplexity is 184.41941466296385
At time: 1254.2823162078857 and batch: 200, loss is 5.22230393409729 and perplexity is 185.36075144598988
At time: 1255.9475259780884 and batch: 250, loss is 5.242993860244751 and perplexity is 189.23580071292892
At time: 1257.6179676055908 and batch: 300, loss is 5.263515529632568 and perplexity is 193.15935651310699
At time: 1259.2947914600372 and batch: 350, loss is 5.209593744277954 and perplexity is 183.01969029211995
At time: 1260.9662399291992 and batch: 400, loss is 5.235181865692138 and perplexity is 187.76325093282924
At time: 1262.6307129859924 and batch: 450, loss is 5.207661180496216 and perplexity is 182.66633461848113
At time: 1264.297042131424 and batch: 500, loss is 5.196583909988403 and perplexity is 180.6540560706464
At time: 1265.966135263443 and batch: 550, loss is 5.210814075469971 and perplexity is 183.24317126160147
At time: 1267.6348526477814 and batch: 600, loss is 5.176048860549927 and perplexity is 176.98214655224118
At time: 1269.2994530200958 and batch: 650, loss is 5.1662748908996585 and perplexity is 175.2607545435869
At time: 1270.9644091129303 and batch: 700, loss is 5.168841781616211 and perplexity is 175.71120763199275
At time: 1272.6762795448303 and batch: 750, loss is 5.169563322067261 and perplexity is 175.8380361264441
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.137990552325581 and perplexity of 170.37306835182642
Finished 46 epochs...
Completing Train Step...
At time: 1277.0544843673706 and batch: 50, loss is 5.227505397796631 and perplexity is 186.32741050749723
At time: 1278.7454526424408 and batch: 100, loss is 5.22905065536499 and perplexity is 186.61555692165342
At time: 1280.4127612113953 and batch: 150, loss is 5.210499811172485 and perplexity is 183.1855935229036
At time: 1282.0803363323212 and batch: 200, loss is 5.218330736160278 and perplexity is 184.62573763534866
At time: 1283.744014263153 and batch: 250, loss is 5.2395562744140625 and perplexity is 188.5864032250344
At time: 1285.4101300239563 and batch: 300, loss is 5.259868888854981 and perplexity is 192.4562564828455
At time: 1287.0776116847992 and batch: 350, loss is 5.203956813812256 and perplexity is 181.99092329219818
At time: 1288.7450425624847 and batch: 400, loss is 5.232473526000977 and perplexity is 187.25541227799985
At time: 1290.408851146698 and batch: 450, loss is 5.200947437286377 and perplexity is 181.44406734016192
At time: 1292.0736992359161 and batch: 500, loss is 5.186103019714356 and perplexity is 178.77052849627225
At time: 1293.7413828372955 and batch: 550, loss is 5.201213798522949 and perplexity is 181.49240344345367
At time: 1295.414583683014 and batch: 600, loss is 5.16759503364563 and perplexity is 175.492276544772
At time: 1297.0797498226166 and batch: 650, loss is 5.156692552566528 and perplexity is 173.58936738469768
At time: 1298.7444112300873 and batch: 700, loss is 5.1574626159667964 and perplexity is 173.72309368545302
At time: 1300.4189839363098 and batch: 750, loss is 5.159684038162231 and perplexity is 174.109434976311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.134251439294149 and perplexity of 169.73721369877845
Finished 47 epochs...
Completing Train Step...
At time: 1304.773342370987 and batch: 50, loss is 5.216543550491333 and perplexity is 184.2960718376698
At time: 1306.467777967453 and batch: 100, loss is 5.215988254547119 and perplexity is 184.1937613853623
At time: 1308.141932964325 and batch: 150, loss is 5.2000463008880615 and perplexity is 181.2806351352371
At time: 1309.809404373169 and batch: 200, loss is 5.206513204574585 and perplexity is 182.456758381899
At time: 1311.4728758335114 and batch: 250, loss is 5.228545408248902 and perplexity is 186.52129376480093
At time: 1313.1877019405365 and batch: 300, loss is 5.247072162628174 and perplexity is 190.0091374085701
At time: 1314.8541142940521 and batch: 350, loss is 5.1924851036071775 and perplexity is 179.91510551499982
At time: 1316.521897315979 and batch: 400, loss is 5.220053520202637 and perplexity is 184.9440820505579
At time: 1318.1889326572418 and batch: 450, loss is 5.18429744720459 and perplexity is 178.44803657335117
At time: 1319.8541457653046 and batch: 500, loss is 5.1710307407379155 and perplexity is 176.096253553869
At time: 1321.5211689472198 and batch: 550, loss is 5.185287570953369 and perplexity is 178.62480971148497
At time: 1323.1882424354553 and batch: 600, loss is 5.150695934295654 and perplexity is 172.55153307070623
At time: 1324.855017900467 and batch: 650, loss is 5.141936731338501 and perplexity is 171.04671927790386
At time: 1326.5204889774323 and batch: 700, loss is 5.142625226974487 and perplexity is 171.1645247472986
At time: 1328.1875159740448 and batch: 750, loss is 5.145976705551147 and perplexity is 171.7391413548378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.121999696243641 and perplexity of 167.670324323376
Finished 48 epochs...
Completing Train Step...
At time: 1332.5312883853912 and batch: 50, loss is 5.20362268447876 and perplexity is 181.93012494411772
At time: 1334.2229297161102 and batch: 100, loss is 5.204206266403198 and perplexity is 182.03632706235007
At time: 1335.8895354270935 and batch: 150, loss is 5.184763708114624 and perplexity is 178.5312593175262
At time: 1337.5579164028168 and batch: 200, loss is 5.192931108474731 and perplexity is 179.9953664248537
At time: 1339.2229928970337 and batch: 250, loss is 5.218545503616333 and perplexity is 184.66539349358433
At time: 1340.889440536499 and batch: 300, loss is 5.234830131530762 and perplexity is 187.697219796609
At time: 1342.5571377277374 and batch: 350, loss is 5.18031268119812 and perplexity is 177.73837775447916
At time: 1344.2245585918427 and batch: 400, loss is 5.2082160949707035 and perplexity is 182.76772694099435
At time: 1345.8901014328003 and batch: 450, loss is 5.176356439590454 and perplexity is 177.03659092361264
At time: 1347.554007768631 and batch: 500, loss is 5.162540645599365 and perplexity is 174.60750834448498
At time: 1349.2246952056885 and batch: 550, loss is 5.179454250335693 and perplexity is 177.58586711486248
At time: 1350.8942346572876 and batch: 600, loss is 5.144567289352417 and perplexity is 171.49725992290095
At time: 1352.5617156028748 and batch: 650, loss is 5.1375838565826415 and perplexity is 170.3037924382806
At time: 1354.2247524261475 and batch: 700, loss is 5.139657888412476 and perplexity is 170.65737446798352
At time: 1355.9142804145813 and batch: 750, loss is 5.140975151062012 and perplexity is 170.8823231789784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.114245037699854 and perplexity of 166.37512660885744
Finished 49 epochs...
Completing Train Step...
At time: 1360.275931596756 and batch: 50, loss is 5.196296815872192 and perplexity is 180.6021987983939
At time: 1361.968139886856 and batch: 100, loss is 5.199152097702027 and perplexity is 181.11860586807276
At time: 1363.6411821842194 and batch: 150, loss is 5.180153455734253 and perplexity is 177.71007953179281
At time: 1365.3197238445282 and batch: 200, loss is 5.18680890083313 and perplexity is 178.896763785243
At time: 1366.9990403652191 and batch: 250, loss is 5.208251113891602 and perplexity is 182.77412738163446
At time: 1368.6668481826782 and batch: 300, loss is 5.227497892379761 and perplexity is 186.32601204785504
At time: 1370.332729101181 and batch: 350, loss is 5.176010227203369 and perplexity is 176.97530927171317
At time: 1371.9995477199554 and batch: 400, loss is 5.203202857971191 and perplexity is 181.85376188588046
At time: 1373.6652772426605 and batch: 450, loss is 5.173314208984375 and perplexity is 176.49882320976874
At time: 1375.3297295570374 and batch: 500, loss is 5.161047620773315 and perplexity is 174.3470095137131
At time: 1377.0046708583832 and batch: 550, loss is 5.174286222457885 and perplexity is 176.67046584995265
At time: 1378.6760177612305 and batch: 600, loss is 5.141583337783813 and perplexity is 170.98628314925867
At time: 1380.3435368537903 and batch: 650, loss is 5.129881391525268 and perplexity is 168.99707237112924
At time: 1382.007755279541 and batch: 700, loss is 5.135561075210571 and perplexity is 169.95965327582837
At time: 1383.6727159023285 and batch: 750, loss is 5.136295766830444 and perplexity is 170.08456708975402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.113387706667878 and perplexity of 166.23254917662834
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f231118f208>
SETTINGS FOR THIS RUN
{'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 13.73398522151051, 'dropout': 0.5229212356086618, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 7.588987817527516}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1216087341308594 and batch: 50, loss is 7.415976858139038 and perplexity is 1662.3322382899378
At time: 3.782648801803589 and batch: 100, loss is 6.504307041168213 and perplexity is 668.0126036967873
At time: 5.44319224357605 and batch: 150, loss is 6.217769498825073 and perplexity is 501.58320144968883
At time: 7.103246688842773 and batch: 200, loss is 6.154346828460693 and perplexity is 470.7592554706906
At time: 8.773895025253296 and batch: 250, loss is 6.177662029266357 and perplexity is 481.8640543090017
At time: 10.47892951965332 and batch: 300, loss is 6.169171123504639 and perplexity is 477.78991308573256
At time: 12.150981426239014 and batch: 350, loss is 6.114123907089233 and perplexity is 452.19970482444984
At time: 13.819136619567871 and batch: 400, loss is 6.156897373199463 and perplexity is 471.961480525619
At time: 15.493695974349976 and batch: 450, loss is 6.138533563613891 and perplexity is 463.3735645327337
At time: 17.17343306541443 and batch: 500, loss is 6.145779457092285 and perplexity is 466.74331370032576
At time: 18.85000777244568 and batch: 550, loss is 6.172753858566284 and perplexity is 479.5047778782223
At time: 20.519914865493774 and batch: 600, loss is 6.171707754135132 and perplexity is 479.0034280831666
At time: 22.189242362976074 and batch: 650, loss is 6.177917270660401 and perplexity is 481.987061659579
At time: 23.859958171844482 and batch: 700, loss is 6.183420972824097 and perplexity is 484.6470881760451
At time: 25.525302171707153 and batch: 750, loss is 6.164420938491821 and perplexity is 475.5257045635249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.6239404012990555 and perplexity of 276.9786427693296
Finished 1 epochs...
Completing Train Step...
At time: 29.900508165359497 and batch: 50, loss is 5.909637393951416 and perplexity is 368.57248456364874
At time: 31.573973894119263 and batch: 100, loss is 5.81797384262085 and perplexity is 336.28998643811997
At time: 33.23120331764221 and batch: 150, loss is 5.798391828536987 and perplexity is 329.76880846720456
At time: 34.88479161262512 and batch: 200, loss is 5.758781185150147 and perplexity is 316.961775678256
At time: 36.5382878780365 and batch: 250, loss is 5.785690546035767 and perplexity is 325.6068089882376
At time: 38.19394397735596 and batch: 300, loss is 5.786434087753296 and perplexity is 325.8490012631039
At time: 39.84937310218811 and batch: 350, loss is 5.708810806274414 and perplexity is 301.51229846813914
At time: 41.503923177719116 and batch: 400, loss is 5.735734729766846 and perplexity is 309.7404627143706
At time: 43.1625759601593 and batch: 450, loss is 5.687098264694214 and perplexity is 295.0362601928409
At time: 44.82051205635071 and batch: 500, loss is 5.681348829269409 and perplexity is 293.3448352953088
At time: 46.47637462615967 and batch: 550, loss is 5.704472827911377 and perplexity is 300.2071774811488
At time: 48.131563663482666 and batch: 600, loss is 5.678229942321777 and perplexity is 292.43135118467524
At time: 49.78377437591553 and batch: 650, loss is 5.666963710784912 and perplexity is 289.1552412326466
At time: 51.438027143478394 and batch: 700, loss is 5.663141393661499 and perplexity is 288.05210780872096
At time: 53.09391665458679 and batch: 750, loss is 5.652289457321167 and perplexity is 284.9430846438078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.4039427291515265 and perplexity of 222.28108489349162
Finished 2 epochs...
Completing Train Step...
At time: 57.42940425872803 and batch: 50, loss is 5.659271459579468 and perplexity is 286.9395193498554
At time: 59.10957956314087 and batch: 100, loss is 5.651020498275757 and perplexity is 284.5817328579196
At time: 60.791332483291626 and batch: 150, loss is 5.639539976119995 and perplexity is 281.3332686807315
At time: 62.44553232192993 and batch: 200, loss is 5.631861410140991 and perplexity is 279.1813051852908
At time: 64.09815454483032 and batch: 250, loss is 5.659106044769287 and perplexity is 286.8920592291404
At time: 65.75445437431335 and batch: 300, loss is 5.671770887374878 and perplexity is 290.54860793579576
At time: 67.41093015670776 and batch: 350, loss is 5.5978592491149906 and perplexity is 269.8481110701951
At time: 69.06509304046631 and batch: 400, loss is 5.639529390335083 and perplexity is 281.33029056302365
At time: 70.71757793426514 and batch: 450, loss is 5.6100829982757565 and perplexity is 273.1669094463763
At time: 72.37190675735474 and batch: 500, loss is 5.5959258556365965 and perplexity is 269.32689251473306
At time: 74.02662181854248 and batch: 550, loss is 5.60822099685669 and perplexity is 272.6587455211391
At time: 75.6826069355011 and batch: 600, loss is 5.569511938095093 and perplexity is 262.30604640528827
At time: 77.3342056274414 and batch: 650, loss is 5.570588932037354 and perplexity is 262.58870060939677
At time: 78.98639798164368 and batch: 700, loss is 5.550763292312622 and perplexity is 257.433978309034
At time: 80.64111304283142 and batch: 750, loss is 5.5449582862854 and perplexity is 255.94390164972555
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.387898289880087 and perplexity of 218.74316737222728
Finished 3 epochs...
Completing Train Step...
At time: 84.96858954429626 and batch: 50, loss is 5.59427399635315 and perplexity is 268.88236963275386
At time: 86.64434051513672 and batch: 100, loss is 5.578631916046143 and perplexity is 264.7092135237699
At time: 88.3023989200592 and batch: 150, loss is 5.5650120544433594 and perplexity is 261.1283514477183
At time: 89.96162438392639 and batch: 200, loss is 5.551443471908569 and perplexity is 257.6091392120663
At time: 91.61996150016785 and batch: 250, loss is 5.5836878490448 and perplexity is 266.0509545897568
At time: 93.28381109237671 and batch: 300, loss is 5.601847581863403 and perplexity is 270.9265041944989
At time: 94.94823455810547 and batch: 350, loss is 5.5276067924499515 and perplexity is 251.54119980613658
At time: 96.61957287788391 and batch: 400, loss is 5.568377428054809 and perplexity is 262.0086263069594
At time: 98.28105902671814 and batch: 450, loss is 5.544416160583496 and perplexity is 255.80518548659163
At time: 99.97119140625 and batch: 500, loss is 5.535746278762818 and perplexity is 253.59697107034432
At time: 101.64078760147095 and batch: 550, loss is 5.545107612609863 and perplexity is 255.98212366553315
At time: 103.30674028396606 and batch: 600, loss is 5.506863107681275 and perplexity is 246.3770553201651
At time: 104.97280883789062 and batch: 650, loss is 5.497984170913696 and perplexity is 244.19917197710168
At time: 106.63616037368774 and batch: 700, loss is 5.492068281173706 and perplexity is 242.75878139787972
At time: 108.31567406654358 and batch: 750, loss is 5.47498496055603 and perplexity is 238.6468777873232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.372907061909521 and perplexity of 215.4883961472351
Finished 4 epochs...
Completing Train Step...
At time: 112.6433687210083 and batch: 50, loss is 5.531885004043579 and perplexity is 252.6196515609058
At time: 114.32572603225708 and batch: 100, loss is 5.52328203201294 and perplexity is 250.45569334653365
At time: 115.9841845035553 and batch: 150, loss is 5.516449203491211 and perplexity is 248.75020582889678
At time: 117.64321327209473 and batch: 200, loss is 5.520326929092407 and perplexity is 249.71666348811408
At time: 119.31156063079834 and batch: 250, loss is 5.5359240245819095 and perplexity is 253.64205087794136
At time: 120.97251415252686 and batch: 300, loss is 5.545671710968017 and perplexity is 256.12656349651536
At time: 122.63796710968018 and batch: 350, loss is 5.473379764556885 and perplexity is 238.2641100646873
At time: 124.29606366157532 and batch: 400, loss is 5.523519630432129 and perplexity is 250.51520829339836
At time: 125.95722484588623 and batch: 450, loss is 5.499425554275513 and perplexity is 244.55141039487248
At time: 127.62092208862305 and batch: 500, loss is 5.488316259384155 and perplexity is 241.8496517641029
At time: 129.287517786026 and batch: 550, loss is 5.503137445449829 and perplexity is 245.46084543453927
At time: 130.94429731369019 and batch: 600, loss is 5.4586896896362305 and perplexity is 234.78957550641817
At time: 132.60249304771423 and batch: 650, loss is 5.4398894882202145 and perplexity is 230.41671829193666
At time: 134.26074194908142 and batch: 700, loss is 5.451404495239258 and perplexity is 233.0853033212716
At time: 135.917875289917 and batch: 750, loss is 5.436646785736084 and perplexity is 229.67075554842407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.369571419649346 and perplexity of 214.77080143145915
Finished 5 epochs...
Completing Train Step...
At time: 140.26323771476746 and batch: 50, loss is 5.490919437408447 and perplexity is 242.48004962569502
At time: 141.9534411430359 and batch: 100, loss is 5.485623846054077 and perplexity is 241.19936834651952
At time: 143.6112082004547 and batch: 150, loss is 5.48424840927124 and perplexity is 240.86784191239053
At time: 145.27093315124512 and batch: 200, loss is 5.469530420303345 and perplexity is 237.3487124523756
At time: 146.9338550567627 and batch: 250, loss is 5.491371469497681 and perplexity is 242.58968316619715
At time: 148.6027011871338 and batch: 300, loss is 5.505216970443725 and perplexity is 245.9718185041394
At time: 150.2662434577942 and batch: 350, loss is 5.441010637283325 and perplexity is 230.67519464796203
At time: 151.9275951385498 and batch: 400, loss is 5.4798558235168455 and perplexity is 239.8121296121998
At time: 153.5898892879486 and batch: 450, loss is 5.4484559917449955 and perplexity is 232.39906267859664
At time: 155.2464075088501 and batch: 500, loss is 5.448149042129517 and perplexity is 232.32773882264487
At time: 156.9031581878662 and batch: 550, loss is 5.469508638381958 and perplexity is 237.34354259768463
At time: 158.5618977546692 and batch: 600, loss is 5.431559133529663 and perplexity is 228.50523801018545
At time: 160.22567534446716 and batch: 650, loss is 5.418627157211303 and perplexity is 225.56923873710502
At time: 161.8852939605713 and batch: 700, loss is 5.414269294738769 and perplexity is 224.58837779748515
At time: 163.5443036556244 and batch: 750, loss is 5.404033937454224 and perplexity is 222.30135969856767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.369539482648983 and perplexity of 214.76394240582474
Finished 6 epochs...
Completing Train Step...
At time: 167.88790488243103 and batch: 50, loss is 5.464841613769531 and perplexity is 236.23843523029424
At time: 169.56911444664001 and batch: 100, loss is 5.4423086547851565 and perplexity is 230.97480949844615
At time: 171.22792410850525 and batch: 150, loss is 5.430845079421997 and perplexity is 228.3421311468542
At time: 172.89136791229248 and batch: 200, loss is 5.43212495803833 and perplexity is 228.63456845992908
At time: 174.54843258857727 and batch: 250, loss is 5.442739734649658 and perplexity is 231.0743995521192
At time: 176.2047803401947 and batch: 300, loss is 5.46254942893982 and perplexity is 235.69755321046605
At time: 177.86730360984802 and batch: 350, loss is 5.397968530654907 and perplexity is 220.9570924051483
At time: 179.5305950641632 and batch: 400, loss is 5.443469381332397 and perplexity is 231.2430637463704
At time: 181.19368934631348 and batch: 450, loss is 5.425179929733276 and perplexity is 227.05219608120464
At time: 182.8501181602478 and batch: 500, loss is 5.425214567184448 and perplexity is 227.0600607267648
At time: 184.53537273406982 and batch: 550, loss is 5.424024868011474 and perplexity is 226.79008818520037
At time: 186.19195175170898 and batch: 600, loss is 5.372024812698364 and perplexity is 215.29836551923273
At time: 187.856112241745 and batch: 650, loss is 5.370111255645752 and perplexity is 214.88677374124615
At time: 189.51049304008484 and batch: 700, loss is 5.335420846939087 and perplexity is 207.56008145476858
At time: 191.17857265472412 and batch: 750, loss is 5.305231676101685 and perplexity is 201.387653716032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3012950808502906 and perplexity of 200.5964304194444
Finished 7 epochs...
Completing Train Step...
At time: 195.50557684898376 and batch: 50, loss is 5.405405797958374 and perplexity is 222.60653543536537
At time: 197.18561816215515 and batch: 100, loss is 5.409123592376709 and perplexity is 223.43568111183868
At time: 198.84433937072754 and batch: 150, loss is 5.392939863204956 and perplexity is 219.8487617155406
At time: 200.50693678855896 and batch: 200, loss is 5.396742610931397 and perplexity is 220.68638271559684
At time: 202.16609501838684 and batch: 250, loss is 5.41478310585022 and perplexity is 224.70380345245283
At time: 203.8214659690857 and batch: 300, loss is 5.449205522537231 and perplexity is 232.57331822895333
At time: 205.47920560836792 and batch: 350, loss is 5.3895965194702145 and perplexity is 219.11495909619708
At time: 207.1427981853485 and batch: 400, loss is 5.439890518188476 and perplexity is 230.4169556139657
At time: 208.80088424682617 and batch: 450, loss is 5.409378757476807 and perplexity is 223.4927013742562
At time: 210.45638298988342 and batch: 500, loss is 5.397501554489136 and perplexity is 220.85393479728359
At time: 212.1130337715149 and batch: 550, loss is 5.4163596820831295 and perplexity is 225.05834553629452
At time: 213.76988744735718 and batch: 600, loss is 5.385671329498291 and perplexity is 218.25657701452295
At time: 215.433935880661 and batch: 650, loss is 5.383210048675537 and perplexity is 217.7200468336537
At time: 217.08889532089233 and batch: 700, loss is 5.369721050262451 and perplexity is 214.8029401225605
At time: 218.74509572982788 and batch: 750, loss is 5.369943132400513 and perplexity is 214.85064931624865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.393858798714572 and perplexity of 220.05088140268174
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 223.08829927444458 and batch: 50, loss is 5.389282102584839 and perplexity is 219.04607648271474
At time: 224.7714819908142 and batch: 100, loss is 5.309674625396728 and perplexity is 202.2843994729732
At time: 226.43011665344238 and batch: 150, loss is 5.262883787155151 and perplexity is 193.03736807938196
At time: 228.09922742843628 and batch: 200, loss is 5.234699983596801 and perplexity is 187.6727929808266
At time: 229.76940321922302 and batch: 250, loss is 5.225230665206909 and perplexity is 185.90404717619535
At time: 231.4327414035797 and batch: 300, loss is 5.229643898010254 and perplexity is 186.7262980732281
At time: 233.09095430374146 and batch: 350, loss is 5.158291482925415 and perplexity is 173.8671467099068
At time: 234.75353932380676 and batch: 400, loss is 5.15902268409729 and perplexity is 173.99432506214532
At time: 236.41649293899536 and batch: 450, loss is 5.130493144989014 and perplexity is 169.1004885448912
At time: 238.0744149684906 and batch: 500, loss is 5.094290714263916 and perplexity is 163.08812750842267
At time: 239.7330858707428 and batch: 550, loss is 5.079313383102417 and perplexity is 160.66370363432418
At time: 241.39401149749756 and batch: 600, loss is 4.995369462966919 and perplexity is 147.72751514814067
At time: 243.06475257873535 and batch: 650, loss is 4.949335899353027 and perplexity is 141.0812406608838
At time: 244.72423219680786 and batch: 700, loss is 4.914617052078247 and perplexity is 136.2671166004379
At time: 246.38226556777954 and batch: 750, loss is 4.94597864151001 and perplexity is 140.60838874620515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.048164190248001 and perplexity of 155.7362996770032
Finished 9 epochs...
Completing Train Step...
At time: 250.7292594909668 and batch: 50, loss is 5.139786605834961 and perplexity is 170.67934245915436
At time: 252.41916942596436 and batch: 100, loss is 5.1243597030639645 and perplexity is 168.06649473089996
At time: 254.07778596878052 and batch: 150, loss is 5.101177158355713 and perplexity is 164.21510074593044
At time: 255.7399182319641 and batch: 200, loss is 5.0907541847229005 and perplexity is 162.51238020145428
At time: 257.40296840667725 and batch: 250, loss is 5.093048086166382 and perplexity is 162.8855954808617
At time: 259.0618894100189 and batch: 300, loss is 5.108441944122315 and perplexity is 165.41243218471766
At time: 260.7193615436554 and batch: 350, loss is 5.0529982948303225 and perplexity is 156.49096783969168
At time: 262.3836622238159 and batch: 400, loss is 5.062650671005249 and perplexity is 158.00879104512435
At time: 264.0473520755768 and batch: 450, loss is 5.038109159469604 and perplexity is 154.17821282340418
At time: 265.7061839103699 and batch: 500, loss is 5.005706119537353 and perplexity is 149.26244308291348
At time: 267.41028213500977 and batch: 550, loss is 5.011568775177002 and perplexity is 150.1400875364719
At time: 269.0757260322571 and batch: 600, loss is 4.949250106811523 and perplexity is 141.0691374618785
At time: 270.735985994339 and batch: 650, loss is 4.9368181324005125 and perplexity is 139.3262259183933
At time: 272.3988959789276 and batch: 700, loss is 4.9212745189666744 and perplexity is 137.17733693617382
At time: 274.0769190788269 and batch: 750, loss is 4.9460718631744385 and perplexity is 140.62149710521874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.024984315384266 and perplexity of 152.1678693993937
Finished 10 epochs...
Completing Train Step...
At time: 278.4040675163269 and batch: 50, loss is 5.0903028297424315 and perplexity is 162.4390459803904
At time: 280.0869414806366 and batch: 100, loss is 5.073784618377686 and perplexity is 159.7778828209822
At time: 281.74882078170776 and batch: 150, loss is 5.050159816741943 and perplexity is 156.0474014809607
At time: 283.4123773574829 and batch: 200, loss is 5.0457886219024655 and perplexity is 155.36677654075058
At time: 285.0721592903137 and batch: 250, loss is 5.04924391746521 and perplexity is 155.90454321068808
At time: 286.73045921325684 and batch: 300, loss is 5.066211109161377 and perplexity is 158.57237428002642
At time: 288.390421628952 and batch: 350, loss is 5.0140015983581545 and perplexity is 150.50579649409107
At time: 290.0504150390625 and batch: 400, loss is 5.028667755126953 and perplexity is 152.729404131879
At time: 291.70905661582947 and batch: 450, loss is 5.004238758087158 and perplexity is 149.0435817415894
At time: 293.38481545448303 and batch: 500, loss is 4.974945220947266 and perplexity is 144.7408961618281
At time: 295.0522081851959 and batch: 550, loss is 4.991271429061889 and perplexity is 147.12336154883147
At time: 296.708692073822 and batch: 600, loss is 4.939233427047729 and perplexity is 139.66314652354833
At time: 298.36588168144226 and batch: 650, loss is 4.9359853553771975 and perplexity is 139.2102465378877
At time: 300.0244610309601 and batch: 700, loss is 4.925529918670654 and perplexity is 137.76232513169407
At time: 301.6849980354309 and batch: 750, loss is 4.943357963562011 and perplexity is 140.2403818668922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0158641726471656 and perplexity of 150.78638594922253
Finished 11 epochs...
Completing Train Step...
At time: 306.0102252960205 and batch: 50, loss is 5.062059698104858 and perplexity is 157.91543971836217
At time: 307.6957323551178 and batch: 100, loss is 5.042993440628051 and perplexity is 154.9331046144795
At time: 309.3577630519867 and batch: 150, loss is 5.020580253601074 and perplexity is 151.49918624291053
At time: 311.0159378051758 and batch: 200, loss is 5.020639476776123 and perplexity is 151.5081587714253
At time: 312.674996137619 and batch: 250, loss is 5.026317796707153 and perplexity is 152.3709177616928
At time: 314.3337445259094 and batch: 300, loss is 5.043832578659058 and perplexity is 155.06316943836137
At time: 315.9895613193512 and batch: 350, loss is 4.992526435852051 and perplexity is 147.30811827779138
At time: 317.6483955383301 and batch: 400, loss is 5.010547370910644 and perplexity is 149.98681210192777
At time: 319.3095774650574 and batch: 450, loss is 4.987080030441284 and perplexity is 146.50799940906953
At time: 320.9742102622986 and batch: 500, loss is 4.959917860031128 and perplexity is 142.58208372805794
At time: 322.6325511932373 and batch: 550, loss is 4.982258634567261 and perplexity is 145.80332646388257
At time: 324.28929471969604 and batch: 600, loss is 4.935726757049561 and perplexity is 139.17425165525194
At time: 325.9477412700653 and batch: 650, loss is 4.935159893035888 and perplexity is 139.0953811369001
At time: 327.6174666881561 and batch: 700, loss is 4.924688501358032 and perplexity is 137.64645827932492
At time: 329.2848243713379 and batch: 750, loss is 4.939012651443481 and perplexity is 139.63231571145425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0104522705078125 and perplexity of 149.9725489739028
Finished 12 epochs...
Completing Train Step...
At time: 333.6673016548157 and batch: 50, loss is 5.04377932548523 and perplexity is 155.05491205231291
At time: 335.3612470626831 and batch: 100, loss is 5.023579425811768 and perplexity is 151.95424044408796
At time: 337.0234353542328 and batch: 150, loss is 5.002296857833862 and perplexity is 148.75443481056635
At time: 338.6822700500488 and batch: 200, loss is 5.005692615509033 and perplexity is 149.2604274522645
At time: 340.3436224460602 and batch: 250, loss is 5.010942440032959 and perplexity is 150.046078966627
At time: 342.0046339035034 and batch: 300, loss is 5.03038556098938 and perplexity is 152.9919890680622
At time: 343.660902261734 and batch: 350, loss is 4.980835552215576 and perplexity is 145.59598389094498
At time: 345.3184325695038 and batch: 400, loss is 5.00051194190979 and perplexity is 148.48915747040044
At time: 346.9783978462219 and batch: 450, loss is 4.976021738052368 and perplexity is 144.89679611177962
At time: 348.64268922805786 and batch: 500, loss is 4.9506122016906735 and perplexity is 141.2614179340165
At time: 350.3303279876709 and batch: 550, loss is 4.9758607769012455 and perplexity is 144.87347523361154
At time: 351.9875738620758 and batch: 600, loss is 4.93280478477478 and perplexity is 138.7681819017486
At time: 353.64528799057007 and batch: 650, loss is 4.932768049240113 and perplexity is 138.76308427202426
At time: 355.30394744873047 and batch: 700, loss is 4.920982465744019 and perplexity is 137.13727970255312
At time: 356.9661190509796 and batch: 750, loss is 4.9332616329193115 and perplexity is 138.83159237154527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.007096401480741 and perplexity of 149.47010428271705
Finished 13 epochs...
Completing Train Step...
At time: 361.28649640083313 and batch: 50, loss is 5.029688119888306 and perplexity is 152.8853233675965
At time: 362.97138500213623 and batch: 100, loss is 5.010040550231934 and perplexity is 149.91081494411299
At time: 364.63254261016846 and batch: 150, loss is 4.989042940139771 and perplexity is 146.79586381550433
At time: 366.2899839878082 and batch: 200, loss is 4.993531665802002 and perplexity is 147.4562712615878
At time: 367.9496445655823 and batch: 250, loss is 5.0000858402252195 and perplexity is 148.42589946838962
At time: 369.6088993549347 and batch: 300, loss is 5.020983562469483 and perplexity is 151.56029953121615
At time: 371.27463150024414 and batch: 350, loss is 4.97212028503418 and perplexity is 144.33258939792162
At time: 372.94821643829346 and batch: 400, loss is 4.99235538482666 and perplexity is 147.28292322799254
At time: 374.6073920726776 and batch: 450, loss is 4.968024072647094 and perplexity is 143.74258168080533
At time: 376.26666021347046 and batch: 500, loss is 4.943679513931275 and perplexity is 140.2854834642962
At time: 377.9252059459686 and batch: 550, loss is 4.969821882247925 and perplexity is 144.0012359100608
At time: 379.58701515197754 and batch: 600, loss is 4.928559370040894 and perplexity is 138.18030219726072
At time: 381.2454936504364 and batch: 650, loss is 4.928350915908814 and perplexity is 138.15150094427
At time: 382.90572023391724 and batch: 700, loss is 4.915823879241944 and perplexity is 136.4316667300817
At time: 384.5644383430481 and batch: 750, loss is 4.927031764984131 and perplexity is 137.9693784140153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.003847698832667 and perplexity of 148.98530826456064
Finished 14 epochs...
Completing Train Step...
At time: 388.91265869140625 and batch: 50, loss is 5.017612619400024 and perplexity is 151.05025853246846
At time: 390.59934663772583 and batch: 100, loss is 4.998422546386719 and perplexity is 148.17922878411338
At time: 392.2586507797241 and batch: 150, loss is 4.977394514083862 and perplexity is 145.09584355304142
At time: 393.9158773422241 and batch: 200, loss is 4.98375825881958 and perplexity is 146.02214069687477
At time: 395.5816843509674 and batch: 250, loss is 4.990413045883178 and perplexity is 146.99712751641852
At time: 397.24219155311584 and batch: 300, loss is 5.011777677536011 and perplexity is 150.17145543123956
At time: 398.90245819091797 and batch: 350, loss is 4.96417540550232 and perplexity is 143.1904275396893
At time: 400.565557718277 and batch: 400, loss is 4.985236978530883 and perplexity is 146.2382262402115
At time: 402.2302086353302 and batch: 450, loss is 4.9612069511413575 and perplexity is 142.76600354390044
At time: 403.89391899108887 and batch: 500, loss is 4.93775894165039 and perplexity is 139.45736700018628
At time: 405.55430340766907 and batch: 550, loss is 4.964417219161987 and perplexity is 143.22505712778698
At time: 407.2171673774719 and batch: 600, loss is 4.924329137802124 and perplexity is 137.59700204553573
At time: 408.87670278549194 and batch: 650, loss is 4.923608274459839 and perplexity is 137.49784915288924
At time: 410.5371341705322 and batch: 700, loss is 4.9098153591156 and perplexity is 135.6143721390139
At time: 412.20471811294556 and batch: 750, loss is 4.921130475997924 and perplexity is 137.1575789283515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.000276432480923 and perplexity of 148.454190991361
Finished 15 epochs...
Completing Train Step...
At time: 416.5308117866516 and batch: 50, loss is 5.006555414199829 and perplexity is 149.38926472598106
At time: 418.22418117523193 and batch: 100, loss is 4.98804931640625 and perplexity is 146.65007640214603
At time: 419.89296078681946 and batch: 150, loss is 4.96688473701477 and perplexity is 143.57890389528643
At time: 421.55470061302185 and batch: 200, loss is 4.97430643081665 and perplexity is 144.64846663054266
At time: 423.2182831764221 and batch: 250, loss is 4.982288866043091 and perplexity is 145.80773438025113
At time: 424.88935685157776 and batch: 300, loss is 5.00405291557312 and perplexity is 149.01588568128972
At time: 426.56004881858826 and batch: 350, loss is 4.957575159072876 and perplexity is 142.24844750142245
At time: 428.2217652797699 and batch: 400, loss is 4.978439769744873 and perplexity is 145.24758509544196
At time: 429.8877902030945 and batch: 450, loss is 4.954455890655518 and perplexity is 141.8054277197287
At time: 431.5566146373749 and batch: 500, loss is 4.931756792068481 and perplexity is 138.62283003639973
At time: 433.2575898170471 and batch: 550, loss is 4.959658308029175 and perplexity is 142.54508106504903
At time: 434.91481590270996 and batch: 600, loss is 4.919763641357422 and perplexity is 136.97023526133853
At time: 436.57554864883423 and batch: 650, loss is 4.917058696746826 and perplexity is 136.60023899695375
At time: 438.2406575679779 and batch: 700, loss is 4.90515830039978 and perplexity is 134.98427638052826
At time: 439.9026997089386 and batch: 750, loss is 4.915245485305786 and perplexity is 136.35277829783988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.998574545217115 and perplexity of 148.20175356540437
Finished 16 epochs...
Completing Train Step...
At time: 444.253137588501 and batch: 50, loss is 4.997793788909912 and perplexity is 148.08608927025927
At time: 445.94229555130005 and batch: 100, loss is 4.980239019393921 and perplexity is 145.5091570079704
At time: 447.6057770252228 and batch: 150, loss is 4.960766925811767 and perplexity is 142.70319670545004
At time: 449.2839937210083 and batch: 200, loss is 4.968488159179688 and perplexity is 143.80930615889187
At time: 450.9544994831085 and batch: 250, loss is 4.976160030364991 and perplexity is 144.9168356104279
At time: 452.6253843307495 and batch: 300, loss is 4.997499113082886 and perplexity is 148.04245830824303
At time: 454.2941806316376 and batch: 350, loss is 4.951958332061768 and perplexity is 141.45170226398517
At time: 455.9644935131073 and batch: 400, loss is 4.974012212753296 and perplexity is 144.60591469890386
At time: 457.63971638679504 and batch: 450, loss is 4.951735982894897 and perplexity is 141.4202540922013
At time: 459.3160209655762 and batch: 500, loss is 4.927552938461304 and perplexity is 138.04130313570735
At time: 461.00233721733093 and batch: 550, loss is 4.9538601112365725 and perplexity is 141.72096812654053
At time: 462.67473340034485 and batch: 600, loss is 4.914076557159424 and perplexity is 136.19348481690764
At time: 464.33552074432373 and batch: 650, loss is 4.9118834972381595 and perplexity is 135.89513161661907
At time: 465.9960162639618 and batch: 700, loss is 4.900598964691162 and perplexity is 134.37023861497434
At time: 467.6689102649689 and batch: 750, loss is 4.908535385131836 and perplexity is 135.4409003142465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.996890045875727 and perplexity of 147.9523179551734
Finished 17 epochs...
Completing Train Step...
At time: 472.1761212348938 and batch: 50, loss is 4.988747606277466 and perplexity is 146.75251642736413
At time: 473.8713631629944 and batch: 100, loss is 4.972403469085694 and perplexity is 144.37346787314578
At time: 475.5331234931946 and batch: 150, loss is 4.9531597232818605 and perplexity is 141.62174321955558
At time: 477.1901590824127 and batch: 200, loss is 4.961547842025757 and perplexity is 142.81467946922893
At time: 478.8501036167145 and batch: 250, loss is 4.968621120452881 and perplexity is 143.82842849857295
At time: 480.51073384284973 and batch: 300, loss is 4.9896923828125 and perplexity is 146.89123027781937
At time: 482.17267179489136 and batch: 350, loss is 4.944563722610473 and perplexity is 140.40957996185583
At time: 483.8313777446747 and batch: 400, loss is 4.96698335647583 and perplexity is 143.59306426764078
At time: 485.49260091781616 and batch: 450, loss is 4.943821086883545 and perplexity is 140.3053455002812
At time: 487.1545467376709 and batch: 500, loss is 4.919764804840088 and perplexity is 136.9703946239257
At time: 488.81430411338806 and batch: 550, loss is 4.94661587715149 and perplexity is 140.69801797744265
At time: 490.4732222557068 and batch: 600, loss is 4.907983140945435 and perplexity is 135.36612451359795
At time: 492.13067746162415 and batch: 650, loss is 4.9057868957519535 and perplexity is 135.06915354317775
At time: 493.7951719760895 and batch: 700, loss is 4.895800838470459 and perplexity is 133.7270575157853
At time: 495.46704268455505 and batch: 750, loss is 4.903778085708618 and perplexity is 134.79809761208347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9942712118459305 and perplexity of 147.56536229760175
Finished 18 epochs...
Completing Train Step...
At time: 499.84555864334106 and batch: 50, loss is 4.980521011352539 and perplexity is 145.55019520610085
At time: 501.5401453971863 and batch: 100, loss is 4.965444278717041 and perplexity is 143.3722333576724
At time: 503.2027723789215 and batch: 150, loss is 4.945800275802612 and perplexity is 140.58331126802796
At time: 504.86089849472046 and batch: 200, loss is 4.955253744125367 and perplexity is 141.91861281883615
At time: 506.5199112892151 and batch: 250, loss is 4.962242593765259 and perplexity is 142.91393469109326
At time: 508.1946806907654 and batch: 300, loss is 4.984300050735474 and perplexity is 146.1012757476755
At time: 509.86379313468933 and batch: 350, loss is 4.938466844558715 and perplexity is 139.556124227013
At time: 511.53378415107727 and batch: 400, loss is 4.960994672775269 and perplexity is 142.7357006263758
At time: 513.1936345100403 and batch: 450, loss is 4.937509326934815 and perplexity is 139.42256073344655
At time: 514.8515031337738 and batch: 500, loss is 4.913305864334107 and perplexity is 136.08856191216265
At time: 516.5609774589539 and batch: 550, loss is 4.940545835494995 and perplexity is 139.8465619484676
At time: 518.2185175418854 and batch: 600, loss is 4.903045272827148 and perplexity is 134.69935201519738
At time: 519.8763554096222 and batch: 650, loss is 4.9007085514068605 and perplexity is 134.38496461498403
At time: 521.5371789932251 and batch: 700, loss is 4.890870561599732 and perplexity is 133.06936872328316
At time: 523.2006957530975 and batch: 750, loss is 4.897600927352905 and perplexity is 133.9679948945907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.992748171784157 and perplexity of 147.3407854023083
Finished 19 epochs...
Completing Train Step...
At time: 527.5941476821899 and batch: 50, loss is 4.9733868598937985 and perplexity is 144.51551324599626
At time: 529.2817649841309 and batch: 100, loss is 4.958650531768799 and perplexity is 142.40149987732647
At time: 530.958297252655 and batch: 150, loss is 4.938924760818481 and perplexity is 139.62004387925356
At time: 532.6262040138245 and batch: 200, loss is 4.949322986602783 and perplexity is 141.07941892582082
At time: 534.2836539745331 and batch: 250, loss is 4.955671758651733 and perplexity is 141.97794926143968
At time: 535.9436094760895 and batch: 300, loss is 4.9777689456939695 and perplexity is 145.1501821957808
At time: 537.6129441261292 and batch: 350, loss is 4.931954936981201 and perplexity is 138.6503001664017
At time: 539.2772126197815 and batch: 400, loss is 4.955079860687256 and perplexity is 141.89393766786955
At time: 540.9421675205231 and batch: 450, loss is 4.931898183822632 and perplexity is 138.6424315472173
At time: 542.6104559898376 and batch: 500, loss is 4.907159194946289 and perplexity is 135.25463607341962
At time: 544.273211479187 and batch: 550, loss is 4.934320240020752 and perplexity is 138.97863829933357
At time: 545.9319906234741 and batch: 600, loss is 4.89802056312561 and perplexity is 134.02422445478834
At time: 547.5928795337677 and batch: 650, loss is 4.894614849090576 and perplexity is 133.56855265688733
At time: 549.2579889297485 and batch: 700, loss is 4.884538164138794 and perplexity is 132.22938295940907
At time: 550.9269180297852 and batch: 750, loss is 4.891055803298951 and perplexity is 133.09402100250404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.991264165833939 and perplexity of 147.12229296220036
Finished 20 epochs...
Completing Train Step...
At time: 555.3066489696503 and batch: 50, loss is 4.9658650875091555 and perplexity is 143.43257834997553
At time: 557.0035655498505 and batch: 100, loss is 4.952596960067749 and perplexity is 141.54206613392245
At time: 558.6653714179993 and batch: 150, loss is 4.93234091758728 and perplexity is 138.703826822754
At time: 560.3294222354889 and batch: 200, loss is 4.943176927566529 and perplexity is 140.21499560773674
At time: 561.9896759986877 and batch: 250, loss is 4.950363178253173 and perplexity is 141.22624490977188
At time: 563.6546967029572 and batch: 300, loss is 4.9724092864990235 and perplexity is 144.37430775572523
At time: 565.3155715465546 and batch: 350, loss is 4.926608266830445 and perplexity is 137.91096100769673
At time: 566.9824612140656 and batch: 400, loss is 4.950141897201538 and perplexity is 141.19499767511775
At time: 568.6473035812378 and batch: 450, loss is 4.926136999130249 and perplexity is 137.84598333841186
At time: 570.3124315738678 and batch: 500, loss is 4.901078090667725 and perplexity is 134.43463431236552
At time: 571.9793162345886 and batch: 550, loss is 4.929154529571533 and perplexity is 138.26256599867446
At time: 573.6453323364258 and batch: 600, loss is 4.8926425075531 and perplexity is 133.30536948125433
At time: 575.3125596046448 and batch: 650, loss is 4.889654569625854 and perplexity is 132.90765577969367
At time: 576.9830639362335 and batch: 700, loss is 4.8799881172180175 and perplexity is 131.62909975664218
At time: 578.6509408950806 and batch: 750, loss is 4.885883836746216 and perplexity is 132.40744019445637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.989287975222566 and perplexity of 146.831838359481
Finished 21 epochs...
Completing Train Step...
At time: 583.0782098770142 and batch: 50, loss is 4.9589717578887935 and perplexity is 142.44725030633646
At time: 584.7831137180328 and batch: 100, loss is 4.946969633102417 and perplexity is 140.74779954332928
At time: 586.4478514194489 and batch: 150, loss is 4.925981721878052 and perplexity is 137.82458065461057
At time: 588.1227133274078 and batch: 200, loss is 4.935957260131836 and perplexity is 139.20633544679623
At time: 589.8087556362152 and batch: 250, loss is 4.94328667640686 and perplexity is 140.23038488536397
At time: 591.4896385669708 and batch: 300, loss is 4.96643443107605 and perplexity is 143.5142640171402
At time: 593.1633794307709 and batch: 350, loss is 4.920840902328491 and perplexity is 137.1178674549067
At time: 594.836678981781 and batch: 400, loss is 4.944722909927368 and perplexity is 140.43193316528226
At time: 596.5008988380432 and batch: 450, loss is 4.921806831359863 and perplexity is 137.25037757118244
At time: 598.1716330051422 and batch: 500, loss is 4.8958601379394535 and perplexity is 133.7349876944116
At time: 599.8841817378998 and batch: 550, loss is 4.924895429611206 and perplexity is 137.6749441676582
At time: 601.5528140068054 and batch: 600, loss is 4.88761570930481 and perplexity is 132.63695169164868
At time: 603.2135396003723 and batch: 650, loss is 4.884550695419311 and perplexity is 132.2310399732818
At time: 604.8893620967865 and batch: 700, loss is 4.874391422271729 and perplexity is 130.89446950751824
At time: 606.5767138004303 and batch: 750, loss is 4.880024709701538 and perplexity is 131.63391648043327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.987024706463481 and perplexity of 146.49989422796983
Finished 22 epochs...
Completing Train Step...
At time: 610.9749920368195 and batch: 50, loss is 4.952984580993652 and perplexity is 141.59694143537206
At time: 612.6750881671906 and batch: 100, loss is 4.941104965209961 and perplexity is 139.92477618070185
At time: 614.342414855957 and batch: 150, loss is 4.9205312824249265 and perplexity is 137.0754196056969
At time: 616.0078792572021 and batch: 200, loss is 4.928752908706665 and perplexity is 138.2070480166754
At time: 617.6748855113983 and batch: 250, loss is 4.935194072723388 and perplexity is 139.1001354548101
At time: 619.3468000888824 and batch: 300, loss is 4.960390911102295 and perplexity is 142.64954829132864
At time: 621.017984867096 and batch: 350, loss is 4.914815664291382 and perplexity is 136.29418360187293
At time: 622.6833016872406 and batch: 400, loss is 4.938871440887451 and perplexity is 139.61259954661085
At time: 624.3422825336456 and batch: 450, loss is 4.915232229232788 and perplexity is 136.35097080743736
At time: 626.0068092346191 and batch: 500, loss is 4.889883708953858 and perplexity is 132.93811364003912
At time: 627.6772925853729 and batch: 550, loss is 4.919393577575684 and perplexity is 136.91955691578062
At time: 629.3415493965149 and batch: 600, loss is 4.88125807762146 and perplexity is 131.79636969189008
At time: 631.0016324520111 and batch: 650, loss is 4.878092374801636 and perplexity is 131.37980126652593
At time: 632.6628432273865 and batch: 700, loss is 4.867492551803589 and perplexity is 129.99455328934656
At time: 634.325843334198 and batch: 750, loss is 4.874281873703003 and perplexity is 130.8801309911259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.986579717591751 and perplexity of 146.43471790777284
Finished 23 epochs...
Completing Train Step...
At time: 638.8593237400055 and batch: 50, loss is 4.947974700927734 and perplexity is 140.8893317409493
At time: 640.5667324066162 and batch: 100, loss is 4.935330381393433 and perplexity is 139.11909730157967
At time: 642.2412641048431 and batch: 150, loss is 4.913051614761352 and perplexity is 136.0539658516429
At time: 643.9145784378052 and batch: 200, loss is 4.918853931427002 and perplexity is 136.8456887373434
At time: 645.5957071781158 and batch: 250, loss is 4.9255121803283695 and perplexity is 137.75988147809016
At time: 647.2583000659943 and batch: 300, loss is 4.952896938323975 and perplexity is 141.58453204521052
At time: 648.924512386322 and batch: 350, loss is 4.91006085395813 and perplexity is 135.64766885486566
At time: 650.5984103679657 and batch: 400, loss is 4.934935159683228 and perplexity is 139.06412527780614
At time: 652.2671558856964 and batch: 450, loss is 4.9110782432556155 and perplexity is 135.78574556836273
At time: 653.9287402629852 and batch: 500, loss is 4.88498740196228 and perplexity is 132.2887987445204
At time: 655.6081244945526 and batch: 550, loss is 4.913789691925049 and perplexity is 136.15442124422367
At time: 657.2940475940704 and batch: 600, loss is 4.875286111831665 and perplexity is 131.01163182697658
At time: 658.9697489738464 and batch: 650, loss is 4.870709047317505 and perplexity is 130.41335336003968
At time: 660.6548068523407 and batch: 700, loss is 4.8606899452209475 and perplexity is 129.1132524554073
At time: 662.3532705307007 and batch: 750, loss is 4.868604145050049 and perplexity is 130.13913469983174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.985609542491824 and perplexity of 146.29271948349947
Finished 24 epochs...
Completing Train Step...
At time: 666.743004322052 and batch: 50, loss is 4.941616840362549 and perplexity is 139.99641853126548
At time: 668.4441981315613 and batch: 100, loss is 4.929974317550659 and perplexity is 138.37595846078153
At time: 670.1253275871277 and batch: 150, loss is 4.908096971511841 and perplexity is 135.38153419325323
At time: 671.8047170639038 and batch: 200, loss is 4.911193704605102 and perplexity is 135.8014244789238
At time: 673.4747433662415 and batch: 250, loss is 4.917936477661133 and perplexity is 136.7201967202327
At time: 675.1503961086273 and batch: 300, loss is 4.946111640930176 and perplexity is 140.62709082403407
At time: 676.8258318901062 and batch: 350, loss is 4.905178003311157 and perplexity is 134.98693598996402
At time: 678.4878144264221 and batch: 400, loss is 4.930734281539917 and perplexity is 138.48115917550498
At time: 680.1478972434998 and batch: 450, loss is 4.907928094863892 and perplexity is 135.35867334395058
At time: 681.8169848918915 and batch: 500, loss is 4.878582267761231 and perplexity is 131.4441790740292
At time: 683.534913778305 and batch: 550, loss is 4.904860019683838 and perplexity is 134.94401917820062
At time: 685.2003717422485 and batch: 600, loss is 4.86791672706604 and perplexity is 130.0497054593715
At time: 686.8733289241791 and batch: 650, loss is 4.863812570571899 and perplexity is 129.51705490443445
At time: 688.5470824241638 and batch: 700, loss is 4.855370016098022 and perplexity is 128.4282029266518
At time: 690.2246754169464 and batch: 750, loss is 4.861926174163818 and perplexity is 129.27296469510725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984175216319949 and perplexity of 146.08303841865697
Finished 25 epochs...
Completing Train Step...
At time: 694.6582961082458 and batch: 50, loss is 4.935883274078369 and perplexity is 139.19603650041282
At time: 696.3692126274109 and batch: 100, loss is 4.923396654129029 and perplexity is 137.46875489114308
At time: 698.0480220317841 and batch: 150, loss is 4.901336441040039 and perplexity is 134.4693700369925
At time: 699.7210414409637 and batch: 200, loss is 4.903342428207398 and perplexity is 134.73938460001924
At time: 701.3958356380463 and batch: 250, loss is 4.9089843273162845 and perplexity is 135.50171909893476
At time: 703.0756521224976 and batch: 300, loss is 4.940014314651489 and perplexity is 139.77225033673417
At time: 704.7595808506012 and batch: 350, loss is 4.898579158782959 and perplexity is 134.0991107181719
At time: 706.4364709854126 and batch: 400, loss is 4.924062070846557 and perplexity is 137.56025933970403
At time: 708.1159217357635 and batch: 450, loss is 4.901443119049072 and perplexity is 134.48371572683524
At time: 709.7890005111694 and batch: 500, loss is 4.87276593208313 and perplexity is 130.68187466381838
At time: 711.4599933624268 and batch: 550, loss is 4.8977954387664795 and perplexity is 133.99405573313447
At time: 713.1329610347748 and batch: 600, loss is 4.860384187698364 and perplexity is 129.07378114183686
At time: 714.8024988174438 and batch: 650, loss is 4.856843585968018 and perplexity is 128.61759036048053
At time: 716.4837515354156 and batch: 700, loss is 4.849802694320679 and perplexity is 127.71518842790674
At time: 718.1603293418884 and batch: 750, loss is 4.855514879226685 and perplexity is 128.4468087855552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983095745707667 and perplexity of 145.92543115324116
Finished 26 epochs...
Completing Train Step...
At time: 722.614804983139 and batch: 50, loss is 4.929490098953247 and perplexity is 138.3089704679849
At time: 724.3757576942444 and batch: 100, loss is 4.9174501323699955 and perplexity is 136.65371966306662
At time: 726.0418131351471 and batch: 150, loss is 4.893850793838501 and perplexity is 133.4665378801436
At time: 727.703946352005 and batch: 200, loss is 4.896852293014526 and perplexity is 133.86773938540964
At time: 729.3622872829437 and batch: 250, loss is 4.902815828323364 and perplexity is 134.66844953455194
At time: 731.0402851104736 and batch: 300, loss is 4.934350919723511 and perplexity is 138.9829021880535
At time: 732.7163591384888 and batch: 350, loss is 4.89278881072998 and perplexity is 133.32487390704952
At time: 734.3942165374756 and batch: 400, loss is 4.918252916336059 and perplexity is 136.76346712397208
At time: 736.065765619278 and batch: 450, loss is 4.896893758773803 and perplexity is 133.87329042795423
At time: 737.732227563858 and batch: 500, loss is 4.867758302688599 and perplexity is 130.02910404767363
At time: 739.4021942615509 and batch: 550, loss is 4.892050657272339 and perplexity is 133.22649600384707
At time: 741.0712194442749 and batch: 600, loss is 4.854956703186035 and perplexity is 128.3751328601463
At time: 742.7426352500916 and batch: 650, loss is 4.851857719421386 and perplexity is 127.97791620949951
At time: 744.4174160957336 and batch: 700, loss is 4.844125232696533 and perplexity is 126.99214481515749
At time: 746.0851895809174 and batch: 750, loss is 4.849392261505127 and perplexity is 127.66278067919716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982263964275981 and perplexity of 145.80410355521397
Finished 27 epochs...
Completing Train Step...
At time: 750.4869275093079 and batch: 50, loss is 4.923820962905884 and perplexity is 137.5270964669328
At time: 752.1902849674225 and batch: 100, loss is 4.912504510879517 and perplexity is 135.97955055709076
At time: 753.86270403862 and batch: 150, loss is 4.887601165771485 and perplexity is 132.63502269574883
At time: 755.5332505702972 and batch: 200, loss is 4.891751956939697 and perplexity is 133.18670714794214
At time: 757.2016696929932 and batch: 250, loss is 4.898286514282226 and perplexity is 134.05987309248476
At time: 758.8736655712128 and batch: 300, loss is 4.931024389266968 and perplexity is 138.52133945785616
At time: 760.5513458251953 and batch: 350, loss is 4.888341093063355 and perplexity is 132.73319928618935
At time: 762.233635187149 and batch: 400, loss is 4.9126244735717775 and perplexity is 135.9958640085512
At time: 763.9013240337372 and batch: 450, loss is 4.891901378631592 and perplexity is 133.20660961795537
At time: 765.5699605941772 and batch: 500, loss is 4.862523460388184 and perplexity is 129.3502007198132
At time: 767.2953059673309 and batch: 550, loss is 4.88694317817688 and perplexity is 132.54777920192092
At time: 768.9757218360901 and batch: 600, loss is 4.849574327468872 and perplexity is 127.68602584240823
At time: 770.6511597633362 and batch: 650, loss is 4.846479711532592 and perplexity is 127.29149740384767
At time: 772.3192281723022 and batch: 700, loss is 4.839652347564697 and perplexity is 126.42539199379208
At time: 773.9892227649689 and batch: 750, loss is 4.84375379562378 and perplexity is 126.94498398380746
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980289547942405 and perplexity of 145.51650956022047
Finished 28 epochs...
Completing Train Step...
At time: 778.364729642868 and batch: 50, loss is 4.91663685798645 and perplexity is 136.54262787362495
At time: 780.048910856247 and batch: 100, loss is 4.906900005340576 and perplexity is 135.219584020385
At time: 781.7129380702972 and batch: 150, loss is 4.882143602371216 and perplexity is 131.91313032876724
At time: 783.3883864879608 and batch: 200, loss is 4.8838504123687745 and perplexity is 132.13847323247631
At time: 785.0752909183502 and batch: 250, loss is 4.891331481933594 and perplexity is 133.13071723844484
At time: 786.7584574222565 and batch: 300, loss is 4.925575342178345 and perplexity is 137.76858292185344
At time: 788.434900522232 and batch: 350, loss is 4.882700843811035 and perplexity is 131.9866582759496
At time: 790.103529214859 and batch: 400, loss is 4.907770309448242 and perplexity is 135.33731740428743
At time: 791.7721650600433 and batch: 450, loss is 4.88748122215271 and perplexity is 132.6191149251873
At time: 793.4477987289429 and batch: 500, loss is 4.857095565795898 and perplexity is 128.65000348231692
At time: 795.1363246440887 and batch: 550, loss is 4.88288462638855 and perplexity is 132.0109173533347
At time: 796.827573299408 and batch: 600, loss is 4.84488395690918 and perplexity is 127.08853339179419
At time: 798.529666185379 and batch: 650, loss is 4.842076539993286 and perplexity is 126.73224325482373
At time: 800.2281880378723 and batch: 700, loss is 4.834087524414063 and perplexity is 125.72381094333524
At time: 801.9189622402191 and batch: 750, loss is 4.83853627204895 and perplexity is 126.28437041922069
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978912353515625 and perplexity of 145.31624296891954
Finished 29 epochs...
Completing Train Step...
At time: 806.2750461101532 and batch: 50, loss is 4.910073843002319 and perplexity is 135.64943079987358
At time: 807.9945075511932 and batch: 100, loss is 4.901890516281128 and perplexity is 134.54389683043374
At time: 809.678183555603 and batch: 150, loss is 4.87692624092102 and perplexity is 131.22668412393594
At time: 811.3576800823212 and batch: 200, loss is 4.878814697265625 and perplexity is 131.4747341302376
At time: 813.0378091335297 and batch: 250, loss is 4.885397043228149 and perplexity is 132.34300079643904
At time: 814.7124857902527 and batch: 300, loss is 4.920281658172607 and perplexity is 137.04120652695124
At time: 816.3867521286011 and batch: 350, loss is 4.878605604171753 and perplexity is 131.44724654514457
At time: 818.0515015125275 and batch: 400, loss is 4.903124723434448 and perplexity is 134.7100543856672
At time: 819.7135255336761 and batch: 450, loss is 4.882623748779297 and perplexity is 131.97648315257157
At time: 821.3842399120331 and batch: 500, loss is 4.851528491973877 and perplexity is 127.93578930185593
At time: 823.0487949848175 and batch: 550, loss is 4.877974472045898 and perplexity is 131.36431213903174
At time: 824.7126622200012 and batch: 600, loss is 4.840058355331421 and perplexity is 126.47673210638968
At time: 826.3806636333466 and batch: 650, loss is 4.8368789577484135 and perplexity is 126.07525086247219
At time: 828.0527377128601 and batch: 700, loss is 4.829406576156616 and perplexity is 125.13667952753555
At time: 829.7123823165894 and batch: 750, loss is 4.833328552246094 and perplexity is 125.62842627171258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978631662767987 and perplexity of 145.27545976803802
Finished 30 epochs...
Completing Train Step...
At time: 834.0674071311951 and batch: 50, loss is 4.904060878753662 and perplexity is 134.8362229671119
At time: 835.7601163387299 and batch: 100, loss is 4.896689357757569 and perplexity is 133.84592938775157
At time: 837.4203460216522 and batch: 150, loss is 4.871682767868042 and perplexity is 130.54040136686012
At time: 839.0810823440552 and batch: 200, loss is 4.872820491790772 and perplexity is 130.68900482320203
At time: 840.7501361370087 and batch: 250, loss is 4.879860162734985 and perplexity is 131.6122583007237
At time: 842.4107582569122 and batch: 300, loss is 4.915472469329834 and perplexity is 136.38373171298045
At time: 844.0723214149475 and batch: 350, loss is 4.873595514297485 and perplexity is 130.7903310032832
At time: 845.7384655475616 and batch: 400, loss is 4.898735980987549 and perplexity is 134.12014208539787
At time: 847.4027111530304 and batch: 450, loss is 4.878087501525879 and perplexity is 131.3791610180856
At time: 849.0606107711792 and batch: 500, loss is 4.846555128097534 and perplexity is 127.30109765333249
At time: 850.7497718334198 and batch: 550, loss is 4.873042201995849 and perplexity is 130.7179831215363
At time: 852.4113507270813 and batch: 600, loss is 4.834897298812866 and perplexity is 125.82566009861087
At time: 854.0771923065186 and batch: 650, loss is 4.831905708312989 and perplexity is 125.44980373369845
At time: 855.7413856983185 and batch: 700, loss is 4.824156465530396 and perplexity is 124.48141971498312
At time: 857.4006431102753 and batch: 750, loss is 4.828294887542724 and perplexity is 124.99764380225984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.977900660315226 and perplexity of 145.16930185619572
Finished 31 epochs...
Completing Train Step...
At time: 861.7791862487793 and batch: 50, loss is 4.898846635818481 and perplexity is 134.1349839481917
At time: 863.4677584171295 and batch: 100, loss is 4.891799688339233 and perplexity is 133.19306448759505
At time: 865.1260139942169 and batch: 150, loss is 4.867028141021729 and perplexity is 129.934196433488
At time: 866.7843174934387 and batch: 200, loss is 4.86787938117981 and perplexity is 130.0448487285571
At time: 868.4541652202606 and batch: 250, loss is 4.875261192321777 and perplexity is 131.00836712199947
At time: 870.109735250473 and batch: 300, loss is 4.910787773132324 and perplexity is 135.7463095938698
At time: 871.7778129577637 and batch: 350, loss is 4.869568719863891 and perplexity is 130.26472419195153
At time: 873.4507927894592 and batch: 400, loss is 4.894325895309448 and perplexity is 133.52996309412583
At time: 875.120593547821 and batch: 450, loss is 4.873881072998047 and perplexity is 130.82768465332651
At time: 876.7940495014191 and batch: 500, loss is 4.841214733123779 and perplexity is 126.62307158620263
At time: 878.4742937088013 and batch: 550, loss is 4.869095020294189 and perplexity is 130.20303246098123
At time: 880.1365849971771 and batch: 600, loss is 4.830436410903931 and perplexity is 125.26561600851534
At time: 881.7997782230377 and batch: 650, loss is 4.82740345954895 and perplexity is 124.88626705297021
At time: 883.4684948921204 and batch: 700, loss is 4.819147338867188 and perplexity is 123.85943561551767
At time: 885.1370451450348 and batch: 750, loss is 4.823403158187866 and perplexity is 124.38768225849321
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982661757358285 and perplexity of 145.86211495648175
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 889.4904532432556 and batch: 50, loss is 4.896676740646362 and perplexity is 133.84424064942945
At time: 891.1736099720001 and batch: 100, loss is 4.893008241653442 and perplexity is 133.35413271727785
At time: 892.8364014625549 and batch: 150, loss is 4.86418197631836 and perplexity is 129.5649080868569
At time: 894.5066556930542 and batch: 200, loss is 4.862739610671997 and perplexity is 129.37816282430958
At time: 896.1733000278473 and batch: 250, loss is 4.859982347488403 and perplexity is 129.02192452625678
At time: 897.8356838226318 and batch: 300, loss is 4.886227436065674 and perplexity is 132.45294311773745
At time: 899.4992074966431 and batch: 350, loss is 4.842219743728638 and perplexity is 126.75039308497804
At time: 901.1662650108337 and batch: 400, loss is 4.860597324371338 and perplexity is 129.1012944300588
At time: 902.8291807174683 and batch: 450, loss is 4.830258769989014 and perplexity is 125.2433656862222
At time: 904.4969210624695 and batch: 500, loss is 4.783841276168824 and perplexity is 119.56274258877468
At time: 906.1564812660217 and batch: 550, loss is 4.797785606384277 and perplexity is 121.2416433216993
At time: 907.8205602169037 and batch: 600, loss is 4.753671112060547 and perplexity is 116.00938721049107
At time: 909.4883358478546 and batch: 650, loss is 4.740440282821655 and perplexity is 114.48459618647195
At time: 911.1675884723663 and batch: 700, loss is 4.728932666778564 and perplexity is 113.17470274133453
At time: 912.836715221405 and batch: 750, loss is 4.755023880004883 and perplexity is 116.16642718612559
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.941697231558866 and perplexity of 140.00767346322394
Finished 33 epochs...
Completing Train Step...
At time: 917.1991610527039 and batch: 50, loss is 4.870658664703369 and perplexity is 130.40678295989753
At time: 918.8904011249542 and batch: 100, loss is 4.866621475219727 and perplexity is 129.8813673819003
At time: 920.5510160923004 and batch: 150, loss is 4.840319976806641 and perplexity is 126.50982546440214
At time: 922.2159626483917 and batch: 200, loss is 4.838597469329834 and perplexity is 126.29209891578753
At time: 923.8816437721252 and batch: 250, loss is 4.8372047328948975 and perplexity is 126.11632973666474
At time: 925.5476558208466 and batch: 300, loss is 4.867889251708984 and perplexity is 130.04613234636545
At time: 927.2180652618408 and batch: 350, loss is 4.826475057601929 and perplexity is 124.77037620445086
At time: 928.8933629989624 and batch: 400, loss is 4.846021280288697 and perplexity is 127.23315637805173
At time: 930.5540788173676 and batch: 450, loss is 4.8185638999938964 and perplexity is 123.78719228278929
At time: 932.2119188308716 and batch: 500, loss is 4.774898357391358 and perplexity is 118.49826953535154
At time: 933.8943266868591 and batch: 550, loss is 4.7938217926025395 and perplexity is 120.76201523118354
At time: 935.5647964477539 and batch: 600, loss is 4.754603157043457 and perplexity is 116.11756358260233
At time: 937.2425758838654 and batch: 650, loss is 4.74636342048645 and perplexity is 115.16471644472311
At time: 938.9191372394562 and batch: 700, loss is 4.7378190231323245 and perplexity is 114.18489529797174
At time: 940.5873737335205 and batch: 750, loss is 4.7613082599639895 and perplexity is 116.8987598712875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.937339605287064 and perplexity of 139.39889971465556
Finished 34 epochs...
Completing Train Step...
At time: 944.961042881012 and batch: 50, loss is 4.863240928649902 and perplexity is 129.44303868364034
At time: 946.6443631649017 and batch: 100, loss is 4.857592525482178 and perplexity is 128.71395323659615
At time: 948.3022079467773 and batch: 150, loss is 4.830943775177002 and perplexity is 125.32918743229321
At time: 949.9659101963043 and batch: 200, loss is 4.829183750152588 and perplexity is 125.1087989276557
At time: 951.6255598068237 and batch: 250, loss is 4.828248996734619 and perplexity is 124.99190769099286
At time: 953.2839901447296 and batch: 300, loss is 4.859811038970947 and perplexity is 128.99982386471672
At time: 954.9412198066711 and batch: 350, loss is 4.820156135559082 and perplexity is 123.98444764968801
At time: 956.6047313213348 and batch: 400, loss is 4.8400419902801515 and perplexity is 126.47466232512042
At time: 958.2647950649261 and batch: 450, loss is 4.8144295787811275 and perplexity is 123.27647273465696
At time: 959.9246230125427 and batch: 500, loss is 4.772679710388184 and perplexity is 118.2356551368141
At time: 961.5814623832703 and batch: 550, loss is 4.794295806884765 and perplexity is 120.8192717202782
At time: 963.2502090930939 and batch: 600, loss is 4.757548856735229 and perplexity is 116.46011533345282
At time: 964.9274129867554 and batch: 650, loss is 4.750975694656372 and perplexity is 115.69711453082623
At time: 966.5974216461182 and batch: 700, loss is 4.743072843551635 and perplexity is 114.78638089757965
At time: 968.2574963569641 and batch: 750, loss is 4.764492797851562 and perplexity is 117.27162178244124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.935841050258903 and perplexity of 139.19015923617832
Finished 35 epochs...
Completing Train Step...
At time: 972.5998141765594 and batch: 50, loss is 4.858479537963867 and perplexity is 128.8281747701585
At time: 974.2846329212189 and batch: 100, loss is 4.85201774597168 and perplexity is 127.99839771269274
At time: 975.9418573379517 and batch: 150, loss is 4.825145282745361 and perplexity is 124.60456996224862
At time: 977.606773853302 and batch: 200, loss is 4.823518190383911 and perplexity is 124.40199166975007
At time: 979.2672300338745 and batch: 250, loss is 4.822803993225097 and perplexity is 124.31317584053042
At time: 980.9271695613861 and batch: 300, loss is 4.8551632690429685 and perplexity is 128.40165351851144
At time: 982.5869266986847 and batch: 350, loss is 4.816715793609619 and perplexity is 123.55863164967634
At time: 984.2585089206696 and batch: 400, loss is 4.836856594085694 and perplexity is 126.07243138961157
At time: 985.9178078174591 and batch: 450, loss is 4.812287406921387 and perplexity is 123.0126759936524
At time: 987.5888235569 and batch: 500, loss is 4.771943597793579 and perplexity is 118.1486524077675
At time: 989.2472686767578 and batch: 550, loss is 4.79549123764038 and perplexity is 120.96378915664745
At time: 990.9110488891602 and batch: 600, loss is 4.759795455932617 and perplexity is 116.7220486545138
At time: 992.5718710422516 and batch: 650, loss is 4.754004602432251 and perplexity is 116.04808167591023
At time: 994.233460187912 and batch: 700, loss is 4.746271495819092 and perplexity is 115.15413045303688
At time: 995.8941791057587 and batch: 750, loss is 4.766171798706055 and perplexity is 117.468686325095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.934998623160428 and perplexity of 139.07295105080428
Finished 36 epochs...
Completing Train Step...
At time: 1000.2276918888092 and batch: 50, loss is 4.8547874927520756 and perplexity is 128.3534122859311
At time: 1001.9159092903137 and batch: 100, loss is 4.847915182113647 and perplexity is 127.4743518132532
At time: 1003.5902268886566 and batch: 150, loss is 4.821058731079102 and perplexity is 124.09640597562962
At time: 1005.2639465332031 and batch: 200, loss is 4.819556102752686 and perplexity is 123.9100752287929
At time: 1006.9276702404022 and batch: 250, loss is 4.819112176895142 and perplexity is 123.8550805500717
At time: 1008.5888776779175 and batch: 300, loss is 4.8519383144378665 and perplexity is 127.9882310074206
At time: 1010.2549164295197 and batch: 350, loss is 4.814547719955445 and perplexity is 123.29103762225104
At time: 1011.9242541790009 and batch: 400, loss is 4.8350676155090335 and perplexity is 125.84709213439695
At time: 1013.5882568359375 and batch: 450, loss is 4.811264610290527 and perplexity is 122.88692336373876
At time: 1015.2495119571686 and batch: 500, loss is 4.771813917160034 and perplexity is 118.13333180908468
At time: 1016.9379320144653 and batch: 550, loss is 4.7962770175933835 and perplexity is 121.05887743154344
At time: 1018.5998136997223 and batch: 600, loss is 4.761577005386353 and perplexity is 116.930180099715
At time: 1020.2602331638336 and batch: 650, loss is 4.756081619262695 and perplexity is 116.28936598373434
At time: 1021.9216945171356 and batch: 700, loss is 4.748377599716187 and perplexity is 115.39691258840946
At time: 1023.5851962566376 and batch: 750, loss is 4.76722053527832 and perplexity is 117.59194465400165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.934480888898983 and perplexity of 139.00096685515504
Finished 37 epochs...
Completing Train Step...
At time: 1027.908763408661 and batch: 50, loss is 4.851954879760743 and perplexity is 127.99035119135229
At time: 1029.5946187973022 and batch: 100, loss is 4.844778261184692 and perplexity is 127.07510138704855
At time: 1031.2507979869843 and batch: 150, loss is 4.817979364395142 and perplexity is 123.71485540602959
At time: 1032.9090280532837 and batch: 200, loss is 4.8166288471221925 and perplexity is 123.54788912768221
At time: 1034.5745210647583 and batch: 250, loss is 4.816406536102295 and perplexity is 123.52042612322138
At time: 1036.2452795505524 and batch: 300, loss is 4.849607229232788 and perplexity is 127.69022700699836
At time: 1037.9041664600372 and batch: 350, loss is 4.812972583770752 and perplexity is 123.09699031323306
At time: 1039.5629341602325 and batch: 400, loss is 4.833748655319214 and perplexity is 125.68121424707952
At time: 1041.222708940506 and batch: 450, loss is 4.810564889907837 and perplexity is 122.80096695492776
At time: 1042.888103723526 and batch: 500, loss is 4.771658687591553 and perplexity is 118.11499544617439
At time: 1044.546469926834 and batch: 550, loss is 4.796777029037475 and perplexity is 121.11942339124303
At time: 1046.2039518356323 and batch: 600, loss is 4.762770414352417 and perplexity is 117.06980892562399
At time: 1047.87575507164 and batch: 650, loss is 4.757502126693725 and perplexity is 116.45467327458454
At time: 1049.545907497406 and batch: 700, loss is 4.749739208221436 and perplexity is 115.55414502628956
At time: 1051.2234802246094 and batch: 750, loss is 4.767832059860229 and perplexity is 117.66387701075256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.934235683707303 and perplexity of 138.96688727484948
Finished 38 epochs...
Completing Train Step...
At time: 1055.5935113430023 and batch: 50, loss is 4.849479942321778 and perplexity is 127.67397474680834
At time: 1057.294103860855 and batch: 100, loss is 4.842250862121582 and perplexity is 126.75433741488618
At time: 1058.96408700943 and batch: 150, loss is 4.815394515991211 and perplexity is 123.395484200388
At time: 1060.629860162735 and batch: 200, loss is 4.814303560256958 and perplexity is 123.26093859431339
At time: 1062.2977805137634 and batch: 250, loss is 4.814078006744385 and perplexity is 123.23313979182703
At time: 1063.972406387329 and batch: 300, loss is 4.8477983570098875 and perplexity is 127.45946047873369
At time: 1065.643432378769 and batch: 350, loss is 4.811689939498901 and perplexity is 122.93920187857415
At time: 1067.3088619709015 and batch: 400, loss is 4.832676820755005 and perplexity is 125.54657694492435
At time: 1068.9764332771301 and batch: 450, loss is 4.810003681182861 and perplexity is 122.7320693156237
At time: 1070.6509466171265 and batch: 500, loss is 4.771484317779541 and perplexity is 118.0944015521516
At time: 1072.320739030838 and batch: 550, loss is 4.796991033554077 and perplexity is 121.14534626860447
At time: 1073.991755247116 and batch: 600, loss is 4.763609743118286 and perplexity is 117.16811023165609
At time: 1075.6601140499115 and batch: 650, loss is 4.758551073074341 and perplexity is 116.5768920718979
At time: 1077.3412976264954 and batch: 700, loss is 4.750664110183716 and perplexity is 115.6610707220445
At time: 1079.0173654556274 and batch: 750, loss is 4.7681419563293455 and perplexity is 117.70034628133796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.933886150981104 and perplexity of 138.9183222879095
Finished 39 epochs...
Completing Train Step...
At time: 1083.3739693164825 and batch: 50, loss is 4.847468042373658 and perplexity is 127.4173657060541
At time: 1085.0645518302917 and batch: 100, loss is 4.8400171279907225 and perplexity is 126.47151791454904
At time: 1086.7330017089844 and batch: 150, loss is 4.8133440017700195 and perplexity is 123.14271924283914
At time: 1088.4077756404877 and batch: 200, loss is 4.8123265075683594 and perplexity is 123.01748596290543
At time: 1090.075119972229 and batch: 250, loss is 4.812200517654419 and perplexity is 123.00198797675122
At time: 1091.740783214569 and batch: 300, loss is 4.84619026184082 and perplexity is 127.25465825095682
At time: 1093.4210028648376 and batch: 350, loss is 4.810584535598755 and perplexity is 122.80337948846692
At time: 1095.0937633514404 and batch: 400, loss is 4.8317358016967775 and perplexity is 125.42849079269783
At time: 1096.763419866562 and batch: 450, loss is 4.809512386322021 and perplexity is 122.67178649024237
At time: 1098.4231224060059 and batch: 500, loss is 4.771222009658813 and perplexity is 118.06342849403185
At time: 1100.1141023635864 and batch: 550, loss is 4.7970758724212645 and perplexity is 121.15562453853916
At time: 1101.7807247638702 and batch: 600, loss is 4.764084129333496 and perplexity is 117.22370635398666
At time: 1103.4419784545898 and batch: 650, loss is 4.759212055206299 and perplexity is 116.65397278623769
At time: 1105.104005098343 and batch: 700, loss is 4.751296730041504 and perplexity is 115.73426336127375
At time: 1106.766755580902 and batch: 750, loss is 4.768297710418701 and perplexity is 117.71868001933014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.933661172556323 and perplexity of 138.88707217801993
Finished 40 epochs...
Completing Train Step...
At time: 1111.1007041931152 and batch: 50, loss is 4.845636672973633 and perplexity is 127.18423098452533
At time: 1112.78604722023 and batch: 100, loss is 4.838103818893432 and perplexity is 126.22977015162029
At time: 1114.4490802288055 and batch: 150, loss is 4.811549139022827 and perplexity is 122.92189319898526
At time: 1116.1055810451508 and batch: 200, loss is 4.810667600631714 and perplexity is 122.81358057890264
At time: 1117.7632570266724 and batch: 250, loss is 4.810584716796875 and perplexity is 122.80340174021038
At time: 1119.4227585792542 and batch: 300, loss is 4.844825134277344 and perplexity is 127.08105792964929
At time: 1121.0869274139404 and batch: 350, loss is 4.809592771530151 and perplexity is 122.68164788368084
At time: 1122.751981496811 and batch: 400, loss is 4.83095199584961 and perplexity is 125.3302177267461
At time: 1124.412991285324 and batch: 450, loss is 4.8089656639099125 and perplexity is 122.60473740552352
At time: 1126.0711629390717 and batch: 500, loss is 4.770864772796631 and perplexity is 118.02125941792082
At time: 1127.7361369132996 and batch: 550, loss is 4.797023067474365 and perplexity is 121.14922709112874
At time: 1129.3944082260132 and batch: 600, loss is 4.764368915557862 and perplexity is 117.25709480478508
At time: 1131.0537781715393 and batch: 650, loss is 4.759576349258423 and perplexity is 116.6964768762039
At time: 1132.712483406067 and batch: 700, loss is 4.751678953170776 and perplexity is 115.77850812872782
At time: 1134.372145652771 and batch: 750, loss is 4.768364019393921 and perplexity is 117.72648608316965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.933434774709302 and perplexity of 138.85563200302806
Finished 41 epochs...
Completing Train Step...
At time: 1138.7697875499725 and batch: 50, loss is 4.844064064025879 and perplexity is 126.98437711204811
At time: 1140.471616268158 and batch: 100, loss is 4.836449851989746 and perplexity is 126.02116285187005
At time: 1142.1407451629639 and batch: 150, loss is 4.810033750534058 and perplexity is 122.73575984480466
At time: 1143.8138897418976 and batch: 200, loss is 4.809125099182129 and perplexity is 122.62428648357168
At time: 1145.489021539688 and batch: 250, loss is 4.80907943725586 and perplexity is 122.61868735027801
At time: 1147.160975933075 and batch: 300, loss is 4.843651647567749 and perplexity is 126.93201746273441
At time: 1148.826031446457 and batch: 350, loss is 4.808682155609131 and perplexity is 122.56998287158613
At time: 1150.4996571540833 and batch: 400, loss is 4.830169343948365 and perplexity is 125.23216616868305
At time: 1152.1747107505798 and batch: 450, loss is 4.808491744995117 and perplexity is 122.5466464677079
At time: 1153.8388240337372 and batch: 500, loss is 4.770611248016357 and perplexity is 117.99134189663606
At time: 1155.5031945705414 and batch: 550, loss is 4.796905670166016 and perplexity is 121.13500533277406
At time: 1157.1654901504517 and batch: 600, loss is 4.764555654525757 and perplexity is 117.27899331823657
At time: 1158.8393511772156 and batch: 650, loss is 4.759792594909668 and perplexity is 116.72171471053163
At time: 1160.515285730362 and batch: 700, loss is 4.75194670677185 and perplexity is 115.80951239177257
At time: 1162.187513589859 and batch: 750, loss is 4.768333435058594 and perplexity is 117.72288555190256
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.933274734851926 and perplexity of 138.8334113456293
Finished 42 epochs...
Completing Train Step...
At time: 1166.5269606113434 and batch: 50, loss is 4.8425709247589115 and perplexity is 126.79491323546783
At time: 1168.227207660675 and batch: 100, loss is 4.834975957870483 and perplexity is 125.83555781572578
At time: 1169.8908495903015 and batch: 150, loss is 4.8086032295227055 and perplexity is 122.56030928427904
At time: 1171.5483965873718 and batch: 200, loss is 4.807767724990844 and perplexity is 122.45795235623869
At time: 1173.2109203338623 and batch: 250, loss is 4.807800884246826 and perplexity is 122.46201303815212
At time: 1174.8760635852814 and batch: 300, loss is 4.842491817474365 and perplexity is 126.78488323091595
At time: 1176.5469017028809 and batch: 350, loss is 4.807865600585938 and perplexity is 122.4699385877697
At time: 1178.212867975235 and batch: 400, loss is 4.829429340362549 and perplexity is 125.13952819710177
At time: 1179.8727622032166 and batch: 450, loss is 4.807991895675659 and perplexity is 122.48540691641824
At time: 1181.5334713459015 and batch: 500, loss is 4.770211191177368 and perplexity is 117.94414809409992
At time: 1183.2432832717896 and batch: 550, loss is 4.796681804656982 and perplexity is 121.10789041830378
At time: 1184.9230420589447 and batch: 600, loss is 4.7646816444396975 and perplexity is 117.2937702193625
At time: 1186.5927991867065 and batch: 650, loss is 4.7599123668670655 and perplexity is 116.73569553601197
At time: 1188.258606672287 and batch: 700, loss is 4.752087211608886 and perplexity is 115.82578533162503
At time: 1189.9302434921265 and batch: 750, loss is 4.768271417617798 and perplexity is 117.71558490620353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.93323641045149 and perplexity of 138.8280907403341
Finished 43 epochs...
Completing Train Step...
At time: 1194.2701165676117 and batch: 50, loss is 4.841185979843139 and perplexity is 126.6194308098322
At time: 1195.956199645996 and batch: 100, loss is 4.83365686416626 and perplexity is 125.66967835297372
At time: 1197.6295857429504 and batch: 150, loss is 4.807243680953979 and perplexity is 122.39379580843607
At time: 1199.290884256363 and batch: 200, loss is 4.806505746841431 and perplexity is 122.3035105677213
At time: 1200.9500479698181 and batch: 250, loss is 4.806586952209472 and perplexity is 122.31344267257437
At time: 1202.6099936962128 and batch: 300, loss is 4.8414200401306156 and perplexity is 126.64907085885149
At time: 1204.2710635662079 and batch: 350, loss is 4.807115259170533 and perplexity is 122.37807878812104
At time: 1205.9338212013245 and batch: 400, loss is 4.828711719512939 and perplexity is 125.04975767700815
At time: 1207.6032509803772 and batch: 450, loss is 4.8075150108337406 and perplexity is 122.42700940805355
At time: 1209.2662167549133 and batch: 500, loss is 4.769775743484497 and perplexity is 117.8928007672736
At time: 1210.9284312725067 and batch: 550, loss is 4.7964996814727785 and perplexity is 121.08583587205163
At time: 1212.5919914245605 and batch: 600, loss is 4.7646942043304445 and perplexity is 117.2952434255534
At time: 1214.2563633918762 and batch: 650, loss is 4.76002010345459 and perplexity is 116.74827291900128
At time: 1215.9165196418762 and batch: 700, loss is 4.752119312286377 and perplexity is 115.8295034774822
At time: 1217.5779447555542 and batch: 750, loss is 4.768094997406006 and perplexity is 117.6948193295708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932996173237646 and perplexity of 138.79474307244715
Finished 44 epochs...
Completing Train Step...
At time: 1221.9229128360748 and batch: 50, loss is 4.83995792388916 and perplexity is 126.46403050360256
At time: 1223.614800453186 and batch: 100, loss is 4.8323555755615235 and perplexity is 125.50625218793675
At time: 1225.2751195430756 and batch: 150, loss is 4.806117057800293 and perplexity is 122.25598177103052
At time: 1226.9380354881287 and batch: 200, loss is 4.8052795886993405 and perplexity is 122.15363902431625
At time: 1228.601613998413 and batch: 250, loss is 4.805372819900513 and perplexity is 122.16502808570978
At time: 1230.2612063884735 and batch: 300, loss is 4.840406141281128 and perplexity is 126.5207265866687
At time: 1231.921240568161 and batch: 350, loss is 4.8063264751434325 and perplexity is 122.28158697490922
At time: 1233.5807349681854 and batch: 400, loss is 4.8280111598968505 and perplexity is 124.96218354580948
At time: 1235.2385432720184 and batch: 450, loss is 4.806991243362427 and perplexity is 122.36290291282941
At time: 1236.901921749115 and batch: 500, loss is 4.769315767288208 and perplexity is 117.83858535506221
At time: 1238.5703327655792 and batch: 550, loss is 4.796205272674561 and perplexity is 121.05019238376708
At time: 1240.2425844669342 and batch: 600, loss is 4.7646238803863525 and perplexity is 117.28699505144498
At time: 1241.9014029502869 and batch: 650, loss is 4.759934940338135 and perplexity is 116.73833069560015
At time: 1243.5639457702637 and batch: 700, loss is 4.752140951156616 and perplexity is 115.83200992419613
At time: 1245.2250814437866 and batch: 750, loss is 4.767927131652832 and perplexity is 117.67506405824248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932830455691316 and perplexity of 138.77174425388822
Finished 45 epochs...
Completing Train Step...
At time: 1249.556203365326 and batch: 50, loss is 4.838799972534179 and perplexity is 126.31767606014095
At time: 1251.244787454605 and batch: 100, loss is 4.831171827316284 and perplexity is 125.35777228089569
At time: 1252.9041872024536 and batch: 150, loss is 4.805033760070801 and perplexity is 122.12361385343827
At time: 1254.5628056526184 and batch: 200, loss is 4.804132261276245 and perplexity is 122.01356917278886
At time: 1256.2185640335083 and batch: 250, loss is 4.804150562286377 and perplexity is 122.01580216478752
At time: 1257.881958246231 and batch: 300, loss is 4.8394537353515625 and perplexity is 126.400284860266
At time: 1259.5418264865875 and batch: 350, loss is 4.805590953826904 and perplexity is 122.19167932961835
At time: 1261.2019650936127 and batch: 400, loss is 4.827340097427368 and perplexity is 124.87835424482203
At time: 1262.857845067978 and batch: 450, loss is 4.806482276916504 and perplexity is 122.30064014719437
At time: 1264.5158216953278 and batch: 500, loss is 4.768875284194946 and perplexity is 117.78669088063782
At time: 1266.2270531654358 and batch: 550, loss is 4.7959443473815915 and perplexity is 121.0186114471671
At time: 1267.8939309120178 and batch: 600, loss is 4.764487848281861 and perplexity is 117.27104133981169
At time: 1269.5509223937988 and batch: 650, loss is 4.759815349578857 and perplexity is 116.72437070475505
At time: 1271.2094013690948 and batch: 700, loss is 4.752084007263184 and perplexity is 115.8254141863622
At time: 1272.87548494339 and batch: 750, loss is 4.767723026275635 and perplexity is 117.6510483958525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932689223178597 and perplexity of 138.75214655570156
Finished 46 epochs...
Completing Train Step...
At time: 1277.1978478431702 and batch: 50, loss is 4.837748556137085 and perplexity is 126.18493338043218
At time: 1278.8824512958527 and batch: 100, loss is 4.830127487182617 and perplexity is 125.22692446494081
At time: 1280.5476622581482 and batch: 150, loss is 4.803983001708985 and perplexity is 121.99535883932128
At time: 1282.2112591266632 and batch: 200, loss is 4.803112297058106 and perplexity is 121.88918314353819
At time: 1283.872695684433 and batch: 250, loss is 4.803005886077881 and perplexity is 121.87621348615032
At time: 1285.5309364795685 and batch: 300, loss is 4.8384240627288815 and perplexity is 126.27020093086986
At time: 1287.1954228878021 and batch: 350, loss is 4.804858255386352 and perplexity is 122.10218246783451
At time: 1288.8545670509338 and batch: 400, loss is 4.826673974990845 and perplexity is 124.79519767052828
At time: 1290.5155408382416 and batch: 450, loss is 4.805933113098145 and perplexity is 122.23349549905649
At time: 1292.1729741096497 and batch: 500, loss is 4.768323621749878 and perplexity is 117.72173030655213
At time: 1293.837298154831 and batch: 550, loss is 4.795507888793946 and perplexity is 120.9658033600459
At time: 1295.497062921524 and batch: 600, loss is 4.764312515258789 and perplexity is 117.25048165606322
At time: 1297.1539115905762 and batch: 650, loss is 4.759640626907348 and perplexity is 116.70397809245306
At time: 1298.811044216156 and batch: 700, loss is 4.7520064353942875 and perplexity is 115.81642974099267
At time: 1300.4762163162231 and batch: 750, loss is 4.767530117034912 and perplexity is 117.62835461042704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932587734488553 and perplexity of 138.73806549665306
Finished 47 epochs...
Completing Train Step...
At time: 1304.7900686264038 and batch: 50, loss is 4.836681509017945 and perplexity is 126.05035992166906
At time: 1306.4733729362488 and batch: 100, loss is 4.829000787734985 and perplexity is 125.08591081323681
At time: 1308.1323990821838 and batch: 150, loss is 4.8029514408111575 and perplexity is 121.86957808383458
At time: 1309.791553735733 and batch: 200, loss is 4.80202133178711 and perplexity is 121.75627878801858
At time: 1311.4538087844849 and batch: 250, loss is 4.801859140396118 and perplexity is 121.73653256917665
At time: 1313.1109583377838 and batch: 300, loss is 4.837503490447998 and perplexity is 126.15401357162771
At time: 1314.7697746753693 and batch: 350, loss is 4.8041094779968265 and perplexity is 122.01078933521657
At time: 1316.4291570186615 and batch: 400, loss is 4.825941171646118 and perplexity is 124.70378083164302
At time: 1318.0885405540466 and batch: 450, loss is 4.805399980545044 and perplexity is 122.16834621167271
At time: 1319.748386144638 and batch: 500, loss is 4.767834157943725 and perplexity is 117.66412387964999
At time: 1321.4181871414185 and batch: 550, loss is 4.795113649368286 and perplexity is 120.91812327050788
At time: 1323.0922346115112 and batch: 600, loss is 4.76407826423645 and perplexity is 117.22301882758897
At time: 1324.7585480213165 and batch: 650, loss is 4.759438991546631 and perplexity is 116.68044881598001
At time: 1326.420695066452 and batch: 700, loss is 4.75180290222168 and perplexity is 115.79285965433607
At time: 1328.096641778946 and batch: 750, loss is 4.767252006530762 and perplexity is 117.5956454780114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9324560830759445 and perplexity of 138.71980163660567
Finished 48 epochs...
Completing Train Step...
At time: 1332.4658224582672 and batch: 50, loss is 4.835618553161621 and perplexity is 125.91644513875886
At time: 1334.1542825698853 and batch: 100, loss is 4.827925281524658 and perplexity is 124.95145245769177
At time: 1335.8227710723877 and batch: 150, loss is 4.801916923522949 and perplexity is 121.7435670899144
At time: 1337.5001893043518 and batch: 200, loss is 4.800977611541748 and perplexity is 121.62926558950122
At time: 1339.1749880313873 and batch: 250, loss is 4.8007399845123295 and perplexity is 121.60036662215265
At time: 1340.8474657535553 and batch: 300, loss is 4.836601963043213 and perplexity is 126.04033352170899
At time: 1342.5171988010406 and batch: 350, loss is 4.803399715423584 and perplexity is 121.92422136840025
At time: 1344.184877872467 and batch: 400, loss is 4.825199937820434 and perplexity is 124.61138042048569
At time: 1345.8499500751495 and batch: 450, loss is 4.804872341156006 and perplexity is 122.10390238316411
At time: 1347.5187528133392 and batch: 500, loss is 4.767367534637451 and perplexity is 117.60923186507804
At time: 1349.2177424430847 and batch: 550, loss is 4.794745540618896 and perplexity is 120.87362044281485
At time: 1350.8924601078033 and batch: 600, loss is 4.763779325485229 and perplexity is 117.18798156198548
At time: 1352.559369802475 and batch: 650, loss is 4.759207553863526 and perplexity is 116.65344768790216
At time: 1354.2279996871948 and batch: 700, loss is 4.7516280364990235 and perplexity is 115.7726132225092
At time: 1355.9033463001251 and batch: 750, loss is 4.766985778808594 and perplexity is 117.56434242423416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932317334552144 and perplexity of 138.70055580410292
Finished 49 epochs...
Completing Train Step...
At time: 1360.2787930965424 and batch: 50, loss is 4.834621419906616 and perplexity is 125.7909522409278
At time: 1361.9751846790314 and batch: 100, loss is 4.826913013458252 and perplexity is 124.82503208896394
At time: 1363.6343431472778 and batch: 150, loss is 4.800918312072754 and perplexity is 121.62205325248352
At time: 1365.296064376831 and batch: 200, loss is 4.7999739074707035 and perplexity is 121.50724704596895
At time: 1366.9561727046967 and batch: 250, loss is 4.799545240402222 and perplexity is 121.45517205279323
At time: 1368.614596605301 and batch: 300, loss is 4.835655689239502 and perplexity is 125.92112126849808
At time: 1370.2773687839508 and batch: 350, loss is 4.802776069641113 and perplexity is 121.84820754727595
At time: 1371.9576189517975 and batch: 400, loss is 4.824420576095581 and perplexity is 124.51430091504412
At time: 1373.628735780716 and batch: 450, loss is 4.804325981140137 and perplexity is 122.03720791437492
At time: 1375.2867493629456 and batch: 500, loss is 4.767097749710083 and perplexity is 117.57750694664637
At time: 1376.946573972702 and batch: 550, loss is 4.794486846923828 and perplexity is 120.84235524353556
At time: 1378.607591867447 and batch: 600, loss is 4.763460483551025 and perplexity is 117.15062307532133
At time: 1380.2708268165588 and batch: 650, loss is 4.758937273025513 and perplexity is 116.6219227567883
At time: 1381.9275653362274 and batch: 700, loss is 4.751286334991455 and perplexity is 115.73306030406674
At time: 1383.585741519928 and batch: 750, loss is 4.766581106185913 and perplexity is 117.51677697831758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932236427484557 and perplexity of 138.6893344028116
Finished Training.
Improved accuracyfrom -142.25441600961204 to -138.6893344028116
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23008c4438>
SETTINGS FOR THIS RUN
{'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 13.708709542165955, 'dropout': 0.4697228231883231, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 7.584202789506056}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1117918491363525 and batch: 50, loss is 7.542807674407959 and perplexity is 1887.1210153776221
At time: 3.789721965789795 and batch: 100, loss is 6.504241714477539 and perplexity is 667.9689660694258
At time: 5.449251890182495 and batch: 150, loss is 6.186327123641968 and perplexity is 486.0575942867432
At time: 7.107357978820801 and batch: 200, loss is 6.133066568374634 and perplexity is 460.8472155227386
At time: 8.769777297973633 and batch: 250, loss is 6.1510665988922115 and perplexity is 469.2175869369642
At time: 10.438425302505493 and batch: 300, loss is 6.137709512710571 and perplexity is 462.99187841430006
At time: 12.100530862808228 and batch: 350, loss is 6.068077325820923 and perplexity is 431.84957682807254
At time: 13.760066747665405 and batch: 400, loss is 6.1093931484222415 and perplexity is 450.0655093138817
At time: 15.425901412963867 and batch: 450, loss is 6.092108201980591 and perplexity is 442.35299828110783
At time: 17.08843159675598 and batch: 500, loss is 6.105385637283325 and perplexity is 448.265476007892
At time: 18.751831769943237 and batch: 550, loss is 6.121964702606201 and perplexity is 455.7592468271699
At time: 20.416423797607422 and batch: 600, loss is 6.118294696807862 and perplexity is 454.0896732948883
At time: 22.091975212097168 and batch: 650, loss is 6.111933488845825 and perplexity is 451.2102823614436
At time: 23.757280588150024 and batch: 700, loss is 6.121288146972656 and perplexity is 455.4510046244458
At time: 25.426121711730957 and batch: 750, loss is 6.103996829986572 and perplexity is 447.64335374789215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.585699746775073 and perplexity of 266.5867607142625
Finished 1 epochs...
Completing Train Step...
At time: 29.789345502853394 and batch: 50, loss is 5.876885032653808 and perplexity is 356.69641171912144
At time: 31.442304372787476 and batch: 100, loss is 5.79569787979126 and perplexity is 328.88162375088274
At time: 33.0973596572876 and batch: 150, loss is 5.772065696716308 and perplexity is 321.2005507440291
At time: 34.75595474243164 and batch: 200, loss is 5.750170755386352 and perplexity is 314.244314614032
At time: 36.41689682006836 and batch: 250, loss is 5.770162992477417 and perplexity is 320.589982144528
At time: 38.07124304771423 and batch: 300, loss is 5.771080436706543 and perplexity is 320.88424053575386
At time: 39.729233503341675 and batch: 350, loss is 5.691452474594116 and perplexity is 296.3237108789847
At time: 41.39858961105347 and batch: 400, loss is 5.7295402240753175 and perplexity is 307.82770406869315
At time: 43.06469130516052 and batch: 450, loss is 5.687341279983521 and perplexity is 295.1079672275677
At time: 44.72188448905945 and batch: 500, loss is 5.681386232376099 and perplexity is 293.35580750867626
At time: 46.37761187553406 and batch: 550, loss is 5.700325994491577 and perplexity is 298.9648459758926
At time: 48.03192114830017 and batch: 600, loss is 5.667440528869629 and perplexity is 289.2931485566973
At time: 49.683526039123535 and batch: 650, loss is 5.6530286693573 and perplexity is 285.15379587213243
At time: 51.36350750923157 and batch: 700, loss is 5.6373758888244625 and perplexity is 280.7250972335729
At time: 53.04491186141968 and batch: 750, loss is 5.623885688781738 and perplexity is 276.9634889850951
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.415512439816497 and perplexity of 224.86774734632021
Finished 2 epochs...
Completing Train Step...
At time: 57.48674535751343 and batch: 50, loss is 5.667300424575806 and perplexity is 289.25262018357665
At time: 59.19137692451477 and batch: 100, loss is 5.64209363937378 and perplexity is 282.0526172070602
At time: 60.8521842956543 and batch: 150, loss is 5.642029695510864 and perplexity is 282.0345822497891
At time: 62.50965452194214 and batch: 200, loss is 5.6224525451660154 and perplexity is 276.5668448209827
At time: 64.17347455024719 and batch: 250, loss is 5.6595982074737545 and perplexity is 287.0332915526911
At time: 65.83633041381836 and batch: 300, loss is 5.654327688217163 and perplexity is 285.5244567265293
At time: 67.50146007537842 and batch: 350, loss is 5.586162214279175 and perplexity is 266.7100769407925
At time: 69.16051173210144 and batch: 400, loss is 5.612685117721558 and perplexity is 273.8786479861435
At time: 70.81573224067688 and batch: 450, loss is 5.588499383926392 and perplexity is 267.3341526385373
At time: 72.47575998306274 and batch: 500, loss is 5.582474794387817 and perplexity is 265.7284159083731
At time: 74.13478922843933 and batch: 550, loss is 5.586803426742554 and perplexity is 266.88114960733725
At time: 75.79316020011902 and batch: 600, loss is 5.545034275054932 and perplexity is 255.96335125084735
At time: 77.44994807243347 and batch: 650, loss is 5.552708129882813 and perplexity is 257.9351327559279
At time: 79.1107759475708 and batch: 700, loss is 5.542945938110352 and perplexity is 255.42937128707854
At time: 80.7699077129364 and batch: 750, loss is 5.542808589935302 and perplexity is 255.39429093824438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.366717316383539 and perplexity of 214.1586973053981
Finished 3 epochs...
Completing Train Step...
At time: 85.14528203010559 and batch: 50, loss is 5.5843092918396 and perplexity is 266.2163414224414
At time: 86.84447073936462 and batch: 100, loss is 5.562274951934814 and perplexity is 260.4145936416658
At time: 88.50666737556458 and batch: 150, loss is 5.5549564456939695 and perplexity is 258.51570480325034
At time: 90.16559600830078 and batch: 200, loss is 5.544820041656494 and perplexity is 255.9085212256543
At time: 91.82206535339355 and batch: 250, loss is 5.570702924728393 and perplexity is 262.6186355081632
At time: 93.48125672340393 and batch: 300, loss is 5.595573968887329 and perplexity is 269.232136622685
At time: 95.13961815834045 and batch: 350, loss is 5.520291633605957 and perplexity is 249.7078497725447
At time: 96.79930806159973 and batch: 400, loss is 5.556057901382446 and perplexity is 258.8006052706831
At time: 98.4585211277008 and batch: 450, loss is 5.516673793792725 and perplexity is 248.8060789866753
At time: 100.12907862663269 and batch: 500, loss is 5.5040577793121335 and perplexity is 245.68685534879847
At time: 101.79673385620117 and batch: 550, loss is 5.50914981842041 and perplexity is 246.94109302820294
At time: 103.47912907600403 and batch: 600, loss is 5.462108316421509 and perplexity is 235.59360699688327
At time: 105.14535808563232 and batch: 650, loss is 5.474158563613892 and perplexity is 238.44974220460938
At time: 106.8174319267273 and batch: 700, loss is 5.47196102142334 and perplexity is 237.92631417402353
At time: 108.48681282997131 and batch: 750, loss is 5.473578824996948 and perplexity is 238.3115437442159
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.345513277275618 and perplexity of 209.66547350807087
Finished 4 epochs...
Completing Train Step...
At time: 112.9117271900177 and batch: 50, loss is 5.517950315475463 and perplexity is 249.1238881432826
At time: 114.60114693641663 and batch: 100, loss is 5.520602016448975 and perplexity is 249.7853668342416
At time: 116.26106190681458 and batch: 150, loss is 5.529396018981934 and perplexity is 251.99166686835156
At time: 117.93420553207397 and batch: 200, loss is 5.52559905052185 and perplexity is 251.03667663827875
At time: 119.62437534332275 and batch: 250, loss is 5.551923322677612 and perplexity is 257.73278281849707
At time: 121.31582856178284 and batch: 300, loss is 5.570359621047974 and perplexity is 262.5284930380527
At time: 123.01023530960083 and batch: 350, loss is 5.502758598327636 and perplexity is 245.3678709123126
At time: 124.70062613487244 and batch: 400, loss is 5.542117252349853 and perplexity is 255.21778828429885
At time: 126.36253476142883 and batch: 450, loss is 5.510942754745483 and perplexity is 247.38423983233517
At time: 128.02308893203735 and batch: 500, loss is 5.506410837173462 and perplexity is 246.2656514384861
At time: 129.73024725914001 and batch: 550, loss is 5.515068483352661 and perplexity is 248.40698840836808
At time: 131.39236402511597 and batch: 600, loss is 5.468959112167358 and perplexity is 237.21315192898768
At time: 133.04930567741394 and batch: 650, loss is 5.471486492156982 and perplexity is 237.81343795836338
At time: 134.70809984207153 and batch: 700, loss is 5.445118274688721 and perplexity is 231.62467342825065
At time: 136.37271428108215 and batch: 750, loss is 5.454054327011108 and perplexity is 233.70375920370753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3698918542196585 and perplexity of 214.8396324482612
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 140.76202940940857 and batch: 50, loss is 5.467774820327759 and perplexity is 236.932388614579
At time: 142.44986295700073 and batch: 100, loss is 5.412437410354614 and perplexity is 224.17733446221698
At time: 144.11361622810364 and batch: 150, loss is 5.378279542922973 and perplexity is 216.64921892119094
At time: 145.7856330871582 and batch: 200, loss is 5.3459602642059325 and perplexity is 209.7592121828877
At time: 147.44447946548462 and batch: 250, loss is 5.34325044631958 and perplexity is 209.1915723655059
At time: 149.11278343200684 and batch: 300, loss is 5.344709949493408 and perplexity is 209.49711104240893
At time: 150.77470660209656 and batch: 350, loss is 5.273442287445068 and perplexity is 195.08635124554232
At time: 152.44017457962036 and batch: 400, loss is 5.270607051849365 and perplexity is 194.5340188444936
At time: 154.10701060295105 and batch: 450, loss is 5.230548954010009 and perplexity is 186.89537232890504
At time: 155.76778435707092 and batch: 500, loss is 5.18862943649292 and perplexity is 179.2227483663767
At time: 157.43109941482544 and batch: 550, loss is 5.15966682434082 and perplexity is 174.10643791338688
At time: 159.0955467224121 and batch: 600, loss is 5.096524248123169 and perplexity is 163.4527974629987
At time: 160.76142764091492 and batch: 650, loss is 5.039584512710571 and perplexity is 154.40584802925858
At time: 162.42246770858765 and batch: 700, loss is 5.018808326721191 and perplexity is 151.2309784549956
At time: 164.08557033538818 and batch: 750, loss is 5.042440500259399 and perplexity is 154.8474595269928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0676184365915695 and perplexity of 158.79569463759546
Finished 6 epochs...
Completing Train Step...
At time: 168.460294008255 and batch: 50, loss is 5.23059609413147 and perplexity is 186.90418280711893
At time: 170.14677619934082 and batch: 100, loss is 5.222451429367066 and perplexity is 185.38809329637743
At time: 171.8104784488678 and batch: 150, loss is 5.21128659248352 and perplexity is 183.3297772374265
At time: 173.47986268997192 and batch: 200, loss is 5.192860546112061 and perplexity is 179.98266597462123
At time: 175.14783430099487 and batch: 250, loss is 5.204282999038696 and perplexity is 182.05029572540124
At time: 176.84725737571716 and batch: 300, loss is 5.217590942382812 and perplexity is 184.48920317338565
At time: 178.5102822780609 and batch: 350, loss is 5.156650371551514 and perplexity is 173.58204536341196
At time: 180.17327976226807 and batch: 400, loss is 5.160646677017212 and perplexity is 174.27712018063536
At time: 181.8643147945404 and batch: 450, loss is 5.130617027282715 and perplexity is 169.12143839890925
At time: 183.53713941574097 and batch: 500, loss is 5.094790468215942 and perplexity is 163.1696518140237
At time: 185.19816327095032 and batch: 550, loss is 5.08642276763916 and perplexity is 161.80999356440273
At time: 186.85933709144592 and batch: 600, loss is 5.043087635040283 and perplexity is 154.94769913455426
At time: 188.5189757347107 and batch: 650, loss is 5.019222421646118 and perplexity is 151.29361540359173
At time: 190.17515563964844 and batch: 700, loss is 5.0121847438812255 and perplexity is 150.23259762037696
At time: 191.83550381660461 and batch: 750, loss is 5.03840334892273 and perplexity is 154.22357710003723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.040575958961664 and perplexity of 154.55900904134364
Finished 7 epochs...
Completing Train Step...
At time: 196.46227478981018 and batch: 50, loss is 5.177786874771118 and perplexity is 177.29001149917673
At time: 198.17273688316345 and batch: 100, loss is 5.166922349929809 and perplexity is 175.37426544461076
At time: 199.83586716651917 and batch: 150, loss is 5.154439678192139 and perplexity is 173.19873253808848
At time: 201.50072813034058 and batch: 200, loss is 5.142032442092895 and perplexity is 171.06309107190845
At time: 203.16563892364502 and batch: 250, loss is 5.154880962371826 and perplexity is 173.27517926482443
At time: 204.82528162002563 and batch: 300, loss is 5.168135423660278 and perplexity is 175.58713644702823
At time: 206.48858880996704 and batch: 350, loss is 5.110461759567261 and perplexity is 165.74687241009673
At time: 208.15144228935242 and batch: 400, loss is 5.119899244308471 and perplexity is 167.3185104795622
At time: 209.81558179855347 and batch: 450, loss is 5.091305112838745 and perplexity is 162.601937508434
At time: 211.47647213935852 and batch: 500, loss is 5.058851594924927 and perplexity is 157.40964245275183
At time: 213.1814591884613 and batch: 550, loss is 5.061819534301758 and perplexity is 157.87751869960155
At time: 214.84475803375244 and batch: 600, loss is 5.02567138671875 and perplexity is 152.27245550543097
At time: 216.50945115089417 and batch: 650, loss is 5.0126534271240235 and perplexity is 150.3030256243271
At time: 218.16954064369202 and batch: 700, loss is 5.0090585136413575 and perplexity is 149.76366930154288
At time: 219.83159804344177 and batch: 750, loss is 5.029274978637695 and perplexity is 152.82217317977248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.027422527934229 and perplexity of 152.53933968599296
Finished 8 epochs...
Completing Train Step...
At time: 224.21211743354797 and batch: 50, loss is 5.143869276046753 and perplexity is 171.37759432253915
At time: 225.91529941558838 and batch: 100, loss is 5.132408771514893 and perplexity is 169.42473239218975
At time: 227.57702326774597 and batch: 150, loss is 5.117509117126465 and perplexity is 166.91907549996787
At time: 229.23910093307495 and batch: 200, loss is 5.110299949645996 and perplexity is 165.7200550914348
At time: 230.90121817588806 and batch: 250, loss is 5.1256399345397945 and perplexity is 168.2817965361543
At time: 232.56226062774658 and batch: 300, loss is 5.141116380691528 and perplexity is 170.90645853044512
At time: 234.22338008880615 and batch: 350, loss is 5.082054090499878 and perplexity is 161.10463979787406
At time: 235.88549399375916 and batch: 400, loss is 5.095413312911988 and perplexity is 163.27131282243508
At time: 237.5498387813568 and batch: 450, loss is 5.067595510482788 and perplexity is 158.79205411195792
At time: 239.20901608467102 and batch: 500, loss is 5.040343589782715 and perplexity is 154.5230984637308
At time: 240.86999797821045 and batch: 550, loss is 5.047769145965576 and perplexity is 155.67478909275204
At time: 242.53350043296814 and batch: 600, loss is 5.0157378387451175 and perplexity is 150.76733771995376
At time: 244.19493675231934 and batch: 650, loss is 5.007047128677368 and perplexity is 149.4627396530981
At time: 245.85551595687866 and batch: 700, loss is 5.003065557479858 and perplexity is 148.8688262526433
At time: 247.51649737358093 and batch: 750, loss is 5.020142879486084 and perplexity is 151.43293890889638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.018875831781432 and perplexity of 151.24118765588892
Finished 9 epochs...
Completing Train Step...
At time: 251.9025309085846 and batch: 50, loss is 5.118102540969849 and perplexity is 167.01815865553093
At time: 253.59279036521912 and batch: 100, loss is 5.1046185970306395 and perplexity is 164.781210501922
At time: 255.25216698646545 and batch: 150, loss is 5.091297454833985 and perplexity is 162.60069230679045
At time: 256.91467118263245 and batch: 200, loss is 5.086251478195191 and perplexity is 161.78227959419948
At time: 258.58021998405457 and batch: 250, loss is 5.1010330772399906 and perplexity is 164.19144215541544
At time: 260.24001479148865 and batch: 300, loss is 5.1219593334198 and perplexity is 167.66355681219076
At time: 261.89899373054504 and batch: 350, loss is 5.064215488433838 and perplexity is 158.25623951056477
At time: 263.56061935424805 and batch: 400, loss is 5.079394540786743 and perplexity is 160.676743257592
At time: 265.2230064868927 and batch: 450, loss is 5.052659530639648 and perplexity is 156.43796328215353
At time: 266.8817341327667 and batch: 500, loss is 5.028230676651001 and perplexity is 152.66266398309162
At time: 268.54164004325867 and batch: 550, loss is 5.037533187866211 and perplexity is 154.08943611984046
At time: 270.2021338939667 and batch: 600, loss is 5.006814956665039 and perplexity is 149.4280426160609
At time: 271.865083694458 and batch: 650, loss is 5.0004894733428955 and perplexity is 148.48582116931385
At time: 273.5253188610077 and batch: 700, loss is 4.9976043796539305 and perplexity is 148.05804305046
At time: 275.18361139297485 and batch: 750, loss is 5.012107048034668 and perplexity is 150.220925624963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0102049361827765 and perplexity of 149.93546020158072
Finished 10 epochs...
Completing Train Step...
At time: 279.5603675842285 and batch: 50, loss is 5.09882532119751 and perplexity is 163.82934736243328
At time: 281.2455949783325 and batch: 100, loss is 5.087161951065063 and perplexity is 161.92964504653924
At time: 282.905011177063 and batch: 150, loss is 5.074628410339355 and perplexity is 159.9127590098786
At time: 284.56390476226807 and batch: 200, loss is 5.070286045074463 and perplexity is 159.21986488819928
At time: 286.2246174812317 and batch: 250, loss is 5.083462572097778 and perplexity is 161.33171259475424
At time: 287.8839018344879 and batch: 300, loss is 5.106705188751221 and perplexity is 165.12540057866514
At time: 289.54339241981506 and batch: 350, loss is 5.051214752197265 and perplexity is 156.21210827973968
At time: 291.2035086154938 and batch: 400, loss is 5.067952690124511 and perplexity is 158.84878153129065
At time: 292.8658492565155 and batch: 450, loss is 5.040084047317505 and perplexity is 154.48299836188835
At time: 294.529746055603 and batch: 500, loss is 5.016567821502686 and perplexity is 150.8925239546875
At time: 296.21923995018005 and batch: 550, loss is 5.027679195404053 and perplexity is 152.57849659729646
At time: 297.8857717514038 and batch: 600, loss is 4.997734289169312 and perplexity is 148.07727844848458
At time: 299.5474934577942 and batch: 650, loss is 4.989318733215332 and perplexity is 146.83635468155924
At time: 301.20713686943054 and batch: 700, loss is 4.9836024570465085 and perplexity is 145.99939196063926
At time: 302.864337682724 and batch: 750, loss is 4.996787338256836 and perplexity is 147.93712290522396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0037046920421515 and perplexity of 148.96400387716415
Finished 11 epochs...
Completing Train Step...
At time: 307.2243444919586 and batch: 50, loss is 5.077511873245239 and perplexity is 160.3745269440739
At time: 308.91250252723694 and batch: 100, loss is 5.06631422996521 and perplexity is 158.58872723387825
At time: 310.5683469772339 and batch: 150, loss is 5.05343279838562 and perplexity is 156.55897849595593
At time: 312.2266139984131 and batch: 200, loss is 5.049483194351196 and perplexity is 155.94185202768932
At time: 313.885217666626 and batch: 250, loss is 5.0613028430938725 and perplexity is 157.79596584440262
At time: 315.54226875305176 and batch: 300, loss is 5.086348419189453 and perplexity is 161.79796368944093
At time: 317.19755601882935 and batch: 350, loss is 5.031446580886841 and perplexity is 153.15440275945795
At time: 318.85605907440186 and batch: 400, loss is 5.048222036361694 and perplexity is 155.74530867726574
At time: 320.5179421901703 and batch: 450, loss is 5.020726728439331 and perplexity is 151.52137868698912
At time: 322.1929655075073 and batch: 500, loss is 4.996765022277832 and perplexity is 147.93382158033154
At time: 323.8611602783203 and batch: 550, loss is 5.008140525817871 and perplexity is 149.62625116046374
At time: 325.523761510849 and batch: 600, loss is 4.980343551635742 and perplexity is 145.5243682013741
At time: 327.1816773414612 and batch: 650, loss is 4.971606340408325 and perplexity is 144.2584294979383
At time: 328.8395080566406 and batch: 700, loss is 4.96450587272644 and perplexity is 143.23775510247242
At time: 330.4962453842163 and batch: 750, loss is 4.976061115264892 and perplexity is 144.90250185605163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.991346847179324 and perplexity of 147.1344577342116
Finished 12 epochs...
Completing Train Step...
At time: 334.8580746650696 and batch: 50, loss is 5.053618993759155 and perplexity is 156.5881317674552
At time: 336.5973346233368 and batch: 100, loss is 5.041928739547729 and perplexity is 154.7682349546458
At time: 338.2896075248718 and batch: 150, loss is 5.028034591674805 and perplexity is 152.63273206295435
At time: 339.9928560256958 and batch: 200, loss is 5.023309297561646 and perplexity is 151.9131988545141
At time: 341.6817100048065 and batch: 250, loss is 5.038435354232788 and perplexity is 154.22851315243014
At time: 343.34902334213257 and batch: 300, loss is 5.06514349937439 and perplexity is 158.40317119880336
At time: 345.00457406044006 and batch: 350, loss is 5.010217924118042 and perplexity is 149.93740756627784
At time: 346.6618084907532 and batch: 400, loss is 5.036989555358887 and perplexity is 154.00569085871606
At time: 348.3208749294281 and batch: 450, loss is 5.015967645645142 and perplexity is 150.8019890758679
At time: 349.98166966438293 and batch: 500, loss is 4.991495590209961 and perplexity is 147.15634458708422
At time: 351.65897154808044 and batch: 550, loss is 4.99883092880249 and perplexity is 148.23975493359774
At time: 353.35423040390015 and batch: 600, loss is 4.967789878845215 and perplexity is 143.70892200070762
At time: 355.03701663017273 and batch: 650, loss is 4.958592958450318 and perplexity is 142.39330158642548
At time: 356.69828248023987 and batch: 700, loss is 4.946394710540772 and perplexity is 140.66690371451674
At time: 358.37320470809937 and batch: 750, loss is 4.957053127288819 and perplexity is 142.1742086697947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980840283770894 and perplexity of 145.5966727880265
Finished 13 epochs...
Completing Train Step...
At time: 362.7664861679077 and batch: 50, loss is 5.0322236061096195 and perplexity is 153.273453840238
At time: 364.4785511493683 and batch: 100, loss is 5.025560483932495 and perplexity is 152.255569002243
At time: 366.13853764533997 and batch: 150, loss is 5.007503290176391 and perplexity is 149.53093435318212
At time: 367.80231738090515 and batch: 200, loss is 5.002421646118164 and perplexity is 148.77299877952754
At time: 369.466744184494 and batch: 250, loss is 5.016434412002564 and perplexity is 150.8723948012346
At time: 371.13919043540955 and batch: 300, loss is 5.047180194854736 and perplexity is 155.5831312464306
At time: 372.80524230003357 and batch: 350, loss is 4.995565423965454 and perplexity is 147.7564668161264
At time: 374.46953105926514 and batch: 400, loss is 5.008631687164307 and perplexity is 149.6997598422797
At time: 376.1337745189667 and batch: 450, loss is 4.982748508453369 and perplexity is 145.87476920356252
At time: 377.7970027923584 and batch: 500, loss is 4.9575340270996096 and perplexity is 142.2425966624117
At time: 379.501891374588 and batch: 550, loss is 4.968302631378174 and perplexity is 143.78262800932873
At time: 381.1626362800598 and batch: 600, loss is 4.937649803161621 and perplexity is 139.44214766442695
At time: 382.82668948173523 and batch: 650, loss is 4.928319864273071 and perplexity is 138.1472111807878
At time: 384.4958026409149 and batch: 700, loss is 4.922370309829712 and perplexity is 137.32773699706107
At time: 386.1564874649048 and batch: 750, loss is 4.933552160263061 and perplexity is 138.8719326049922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.968917491824128 and perplexity of 143.87106144443166
Finished 14 epochs...
Completing Train Step...
At time: 390.52445125579834 and batch: 50, loss is 5.011014080047607 and perplexity is 150.0568286549713
At time: 392.21522760391235 and batch: 100, loss is 5.002848358154297 and perplexity is 148.83649555521401
At time: 393.87869238853455 and batch: 150, loss is 4.985137519836425 and perplexity is 146.2236823004224
At time: 395.54952478408813 and batch: 200, loss is 4.978181428909302 and perplexity is 145.21006655943737
At time: 397.2247495651245 and batch: 250, loss is 4.990693473815918 and perplexity is 147.0383553974612
At time: 398.8997666835785 and batch: 300, loss is 5.018776330947876 and perplexity is 151.22613978029972
At time: 400.5639214515686 and batch: 350, loss is 4.969831161499023 and perplexity is 144.00257213988687
At time: 402.22905349731445 and batch: 400, loss is 4.982678899765014 and perplexity is 145.8646154056146
At time: 403.8947801589966 and batch: 450, loss is 4.954471130371093 and perplexity is 141.8075888105814
At time: 405.5644736289978 and batch: 500, loss is 4.928182125091553 and perplexity is 138.12818420740066
At time: 407.22722864151 and batch: 550, loss is 4.938532104492188 and perplexity is 139.56523194757708
At time: 408.8980784416199 and batch: 600, loss is 4.909282083511353 and perplexity is 135.5420715825417
At time: 410.56859016418457 and batch: 650, loss is 4.89888072013855 and perplexity is 134.1395559258329
At time: 412.2375650405884 and batch: 700, loss is 4.896715211868286 and perplexity is 133.84938989996303
At time: 413.90278029441833 and batch: 750, loss is 4.906787433624268 and perplexity is 135.20436297647888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.954435481581577 and perplexity of 141.8025336318022
Finished 15 epochs...
Completing Train Step...
At time: 418.30609488487244 and batch: 50, loss is 4.982096443176269 and perplexity is 145.77968033720313
At time: 420.00819206237793 and batch: 100, loss is 4.972294387817382 and perplexity is 144.3577202910584
At time: 421.6705758571625 and batch: 150, loss is 4.955491018295288 and perplexity is 141.9522904351454
At time: 423.33647108078003 and batch: 200, loss is 4.954145555496216 and perplexity is 141.76142733751337
At time: 424.99790716171265 and batch: 250, loss is 4.966189708709717 and perplexity is 143.4791471639623
At time: 426.6622955799103 and batch: 300, loss is 4.999168519973755 and perplexity is 148.28980781432372
At time: 428.3235137462616 and batch: 350, loss is 4.947494344711304 and perplexity is 140.82167092656627
At time: 429.9848086833954 and batch: 400, loss is 4.961546325683594 and perplexity is 142.81446291347316
At time: 431.65295147895813 and batch: 450, loss is 4.931319704055786 and perplexity is 138.56225289883974
At time: 433.316193819046 and batch: 500, loss is 4.907045030593872 and perplexity is 135.23919569686797
At time: 434.97956585884094 and batch: 550, loss is 4.918985595703125 and perplexity is 136.86370761208667
At time: 436.6411645412445 and batch: 600, loss is 4.891830167770386 and perplexity is 133.1971241983026
At time: 438.3068616390228 and batch: 650, loss is 4.881612768173218 and perplexity is 131.8431249103027
At time: 439.96955966949463 and batch: 700, loss is 4.8773320770263675 and perplexity is 131.27995145851114
At time: 441.6316010951996 and batch: 750, loss is 4.888141813278199 and perplexity is 132.70675087815656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.941818592160247 and perplexity of 140.02466590975925
Finished 16 epochs...
Completing Train Step...
At time: 446.0075333118439 and batch: 50, loss is 4.9658522796630855 and perplexity is 143.43074129935496
At time: 447.71016550064087 and batch: 100, loss is 4.953556728363037 and perplexity is 141.6779789334123
At time: 449.3694558143616 and batch: 150, loss is 4.93586106300354 and perplexity is 139.19294484116486
At time: 451.0287535190582 and batch: 200, loss is 4.938027687072754 and perplexity is 139.49485056572524
At time: 452.6895258426666 and batch: 250, loss is 4.948341703414917 and perplexity is 140.9410479655268
At time: 454.35149121284485 and batch: 300, loss is 4.982358522415161 and perplexity is 145.81789117178636
At time: 456.01332211494446 and batch: 350, loss is 4.931759662628174 and perplexity is 138.6232279620793
At time: 457.6772277355194 and batch: 400, loss is 4.948853569030762 and perplexity is 141.01320930872114
At time: 459.3369300365448 and batch: 450, loss is 4.916369075775147 and perplexity is 136.50606908191122
At time: 460.997088432312 and batch: 500, loss is 4.890473461151123 and perplexity is 133.0165373076507
At time: 462.69572949409485 and batch: 550, loss is 4.906931142807007 and perplexity is 135.22379448119446
At time: 464.3566098213196 and batch: 600, loss is 4.879258642196655 and perplexity is 131.5331146299179
At time: 466.0187141895294 and batch: 650, loss is 4.867189912796021 and perplexity is 129.95521781927386
At time: 467.6805171966553 and batch: 700, loss is 4.863859987258911 and perplexity is 129.52319631969112
At time: 469.34566855430603 and batch: 750, loss is 4.874642190933227 and perplexity is 130.92729785442245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.938273230264353 and perplexity of 139.529106782063
Finished 17 epochs...
Completing Train Step...
At time: 473.7508635520935 and batch: 50, loss is 4.948856439590454 and perplexity is 141.01361409613685
At time: 475.4376177787781 and batch: 100, loss is 4.939135980606079 and perplexity is 139.64953750997572
At time: 477.0968041419983 and batch: 150, loss is 4.9216993713378905 and perplexity is 137.23562943502463
At time: 478.757488489151 and batch: 200, loss is 4.923901720046997 and perplexity is 137.53820321053743
At time: 480.41720747947693 and batch: 250, loss is 4.932146339416504 and perplexity is 138.67684071139047
At time: 482.0781741142273 and batch: 300, loss is 4.971017332077026 and perplexity is 144.17348510003876
At time: 483.7435863018036 and batch: 350, loss is 4.920077447891235 and perplexity is 137.0132241608475
At time: 485.4120900630951 and batch: 400, loss is 4.933947134017944 and perplexity is 138.92679420739438
At time: 487.0747411251068 and batch: 450, loss is 4.901968221664429 and perplexity is 134.55435202171483
At time: 488.737074136734 and batch: 500, loss is 4.877688465118408 and perplexity is 131.32674640802068
At time: 490.40050196647644 and batch: 550, loss is 4.892443246841431 and perplexity is 133.27880960471995
At time: 492.05964732170105 and batch: 600, loss is 4.863015909194946 and perplexity is 129.41391475847564
At time: 493.72000336647034 and batch: 650, loss is 4.854251861572266 and perplexity is 128.28468060533461
At time: 495.38023042678833 and batch: 700, loss is 4.852842178344726 and perplexity is 128.10396724695644
At time: 497.0399053096771 and batch: 750, loss is 4.86333381652832 and perplexity is 129.45506293132448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.930111197538154 and perplexity of 138.3949006566194
Finished 18 epochs...
Completing Train Step...
At time: 501.3835413455963 and batch: 50, loss is 4.936277055740357 and perplexity is 139.25086014057166
At time: 503.07064986228943 and batch: 100, loss is 4.925588092803955 and perplexity is 137.7703395686743
At time: 504.7307026386261 and batch: 150, loss is 4.909890336990356 and perplexity is 135.62454059762322
At time: 506.39180517196655 and batch: 200, loss is 4.913810806274414 and perplexity is 136.15729608659151
At time: 508.0545814037323 and batch: 250, loss is 4.920842304229736 and perplexity is 137.11805968075052
At time: 509.72002816200256 and batch: 300, loss is 4.959343566894531 and perplexity is 142.50022332415517
At time: 511.3850448131561 and batch: 350, loss is 4.909127044677734 and perplexity is 135.5210589267882
At time: 513.0425670146942 and batch: 400, loss is 4.926939525604248 and perplexity is 137.9566527910184
At time: 514.7028000354767 and batch: 450, loss is 4.8945033073425295 and perplexity is 133.55365501790968
At time: 516.3643043041229 and batch: 500, loss is 4.870740299224853 and perplexity is 130.41742908976266
At time: 518.0351595878601 and batch: 550, loss is 4.882432317733764 and perplexity is 131.95122117445655
At time: 519.6974294185638 and batch: 600, loss is 4.852488946914673 and perplexity is 128.05872489039186
At time: 521.3625433444977 and batch: 650, loss is 4.843734455108643 and perplexity is 126.94252882616524
At time: 523.0245122909546 and batch: 700, loss is 4.841464433670044 and perplexity is 126.65469338417323
At time: 524.6938452720642 and batch: 750, loss is 4.851841983795166 and perplexity is 127.97590241268976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.930379113485647 and perplexity of 138.4319838249213
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 529.0738377571106 and batch: 50, loss is 4.928846378326416 and perplexity is 138.21996678064698
At time: 530.7642719745636 and batch: 100, loss is 4.922145376205444 and perplexity is 137.29685084526307
At time: 532.4296901226044 and batch: 150, loss is 4.902029333114624 and perplexity is 134.56257508455596
At time: 534.0972757339478 and batch: 200, loss is 4.900554666519165 and perplexity is 134.36428639087015
At time: 535.7611765861511 and batch: 250, loss is 4.9011573982238765 and perplexity is 134.44529641746215
At time: 537.4280602931976 and batch: 300, loss is 4.9320543670654295 and perplexity is 138.66408686282037
At time: 539.0930533409119 and batch: 350, loss is 4.880538702011108 and perplexity is 131.7015926922191
At time: 540.7537677288055 and batch: 400, loss is 4.89248875617981 and perplexity is 133.28487517318393
At time: 542.4162404537201 and batch: 450, loss is 4.8517866039276125 and perplexity is 127.96881532040709
At time: 544.0838356018066 and batch: 500, loss is 4.8066692066192624 and perplexity is 122.32350390639554
At time: 545.7744626998901 and batch: 550, loss is 4.809275903701782 and perplexity is 122.6427801746237
At time: 547.4372756481171 and batch: 600, loss is 4.7809398746490475 and perplexity is 119.21634582676595
At time: 549.0966811180115 and batch: 650, loss is 4.754606733322143 and perplexity is 116.11797885211267
At time: 550.7579109668732 and batch: 700, loss is 4.744411344528198 and perplexity is 114.94012545117087
At time: 552.4210405349731 and batch: 750, loss is 4.774136409759522 and perplexity is 118.40801444869376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.882694333098655 and perplexity of 131.98579895157698
Finished 20 epochs...
Completing Train Step...
At time: 556.7672595977783 and batch: 50, loss is 4.899541702270508 and perplexity is 134.22824908455604
At time: 558.4550898075104 and batch: 100, loss is 4.891164407730103 and perplexity is 133.10847638789002
At time: 560.1214542388916 and batch: 150, loss is 4.875278129577636 and perplexity is 131.01058606302445
At time: 561.7822988033295 and batch: 200, loss is 4.874794425964356 and perplexity is 130.947231092917
At time: 563.4422814846039 and batch: 250, loss is 4.8770988655090335 and perplexity is 131.24933903155997
At time: 565.1046912670135 and batch: 300, loss is 4.91199728012085 and perplexity is 135.9105950361575
At time: 566.7670137882233 and batch: 350, loss is 4.863212404251098 and perplexity is 129.43934645144216
At time: 568.4279892444611 and batch: 400, loss is 4.876576089859009 and perplexity is 131.18074300476098
At time: 570.0934038162231 and batch: 450, loss is 4.839240207672119 and perplexity is 126.37329778210538
At time: 571.7564990520477 and batch: 500, loss is 4.797019157409668 and perplexity is 121.14875339073893
At time: 573.4251070022583 and batch: 550, loss is 4.803273935317993 and perplexity is 121.90888669138126
At time: 575.0992693901062 and batch: 600, loss is 4.778165826797485 and perplexity is 118.88609226013706
At time: 576.7696475982666 and batch: 650, loss is 4.757719087600708 and perplexity is 116.479942127198
At time: 578.4321866035461 and batch: 700, loss is 4.752302894592285 and perplexity is 115.85076967681204
At time: 580.0909445285797 and batch: 750, loss is 4.780307970046997 and perplexity is 119.14103626593337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.877055323401163 and perplexity of 131.24362428309894
Finished 21 epochs...
Completing Train Step...
At time: 584.3980975151062 and batch: 50, loss is 4.891219434738159 and perplexity is 133.11580115062057
At time: 586.081209897995 and batch: 100, loss is 4.880047855377197 and perplexity is 131.6369632716297
At time: 587.7500276565552 and batch: 150, loss is 4.865120553970337 and perplexity is 129.68657190060463
At time: 589.4085142612457 and batch: 200, loss is 4.865221338272095 and perplexity is 129.69964292986717
At time: 591.0670397281647 and batch: 250, loss is 4.867921209335327 and perplexity is 130.05028837847837
At time: 592.7374684810638 and batch: 300, loss is 4.903642110824585 and perplexity is 134.77976970248866
At time: 594.4015092849731 and batch: 350, loss is 4.855767602920532 and perplexity is 128.47927443978168
At time: 596.0663714408875 and batch: 400, loss is 4.869813795089722 and perplexity is 130.29665276094144
At time: 597.7275314331055 and batch: 450, loss is 4.834055995941162 and perplexity is 125.7198471260558
At time: 599.3870778083801 and batch: 500, loss is 4.793223104476929 and perplexity is 120.68973808453337
At time: 601.0479881763458 and batch: 550, loss is 4.802226696014404 and perplexity is 121.78128573980895
At time: 602.7265520095825 and batch: 600, loss is 4.778446607589721 and perplexity is 118.91947787810852
At time: 604.4116711616516 and batch: 650, loss is 4.760624685287476 and perplexity is 116.81887814499676
At time: 606.1003963947296 and batch: 700, loss is 4.7567312622070315 and perplexity is 116.36493709430857
At time: 607.788046836853 and batch: 750, loss is 4.782936916351319 and perplexity is 119.45466372716267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.874597948650981 and perplexity of 130.92150546009242
Finished 22 epochs...
Completing Train Step...
At time: 612.1184210777283 and batch: 50, loss is 4.8854299163818355 and perplexity is 132.34735139975217
At time: 613.8030545711517 and batch: 100, loss is 4.873096075057983 and perplexity is 130.7250254892583
At time: 615.4626407623291 and batch: 150, loss is 4.858711204528809 and perplexity is 128.8580234082074
At time: 617.1244976520538 and batch: 200, loss is 4.859436521530151 and perplexity is 128.95152022666184
At time: 618.7959144115448 and batch: 250, loss is 4.862404804229737 and perplexity is 129.3348534324445
At time: 620.4527258872986 and batch: 300, loss is 4.898274517059326 and perplexity is 134.05826475595308
At time: 622.1113290786743 and batch: 350, loss is 4.850932903289795 and perplexity is 127.85961488002228
At time: 623.7699732780457 and batch: 400, loss is 4.865485134124756 and perplexity is 129.73386167094628
At time: 625.4384489059448 and batch: 450, loss is 4.830820188522339 and perplexity is 125.31369937436517
At time: 627.0971713066101 and batch: 500, loss is 4.791570854187012 and perplexity is 120.4904930764315
At time: 628.7802369594574 and batch: 550, loss is 4.80185417175293 and perplexity is 121.73592770528599
At time: 630.4400043487549 and batch: 600, loss is 4.7793867301940915 and perplexity is 119.03132933616551
At time: 632.0957250595093 and batch: 650, loss is 4.7627615547180175 and perplexity is 117.06877173451227
At time: 633.7536070346832 and batch: 700, loss is 4.759052209854126 and perplexity is 116.63532768108168
At time: 635.4246544837952 and batch: 750, loss is 4.784396486282349 and perplexity is 119.62914346417384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8731909463571945 and perplexity of 130.73742813058394
Finished 23 epochs...
Completing Train Step...
At time: 639.7675440311432 and batch: 50, loss is 4.8806956768035885 and perplexity is 131.72226814512126
At time: 641.4497354030609 and batch: 100, loss is 4.868062629699707 and perplexity is 130.06868143819452
At time: 643.1091177463531 and batch: 150, loss is 4.853721303939819 and perplexity is 128.21663624123804
At time: 644.7679581642151 and batch: 200, loss is 4.855417394638062 and perplexity is 128.43428781155174
At time: 646.4258692264557 and batch: 250, loss is 4.858536357879639 and perplexity is 128.83549498416295
At time: 648.0827395915985 and batch: 300, loss is 4.894177017211914 and perplexity is 133.51008488700808
At time: 649.7486302852631 and batch: 350, loss is 4.847626514434815 and perplexity is 127.43755939865054
At time: 651.4067015647888 and batch: 400, loss is 4.862347764968872 and perplexity is 129.32747647839065
At time: 653.0662500858307 and batch: 450, loss is 4.828594446182251 and perplexity is 125.03509353529678
At time: 654.7275891304016 and batch: 500, loss is 4.790658254623413 and perplexity is 120.38058366430094
At time: 656.3878529071808 and batch: 550, loss is 4.801631870269776 and perplexity is 121.70886863575106
At time: 658.0484278202057 and batch: 600, loss is 4.779584417343139 and perplexity is 119.05486262634724
At time: 659.7073271274567 and batch: 650, loss is 4.763810663223267 and perplexity is 117.19165402579586
At time: 661.3663566112518 and batch: 700, loss is 4.760701866149902 and perplexity is 116.82789467470698
At time: 663.0264294147491 and batch: 750, loss is 4.784850473403931 and perplexity is 119.68346588457909
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.871911692064862 and perplexity of 130.5702886442256
Finished 24 epochs...
Completing Train Step...
At time: 667.3684270381927 and batch: 50, loss is 4.877031230926514 and perplexity is 131.24046233749775
At time: 669.0626907348633 and batch: 100, loss is 4.864159154891968 and perplexity is 129.56195126458354
At time: 670.7268378734589 and batch: 150, loss is 4.850134534835815 and perplexity is 127.75757653448201
At time: 672.3902540206909 and batch: 200, loss is 4.852601728439331 and perplexity is 128.07316836309388
At time: 674.043422460556 and batch: 250, loss is 4.855446138381958 and perplexity is 128.43797954688497
At time: 675.6982431411743 and batch: 300, loss is 4.890991439819336 and perplexity is 133.085454883872
At time: 677.3525047302246 and batch: 350, loss is 4.845098295211792 and perplexity is 127.11577625181276
At time: 679.0103204250336 and batch: 400, loss is 4.859978179931641 and perplexity is 129.02138682118326
At time: 680.6768057346344 and batch: 450, loss is 4.826472415924072 and perplexity is 124.77004660174619
At time: 682.3328211307526 and batch: 500, loss is 4.7895173072814945 and perplexity is 120.24331408092475
At time: 683.9948501586914 and batch: 550, loss is 4.801197643280029 and perplexity is 121.65603083272822
At time: 685.6565690040588 and batch: 600, loss is 4.779455890655518 and perplexity is 119.03956188250665
At time: 687.3215544223785 and batch: 650, loss is 4.7639450836181645 and perplexity is 117.20740803301493
At time: 688.9884583950043 and batch: 700, loss is 4.761326341629029 and perplexity is 116.90087361461691
At time: 690.6575784683228 and batch: 750, loss is 4.784663419723511 and perplexity is 119.66108074547661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.870655503383903 and perplexity of 130.4063707030476
Finished 25 epochs...
Completing Train Step...
At time: 695.0270001888275 and batch: 50, loss is 4.873913040161133 and perplexity is 130.83186691010513
At time: 696.7116782665253 and batch: 100, loss is 4.860916929244995 and perplexity is 129.14256242732932
At time: 698.3759787082672 and batch: 150, loss is 4.847077398300171 and perplexity is 127.36760058813883
At time: 700.0408210754395 and batch: 200, loss is 4.850151395797729 and perplexity is 127.75973066827449
At time: 701.7043015956879 and batch: 250, loss is 4.852729406356811 and perplexity is 128.08952152246204
At time: 703.3756487369537 and batch: 300, loss is 4.8883856010437015 and perplexity is 132.73910710428606
At time: 705.0472583770752 and batch: 350, loss is 4.842757358551025 and perplexity is 126.8185542956408
At time: 706.7118630409241 and batch: 400, loss is 4.857854318618775 and perplexity is 128.74765407726963
At time: 708.3710238933563 and batch: 450, loss is 4.8250361251831055 and perplexity is 124.59096917347358
At time: 710.0290379524231 and batch: 500, loss is 4.788119888305664 and perplexity is 120.07540114179974
At time: 711.7149343490601 and batch: 550, loss is 4.800355710983276 and perplexity is 121.55364779711539
At time: 713.3776345252991 and batch: 600, loss is 4.779155883789063 and perplexity is 119.00385455305191
At time: 715.0390210151672 and batch: 650, loss is 4.763985509872437 and perplexity is 117.21214638527091
At time: 716.6961033344269 and batch: 700, loss is 4.761405544281006 and perplexity is 116.91013284049843
At time: 718.3562514781952 and batch: 750, loss is 4.78413990020752 and perplexity is 119.59845222945732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.869296406590661 and perplexity of 130.22925620794098
Finished 26 epochs...
Completing Train Step...
At time: 722.6925547122955 and batch: 50, loss is 4.871081714630127 and perplexity is 130.461963211055
At time: 724.385778427124 and batch: 100, loss is 4.858083066940307 and perplexity is 128.77710825570958
At time: 726.0443148612976 and batch: 150, loss is 4.84432095527649 and perplexity is 127.01700247789852
At time: 727.7055497169495 and batch: 200, loss is 4.847601366043091 and perplexity is 127.43435458928452
At time: 729.3760583400726 and batch: 250, loss is 4.849930467605591 and perplexity is 127.73150805964967
At time: 731.0422542095184 and batch: 300, loss is 4.886097574234009 and perplexity is 132.43574365273594
At time: 732.7034709453583 and batch: 350, loss is 4.84081389427185 and perplexity is 126.57232631057872
At time: 734.3725278377533 and batch: 400, loss is 4.855955753326416 and perplexity is 128.50345014167792
At time: 736.034494638443 and batch: 450, loss is 4.823608493804931 and perplexity is 124.41322610242578
At time: 737.6953897476196 and batch: 500, loss is 4.787001705169677 and perplexity is 119.94120989237422
At time: 739.3535804748535 and batch: 550, loss is 4.799582738876342 and perplexity is 121.45972652181155
At time: 741.014732837677 and batch: 600, loss is 4.778600130081177 and perplexity is 118.9377360941225
At time: 742.6781015396118 and batch: 650, loss is 4.763766536712646 and perplexity is 117.18648288112297
At time: 744.3347721099854 and batch: 700, loss is 4.761508197784424 and perplexity is 116.92213469122501
At time: 745.9905483722687 and batch: 750, loss is 4.784157466888428 and perplexity is 119.6005531957582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.868789317996003 and perplexity of 130.16323517809752
Finished 27 epochs...
Completing Train Step...
At time: 750.3049564361572 and batch: 50, loss is 4.868764410018921 and perplexity is 130.15999311559546
At time: 751.9812889099121 and batch: 100, loss is 4.8558032512664795 and perplexity is 128.48385459504092
At time: 753.6380825042725 and batch: 150, loss is 4.84217532157898 and perplexity is 126.74476268510534
At time: 755.3035318851471 and batch: 200, loss is 4.845342769622802 and perplexity is 127.14685660536281
At time: 756.9665648937225 and batch: 250, loss is 4.8472609710693355 and perplexity is 127.39098395748988
At time: 758.6236119270325 and batch: 300, loss is 4.884102687835694 and perplexity is 132.1718127327012
At time: 760.2868282794952 and batch: 350, loss is 4.839372873306274 and perplexity is 126.39006428794096
At time: 761.9517042636871 and batch: 400, loss is 4.854620695114136 and perplexity is 128.3320050253302
At time: 763.610518693924 and batch: 450, loss is 4.822390117645264 and perplexity is 124.26173629831793
At time: 765.2655022144318 and batch: 500, loss is 4.786141567230224 and perplexity is 119.83808826301255
At time: 766.9316923618317 and batch: 550, loss is 4.798731842041016 and perplexity is 121.35642078239086
At time: 768.5909376144409 and batch: 600, loss is 4.7779050636291505 and perplexity is 118.85509518767142
At time: 770.2501740455627 and batch: 650, loss is 4.763086471557617 and perplexity is 117.1068155300403
At time: 771.9245352745056 and batch: 700, loss is 4.761028060913086 and perplexity is 116.86600953824012
At time: 773.5965805053711 and batch: 750, loss is 4.783597412109375 and perplexity is 119.53358908789824
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8681420614553055 and perplexity of 130.07901343224836
Finished 28 epochs...
Completing Train Step...
At time: 777.9321579933167 and batch: 50, loss is 4.866632051467896 and perplexity is 129.88274104673832
At time: 779.6246695518494 and batch: 100, loss is 4.853756923675537 and perplexity is 128.22120336527493
At time: 781.2819283008575 and batch: 150, loss is 4.8402790832519536 and perplexity is 126.50465213371467
At time: 782.9548513889313 and batch: 200, loss is 4.843280277252197 and perplexity is 126.88488743124464
At time: 784.6138384342194 and batch: 250, loss is 4.8448437976837155 and perplexity is 127.08342971720829
At time: 786.2732956409454 and batch: 300, loss is 4.8822471714019775 and perplexity is 131.92679315133051
At time: 787.9318087100983 and batch: 350, loss is 4.838047180175781 and perplexity is 126.22262086177473
At time: 789.6124799251556 and batch: 400, loss is 4.853592205047607 and perplexity is 128.20008468395298
At time: 791.303802728653 and batch: 450, loss is 4.821371126174927 and perplexity is 124.13517914022378
At time: 792.9729325771332 and batch: 500, loss is 4.785269184112549 and perplexity is 119.7335891262262
At time: 794.6796386241913 and batch: 550, loss is 4.797746562957764 and perplexity is 121.23690972491646
At time: 796.3532483577728 and batch: 600, loss is 4.777105207443237 and perplexity is 118.76006621438074
At time: 798.0212006568909 and batch: 650, loss is 4.76233211517334 and perplexity is 117.01850856775081
At time: 799.6800284385681 and batch: 700, loss is 4.760541925430298 and perplexity is 116.80921063137232
At time: 801.3368000984192 and batch: 750, loss is 4.7829673385620115 and perplexity is 119.45829785738968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.867604100426962 and perplexity of 130.00905481160936
Finished 29 epochs...
Completing Train Step...
At time: 805.6988334655762 and batch: 50, loss is 4.864649314880371 and perplexity is 129.625472915727
At time: 807.3900835514069 and batch: 100, loss is 4.851708602905274 and perplexity is 127.95883401126581
At time: 809.0667634010315 and batch: 150, loss is 4.838352108001709 and perplexity is 126.26111551988497
At time: 810.7369654178619 and batch: 200, loss is 4.8412369537353515 and perplexity is 126.62588525955306
At time: 812.4030094146729 and batch: 250, loss is 4.842591142654419 and perplexity is 126.79747678768918
At time: 814.0648016929626 and batch: 300, loss is 4.880602073669434 and perplexity is 131.7099391050123
At time: 815.7211034297943 and batch: 350, loss is 4.836837816238403 and perplexity is 126.0700640429743
At time: 817.3803565502167 and batch: 400, loss is 4.852632503509522 and perplexity is 128.07710988448983
At time: 819.0407218933105 and batch: 450, loss is 4.820367708206176 and perplexity is 124.01068214262848
At time: 820.7118544578552 and batch: 500, loss is 4.784452114105225 and perplexity is 119.63579835807418
At time: 822.3817360401154 and batch: 550, loss is 4.796914138793945 and perplexity is 121.13603118440724
At time: 824.0445494651794 and batch: 600, loss is 4.776238584518433 and perplexity is 118.65719060206868
At time: 825.7236461639404 and batch: 650, loss is 4.761526565551758 and perplexity is 116.92428230951471
At time: 827.3967354297638 and batch: 700, loss is 4.759949712753296 and perplexity is 116.74005521542405
At time: 829.062506198883 and batch: 750, loss is 4.782319793701172 and perplexity is 119.3809682904114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8671147546102835 and perplexity of 129.9454509879087
Finished 30 epochs...
Completing Train Step...
At time: 833.3748457431793 and batch: 50, loss is 4.862625303268433 and perplexity is 129.36337478764642
At time: 835.0582385063171 and batch: 100, loss is 4.849667520523071 and perplexity is 127.69792584762531
At time: 836.7195281982422 and batch: 150, loss is 4.836457567214966 and perplexity is 126.02213513727457
At time: 838.3757646083832 and batch: 200, loss is 4.8393837833404545 and perplexity is 126.39144321538443
At time: 840.0323159694672 and batch: 250, loss is 4.84059157371521 and perplexity is 126.54418980831547
At time: 841.6912891864777 and batch: 300, loss is 4.879225730895996 and perplexity is 131.5287857752702
At time: 843.3585896492004 and batch: 350, loss is 4.835792121887207 and perplexity is 125.9383021924675
At time: 845.0285441875458 and batch: 400, loss is 4.851751070022583 and perplexity is 127.9642681694663
At time: 846.6965548992157 and batch: 450, loss is 4.819665546417236 and perplexity is 123.92363714361997
At time: 848.3505671024323 and batch: 500, loss is 4.783722906112671 and perplexity is 119.54859077781256
At time: 850.0082876682281 and batch: 550, loss is 4.796035671234131 and perplexity is 121.02966383765923
At time: 851.663868188858 and batch: 600, loss is 4.775323905944824 and perplexity is 118.54870703358165
At time: 853.3215570449829 and batch: 650, loss is 4.760603542327881 and perplexity is 116.81640827428653
At time: 854.9958333969116 and batch: 700, loss is 4.759202146530152 and perplexity is 116.6528169055267
At time: 856.6643226146698 and batch: 750, loss is 4.781779050827026 and perplexity is 119.31643133302228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.867083527321039 and perplexity of 129.94139320708183
Finished 31 epochs...
Completing Train Step...
At time: 861.0557622909546 and batch: 50, loss is 4.860976810455322 and perplexity is 129.15029587181394
At time: 862.7413129806519 and batch: 100, loss is 4.847634916305542 and perplexity is 127.4386301170484
At time: 864.3999025821686 and batch: 150, loss is 4.8347125816345216 and perplexity is 125.80242008419518
At time: 866.0560455322266 and batch: 200, loss is 4.837535085678101 and perplexity is 126.15799949968279
At time: 867.7144725322723 and batch: 250, loss is 4.838962869644165 and perplexity is 126.33825452055163
At time: 869.3836092948914 and batch: 300, loss is 4.87803469657898 and perplexity is 131.37222373162749
At time: 871.050169467926 and batch: 350, loss is 4.834811401367188 and perplexity is 125.81485245998856
At time: 872.7035415172577 and batch: 400, loss is 4.850809535980225 and perplexity is 127.84384215627104
At time: 874.36945271492 and batch: 450, loss is 4.818876686096192 and perplexity is 123.8259172521779
At time: 876.0407600402832 and batch: 500, loss is 4.783016872406006 and perplexity is 119.46421523263355
At time: 877.7426908016205 and batch: 550, loss is 4.79527886390686 and perplexity is 120.93810235282129
At time: 879.3963434696198 and batch: 600, loss is 4.774643239974975 and perplexity is 118.46804241887776
At time: 881.052460193634 and batch: 650, loss is 4.759764413833619 and perplexity is 116.7184254133609
At time: 882.7097804546356 and batch: 700, loss is 4.758486204147339 and perplexity is 116.5693300992671
At time: 884.3717753887177 and batch: 750, loss is 4.78081298828125 and perplexity is 119.20121985732183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8663550088571945 and perplexity of 129.846762977033
Finished 32 epochs...
Completing Train Step...
At time: 888.7162737846375 and batch: 50, loss is 4.858912715911865 and perplexity is 128.88399238315645
At time: 890.4028742313385 and batch: 100, loss is 4.846003475189209 and perplexity is 127.23089099921205
At time: 892.0613021850586 and batch: 150, loss is 4.832988500595093 and perplexity is 125.58571338065235
At time: 893.7253799438477 and batch: 200, loss is 4.835739736557007 and perplexity is 125.9317050457207
At time: 895.3896353244781 and batch: 250, loss is 4.837455310821533 and perplexity is 126.14793566479366
At time: 897.0572898387909 and batch: 300, loss is 4.8768675327301025 and perplexity is 131.21898026885282
At time: 898.7172374725342 and batch: 350, loss is 4.8336992263793945 and perplexity is 125.67500211143498
At time: 900.3746140003204 and batch: 400, loss is 4.849941883087158 and perplexity is 127.73296618464805
At time: 902.0370569229126 and batch: 450, loss is 4.818065633773804 and perplexity is 123.72552867011804
At time: 903.6997480392456 and batch: 500, loss is 4.782449188232422 and perplexity is 119.39641653428117
At time: 905.3857548236847 and batch: 550, loss is 4.794019498825073 and perplexity is 120.78589299335377
At time: 907.05091381073 and batch: 600, loss is 4.773454151153564 and perplexity is 118.32725711364779
At time: 908.7515165805817 and batch: 650, loss is 4.758416948318481 and perplexity is 116.56125727324006
At time: 910.4588820934296 and batch: 700, loss is 4.7573566818237305 and perplexity is 116.43773677146342
At time: 912.1250519752502 and batch: 750, loss is 4.7797975254058835 and perplexity is 119.08023688111491
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.866227260855741 and perplexity of 129.83017637204284
Finished 33 epochs...
Completing Train Step...
At time: 916.5827543735504 and batch: 50, loss is 4.856966857910156 and perplexity is 128.63344627791224
At time: 918.2884559631348 and batch: 100, loss is 4.844169063568115 and perplexity is 126.99771111353577
At time: 919.9561307430267 and batch: 150, loss is 4.831023540496826 and perplexity is 125.33918475372649
At time: 921.6326825618744 and batch: 200, loss is 4.833670988082885 and perplexity is 125.67145331356781
At time: 923.3110342025757 and batch: 250, loss is 4.8357719135284425 and perplexity is 125.93575721178968
At time: 924.9731919765472 and batch: 300, loss is 4.875727005004883 and perplexity is 131.06940669639815
At time: 926.6458086967468 and batch: 350, loss is 4.832683744430542 and perplexity is 125.54744619169719
At time: 928.3079831600189 and batch: 400, loss is 4.84923565864563 and perplexity is 127.64278988803983
At time: 929.969797372818 and batch: 450, loss is 4.818043527603149 and perplexity is 123.72279360269793
At time: 931.6313269138336 and batch: 500, loss is 4.783432445526123 and perplexity is 119.51387166652505
At time: 933.2948746681213 and batch: 550, loss is 4.793745927810669 and perplexity is 120.7528539935442
At time: 934.9575612545013 and batch: 600, loss is 4.772835111618042 and perplexity is 118.25403053077783
At time: 936.6227467060089 and batch: 650, loss is 4.757419309616089 and perplexity is 116.44502923821778
At time: 938.3032946586609 and batch: 700, loss is 4.756555461883545 and perplexity is 116.34448189879261
At time: 939.982052564621 and batch: 750, loss is 4.7792371559143065 and perplexity is 119.01352664225395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.865515775458757 and perplexity of 129.7378369504366
Finished 34 epochs...
Completing Train Step...
At time: 944.4008810520172 and batch: 50, loss is 4.855355367660523 and perplexity is 128.42632166792686
At time: 946.094476222992 and batch: 100, loss is 4.842210321426392 and perplexity is 126.74919881009102
At time: 947.7589375972748 and batch: 150, loss is 4.829213228225708 and perplexity is 125.11248694833607
At time: 949.421808719635 and batch: 200, loss is 4.8319548320770265 and perplexity is 125.45596645162237
At time: 951.0846364498138 and batch: 250, loss is 4.834345474243164 and perplexity is 125.75624556195433
At time: 952.7485332489014 and batch: 300, loss is 4.874308137893677 and perplexity is 130.88356849698312
At time: 954.4125909805298 and batch: 350, loss is 4.831325168609619 and perplexity is 125.37699627770543
At time: 956.0758435726166 and batch: 400, loss is 4.84799033164978 and perplexity is 127.48393181162248
At time: 957.7415871620178 and batch: 450, loss is 4.817113933563232 and perplexity is 123.60783507182396
At time: 959.4041967391968 and batch: 500, loss is 4.782640008926392 and perplexity is 119.41920201524323
At time: 961.0954837799072 and batch: 550, loss is 4.792836084365844 and perplexity is 120.64303776625727
At time: 962.7690000534058 and batch: 600, loss is 4.7719806289672855 and perplexity is 118.15302767204804
At time: 964.4400517940521 and batch: 650, loss is 4.7566290378570555 and perplexity is 116.35304237222958
At time: 966.104184627533 and batch: 700, loss is 4.7561094760894775 and perplexity is 116.29260548158001
At time: 967.768741607666 and batch: 750, loss is 4.7791226291656494 and perplexity is 118.9998971904847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.865388382312863 and perplexity of 129.7213102919599
Finished 35 epochs...
Completing Train Step...
At time: 972.1544816493988 and batch: 50, loss is 4.853716974258423 and perplexity is 128.21608110525517
At time: 973.876827955246 and batch: 100, loss is 4.840649690628052 and perplexity is 126.55154437967555
At time: 975.541068315506 and batch: 150, loss is 4.827333946228027 and perplexity is 124.87758609553423
At time: 977.2086751461029 and batch: 200, loss is 4.830121870040894 and perplexity is 125.22622104953412
At time: 978.8854894638062 and batch: 250, loss is 4.832884731292725 and perplexity is 125.57268211492185
At time: 980.5508506298065 and batch: 300, loss is 4.873037538528442 and perplexity is 130.71737352390394
At time: 982.21644115448 and batch: 350, loss is 4.830020399093628 and perplexity is 125.21351487092657
At time: 983.8839964866638 and batch: 400, loss is 4.846730432510376 and perplexity is 127.32341605371064
At time: 985.5505475997925 and batch: 450, loss is 4.81623007774353 and perplexity is 123.4986318345025
At time: 987.2150316238403 and batch: 500, loss is 4.781949090957641 and perplexity is 119.33672163962497
At time: 988.8826458454132 and batch: 550, loss is 4.792043724060059 and perplexity is 120.54748287391311
At time: 990.5702657699585 and batch: 600, loss is 4.771318731307983 and perplexity is 118.0748483358065
At time: 992.2527174949646 and batch: 650, loss is 4.75553129196167 and perplexity is 116.2253863772914
At time: 993.9181668758392 and batch: 700, loss is 4.755389699935913 and perplexity is 116.20893095439563
At time: 995.5945861339569 and batch: 750, loss is 4.77859769821167 and perplexity is 118.93744685342061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.865004783452943 and perplexity of 129.6715588881255
Finished 36 epochs...
Completing Train Step...
At time: 999.9664039611816 and batch: 50, loss is 4.8521780586242675 and perplexity is 128.0189191202337
At time: 1001.6598734855652 and batch: 100, loss is 4.838931655883789 and perplexity is 126.33431109009362
At time: 1003.3226432800293 and batch: 150, loss is 4.825638341903686 and perplexity is 124.66602253526966
At time: 1004.9870507717133 and batch: 200, loss is 4.828341722488403 and perplexity is 125.0034981972114
At time: 1006.6597332954407 and batch: 250, loss is 4.831503276824951 and perplexity is 125.39932893951234
At time: 1008.32173204422 and batch: 300, loss is 4.8717603015899655 and perplexity is 130.55052304242002
At time: 1009.9893791675568 and batch: 350, loss is 4.828862953186035 and perplexity is 125.06867084129898
At time: 1011.657112121582 and batch: 400, loss is 4.845654382705688 and perplexity is 127.1864834031225
At time: 1013.335333108902 and batch: 450, loss is 4.8154251003265385 and perplexity is 123.39925822696743
At time: 1015.0031576156616 and batch: 500, loss is 4.781284503936767 and perplexity is 119.25743835154985
At time: 1016.6693177223206 and batch: 550, loss is 4.79116008758545 and perplexity is 120.44100976981056
At time: 1018.3363609313965 and batch: 600, loss is 4.770360736846924 and perplexity is 117.96178744960862
At time: 1020.0020084381104 and batch: 650, loss is 4.754741067886353 and perplexity is 116.13357855796501
At time: 1021.6648063659668 and batch: 700, loss is 4.7549130249023435 and perplexity is 116.153550258678
At time: 1023.3280491828918 and batch: 750, loss is 4.778126993179321 and perplexity is 118.88147557266716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.864891584529433 and perplexity of 129.65688103802356
Finished 37 epochs...
Completing Train Step...
At time: 1027.6986651420593 and batch: 50, loss is 4.85090313911438 and perplexity is 127.85580930065174
At time: 1029.3870215415955 and batch: 100, loss is 4.837393703460694 and perplexity is 126.14016426279224
At time: 1031.0484669208527 and batch: 150, loss is 4.823990716934204 and perplexity is 124.46078880423283
At time: 1032.7157950401306 and batch: 200, loss is 4.826766099929809 and perplexity is 124.80669495008169
At time: 1034.3845236301422 and batch: 250, loss is 4.830161542892456 and perplexity is 125.23118922936378
At time: 1036.0451745986938 and batch: 300, loss is 4.87046404838562 and perplexity is 130.38140614143992
At time: 1037.707005739212 and batch: 350, loss is 4.827395343780518 and perplexity is 124.88525350905928
At time: 1039.371085882187 and batch: 400, loss is 4.8443332767486575 and perplexity is 127.01856752400121
At time: 1041.0352981090546 and batch: 450, loss is 4.814431028366089 and perplexity is 123.27665143450746
At time: 1042.6948635578156 and batch: 500, loss is 4.780422515869141 and perplexity is 119.15468415552296
At time: 1044.382534980774 and batch: 550, loss is 4.790254220962525 and perplexity is 120.33195568071585
At time: 1046.0463824272156 and batch: 600, loss is 4.769436626434326 and perplexity is 117.8528280865329
At time: 1047.7107572555542 and batch: 650, loss is 4.753895044326782 and perplexity is 116.03536836437442
At time: 1049.3726325035095 and batch: 700, loss is 4.754427366256714 and perplexity is 116.09715297879606
At time: 1051.0313589572906 and batch: 750, loss is 4.777720556259156 and perplexity is 118.83316756961364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.864934522052144 and perplexity of 129.66244830281914
Annealing...
Finished 38 epochs...
Completing Train Step...
At time: 1055.3497729301453 and batch: 50, loss is 4.850883417129516 and perplexity is 127.85328775518097
At time: 1057.03417634964 and batch: 100, loss is 4.837389945983887 and perplexity is 126.13969029494105
At time: 1058.702941417694 and batch: 150, loss is 4.822653617858887 and perplexity is 124.29448360664958
At time: 1060.3640880584717 and batch: 200, loss is 4.825553493499756 and perplexity is 124.65544527097153
At time: 1062.0262746810913 and batch: 250, loss is 4.827588882446289 and perplexity is 124.90942597347569
At time: 1063.6883370876312 and batch: 300, loss is 4.865394964218139 and perplexity is 129.72216410814636
At time: 1065.3501725196838 and batch: 350, loss is 4.820638198852539 and perplexity is 124.04423040923866
At time: 1067.0137836933136 and batch: 400, loss is 4.83713454246521 and perplexity is 126.10747788795112
At time: 1068.678385734558 and batch: 450, loss is 4.806735439300537 and perplexity is 122.33160598835052
At time: 1070.3491020202637 and batch: 500, loss is 4.7680197429656985 and perplexity is 117.68596260507314
At time: 1072.0251195430756 and batch: 550, loss is 4.775111484527588 and perplexity is 118.52352742366091
At time: 1073.6970767974854 and batch: 600, loss is 4.753321475982666 and perplexity is 115.9688332333293
At time: 1075.3609795570374 and batch: 650, loss is 4.738028841018677 and perplexity is 114.20885584494427
At time: 1077.0230567455292 and batch: 700, loss is 4.738576831817627 and perplexity is 114.271458398351
At time: 1078.6891252994537 and batch: 750, loss is 4.764006328582764 and perplexity is 117.21458661639448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.859217089275981 and perplexity of 128.9232272082138
Finished 39 epochs...
Completing Train Step...
At time: 1083.0862743854523 and batch: 50, loss is 4.84758056640625 and perplexity is 127.43170402855341
At time: 1084.7763135433197 and batch: 100, loss is 4.834224100112915 and perplexity is 125.7409829332888
At time: 1086.4400651454926 and batch: 150, loss is 4.820168991088867 and perplexity is 123.98604154569277
At time: 1088.1065640449524 and batch: 200, loss is 4.823253211975097 and perplexity is 124.36903219491158
At time: 1089.7719864845276 and batch: 250, loss is 4.8257596302032475 and perplexity is 124.68114398216477
At time: 1091.4374809265137 and batch: 300, loss is 4.86382080078125 and perplexity is 129.51812086129726
At time: 1093.1012916564941 and batch: 350, loss is 4.819346370697022 and perplexity is 123.88409003905657
At time: 1094.765864610672 and batch: 400, loss is 4.835976696014404 and perplexity is 125.96154929001577
At time: 1096.4303143024445 and batch: 450, loss is 4.805896625518799 and perplexity is 122.22903557605719
At time: 1098.096039056778 and batch: 500, loss is 4.76768892288208 and perplexity is 117.6470361642626
At time: 1099.760354757309 and batch: 550, loss is 4.775220537185669 and perplexity is 118.53645343416676
At time: 1101.4237837791443 and batch: 600, loss is 4.75366584777832 and perplexity is 116.00877650594327
At time: 1103.0882380008698 and batch: 650, loss is 4.738889226913452 and perplexity is 114.30716181804361
At time: 1104.754045009613 and batch: 700, loss is 4.739728174209595 and perplexity is 114.40309974018379
At time: 1106.4170334339142 and batch: 750, loss is 4.764619359970093 and perplexity is 117.2864648666038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.858713904092478 and perplexity of 128.85837126911545
Finished 40 epochs...
Completing Train Step...
At time: 1110.816505908966 and batch: 50, loss is 4.846179132461548 and perplexity is 127.25324199348202
At time: 1112.5076117515564 and batch: 100, loss is 4.83275673866272 and perplexity is 125.55661076561027
At time: 1114.1717765331268 and batch: 150, loss is 4.818845300674439 and perplexity is 123.82203098452737
At time: 1115.8358681201935 and batch: 200, loss is 4.82205057144165 and perplexity is 124.21955085986147
At time: 1117.5098366737366 and batch: 250, loss is 4.824807758331299 and perplexity is 124.56251997463053
At time: 1119.1729233264923 and batch: 300, loss is 4.86306321144104 and perplexity is 129.42003647210385
At time: 1120.8385634422302 and batch: 350, loss is 4.818768987655639 and perplexity is 123.81258211208966
At time: 1122.50524020195 and batch: 400, loss is 4.835543947219849 and perplexity is 125.90705137420439
At time: 1124.170737028122 and batch: 450, loss is 4.805712471008301 and perplexity is 122.20652862028429
At time: 1125.8383927345276 and batch: 500, loss is 4.767764778137207 and perplexity is 117.655960648686
At time: 1127.5420904159546 and batch: 550, loss is 4.77552493095398 and perplexity is 118.57254068399806
At time: 1129.2258577346802 and batch: 600, loss is 4.754112071990967 and perplexity is 116.06055398222242
At time: 1130.9015831947327 and batch: 650, loss is 4.739572772979736 and perplexity is 114.38532273910423
At time: 1132.572565793991 and batch: 700, loss is 4.740440912246704 and perplexity is 114.48466824596719
At time: 1134.2390203475952 and batch: 750, loss is 4.764881525039673 and perplexity is 117.31721731175871
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.858467279478561 and perplexity of 128.8265955415506
Finished 41 epochs...
Completing Train Step...
At time: 1138.6605651378632 and batch: 50, loss is 4.84514741897583 and perplexity is 127.12202081058771
At time: 1140.360053062439 and batch: 100, loss is 4.831658000946045 and perplexity is 125.41873274154769
At time: 1142.0268595218658 and batch: 150, loss is 4.817862939834595 and perplexity is 123.70045279678081
At time: 1143.6933662891388 and batch: 200, loss is 4.821188735961914 and perplexity is 124.11254016308472
At time: 1145.358951330185 and batch: 250, loss is 4.8241300296783445 and perplexity is 124.47812898608518
At time: 1147.024480342865 and batch: 300, loss is 4.862557973861694 and perplexity is 129.35466512157984
At time: 1148.6930294036865 and batch: 350, loss is 4.8184237575531 and perplexity is 123.76984565905097
At time: 1150.3576691150665 and batch: 400, loss is 4.835305643081665 and perplexity is 125.87705077761056
At time: 1152.022977590561 and batch: 450, loss is 4.805690011978149 and perplexity is 122.20378401099407
At time: 1153.6946783065796 and batch: 500, loss is 4.767914209365845 and perplexity is 117.67354343711845
At time: 1155.3768877983093 and batch: 550, loss is 4.7758287525177 and perplexity is 118.60857105184722
At time: 1157.046809911728 and batch: 600, loss is 4.754502658843994 and perplexity is 116.10589456290383
At time: 1158.722622871399 and batch: 650, loss is 4.740103569030762 and perplexity is 114.44605413327561
At time: 1160.3929059505463 and batch: 700, loss is 4.740942096710205 and perplexity is 114.54206056386873
At time: 1162.0567519664764 and batch: 750, loss is 4.76498966217041 and perplexity is 117.32990434498217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.858358693677325 and perplexity of 128.81260756191494
Finished 42 epochs...
Completing Train Step...
At time: 1166.4110457897186 and batch: 50, loss is 4.844298181533813 and perplexity is 127.01410985830654
At time: 1168.1010220050812 and batch: 100, loss is 4.830744457244873 and perplexity is 125.3042095671697
At time: 1169.7713453769684 and batch: 150, loss is 4.817037220001221 and perplexity is 123.59835303820805
At time: 1171.4380502700806 and batch: 200, loss is 4.820498723983764 and perplexity is 124.02693056295506
At time: 1173.103874206543 and batch: 250, loss is 4.823612089157105 and perplexity is 124.41367341259276
At time: 1174.7685310840607 and batch: 300, loss is 4.862174129486084 and perplexity is 129.30502258902692
At time: 1176.4323489665985 and batch: 350, loss is 4.818170251846314 and perplexity is 123.73847327355571
At time: 1178.096544265747 and batch: 400, loss is 4.835154819488525 and perplexity is 125.85806698015668
At time: 1179.7588131427765 and batch: 450, loss is 4.805718078613281 and perplexity is 122.20721390814421
At time: 1181.4247663021088 and batch: 500, loss is 4.76806601524353 and perplexity is 117.69140832862372
At time: 1183.0885808467865 and batch: 550, loss is 4.776088962554931 and perplexity is 118.63943820833434
At time: 1184.749562740326 and batch: 600, loss is 4.754826965332032 and perplexity is 116.1435545641713
At time: 1186.411578655243 and batch: 650, loss is 4.7405269336700435 and perplexity is 114.49451680366761
At time: 1188.0757150650024 and batch: 700, loss is 4.7413087081909175 and perplexity is 114.58406069669098
At time: 1189.7419254779816 and batch: 750, loss is 4.765021028518677 and perplexity is 117.333584613342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.858222429142442 and perplexity of 128.79505616770186
Finished 43 epochs...
Completing Train Step...
At time: 1194.0725548267365 and batch: 50, loss is 4.843542490005493 and perplexity is 126.91816262932964
At time: 1195.7607591152191 and batch: 100, loss is 4.829949951171875 and perplexity is 125.2046941497329
At time: 1197.424904346466 and batch: 150, loss is 4.816318826675415 and perplexity is 123.50959269254213
At time: 1199.0861625671387 and batch: 200, loss is 4.81991415977478 and perplexity is 123.95445004522
At time: 1200.7485535144806 and batch: 250, loss is 4.823181638717651 and perplexity is 124.36013101669893
At time: 1202.41015958786 and batch: 300, loss is 4.86186053276062 and perplexity is 129.26447931480823
At time: 1204.0732147693634 and batch: 350, loss is 4.817980251312256 and perplexity is 123.71496513090075
At time: 1205.7334322929382 and batch: 400, loss is 4.835046873092652 and perplexity is 125.84448178868398
At time: 1207.3947567939758 and batch: 450, loss is 4.8057708644866945 and perplexity is 122.21366489292674
At time: 1209.0584509372711 and batch: 500, loss is 4.768203678131104 and perplexity is 117.70761118297713
At time: 1210.7478878498077 and batch: 550, loss is 4.7763067245483395 and perplexity is 118.66527618205764
At time: 1212.4143888950348 and batch: 600, loss is 4.755097961425781 and perplexity is 116.17503327888406
At time: 1214.076164484024 and batch: 650, loss is 4.740876913070679 and perplexity is 114.53459453881636
At time: 1215.7385268211365 and batch: 700, loss is 4.741593990325928 and perplexity is 114.61675414537137
At time: 1217.4034950733185 and batch: 750, loss is 4.76500129699707 and perplexity is 117.33126946602276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.858126263285792 and perplexity of 128.7826710763142
Finished 44 epochs...
Completing Train Step...
At time: 1221.7393786907196 and batch: 50, loss is 4.842858514785767 and perplexity is 126.83138343195134
At time: 1223.4272420406342 and batch: 100, loss is 4.829241495132447 and perplexity is 125.11602354132054
At time: 1225.0893533229828 and batch: 150, loss is 4.815670280456543 and perplexity is 123.42951698241323
At time: 1226.749681711197 and batch: 200, loss is 4.819402475357055 and perplexity is 123.89104070879242
At time: 1228.4111840724945 and batch: 250, loss is 4.822811574935913 and perplexity is 124.31411835065313
At time: 1230.07901597023 and batch: 300, loss is 4.8615900325775145 and perplexity is 129.22951797822174
At time: 1231.7422919273376 and batch: 350, loss is 4.817824506759644 and perplexity is 123.69569869936487
At time: 1233.4006781578064 and batch: 400, loss is 4.834965677261352 and perplexity is 125.8342641561909
At time: 1235.0603744983673 and batch: 450, loss is 4.8058344173431395 and perplexity is 122.22143216724092
At time: 1236.7241287231445 and batch: 500, loss is 4.768332061767578 and perplexity is 117.72272388423247
At time: 1238.3898780345917 and batch: 550, loss is 4.776494951248169 and perplexity is 118.68761425762287
At time: 1240.054071187973 and batch: 600, loss is 4.755327558517456 and perplexity is 116.20170979095772
At time: 1241.71493268013 and batch: 650, loss is 4.741168823242187 and perplexity is 114.56803323226178
At time: 1243.3756756782532 and batch: 700, loss is 4.741806144714356 and perplexity is 114.64107317235359
At time: 1245.0388803482056 and batch: 750, loss is 4.764940719604493 and perplexity is 117.32416205892694
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.858052808184956 and perplexity of 128.7732116796491
Finished 45 epochs...
Completing Train Step...
At time: 1249.3875224590302 and batch: 50, loss is 4.842225551605225 and perplexity is 126.75112923775615
At time: 1251.0740549564362 and batch: 100, loss is 4.828586874008178 and perplexity is 125.03414675138795
At time: 1252.7384281158447 and batch: 150, loss is 4.815072059631348 and perplexity is 123.35570095625393
At time: 1254.3983812332153 and batch: 200, loss is 4.8189414119720455 and perplexity is 123.8339322525116
At time: 1256.0592212677002 and batch: 250, loss is 4.822476148605347 and perplexity is 124.27242711465725
At time: 1257.7270114421844 and batch: 300, loss is 4.861352481842041 and perplexity is 129.198823057126
At time: 1259.3909850120544 and batch: 350, loss is 4.817694473266601 and perplexity is 123.67961516131064
At time: 1261.0537238121033 and batch: 400, loss is 4.834899406433106 and perplexity is 125.82592529159828
At time: 1262.7259147167206 and batch: 450, loss is 4.805897159576416 and perplexity is 122.22910085342207
At time: 1264.3876023292542 and batch: 500, loss is 4.768452081680298 and perplexity is 117.7368538031971
At time: 1266.0505788326263 and batch: 550, loss is 4.776658306121826 and perplexity is 118.70700404151934
At time: 1267.7099545001984 and batch: 600, loss is 4.75551999092102 and perplexity is 116.22407291689714
At time: 1269.3708040714264 and batch: 650, loss is 4.7414134502410885 and perplexity is 114.59606309469218
At time: 1271.0308785438538 and batch: 700, loss is 4.741973075866699 and perplexity is 114.66021193618768
At time: 1272.7040243148804 and batch: 750, loss is 4.7648557472229 and perplexity is 117.31419316900462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.857966933139535 and perplexity of 128.762153749054
Finished 46 epochs...
Completing Train Step...
At time: 1277.0943784713745 and batch: 50, loss is 4.841634902954102 and perplexity is 126.6762859594601
At time: 1278.782015800476 and batch: 100, loss is 4.827979288101196 and perplexity is 124.95820084009934
At time: 1280.44478225708 and batch: 150, loss is 4.814520235061646 and perplexity is 123.28764902774341
At time: 1282.1045026779175 and batch: 200, loss is 4.818520402908325 and perplexity is 123.78180801779466
At time: 1283.7721807956696 and batch: 250, loss is 4.8221738815307615 and perplexity is 124.2348693281888
At time: 1285.4334409236908 and batch: 300, loss is 4.861138610839844 and perplexity is 129.1711941299746
At time: 1287.0966155529022 and batch: 350, loss is 4.817582569122314 and perplexity is 123.66577567417269
At time: 1288.7665767669678 and batch: 400, loss is 4.834844388961792 and perplexity is 125.81900285779216
At time: 1290.4328622817993 and batch: 450, loss is 4.805958061218262 and perplexity is 122.23654503302448
At time: 1292.1075949668884 and batch: 500, loss is 4.768550825119019 and perplexity is 117.74848011900755
At time: 1293.8004686832428 and batch: 550, loss is 4.776796293258667 and perplexity is 118.7233852112994
At time: 1295.4715023040771 and batch: 600, loss is 4.755687227249146 and perplexity is 116.24351142945896
At time: 1297.1332840919495 and batch: 650, loss is 4.741626214981079 and perplexity is 114.62044769025957
At time: 1298.7966072559357 and batch: 700, loss is 4.742105541229248 and perplexity is 114.6754014487516
At time: 1300.4670748710632 and batch: 750, loss is 4.764752178192139 and perplexity is 117.30204368088978
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8579016397165695 and perplexity of 128.7537467017528
Finished 47 epochs...
Completing Train Step...
At time: 1304.8897104263306 and batch: 50, loss is 4.841080188751221 and perplexity is 126.60603631051055
At time: 1306.586475610733 and batch: 100, loss is 4.827408370971679 and perplexity is 124.88688042372702
At time: 1308.2479374408722 and batch: 150, loss is 4.814002170562744 and perplexity is 123.22379461540929
At time: 1309.9065926074982 and batch: 200, loss is 4.818130311965942 and perplexity is 123.73353127242788
At time: 1311.5842349529266 and batch: 250, loss is 4.821896314620972 and perplexity is 124.20039062471616
At time: 1313.2601940631866 and batch: 300, loss is 4.860939264297485 and perplexity is 129.1454468654518
At time: 1314.9276423454285 and batch: 350, loss is 4.817483263015747 and perplexity is 123.65349551723224
At time: 1316.602490901947 and batch: 400, loss is 4.834794702529908 and perplexity is 125.81275151578161
At time: 1318.2662677764893 and batch: 450, loss is 4.806018123626709 and perplexity is 122.24388707480753
At time: 1319.9256217479706 and batch: 500, loss is 4.768637771606445 and perplexity is 117.75871838083773
At time: 1321.5872859954834 and batch: 550, loss is 4.77691533088684 and perplexity is 118.73751860266965
At time: 1323.2505142688751 and batch: 600, loss is 4.755827722549438 and perplexity is 116.25984424381924
At time: 1324.9106397628784 and batch: 650, loss is 4.741807746887207 and perplexity is 114.6412568473158
At time: 1326.5719680786133 and batch: 700, loss is 4.74221399307251 and perplexity is 114.68783888183465
At time: 1328.2450575828552 and batch: 750, loss is 4.7646363258361815 and perplexity is 117.28845474994075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8578398948492 and perplexity of 128.74579706416657
Finished 48 epochs...
Completing Train Step...
At time: 1332.6205208301544 and batch: 50, loss is 4.840558090209961 and perplexity is 126.53995273620815
At time: 1334.308564901352 and batch: 100, loss is 4.826868782043457 and perplexity is 124.819511023296
At time: 1335.9734377861023 and batch: 150, loss is 4.8135144138336186 and perplexity is 123.16370603589307
At time: 1337.6366798877716 and batch: 200, loss is 4.817767753601074 and perplexity is 123.68867877696543
At time: 1339.2975142002106 and batch: 250, loss is 4.821641893386841 and perplexity is 124.16879542746884
At time: 1340.9598202705383 and batch: 300, loss is 4.860747137069702 and perplexity is 129.12063689217672
At time: 1342.6252222061157 and batch: 350, loss is 4.817390747070313 and perplexity is 123.64205612635939
At time: 1344.2895984649658 and batch: 400, loss is 4.834747724533081 and perplexity is 125.80684122356807
At time: 1345.950054883957 and batch: 450, loss is 4.806073083877563 and perplexity is 122.25060581413668
At time: 1347.613363981247 and batch: 500, loss is 4.7687161254882815 and perplexity is 117.76794559503222
At time: 1349.2863235473633 and batch: 550, loss is 4.777014493942261 and perplexity is 118.74929356161812
At time: 1350.9490916728973 and batch: 600, loss is 4.755945863723755 and perplexity is 116.27358012971501
At time: 1352.6093521118164 and batch: 650, loss is 4.741965284347534 and perplexity is 114.65931856242929
At time: 1354.2714982032776 and batch: 700, loss is 4.742304630279541 and perplexity is 114.69823433833182
At time: 1355.9352688789368 and batch: 750, loss is 4.764512186050415 and perplexity is 117.27389549000561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.857804409293241 and perplexity of 128.7412285290393
Finished 49 epochs...
Completing Train Step...
At time: 1360.2775316238403 and batch: 50, loss is 4.840088682174683 and perplexity is 126.4805678045825
At time: 1361.965320110321 and batch: 100, loss is 4.826367139816284 and perplexity is 124.75691198826408
At time: 1363.6301028728485 and batch: 150, loss is 4.813057880401612 and perplexity is 123.10749051958551
At time: 1365.2959105968475 and batch: 200, loss is 4.81741418838501 and perplexity is 123.64495449267751
At time: 1366.956518650055 and batch: 250, loss is 4.821367378234863 and perplexity is 124.1347138898845
At time: 1368.6190927028656 and batch: 300, loss is 4.860559644699097 and perplexity is 129.0964300272439
At time: 1370.282077550888 and batch: 350, loss is 4.81729866027832 and perplexity is 123.63067085028017
At time: 1371.946890592575 and batch: 400, loss is 4.834693298339844 and perplexity is 125.79999422244693
At time: 1373.6082265377045 and batch: 450, loss is 4.806113328933716 and perplexity is 122.25552589563617
At time: 1375.2732090950012 and batch: 500, loss is 4.768769197463989 and perplexity is 117.77419593843753
At time: 1376.965558052063 and batch: 550, loss is 4.7770850944519045 and perplexity is 118.75767761821926
At time: 1378.6345460414886 and batch: 600, loss is 4.756041822433471 and perplexity is 116.28473812778331
At time: 1380.297836303711 and batch: 650, loss is 4.742106962203979 and perplexity is 114.67556439971519
At time: 1381.9654834270477 and batch: 700, loss is 4.742390155792236 and perplexity is 114.70804438312739
At time: 1383.634567975998 and batch: 750, loss is 4.764412670135498 and perplexity is 117.26222545168699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.857736631881359 and perplexity of 128.73250307746468
Finished Training.
Improved accuracyfrom -138.6893344028116 to -128.73250307746468
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f23004c6978>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'best_accuracy': -205.22238492976274, 'params': {'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 18.976557961124893, 'dropout': 0.6137351798711002, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 7.8405872643857855}}, {'best_accuracy': -142.25441600961204, 'params': {'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 15.437431618724457, 'dropout': 0.47422080821405554, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 2.068774202983712}}, {'best_accuracy': -156.5778580240155, 'params': {'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 20.848620798797626, 'dropout': 0.30043171347418685, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 3.5148099363181053}}, {'best_accuracy': -166.23254917662834, 'params': {'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 13.552501739861292, 'dropout': 0.7242406468608337, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 6.889933187403387}}, {'best_accuracy': -138.6893344028116, 'params': {'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 13.73398522151051, 'dropout': 0.5229212356086618, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 7.588987817527516}}, {'best_accuracy': -128.73250307746468, 'params': {'wordvec_source': 'glove', 'seq_len': 35, 'data': 'wikitext', 'wordvec_dim': 200, 'num_layers': 1, 'lr': 13.708709542165955, 'dropout': 0.4697228231883231, 'batch_size': 80, 'tune_wordvecs': True, 'anneal': 7.584202789506056}}]
