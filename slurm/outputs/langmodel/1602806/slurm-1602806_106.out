Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'seq_len': 50, 'wordvec_source': 'glove', 'num_layers': 1, 'wordvec_dim': 200, 'tune_wordvecs': True, 'lr': 7.400849442496637, 'data': 'gigasmall', 'anneal': 2.145302412606373, 'batch_size': 80, 'dropout': 0.655916508963181}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 5360 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 12.511018514633179 and batch: 50, loss is 8.736601781845092 and perplexity is 6226.700033783828
At time: 18.56083345413208 and batch: 100, loss is 7.695507879257202 and perplexity is 2198.4500740308836
At time: 24.59648036956787 and batch: 150, loss is 7.367147359848023 and perplexity is 1583.1112887595298
At time: 30.64673686027527 and batch: 200, loss is 7.152230453491211 and perplexity is 1276.950960896343
At time: 36.689358711242676 and batch: 250, loss is 6.9477613830566405 and perplexity is 1040.817127398288
At time: 42.73762059211731 and batch: 300, loss is 6.761815948486328 and perplexity is 864.210132794461
At time: 48.79122591018677 and batch: 350, loss is 6.592111539840698 and perplexity is 729.3192317855462
At time: 55.36555767059326 and batch: 400, loss is 6.542658281326294 and perplexity is 694.1293186957344
At time: 61.42315912246704 and batch: 450, loss is 6.447418851852417 and perplexity is 631.0713002833787
At time: 67.49271702766418 and batch: 500, loss is 6.35165452003479 and perplexity is 573.4406936840664
At time: 73.54153323173523 and batch: 550, loss is 6.3187172412872314 and perplexity is 554.8607832486969
At time: 79.60407853126526 and batch: 600, loss is 6.255587892532349 and perplexity is 520.9155271046417
At time: 85.67820692062378 and batch: 650, loss is 6.22608736038208 and perplexity is 505.772700761485
At time: 91.77630543708801 and batch: 700, loss is 6.229417810440063 and perplexity is 507.4599595881159
At time: 97.84039568901062 and batch: 750, loss is 6.200986452102661 and perplexity is 493.2353542438066
At time: 103.89431023597717 and batch: 800, loss is 6.146415500640869 and perplexity is 467.0402772046989
At time: 109.97665357589722 and batch: 850, loss is 6.147031679153442 and perplexity is 467.32814606821444
At time: 116.02334427833557 and batch: 900, loss is 6.090188617706299 and perplexity is 441.5046788926911
At time: 122.08954691886902 and batch: 950, loss is 6.112742519378662 and perplexity is 451.575472961858
At time: 128.17627429962158 and batch: 1000, loss is 6.123771286010742 and perplexity is 456.5833581076947
At time: 134.2265248298645 and batch: 1050, loss is 6.094601335525513 and perplexity is 443.4572192930821
At time: 140.27782154083252 and batch: 1100, loss is 6.071100797653198 and perplexity is 433.15723770114806
At time: 146.3346815109253 and batch: 1150, loss is 6.065785732269287 and perplexity is 430.8610861645319
At time: 152.38723874092102 and batch: 1200, loss is 6.049740467071533 and perplexity is 424.00297303040327
At time: 158.4584550857544 and batch: 1250, loss is 5.98795220375061 and perplexity is 398.5975270820767
At time: 164.52594780921936 and batch: 1300, loss is 6.010497970581055 and perplexity is 407.6862855108326
At time: 170.58416867256165 and batch: 1350, loss is 6.0005809020996095 and perplexity is 403.6632142070685
At time: 176.6598641872406 and batch: 1400, loss is 5.968777351379394 and perplexity is 391.0272893873362
At time: 182.73306369781494 and batch: 1450, loss is 6.000030031204224 and perplexity is 403.4409091271449
At time: 188.80997514724731 and batch: 1500, loss is 5.972948961257934 and perplexity is 392.66190981937797
At time: 194.88905119895935 and batch: 1550, loss is 5.990778141021728 and perplexity is 399.7255317746051
At time: 200.96562719345093 and batch: 1600, loss is 5.9776856803894045 and perplexity is 394.5262509441817
At time: 207.05268597602844 and batch: 1650, loss is 5.983389825820923 and perplexity is 396.78311668196096
At time: 213.12250804901123 and batch: 1700, loss is 5.967614469528198 and perplexity is 390.57283513869714
At time: 219.19328117370605 and batch: 1750, loss is 5.980792064666748 and perplexity is 395.75370657450924
At time: 225.243910074234 and batch: 1800, loss is 5.951295299530029 and perplexity is 384.25073665073234
At time: 231.33920741081238 and batch: 1850, loss is 5.973632717132569 and perplexity is 392.93048651694573
At time: 237.41448736190796 and batch: 1900, loss is 5.937622280120849 and perplexity is 379.03262384359147
At time: 243.48982214927673 and batch: 1950, loss is 5.903752336502075 and perplexity is 366.4097843670882
At time: 249.57334232330322 and batch: 2000, loss is 5.917478981018067 and perplexity is 371.4740393207014
At time: 255.65067958831787 and batch: 2050, loss is 5.93979850769043 and perplexity is 379.85838328370266
At time: 261.7307949066162 and batch: 2100, loss is 5.916304569244385 and perplexity is 371.03803191153685
At time: 267.8050673007965 and batch: 2150, loss is 5.9285631275177 and perplexity is 375.61441587359735
At time: 273.8925235271454 and batch: 2200, loss is 5.9121437263488765 and perplexity is 369.4974083215382
At time: 279.9800012111664 and batch: 2250, loss is 5.889447193145752 and perplexity is 361.2055522656489
At time: 286.06275367736816 and batch: 2300, loss is 5.9026344108581545 and perplexity is 366.00039434939697
At time: 292.1354115009308 and batch: 2350, loss is 5.893482637405396 and perplexity is 362.666122180238
At time: 298.2148563861847 and batch: 2400, loss is 5.9121778774261475 and perplexity is 369.5100272715554
At time: 304.29892230033875 and batch: 2450, loss is 5.900748596191407 and perplexity is 365.3108358318365
At time: 310.38575887680054 and batch: 2500, loss is 5.890692176818848 and perplexity is 361.65552732853325
At time: 316.4328315258026 and batch: 2550, loss is 5.912921886444092 and perplexity is 369.78504836046505
At time: 322.5147075653076 and batch: 2600, loss is 5.931201553344726 and perplexity is 376.60675518060657
At time: 328.60214853286743 and batch: 2650, loss is 5.926483678817749 and perplexity is 374.83415650065587
At time: 334.6841788291931 and batch: 2700, loss is 5.914697761535645 and perplexity is 370.4423238639351
At time: 340.76474380493164 and batch: 2750, loss is 5.857221965789795 and perplexity is 349.7511724493756
At time: 346.85593485832214 and batch: 2800, loss is 5.8543720722198485 and perplexity is 348.75583780449983
At time: 352.94500756263733 and batch: 2850, loss is 5.870474081039429 and perplexity is 354.4169628080699
At time: 359.0152974128723 and batch: 2900, loss is 5.873577651977539 and perplexity is 355.5186296601417
At time: 365.0956108570099 and batch: 2950, loss is 5.888340053558349 and perplexity is 360.8058685931802
At time: 371.17928767204285 and batch: 3000, loss is 5.900723638534546 and perplexity is 365.3017186431208
At time: 377.25949001312256 and batch: 3050, loss is 5.841006832122803 and perplexity is 344.1256430532341
At time: 383.3153555393219 and batch: 3100, loss is 5.867144212722779 and perplexity is 353.23876370532133
At time: 389.39124822616577 and batch: 3150, loss is 5.854997787475586 and perplexity is 348.97412793935337
At time: 395.46480798721313 and batch: 3200, loss is 5.86051911354065 and perplexity is 350.9062569366579
At time: 401.5449073314667 and batch: 3250, loss is 5.8573840141296385 and perplexity is 349.8078536386517
At time: 407.6300618648529 and batch: 3300, loss is 5.8519346809387205 and perplexity is 347.90681848266547
At time: 413.6953420639038 and batch: 3350, loss is 5.859638719558716 and perplexity is 350.5974571325183
At time: 419.75698137283325 and batch: 3400, loss is 5.854999208450318 and perplexity is 348.9746238231235
At time: 425.8285799026489 and batch: 3450, loss is 5.852350435256958 and perplexity is 348.05149231709566
At time: 431.8951120376587 and batch: 3500, loss is 5.868061103820801 and perplexity is 353.56279371065204
At time: 437.9715778827667 and batch: 3550, loss is 5.835164680480957 and perplexity is 342.1210700737902
At time: 444.0447635650635 and batch: 3600, loss is 5.843410959243775 and perplexity is 344.9539601355407
At time: 450.1272974014282 and batch: 3650, loss is 5.862924308776855 and perplexity is 351.75127079848954
At time: 456.213894367218 and batch: 3700, loss is 5.8539384078979495 and perplexity is 348.6046276301887
At time: 462.27513432502747 and batch: 3750, loss is 5.844580736160278 and perplexity is 345.3577154211294
At time: 468.3386216163635 and batch: 3800, loss is 5.8100435924530025 and perplexity is 333.6336692676177
At time: 474.40421986579895 and batch: 3850, loss is 5.861594820022583 and perplexity is 351.2839321692248
At time: 480.48689007759094 and batch: 3900, loss is 5.860117835998535 and perplexity is 350.76547438468896
At time: 486.56370282173157 and batch: 3950, loss is 5.848148450851441 and perplexity is 346.59205379042896
At time: 492.6458201408386 and batch: 4000, loss is 5.827963800430298 and perplexity is 339.6663459792582
At time: 498.73600697517395 and batch: 4050, loss is 5.8475045967102055 and perplexity is 346.3689708854094
At time: 504.8248109817505 and batch: 4100, loss is 5.814322023391724 and perplexity is 335.06415581728317
At time: 510.9225296974182 and batch: 4150, loss is 5.856345100402832 and perplexity is 349.44462217351025
At time: 517.0176510810852 and batch: 4200, loss is 5.801801376342773 and perplexity is 330.8950899487211
At time: 523.0896587371826 and batch: 4250, loss is 5.856563110351562 and perplexity is 349.5208128825388
At time: 529.1721396446228 and batch: 4300, loss is 5.8579142379760745 and perplexity is 349.99337928503587
At time: 535.2581162452698 and batch: 4350, loss is 5.875027399063111 and perplexity is 356.03441554656246
At time: 541.3364071846008 and batch: 4400, loss is 5.84339243888855 and perplexity is 344.9475715248227
At time: 547.4348316192627 and batch: 4450, loss is 5.874690294265747 and perplexity is 355.91441486459433
At time: 553.5175476074219 and batch: 4500, loss is 5.8339712524414065 and perplexity is 341.7130167354225
At time: 559.5919210910797 and batch: 4550, loss is 5.851443347930908 and perplexity is 347.7359223659913
At time: 565.6707928180695 and batch: 4600, loss is 5.847346086502075 and perplexity is 346.3140722188588
At time: 571.7442874908447 and batch: 4650, loss is 5.807374973297119 and perplexity is 332.7445150014939
At time: 577.8300387859344 and batch: 4700, loss is 5.881200590133667 and perplexity is 358.2390819335179
At time: 583.9075281620026 and batch: 4750, loss is 5.828342475891113 and perplexity is 339.7949936456654
At time: 589.9913866519928 and batch: 4800, loss is 5.844117994308472 and perplexity is 345.1979409223846
At time: 596.0670967102051 and batch: 4850, loss is 5.8071670246124265 and perplexity is 332.6753284111403
At time: 602.1566882133484 and batch: 4900, loss is 5.835732021331787 and perplexity is 342.3152244034064
At time: 608.238888502121 and batch: 4950, loss is 5.853518600463867 and perplexity is 348.45831153039614
At time: 614.3173303604126 and batch: 5000, loss is 5.819680309295654 and perplexity is 336.86434401470007
At time: 620.4105081558228 and batch: 5050, loss is 5.83206018447876 and perplexity is 341.06060353826075
At time: 626.4910326004028 and batch: 5100, loss is 5.809347772598267 and perplexity is 333.40160108452096
At time: 632.5734856128693 and batch: 5150, loss is 5.823455400466919 and perplexity is 338.1384410287693
At time: 638.6510443687439 and batch: 5200, loss is 5.861765956878662 and perplexity is 351.34405494143147
At time: 644.7312543392181 and batch: 5250, loss is 5.850062894821167 and perplexity is 347.2562204097998
At time: 650.8150646686554 and batch: 5300, loss is 5.829465618133545 and perplexity is 340.17684615399634
At time: 656.9054555892944 and batch: 5350, loss is 5.805510168075561 and perplexity is 332.124589492287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 356 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
