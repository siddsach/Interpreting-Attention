Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'dropout': 0.8854947775984927, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 18.411270472382746, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 6.977251598321425, 'wordvec_source': 'glove', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.3423781394958496 and batch: 50, loss is 7.471271162033081 and perplexity is 1756.838493170858
At time: 4.01271915435791 and batch: 100, loss is 6.813322734832764 and perplexity is 909.8891098998304
At time: 5.65971565246582 and batch: 150, loss is 6.702192144393921 and perplexity is 814.1886894768109
At time: 7.307568073272705 and batch: 200, loss is 6.663047370910644 and perplexity is 782.933193312419
At time: 8.955710649490356 and batch: 250, loss is 6.677052822113037 and perplexity is 793.9756729002081
At time: 10.604860544204712 and batch: 300, loss is 6.640887889862061 and perplexity is 765.77461477784
At time: 12.254929780960083 and batch: 350, loss is 6.579701232910156 and perplexity is 720.3240879657492
At time: 13.90617847442627 and batch: 400, loss is 6.599732370376587 and perplexity is 734.8984823167774
At time: 15.556338548660278 and batch: 450, loss is 6.583790445327759 and perplexity is 723.2756768950541
At time: 17.206149578094482 and batch: 500, loss is 6.584731130599976 and perplexity is 723.9563717816942
At time: 18.860561847686768 and batch: 550, loss is 6.609190282821655 and perplexity is 741.8820607975482
At time: 20.520748376846313 and batch: 600, loss is 6.602360725402832 and perplexity is 736.8325970932563
At time: 22.196966886520386 and batch: 650, loss is 6.613550252914429 and perplexity is 745.1237059979275
At time: 23.871894121170044 and batch: 700, loss is 6.593187665939331 and perplexity is 730.1044936897389
At time: 25.54176688194275 and batch: 750, loss is 6.581242189407349 and perplexity is 721.4349317103148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.095488082530887 and perplexity of 443.8506280555437
Finished 1 epochs...
Completing Train Step...
At time: 29.956173419952393 and batch: 50, loss is 6.519832544326782 and perplexity is 678.4647630527081
At time: 31.610053539276123 and batch: 100, loss is 6.604171543121338 and perplexity is 738.1680754046138
At time: 33.26384115219116 and batch: 150, loss is 6.695542402267456 and perplexity is 808.7925061478112
At time: 34.91747188568115 and batch: 200, loss is 6.727187881469726 and perplexity is 834.7964155991202
At time: 36.57114601135254 and batch: 250, loss is 6.80939847946167 and perplexity is 906.325469563843
At time: 38.22523856163025 and batch: 300, loss is 6.8487683868408205 and perplexity is 942.7191261051961
At time: 39.879565954208374 and batch: 350, loss is 6.792180862426758 and perplexity is 890.8542754637949
At time: 41.535818338394165 and batch: 400, loss is 6.830664253234863 and perplexity is 925.8055776263775
At time: 43.1907696723938 and batch: 450, loss is 6.820942869186402 and perplexity is 916.8490714067476
At time: 44.847596883773804 and batch: 500, loss is 6.866234397888183 and perplexity is 959.3293032870129
At time: 46.50141644477844 and batch: 550, loss is 6.874190359115601 and perplexity is 966.992132193458
At time: 48.15584421157837 and batch: 600, loss is 6.844543085098267 and perplexity is 938.7442567637707
At time: 49.85452127456665 and batch: 650, loss is 6.866586561203003 and perplexity is 959.6672033689765
At time: 51.50851368904114 and batch: 700, loss is 6.853009786605835 and perplexity is 946.7260662958317
At time: 53.16670846939087 and batch: 750, loss is 6.782470302581787 and perplexity is 882.2454476250433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.376241284747456 and perplexity of 587.7144994012835
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 57.48489570617676 and batch: 50, loss is 6.771449174880981 and perplexity is 872.5754926676343
At time: 59.1702721118927 and batch: 100, loss is 6.704745235443116 and perplexity is 816.2700431444098
At time: 60.82923126220703 and batch: 150, loss is 6.668677883148193 and perplexity is 787.3539420994991
At time: 62.487714767456055 and batch: 200, loss is 6.6304780292510985 and perplexity is 757.8443558372237
At time: 64.14644622802734 and batch: 250, loss is 6.669521532058716 and perplexity is 788.0184726707785
At time: 65.8034074306488 and batch: 300, loss is 6.646398372650147 and perplexity is 770.0060505316332
At time: 67.45861864089966 and batch: 350, loss is 6.519203557968139 and perplexity is 678.0381521522115
At time: 69.11444091796875 and batch: 400, loss is 6.507420778274536 and perplexity is 670.0958610030377
At time: 70.77134990692139 and batch: 450, loss is 6.465842590332032 and perplexity is 642.805757243846
At time: 72.4286196231842 and batch: 500, loss is 6.389665737152099 and perplexity is 595.6574402550609
At time: 74.08613801002502 and batch: 550, loss is 6.329311151504516 and perplexity is 560.7701750848578
At time: 75.74533176422119 and batch: 600, loss is 6.242552289962768 and perplexity is 514.1691464220412
At time: 77.42462182044983 and batch: 650, loss is 6.155953559875488 and perplexity is 471.51624713355045
At time: 79.1333122253418 and batch: 700, loss is 6.110062580108643 and perplexity is 450.36689829518457
At time: 80.84057855606079 and batch: 750, loss is 6.08836371421814 and perplexity is 440.6997101826418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.798890224722928 and perplexity of 329.93320494747337
Finished 3 epochs...
Completing Train Step...
At time: 85.26234173774719 and batch: 50, loss is 6.273290548324585 and perplexity is 530.2192224682618
At time: 86.95564842224121 and batch: 100, loss is 6.281818065643311 and perplexity is 534.7600093734814
At time: 88.64074087142944 and batch: 150, loss is 6.283844690322876 and perplexity is 535.8448661335533
At time: 90.33774995803833 and batch: 200, loss is 6.2758299827575685 and perplexity is 531.5673904864736
At time: 92.04264235496521 and batch: 250, loss is 6.314815769195556 and perplexity is 552.7002268039952
At time: 93.74131989479065 and batch: 300, loss is 6.30879578590393 and perplexity is 549.3829755899461
At time: 95.43718433380127 and batch: 350, loss is 6.204798936843872 and perplexity is 495.1193956632175
At time: 97.13254952430725 and batch: 400, loss is 6.198130626678466 and perplexity is 491.8287696150952
At time: 98.83010339736938 and batch: 450, loss is 6.177681789398194 and perplexity is 481.87357610031773
At time: 100.5380470752716 and batch: 500, loss is 6.146561660766602 and perplexity is 467.1085448592201
At time: 102.24966502189636 and batch: 550, loss is 6.117410135269165 and perplexity is 453.68818063343457
At time: 103.96258902549744 and batch: 600, loss is 6.068169746398926 and perplexity is 431.88949045996543
At time: 105.67343997955322 and batch: 650, loss is 6.0362258148193355 and perplexity is 418.31126763581966
At time: 107.38704538345337 and batch: 700, loss is 6.023171253204346 and perplexity is 412.88588744813194
At time: 109.09946846961975 and batch: 750, loss is 6.011482572555542 and perplexity is 408.0878919113008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.728688794513081 and perplexity of 307.56572200683644
Finished 4 epochs...
Completing Train Step...
At time: 113.52476716041565 and batch: 50, loss is 6.162265224456787 and perplexity is 474.50171124311447
At time: 115.22264385223389 and batch: 100, loss is 6.169878845214844 and perplexity is 478.1281750636942
At time: 116.90633368492126 and batch: 150, loss is 6.164668579101562 and perplexity is 475.64347862116847
At time: 118.60995173454285 and batch: 200, loss is 6.16598147392273 and perplexity is 476.26835859205283
At time: 120.33214616775513 and batch: 250, loss is 6.202542247772217 and perplexity is 494.0033249198309
At time: 122.05107879638672 and batch: 300, loss is 6.194835510253906 and perplexity is 490.210803715232
At time: 123.77316045761108 and batch: 350, loss is 6.101147756576538 and perplexity is 446.3698000570052
At time: 125.48705887794495 and batch: 400, loss is 6.1028358745574955 and perplexity is 447.123961320135
At time: 127.2000584602356 and batch: 450, loss is 6.080976514816284 and perplexity is 437.45616864143426
At time: 128.9608600139618 and batch: 500, loss is 6.049942054748535 and perplexity is 424.0884554205871
At time: 130.67075037956238 and batch: 550, loss is 6.031752624511719 and perplexity is 416.44426057994235
At time: 132.38203525543213 and batch: 600, loss is 5.995159740447998 and perplexity is 401.48081159360333
At time: 134.09191703796387 and batch: 650, loss is 5.978042240142822 and perplexity is 394.6669482089356
At time: 135.8025085926056 and batch: 700, loss is 5.966555156707764 and perplexity is 390.1593153891765
At time: 137.51673245429993 and batch: 750, loss is 5.955030822753907 and perplexity is 385.6887984857323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.67620849609375 and perplexity of 291.8408140035347
Finished 5 epochs...
Completing Train Step...
At time: 142.06186819076538 and batch: 50, loss is 6.07429009437561 and perplexity is 434.5409099574852
At time: 143.74975419044495 and batch: 100, loss is 6.075287399291992 and perplexity is 434.97449591612127
At time: 145.41056990623474 and batch: 150, loss is 6.062170934677124 and perplexity is 429.30642213846505
At time: 147.09627985954285 and batch: 200, loss is 6.067630624771118 and perplexity is 431.65671224835774
At time: 148.79151105880737 and batch: 250, loss is 6.104910745620727 and perplexity is 448.0526490095888
At time: 150.48387455940247 and batch: 300, loss is 6.095836782455445 and perplexity is 444.0054257234608
At time: 152.18953704833984 and batch: 350, loss is 6.006110877990722 and perplexity is 405.90164558114446
At time: 153.89540934562683 and batch: 400, loss is 6.012087364196777 and perplexity is 408.33477470601815
At time: 155.60321736335754 and batch: 450, loss is 5.982338876724243 and perplexity is 396.36633686945095
At time: 157.309654712677 and batch: 500, loss is 5.962366046905518 and perplexity is 388.5283137842021
At time: 159.01819777488708 and batch: 550, loss is 5.952080736160278 and perplexity is 384.55265980973064
At time: 160.72755026817322 and batch: 600, loss is 5.922991466522217 and perplexity is 373.5274390459309
At time: 162.4340751171112 and batch: 650, loss is 5.916411209106445 and perplexity is 371.0776014658873
At time: 164.14130687713623 and batch: 700, loss is 5.911055164337158 and perplexity is 369.09540632116074
At time: 165.84702444076538 and batch: 750, loss is 5.892234649658203 and perplexity is 362.21380160740796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.616748898528343 and perplexity of 274.9938953047469
Finished 6 epochs...
Completing Train Step...
At time: 170.33786582946777 and batch: 50, loss is 5.998078145980835 and perplexity is 402.6542068041973
At time: 172.04992938041687 and batch: 100, loss is 5.996892623901367 and perplexity is 402.1771341975702
At time: 173.7340853214264 and batch: 150, loss is 5.984629049301147 and perplexity is 397.27512442752163
At time: 175.42895412445068 and batch: 200, loss is 5.996288137435913 and perplexity is 401.93409702697346
At time: 177.12508368492126 and batch: 250, loss is 6.032996292114258 and perplexity is 416.9625010077069
At time: 178.82897758483887 and batch: 300, loss is 6.026442136764526 and perplexity is 414.23860018652556
At time: 180.53569221496582 and batch: 350, loss is 5.945349340438843 and perplexity is 381.9727765300211
At time: 182.2415702342987 and batch: 400, loss is 5.959521818161011 and perplexity is 387.4248204237547
At time: 183.94944381713867 and batch: 450, loss is 5.931836585998536 and perplexity is 376.84598872030665
At time: 185.6547646522522 and batch: 500, loss is 5.921654558181762 and perplexity is 373.02840075583646
At time: 187.36243629455566 and batch: 550, loss is 5.925390968322754 and perplexity is 374.42479498151107
At time: 189.0707130432129 and batch: 600, loss is 5.904369010925293 and perplexity is 366.6358095943495
At time: 190.77998232841492 and batch: 650, loss is 5.900221910476684 and perplexity is 365.1184824924953
At time: 192.48829102516174 and batch: 700, loss is 5.892345886230469 and perplexity is 362.25409527014915
At time: 194.19695401191711 and batch: 750, loss is 5.873512506484985 and perplexity is 355.49546997828327
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.594659849654796 and perplexity of 268.98613880138197
Finished 7 epochs...
Completing Train Step...
At time: 198.64388298988342 and batch: 50, loss is 5.9645007228851314 and perplexity is 389.35858170424655
At time: 200.3365569114685 and batch: 100, loss is 5.960943508148193 and perplexity is 387.97601012934103
At time: 202.02339959144592 and batch: 150, loss is 5.950120143890381 and perplexity is 383.7994474499783
At time: 203.71732091903687 and batch: 200, loss is 5.965522499084472 and perplexity is 389.75662235564266
At time: 205.41771340370178 and batch: 250, loss is 6.002309684753418 and perplexity is 404.3616637295257
At time: 207.1186821460724 and batch: 300, loss is 5.998652667999267 and perplexity is 402.8856069777031
At time: 208.82565689086914 and batch: 350, loss is 5.919013185501099 and perplexity is 372.0443938655327
At time: 210.5319619178772 and batch: 400, loss is 5.935492486953735 and perplexity is 378.2262217908282
At time: 212.2505829334259 and batch: 450, loss is 5.908211288452148 and perplexity is 368.0472359355319
At time: 213.99346828460693 and batch: 500, loss is 5.903533058166504 and perplexity is 366.3294474478301
At time: 215.70218777656555 and batch: 550, loss is 5.9150214195251465 and perplexity is 370.56223988654585
At time: 217.4103033542633 and batch: 600, loss is 5.890099821090698 and perplexity is 361.4413620425823
At time: 219.11674046516418 and batch: 650, loss is 5.887622003555298 and perplexity is 360.5468849308732
At time: 220.8246066570282 and batch: 700, loss is 5.878355827331543 and perplexity is 357.2214249016983
At time: 222.54232573509216 and batch: 750, loss is 5.85928768157959 and perplexity is 350.47440570880497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.582194128701853 and perplexity of 265.65384552540536
Finished 8 epochs...
Completing Train Step...
At time: 226.9817852973938 and batch: 50, loss is 5.937883825302124 and perplexity is 379.13177096506433
At time: 228.68233036994934 and batch: 100, loss is 5.930209922790527 and perplexity is 376.2334855186399
At time: 230.39170932769775 and batch: 150, loss is 5.921122913360596 and perplexity is 372.83013484661836
At time: 232.12062215805054 and batch: 200, loss is 5.931006269454956 and perplexity is 376.5332171291733
At time: 233.84433722496033 and batch: 250, loss is 5.962585859298706 and perplexity is 388.61372650972015
At time: 235.55767703056335 and batch: 300, loss is 5.960298652648926 and perplexity is 387.72590231599185
At time: 237.26455211639404 and batch: 350, loss is 5.881548795700073 and perplexity is 358.36384449620033
At time: 238.97265028953552 and batch: 400, loss is 5.9013033294677735 and perplexity is 365.5135421274554
At time: 240.68184685707092 and batch: 450, loss is 5.871020078659058 and perplexity is 354.6105264639581
At time: 242.3896520137787 and batch: 500, loss is 5.868924608230591 and perplexity is 353.86822859536517
At time: 244.0992875099182 and batch: 550, loss is 5.882974481582641 and perplexity is 358.8751231448958
At time: 245.8092839717865 and batch: 600, loss is 5.862336769104004 and perplexity is 351.5446636728044
At time: 247.5177867412567 and batch: 650, loss is 5.859615297317505 and perplexity is 350.58924545047756
At time: 249.2273952960968 and batch: 700, loss is 5.848520421981812 and perplexity is 346.7210000091036
At time: 250.93286085128784 and batch: 750, loss is 5.826233234405517 and perplexity is 339.07903927423524
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.550815316133721 and perplexity of 257.44737135664207
Finished 9 epochs...
Completing Train Step...
At time: 255.34188580513 and batch: 50, loss is 5.898038110733032 and perplexity is 364.3220068322282
At time: 257.039843082428 and batch: 100, loss is 5.886580448150635 and perplexity is 360.1715508737785
At time: 258.7367672920227 and batch: 150, loss is 5.882086591720581 and perplexity is 358.55662297876825
At time: 260.4438009262085 and batch: 200, loss is 5.896398077011108 and perplexity is 363.72499614802706
At time: 262.1518795490265 and batch: 250, loss is 5.933434381484985 and perplexity is 377.4485928310331
At time: 263.8748769760132 and batch: 300, loss is 5.93462965965271 and perplexity is 377.9000186294377
At time: 265.588844537735 and batch: 350, loss is 5.859866542816162 and perplexity is 350.6773404865519
At time: 267.3129098415375 and batch: 400, loss is 5.881948919296264 and perplexity is 358.5072630170587
At time: 269.0477249622345 and batch: 450, loss is 5.849954118728638 and perplexity is 347.218449289372
At time: 270.77695322036743 and batch: 500, loss is 5.852386636734009 and perplexity is 348.06409252327893
At time: 272.50901770591736 and batch: 550, loss is 5.869191942214965 and perplexity is 353.96284224501755
At time: 274.2385187149048 and batch: 600, loss is 5.848114032745361 and perplexity is 346.58012495364034
At time: 275.96431708335876 and batch: 650, loss is 5.845462551116944 and perplexity is 345.66239133413313
At time: 277.6917562484741 and batch: 700, loss is 5.83328444480896 and perplexity is 341.4784062027969
At time: 279.41302251815796 and batch: 750, loss is 5.810472927093506 and perplexity is 333.7769405125259
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.535004726676053 and perplexity of 253.40898541644137
Finished 10 epochs...
Completing Train Step...
At time: 284.0113697052002 and batch: 50, loss is 5.879244318008423 and perplexity is 357.53895384717066
At time: 285.73280334472656 and batch: 100, loss is 5.864107255935669 and perplexity is 352.1676201759943
At time: 287.4264166355133 and batch: 150, loss is 5.861334476470947 and perplexity is 351.19248956645373
At time: 289.1363458633423 and batch: 200, loss is 5.876792659759522 and perplexity is 356.6634641609434
At time: 290.86056661605835 and batch: 250, loss is 5.912706432342529 and perplexity is 369.7053852372805
At time: 292.58659982681274 and batch: 300, loss is 5.914232788085937 and perplexity is 370.27011805735697
At time: 294.3114082813263 and batch: 350, loss is 5.843543539047241 and perplexity is 344.999697095612
At time: 296.0231909751892 and batch: 400, loss is 5.8659421253204345 and perplexity is 352.8143949527188
At time: 297.7444496154785 and batch: 450, loss is 5.837031564712524 and perplexity is 342.7603670658587
At time: 299.5038857460022 and batch: 500, loss is 5.838933267593384 and perplexity is 343.41281562837753
At time: 301.2135422229767 and batch: 550, loss is 5.855818729400635 and perplexity is 349.2607330587191
At time: 302.92290782928467 and batch: 600, loss is 5.836107892990112 and perplexity is 342.4439151785652
At time: 304.63305926322937 and batch: 650, loss is 5.835661172866821 and perplexity is 342.2909727543291
At time: 306.3442847728729 and batch: 700, loss is 5.823594751358033 and perplexity is 338.18556420509844
At time: 308.053836107254 and batch: 750, loss is 5.7995037174224855 and perplexity is 330.13567866177243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.524889568949854 and perplexity of 250.8586339087949
Finished 11 epochs...
Completing Train Step...
At time: 312.5011234283447 and batch: 50, loss is 5.863959932327271 and perplexity is 352.1157413930076
At time: 314.1964445114136 and batch: 100, loss is 5.850387716293335 and perplexity is 347.369035007843
At time: 315.8940603733063 and batch: 150, loss is 5.847705497741699 and perplexity is 346.4385637593436
At time: 317.6029841899872 and batch: 200, loss is 5.866314992904663 and perplexity is 352.9459725328288
At time: 319.3095395565033 and batch: 250, loss is 5.900311632156372 and perplexity is 365.15124300567163
At time: 321.01756525039673 and batch: 300, loss is 5.902974157333374 and perplexity is 366.1247628189845
At time: 322.7268352508545 and batch: 350, loss is 5.83468318939209 and perplexity is 341.95638147846324
At time: 324.43551206588745 and batch: 400, loss is 5.857505359649658 and perplexity is 349.85030383007665
At time: 326.1608021259308 and batch: 450, loss is 5.828398895263672 and perplexity is 339.8141652068241
At time: 327.87603545188904 and batch: 500, loss is 5.829680376052856 and perplexity is 340.24990967087774
At time: 329.59834599494934 and batch: 550, loss is 5.845691032409668 and perplexity is 345.7413777472667
At time: 331.309707403183 and batch: 600, loss is 5.8275526332855225 and perplexity is 339.5267150453991
At time: 333.0186302661896 and batch: 650, loss is 5.82556116104126 and perplexity is 338.8512298444607
At time: 334.7278928756714 and batch: 700, loss is 5.814394454956055 and perplexity is 335.08842591719036
At time: 336.447140455246 and batch: 750, loss is 5.792003183364868 and perplexity is 327.6687479745277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.518557881200036 and perplexity of 249.2752932685156
Finished 12 epochs...
Completing Train Step...
At time: 341.0307788848877 and batch: 50, loss is 5.853194780349732 and perplexity is 348.34549198778933
At time: 342.723592042923 and batch: 100, loss is 5.841207761764526 and perplexity is 344.1947950425101
At time: 344.4123296737671 and batch: 150, loss is 5.841574993133545 and perplexity is 344.32121738000603
At time: 346.14611530303955 and batch: 200, loss is 5.860077295303345 and perplexity is 350.75125439675475
At time: 347.865704536438 and batch: 250, loss is 5.895860252380371 and perplexity is 363.52942848155186
At time: 349.5795555114746 and batch: 300, loss is 5.897982912063599 and perplexity is 364.30189729722065
At time: 351.3043966293335 and batch: 350, loss is 5.829789657592773 and perplexity is 340.2870947367468
At time: 353.03693437576294 and batch: 400, loss is 5.853457860946655 and perplexity is 348.4371469835539
At time: 354.7622985839844 and batch: 450, loss is 5.823349027633667 and perplexity is 338.10247419774646
At time: 356.47055101394653 and batch: 500, loss is 5.824949731826782 and perplexity is 338.64410962930003
At time: 358.18147134780884 and batch: 550, loss is 5.843388051986694 and perplexity is 344.9460582770002
At time: 359.892147064209 and batch: 600, loss is 5.821618328094482 and perplexity is 337.5178264721637
At time: 361.60197377204895 and batch: 650, loss is 5.822038917541504 and perplexity is 337.6598127650348
At time: 363.3157775402069 and batch: 700, loss is 5.808414678573609 and perplexity is 333.09065113806713
At time: 365.02529978752136 and batch: 750, loss is 5.7846775817871094 and perplexity is 325.27714792723634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.518206219340479 and perplexity of 249.18764806698374
Finished 13 epochs...
Completing Train Step...
At time: 369.44028186798096 and batch: 50, loss is 5.846869173049927 and perplexity is 346.14894975684103
At time: 371.1494505405426 and batch: 100, loss is 5.83240421295166 and perplexity is 341.1779582823918
At time: 372.8460624217987 and batch: 150, loss is 5.832167329788208 and perplexity is 341.0971485399454
At time: 374.5411081314087 and batch: 200, loss is 5.849748525619507 and perplexity is 347.1470709065382
At time: 376.24302649497986 and batch: 250, loss is 5.883918428421021 and perplexity is 359.2140421183425
At time: 377.95263171195984 and batch: 300, loss is 5.887445039749146 and perplexity is 360.48308682696376
At time: 379.66260719299316 and batch: 350, loss is 5.819565944671631 and perplexity is 336.8258208535354
At time: 381.36993384361267 and batch: 400, loss is 5.843559846878052 and perplexity is 345.0053233381779
At time: 383.0784158706665 and batch: 450, loss is 5.812784919738769 and perplexity is 334.54952310264144
At time: 384.8293397426605 and batch: 500, loss is 5.812940683364868 and perplexity is 334.6016378081519
At time: 386.53751492500305 and batch: 550, loss is 5.831710844039917 and perplexity is 340.94147808622915
At time: 388.24575996398926 and batch: 600, loss is 5.81126781463623 and perplexity is 334.04236112038006
At time: 389.953186750412 and batch: 650, loss is 5.8123931789398195 and perplexity is 334.41849207194895
At time: 391.66458082199097 and batch: 700, loss is 5.7994129180908205 and perplexity is 330.10570392365486
At time: 393.3717586994171 and batch: 750, loss is 5.775566778182983 and perplexity is 322.32707090805906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.508547405863917 and perplexity of 246.7923774116233
Finished 14 epochs...
Completing Train Step...
At time: 397.78002643585205 and batch: 50, loss is 5.837438344955444 and perplexity is 342.8998235733901
At time: 399.4821014404297 and batch: 100, loss is 5.822415065765381 and perplexity is 337.78684679419536
At time: 401.18024802207947 and batch: 150, loss is 5.821588668823242 and perplexity is 337.50781608785115
At time: 402.88033652305603 and batch: 200, loss is 5.841127967834472 and perplexity is 344.1673314828369
At time: 404.59420251846313 and batch: 250, loss is 5.8771946144104 and perplexity is 356.80685551564164
At time: 406.29984402656555 and batch: 300, loss is 5.880475177764892 and perplexity is 357.9793051065583
At time: 408.01835775375366 and batch: 350, loss is 5.813917350769043 and perplexity is 334.9285919579235
At time: 409.725937128067 and batch: 400, loss is 5.837041616439819 and perplexity is 342.76381241691183
At time: 411.44549012184143 and batch: 450, loss is 5.805004730224609 and perplexity is 331.9567635698852
At time: 413.1511387825012 and batch: 500, loss is 5.808288564682007 and perplexity is 333.04864642854034
At time: 414.85850834846497 and batch: 550, loss is 5.825843210220337 and perplexity is 338.9468160350373
At time: 416.5654754638672 and batch: 600, loss is 5.805620326995849 and perplexity is 332.1611779937044
At time: 418.2713499069214 and batch: 650, loss is 5.806479434967041 and perplexity is 332.4466629230697
At time: 419.97799706459045 and batch: 700, loss is 5.79350396156311 and perplexity is 328.1608752823306
At time: 421.6924421787262 and batch: 750, loss is 5.768748626708985 and perplexity is 320.13687115595104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.509932052257449 and perplexity of 247.13433427699314
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 426.09915947914124 and batch: 50, loss is 5.833554286956787 and perplexity is 341.5705639028197
At time: 427.8022427558899 and batch: 100, loss is 5.816272125244141 and perplexity is 335.7182025698402
At time: 429.49570393562317 and batch: 150, loss is 5.8081728458404545 and perplexity is 333.01010865481123
At time: 431.2186119556427 and batch: 200, loss is 5.825340967178345 and perplexity is 338.77662509726304
At time: 432.9407730102539 and batch: 250, loss is 5.855713777542114 and perplexity is 349.22407941914355
At time: 434.6625385284424 and batch: 300, loss is 5.8469493961334225 and perplexity is 346.17672000683183
At time: 436.38447618484497 and batch: 350, loss is 5.780550556182861 and perplexity is 323.937487117957
At time: 438.12125730514526 and batch: 400, loss is 5.7935354518890385 and perplexity is 328.17120933796065
At time: 439.84256625175476 and batch: 450, loss is 5.755660581588745 and perplexity is 315.9742053414283
At time: 441.55736923217773 and batch: 500, loss is 5.733771228790284 and perplexity is 309.13288369954626
At time: 443.27725744247437 and batch: 550, loss is 5.746037158966065 and perplexity is 312.94803643122617
At time: 444.99339604377747 and batch: 600, loss is 5.718326940536499 and perplexity is 304.39522538615194
At time: 446.71125197410583 and batch: 650, loss is 5.705248756408691 and perplexity is 300.4402071807555
At time: 448.42538022994995 and batch: 700, loss is 5.689218044281006 and perplexity is 295.66233537081496
At time: 450.14738607406616 and batch: 750, loss is 5.68699049949646 and perplexity is 295.004467265033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.445066939952762 and perplexity of 231.61278334198846
Finished 16 epochs...
Completing Train Step...
At time: 454.57472681999207 and batch: 50, loss is 5.786994132995606 and perplexity is 326.031542556893
At time: 456.27022433280945 and batch: 100, loss is 5.777531032562256 and perplexity is 322.9608254924895
At time: 457.95950651168823 and batch: 150, loss is 5.773866758346558 and perplexity is 321.77957400335623
At time: 459.66016006469727 and batch: 200, loss is 5.791469068527221 and perplexity is 327.4937819645318
At time: 461.3701591491699 and batch: 250, loss is 5.822955446243286 and perplexity is 337.9694295395287
At time: 463.08034920692444 and batch: 300, loss is 5.817782526016235 and perplexity is 336.2256547338066
At time: 464.79023337364197 and batch: 350, loss is 5.756919555664062 and perplexity is 316.37225919158425
At time: 466.5002348423004 and batch: 400, loss is 5.76997561454773 and perplexity is 320.52991628507925
At time: 468.21901392936707 and batch: 450, loss is 5.735453147888183 and perplexity is 309.6532576912349
At time: 469.9886403083801 and batch: 500, loss is 5.718984022140503 and perplexity is 304.59530361576304
At time: 471.71218633651733 and batch: 550, loss is 5.7381362628936765 and perplexity is 310.485208604327
At time: 473.4377336502075 and batch: 600, loss is 5.715980091094971 and perplexity is 303.6816932252209
At time: 475.15802097320557 and batch: 650, loss is 5.710516519546509 and perplexity is 302.0270308654776
At time: 476.8704445362091 and batch: 700, loss is 5.69772439956665 and perplexity is 298.18807141436855
At time: 478.5838460922241 and batch: 750, loss is 5.691281633377075 and perplexity is 296.2730908996927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.439408058343932 and perplexity of 230.30581549785717
Finished 17 epochs...
Completing Train Step...
At time: 482.9919149875641 and batch: 50, loss is 5.776833629608154 and perplexity is 322.7356701798477
At time: 484.68053126335144 and batch: 100, loss is 5.765463771820069 and perplexity is 319.08699328188465
At time: 486.370817899704 and batch: 150, loss is 5.761068687438965 and perplexity is 317.68765637594237
At time: 488.0806984901428 and batch: 200, loss is 5.779033603668213 and perplexity is 323.4464618574465
At time: 489.79130244255066 and batch: 250, loss is 5.8112317562103275 and perplexity is 334.0303162958132
At time: 491.50794291496277 and batch: 300, loss is 5.806934356689453 and perplexity is 332.5979345373395
At time: 493.2474398612976 and batch: 350, loss is 5.74813479423523 and perplexity is 313.60517624873404
At time: 494.95686626434326 and batch: 400, loss is 5.761009893417358 and perplexity is 317.6689787900798
At time: 496.6663820743561 and batch: 450, loss is 5.728225212097168 and perplexity is 307.4231729905382
At time: 498.38139367103577 and batch: 500, loss is 5.715468721389771 and perplexity is 303.5264393067401
At time: 500.09609746932983 and batch: 550, loss is 5.73822979927063 and perplexity is 310.51425162410567
At time: 501.8086998462677 and batch: 600, loss is 5.718234968185425 and perplexity is 304.3672307290045
At time: 503.51895785331726 and batch: 650, loss is 5.714938898086547 and perplexity is 303.3656665203963
At time: 505.2307050228119 and batch: 700, loss is 5.702116899490356 and perplexity is 299.5007433404107
At time: 506.9432096481323 and batch: 750, loss is 5.693262329101563 and perplexity is 296.8604992907215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.437642297079397 and perplexity of 229.89950923535284
Finished 18 epochs...
Completing Train Step...
At time: 511.35922408103943 and batch: 50, loss is 5.7709761905670165 and perplexity is 320.85079133594826
At time: 513.0704162120819 and batch: 100, loss is 5.758111410140991 and perplexity is 316.7495536804779
At time: 514.7707743644714 and batch: 150, loss is 5.753689270019532 and perplexity is 315.35193528046625
At time: 516.4829800128937 and batch: 200, loss is 5.772258567810058 and perplexity is 321.2625070201596
At time: 518.1939277648926 and batch: 250, loss is 5.804881906509399 and perplexity is 331.9159939106862
At time: 519.9038324356079 and batch: 300, loss is 5.80138165473938 and perplexity is 330.75623527320494
At time: 521.635381937027 and batch: 350, loss is 5.743928804397583 and perplexity is 312.2889260722414
At time: 523.3444333076477 and batch: 400, loss is 5.757502975463868 and perplexity is 316.5568908853616
At time: 525.0543777942657 and batch: 450, loss is 5.725598564147949 and perplexity is 306.61674011494006
At time: 526.7653245925903 and batch: 500, loss is 5.715010452270508 and perplexity is 303.38737437974055
At time: 528.4863409996033 and batch: 550, loss is 5.739581966400147 and perplexity is 310.93440278167526
At time: 530.1975438594818 and batch: 600, loss is 5.720296859741211 and perplexity is 304.99545038974185
At time: 531.9088344573975 and batch: 650, loss is 5.717737865447998 and perplexity is 304.2159665454609
At time: 533.6215691566467 and batch: 700, loss is 5.70465594291687 and perplexity is 300.2621549535487
At time: 535.3491735458374 and batch: 750, loss is 5.694091720581055 and perplexity is 297.1068149913828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.436521663222202 and perplexity of 229.64202036387155
Finished 19 epochs...
Completing Train Step...
At time: 539.7859442234039 and batch: 50, loss is 5.767173156738282 and perplexity is 319.63290222712874
At time: 541.4970581531525 and batch: 100, loss is 5.753937168121338 and perplexity is 315.4301201171591
At time: 543.2015669345856 and batch: 150, loss is 5.74932071685791 and perplexity is 313.97730833815444
At time: 544.9127886295319 and batch: 200, loss is 5.768173561096192 and perplexity is 319.95282437452175
At time: 546.6232059001923 and batch: 250, loss is 5.801133823394776 and perplexity is 330.6742736674278
At time: 548.3346147537231 and batch: 300, loss is 5.798243160247803 and perplexity is 329.7197859467608
At time: 550.0499150753021 and batch: 350, loss is 5.741629447937012 and perplexity is 311.57168742218005
At time: 551.7599658966064 and batch: 400, loss is 5.7557776927947994 and perplexity is 316.01121162858107
At time: 553.469530582428 and batch: 450, loss is 5.724598226547241 and perplexity is 306.31017322155617
At time: 555.2245161533356 and batch: 500, loss is 5.714865627288819 and perplexity is 303.34343949031296
At time: 556.9346807003021 and batch: 550, loss is 5.740242004394531 and perplexity is 311.1396990454864
At time: 558.6463949680328 and batch: 600, loss is 5.721180410385132 and perplexity is 305.2650484005358
At time: 560.3573591709137 and batch: 650, loss is 5.71890531539917 and perplexity is 304.5713308554116
At time: 562.0682945251465 and batch: 700, loss is 5.705774116516113 and perplexity is 300.59808794877114
At time: 563.7798669338226 and batch: 750, loss is 5.693979396820068 and perplexity is 297.07344471068103
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.435582005700399 and perplexity of 229.4263368622968
Finished 20 epochs...
Completing Train Step...
At time: 568.1969220638275 and batch: 50, loss is 5.763915996551514 and perplexity is 318.59350033177515
At time: 569.8901355266571 and batch: 100, loss is 5.750311994552613 and perplexity is 314.2887013535294
At time: 571.5876953601837 and batch: 150, loss is 5.746077928543091 and perplexity is 312.9607954503909
At time: 573.2965123653412 and batch: 200, loss is 5.765149040222168 and perplexity is 318.98658232469904
At time: 575.0072588920593 and batch: 250, loss is 5.798088073730469 and perplexity is 329.668654818439
At time: 576.7156045436859 and batch: 300, loss is 5.795852270126343 and perplexity is 328.9324038148509
At time: 578.424482345581 and batch: 350, loss is 5.739866170883179 and perplexity is 311.02278429149044
At time: 580.1342720985413 and batch: 400, loss is 5.754384536743164 and perplexity is 315.5712652248704
At time: 581.843759059906 and batch: 450, loss is 5.7235825252532955 and perplexity is 305.99921153109574
At time: 583.5528156757355 and batch: 500, loss is 5.7142244243621825 and perplexity is 303.1489971343016
At time: 585.26154088974 and batch: 550, loss is 5.740217761993408 and perplexity is 311.13215636352356
At time: 586.9731020927429 and batch: 600, loss is 5.721451015472412 and perplexity is 305.34766585344926
At time: 588.6868183612823 and batch: 650, loss is 5.718995008468628 and perplexity is 304.59865001809635
At time: 590.3962185382843 and batch: 700, loss is 5.7059898185729985 and perplexity is 300.66293456816067
At time: 592.1068272590637 and batch: 750, loss is 5.693360023498535 and perplexity is 296.889502314878
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.434723964957303 and perplexity of 229.22956414930468
Finished 21 epochs...
Completing Train Step...
At time: 596.5306868553162 and batch: 50, loss is 5.761205129623413 and perplexity is 317.7310053309962
At time: 598.2403755187988 and batch: 100, loss is 5.747350425720215 and perplexity is 313.3592906673993
At time: 599.930908203125 and batch: 150, loss is 5.743151969909668 and perplexity is 312.04642346866297
At time: 601.6408948898315 and batch: 200, loss is 5.762576160430908 and perplexity is 318.1669230878546
At time: 603.349609375 and batch: 250, loss is 5.796017036437989 and perplexity is 328.9866052589717
At time: 605.0583488941193 and batch: 300, loss is 5.79408480644226 and perplexity is 328.3515412145803
At time: 606.7680237293243 and batch: 350, loss is 5.738590269088745 and perplexity is 310.6262028162116
At time: 608.477454662323 and batch: 400, loss is 5.753180637359619 and perplexity is 315.1915777718061
At time: 610.1870031356812 and batch: 450, loss is 5.722901620864868 and perplexity is 305.7909262443391
At time: 611.896954536438 and batch: 500, loss is 5.713814611434937 and perplexity is 303.0247882093443
At time: 613.6071050167084 and batch: 550, loss is 5.739842309951782 and perplexity is 311.01536308671035
At time: 615.3172311782837 and batch: 600, loss is 5.7213637733459475 and perplexity is 305.3210278357648
At time: 617.0284359455109 and batch: 650, loss is 5.719273471832276 and perplexity is 304.6834813934098
At time: 618.7370655536652 and batch: 700, loss is 5.705891056060791 and perplexity is 300.63324180770496
At time: 620.4435749053955 and batch: 750, loss is 5.692592554092407 and perplexity is 296.66173611782443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.433891118958939 and perplexity of 229.038730702563
Finished 22 epochs...
Completing Train Step...
At time: 624.827794790268 and batch: 50, loss is 5.758958873748779 and perplexity is 317.0181011760475
At time: 626.5222246646881 and batch: 100, loss is 5.745039377212525 and perplexity is 312.63593831946247
At time: 628.2230253219604 and batch: 150, loss is 5.741267070770264 and perplexity is 311.45880141174484
At time: 629.9305086135864 and batch: 200, loss is 5.760910139083863 and perplexity is 317.63729151332615
At time: 631.6411304473877 and batch: 250, loss is 5.794345426559448 and perplexity is 328.4371273839848
At time: 633.3613913059235 and batch: 300, loss is 5.79264726638794 and perplexity is 327.8798618324168
At time: 635.0815563201904 and batch: 350, loss is 5.737378816604615 and perplexity is 310.2501217793223
At time: 636.8027801513672 and batch: 400, loss is 5.7520542907714844 and perplexity is 314.8367626734734
At time: 638.5238995552063 and batch: 450, loss is 5.721865568161011 and perplexity is 305.47427479051066
At time: 640.2698287963867 and batch: 500, loss is 5.712923202514649 and perplexity is 302.75478956754364
At time: 641.978218793869 and batch: 550, loss is 5.739168453216553 and perplexity is 310.8058538870551
At time: 643.686980009079 and batch: 600, loss is 5.720674695968628 and perplexity is 305.11071049344673
At time: 645.3962919712067 and batch: 650, loss is 5.71827844619751 and perplexity is 304.3804642988229
At time: 647.109222650528 and batch: 700, loss is 5.7045629024505615 and perplexity is 300.23421972221075
At time: 648.8248209953308 and batch: 750, loss is 5.69039095878601 and perplexity is 296.00932546763335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.431938703670058 and perplexity of 228.59198823831457
Finished 23 epochs...
Completing Train Step...
At time: 653.2441613674164 and batch: 50, loss is 5.755697641372681 and perplexity is 315.98591549419405
At time: 654.9438097476959 and batch: 100, loss is 5.742060031890869 and perplexity is 311.70587407849763
At time: 656.6442551612854 and batch: 150, loss is 5.737604875564575 and perplexity is 310.3202645270788
At time: 658.3526167869568 and batch: 200, loss is 5.757310180664063 and perplexity is 316.49586624576216
At time: 660.0610311031342 and batch: 250, loss is 5.790350408554077 and perplexity is 327.127632615688
At time: 661.7691097259521 and batch: 300, loss is 5.789074964523316 and perplexity is 326.71066559494545
At time: 663.4765124320984 and batch: 350, loss is 5.734134225845337 and perplexity is 309.245118395149
At time: 665.1834797859192 and batch: 400, loss is 5.7493874454498295 and perplexity is 313.9982603008743
At time: 666.89080119133 and batch: 450, loss is 5.7191980266571045 and perplexity is 304.6604953618878
At time: 668.5980563163757 and batch: 500, loss is 5.710935926437378 and perplexity is 302.1537296507658
At time: 670.3056364059448 and batch: 550, loss is 5.737382583618164 and perplexity is 310.25129049793594
At time: 672.0139820575714 and batch: 600, loss is 5.719186544418335 and perplexity is 304.65699719741986
At time: 673.7215361595154 and batch: 650, loss is 5.717162199020386 and perplexity is 304.0408900244497
At time: 675.4287195205688 and batch: 700, loss is 5.703954544067383 and perplexity is 300.05162526479154
At time: 677.1431756019592 and batch: 750, loss is 5.688812341690063 and perplexity is 295.54240872423145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.429904671602471 and perplexity of 228.12749735866086
Finished 24 epochs...
Completing Train Step...
At time: 681.5445492267609 and batch: 50, loss is 5.753281211853027 and perplexity is 315.22327959923797
At time: 683.2514486312866 and batch: 100, loss is 5.7395850276947025 and perplexity is 310.93535464492663
At time: 684.944837808609 and batch: 150, loss is 5.735741872787475 and perplexity is 309.74267520478736
At time: 686.6515665054321 and batch: 200, loss is 5.7556189346313475 and perplexity is 315.96104625117965
At time: 688.3578581809998 and batch: 250, loss is 5.789068279266357 and perplexity is 326.70848145749557
At time: 690.063775062561 and batch: 300, loss is 5.787881402969361 and perplexity is 326.3209489269057
At time: 691.7720954418182 and batch: 350, loss is 5.732731695175171 and perplexity is 308.8116966466967
At time: 693.4808356761932 and batch: 400, loss is 5.748186473846435 and perplexity is 313.6213836611074
At time: 695.1891007423401 and batch: 450, loss is 5.718614101409912 and perplexity is 304.48264833656134
At time: 696.8968389034271 and batch: 500, loss is 5.71043251991272 and perplexity is 302.0016617710028
At time: 698.6042773723602 and batch: 550, loss is 5.737215690612793 and perplexity is 310.1995160481614
At time: 700.3116273880005 and batch: 600, loss is 5.71965012550354 and perplexity is 304.7982631603801
At time: 702.0177118778229 and batch: 650, loss is 5.7169640922546385 and perplexity is 303.9806634329166
At time: 703.7248725891113 and batch: 700, loss is 5.7035121154785156 and perplexity is 299.91890320981906
At time: 705.432610988617 and batch: 750, loss is 5.688198766708374 and perplexity is 295.36112691688817
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.430619350699491 and perplexity of 228.29059358626887
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 709.8337967395782 and batch: 50, loss is 5.752958354949951 and perplexity is 315.1215240145394
At time: 711.5309102535248 and batch: 100, loss is 5.7387660789489745 and perplexity is 310.68081876638286
At time: 713.2317459583282 and batch: 150, loss is 5.73437536239624 and perplexity is 309.31969768791777
At time: 714.9406433105469 and batch: 200, loss is 5.752607107162476 and perplexity is 315.010857713228
At time: 716.6487653255463 and batch: 250, loss is 5.783904371261596 and perplexity is 325.02573742190015
At time: 718.3569533824921 and batch: 300, loss is 5.780125417709351 and perplexity is 323.7997980996941
At time: 720.0648076534271 and batch: 350, loss is 5.722278318405151 and perplexity is 305.6003853963104
At time: 721.7740759849548 and batch: 400, loss is 5.73883059501648 and perplexity is 310.70086331764986
At time: 723.4887464046478 and batch: 450, loss is 5.70826434135437 and perplexity is 301.34757758525575
At time: 725.2254481315613 and batch: 500, loss is 5.694611978530884 and perplexity is 297.2614273895078
At time: 726.9340183734894 and batch: 550, loss is 5.716731491088868 and perplexity is 303.90996539877153
At time: 728.641321182251 and batch: 600, loss is 5.699097604751587 and perplexity is 298.5978260943573
At time: 730.3504137992859 and batch: 650, loss is 5.696349630355835 and perplexity is 297.7784132919633
At time: 732.0732989311218 and batch: 700, loss is 5.68270128250122 and perplexity is 293.7418388697084
At time: 733.7969055175781 and batch: 750, loss is 5.672426614761353 and perplexity is 290.7391910935905
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.420697589253271 and perplexity of 226.03674832307618
Finished 26 epochs...
Completing Train Step...
At time: 738.1988027095795 and batch: 50, loss is 5.7473067283630375 and perplexity is 313.34559799371925
At time: 739.8991377353668 and batch: 100, loss is 5.734272575378418 and perplexity is 309.2879052725904
At time: 741.5988414287567 and batch: 150, loss is 5.730863447189331 and perplexity is 308.23529841150634
At time: 743.3026757240295 and batch: 200, loss is 5.749149875640869 and perplexity is 313.923672654398
At time: 745.0127186775208 and batch: 250, loss is 5.780906772613525 and perplexity is 324.0528995280543
At time: 746.7232873439789 and batch: 300, loss is 5.777898569107055 and perplexity is 323.0795472143459
At time: 748.4348840713501 and batch: 350, loss is 5.720426750183106 and perplexity is 305.0350689565497
At time: 750.1594066619873 and batch: 400, loss is 5.737436981201172 and perplexity is 310.2681678773042
At time: 751.882043838501 and batch: 450, loss is 5.707150249481201 and perplexity is 301.0120356450495
At time: 753.6042230129242 and batch: 500, loss is 5.694203481674195 and perplexity is 297.14002182938725
At time: 755.3265278339386 and batch: 550, loss is 5.716982841491699 and perplexity is 303.9863628918672
At time: 757.0507073402405 and batch: 600, loss is 5.699661674499512 and perplexity is 298.76630360692036
At time: 758.7738242149353 and batch: 650, loss is 5.6974137592315675 and perplexity is 298.09545655764657
At time: 760.4858584403992 and batch: 700, loss is 5.683810386657715 and perplexity is 294.0678098986316
At time: 762.1981287002563 and batch: 750, loss is 5.672908630371094 and perplexity is 290.8793657025218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.419833870821221 and perplexity of 225.84160050574422
Finished 27 epochs...
Completing Train Step...
At time: 766.6127457618713 and batch: 50, loss is 5.745622386932373 and perplexity is 312.8182612531337
At time: 768.3258211612701 and batch: 100, loss is 5.732377443313599 and perplexity is 308.70231890306286
At time: 770.0090672969818 and batch: 150, loss is 5.7291871929168705 and perplexity is 307.7190504779037
At time: 771.7108416557312 and batch: 200, loss is 5.747597751617431 and perplexity is 313.4368021200145
At time: 773.4195580482483 and batch: 250, loss is 5.779510536193848 and perplexity is 323.6007607875721
At time: 775.1280052661896 and batch: 300, loss is 5.776949415206909 and perplexity is 322.773040486089
At time: 776.8392214775085 and batch: 350, loss is 5.719672212600708 and perplexity is 304.8049953435821
At time: 778.5468873977661 and batch: 400, loss is 5.736944656372071 and perplexity is 310.1154527503876
At time: 780.2538774013519 and batch: 450, loss is 5.707004404067993 and perplexity is 300.9681376215705
At time: 781.9594833850861 and batch: 500, loss is 5.694421701431274 and perplexity is 297.2048707281676
At time: 783.664742231369 and batch: 550, loss is 5.717538099288941 and perplexity is 304.155200559996
At time: 785.3704233169556 and batch: 600, loss is 5.7003576278686525 and perplexity is 298.9743033931818
At time: 787.075544834137 and batch: 650, loss is 5.6981628799438475 and perplexity is 298.31884970215924
At time: 788.7829213142395 and batch: 700, loss is 5.684441385269165 and perplexity is 294.2534248335741
At time: 790.4896965026855 and batch: 750, loss is 5.673097925186157 and perplexity is 290.9344328700585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.41942436750545 and perplexity of 225.7491365549375
Finished 28 epochs...
Completing Train Step...
At time: 795.0371091365814 and batch: 50, loss is 5.744436616897583 and perplexity is 312.44755056485394
At time: 796.7312309741974 and batch: 100, loss is 5.731134185791015 and perplexity is 308.3187609029373
At time: 798.428150177002 and batch: 150, loss is 5.728033542633057 and perplexity is 307.3642550022856
At time: 800.1407558917999 and batch: 200, loss is 5.746515884399414 and perplexity is 313.0978884817145
At time: 801.8635811805725 and batch: 250, loss is 5.77864517211914 and perplexity is 323.32084944471643
At time: 803.5881814956665 and batch: 300, loss is 5.776416463851929 and perplexity is 322.60106398843516
At time: 805.3106544017792 and batch: 350, loss is 5.719263420104981 and perplexity is 304.6804188135357
At time: 807.0287778377533 and batch: 400, loss is 5.736742668151855 and perplexity is 310.05281940783686
At time: 808.7391819953918 and batch: 450, loss is 5.7070777893066404 and perplexity is 300.99022505061095
At time: 810.4908854961395 and batch: 500, loss is 5.694721260070801 and perplexity is 297.29391435113104
At time: 812.2068507671356 and batch: 550, loss is 5.718011445999146 and perplexity is 304.29920550301705
At time: 813.9179964065552 and batch: 600, loss is 5.700901308059692 and perplexity is 299.13689399420616
At time: 815.6269509792328 and batch: 650, loss is 5.69865912437439 and perplexity is 298.46692550770797
At time: 817.3377540111542 and batch: 700, loss is 5.684828939437867 and perplexity is 294.3674860760505
At time: 819.0527489185333 and batch: 750, loss is 5.6731148529052735 and perplexity is 290.9393577681029
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.419131966524346 and perplexity of 225.68313693557238
Finished 29 epochs...
Completing Train Step...
At time: 823.432354927063 and batch: 50, loss is 5.74345929145813 and perplexity is 312.1423367960766
At time: 825.138790845871 and batch: 100, loss is 5.730127773284912 and perplexity is 308.00862113654557
At time: 826.8407917022705 and batch: 150, loss is 5.7271136283874515 and perplexity is 307.08163625795396
At time: 828.5435228347778 and batch: 200, loss is 5.745695896148682 and perplexity is 312.8412571235593
At time: 830.2531895637512 and batch: 250, loss is 5.778046703338623 and perplexity is 323.1274098997712
At time: 831.9617283344269 and batch: 300, loss is 5.776080484390259 and perplexity is 322.49269486254195
At time: 833.6700608730316 and batch: 350, loss is 5.7190400505065915 and perplexity is 304.6123700710413
At time: 835.3787808418274 and batch: 400, loss is 5.736696557998657 and perplexity is 310.0385231544383
At time: 837.0895602703094 and batch: 450, loss is 5.707250471115112 and perplexity is 301.0422050748782
At time: 838.7970924377441 and batch: 500, loss is 5.695028562545776 and perplexity is 297.38528754569154
At time: 840.5041313171387 and batch: 550, loss is 5.718383493423462 and perplexity is 304.4124403016974
At time: 842.2118601799011 and batch: 600, loss is 5.701302213668823 and perplexity is 299.25684369555387
At time: 843.9176371097565 and batch: 650, loss is 5.698983812332154 and perplexity is 298.5638498584443
At time: 845.6255781650543 and batch: 700, loss is 5.68507287979126 and perplexity is 294.4393029437914
At time: 847.3323192596436 and batch: 750, loss is 5.673014421463012 and perplexity is 290.9101397760188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418904504110647 and perplexity of 225.63180834239796
Finished 30 epochs...
Completing Train Step...
At time: 851.7268846035004 and batch: 50, loss is 5.742599306106567 and perplexity is 311.874014352052
At time: 853.4492664337158 and batch: 100, loss is 5.729257173538208 and perplexity is 307.74058560176553
At time: 855.1406879425049 and batch: 150, loss is 5.726333904266357 and perplexity is 306.84229062292724
At time: 856.8476572036743 and batch: 200, loss is 5.745034284591675 and perplexity is 312.63434618721874
At time: 858.5618851184845 and batch: 250, loss is 5.777595129013061 and perplexity is 322.9815267985859
At time: 860.2784080505371 and batch: 300, loss is 5.775855894088745 and perplexity is 322.4202742637533
At time: 861.9996690750122 and batch: 350, loss is 5.7189437294006344 and perplexity is 304.58303088368206
At time: 863.7185447216034 and batch: 400, loss is 5.736761322021485 and perplexity is 310.05860314665273
At time: 865.4270584583282 and batch: 450, loss is 5.707486791610718 and perplexity is 301.1133559248505
At time: 867.1351346969604 and batch: 500, loss is 5.6953298950195315 and perplexity is 297.4749128928818
At time: 868.843513250351 and batch: 550, loss is 5.718668756484985 and perplexity is 304.49929031334466
At time: 870.5515832901001 and batch: 600, loss is 5.701590948104858 and perplexity is 299.3432619269072
At time: 872.2593252658844 and batch: 650, loss is 5.699189281463623 and perplexity is 298.62520181611364
At time: 873.9793789386749 and batch: 700, loss is 5.685225839614868 and perplexity is 294.48434377226374
At time: 875.7032845020294 and batch: 750, loss is 5.672830667495727 and perplexity is 290.8566887947769
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418736302575399 and perplexity of 225.59385991741524
Finished 31 epochs...
Completing Train Step...
At time: 880.0991761684418 and batch: 50, loss is 5.741822366714477 and perplexity is 311.6318012495671
At time: 881.796334028244 and batch: 100, loss is 5.728491773605347 and perplexity is 307.50513109813267
At time: 883.5002021789551 and batch: 150, loss is 5.725647430419922 and perplexity is 306.6317236980469
At time: 885.2097809314728 and batch: 200, loss is 5.744474105834961 and perplexity is 312.45926411107365
At time: 886.9205894470215 and batch: 250, loss is 5.777228994369507 and perplexity is 322.8632937183309
At time: 888.6293976306915 and batch: 300, loss is 5.775703439712524 and perplexity is 322.3711236286693
At time: 890.3380029201508 and batch: 350, loss is 5.718935079574585 and perplexity is 304.5803963048417
At time: 892.048807144165 and batch: 400, loss is 5.736897888183594 and perplexity is 310.1009495515907
At time: 893.7577848434448 and batch: 450, loss is 5.707755556106568 and perplexity is 301.19429538048814
At time: 895.4936563968658 and batch: 500, loss is 5.6956109523773195 and perplexity is 297.5585321562619
At time: 897.2017660140991 and batch: 550, loss is 5.718877420425415 and perplexity is 304.56283496462777
At time: 898.9117753505707 and batch: 600, loss is 5.701791038513184 and perplexity is 299.4031636350944
At time: 900.621107339859 and batch: 650, loss is 5.699307956695557 and perplexity is 298.6606433341741
At time: 902.3305330276489 and batch: 700, loss is 5.685314168930054 and perplexity is 294.5103565215093
At time: 904.0396618843079 and batch: 750, loss is 5.672597665786743 and perplexity is 290.78892658388014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418619200240734 and perplexity of 225.56744389645175
Finished 32 epochs...
Completing Train Step...
At time: 908.462574005127 and batch: 50, loss is 5.741123151779175 and perplexity is 311.4139798006978
At time: 910.1866793632507 and batch: 100, loss is 5.7278235721588135 and perplexity is 307.2997243588993
At time: 911.8755414485931 and batch: 150, loss is 5.725033149719239 and perplexity is 306.44342358844455
At time: 913.5734558105469 and batch: 200, loss is 5.743984880447388 and perplexity is 312.3064384926226
At time: 915.2830045223236 and batch: 250, loss is 5.7769105339050295 and perplexity is 322.7604908940371
At time: 916.997828245163 and batch: 300, loss is 5.775589256286621 and perplexity is 322.3343162907952
At time: 918.7067821025848 and batch: 350, loss is 5.718968152999878 and perplexity is 304.5904699884092
At time: 920.4149854183197 and batch: 400, loss is 5.737059326171875 and perplexity is 310.15101566622786
At time: 922.1219816207886 and batch: 450, loss is 5.708024044036865 and perplexity is 301.2751732703549
At time: 923.8429837226868 and batch: 500, loss is 5.695849018096924 and perplexity is 297.6293790751208
At time: 925.5667154788971 and batch: 550, loss is 5.719013357162476 and perplexity is 304.60423905674764
At time: 927.2752251625061 and batch: 600, loss is 5.701917991638184 and perplexity is 299.4411762152097
At time: 928.9836416244507 and batch: 650, loss is 5.699372425079345 and perplexity is 298.67989812380705
At time: 930.6937804222107 and batch: 700, loss is 5.685355081558227 and perplexity is 294.5224059607043
At time: 932.4063692092896 and batch: 750, loss is 5.672361059188843 and perplexity is 290.7201321441842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418512033861737 and perplexity of 225.5432719455032
Finished 33 epochs...
Completing Train Step...
At time: 936.839467048645 and batch: 50, loss is 5.740506992340088 and perplexity is 311.2221582399814
At time: 938.5545356273651 and batch: 100, loss is 5.727238168716431 and perplexity is 307.11988268751907
At time: 940.2489516735077 and batch: 150, loss is 5.724477796554566 and perplexity is 306.27328651081655
At time: 941.9761154651642 and batch: 200, loss is 5.743542156219482 and perplexity is 312.16820346802615
At time: 943.7023015022278 and batch: 250, loss is 5.77662561416626 and perplexity is 322.66854315877094
At time: 945.4323072433472 and batch: 300, loss is 5.775489177703857 and perplexity is 322.3020591433963
At time: 947.1612331867218 and batch: 350, loss is 5.719008083343506 and perplexity is 304.60263263336935
At time: 948.9024546146393 and batch: 400, loss is 5.737213764190674 and perplexity is 310.19891847352807
At time: 950.6311318874359 and batch: 450, loss is 5.708266983032226 and perplexity is 301.3483736495299
At time: 952.3393752574921 and batch: 500, loss is 5.696028699874878 and perplexity is 297.68286245597506
At time: 954.0486259460449 and batch: 550, loss is 5.719077949523926 and perplexity is 304.6239147993004
At time: 955.7597525119781 and batch: 600, loss is 5.701987209320069 and perplexity is 299.4619035566292
At time: 957.4709079265594 and batch: 650, loss is 5.6994241523742675 and perplexity is 298.69534842658237
At time: 959.1905233860016 and batch: 700, loss is 5.6853658390045165 and perplexity is 294.52557428670895
At time: 960.9029695987701 and batch: 750, loss is 5.6721558761596675 and perplexity is 290.6604874260799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418397770371548 and perplexity of 225.5175020563687
Finished 34 epochs...
Completing Train Step...
At time: 965.31503033638 and batch: 50, loss is 5.739969863891601 and perplexity is 311.05503685183515
At time: 967.0381939411163 and batch: 100, loss is 5.7267103099823 and perplexity is 306.9578095546399
At time: 968.7334299087524 and batch: 150, loss is 5.723971672058106 and perplexity is 306.1183133190746
At time: 970.429990530014 and batch: 200, loss is 5.743128356933593 and perplexity is 312.0390552109249
At time: 972.1270081996918 and batch: 250, loss is 5.776364669799805 and perplexity is 322.584355604813
At time: 973.8356018066406 and batch: 300, loss is 5.7753876686096195 and perplexity is 322.2693442137612
At time: 975.5440828800201 and batch: 350, loss is 5.719035835266113 and perplexity is 304.611086059355
At time: 977.2537455558777 and batch: 400, loss is 5.737346982955932 and perplexity is 310.24024554314104
At time: 978.9640083312988 and batch: 450, loss is 5.708475542068482 and perplexity is 301.41122913022673
At time: 980.6711020469666 and batch: 500, loss is 5.696153478622437 and perplexity is 297.72000926824006
At time: 982.4258275032043 and batch: 550, loss is 5.719093370437622 and perplexity is 304.628612414621
At time: 984.1296949386597 and batch: 600, loss is 5.702020196914673 and perplexity is 299.47178224743936
At time: 985.8282337188721 and batch: 650, loss is 5.699477319717407 and perplexity is 298.71122968684483
At time: 987.5523300170898 and batch: 700, loss is 5.685349636077881 and perplexity is 294.52080214909773
At time: 989.2612881660461 and batch: 750, loss is 5.67198504447937 and perplexity is 290.610837647615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418281377748001 and perplexity of 225.49125501015965
Finished 35 epochs...
Completing Train Step...
At time: 993.6975107192993 and batch: 50, loss is 5.739493761062622 and perplexity is 310.9069779172574
At time: 995.4468815326691 and batch: 100, loss is 5.72621675491333 and perplexity is 306.8063463525645
At time: 997.1315765380859 and batch: 150, loss is 5.723505239486695 and perplexity is 305.97556306131014
At time: 998.8468780517578 and batch: 200, loss is 5.742737293243408 and perplexity is 311.91705192359495
At time: 1000.570086479187 and batch: 250, loss is 5.7761203479766845 and perplexity is 322.50555083419613
At time: 1002.2940075397491 and batch: 300, loss is 5.775281887054444 and perplexity is 322.23525586433595
At time: 1004.0169587135315 and batch: 350, loss is 5.7190443134307865 and perplexity is 304.6136686132516
At time: 1005.7389471530914 and batch: 400, loss is 5.737454481124878 and perplexity is 310.27359759408034
At time: 1007.4614424705505 and batch: 450, loss is 5.708647203445435 and perplexity is 301.4629742380395
At time: 1009.1838638782501 and batch: 500, loss is 5.696233377456665 and perplexity is 297.7437977002285
At time: 1010.905458688736 and batch: 550, loss is 5.719075803756714 and perplexity is 304.62326114799333
At time: 1012.6283650398254 and batch: 600, loss is 5.702029333114624 and perplexity is 299.47451829402024
At time: 1014.34592628479 and batch: 650, loss is 5.699527149200439 and perplexity is 298.72611468384883
At time: 1016.0590682029724 and batch: 700, loss is 5.685291166305542 and perplexity is 294.50358208827856
At time: 1017.7689089775085 and batch: 750, loss is 5.67183108329773 and perplexity is 290.56609830380285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418168533680051 and perplexity of 225.4658110952815
Finished 36 epochs...
Completing Train Step...
At time: 1022.174613237381 and batch: 50, loss is 5.739054946899414 and perplexity is 310.7705774613218
At time: 1023.8655245304108 and batch: 100, loss is 5.725747194290161 and perplexity is 306.6623159915184
At time: 1025.5463325977325 and batch: 150, loss is 5.7230666065216065 and perplexity is 305.84138152321435
At time: 1027.2424352169037 and batch: 200, loss is 5.742370443344116 and perplexity is 311.8026461706619
At time: 1028.9445536136627 and batch: 250, loss is 5.775893573760986 and perplexity is 322.4324231828936
At time: 1030.6521520614624 and batch: 300, loss is 5.775177497863769 and perplexity is 322.2016197424237
At time: 1032.358419418335 and batch: 350, loss is 5.719033737182617 and perplexity is 304.61044696053295
At time: 1034.0641825199127 and batch: 400, loss is 5.73753851890564 and perplexity is 310.299673394312
At time: 1035.770203113556 and batch: 450, loss is 5.708783082962036 and perplexity is 301.50393966437025
At time: 1037.4796731472015 and batch: 500, loss is 5.696282739639282 and perplexity is 297.75849534669464
At time: 1039.1866147518158 and batch: 550, loss is 5.719033861160279 and perplexity is 304.6104847254263
At time: 1040.8980474472046 and batch: 600, loss is 5.702019319534302 and perplexity is 299.47151949689123
At time: 1042.6140995025635 and batch: 650, loss is 5.699566125869751 and perplexity is 298.73775825974803
At time: 1044.324069738388 and batch: 700, loss is 5.685161962509155 and perplexity is 294.4655335654829
At time: 1046.0319130420685 and batch: 750, loss is 5.671676225662232 and perplexity is 290.52110540870024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.418063496434411 and perplexity of 225.44213003121672
Finished 37 epochs...
Completing Train Step...
At time: 1050.4366579055786 and batch: 50, loss is 5.738638935089111 and perplexity is 310.64132011895776
At time: 1052.121733903885 and batch: 100, loss is 5.725295639038086 and perplexity is 306.52387227196556
At time: 1053.7963519096375 and batch: 150, loss is 5.722644987106324 and perplexity is 305.7124600385775
At time: 1055.4887409210205 and batch: 200, loss is 5.742028999328613 and perplexity is 311.6962011966427
At time: 1057.185498714447 and batch: 250, loss is 5.775685205459594 and perplexity is 322.36524548565575
At time: 1058.8828434944153 and batch: 300, loss is 5.775080070495606 and perplexity is 322.1702300157283
At time: 1060.5793290138245 and batch: 350, loss is 5.719010667800903 and perplexity is 304.60341986691384
At time: 1062.2871208190918 and batch: 400, loss is 5.737596578598023 and perplexity is 310.3176898209048
At time: 1063.9953014850616 and batch: 450, loss is 5.70888349533081 and perplexity is 301.53421590917605
At time: 1065.7027914524078 and batch: 500, loss is 5.696316404342651 and perplexity is 297.7685194668447
At time: 1067.4469368457794 and batch: 550, loss is 5.718966474533081 and perplexity is 304.58995874384783
At time: 1069.1609618663788 and batch: 600, loss is 5.701996898651123 and perplexity is 299.46480515620806
At time: 1070.8712105751038 and batch: 650, loss is 5.699597673416138 and perplexity is 298.74718285169484
At time: 1072.579822063446 and batch: 700, loss is 5.685020761489868 and perplexity is 294.4239576673473
At time: 1074.2893323898315 and batch: 750, loss is 5.671519985198975 and perplexity is 290.4757178023879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.417965556299964 and perplexity of 225.42005127990706
Finished 38 epochs...
Completing Train Step...
At time: 1078.701477766037 and batch: 50, loss is 5.738240804672241 and perplexity is 310.51766897695546
At time: 1080.3883712291718 and batch: 100, loss is 5.724858751296997 and perplexity is 306.3899849988028
At time: 1082.0539512634277 and batch: 150, loss is 5.722234439849854 and perplexity is 305.5869763870869
At time: 1083.745625257492 and batch: 200, loss is 5.7417083358764645 and perplexity is 311.59626764012125
At time: 1085.4437820911407 and batch: 250, loss is 5.775483856201172 and perplexity is 322.30034401668655
At time: 1087.143061876297 and batch: 300, loss is 5.774983892440796 and perplexity is 322.1392457997124
At time: 1088.841519832611 and batch: 350, loss is 5.718968181610108 and perplexity is 304.5904787028127
At time: 1090.5388722419739 and batch: 400, loss is 5.737604608535767 and perplexity is 310.3201816626395
At time: 1092.2370519638062 and batch: 450, loss is 5.708917102813721 and perplexity is 301.5443498854719
At time: 1093.94509267807 and batch: 500, loss is 5.69630919456482 and perplexity is 297.76637262971315
At time: 1095.654263973236 and batch: 550, loss is 5.718904638290406 and perplexity is 304.57112462756385
At time: 1097.3674771785736 and batch: 600, loss is 5.7020034503936765 and perplexity is 299.4667671789427
At time: 1099.0825054645538 and batch: 650, loss is 5.699583301544189 and perplexity is 298.74288932629094
At time: 1100.791749238968 and batch: 700, loss is 5.684959754943848 and perplexity is 294.4059964265066
At time: 1102.5017783641815 and batch: 750, loss is 5.671237468719482 and perplexity is 290.393665216366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.417692672374637 and perplexity of 225.35854616372671
Finished 39 epochs...
Completing Train Step...
At time: 1106.8960518836975 and batch: 50, loss is 5.737558603286743 and perplexity is 310.3059056337937
At time: 1108.5824847221375 and batch: 100, loss is 5.723783912658692 and perplexity is 306.0608421239636
At time: 1110.2561583518982 and batch: 150, loss is 5.72125428199768 and perplexity is 305.2875996548545
At time: 1111.9622159004211 and batch: 200, loss is 5.740896873474121 and perplexity is 311.34352154506627
At time: 1113.6725387573242 and batch: 250, loss is 5.7744815635681155 and perplexity is 321.9774665921091
At time: 1115.3888618946075 and batch: 300, loss is 5.774129753112793 and perplexity is 321.8642114763188
At time: 1117.0981404781342 and batch: 350, loss is 5.718629198074341 and perplexity is 304.487245043625
At time: 1118.807014465332 and batch: 400, loss is 5.73686637878418 and perplexity is 310.0911786108516
At time: 1120.5157697200775 and batch: 450, loss is 5.708259611129761 and perplexity is 301.3461521468996
At time: 1122.223584651947 and batch: 500, loss is 5.695822019577026 and perplexity is 297.6213436308807
At time: 1123.9332706928253 and batch: 550, loss is 5.71808277130127 and perplexity is 304.32091050983456
At time: 1125.642514705658 and batch: 600, loss is 5.701411256790161 and perplexity is 299.2894773750789
At time: 1127.35231590271 and batch: 650, loss is 5.698983240127563 and perplexity is 298.5636790188878
At time: 1129.0607914924622 and batch: 700, loss is 5.684905052185059 and perplexity is 294.3898920467789
At time: 1130.7705738544464 and batch: 750, loss is 5.670770826339722 and perplexity is 290.25818683779704
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.41718345464662 and perplexity of 225.2438188099405
Finished 40 epochs...
Completing Train Step...
At time: 1135.1636900901794 and batch: 50, loss is 5.737056035995483 and perplexity is 310.149995216357
At time: 1136.8631110191345 and batch: 100, loss is 5.723062973022461 and perplexity is 305.8402702508349
At time: 1138.5418281555176 and batch: 150, loss is 5.72063946723938 and perplexity is 305.0999620201646
At time: 1140.2384266853333 and batch: 200, loss is 5.740489635467529 and perplexity is 311.2167564435227
At time: 1141.9349875450134 and batch: 250, loss is 5.774147243499756 and perplexity is 321.8698410551586
At time: 1143.6357486248016 and batch: 300, loss is 5.7738711643219 and perplexity is 321.7809917593481
At time: 1145.344164609909 and batch: 350, loss is 5.718407812118531 and perplexity is 304.4198433050235
At time: 1147.052371263504 and batch: 400, loss is 5.736702632904053 and perplexity is 310.04040661485635
At time: 1148.7594757080078 and batch: 450, loss is 5.708088111877442 and perplexity is 301.2944759384593
At time: 1150.465850353241 and batch: 500, loss is 5.695695323944092 and perplexity is 297.5836386949504
At time: 1152.2089986801147 and batch: 550, loss is 5.717994613647461 and perplexity is 304.2940834748806
At time: 1153.9041180610657 and batch: 600, loss is 5.701389675140381 and perplexity is 299.2830182840945
At time: 1155.6095077991486 and batch: 650, loss is 5.699023494720459 and perplexity is 298.5756978201444
At time: 1157.3190517425537 and batch: 700, loss is 5.684849081039428 and perplexity is 294.3734151683783
At time: 1159.0305540561676 and batch: 750, loss is 5.670622749328613 and perplexity is 290.2152094550999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.4170109948446585 and perplexity of 225.20497665500685
Finished 41 epochs...
Completing Train Step...
At time: 1163.4465448856354 and batch: 50, loss is 5.736619739532471 and perplexity is 310.014707385388
At time: 1165.1665124893188 and batch: 100, loss is 5.722565050125122 and perplexity is 305.68802328411954
At time: 1166.8533642292023 and batch: 150, loss is 5.720178604125977 and perplexity is 304.959385097616
At time: 1168.5533225536346 and batch: 200, loss is 5.740158681869507 and perplexity is 311.1137751801625
At time: 1170.2628803253174 and batch: 250, loss is 5.773902425765991 and perplexity is 321.79105125506794
At time: 1171.973294019699 and batch: 300, loss is 5.773710536956787 and perplexity is 321.729309077436
At time: 1173.6830444335938 and batch: 350, loss is 5.718218402862549 and perplexity is 304.362188829315
At time: 1175.391850233078 and batch: 400, loss is 5.736576185226441 and perplexity is 310.0012052039899
At time: 1177.1046726703644 and batch: 450, loss is 5.70793363571167 and perplexity is 301.2479367177408
At time: 1178.810986995697 and batch: 500, loss is 5.695580854415893 and perplexity is 297.5495763858149
At time: 1180.5190780162811 and batch: 550, loss is 5.717979154586792 and perplexity is 304.28937941054323
At time: 1182.2256696224213 and batch: 600, loss is 5.701433954238891 and perplexity is 299.29627055974083
At time: 1183.93208360672 and batch: 650, loss is 5.699088544845581 and perplexity is 298.59512083837404
At time: 1185.6382353305817 and batch: 700, loss is 5.684812126159668 and perplexity is 294.36253683522096
At time: 1187.3540234565735 and batch: 750, loss is 5.670497694015503 and perplexity is 290.1789187704309
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.41687934343205 and perplexity of 225.17533005325498
Finished 42 epochs...
Completing Train Step...
At time: 1191.7734034061432 and batch: 50, loss is 5.73621376991272 and perplexity is 309.88887637602517
At time: 1193.4672420024872 and batch: 100, loss is 5.722106847763062 and perplexity is 305.5479883944058
At time: 1195.1595697402954 and batch: 150, loss is 5.71975191116333 and perplexity is 304.8292888316563
At time: 1196.8592104911804 and batch: 200, loss is 5.7398605632781985 and perplexity is 311.0210402034663
At time: 1198.5557324886322 and batch: 250, loss is 5.7736784362792966 and perplexity is 321.7189815144079
At time: 1200.2655727863312 and batch: 300, loss is 5.773567209243774 and perplexity is 321.6831996558147
At time: 1201.979838848114 and batch: 350, loss is 5.718039922714233 and perplexity is 304.30787106817627
At time: 1203.686616897583 and batch: 400, loss is 5.736457691192627 and perplexity is 309.9644740869503
At time: 1205.399405002594 and batch: 450, loss is 5.707784290313721 and perplexity is 301.2029500841074
At time: 1207.1062681674957 and batch: 500, loss is 5.6954829883575435 and perplexity is 297.5204578064939
At time: 1208.8141868114471 and batch: 550, loss is 5.7179993724823 and perplexity is 304.295531563612
At time: 1210.5225095748901 and batch: 600, loss is 5.701505222320557 and perplexity is 299.3176015908951
At time: 1212.2309184074402 and batch: 650, loss is 5.699164371490479 and perplexity is 298.61776316300507
At time: 1213.9405558109283 and batch: 700, loss is 5.684790296554565 and perplexity is 294.356111087421
At time: 1215.6504774093628 and batch: 750, loss is 5.670382032394409 and perplexity is 290.14535814715407
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.416768983353016 and perplexity of 225.15048105722772
Finished 43 epochs...
Completing Train Step...
At time: 1220.0629935264587 and batch: 50, loss is 5.735832080841065 and perplexity is 309.77061774893355
At time: 1221.773785829544 and batch: 100, loss is 5.721677551269531 and perplexity is 305.41684586591066
At time: 1223.4538016319275 and batch: 150, loss is 5.7193528842926025 and perplexity is 304.70767801903224
At time: 1225.1624748706818 and batch: 200, loss is 5.7395827293396 and perplexity is 310.9346400058889
At time: 1226.8721401691437 and batch: 250, loss is 5.773465414047241 and perplexity is 321.6504555179105
At time: 1228.5813219547272 and batch: 300, loss is 5.773428077697754 and perplexity is 321.63844648827876
At time: 1230.2905435562134 and batch: 350, loss is 5.7178700733184815 and perplexity is 304.2561889493655
At time: 1232.0175549983978 and batch: 400, loss is 5.7363409996032715 and perplexity is 309.92830595012504
At time: 1233.7346794605255 and batch: 450, loss is 5.707639484405518 and perplexity is 301.1593372751394
At time: 1235.4427495002747 and batch: 500, loss is 5.695398473739624 and perplexity is 297.49531404120205
At time: 1237.195811033249 and batch: 550, loss is 5.718037204742432 and perplexity is 304.3070439690878
At time: 1238.903076171875 and batch: 600, loss is 5.701586704254151 and perplexity is 299.34199156148895
At time: 1240.6103520393372 and batch: 650, loss is 5.699244346618652 and perplexity is 298.64164611189705
At time: 1242.3188905715942 and batch: 700, loss is 5.684779396057129 and perplexity is 294.35290247687453
At time: 1244.0283126831055 and batch: 750, loss is 5.670271492004394 and perplexity is 290.11328713870705
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.416668914085211 and perplexity of 225.1279515407172
Finished 44 epochs...
Completing Train Step...
At time: 1248.4379785060883 and batch: 50, loss is 5.73547101020813 and perplexity is 309.658788866196
At time: 1250.143149614334 and batch: 100, loss is 5.721276292800903 and perplexity is 305.29431935408957
At time: 1251.8377516269684 and batch: 150, loss is 5.718980016708374 and perplexity is 304.59408358239125
At time: 1253.5349645614624 and batch: 200, loss is 5.739319543838501 and perplexity is 310.8528172845994
At time: 1255.2439262866974 and batch: 250, loss is 5.773259944915772 and perplexity is 321.58437306736295
At time: 1256.9541821479797 and batch: 300, loss is 5.7732901096344 and perplexity is 321.59407371579954
At time: 1258.6633956432343 and batch: 350, loss is 5.717709007263184 and perplexity is 304.2071875515484
At time: 1260.372522354126 and batch: 400, loss is 5.736226387023926 and perplexity is 309.892786303106
At time: 1262.0822274684906 and batch: 450, loss is 5.707499561309814 and perplexity is 301.1172010763502
At time: 1263.8014442920685 and batch: 500, loss is 5.695324544906616 and perplexity is 297.4733213727657
At time: 1265.5087342262268 and batch: 550, loss is 5.718082332611084 and perplexity is 304.3207770072671
At time: 1267.2140836715698 and batch: 600, loss is 5.701668434143066 and perplexity is 299.36645774900273
At time: 1268.9238634109497 and batch: 650, loss is 5.699324007034302 and perplexity is 298.66543697713917
At time: 1270.6320519447327 and batch: 700, loss is 5.684776954650879 and perplexity is 294.35218384273577
At time: 1272.3407635688782 and batch: 750, loss is 5.670163164138794 and perplexity is 290.0818614876966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.416574877361919 and perplexity of 225.10678224119428
Finished 45 epochs...
Completing Train Step...
At time: 1276.7580969333649 and batch: 50, loss is 5.735128889083862 and perplexity is 309.5528661734382
At time: 1278.4466240406036 and batch: 100, loss is 5.720902690887451 and perplexity is 305.1802821158044
At time: 1280.1341979503632 and batch: 150, loss is 5.718631706237793 and perplexity is 304.4880087483624
At time: 1281.831514120102 and batch: 200, loss is 5.73906867980957 and perplexity is 310.77484527504583
At time: 1283.5278751850128 and batch: 250, loss is 5.773060026168824 and perplexity is 321.5200887484956
At time: 1285.2309741973877 and batch: 300, loss is 5.773153076171875 and perplexity is 321.550007585687
At time: 1286.9404985904694 and batch: 350, loss is 5.717556390762329 and perplexity is 304.1607640576351
At time: 1288.6619329452515 and batch: 400, loss is 5.736115121841431 and perplexity is 309.8583079438401
At time: 1290.3865339756012 and batch: 450, loss is 5.707364902496338 and perplexity is 301.0766557212921
At time: 1292.118947982788 and batch: 500, loss is 5.695256986618042 and perplexity is 297.4532252631144
At time: 1293.8433544635773 and batch: 550, loss is 5.718127756118775 and perplexity is 304.3346006383784
At time: 1295.564373254776 and batch: 600, loss is 5.701743812561035 and perplexity is 299.38902436948814
At time: 1297.2742676734924 and batch: 650, loss is 5.699401388168335 and perplexity is 298.68854894155237
At time: 1298.9843764305115 and batch: 700, loss is 5.684781198501587 and perplexity is 294.35343303211033
At time: 1300.6953418254852 and batch: 750, loss is 5.670056009292603 and perplexity is 290.05077947576996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.416488292605378 and perplexity of 225.08729226903773
Finished 46 epochs...
Completing Train Step...
At time: 1305.1090650558472 and batch: 50, loss is 5.734805917739868 and perplexity is 309.45290561128036
At time: 1306.8130149841309 and batch: 100, loss is 5.720556030273437 and perplexity is 305.07450646700624
At time: 1308.4919641017914 and batch: 150, loss is 5.718305034637451 and perplexity is 304.38855740810834
At time: 1310.1857886314392 and batch: 200, loss is 5.7388284873962405 and perplexity is 310.70020847891203
At time: 1311.8845663070679 and batch: 250, loss is 5.7728645133972165 and perplexity is 321.4572336095084
At time: 1313.5862038135529 and batch: 300, loss is 5.773017330169678 and perplexity is 321.50636142011405
At time: 1315.296331167221 and batch: 350, loss is 5.717412519454956 and perplexity is 304.1170071986122
At time: 1317.004725933075 and batch: 400, loss is 5.736007041931153 and perplexity is 309.8248202954221
At time: 1318.712424993515 and batch: 450, loss is 5.7072345161437985 and perplexity is 301.03740199344855
At time: 1320.425443649292 and batch: 500, loss is 5.695192422866821 and perplexity is 297.4340211870289
At time: 1322.160620212555 and batch: 550, loss is 5.718169164657593 and perplexity is 304.3472029504225
At time: 1323.8696417808533 and batch: 600, loss is 5.701809558868408 and perplexity is 299.4087087393888
At time: 1325.5840516090393 and batch: 650, loss is 5.699473648071289 and perplexity is 298.71013292693146
At time: 1327.2927346229553 and batch: 700, loss is 5.684790410995483 and perplexity is 294.35614477380653
At time: 1329.003114938736 and batch: 750, loss is 5.669950084686279 and perplexity is 290.0200575882708
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.416406320971112 and perplexity of 225.06884225203723
Finished 47 epochs...
Completing Train Step...
At time: 1333.4212419986725 and batch: 50, loss is 5.73450345993042 and perplexity is 309.3593233163833
At time: 1335.1231627464294 and batch: 100, loss is 5.72023452758789 and perplexity is 304.97643995905275
At time: 1336.8128216266632 and batch: 150, loss is 5.7179976654052735 and perplexity is 304.29501210814414
At time: 1338.527666091919 and batch: 200, loss is 5.738597774505616 and perplexity is 310.6285342041036
At time: 1340.2360634803772 and batch: 250, loss is 5.7726726055145265 and perplexity is 321.39554935146805
At time: 1341.9455170631409 and batch: 300, loss is 5.7728835964202885 and perplexity is 321.4633680438456
At time: 1343.6566925048828 and batch: 350, loss is 5.717276220321655 and perplexity is 304.0755591388405
At time: 1345.3792247772217 and batch: 400, loss is 5.73590121269226 and perplexity is 309.79203350543145
At time: 1347.102127790451 and batch: 450, loss is 5.707106323242187 and perplexity is 300.99881360882483
At time: 1348.845718383789 and batch: 500, loss is 5.695127925872803 and perplexity is 297.4148382053725
At time: 1350.5912411212921 and batch: 550, loss is 5.718202676773071 and perplexity is 304.3574024399355
At time: 1352.317310810089 and batch: 600, loss is 5.701862487792969 and perplexity is 299.4245565397463
At time: 1354.0250458717346 and batch: 650, loss is 5.699539499282837 and perplexity is 298.7298039987611
At time: 1355.7491447925568 and batch: 700, loss is 5.684803113937378 and perplexity is 294.3598839865594
At time: 1357.4571526050568 and batch: 750, loss is 5.669846258163452 and perplexity is 289.9899473772876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.4163300270258 and perplexity of 225.0516715171148
Finished 48 epochs...
Completing Train Step...
At time: 1361.8676071166992 and batch: 50, loss is 5.734222621917724 and perplexity is 309.2724556572556
At time: 1363.589756011963 and batch: 100, loss is 5.719936790466309 and perplexity is 304.885650668011
At time: 1365.2877624034882 and batch: 150, loss is 5.717707710266113 and perplexity is 304.20679299597316
At time: 1366.9910748004913 and batch: 200, loss is 5.7383760070800784 and perplexity is 310.55965455168865
At time: 1368.7228844165802 and batch: 250, loss is 5.772484273910522 and perplexity is 321.33502611153716
At time: 1370.445881843567 and batch: 300, loss is 5.77275131225586 and perplexity is 321.4208463433453
At time: 1372.1685280799866 and batch: 350, loss is 5.717146129608154 and perplexity is 304.0360043053079
At time: 1373.8903455734253 and batch: 400, loss is 5.735796022415161 and perplexity is 309.7594481094473
At time: 1375.6005909442902 and batch: 450, loss is 5.706978015899658 and perplexity is 300.960195728478
At time: 1377.3109118938446 and batch: 500, loss is 5.695061330795288 and perplexity is 297.3950325006567
At time: 1379.0222277641296 and batch: 550, loss is 5.718225622177124 and perplexity is 304.3643861236325
At time: 1380.7333130836487 and batch: 600, loss is 5.701901016235351 and perplexity is 299.43609312376265
At time: 1382.4427478313446 and batch: 650, loss is 5.699598169326782 and perplexity is 298.74733100363943
At time: 1384.1535239219666 and batch: 700, loss is 5.684818382263184 and perplexity is 294.3643784034832
At time: 1385.863238811493 and batch: 750, loss is 5.669745178222656 and perplexity is 289.9606366919613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.416256571924964 and perplexity of 225.03514093102564
Finished 49 epochs...
Completing Train Step...
At time: 1390.2762925624847 and batch: 50, loss is 5.733964204788208 and perplexity is 309.1925446826532
At time: 1392.009902715683 and batch: 100, loss is 5.719659252166748 and perplexity is 304.80104496416874
At time: 1393.6930522918701 and batch: 150, loss is 5.717432680130005 and perplexity is 304.12313846457636
At time: 1395.3912885189056 and batch: 200, loss is 5.73816297531128 and perplexity is 310.49350252565375
At time: 1397.1031966209412 and batch: 250, loss is 5.772298450469971 and perplexity is 321.2753200789774
At time: 1398.8139293193817 and batch: 300, loss is 5.772619819641113 and perplexity is 321.3785846544364
At time: 1400.523975610733 and batch: 350, loss is 5.717020177841187 and perplexity is 303.9977128448332
At time: 1402.2328577041626 and batch: 400, loss is 5.735689296722412 and perplexity is 309.72639058183734
At time: 1403.9472501277924 and batch: 450, loss is 5.706847476959228 and perplexity is 300.92091126754787
At time: 1405.66188788414 and batch: 500, loss is 5.694990158081055 and perplexity is 297.3738668422118
At time: 1407.4061770439148 and batch: 550, loss is 5.7182354068756105 and perplexity is 304.3673642519507
At time: 1409.1263227462769 and batch: 600, loss is 5.701923341751098 and perplexity is 299.4427782635993
At time: 1410.8351826667786 and batch: 650, loss is 5.699648714065551 and perplexity is 298.762431491065
At time: 1412.5482082366943 and batch: 700, loss is 5.684834709167481 and perplexity is 294.3691845017521
At time: 1414.2586789131165 and batch: 750, loss is 5.669647808074951 and perplexity is 289.932404556444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.4161834716796875 and perplexity of 225.01869140826716
Finished Training.
Improved accuracyfrom -10000000 to -225.01869140826716
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb44fd67898>
SETTINGS FOR THIS RUN
{'dropout': 0.029713015436989565, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 29.533192484499416, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 5.136763059877755, 'wordvec_source': 'glove', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.123070478439331 and batch: 50, loss is 7.920907316207885 and perplexity is 2754.2689052030355
At time: 3.7767293453216553 and batch: 100, loss is 6.6248277759552 and perplexity is 753.574417743707
At time: 5.4379799365997314 and batch: 150, loss is 6.298635396957398 and perplexity is 543.8292924448672
At time: 7.106765270233154 and batch: 200, loss is 6.204367189407349 and perplexity is 494.90567527330217
At time: 8.77315902709961 and batch: 250, loss is 6.250125131607056 and perplexity is 518.0776485012389
At time: 10.441794633865356 and batch: 300, loss is 6.306910314559937 and perplexity is 548.3481056479837
At time: 12.109914779663086 and batch: 350, loss is 6.260566911697388 and perplexity is 523.515643139706
At time: 13.778418779373169 and batch: 400, loss is 6.321954679489136 and perplexity is 556.6600216351827
At time: 15.445298194885254 and batch: 450, loss is 6.344322576522827 and perplexity is 569.2516346486801
At time: 17.121158599853516 and batch: 500, loss is 6.425120191574097 and perplexity is 617.1549899150065
At time: 18.796465158462524 and batch: 550, loss is 6.473558368682862 and perplexity is 647.7846874493611
At time: 20.461236715316772 and batch: 600, loss is 6.510212373733521 and perplexity is 671.9691110273083
At time: 22.153406381607056 and batch: 650, loss is 6.572803192138672 and perplexity is 715.3723612793699
At time: 23.817811250686646 and batch: 700, loss is 6.664315662384033 and perplexity is 783.9268107708142
At time: 25.48255205154419 and batch: 750, loss is 6.641709280014038 and perplexity is 766.4038729028873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.3647340286609735 and perplexity of 580.990281002356
Finished 1 epochs...
Completing Train Step...
At time: 29.86965298652649 and batch: 50, loss is 6.79023455619812 and perplexity is 889.1220864705748
At time: 31.523786783218384 and batch: 100, loss is 6.709567213058472 and perplexity is 820.2155840304895
At time: 33.17949295043945 and batch: 150, loss is 6.682192459106445 and perplexity is 798.0669244081523
At time: 34.84176421165466 and batch: 200, loss is 6.643216438293457 and perplexity is 767.5598357359725
At time: 36.496713638305664 and batch: 250, loss is 6.705137128829956 and perplexity is 816.5899966659338
At time: 38.15554475784302 and batch: 300, loss is 6.6123448181152344 and perplexity is 744.2260490951437
At time: 39.81434106826782 and batch: 350, loss is 6.382255897521973 and perplexity is 591.2600263344963
At time: 41.47658586502075 and batch: 400, loss is 6.472999944686889 and perplexity is 647.4230499186011
At time: 43.133455991744995 and batch: 450, loss is 6.454135513305664 and perplexity is 635.3242594147472
At time: 44.789634466171265 and batch: 500, loss is 6.384970960617065 and perplexity is 592.8675158418424
At time: 46.48198890686035 and batch: 550, loss is 6.369506988525391 and perplexity is 583.7699526487924
At time: 48.143075704574585 and batch: 600, loss is 6.4207398891448975 and perplexity is 614.4575764697717
At time: 49.79797410964966 and batch: 650, loss is 6.509178628921509 and perplexity is 671.2748253638808
At time: 51.45478892326355 and batch: 700, loss is 6.527181005477905 and perplexity is 683.4687984682422
At time: 53.11207342147827 and batch: 750, loss is 6.529758806228638 and perplexity is 685.2329176470187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.307915975881177 and perplexity of 548.8998355086003
Finished 2 epochs...
Completing Train Step...
At time: 57.475175619125366 and batch: 50, loss is 6.6662959957122805 and perplexity is 785.4807853470581
At time: 59.13210082054138 and batch: 100, loss is 6.673475370407105 and perplexity is 791.1403379355977
At time: 60.78979277610779 and batch: 150, loss is 6.667300729751587 and perplexity is 786.2703812298239
At time: 62.44579482078552 and batch: 200, loss is 6.668311853408813 and perplexity is 787.0657998788004
At time: 64.10326147079468 and batch: 250, loss is 6.728126268386841 and perplexity is 835.5801452973099
At time: 65.76018905639648 and batch: 300, loss is 6.7623752784729 and perplexity is 864.6936466457248
At time: 67.41747069358826 and batch: 350, loss is 6.681887168884277 and perplexity is 797.823319566475
At time: 69.07654309272766 and batch: 400, loss is 6.716297874450683 and perplexity is 825.7547977698948
At time: 70.7353847026825 and batch: 450, loss is 6.710065145492553 and perplexity is 820.6240976703723
At time: 72.42229175567627 and batch: 500, loss is 6.807849359512329 and perplexity is 904.9225496245359
At time: 74.11589002609253 and batch: 550, loss is 6.817270889282226 and perplexity is 913.4885936207278
At time: 75.81264305114746 and batch: 600, loss is 6.698593053817749 and perplexity is 811.2636175940942
At time: 77.511483669281 and batch: 650, loss is 6.9912411403656005 and perplexity is 1087.0698455205297
At time: 79.20205545425415 and batch: 700, loss is 6.9023700714111325 and perplexity is 994.6292666653752
At time: 80.95470309257507 and batch: 750, loss is 6.805023059844971 and perplexity is 902.3685781679732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.403854015261628 and perplexity of 604.1690333135174
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 85.32119822502136 and batch: 50, loss is 6.412086715698242 and perplexity is 609.1635067704523
At time: 87.0103018283844 and batch: 100, loss is 6.264635877609253 and perplexity is 525.6501501190452
At time: 88.69956159591675 and batch: 150, loss is 6.222413139343262 and perplexity is 503.917789826637
At time: 90.38425946235657 and batch: 200, loss is 6.140640745162964 and perplexity is 464.3510062201186
At time: 92.07889795303345 and batch: 250, loss is 6.150671033859253 and perplexity is 469.0320175715187
At time: 93.77318596839905 and batch: 300, loss is 6.117862520217895 and perplexity is 453.89346876890005
At time: 95.46971011161804 and batch: 350, loss is 5.992477750778198 and perplexity is 400.40548685378184
At time: 97.16675472259521 and batch: 400, loss is 5.977534160614014 and perplexity is 394.46647694383887
At time: 98.86221694946289 and batch: 450, loss is 5.934296407699585 and perplexity is 377.77410369200805
At time: 100.55653142929077 and batch: 500, loss is 5.900335121154785 and perplexity is 365.1598201433729
At time: 102.25829577445984 and batch: 550, loss is 5.917934875488282 and perplexity is 371.64343089046224
At time: 103.95386123657227 and batch: 600, loss is 5.912499418258667 and perplexity is 369.6288589369411
At time: 105.65948033332825 and batch: 650, loss is 5.919577865600586 and perplexity is 372.25453925775105
At time: 107.37407159805298 and batch: 700, loss is 5.9021470165252685 and perplexity is 365.82205129658416
At time: 109.08158493041992 and batch: 750, loss is 5.902476491928101 and perplexity is 365.94260052221193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.641195252884266 and perplexity of 281.79933873444344
Finished 4 epochs...
Completing Train Step...
At time: 113.45936465263367 and batch: 50, loss is 6.0281155490875244 and perplexity is 414.9323724863991
At time: 115.17471480369568 and batch: 100, loss is 6.003546810150146 and perplexity is 404.8622193743983
At time: 116.85351824760437 and batch: 150, loss is 5.997031087875366 and perplexity is 402.23282509732553
At time: 118.52397084236145 and batch: 200, loss is 5.957809295654297 and perplexity is 386.76191448180265
At time: 120.19386315345764 and batch: 250, loss is 5.981608409881591 and perplexity is 396.07691012401443
At time: 121.92466592788696 and batch: 300, loss is 5.957642621994019 and perplexity is 386.6974568297055
At time: 123.62277626991272 and batch: 350, loss is 5.861940717697143 and perplexity is 351.40546148161394
At time: 125.31941556930542 and batch: 400, loss is 5.889323787689209 and perplexity is 361.1609802798353
At time: 127.01664066314697 and batch: 450, loss is 5.8752757167816165 and perplexity is 356.1228361780909
At time: 128.71377658843994 and batch: 500, loss is 5.863920564651489 and perplexity is 352.10187968751626
At time: 130.40911722183228 and batch: 550, loss is 5.891994981765747 and perplexity is 362.1270009910342
At time: 132.10299515724182 and batch: 600, loss is 5.870014915466308 and perplexity is 354.2542640959006
At time: 133.79996418952942 and batch: 650, loss is 5.861779899597168 and perplexity is 351.34895366683895
At time: 135.49702382087708 and batch: 700, loss is 5.847487134933472 and perplexity is 346.3629227205783
At time: 137.19454097747803 and batch: 750, loss is 5.839519577026367 and perplexity is 343.61422083870565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.600787140602289 and perplexity of 270.6393548301479
Finished 5 epochs...
Completing Train Step...
At time: 141.61495327949524 and batch: 50, loss is 5.923764867782593 and perplexity is 373.81643737950634
At time: 143.34093928337097 and batch: 100, loss is 5.8923626899719235 and perplexity is 362.2601825454512
At time: 145.0206000804901 and batch: 150, loss is 5.872643909454346 and perplexity is 355.186821733534
At time: 146.70265293121338 and batch: 200, loss is 5.860612773895264 and perplexity is 350.9391244802878
At time: 148.38316226005554 and batch: 250, loss is 5.869650335311889 and perplexity is 354.12513356223826
At time: 150.07132196426392 and batch: 300, loss is 5.8542163276672365 and perplexity is 348.7015252121252
At time: 151.77301478385925 and batch: 350, loss is 5.785480346679687 and perplexity is 325.5383738394173
At time: 153.47000694274902 and batch: 400, loss is 5.814097118377686 and perplexity is 334.9888066821322
At time: 155.17257928848267 and batch: 450, loss is 5.801074905395508 and perplexity is 330.65479157474255
At time: 156.8913218975067 and batch: 500, loss is 5.791049642562866 and perplexity is 327.3564513712325
At time: 158.59883666038513 and batch: 550, loss is 5.816694841384888 and perplexity is 335.86014607160007
At time: 160.3052020072937 and batch: 600, loss is 5.790178098678589 and perplexity is 327.0712701500875
At time: 162.01396918296814 and batch: 650, loss is 5.782294025421143 and perplexity is 324.5027547820789
At time: 163.72155785560608 and batch: 700, loss is 5.767196636199952 and perplexity is 319.6404071237102
At time: 165.46829915046692 and batch: 750, loss is 5.754186887741088 and perplexity is 315.5088990427249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.544327846793241 and perplexity of 255.782595358619
Finished 6 epochs...
Completing Train Step...
At time: 170.06366229057312 and batch: 50, loss is 5.821952533721924 and perplexity is 337.6306456804897
At time: 171.83565139770508 and batch: 100, loss is 5.802386140823364 and perplexity is 331.08864222985915
At time: 173.51869344711304 and batch: 150, loss is 5.79358983039856 and perplexity is 328.18905528440575
At time: 175.2003140449524 and batch: 200, loss is 5.80181396484375 and perplexity is 330.89925544810274
At time: 176.87291407585144 and batch: 250, loss is 5.822618312835694 and perplexity is 337.85550795855033
At time: 178.56308507919312 and batch: 300, loss is 5.823329229354858 and perplexity is 338.0957804169591
At time: 180.26336979866028 and batch: 350, loss is 5.750513410568237 and perplexity is 314.3520105070364
At time: 181.9691400527954 and batch: 400, loss is 5.7825500106811525 and perplexity is 324.58583333712477
At time: 183.66757011413574 and batch: 450, loss is 5.771591882705689 and perplexity is 321.04839747189055
At time: 185.36222386360168 and batch: 500, loss is 5.76566484451294 and perplexity is 319.1511594136962
At time: 187.0564603805542 and batch: 550, loss is 5.789113359451294 and perplexity is 326.7232098682374
At time: 188.75410556793213 and batch: 600, loss is 5.765917510986328 and perplexity is 319.2318083998414
At time: 190.44801235198975 and batch: 650, loss is 5.760199556350708 and perplexity is 317.41166411153245
At time: 192.15983390808105 and batch: 700, loss is 5.749121265411377 and perplexity is 313.91469135455947
At time: 193.86612248420715 and batch: 750, loss is 5.73740330696106 and perplexity is 310.25772000843307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.526575487713481 and perplexity of 251.28191789767007
Finished 7 epochs...
Completing Train Step...
At time: 198.29953145980835 and batch: 50, loss is 5.792897901535034 and perplexity is 327.96205034905165
At time: 199.9815685749054 and batch: 100, loss is 5.780038290023803 and perplexity is 323.7715874016893
At time: 201.641037940979 and batch: 150, loss is 5.773787031173706 and perplexity is 321.7539204502956
At time: 203.3264946937561 and batch: 200, loss is 5.787554426193237 and perplexity is 326.2142669972513
At time: 205.01875591278076 and batch: 250, loss is 5.806338996887207 and perplexity is 332.39997803032645
At time: 206.7547755241394 and batch: 300, loss is 5.8073787498474125 and perplexity is 332.74577163026265
At time: 208.44643259048462 and batch: 350, loss is 5.736803131103516 and perplexity is 310.071566683221
At time: 210.15166211128235 and batch: 400, loss is 5.769691791534424 and perplexity is 320.4389554273875
At time: 211.8531847000122 and batch: 450, loss is 5.761330032348633 and perplexity is 317.7706932779638
At time: 213.54596877098083 and batch: 500, loss is 5.7534765529632566 and perplexity is 315.28486167920516
At time: 215.23777604103088 and batch: 550, loss is 5.772278261184693 and perplexity is 321.26883382536414
At time: 216.93240761756897 and batch: 600, loss is 5.7481969165802 and perplexity is 313.6246587428203
At time: 218.6269190311432 and batch: 650, loss is 5.742257032394409 and perplexity is 311.7672863415723
At time: 220.32043480873108 and batch: 700, loss is 5.72964921951294 and perplexity is 307.8612577125742
At time: 222.01619219779968 and batch: 750, loss is 5.715893850326538 and perplexity is 303.6555046119176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.522363352221112 and perplexity of 250.22571041881702
Finished 8 epochs...
Completing Train Step...
At time: 226.39563488960266 and batch: 50, loss is 5.771450681686401 and perplexity is 321.00306831126403
At time: 228.08942127227783 and batch: 100, loss is 5.75900200843811 and perplexity is 317.03177594828054
At time: 229.76832461357117 and batch: 150, loss is 5.754136171340942 and perplexity is 315.49289797291317
At time: 231.4477767944336 and batch: 200, loss is 5.76708906173706 and perplexity is 319.60602382801096
At time: 233.1427936553955 and batch: 250, loss is 5.787329168319702 and perplexity is 326.1407929407661
At time: 234.8475158214569 and batch: 300, loss is 5.78947283744812 and perplexity is 326.8406807860774
At time: 236.55258536338806 and batch: 350, loss is 5.7171439266204835 and perplexity is 304.0353345184768
At time: 238.25921964645386 and batch: 400, loss is 5.750242805480957 and perplexity is 314.2669567623035
At time: 239.96594262123108 and batch: 450, loss is 5.740645713806153 and perplexity is 311.26533442870704
At time: 241.67075538635254 and batch: 500, loss is 5.733361721038818 and perplexity is 309.0063173041672
At time: 243.37414383888245 and batch: 550, loss is 5.753882236480713 and perplexity is 315.4127934990528
At time: 245.07838702201843 and batch: 600, loss is 5.728482360839844 and perplexity is 307.5022366380651
At time: 246.7841935157776 and batch: 650, loss is 5.722395668029785 and perplexity is 305.63624959110825
At time: 248.4886314868927 and batch: 700, loss is 5.713082494735718 and perplexity is 302.80301989166907
At time: 250.23747897148132 and batch: 750, loss is 5.7023671436309815 and perplexity is 299.57570102501404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.501457036927689 and perplexity of 245.04871730680807
Finished 9 epochs...
Completing Train Step...
At time: 254.61368894577026 and batch: 50, loss is 5.754220123291016 and perplexity is 315.51938532874937
At time: 256.29564690589905 and batch: 100, loss is 5.736889476776123 and perplexity is 310.09834117711694
At time: 257.96360087394714 and batch: 150, loss is 5.735746927261353 and perplexity is 309.7442407950046
At time: 259.6429452896118 and batch: 200, loss is 5.750231285095214 and perplexity is 314.2633363065901
At time: 261.3218631744385 and batch: 250, loss is 5.770275449752807 and perplexity is 320.6260368477063
At time: 263.0004804134369 and batch: 300, loss is 5.769948968887329 and perplexity is 320.5213756675673
At time: 264.69196367263794 and batch: 350, loss is 5.702099857330322 and perplexity is 299.4956392443048
At time: 266.385915517807 and batch: 400, loss is 5.733812370300293 and perplexity is 309.1456021548017
At time: 268.080717086792 and batch: 450, loss is 5.724703531265259 and perplexity is 306.3424308263826
At time: 269.78078293800354 and batch: 500, loss is 5.718550224304199 and perplexity is 304.46319948742035
At time: 271.48737359046936 and batch: 550, loss is 5.738372745513916 and perplexity is 310.5586416424799
At time: 273.19342947006226 and batch: 600, loss is 5.711755495071412 and perplexity is 302.4014668752063
At time: 274.9009072780609 and batch: 650, loss is 5.708106594085693 and perplexity is 301.30004457716893
At time: 276.60630917549133 and batch: 700, loss is 5.699962110519409 and perplexity is 298.8560772509982
At time: 278.312216758728 and batch: 750, loss is 5.68481629371643 and perplexity is 294.36376361035843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.502914783566497 and perplexity of 245.4061967447264
Annealing...
Finished 10 epochs...
Completing Train Step...
At time: 282.710000038147 and batch: 50, loss is 5.728755855560303 and perplexity is 307.5863483778491
At time: 284.39493560791016 and batch: 100, loss is 5.696418199539185 and perplexity is 297.7988324146353
At time: 286.07868909835815 and batch: 150, loss is 5.6816679382324216 and perplexity is 293.43845919882375
At time: 287.78224205970764 and batch: 200, loss is 5.678790893554687 and perplexity is 292.5954369294116
At time: 289.47915387153625 and batch: 250, loss is 5.693255815505982 and perplexity is 296.85856566778256
At time: 291.2032792568207 and batch: 300, loss is 5.68062165260315 and perplexity is 293.1315993154251
At time: 292.8984010219574 and batch: 350, loss is 5.6043852424621585 and perplexity is 271.6148967933177
At time: 294.6001935005188 and batch: 400, loss is 5.626703968048096 and perplexity is 277.74515039633934
At time: 296.29484605789185 and batch: 450, loss is 5.603161725997925 and perplexity is 271.28277471500746
At time: 297.9894516468048 and batch: 500, loss is 5.575158920288086 and perplexity is 263.791474123064
At time: 299.68383049964905 and batch: 550, loss is 5.57319821357727 and perplexity is 263.2747631343822
At time: 301.38648796081543 and batch: 600, loss is 5.541503190994263 and perplexity is 255.06111701106443
At time: 303.09265065193176 and batch: 650, loss is 5.53197639465332 and perplexity is 252.64273967989715
At time: 304.79901218414307 and batch: 700, loss is 5.519498891830445 and perplexity is 249.50997437076796
At time: 306.50710225105286 and batch: 750, loss is 5.525101089477539 and perplexity is 250.9117012716341
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.334188416946766 and perplexity of 207.30443575025
Finished 11 epochs...
Completing Train Step...
At time: 310.8815839290619 and batch: 50, loss is 5.629887351989746 and perplexity is 278.63072866794084
At time: 312.5699071884155 and batch: 100, loss is 5.615836534500122 and perplexity is 274.7431151854665
At time: 314.2460539340973 and batch: 150, loss is 5.613227281570435 and perplexity is 274.0271753475056
At time: 315.9274399280548 and batch: 200, loss is 5.616177997589111 and perplexity is 274.83694583719654
At time: 317.6253938674927 and batch: 250, loss is 5.635668706893921 and perplexity is 280.2462572698902
At time: 319.33173847198486 and batch: 300, loss is 5.629940576553345 and perplexity is 278.64555906154663
At time: 321.04292154312134 and batch: 350, loss is 5.56115719795227 and perplexity is 260.12367680952286
At time: 322.75406861305237 and batch: 400, loss is 5.584741687774658 and perplexity is 266.3314771766407
At time: 324.4677999019623 and batch: 450, loss is 5.565925426483155 and perplexity is 261.36696773886393
At time: 326.1918261051178 and batch: 500, loss is 5.547765560150147 and perplexity is 256.66341573969015
At time: 327.905211687088 and batch: 550, loss is 5.5585246181488035 and perplexity is 259.4397810715812
At time: 329.61922669410706 and batch: 600, loss is 5.535319356918335 and perplexity is 253.48872809099004
At time: 331.32816553115845 and batch: 650, loss is 5.538258152008057 and perplexity is 254.23477522329082
At time: 333.02427983283997 and batch: 700, loss is 5.528283767700195 and perplexity is 251.71154462592972
At time: 334.74752497673035 and batch: 750, loss is 5.5261924266815186 and perplexity is 251.18568002055366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.327672914017079 and perplexity of 205.95813377176398
Finished 12 epochs...
Completing Train Step...
At time: 339.1861720085144 and batch: 50, loss is 5.607494602203369 and perplexity is 272.4607595828804
At time: 340.87735652923584 and batch: 100, loss is 5.590925378799438 and perplexity is 267.9834912498288
At time: 342.5533013343811 and batch: 150, loss is 5.589712600708008 and perplexity is 267.65868374230195
At time: 344.2357578277588 and batch: 200, loss is 5.594873266220093 and perplexity is 269.04355102537926
At time: 345.9186809062958 and batch: 250, loss is 5.618677377700806 and perplexity is 275.52472698880496
At time: 347.608749628067 and batch: 300, loss is 5.615150203704834 and perplexity is 274.55461521878675
At time: 349.30476999282837 and batch: 350, loss is 5.550195093154907 and perplexity is 257.2877460878384
At time: 351.00833463668823 and batch: 400, loss is 5.575048379898071 and perplexity is 263.76231612222995
At time: 352.71809935569763 and batch: 450, loss is 5.55749981880188 and perplexity is 259.17404354045385
At time: 354.4337821006775 and batch: 500, loss is 5.544414405822754 and perplexity is 255.80473661008836
At time: 356.14184379577637 and batch: 550, loss is 5.560175275802612 and perplexity is 259.868380970445
At time: 357.8497905731201 and batch: 600, loss is 5.539367017745971 and perplexity is 254.51684381411818
At time: 359.5664026737213 and batch: 650, loss is 5.54299861907959 and perplexity is 255.44282790838074
At time: 361.28529238700867 and batch: 700, loss is 5.531457548141479 and perplexity is 252.51169087571972
At time: 362.9950032234192 and batch: 750, loss is 5.525794162750244 and perplexity is 251.0856617423071
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.3249628821084665 and perplexity of 205.400736281037
Finished 13 epochs...
Completing Train Step...
At time: 367.3976881504059 and batch: 50, loss is 5.597892684936523 and perplexity is 269.85713381431844
At time: 369.08613681793213 and batch: 100, loss is 5.58059775352478 and perplexity is 265.2301006386138
At time: 370.76425409317017 and batch: 150, loss is 5.5790480041503905 and perplexity is 264.81937879624223
At time: 372.44871401786804 and batch: 200, loss is 5.586403760910034 and perplexity is 266.77450764249
At time: 374.13393998146057 and batch: 250, loss is 5.612096128463745 and perplexity is 273.7173839006043
At time: 375.8626756668091 and batch: 300, loss is 5.610082159042358 and perplexity is 273.16668019567874
At time: 377.5718924999237 and batch: 350, loss is 5.54655873298645 and perplexity is 256.3538541888638
At time: 379.2697215080261 and batch: 400, loss is 5.572520809173584 and perplexity is 263.0964800421489
At time: 380.967267036438 and batch: 450, loss is 5.555928268432617 and perplexity is 258.7670583592439
At time: 382.66437125205994 and batch: 500, loss is 5.54396276473999 and perplexity is 255.689230767425
At time: 384.3639557361603 and batch: 550, loss is 5.561136026382446 and perplexity is 260.1181696412342
At time: 386.0616228580475 and batch: 600, loss is 5.540562906265259 and perplexity is 254.82139965672192
At time: 387.7580485343933 and batch: 650, loss is 5.543828411102295 and perplexity is 255.65488029675893
At time: 389.4535970687866 and batch: 700, loss is 5.53086259841919 and perplexity is 252.36150379666887
At time: 391.14976620674133 and batch: 750, loss is 5.523837785720826 and perplexity is 250.59492371208444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.323619753815407 and perplexity of 205.12504192861115
Finished 14 epochs...
Completing Train Step...
At time: 395.5427556037903 and batch: 50, loss is 5.591909637451172 and perplexity is 268.24738616873736
At time: 397.2456624507904 and batch: 100, loss is 5.574642658233643 and perplexity is 263.6553237423502
At time: 398.92689514160156 and batch: 150, loss is 5.573659162521363 and perplexity is 263.3961473322649
At time: 400.61902952194214 and batch: 200, loss is 5.581275587081909 and perplexity is 265.4099434460464
At time: 402.32225918769836 and batch: 250, loss is 5.607950162887573 and perplexity is 272.5849102698737
At time: 404.02891516685486 and batch: 300, loss is 5.606764650344848 and perplexity is 272.26194891498494
At time: 405.73688888549805 and batch: 350, loss is 5.5442651176452635 and perplexity is 255.76655083757956
At time: 407.4544942378998 and batch: 400, loss is 5.570774211883545 and perplexity is 262.63735751088956
At time: 409.16227865219116 and batch: 450, loss is 5.554951467514038 and perplexity is 258.51441786876
At time: 410.87661123275757 and batch: 500, loss is 5.543441877365113 and perplexity is 255.5560801564728
At time: 412.5948429107666 and batch: 550, loss is 5.560914049148559 and perplexity is 260.0604357374968
At time: 414.30539059638977 and batch: 600, loss is 5.540589418411255 and perplexity is 254.82815560842948
At time: 416.0131607055664 and batch: 650, loss is 5.543375988006591 and perplexity is 255.53924228500944
At time: 417.71899485588074 and batch: 700, loss is 5.530037870407105 and perplexity is 252.1534599968828
At time: 419.43474292755127 and batch: 750, loss is 5.521729059219361 and perplexity is 250.06704432755532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.322630061659702 and perplexity of 204.9221317095642
Finished 15 epochs...
Completing Train Step...
At time: 423.7852416038513 and batch: 50, loss is 5.587768287658691 and perplexity is 267.138777065193
At time: 425.4728081226349 and batch: 100, loss is 5.570533409118652 and perplexity is 262.57412132306683
At time: 427.1440613269806 and batch: 150, loss is 5.569814128875732 and perplexity is 262.3853248522226
At time: 428.82474637031555 and batch: 200, loss is 5.577674102783203 and perplexity is 264.45579291242007
At time: 430.50925374031067 and batch: 250, loss is 5.6048438930511475 and perplexity is 271.73950169858244
At time: 432.20352435112 and batch: 300, loss is 5.604606971740723 and perplexity is 271.6751284457447
At time: 433.8988723754883 and batch: 350, loss is 5.542538528442383 and perplexity is 255.32532808727484
At time: 435.5946433544159 and batch: 400, loss is 5.56921727180481 and perplexity is 262.22876504234074
At time: 437.2938344478607 and batch: 450, loss is 5.553738279342651 and perplexity is 258.2009814020193
At time: 438.99053049087524 and batch: 500, loss is 5.542255802154541 and perplexity is 255.25315110871813
At time: 440.698353767395 and batch: 550, loss is 5.559847602844238 and perplexity is 259.7832430787187
At time: 442.40603947639465 and batch: 600, loss is 5.539267196655273 and perplexity is 254.49143893316017
At time: 444.1131081581116 and batch: 650, loss is 5.542187986373901 and perplexity is 255.23584150395368
At time: 445.8195788860321 and batch: 700, loss is 5.528683710098266 and perplexity is 251.81223487851807
At time: 447.5278925895691 and batch: 750, loss is 5.519538516998291 and perplexity is 249.5198614412689
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.321415036223655 and perplexity of 204.6732973078044
Finished 16 epochs...
Completing Train Step...
At time: 451.9221251010895 and batch: 50, loss is 5.584486713409424 and perplexity is 266.2635781339313
At time: 453.60752391815186 and batch: 100, loss is 5.566790390014648 and perplexity is 261.5931384348961
At time: 455.28177881240845 and batch: 150, loss is 5.566418676376343 and perplexity is 261.4959187677103
At time: 456.96283984184265 and batch: 200, loss is 5.574045734405518 and perplexity is 263.49798856042776
At time: 458.64476442337036 and batch: 250, loss is 5.601811676025391 and perplexity is 270.9167765259671
At time: 460.3509113788605 and batch: 300, loss is 5.60218297958374 and perplexity is 271.0173875665706
At time: 462.0320613384247 and batch: 350, loss is 5.540352268218994 and perplexity is 254.76773022756205
At time: 463.7358920574188 and batch: 400, loss is 5.567219209671021 and perplexity is 261.70533876976435
At time: 465.4430310726166 and batch: 450, loss is 5.551759939193726 and perplexity is 257.6906769783216
At time: 467.1619713306427 and batch: 500, loss is 5.540398273468018 and perplexity is 254.77945115004428
At time: 468.8853015899658 and batch: 550, loss is 5.557676219940186 and perplexity is 259.2197661693972
At time: 470.5929334163666 and batch: 600, loss is 5.537021112442017 and perplexity is 253.9204711906192
At time: 472.30078053474426 and batch: 650, loss is 5.53960841178894 and perplexity is 254.57829008013306
At time: 474.0087921619415 and batch: 700, loss is 5.525862321853638 and perplexity is 251.10277609912958
At time: 475.71706533432007 and batch: 750, loss is 5.5157019233703615 and perplexity is 248.56438918221377
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.318095362463663 and perplexity of 203.99497526025053
Finished 17 epochs...
Completing Train Step...
At time: 480.0931119918823 and batch: 50, loss is 5.579144611358642 and perplexity is 264.84496349293187
At time: 481.7775719165802 and batch: 100, loss is 5.561307439804077 and perplexity is 260.1627612084082
At time: 483.4492778778076 and batch: 150, loss is 5.56007098197937 and perplexity is 259.8412797167252
At time: 485.13958168029785 and batch: 200, loss is 5.567519330978394 and perplexity is 261.78389390562785
At time: 486.8344314098358 and batch: 250, loss is 5.59493257522583 and perplexity is 269.059508204088
At time: 488.53216791152954 and batch: 300, loss is 5.595836172103882 and perplexity is 269.30273941064127
At time: 490.2350344657898 and batch: 350, loss is 5.5347789287567135 and perplexity is 253.35177265429672
At time: 491.94236755371094 and batch: 400, loss is 5.561150608062744 and perplexity is 260.12196262887755
At time: 493.64899611473083 and batch: 450, loss is 5.54554651260376 and perplexity is 256.0944988769836
At time: 495.3563005924225 and batch: 500, loss is 5.532469596862793 and perplexity is 252.76737436984033
At time: 497.0663285255432 and batch: 550, loss is 5.548484907150269 and perplexity is 256.8481122203088
At time: 498.7748210430145 and batch: 600, loss is 5.5291339302062985 and perplexity is 251.92563133487585
At time: 500.4824824333191 and batch: 650, loss is 5.531002721786499 and perplexity is 252.39686801797913
At time: 502.1907331943512 and batch: 700, loss is 5.516274738311767 and perplexity is 248.7068113651217
At time: 503.93421602249146 and batch: 750, loss is 5.506298341751099 and perplexity is 246.23794923822854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.312861952670785 and perplexity of 202.93017465825463
Finished 18 epochs...
Completing Train Step...
At time: 508.32627749443054 and batch: 50, loss is 5.569074745178223 and perplexity is 262.1913931243751
At time: 510.0176260471344 and batch: 100, loss is 5.551188497543335 and perplexity is 257.5434638584389
At time: 511.6967120170593 and batch: 150, loss is 5.549503126144409 and perplexity is 257.1097730381811
At time: 513.391902923584 and batch: 200, loss is 5.557703619003296 and perplexity is 259.22686864542993
At time: 515.0883491039276 and batch: 250, loss is 5.584658689498902 and perplexity is 266.30937304057073
At time: 516.784078836441 and batch: 300, loss is 5.58523962020874 and perplexity is 266.4641252794996
At time: 518.4800159931183 and batch: 350, loss is 5.524109992980957 and perplexity is 250.66314675465347
At time: 520.1815161705017 and batch: 400, loss is 5.550793409347534 and perplexity is 257.4417315738984
At time: 521.8764066696167 and batch: 450, loss is 5.534013833999634 and perplexity is 253.15800867469645
At time: 523.57217669487 and batch: 500, loss is 5.521021118164063 and perplexity is 249.8900742498818
At time: 525.2675969600677 and batch: 550, loss is 5.536815996170044 and perplexity is 253.86839331138384
At time: 526.9730739593506 and batch: 600, loss is 5.51835039138794 and perplexity is 249.22357655028668
At time: 528.6810793876648 and batch: 650, loss is 5.519098148345948 and perplexity is 249.41000490661546
At time: 530.389696598053 and batch: 700, loss is 5.50460823059082 and perplexity is 245.82213122045437
At time: 532.0969684123993 and batch: 750, loss is 5.495933771133423 and perplexity is 243.69897902154003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.304554074309593 and perplexity of 201.25123930351344
Finished 19 epochs...
Completing Train Step...
At time: 536.4780321121216 and batch: 50, loss is 5.559099102020264 and perplexity is 259.58886786129983
At time: 538.1671154499054 and batch: 100, loss is 5.541781921386718 and perplexity is 255.13222020515934
At time: 539.8354444503784 and batch: 150, loss is 5.539845142364502 and perplexity is 254.63856367928972
At time: 541.5200655460358 and batch: 200, loss is 5.54764328956604 and perplexity is 256.632035272422
At time: 543.2108790874481 and batch: 250, loss is 5.574660987854004 and perplexity is 263.6601564886317
At time: 544.9330670833588 and batch: 300, loss is 5.576011610031128 and perplexity is 264.0165023333397
At time: 546.6328296661377 and batch: 350, loss is 5.51572735786438 and perplexity is 248.57071137208413
At time: 548.3288207054138 and batch: 400, loss is 5.54324969291687 and perplexity is 255.50697097137484
At time: 550.0240466594696 and batch: 450, loss is 5.52584924697876 and perplexity is 251.09949298321382
At time: 551.7198133468628 and batch: 500, loss is 5.512829036712646 and perplexity is 247.85131664368396
At time: 553.4160995483398 and batch: 550, loss is 5.529285945892334 and perplexity is 251.96393089354623
At time: 555.1268396377563 and batch: 600, loss is 5.5101378059387205 and perplexity is 247.1851883075618
At time: 556.8393149375916 and batch: 650, loss is 5.512856740951538 and perplexity is 247.8581832708867
At time: 558.55606508255 and batch: 700, loss is 5.497872295379639 and perplexity is 244.17185359247833
At time: 560.2669868469238 and batch: 750, loss is 5.488653984069824 and perplexity is 241.9313441557162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.300073668014171 and perplexity of 200.35156893337287
Finished 20 epochs...
Completing Train Step...
At time: 564.6798875331879 and batch: 50, loss is 5.552743053436279 and perplexity is 257.9441409246252
At time: 566.3677113056183 and batch: 100, loss is 5.5353899002075195 and perplexity is 253.50661065038057
At time: 568.042806148529 and batch: 150, loss is 5.533094425201416 and perplexity is 252.92535994021668
At time: 569.7314250469208 and batch: 200, loss is 5.540483894348145 and perplexity is 254.80126652480254
At time: 571.4264626502991 and batch: 250, loss is 5.567673587799073 and perplexity is 261.8242789715625
At time: 573.1219279766083 and batch: 300, loss is 5.570160112380981 and perplexity is 262.47612155281547
At time: 574.8178901672363 and batch: 350, loss is 5.510525150299072 and perplexity is 247.28095264190526
At time: 576.512913942337 and batch: 400, loss is 5.5376920318603515 and perplexity is 254.09088852721132
At time: 578.2104618549347 and batch: 450, loss is 5.520328626632691 and perplexity is 249.7170873925696
At time: 579.9178178310394 and batch: 500, loss is 5.50791226387024 and perplexity is 246.63567897715552
At time: 581.6253616809845 and batch: 550, loss is 5.5245496368408205 and perplexity is 250.77337349649784
At time: 583.3327214717865 and batch: 600, loss is 5.504012069702148 and perplexity is 245.6756253551233
At time: 585.0398330688477 and batch: 650, loss is 5.507474927902222 and perplexity is 246.52783990641336
At time: 586.7487092018127 and batch: 700, loss is 5.492912311553955 and perplexity is 242.9637636779186
At time: 588.4748075008392 and batch: 750, loss is 5.481947240829467 and perplexity is 240.31420168995894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.2952526003815406 and perplexity of 199.38798508704406
Finished 21 epochs...
Completing Train Step...
At time: 592.8433222770691 and batch: 50, loss is 5.546218967437744 and perplexity is 256.2667687760816
At time: 594.5411427021027 and batch: 100, loss is 5.529365978240967 and perplexity is 251.98409696566466
At time: 596.2291398048401 and batch: 150, loss is 5.5254605102539065 and perplexity is 251.00190035884668
At time: 597.9308295249939 and batch: 200, loss is 5.533670225143433 and perplexity is 253.07103628399517
At time: 599.6273386478424 and batch: 250, loss is 5.562774705886841 and perplexity is 260.54476938921647
At time: 601.3238706588745 and batch: 300, loss is 5.565089855194092 and perplexity is 261.14866821981826
At time: 603.0198707580566 and batch: 350, loss is 5.505707540512085 and perplexity is 246.092514518463
At time: 604.7162611484528 and batch: 400, loss is 5.5318067932128905 and perplexity is 252.59989474071776
At time: 606.4127311706543 and batch: 450, loss is 5.514470481872559 and perplexity is 248.25848506866603
At time: 608.1094162464142 and batch: 500, loss is 5.5022123908996585 and perplexity is 245.23388575380872
At time: 609.8046123981476 and batch: 550, loss is 5.518749084472656 and perplexity is 249.32296007725157
At time: 611.5103635787964 and batch: 600, loss is 5.499463605880737 and perplexity is 244.56071614564607
At time: 613.2230222225189 and batch: 650, loss is 5.50245888710022 and perplexity is 245.29434242575977
At time: 614.9304592609406 and batch: 700, loss is 5.486810541152954 and perplexity is 241.48576835592212
At time: 616.6401851177216 and batch: 750, loss is 5.476393375396729 and perplexity is 238.9832283966805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.291625266851381 and perplexity of 198.66604850718434
Finished 22 epochs...
Completing Train Step...
At time: 621.0609018802643 and batch: 50, loss is 5.541227140426636 and perplexity is 254.99071696236774
At time: 622.7483603954315 and batch: 100, loss is 5.525227022171021 and perplexity is 250.94330124769448
At time: 624.423513174057 and batch: 150, loss is 5.5215968418121335 and perplexity is 250.03398329699144
At time: 626.1132326126099 and batch: 200, loss is 5.528329496383667 and perplexity is 251.72305532666272
At time: 627.8093073368073 and batch: 250, loss is 5.557793016433716 and perplexity is 259.25004389727127
At time: 629.5318083763123 and batch: 300, loss is 5.560541248321533 and perplexity is 259.96350306139055
At time: 631.2301697731018 and batch: 350, loss is 5.50159348487854 and perplexity is 245.08215598341084
At time: 632.9395160675049 and batch: 400, loss is 5.526434783935547 and perplexity is 251.24656406976212
At time: 634.6545345783234 and batch: 450, loss is 5.509451551437378 and perplexity is 247.01561455145222
At time: 636.365709066391 and batch: 500, loss is 5.498893480300904 and perplexity is 244.42132556439273
At time: 638.0754768848419 and batch: 550, loss is 5.517377262115478 and perplexity is 248.98116775923305
At time: 639.7851448059082 and batch: 600, loss is 5.497568016052246 and perplexity is 244.09756844738902
At time: 641.4942309856415 and batch: 650, loss is 5.500430717468261 and perplexity is 244.79734805437064
At time: 643.204262971878 and batch: 700, loss is 5.48319031715393 and perplexity is 240.61311633287852
At time: 644.9142823219299 and batch: 750, loss is 5.470669240951538 and perplexity is 237.61916403563274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.2894507119821945 and perplexity of 198.23450765873952
Finished 23 epochs...
Completing Train Step...
At time: 649.304003238678 and batch: 50, loss is 5.536363496780395 and perplexity is 253.7535440049395
At time: 650.9915351867676 and batch: 100, loss is 5.51994481086731 and perplexity is 249.6212604286695
At time: 652.656252861023 and batch: 150, loss is 5.516013727188111 and perplexity is 248.64190459188958
At time: 654.3392357826233 and batch: 200, loss is 5.523474960327149 and perplexity is 250.5040180026823
At time: 656.0351798534393 and batch: 250, loss is 5.5534712505340575 and perplexity is 258.1320435061895
At time: 657.7318000793457 and batch: 300, loss is 5.556283330917358 and perplexity is 258.8589531471844
At time: 659.428060054779 and batch: 350, loss is 5.497075386047364 and perplexity is 243.97734827551622
At time: 661.1231973171234 and batch: 400, loss is 5.523130559921265 and perplexity is 250.41775917184748
At time: 662.8174266815186 and batch: 450, loss is 5.507084131240845 and perplexity is 246.43151647230553
At time: 664.5160069465637 and batch: 500, loss is 5.495629434585571 and perplexity is 243.62482380019404
At time: 666.2294929027557 and batch: 550, loss is 5.512817630767822 and perplexity is 247.84848968136384
At time: 667.9360339641571 and batch: 600, loss is 5.4928188896179195 and perplexity is 242.94106659294874
At time: 669.6460847854614 and batch: 650, loss is 5.495422105789185 and perplexity is 243.5743185944784
At time: 671.3542132377625 and batch: 700, loss is 5.479322624206543 and perplexity is 239.68429603350506
At time: 673.0984609127045 and batch: 750, loss is 5.467610521316528 and perplexity is 236.89346405512197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.28754478277162 and perplexity of 197.85704654139636
Finished 24 epochs...
Completing Train Step...
At time: 677.4740557670593 and batch: 50, loss is 5.533415946960449 and perplexity is 253.00669402148668
At time: 679.1747426986694 and batch: 100, loss is 5.515898752212524 and perplexity is 248.61331863834553
At time: 680.8586423397064 and batch: 150, loss is 5.512055225372315 and perplexity is 247.65960066971275
At time: 682.5542244911194 and batch: 200, loss is 5.521187858581543 and perplexity is 249.93174449914724
At time: 684.2502443790436 and batch: 250, loss is 5.549986038208008 and perplexity is 257.23396443359843
At time: 685.9501402378082 and batch: 300, loss is 5.55454815864563 and perplexity is 258.41017773332055
At time: 687.6468589305878 and batch: 350, loss is 5.495230236053467 and perplexity is 243.52758853752715
At time: 689.3472676277161 and batch: 400, loss is 5.520192241668701 and perplexity is 249.68303205896842
At time: 691.0545020103455 and batch: 450, loss is 5.504124011993408 and perplexity is 245.70312838687968
At time: 692.7629592418671 and batch: 500, loss is 5.492357873916626 and perplexity is 242.82909275959133
At time: 694.4712100028992 and batch: 550, loss is 5.510253324508667 and perplexity is 247.21374443637677
At time: 696.1792678833008 and batch: 600, loss is 5.489327783584595 and perplexity is 242.0944123094681
At time: 697.885998249054 and batch: 650, loss is 5.491917314529419 and perplexity is 242.7221356854906
At time: 699.5952966213226 and batch: 700, loss is 5.476603450775147 and perplexity is 239.03343816255466
At time: 701.2971861362457 and batch: 750, loss is 5.464140920639038 and perplexity is 236.0729625611215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.285601238871729 and perplexity of 197.47287613249696
Finished 25 epochs...
Completing Train Step...
At time: 705.6594579219818 and batch: 50, loss is 5.528724021911621 and perplexity is 251.8223860909365
At time: 707.3578534126282 and batch: 100, loss is 5.511804580688477 and perplexity is 247.59753388608303
At time: 709.0396921634674 and batch: 150, loss is 5.508718128204346 and perplexity is 246.83451398061953
At time: 710.742173910141 and batch: 200, loss is 5.517922353744507 and perplexity is 249.11692230553643
At time: 712.4384367465973 and batch: 250, loss is 5.5469872093200685 and perplexity is 256.46371928403124
At time: 714.1677687168121 and batch: 300, loss is 5.5519816684722905 and perplexity is 257.74782088122487
At time: 715.8758180141449 and batch: 350, loss is 5.491987323760986 and perplexity is 242.73912907053415
At time: 717.5824055671692 and batch: 400, loss is 5.517620344161987 and perplexity is 249.04169796763944
At time: 719.2896976470947 and batch: 450, loss is 5.501068811416626 and perplexity is 244.9536016076616
At time: 720.9979887008667 and batch: 500, loss is 5.48944993019104 and perplexity is 242.12398512644396
At time: 722.7060132026672 and batch: 550, loss is 5.507109136581421 and perplexity is 246.43767865334695
At time: 724.4134433269501 and batch: 600, loss is 5.486614933013916 and perplexity is 241.4385363937987
At time: 726.1204986572266 and batch: 650, loss is 5.488397808074951 and perplexity is 241.86937509076844
At time: 727.8292124271393 and batch: 700, loss is 5.471784830093384 and perplexity is 237.88439731309984
At time: 729.537709236145 and batch: 750, loss is 5.459493236541748 and perplexity is 234.9783157639335
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.283367068268532 and perplexity of 197.03218051562075
Finished 26 epochs...
Completing Train Step...
At time: 733.9323139190674 and batch: 50, loss is 5.524117565155029 and perplexity is 250.66504482682032
At time: 735.6196057796478 and batch: 100, loss is 5.507911996841431 and perplexity is 246.63561311833274
At time: 737.2806720733643 and batch: 150, loss is 5.505353899002075 and perplexity is 246.0055013766628
At time: 738.9667799472809 and batch: 200, loss is 5.514200887680054 and perplexity is 248.19156504388172
At time: 740.6714379787445 and batch: 250, loss is 5.542518568038941 and perplexity is 255.32023174157996
At time: 742.3729379177094 and batch: 300, loss is 5.547703075408935 and perplexity is 256.64737869361977
At time: 744.0701656341553 and batch: 350, loss is 5.4875510597229 and perplexity is 241.66465927961815
At time: 745.7758545875549 and batch: 400, loss is 5.514410095214844 and perplexity is 248.2434940211375
At time: 747.4852538108826 and batch: 450, loss is 5.498384714126587 and perplexity is 244.29700388967694
At time: 749.2115063667297 and batch: 500, loss is 5.486817207336426 and perplexity is 241.48737814972543
At time: 750.9192335605621 and batch: 550, loss is 5.503502388000488 and perplexity is 245.5504408891616
At time: 752.6276614665985 and batch: 600, loss is 5.481869392395019 and perplexity is 240.29549433376036
At time: 754.3352763652802 and batch: 650, loss is 5.484216375350952 and perplexity is 240.8601260947274
At time: 756.0439717769623 and batch: 700, loss is 5.469005908966064 and perplexity is 237.22425300485438
At time: 757.7775771617889 and batch: 750, loss is 5.456113595962524 and perplexity is 234.1855139599271
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.281806768373001 and perplexity of 196.7249909411984
Finished 27 epochs...
Completing Train Step...
At time: 762.1570003032684 and batch: 50, loss is 5.521037282943726 and perplexity is 249.8941137005204
At time: 763.8425090312958 and batch: 100, loss is 5.506459798812866 and perplexity is 246.27770930369346
At time: 765.510128736496 and batch: 150, loss is 5.502360858917236 and perplexity is 245.27029784561816
At time: 767.191739320755 and batch: 200, loss is 5.511234407424927 and perplexity is 247.45640063116807
At time: 768.8760385513306 and batch: 250, loss is 5.539692478179932 and perplexity is 254.5996924578026
At time: 770.5717301368713 and batch: 300, loss is 5.546145725250244 and perplexity is 256.2479999246972
At time: 772.2695755958557 and batch: 350, loss is 5.48608808517456 and perplexity is 241.31136852456243
At time: 773.9668309688568 and batch: 400, loss is 5.512319927215576 and perplexity is 247.72516529966776
At time: 775.6637103557587 and batch: 450, loss is 5.496083631515503 and perplexity is 243.73550258029752
At time: 777.3621189594269 and batch: 500, loss is 5.484504222869873 and perplexity is 240.92946706376492
At time: 779.0658373832703 and batch: 550, loss is 5.500762796401977 and perplexity is 244.8786535958706
At time: 780.7635161876678 and batch: 600, loss is 5.478925619125366 and perplexity is 239.5891590362867
At time: 782.4640765190125 and batch: 650, loss is 5.482066812515259 and perplexity is 240.34293818217589
At time: 784.174890756607 and batch: 700, loss is 5.467788810729981 and perplexity is 236.9357034171827
At time: 785.8840630054474 and batch: 750, loss is 5.45416259765625 and perplexity is 233.729063830338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.280295083689135 and perplexity of 196.42782944928612
Finished 28 epochs...
Completing Train Step...
At time: 790.3106393814087 and batch: 50, loss is 5.517963008880615 and perplexity is 249.12705039379765
At time: 792.0168468952179 and batch: 100, loss is 5.502915868759155 and perplexity is 245.40646305787394
At time: 793.72456407547 and batch: 150, loss is 5.4993310642242434 and perplexity is 244.52830381125455
At time: 795.4328989982605 and batch: 200, loss is 5.508618726730346 and perplexity is 246.80997948549862
At time: 797.1609728336334 and batch: 250, loss is 5.536640701293945 and perplexity is 253.82389538307618
At time: 798.9113221168518 and batch: 300, loss is 5.543200693130493 and perplexity is 255.4944514911079
At time: 800.6073155403137 and batch: 350, loss is 5.484759931564331 and perplexity is 240.99108270073555
At time: 802.3030924797058 and batch: 400, loss is 5.5103284740448 and perplexity is 247.23232313267846
At time: 803.9982903003693 and batch: 450, loss is 5.493882160186768 and perplexity is 243.19951605552617
At time: 805.7056937217712 and batch: 500, loss is 5.482546195983887 and perplexity is 240.4581822343816
At time: 807.417322397232 and batch: 550, loss is 5.4991863822937015 and perplexity is 244.49292754340206
At time: 809.1262245178223 and batch: 600, loss is 5.476866989135742 and perplexity is 239.0964409444358
At time: 810.8345131874084 and batch: 650, loss is 5.479301271438598 and perplexity is 239.67917816499232
At time: 812.5482861995697 and batch: 700, loss is 5.463804559707642 and perplexity is 235.99357019255092
At time: 814.2573962211609 and batch: 750, loss is 5.450460271835327 and perplexity is 232.8653225945825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.2795562744140625 and perplexity of 196.28276034280597
Finished 29 epochs...
Completing Train Step...
At time: 818.6598365306854 and batch: 50, loss is 5.5158499813079835 and perplexity is 248.6011938375858
At time: 820.3477644920349 and batch: 100, loss is 5.5002193260192875 and perplexity is 244.74560545742443
At time: 822.0192697048187 and batch: 150, loss is 5.496618328094482 and perplexity is 243.86586196795653
At time: 823.709397315979 and batch: 200, loss is 5.505681524276733 and perplexity is 246.08611220096944
At time: 825.3941853046417 and batch: 250, loss is 5.533936414718628 and perplexity is 253.13841012234676
At time: 827.0942995548248 and batch: 300, loss is 5.540142097473145 and perplexity is 254.71419113005484
At time: 828.7903006076813 and batch: 350, loss is 5.481234912872314 and perplexity is 240.14308012016912
At time: 830.4875178337097 and batch: 400, loss is 5.507230110168457 and perplexity is 246.46749290664687
At time: 832.1857242584229 and batch: 450, loss is 5.4912131404876705 and perplexity is 242.5512772222905
At time: 833.8944275379181 and batch: 500, loss is 5.4805375099182125 and perplexity is 239.97566201241327
At time: 835.6224865913391 and batch: 550, loss is 5.497293863296509 and perplexity is 244.03065759864634
At time: 837.3360116481781 and batch: 600, loss is 5.4752780532836915 and perplexity is 238.716833702966
At time: 839.0452337265015 and batch: 650, loss is 5.4772467613220215 and perplexity is 239.18726036679857
At time: 840.7520973682404 and batch: 700, loss is 5.461998538970947 and perplexity is 235.56774555086625
At time: 842.4933335781097 and batch: 750, loss is 5.4487759876251225 and perplexity is 232.47344132099406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.2793188760446945 and perplexity of 196.23616866617837
Finished 30 epochs...
Completing Train Step...
At time: 846.8898911476135 and batch: 50, loss is 5.51349892616272 and perplexity is 248.01740525018207
At time: 848.5939619541168 and batch: 100, loss is 5.498760051727295 and perplexity is 244.38871495120534
At time: 850.2693066596985 and batch: 150, loss is 5.495233631134033 and perplexity is 243.5284153347138
At time: 851.952737569809 and batch: 200, loss is 5.504022789001465 and perplexity is 245.6782588398008
At time: 853.6420292854309 and batch: 250, loss is 5.532326583862305 and perplexity is 252.7312279339729
At time: 855.3385694026947 and batch: 300, loss is 5.538112344741822 and perplexity is 254.1977086480873
At time: 857.0412912368774 and batch: 350, loss is 5.4792953300476075 and perplexity is 239.67775414151285
At time: 858.7578122615814 and batch: 400, loss is 5.505689373016358 and perplexity is 246.0880436743691
At time: 860.4607231616974 and batch: 450, loss is 5.490280342102051 and perplexity is 242.32513127315346
At time: 862.1602952480316 and batch: 500, loss is 5.479194183349609 and perplexity is 239.65351275408656
At time: 863.8575966358185 and batch: 550, loss is 5.496043128967285 and perplexity is 243.72563087126798
At time: 865.5641241073608 and batch: 600, loss is 5.47366060256958 and perplexity is 238.3310330806779
At time: 867.2742993831635 and batch: 650, loss is 5.4761169910430905 and perplexity is 238.9171862984914
At time: 868.9876563549042 and batch: 700, loss is 5.460086011886597 and perplexity is 235.11764640789787
At time: 870.7010595798492 and batch: 750, loss is 5.446921472549438 and perplexity is 232.04271533673077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.277437077012173 and perplexity of 195.86723886857484
Finished 31 epochs...
Completing Train Step...
At time: 875.1659555435181 and batch: 50, loss is 5.511610536575318 and perplexity is 247.54949370330783
At time: 876.8506059646606 and batch: 100, loss is 5.49765341758728 and perplexity is 244.11841564461116
At time: 878.5164890289307 and batch: 150, loss is 5.493541307449341 and perplexity is 243.11663496066421
At time: 880.2040240764618 and batch: 200, loss is 5.50191222190857 and perplexity is 245.16028519259692
At time: 881.9000918865204 and batch: 250, loss is 5.531091394424439 and perplexity is 252.41924970638104
At time: 883.6401708126068 and batch: 300, loss is 5.536420516967773 and perplexity is 253.768013492089
At time: 885.3360419273376 and batch: 350, loss is 5.477993535995483 and perplexity is 239.36594606572842
At time: 887.0382363796234 and batch: 400, loss is 5.504506425857544 and perplexity is 245.79710663778414
At time: 888.7331669330597 and batch: 450, loss is 5.488652353286743 and perplexity is 241.93094961849508
At time: 890.4292135238647 and batch: 500, loss is 5.4783666229248045 and perplexity is 239.45526703273674
At time: 892.1257331371307 and batch: 550, loss is 5.494934797286987 and perplexity is 243.45565167415822
At time: 893.8468027114868 and batch: 600, loss is 5.473133125305176 and perplexity is 238.2053520291886
At time: 895.5585165023804 and batch: 650, loss is 5.475265445709229 and perplexity is 238.71382408168168
At time: 897.2672772407532 and batch: 700, loss is 5.458662261962891 and perplexity is 234.78313586295027
At time: 898.9760038852692 and batch: 750, loss is 5.4461186981201175 and perplexity is 231.85651212793542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.278665586959484 and perplexity of 196.1080115854342
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 903.4235279560089 and batch: 50, loss is 5.511913986206054 and perplexity is 247.62462390430053
At time: 905.1283118724823 and batch: 100, loss is 5.495084924697876 and perplexity is 243.49220378447845
At time: 906.8035929203033 and batch: 150, loss is 5.4888640308380126 and perplexity is 241.98216639001578
At time: 908.4893555641174 and batch: 200, loss is 5.496850070953369 and perplexity is 243.92238268887763
At time: 910.1864695549011 and batch: 250, loss is 5.519776287078858 and perplexity is 249.57919685264
At time: 911.8848876953125 and batch: 300, loss is 5.515941829681396 and perplexity is 248.62402850151557
At time: 913.5819382667542 and batch: 350, loss is 5.458359727859497 and perplexity is 234.71211670085088
At time: 915.2788140773773 and batch: 400, loss is 5.483733730316162 and perplexity is 240.74390420000523
At time: 916.9820046424866 and batch: 450, loss is 5.4594419479370115 and perplexity is 234.96626436302702
At time: 918.6910583972931 and batch: 500, loss is 5.438880853652954 and perplexity is 230.18442919209366
At time: 920.3987159729004 and batch: 550, loss is 5.446747999191285 and perplexity is 232.0024655988957
At time: 922.109046459198 and batch: 600, loss is 5.427185745239258 and perplexity is 227.5080779513879
At time: 923.8187758922577 and batch: 650, loss is 5.4248772144317625 and perplexity is 226.9834743092993
At time: 925.5274667739868 and batch: 700, loss is 5.40901520729065 and perplexity is 223.4114653286513
At time: 927.2735280990601 and batch: 750, loss is 5.411320514678955 and perplexity is 223.92709154040796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.26372474847838 and perplexity of 193.19977331856563
Finished 33 epochs...
Completing Train Step...
At time: 931.6399266719818 and batch: 50, loss is 5.495999898910522 and perplexity is 243.71509482614942
At time: 933.3251750469208 and batch: 100, loss is 5.48001033782959 and perplexity is 239.84918688146018
At time: 935.0033366680145 and batch: 150, loss is 5.475352802276611 and perplexity is 238.73467821279957
At time: 936.6843717098236 and batch: 200, loss is 5.483456001281739 and perplexity is 240.67705191178885
At time: 938.3672325611115 and batch: 250, loss is 5.507756261825562 and perplexity is 246.59720630793015
At time: 940.0538473129272 and batch: 300, loss is 5.506256170272827 and perplexity is 246.22756523885852
At time: 941.7509207725525 and batch: 350, loss is 5.4510619163513185 and perplexity is 233.005466893183
At time: 943.4594151973724 and batch: 400, loss is 5.477692022323608 and perplexity is 239.29378483975307
At time: 945.167204618454 and batch: 450, loss is 5.4553268527984615 and perplexity is 234.0013425649893
At time: 946.8804802894592 and batch: 500, loss is 5.436851167678833 and perplexity is 229.71770090086355
At time: 948.5928146839142 and batch: 550, loss is 5.447845859527588 and perplexity is 232.25731177095625
At time: 950.3115525245667 and batch: 600, loss is 5.4308703327178955 and perplexity is 228.34789761106904
At time: 952.0198428630829 and batch: 650, loss is 5.430473213195801 and perplexity is 228.25723420639383
At time: 953.7274978160858 and batch: 700, loss is 5.414180727005005 and perplexity is 224.56848739467068
At time: 955.4360806941986 and batch: 750, loss is 5.413245058059692 and perplexity is 224.35846390647006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.26126524459484 and perplexity of 192.72518159512322
Finished 34 epochs...
Completing Train Step...
At time: 959.8346717357635 and batch: 50, loss is 5.4906697177886965 and perplexity is 242.4195051597905
At time: 961.523041009903 and batch: 100, loss is 5.474188833236695 and perplexity is 238.45696009760425
At time: 963.1904909610748 and batch: 150, loss is 5.469986925125122 and perplexity is 237.45708801915498
At time: 964.8731391429901 and batch: 200, loss is 5.478276042938233 and perplexity is 239.43357816016808
At time: 966.5573620796204 and batch: 250, loss is 5.503308334350586 and perplexity is 245.50279555289725
At time: 968.2734751701355 and batch: 300, loss is 5.502455978393555 and perplexity is 245.29362893750874
At time: 969.9694240093231 and batch: 350, loss is 5.448584089279175 and perplexity is 232.4288343322683
At time: 971.665590763092 and batch: 400, loss is 5.475856237411499 and perplexity is 238.85489589609168
At time: 973.3618726730347 and batch: 450, loss is 5.4546299648284915 and perplexity is 233.83832665291038
At time: 975.0582900047302 and batch: 500, loss is 5.437387266159058 and perplexity is 229.84088522771205
At time: 976.7531747817993 and batch: 550, loss is 5.449718427658081 and perplexity is 232.6926368718017
At time: 978.4489810466766 and batch: 600, loss is 5.4336675357818605 and perplexity is 228.98752721981052
At time: 980.1450107097626 and batch: 650, loss is 5.433547830581665 and perplexity is 228.96011786257637
At time: 981.840313911438 and batch: 700, loss is 5.4166835594177245 and perplexity is 225.13124863856913
At time: 983.5374641418457 and batch: 750, loss is 5.413860588073731 and perplexity is 224.496605785783
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.260443399118823 and perplexity of 192.56685634487144
Finished 35 epochs...
Completing Train Step...
At time: 987.9336371421814 and batch: 50, loss is 5.487435665130615 and perplexity is 241.6367740937216
At time: 989.6311388015747 and batch: 100, loss is 5.47067551612854 and perplexity is 237.6206551426246
At time: 991.3186225891113 and batch: 150, loss is 5.46683819770813 and perplexity is 236.71057627352346
At time: 993.002201795578 and batch: 200, loss is 5.475307102203369 and perplexity is 238.72376826981431
At time: 994.6874029636383 and batch: 250, loss is 5.500864400863647 and perplexity is 244.90353562368477
At time: 996.382559299469 and batch: 300, loss is 5.500363121032715 and perplexity is 244.78080118547183
At time: 998.0786190032959 and batch: 350, loss is 5.447458915710449 and perplexity is 232.16745862536186
At time: 999.7899947166443 and batch: 400, loss is 5.475235223770142 and perplexity is 238.70660979604637
At time: 1001.5038914680481 and batch: 450, loss is 5.45464243888855 and perplexity is 233.841243584434
At time: 1003.2121179103851 and batch: 500, loss is 5.437956228256225 and perplexity is 229.97169318864323
At time: 1004.9204905033112 and batch: 550, loss is 5.450979051589965 and perplexity is 232.98615975072687
At time: 1006.6314694881439 and batch: 600, loss is 5.435396823883057 and perplexity is 229.3838552098266
At time: 1008.3437011241913 and batch: 650, loss is 5.435304231643677 and perplexity is 229.36261702825573
At time: 1010.0512588024139 and batch: 700, loss is 5.418047723770141 and perplexity is 225.43857423622975
At time: 1011.8017718791962 and batch: 750, loss is 5.413939142227173 and perplexity is 224.51424161927613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.2600285729696585 and perplexity of 192.4869911436294
Finished 36 epochs...
Completing Train Step...
At time: 1016.1856813430786 and batch: 50, loss is 5.485025577545166 and perplexity is 241.05510951712873
At time: 1017.8730142116547 and batch: 100, loss is 5.4682759761810305 and perplexity is 237.0511584265702
At time: 1019.5465753078461 and batch: 150, loss is 5.4647001361846925 and perplexity is 236.20501515118363
At time: 1021.2299482822418 and batch: 200, loss is 5.4733023262023925 and perplexity is 238.2456599984499
At time: 1022.9157891273499 and batch: 250, loss is 5.499179954528809 and perplexity is 244.4913560053966
At time: 1024.6003241539001 and batch: 300, loss is 5.49893027305603 and perplexity is 244.43031866381125
At time: 1026.2945103645325 and batch: 350, loss is 5.446712408065796 and perplexity is 231.9942085169692
At time: 1027.9908611774445 and batch: 400, loss is 5.474943008422851 and perplexity is 238.63686625172764
At time: 1029.6965951919556 and batch: 450, loss is 5.4546755027771 and perplexity is 233.84897541307168
At time: 1031.4039092063904 and batch: 500, loss is 5.438102493286133 and perplexity is 230.00533246529022
At time: 1033.1111998558044 and batch: 550, loss is 5.451566934585571 and perplexity is 233.1231686209024
At time: 1034.8184592723846 and batch: 600, loss is 5.436600856781006 and perplexity is 229.66020725284764
At time: 1036.526795387268 and batch: 650, loss is 5.436536674499512 and perplexity is 229.6454676097948
At time: 1038.2347877025604 and batch: 700, loss is 5.419084453582764 and perplexity is 225.67241432060462
At time: 1039.9431920051575 and batch: 750, loss is 5.413981122970581 and perplexity is 224.52366709188772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.259637522142987 and perplexity of 192.4117336623284
Finished 37 epochs...
Completing Train Step...
At time: 1044.3294622898102 and batch: 50, loss is 5.483091630935669 and perplexity is 240.58937230598696
At time: 1046.0159242153168 and batch: 100, loss is 5.466316633224487 and perplexity is 236.58714863457092
At time: 1047.6775012016296 and batch: 150, loss is 5.462949886322021 and perplexity is 235.79195893708945
At time: 1049.3585422039032 and batch: 200, loss is 5.471694526672363 and perplexity is 237.86291650812464
At time: 1051.0493574142456 and batch: 250, loss is 5.497764358520508 and perplexity is 244.14549987180777
At time: 1052.7916412353516 and batch: 300, loss is 5.497662000656128 and perplexity is 244.12051093877164
At time: 1054.501547574997 and batch: 350, loss is 5.445956993103027 and perplexity is 231.81902279786735
At time: 1056.210355758667 and batch: 400, loss is 5.474459390640259 and perplexity is 238.52148512205812
At time: 1057.9191799163818 and batch: 450, loss is 5.454470119476318 and perplexity is 233.80095167042316
At time: 1059.6284625530243 and batch: 500, loss is 5.438139743804932 and perplexity is 230.013900442831
At time: 1061.3374936580658 and batch: 550, loss is 5.452065782546997 and perplexity is 233.23949064942124
At time: 1063.045767068863 and batch: 600, loss is 5.437624883651734 and perplexity is 229.89550593173465
At time: 1064.7523212432861 and batch: 650, loss is 5.437561130523681 and perplexity is 229.8808498412971
At time: 1066.459951877594 and batch: 700, loss is 5.419662923812866 and perplexity is 225.80299685945857
At time: 1068.1678266525269 and batch: 750, loss is 5.413728895187378 and perplexity is 224.46704312642945
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.259366767351017 and perplexity of 192.35964431544645
Finished 38 epochs...
Completing Train Step...
At time: 1072.5953328609467 and batch: 50, loss is 5.4814215087890625 and perplexity is 240.18789401927015
At time: 1074.30823636055 and batch: 100, loss is 5.464670076370239 and perplexity is 236.19791497897063
At time: 1075.9710659980774 and batch: 150, loss is 5.4616140937805175 and perplexity is 235.47720007007135
At time: 1077.6332516670227 and batch: 200, loss is 5.4704947090148925 and perplexity is 237.57769552164484
At time: 1079.3116471767426 and batch: 250, loss is 5.496713266372681 and perplexity is 243.88901527205329
At time: 1081.0089688301086 and batch: 300, loss is 5.496822738647461 and perplexity is 243.91571581880717
At time: 1082.7036170959473 and batch: 350, loss is 5.4455572128295895 and perplexity is 231.72636464822943
At time: 1084.3993451595306 and batch: 400, loss is 5.474170522689819 and perplexity is 238.4525938602329
At time: 1086.0962252616882 and batch: 450, loss is 5.4543274402618405 and perplexity is 233.76759551396634
At time: 1087.7948610782623 and batch: 500, loss is 5.438185224533081 and perplexity is 230.02436188040258
At time: 1089.4955594539642 and batch: 550, loss is 5.452433404922485 and perplexity is 233.32525046767958
At time: 1091.1917715072632 and batch: 600, loss is 5.438371992111206 and perplexity is 230.06732698550013
At time: 1092.8870873451233 and batch: 650, loss is 5.438319425582886 and perplexity is 230.05523346270078
At time: 1094.582560300827 and batch: 700, loss is 5.420073642730713 and perplexity is 225.89575746994
At time: 1096.3229818344116 and batch: 750, loss is 5.41351728439331 and perplexity is 224.41954850255613
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.259152434593023 and perplexity of 192.31841976039766
Finished 39 epochs...
Completing Train Step...
At time: 1100.7160286903381 and batch: 50, loss is 5.480111541748047 and perplexity is 239.87346178734842
At time: 1102.408756017685 and batch: 100, loss is 5.463405561447144 and perplexity is 235.89942795109965
At time: 1104.0772817134857 and batch: 150, loss is 5.460512237548828 and perplexity is 235.21788094218823
At time: 1105.7427580356598 and batch: 200, loss is 5.469573678970337 and perplexity is 237.35898006336865
At time: 1107.4141550064087 and batch: 250, loss is 5.495942668914795 and perplexity is 243.70114741142288
At time: 1109.1114683151245 and batch: 300, loss is 5.496173782348633 and perplexity is 243.75747652938855
At time: 1110.7944493293762 and batch: 350, loss is 5.445249757766724 and perplexity is 231.6551301554827
At time: 1112.4836626052856 and batch: 400, loss is 5.473850317001343 and perplexity is 238.3762522064151
At time: 1114.179978132248 and batch: 450, loss is 5.454148378372192 and perplexity is 233.72574039401533
At time: 1115.8750250339508 and batch: 500, loss is 5.438152799606323 and perplexity is 230.01690347823583
At time: 1117.5700902938843 and batch: 550, loss is 5.45263334274292 and perplexity is 233.37190567362518
At time: 1119.2645452022552 and batch: 600, loss is 5.438884553909301 and perplexity is 230.18528093506472
At time: 1120.9582192897797 and batch: 650, loss is 5.438892784118653 and perplexity is 230.1871754159124
At time: 1122.6566650867462 and batch: 700, loss is 5.420287475585938 and perplexity is 225.94406656959535
At time: 1124.352903842926 and batch: 750, loss is 5.413185758590698 and perplexity is 224.3451599631584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.258866421011991 and perplexity of 192.26342194589935
Finished 40 epochs...
Completing Train Step...
At time: 1128.7473566532135 and batch: 50, loss is 5.478870267868042 and perplexity is 239.57589784210805
At time: 1130.4333066940308 and batch: 100, loss is 5.462206954956055 and perplexity is 235.61684675117456
At time: 1132.0967321395874 and batch: 150, loss is 5.459540243148804 and perplexity is 234.9893615568998
At time: 1133.7788422107697 and batch: 200, loss is 5.468739652633667 and perplexity is 237.16109895324988
At time: 1135.463196516037 and batch: 250, loss is 5.495251789093017 and perplexity is 243.5328373538383
At time: 1137.1884014606476 and batch: 300, loss is 5.495615739822387 and perplexity is 243.62148743877174
At time: 1138.8703269958496 and batch: 350, loss is 5.444977331161499 and perplexity is 231.59202973029912
At time: 1140.5546889305115 and batch: 400, loss is 5.4735756111145015 and perplexity is 238.31077784015946
At time: 1142.243560552597 and batch: 450, loss is 5.453956289291382 and perplexity is 233.68084854313653
At time: 1143.928136587143 and batch: 500, loss is 5.438099069595337 and perplexity is 230.0045449994985
At time: 1145.6118607521057 and batch: 550, loss is 5.4528274250030515 and perplexity is 233.41720341613149
At time: 1147.3257830142975 and batch: 600, loss is 5.439466676712036 and perplexity is 230.31931604461275
At time: 1149.0245206356049 and batch: 650, loss is 5.439359664916992 and perplexity is 230.2946704798761
At time: 1150.7186620235443 and batch: 700, loss is 5.42056529045105 and perplexity is 226.00684591008545
At time: 1152.413566350937 and batch: 750, loss is 5.412986555099487 and perplexity is 224.30047407499774
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.25876280318859 and perplexity of 192.24350106069497
Finished 41 epochs...
Completing Train Step...
At time: 1156.7962396144867 and batch: 50, loss is 5.477879943847657 and perplexity is 239.33875751803026
At time: 1158.5007016658783 and batch: 100, loss is 5.461258716583252 and perplexity is 235.3935317104847
At time: 1160.165269613266 and batch: 150, loss is 5.458845138549805 and perplexity is 234.82607612776667
At time: 1161.859332561493 and batch: 200, loss is 5.468106527328491 and perplexity is 237.01099378280412
At time: 1163.5595507621765 and batch: 250, loss is 5.4947141456604 and perplexity is 243.40193871475836
At time: 1165.2666082382202 and batch: 300, loss is 5.495196142196655 and perplexity is 243.51928588432943
At time: 1166.9749720096588 and batch: 350, loss is 5.444791383743286 and perplexity is 231.54896979385825
At time: 1168.6828756332397 and batch: 400, loss is 5.473340148925781 and perplexity is 238.2546712685609
At time: 1170.3912088871002 and batch: 450, loss is 5.453727416992187 and perplexity is 233.62737158998277
At time: 1172.09942984581 and batch: 500, loss is 5.438004150390625 and perplexity is 229.98271418710505
At time: 1173.8045434951782 and batch: 550, loss is 5.452820873260498 and perplexity is 233.41567413171683
At time: 1175.503298997879 and batch: 600, loss is 5.439642934799195 and perplexity is 230.35991526455973
At time: 1177.2005269527435 and batch: 650, loss is 5.439604835510254 and perplexity is 230.35113888277573
At time: 1178.8961045742035 and batch: 700, loss is 5.42059103012085 and perplexity is 226.01266332654058
At time: 1180.6307218074799 and batch: 750, loss is 5.412681922912598 and perplexity is 224.23215533763147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.258616247842478 and perplexity of 192.21532881230644
Finished 42 epochs...
Completing Train Step...
At time: 1185.0110507011414 and batch: 50, loss is 5.476960258483887 and perplexity is 239.11874235362913
At time: 1186.7602107524872 and batch: 100, loss is 5.460414791107178 and perplexity is 235.1949609134337
At time: 1188.4286441802979 and batch: 150, loss is 5.458199043273925 and perplexity is 234.67440511156687
At time: 1190.0953459739685 and batch: 200, loss is 5.467557506561279 and perplexity is 236.8809055390013
At time: 1191.783728837967 and batch: 250, loss is 5.494257907867432 and perplexity is 243.29091487999546
At time: 1193.4811804294586 and batch: 300, loss is 5.494790334701538 and perplexity is 243.4204839815441
At time: 1195.1783969402313 and batch: 350, loss is 5.444631767272949 and perplexity is 231.51201371406756
At time: 1196.873564004898 and batch: 400, loss is 5.4731105709075925 and perplexity is 238.19997951155963
At time: 1198.5690970420837 and batch: 450, loss is 5.453489456176758 and perplexity is 233.5717840442238
At time: 1200.2734196186066 and batch: 500, loss is 5.437855176925659 and perplexity is 229.94845541717754
At time: 1201.9718129634857 and batch: 550, loss is 5.452750740051269 and perplexity is 233.39930451543927
At time: 1203.6723911762238 and batch: 600, loss is 5.439756174087524 and perplexity is 230.38600253444795
At time: 1205.3705370426178 and batch: 650, loss is 5.43978816986084 and perplexity is 230.393374030688
At time: 1207.0697002410889 and batch: 700, loss is 5.420570268630981 and perplexity is 226.0079710156306
At time: 1208.7678020000458 and batch: 750, loss is 5.412360277175903 and perplexity is 224.1600436186724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.258493822674419 and perplexity of 192.19179825876842
Finished 43 epochs...
Completing Train Step...
At time: 1213.2352170944214 and batch: 50, loss is 5.476151494979859 and perplexity is 238.92543002419995
At time: 1214.923692703247 and batch: 100, loss is 5.459684658050537 and perplexity is 235.0232999730049
At time: 1216.5877633094788 and batch: 150, loss is 5.457627000808716 and perplexity is 234.5401997755807
At time: 1218.2788169384003 and batch: 200, loss is 5.467063875198364 and perplexity is 236.76400255061603
At time: 1219.9767549037933 and batch: 250, loss is 5.493841848373413 and perplexity is 243.18971243962903
At time: 1221.69917345047 and batch: 300, loss is 5.494426727294922 and perplexity is 243.3319905800495
At time: 1223.3963820934296 and batch: 350, loss is 5.444467277526855 and perplexity is 231.47393549353825
At time: 1225.0924673080444 and batch: 400, loss is 5.472896213531494 and perplexity is 238.1489250611094
At time: 1226.7884256839752 and batch: 450, loss is 5.453239870071411 and perplexity is 233.51349504668985
At time: 1228.4851622581482 and batch: 500, loss is 5.437679061889648 and perplexity is 229.90796160256062
At time: 1230.1815340518951 and batch: 550, loss is 5.452661609649658 and perplexity is 233.3785024687532
At time: 1231.8779544830322 and batch: 600, loss is 5.43981484413147 and perplexity is 230.39951968786326
At time: 1233.573688030243 and batch: 650, loss is 5.439922761917114 and perplexity is 230.42438523553486
At time: 1235.273951292038 and batch: 700, loss is 5.42051176071167 and perplexity is 225.9947481463238
At time: 1236.9801347255707 and batch: 750, loss is 5.412062139511108 and perplexity is 224.09322302808997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.258391269417697 and perplexity of 192.17208937456408
Finished 44 epochs...
Completing Train Step...
At time: 1241.3880338668823 and batch: 50, loss is 5.475425310134888 and perplexity is 238.75198898058892
At time: 1243.0743844509125 and batch: 100, loss is 5.459034862518311 and perplexity is 234.87063248940962
At time: 1244.7519207000732 and batch: 150, loss is 5.45711540222168 and perplexity is 234.42024002900718
At time: 1246.4351201057434 and batch: 200, loss is 5.466625308990478 and perplexity is 236.6601886261571
At time: 1248.11953997612 and batch: 250, loss is 5.493464994430542 and perplexity is 243.0980827042284
At time: 1249.814687013626 and batch: 300, loss is 5.494071178436279 and perplexity is 243.24548954711048
At time: 1251.5172200202942 and batch: 350, loss is 5.444306077957154 and perplexity is 231.43662500203797
At time: 1253.2248055934906 and batch: 400, loss is 5.472667970657349 and perplexity is 238.0945754686683
At time: 1254.9340801239014 and batch: 450, loss is 5.452980079650879 and perplexity is 233.4528383569663
At time: 1256.6431198120117 and batch: 500, loss is 5.437468204498291 and perplexity is 229.85948892011618
At time: 1258.3523943424225 and batch: 550, loss is 5.452514724731445 and perplexity is 233.34422520397376
At time: 1260.0612454414368 and batch: 600, loss is 5.4398274421691895 and perplexity is 230.40242228798638
At time: 1261.7691009044647 and batch: 650, loss is 5.440010137557984 and perplexity is 230.44451959348012
At time: 1263.4759776592255 and batch: 700, loss is 5.420406980514526 and perplexity is 225.97106961260198
At time: 1265.2192134857178 and batch: 750, loss is 5.411780548095703 and perplexity is 224.03012918399418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.258291200149891 and perplexity of 192.15285981644766
Finished 45 epochs...
Completing Train Step...
At time: 1269.596004486084 and batch: 50, loss is 5.474751539230347 and perplexity is 238.59117901763312
At time: 1271.3195309638977 and batch: 100, loss is 5.458445930480957 and perplexity is 234.7323503726843
At time: 1272.9995760917664 and batch: 150, loss is 5.456650772094727 and perplexity is 234.31134662265967
At time: 1274.6855747699738 and batch: 200, loss is 5.4662220287323 and perplexity is 236.5647674862117
At time: 1276.3677899837494 and batch: 250, loss is 5.493101234436035 and perplexity is 243.0096694285806
At time: 1278.0576581954956 and batch: 300, loss is 5.493708848953247 and perplexity is 243.15737049966154
At time: 1279.7606914043427 and batch: 350, loss is 5.444120655059814 and perplexity is 231.39371533082033
At time: 1281.4644916057587 and batch: 400, loss is 5.472404108047486 and perplexity is 238.03175950034804
At time: 1283.1718249320984 and batch: 450, loss is 5.452680711746216 and perplexity is 233.38296053002088
At time: 1284.8700003623962 and batch: 500, loss is 5.437219104766846 and perplexity is 229.8022381140322
At time: 1286.5956091880798 and batch: 550, loss is 5.452324390411377 and perplexity is 233.29981601595804
At time: 1288.3045732975006 and batch: 600, loss is 5.439792737960816 and perplexity is 230.39442649305818
At time: 1290.0137813091278 and batch: 650, loss is 5.4400013732910155 and perplexity is 230.44249992503953
At time: 1291.722787618637 and batch: 700, loss is 5.420233631134034 and perplexity is 225.93190106269537
At time: 1293.43146443367 and batch: 750, loss is 5.411463832855224 and perplexity is 223.959186662637
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.258147483648256 and perplexity of 192.12524626396473
Finished 46 epochs...
Completing Train Step...
At time: 1297.8434009552002 and batch: 50, loss is 5.474057636260986 and perplexity is 238.4256773177508
At time: 1299.5488739013672 and batch: 100, loss is 5.457831039428711 and perplexity is 234.58805991676866
At time: 1301.2097601890564 and batch: 150, loss is 5.456083278656006 and perplexity is 234.1784141935418
At time: 1302.8996217250824 and batch: 200, loss is 5.465624694824219 and perplexity is 236.42350152480955
At time: 1304.595668554306 and batch: 250, loss is 5.49254017829895 and perplexity is 242.87336560280107
At time: 1306.3203926086426 and batch: 300, loss is 5.493285083770752 and perplexity is 243.05435070182017
At time: 1308.0177400112152 and batch: 350, loss is 5.443609771728515 and perplexity is 231.27553033063433
At time: 1309.7166202068329 and batch: 400, loss is 5.471721534729004 and perplexity is 237.86934081000004
At time: 1311.4141924381256 and batch: 450, loss is 5.452063674926758 and perplexity is 233.2389990696682
At time: 1313.1103339195251 and batch: 500, loss is 5.436648426055908 and perplexity is 229.67113228222635
At time: 1314.8080790042877 and batch: 550, loss is 5.451800336837769 and perplexity is 233.17758644387212
At time: 1316.5034096240997 and batch: 600, loss is 5.439676628112793 and perplexity is 230.3676769841836
At time: 1318.1995663642883 and batch: 650, loss is 5.4395205020904545 and perplexity is 230.33171340259915
At time: 1319.9102799892426 and batch: 700, loss is 5.419587841033936 and perplexity is 225.78604357942126
At time: 1321.6183586120605 and batch: 750, loss is 5.410728502273559 and perplexity is 223.79456315740592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.257906181867733 and perplexity of 192.07889169290297
Finished 47 epochs...
Completing Train Step...
At time: 1326.0294823646545 and batch: 50, loss is 5.473282222747803 and perplexity is 238.2408704857859
At time: 1327.7321116924286 and batch: 100, loss is 5.457630710601807 and perplexity is 234.5410698728072
At time: 1329.4142146110535 and batch: 150, loss is 5.455179471969604 and perplexity is 233.96685779442777
At time: 1331.110829114914 and batch: 200, loss is 5.464677677154541 and perplexity is 236.19971027519787
At time: 1332.8071086406708 and batch: 250, loss is 5.492103357315063 and perplexity is 242.76729658855078
At time: 1334.5035662651062 and batch: 300, loss is 5.492937746047974 and perplexity is 242.9699434169017
At time: 1336.204391002655 and batch: 350, loss is 5.443254384994507 and perplexity is 231.19335267854268
At time: 1337.9005620479584 and batch: 400, loss is 5.4714185237884525 and perplexity is 237.7972747162717
At time: 1339.5967540740967 and batch: 450, loss is 5.45168704032898 and perplexity is 233.15116973388896
At time: 1341.2936460971832 and batch: 500, loss is 5.436430578231811 and perplexity is 229.62110437523413
At time: 1343.0018954277039 and batch: 550, loss is 5.451511688232422 and perplexity is 233.11028977175954
At time: 1344.7110087871552 and batch: 600, loss is 5.439622297286987 and perplexity is 230.3551612580519
At time: 1346.4189307689667 and batch: 650, loss is 5.439361095428467 and perplexity is 230.2949999192804
At time: 1348.1263155937195 and batch: 700, loss is 5.419364147186279 and perplexity is 225.73554227921144
At time: 1349.8503754138947 and batch: 750, loss is 5.410510168075562 and perplexity is 223.74570648467804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.257825629655705 and perplexity of 192.06341993644372
Finished 48 epochs...
Completing Train Step...
At time: 1354.224771976471 and batch: 50, loss is 5.472826204299927 and perplexity is 238.1322530214724
At time: 1355.927457332611 and batch: 100, loss is 5.4571866607666015 and perplexity is 234.43694506939318
At time: 1357.5987222194672 and batch: 150, loss is 5.4548336696624755 and perplexity is 233.88596550238753
At time: 1359.2821986675262 and batch: 200, loss is 5.464365844726562 and perplexity is 236.12606702882775
At time: 1360.9657702445984 and batch: 250, loss is 5.491761102676391 and perplexity is 242.6842225722202
At time: 1362.6683433055878 and batch: 300, loss is 5.492594575881958 and perplexity is 242.8865776862163
At time: 1364.379204750061 and batch: 350, loss is 5.44306077003479 and perplexity is 231.1485945199413
At time: 1366.0867607593536 and batch: 400, loss is 5.471180276870728 and perplexity is 237.74062699686738
At time: 1367.796400308609 and batch: 450, loss is 5.451372804641724 and perplexity is 233.07791682577488
At time: 1369.5076777935028 and batch: 500, loss is 5.436154232025147 and perplexity is 229.55765822102566
At time: 1371.215755224228 and batch: 550, loss is 5.451248111724854 and perplexity is 233.04885547237797
At time: 1372.9254245758057 and batch: 600, loss is 5.439484128952026 and perplexity is 230.3233356676663
At time: 1374.6466336250305 and batch: 650, loss is 5.439184246063232 and perplexity is 230.25427599583406
At time: 1376.3548564910889 and batch: 700, loss is 5.4191498184204105 and perplexity is 225.68716584343818
At time: 1378.063798904419 and batch: 750, loss is 5.410304222106934 and perplexity is 223.69963170305027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.257737270621366 and perplexity of 192.04645014785442
Finished 49 epochs...
Completing Train Step...
At time: 1382.4638452529907 and batch: 50, loss is 5.472431726455689 and perplexity is 238.03833364943057
At time: 1384.1510589122772 and batch: 100, loss is 5.456778926849365 and perplexity is 234.34137666000007
At time: 1385.8293192386627 and batch: 150, loss is 5.4544870567321775 and perplexity is 233.80491165049722
At time: 1387.5137591362 and batch: 200, loss is 5.46403242111206 and perplexity is 236.04735014584392
At time: 1389.2108035087585 and batch: 250, loss is 5.491377611160278 and perplexity is 242.59117307475597
At time: 1390.9350695610046 and batch: 300, loss is 5.492231483459473 and perplexity is 242.798403419008
At time: 1392.637391090393 and batch: 350, loss is 5.442874660491944 and perplexity is 231.10557956355674
At time: 1394.3379368782043 and batch: 400, loss is 5.470910558700561 and perplexity is 237.67651267676683
At time: 1396.0403933525085 and batch: 450, loss is 5.4510445976257325 and perplexity is 233.00143157038525
At time: 1397.7528710365295 and batch: 500, loss is 5.435908946990967 and perplexity is 229.5013580680574
At time: 1399.4632182121277 and batch: 550, loss is 5.450998678207397 and perplexity is 232.99073252582508
At time: 1401.1744434833527 and batch: 600, loss is 5.439278030395508 and perplexity is 230.27587125199457
At time: 1402.884961605072 and batch: 650, loss is 5.438996410369873 and perplexity is 230.21103008594332
At time: 1404.5950286388397 and batch: 700, loss is 5.418913116455078 and perplexity is 225.63375156961396
At time: 1406.3052694797516 and batch: 750, loss is 5.410093660354614 and perplexity is 223.6525340752592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.257615555164426 and perplexity of 192.02307654891396
Finished Training.
Improved accuracyfrom -225.01869140826716 to -192.02307654891396
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb44a8f7518>
SETTINGS FOR THIS RUN
{'dropout': 0.8004002862614646, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 26.058052406139705, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 2.8482811379792228, 'wordvec_source': 'glove', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2668297290802 and batch: 50, loss is 7.445952816009521 and perplexity is 1712.9166092249372
At time: 3.9215478897094727 and batch: 100, loss is 6.646654977798462 and perplexity is 770.2036634015888
At time: 5.576448678970337 and batch: 150, loss is 6.589654121398926 and perplexity is 727.5291895975838
At time: 7.233867883682251 and batch: 200, loss is 6.598720407485962 and perplexity is 734.1551684907338
At time: 8.890511751174927 and batch: 250, loss is 6.663997812271118 and perplexity is 783.6776791408494
At time: 10.545056343078613 and batch: 300, loss is 6.645041055679322 and perplexity is 768.9616172254816
At time: 12.201797485351562 and batch: 350, loss is 6.588957834243774 and perplexity is 727.022796685768
At time: 13.868133306503296 and batch: 400, loss is 6.627995100021362 and perplexity is 755.9650160338908
At time: 15.535139322280884 and batch: 450, loss is 6.618061161041259 and perplexity is 748.492482988156
At time: 17.24897027015686 and batch: 500, loss is 6.620863943099976 and perplexity is 750.5932869631504
At time: 18.915979385375977 and batch: 550, loss is 6.623173561096191 and perplexity is 752.3288742268311
At time: 20.581398487091064 and batch: 600, loss is 6.669283723831176 and perplexity is 787.8310976750652
At time: 22.246581077575684 and batch: 650, loss is 6.706186723709107 and perplexity is 817.4475353001538
At time: 23.913306713104248 and batch: 700, loss is 6.705254678726196 and perplexity is 816.685992377345
At time: 25.579266786575317 and batch: 750, loss is 6.665435905456543 and perplexity is 784.8054914260022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.200577846793241 and perplexity of 493.033856828529
Finished 1 epochs...
Completing Train Step...
At time: 30.002537965774536 and batch: 50, loss is 6.6483314514160154 and perplexity is 771.4959724817877
At time: 31.65658450126648 and batch: 100, loss is 6.74758090019226 and perplexity is 851.9952061187156
At time: 33.30972599983215 and batch: 150, loss is 6.967751293182373 and perplexity is 1061.8323142859376
At time: 34.96250605583191 and batch: 200, loss is 7.047729148864746 and perplexity is 1150.2437424605214
At time: 36.61602020263672 and batch: 250, loss is 7.076066274642944 and perplexity is 1183.3045560124722
At time: 38.2693829536438 and batch: 300, loss is 7.1356574821472165 and perplexity is 1255.9624902696696
At time: 39.922234296798706 and batch: 350, loss is 7.090396432876587 and perplexity is 1200.383577812508
At time: 41.608274698257446 and batch: 400, loss is 7.249695787429809 and perplexity is 1407.676550159853
At time: 43.262065410614014 and batch: 450, loss is 7.293679723739624 and perplexity is 1470.9735268544896
At time: 44.91776132583618 and batch: 500, loss is 6.964713745117187 and perplexity is 1058.6118412411536
At time: 46.57257008552551 and batch: 550, loss is 6.980274868011475 and perplexity is 1075.2138682798704
At time: 48.22869825363159 and batch: 600, loss is 7.3096363067626955 and perplexity is 1494.633502232799
At time: 49.8839476108551 and batch: 650, loss is 7.493350076675415 and perplexity is 1796.0589597203868
At time: 51.538440227508545 and batch: 700, loss is 7.229947423934936 and perplexity is 1380.1499393264767
At time: 53.19442582130432 and batch: 750, loss is 7.1787045955657955 and perplexity is 1311.2086121684363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.844379513762718 and perplexity of 938.5907176695923
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 57.56077194213867 and batch: 50, loss is 6.68144287109375 and perplexity is 797.4689271620714
At time: 59.2145311832428 and batch: 100, loss is 6.567875499725342 and perplexity is 711.8558974636275
At time: 60.8685839176178 and batch: 150, loss is 6.618451099395752 and perplexity is 748.7844058275796
At time: 62.528342962265015 and batch: 200, loss is 6.655853891372681 and perplexity is 777.3213878098499
At time: 64.19024920463562 and batch: 250, loss is 6.7108212661743165 and perplexity is 821.2448231646928
At time: 65.84514021873474 and batch: 300, loss is 6.706681909561158 and perplexity is 817.8524239937313
At time: 67.4998848438263 and batch: 350, loss is 6.621473150253296 and perplexity is 751.0506930761939
At time: 69.1546094417572 and batch: 400, loss is 6.648358249664307 and perplexity is 771.5166474994402
At time: 70.80964541435242 and batch: 450, loss is 6.630617246627808 and perplexity is 757.9498682848101
At time: 72.47250771522522 and batch: 500, loss is 6.651851596832276 and perplexity is 774.216536073407
At time: 74.15954494476318 and batch: 550, loss is 6.670298671722412 and perplexity is 788.6311111035955
At time: 75.81696629524231 and batch: 600, loss is 6.658869514465332 and perplexity is 779.6690341665028
At time: 77.48673176765442 and batch: 650, loss is 6.684022645950318 and perplexity is 799.5288734055682
At time: 79.19253516197205 and batch: 700, loss is 6.668393335342407 and perplexity is 787.1299341348964
At time: 80.8975579738617 and batch: 750, loss is 6.647756929397583 and perplexity is 771.0528583600633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.351535530977471 and perplexity of 573.3724645758357
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 85.29440903663635 and batch: 50, loss is 6.675656623840332 and perplexity is 792.8678989531261
At time: 86.9794819355011 and batch: 100, loss is 6.649962549209595 and perplexity is 772.7553846932776
At time: 88.63899087905884 and batch: 150, loss is 6.635676431655884 and perplexity is 761.7941932864798
At time: 90.32142043113708 and batch: 200, loss is 6.616025199890137 and perplexity is 746.970131620869
At time: 92.01790833473206 and batch: 250, loss is 6.645665054321289 and perplexity is 769.4415979684469
At time: 93.71408748626709 and batch: 300, loss is 6.6315882110595705 and perplexity is 758.6861680505688
At time: 95.41027045249939 and batch: 350, loss is 6.542672262191773 and perplexity is 694.139023292203
At time: 97.10461807250977 and batch: 400, loss is 6.566206684112549 and perplexity is 710.6689319166693
At time: 98.80527520179749 and batch: 450, loss is 6.54499249458313 and perplexity is 695.7514570252636
At time: 100.50711393356323 and batch: 500, loss is 6.515865154266358 and perplexity is 675.7783612210912
At time: 102.20470118522644 and batch: 550, loss is 6.511941013336181 and perplexity is 673.1317080104827
At time: 103.90362191200256 and batch: 600, loss is 6.494261569976807 and perplexity is 661.3356948266813
At time: 105.60249638557434 and batch: 650, loss is 6.488158435821533 and perplexity is 657.311766138141
At time: 107.30559587478638 and batch: 700, loss is 6.461185789108276 and perplexity is 639.8192976781337
At time: 108.99981498718262 and batch: 750, loss is 6.439209041595459 and perplexity is 625.9115339816719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.079694171284521 and perplexity of 436.8955590769936
Finished 4 epochs...
Completing Train Step...
At time: 113.40952920913696 and batch: 50, loss is 6.515497961044312 and perplexity is 675.5302655395593
At time: 115.1044054031372 and batch: 100, loss is 6.520337200164795 and perplexity is 678.8072406656635
At time: 116.78081631660461 and batch: 150, loss is 6.520293455123902 and perplexity is 678.7775468646453
At time: 118.4714367389679 and batch: 200, loss is 6.526930637359619 and perplexity is 683.2977010908209
At time: 120.15415382385254 and batch: 250, loss is 6.57140175819397 and perplexity is 714.3705163428921
At time: 121.8489134311676 and batch: 300, loss is 6.548191070556641 and perplexity is 697.9804337946424
At time: 123.54415822029114 and batch: 350, loss is 6.474339599609375 and perplexity is 648.2909546109514
At time: 125.22233462333679 and batch: 400, loss is 6.500561809539795 and perplexity is 665.5154209480943
At time: 126.90892267227173 and batch: 450, loss is 6.485600490570068 and perplexity is 655.6325472178534
At time: 128.60017108917236 and batch: 500, loss is 6.483372192382813 and perplexity is 654.1732289033907
At time: 130.2951009273529 and batch: 550, loss is 6.494467496871948 and perplexity is 661.4718956561888
At time: 131.9958531856537 and batch: 600, loss is 6.485207138061523 and perplexity is 655.3747032258367
At time: 133.69589853286743 and batch: 650, loss is 6.493261003494263 and perplexity is 660.6743154289342
At time: 135.39420461654663 and batch: 700, loss is 6.474481182098389 and perplexity is 648.3827477559079
At time: 137.08939504623413 and batch: 750, loss is 6.4486732673645015 and perplexity is 631.8634226330961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.067849802416425 and perplexity of 431.7513321190307
Finished 5 epochs...
Completing Train Step...
At time: 141.47827649116516 and batch: 50, loss is 6.500907793045044 and perplexity is 665.7457181434493
At time: 143.17718410491943 and batch: 100, loss is 6.487098188400268 and perplexity is 656.6152223527524
At time: 144.87709712982178 and batch: 150, loss is 6.493476982116699 and perplexity is 660.8170223677288
At time: 146.57325983047485 and batch: 200, loss is 6.509791326522827 and perplexity is 671.6862398626746
At time: 148.2581901550293 and batch: 250, loss is 6.556330461502075 and perplexity is 703.6847527662446
At time: 149.9539337158203 and batch: 300, loss is 6.5348944664001465 and perplexity is 688.7610930507008
At time: 151.64912223815918 and batch: 350, loss is 6.4656264686584475 and perplexity is 642.6668479989909
At time: 153.34047603607178 and batch: 400, loss is 6.494725389480591 and perplexity is 661.6425063675784
At time: 155.03282523155212 and batch: 450, loss is 6.475356111526489 and perplexity is 648.9502851439689
At time: 156.73132276535034 and batch: 500, loss is 6.478942489624023 and perplexity is 651.2818446609265
At time: 158.46887350082397 and batch: 550, loss is 6.496828174591064 and perplexity is 663.0352621983399
At time: 160.16679120063782 and batch: 600, loss is 6.488522424697876 and perplexity is 657.5510638575165
At time: 161.86084508895874 and batch: 650, loss is 6.4936565399169925 and perplexity is 660.9356878720012
At time: 163.55397272109985 and batch: 700, loss is 6.468703498840332 and perplexity is 644.6473988318519
At time: 165.24829936027527 and batch: 750, loss is 6.443351240158081 and perplexity is 628.5095608950295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.061879712481831 and perplexity of 429.181416782807
Finished 6 epochs...
Completing Train Step...
At time: 169.64446592330933 and batch: 50, loss is 6.494984312057495 and perplexity is 661.8138427307856
At time: 171.33255100250244 and batch: 100, loss is 6.482936115264892 and perplexity is 653.8880211179239
At time: 173.01839351654053 and batch: 150, loss is 6.491053028106689 and perplexity is 659.2171720615939
At time: 174.72323513031006 and batch: 200, loss is 6.506698789596558 and perplexity is 669.6122339858571
At time: 176.4250259399414 and batch: 250, loss is 6.553236951828003 and perplexity is 701.5112607686498
At time: 178.12907004356384 and batch: 300, loss is 6.5337294006347655 and perplexity is 687.9591083538347
At time: 179.82751727104187 and batch: 350, loss is 6.463200950622559 and perplexity is 641.1099368893834
At time: 181.52778506278992 and batch: 400, loss is 6.490590019226074 and perplexity is 658.9120193063446
At time: 183.23274087905884 and batch: 450, loss is 6.473452968597412 and perplexity is 647.7164144860022
At time: 184.93776941299438 and batch: 500, loss is 6.477136955261231 and perplexity is 650.1069938464501
At time: 186.643639087677 and batch: 550, loss is 6.493678865432739 and perplexity is 660.9504437668238
At time: 188.3501307964325 and batch: 600, loss is 6.484006004333496 and perplexity is 654.5879831378975
At time: 190.0493574142456 and batch: 650, loss is 6.489315996170044 and perplexity is 658.0730847263268
At time: 191.7466378211975 and batch: 700, loss is 6.465025959014892 and perplexity is 642.2810362126173
At time: 193.44389390945435 and batch: 750, loss is 6.43898684501648 and perplexity is 625.7724740299727
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.060392867687137 and perplexity of 428.54376478952923
Finished 7 epochs...
Completing Train Step...
At time: 197.89092326164246 and batch: 50, loss is 6.488477039337158 and perplexity is 657.5212213425049
At time: 199.59755158424377 and batch: 100, loss is 6.4753388500213624 and perplexity is 648.939083381975
At time: 201.26043009757996 and batch: 150, loss is 6.482936162948608 and perplexity is 653.8880522977352
At time: 202.9411370754242 and batch: 200, loss is 6.498560676574707 and perplexity is 664.184967751323
At time: 204.63434553146362 and batch: 250, loss is 6.542702512741089 and perplexity is 694.1600216965644
At time: 206.3437521457672 and batch: 300, loss is 6.522299451828003 and perplexity is 680.1405390082882
At time: 208.0477979183197 and batch: 350, loss is 6.449313297271728 and perplexity is 632.2679635662023
At time: 209.7418622970581 and batch: 400, loss is 6.474432725906372 and perplexity is 648.3513303581719
At time: 211.44177770614624 and batch: 450, loss is 6.451222696304321 and perplexity is 633.4763687008386
At time: 213.13692593574524 and batch: 500, loss is 6.444862976074218 and perplexity is 629.4604199146492
At time: 214.83665919303894 and batch: 550, loss is 6.445382175445556 and perplexity is 629.7873202248252
At time: 216.5373613834381 and batch: 600, loss is 6.4174641990661625 and perplexity is 612.4480968957118
At time: 218.24324345588684 and batch: 650, loss is 6.408790616989136 and perplexity is 607.1589491468861
At time: 219.94074940681458 and batch: 700, loss is 6.377277870178222 and perplexity is 588.3240315503065
At time: 221.63985681533813 and batch: 750, loss is 6.343592472076416 and perplexity is 568.8361731826838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.980683792469113 and perplexity of 395.7108597705772
Finished 8 epochs...
Completing Train Step...
At time: 226.08591032028198 and batch: 50, loss is 6.383062114715576 and perplexity is 591.7369025406856
At time: 227.7802186012268 and batch: 100, loss is 6.365787048339843 and perplexity is 581.6023974302436
At time: 229.46414518356323 and batch: 150, loss is 6.362397623062134 and perplexity is 579.634436574979
At time: 231.15294408798218 and batch: 200, loss is 6.368317165374756 and perplexity is 583.0757826961194
At time: 232.85104775428772 and batch: 250, loss is 6.395924634933472 and perplexity is 599.3972907486378
At time: 234.5553843975067 and batch: 300, loss is 6.37487380027771 and perplexity is 586.9113582174126
At time: 236.26424384117126 and batch: 350, loss is 6.302371892929077 and perplexity is 545.865109447813
At time: 237.97907519340515 and batch: 400, loss is 6.317366123199463 and perplexity is 554.1116070349924
At time: 239.69147634506226 and batch: 450, loss is 6.294505777359009 and perplexity is 541.5881151305299
At time: 241.3866982460022 and batch: 500, loss is 6.294214153289795 and perplexity is 541.4301980278909
At time: 243.13344192504883 and batch: 550, loss is 6.313806514739991 and perplexity is 552.1426930315481
At time: 244.83058094978333 and batch: 600, loss is 6.295400218963623 and perplexity is 542.072750780101
At time: 246.52757024765015 and batch: 650, loss is 6.3000873184204105 and perplexity is 544.6194633610277
At time: 248.22308826446533 and batch: 700, loss is 6.277629880905152 and perplexity is 532.5250192064749
At time: 249.91535329818726 and batch: 750, loss is 6.246835441589355 and perplexity is 516.3761338948692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.899131242619005 and perplexity of 364.7204765849753
Finished 9 epochs...
Completing Train Step...
At time: 254.2945897579193 and batch: 50, loss is 6.29755259513855 and perplexity is 543.2407517918132
At time: 256.0002062320709 and batch: 100, loss is 6.282746696472168 and perplexity is 535.2568346521576
At time: 257.6674907207489 and batch: 150, loss is 6.287757015228271 and perplexity is 537.9453715994355
At time: 259.34653639793396 and batch: 200, loss is 6.304629201889038 and perplexity is 547.0986874097906
At time: 261.0370192527771 and batch: 250, loss is 6.342624320983886 and perplexity is 568.2857203239104
At time: 262.74389719963074 and batch: 300, loss is 6.328034925460815 and perplexity is 560.0549620666994
At time: 264.4556233882904 and batch: 350, loss is 6.259877738952636 and perplexity is 523.1549747226998
At time: 266.1668314933777 and batch: 400, loss is 6.279867067337036 and perplexity is 533.7177105931244
At time: 267.88232827186584 and batch: 450, loss is 6.261842603683472 and perplexity is 524.1839140135199
At time: 269.58995389938354 and batch: 500, loss is 6.262532968521118 and perplexity is 524.5459170989375
At time: 271.29886865615845 and batch: 550, loss is 6.287026615142822 and perplexity is 537.5525997118239
At time: 273.01123094558716 and batch: 600, loss is 6.272239055633545 and perplexity is 529.6619938434415
At time: 274.7155342102051 and batch: 650, loss is 6.274893827438355 and perplexity is 531.0699937029974
At time: 276.4193208217621 and batch: 700, loss is 6.25763463973999 and perplexity is 521.9828013528708
At time: 278.12907552719116 and batch: 750, loss is 6.226748533248902 and perplexity is 506.1072145215323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.889286484829215 and perplexity of 361.1475081936279
Finished 10 epochs...
Completing Train Step...
At time: 282.52512311935425 and batch: 50, loss is 6.278644533157348 and perplexity is 533.0656211316643
At time: 284.21012449264526 and batch: 100, loss is 6.265681667327881 and perplexity is 526.2001571873759
At time: 285.8708646297455 and batch: 150, loss is 6.272325820922852 and perplexity is 529.7079521133347
At time: 287.53329133987427 and batch: 200, loss is 6.294029903411865 and perplexity is 541.3304487696732
At time: 289.211142539978 and batch: 250, loss is 6.329900064468384 and perplexity is 561.1005171725371
At time: 290.89459562301636 and batch: 300, loss is 6.321272516250611 and perplexity is 556.2804181225679
At time: 292.58517026901245 and batch: 350, loss is 6.251632976531982 and perplexity is 518.8594185000928
At time: 294.2819445133209 and batch: 400, loss is 6.272549076080322 and perplexity is 529.8262253476606
At time: 295.97711181640625 and batch: 450, loss is 6.251755867004395 and perplexity is 518.9231852972335
At time: 297.68124985694885 and batch: 500, loss is 6.253829593658447 and perplexity is 520.0004066833056
At time: 299.3902072906494 and batch: 550, loss is 6.276241178512573 and perplexity is 531.7860136863261
At time: 301.0999162197113 and batch: 600, loss is 6.261837606430054 and perplexity is 524.181294540209
At time: 302.8061149120331 and batch: 650, loss is 6.261808080673218 and perplexity is 524.1658179192491
At time: 304.51875734329224 and batch: 700, loss is 6.246343927383423 and perplexity is 516.1223900539152
At time: 306.2207612991333 and batch: 750, loss is 6.214954776763916 and perplexity is 500.1733692208027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.887054798214934 and perplexity of 360.34243879923906
Finished 11 epochs...
Completing Train Step...
At time: 310.6073920726776 and batch: 50, loss is 6.266839447021485 and perplexity is 526.8097338537893
At time: 312.29301261901855 and batch: 100, loss is 6.25023889541626 and perplexity is 518.1365903406563
At time: 313.9537467956543 and batch: 150, loss is 6.258959093093872 and perplexity is 522.6746012520113
At time: 315.62424492836 and batch: 200, loss is 6.2844377040863035 and perplexity is 536.1627237518886
At time: 317.3094046115875 and batch: 250, loss is 6.322704601287842 and perplexity is 557.0776296869943
At time: 318.99458837509155 and batch: 300, loss is 6.311797351837158 and perplexity is 551.0344620966829
At time: 320.67615723609924 and batch: 350, loss is 6.2422732067108155 and perplexity is 514.0256704464061
At time: 322.3696632385254 and batch: 400, loss is 6.264780654907226 and perplexity is 525.7262578366605
At time: 324.0660467147827 and batch: 450, loss is 6.2444752597808835 and perplexity is 515.1588294323241
At time: 325.7603530883789 and batch: 500, loss is 6.248474998474121 and perplexity is 517.2234563672113
At time: 327.5075991153717 and batch: 550, loss is 6.272370634078979 and perplexity is 529.7316905303874
At time: 329.1940038204193 and batch: 600, loss is 6.259278726577759 and perplexity is 522.8416922582663
At time: 330.8877305984497 and batch: 650, loss is 6.259644432067871 and perplexity is 523.0329333024031
At time: 332.5805742740631 and batch: 700, loss is 6.245204133987427 and perplexity is 515.534452289647
At time: 334.27523016929626 and batch: 750, loss is 6.212227840423584 and perplexity is 498.8112862845946
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.885869580645894 and perplexity of 359.9156076037848
Finished 12 epochs...
Completing Train Step...
At time: 338.6621026992798 and batch: 50, loss is 6.263399868011475 and perplexity is 525.0008428460984
At time: 340.3640151023865 and batch: 100, loss is 6.246158533096313 and perplexity is 516.0267127806337
At time: 342.0287437438965 and batch: 150, loss is 6.255496349334717 and perplexity is 520.8678430142047
At time: 343.709570646286 and batch: 200, loss is 6.282340297698974 and perplexity is 535.0393511267235
At time: 345.403751373291 and batch: 250, loss is 6.31736554145813 and perplexity is 554.1112846854611
At time: 347.1102261543274 and batch: 300, loss is 6.3078446865081785 and perplexity is 548.8607061782571
At time: 348.81653666496277 and batch: 350, loss is 6.239649047851563 and perplexity is 512.6785537266021
At time: 350.52567052841187 and batch: 400, loss is 6.261164560317993 and perplexity is 523.828615055995
At time: 352.23231744766235 and batch: 450, loss is 6.242313394546509 and perplexity is 514.0463284406893
At time: 353.94511365890503 and batch: 500, loss is 6.244166584014892 and perplexity is 514.9998369258713
At time: 355.6523587703705 and batch: 550, loss is 6.269609060287475 and perplexity is 528.2708154628606
At time: 357.3467047214508 and batch: 600, loss is 6.257831306457519 and perplexity is 522.0854680922539
At time: 359.04157972335815 and batch: 650, loss is 6.257504215240479 and perplexity is 521.914726446665
At time: 360.7358651161194 and batch: 700, loss is 6.242499628067017 and perplexity is 514.1420700130074
At time: 362.43887543678284 and batch: 750, loss is 6.2071452140808105 and perplexity is 496.2824469176193
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.884769883266715 and perplexity of 359.5200269027734
Finished 13 epochs...
Completing Train Step...
At time: 366.83548378944397 and batch: 50, loss is 6.258812160491943 and perplexity is 522.5978089546713
At time: 368.51922369003296 and batch: 100, loss is 6.241980266571045 and perplexity is 513.8751137477868
At time: 370.17845606803894 and batch: 150, loss is 6.250417623519898 and perplexity is 518.2292041869754
At time: 371.8473057746887 and batch: 200, loss is 6.276613597869873 and perplexity is 531.9840979746863
At time: 373.52893686294556 and batch: 250, loss is 6.313006992340088 and perplexity is 551.7014190082664
At time: 375.2295422554016 and batch: 300, loss is 6.301747512817383 and perplexity is 545.5243885105637
At time: 376.92363142967224 and batch: 350, loss is 6.234139623641968 and perplexity is 509.86175668090647
At time: 378.6179141998291 and batch: 400, loss is 6.2563134288787845 and perplexity is 521.2936073918888
At time: 380.32230162620544 and batch: 450, loss is 6.236642456054687 and perplexity is 511.1394534750325
At time: 382.02920961380005 and batch: 500, loss is 6.2426971817016605 and perplexity is 514.2436506811465
At time: 383.7376379966736 and batch: 550, loss is 6.267260494232178 and perplexity is 527.0315923259624
At time: 385.44361662864685 and batch: 600, loss is 6.253849077224731 and perplexity is 520.0105382443961
At time: 387.1553816795349 and batch: 650, loss is 6.254958162307739 and perplexity is 520.5875941182491
At time: 388.852089881897 and batch: 700, loss is 6.240069656372071 and perplexity is 512.894236050313
At time: 390.55112648010254 and batch: 750, loss is 6.206631507873535 and perplexity is 496.02756901586105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.881162421647892 and perplexity of 358.22540875115925
Finished 14 epochs...
Completing Train Step...
At time: 394.9418444633484 and batch: 50, loss is 6.258512353897094 and perplexity is 522.4411541693355
At time: 396.628050327301 and batch: 100, loss is 6.2413795471191404 and perplexity is 513.566511672024
At time: 398.28883266448975 and batch: 150, loss is 6.251217861175537 and perplexity is 518.6440766865762
At time: 399.970023393631 and batch: 200, loss is 6.278882713317871 and perplexity is 533.1926019084251
At time: 401.6639153957367 and batch: 250, loss is 6.312301597595215 and perplexity is 551.3123889525685
At time: 403.35994243621826 and batch: 300, loss is 6.303956480026245 and perplexity is 546.7307659299515
At time: 405.053542137146 and batch: 350, loss is 6.235227823257446 and perplexity is 510.41689004164704
At time: 406.74737334251404 and batch: 400, loss is 6.257415781021118 and perplexity is 521.8685733660448
At time: 408.4410967826843 and batch: 450, loss is 6.235990543365478 and perplexity is 510.80634377037575
At time: 410.14711236953735 and batch: 500, loss is 6.239403915405274 and perplexity is 512.5528949807143
At time: 411.8934414386749 and batch: 550, loss is 6.263342456817627 and perplexity is 524.9707027861361
At time: 413.58718061447144 and batch: 600, loss is 6.2516632080078125 and perplexity is 518.8751046231687
At time: 415.28257632255554 and batch: 650, loss is 6.253319673538208 and perplexity is 519.7353156067859
At time: 416.98052954673767 and batch: 700, loss is 6.236778440475464 and perplexity is 511.2089652036986
At time: 418.6862201690674 and batch: 750, loss is 6.202482776641846 and perplexity is 493.9739468572726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.884261730105378 and perplexity of 359.33738207424256
Annealing...
Finished 15 epochs...
Completing Train Step...
At time: 423.0861620903015 and batch: 50, loss is 6.247068099975586 and perplexity is 516.4962871096536
At time: 424.7835431098938 and batch: 100, loss is 6.222035436630249 and perplexity is 503.7274946500487
At time: 426.4697778224945 and batch: 150, loss is 6.2218116283416744 and perplexity is 503.61476887651474
At time: 428.1607856750488 and batch: 200, loss is 6.238209381103515 and perplexity is 511.9409985046194
At time: 429.8447411060333 and batch: 250, loss is 6.270535898208618 and perplexity is 528.7606638572322
At time: 431.5257761478424 and batch: 300, loss is 6.25877947807312 and perplexity is 522.5807294733041
At time: 433.2079563140869 and batch: 350, loss is 6.185147819519043 and perplexity is 485.48472242329063
At time: 434.8907103538513 and batch: 400, loss is 6.196939630508423 and perplexity is 491.2433521183645
At time: 436.5882875919342 and batch: 450, loss is 6.174142923355102 and perplexity is 480.17130389798365
At time: 438.2845869064331 and batch: 500, loss is 6.163723449707032 and perplexity is 475.1941463602187
At time: 439.9826455116272 and batch: 550, loss is 6.1874347686767575 and perplexity is 486.5962718443633
At time: 441.6782398223877 and batch: 600, loss is 6.1721917915344235 and perplexity is 479.23533977918703
At time: 443.3750071525574 and batch: 650, loss is 6.17722806930542 and perplexity is 481.6549899688739
At time: 445.07413935661316 and batch: 700, loss is 6.157115497589111 and perplexity is 472.06443806386295
At time: 446.76871824264526 and batch: 750, loss is 6.1368710327148435 and perplexity is 462.60383169350644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.81927490234375 and perplexity of 336.7278045466828
Finished 16 epochs...
Completing Train Step...
At time: 451.1510000228882 and batch: 50, loss is 6.1945022869110105 and perplexity is 490.04748124543534
At time: 452.8371350765228 and batch: 100, loss is 6.181762399673462 and perplexity is 483.8439317590122
At time: 454.51119112968445 and batch: 150, loss is 6.192635583877563 and perplexity is 489.1335613995212
At time: 456.1938920021057 and batch: 200, loss is 6.2146456432342525 and perplexity is 500.01877275843805
At time: 457.8865747451782 and batch: 250, loss is 6.248345775604248 and perplexity is 517.1566235860688
At time: 459.5803475379944 and batch: 300, loss is 6.239748067855835 and perplexity is 512.7293216726622
At time: 461.28779578208923 and batch: 350, loss is 6.168165578842163 and perplexity is 477.3097154593858
At time: 462.996027469635 and batch: 400, loss is 6.182265205383301 and perplexity is 484.08727242198205
At time: 464.70505595207214 and batch: 450, loss is 6.161106538772583 and perplexity is 473.9522313019131
At time: 466.4133858680725 and batch: 500, loss is 6.1600579833984375 and perplexity is 473.455526599395
At time: 468.1229724884033 and batch: 550, loss is 6.188328647613526 and perplexity is 487.0314244603792
At time: 469.831556558609 and batch: 600, loss is 6.175583124160767 and perplexity is 480.86334521632836
At time: 471.5398564338684 and batch: 650, loss is 6.181982316970825 and perplexity is 483.9503491099093
At time: 473.25219345092773 and batch: 700, loss is 6.159532842636108 and perplexity is 473.2069610748843
At time: 474.95768189430237 and batch: 750, loss is 6.136329345703125 and perplexity is 462.3533130637764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.815480786700581 and perplexity of 335.4526409048209
Finished 17 epochs...
Completing Train Step...
At time: 479.4001693725586 and batch: 50, loss is 6.187176189422607 and perplexity is 486.47046440961486
At time: 481.09257435798645 and batch: 100, loss is 6.174284162521363 and perplexity is 480.2391276821829
At time: 482.7571921348572 and batch: 150, loss is 6.186743974685669 and perplexity is 486.2602501379373
At time: 484.4376621246338 and batch: 200, loss is 6.211312990188599 and perplexity is 498.35515733877395
At time: 486.1265778541565 and batch: 250, loss is 6.246314296722412 and perplexity is 516.1070972329046
At time: 487.8257472515106 and batch: 300, loss is 6.239184560775757 and perplexity is 512.4404764605405
At time: 489.5218119621277 and batch: 350, loss is 6.166814594268799 and perplexity is 476.6653127841317
At time: 491.2184808254242 and batch: 400, loss is 6.182190351486206 and perplexity is 484.0510379592695
At time: 492.91952562332153 and batch: 450, loss is 6.162155351638794 and perplexity is 474.4495792669536
At time: 494.6422917842865 and batch: 500, loss is 6.161376152038574 and perplexity is 474.08003233857147
At time: 496.378643989563 and batch: 550, loss is 6.189408378601074 and perplexity is 487.5575713787288
At time: 498.08762884140015 and batch: 600, loss is 6.177049264907837 and perplexity is 481.56887563759
At time: 499.7954523563385 and batch: 650, loss is 6.182354726791382 and perplexity is 484.13061053605895
At time: 501.4951226711273 and batch: 700, loss is 6.159703006744385 and perplexity is 473.28749076688337
At time: 503.19128346443176 and batch: 750, loss is 6.135200309753418 and perplexity is 461.8315941269995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.8148853390715844 and perplexity of 335.25295588204017
Finished 18 epochs...
Completing Train Step...
At time: 507.6027956008911 and batch: 50, loss is 6.184585590362548 and perplexity is 485.21184547418835
At time: 509.2924180030823 and batch: 100, loss is 6.173087043762207 and perplexity is 479.6645683900239
At time: 510.9546456336975 and batch: 150, loss is 6.185739269256592 and perplexity is 485.77194716607863
At time: 512.6484198570251 and batch: 200, loss is 6.210666980743408 and perplexity is 498.0333191665157
At time: 514.351654291153 and batch: 250, loss is 6.244520740509033 and perplexity is 515.1822597638097
At time: 516.05628490448 and batch: 300, loss is 6.237711114883423 and perplexity is 511.6859791374283
At time: 517.7574281692505 and batch: 350, loss is 6.166015853881836 and perplexity is 476.2847329602183
At time: 519.468766450882 and batch: 400, loss is 6.1814158916473385 and perplexity is 483.67630499695287
At time: 521.1801404953003 and batch: 450, loss is 6.161837787628174 and perplexity is 474.2989350765771
At time: 522.8941071033478 and batch: 500, loss is 6.161742162704468 and perplexity is 474.25358244555224
At time: 524.6109516620636 and batch: 550, loss is 6.189299716949463 and perplexity is 487.5045954460453
At time: 526.3197801113129 and batch: 600, loss is 6.177279510498047 and perplexity is 481.6797675132806
At time: 528.0274267196655 and batch: 650, loss is 6.182032499313355 and perplexity is 483.97463548146396
At time: 529.7321860790253 and batch: 700, loss is 6.158542604446411 and perplexity is 472.73860540050634
At time: 531.4261689186096 and batch: 750, loss is 6.1333191776275635 and perplexity is 460.96364449846567
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.813646538313045 and perplexity of 334.83790140397156
Finished 19 epochs...
Completing Train Step...
At time: 535.8447878360748 and batch: 50, loss is 6.180399084091187 and perplexity is 483.1847492264628
At time: 537.5285325050354 and batch: 100, loss is 6.169711313247681 and perplexity is 478.0480800193972
At time: 539.1862061023712 and batch: 150, loss is 6.183055725097656 and perplexity is 484.47010425241814
At time: 540.8561465740204 and batch: 200, loss is 6.207213201522827 and perplexity is 496.31618903871026
At time: 542.5505673885345 and batch: 250, loss is 6.241187963485718 and perplexity is 513.4681301581586
At time: 544.2444131374359 and batch: 300, loss is 6.234212064743042 and perplexity is 509.89869296579246
At time: 545.9386970996857 and batch: 350, loss is 6.163370771408081 and perplexity is 475.02658524633017
At time: 547.6343200206757 and batch: 400, loss is 6.177409372329712 and perplexity is 481.7423233918884
At time: 549.332106590271 and batch: 450, loss is 6.157590417861939 and perplexity is 472.28868428090277
At time: 551.0299360752106 and batch: 500, loss is 6.1578851985931395 and perplexity is 472.42792640653244
At time: 552.7264094352722 and batch: 550, loss is 6.186022310256958 and perplexity is 485.90946000392194
At time: 554.4259965419769 and batch: 600, loss is 6.1761850738525395 and perplexity is 481.1528878950865
At time: 556.1208431720734 and batch: 650, loss is 6.180512285232544 and perplexity is 483.23944938756335
At time: 557.82044672966 and batch: 700, loss is 6.157484893798828 and perplexity is 472.2388490894284
At time: 559.5159084796906 and batch: 750, loss is 6.130967016220093 and perplexity is 459.8806577827248
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.814406284066134 and perplexity of 335.092389738511
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 563.8915674686432 and batch: 50, loss is 6.178660078048706 and perplexity is 482.3452182142467
At time: 565.5762135982513 and batch: 100, loss is 6.165691833496094 and perplexity is 476.1304319970003
At time: 567.2577373981476 and batch: 150, loss is 6.172822551727295 and perplexity is 479.53771770849215
At time: 568.9627645015717 and batch: 200, loss is 6.198420715332031 and perplexity is 491.97146425670496
At time: 570.6581437587738 and batch: 250, loss is 6.226146278381347 and perplexity is 505.80250075496707
At time: 572.3592190742493 and batch: 300, loss is 6.207778625488281 and perplexity is 496.5968974585907
At time: 574.0653612613678 and batch: 350, loss is 6.143884506225586 and perplexity is 465.8596955244283
At time: 575.7784290313721 and batch: 400, loss is 6.157547969818115 and perplexity is 472.2686369756223
At time: 577.4895396232605 and batch: 450, loss is 6.130822591781616 and perplexity is 459.8142445729189
At time: 579.1981337070465 and batch: 500, loss is 6.126211824417115 and perplexity is 457.69902819294776
At time: 580.9271626472473 and batch: 550, loss is 6.151731081008911 and perplexity is 469.52947724361456
At time: 582.6239840984344 and batch: 600, loss is 6.144978694915771 and perplexity is 466.36971291122774
At time: 584.3214657306671 and batch: 650, loss is 6.146164674758911 and perplexity is 466.9231461056285
At time: 586.0167989730835 and batch: 700, loss is 6.124834938049316 and perplexity is 457.0692622980056
At time: 587.7217943668365 and batch: 750, loss is 6.107231912612915 and perplexity is 449.09386197632847
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.785057245298874 and perplexity of 325.400667237919
Finished 21 epochs...
Completing Train Step...
At time: 592.1566932201385 and batch: 50, loss is 6.165066471099854 and perplexity is 475.8327710118179
At time: 593.8434062004089 and batch: 100, loss is 6.155021982192993 and perplexity is 471.07719765688586
At time: 595.5233812332153 and batch: 150, loss is 6.163288078308105 and perplexity is 474.9873054495318
At time: 597.2222695350647 and batch: 200, loss is 6.189214305877686 and perplexity is 487.46295893418693
At time: 598.9193930625916 and batch: 250, loss is 6.217965316772461 and perplexity is 501.6814300597894
At time: 600.6133680343628 and batch: 300, loss is 6.202890758514404 and perplexity is 494.1755203894841
At time: 602.3086347579956 and batch: 350, loss is 6.141348705291748 and perplexity is 464.6798646138634
At time: 604.0048418045044 and batch: 400, loss is 6.156029615402222 and perplexity is 471.5521099138536
At time: 605.7118036746979 and batch: 450, loss is 6.131607503890991 and perplexity is 460.17530002140734
At time: 607.4202747344971 and batch: 500, loss is 6.129376096725464 and perplexity is 459.1496063552317
At time: 609.1282775402069 and batch: 550, loss is 6.155444965362549 and perplexity is 471.2764975303309
At time: 610.8419198989868 and batch: 600, loss is 6.147993745803833 and perplexity is 467.77796323468834
At time: 612.5556843280792 and batch: 650, loss is 6.149545955657959 and perplexity is 468.50461661213325
At time: 614.2633817195892 and batch: 700, loss is 6.128198223114014 and perplexity is 458.60910453410463
At time: 615.9760088920593 and batch: 750, loss is 6.108236293792725 and perplexity is 449.54514999392404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.783724407817042 and perplexity of 324.9672499335876
Finished 22 epochs...
Completing Train Step...
At time: 620.3656029701233 and batch: 50, loss is 6.162159385681153 and perplexity is 474.451493220514
At time: 622.0549566745758 and batch: 100, loss is 6.151408224105835 and perplexity is 469.3779108791288
At time: 623.7281606197357 and batch: 150, loss is 6.159396476745606 and perplexity is 473.1424361858424
At time: 625.4215569496155 and batch: 200, loss is 6.185722322463989 and perplexity is 485.7637149593926
At time: 627.119781255722 and batch: 250, loss is 6.215343952178955 and perplexity is 500.3680622817798
At time: 628.8035304546356 and batch: 300, loss is 6.200991487503051 and perplexity is 493.2378378875548
At time: 630.5195319652557 and batch: 350, loss is 6.140804214477539 and perplexity is 464.42691956541
At time: 632.2353765964508 and batch: 400, loss is 6.156449384689331 and perplexity is 471.7500945578964
At time: 633.9493856430054 and batch: 450, loss is 6.132605323791504 and perplexity is 460.63470125522315
At time: 635.6516046524048 and batch: 500, loss is 6.130425825119018 and perplexity is 459.6318417977479
At time: 637.3586988449097 and batch: 550, loss is 6.156584901809692 and perplexity is 471.81402910425584
At time: 639.0704743862152 and batch: 600, loss is 6.14902473449707 and perplexity is 468.26048572060125
At time: 640.7749230861664 and batch: 650, loss is 6.150512866973877 and perplexity is 468.95783810467975
At time: 642.4716246128082 and batch: 700, loss is 6.1287252235412595 and perplexity is 458.8508554239489
At time: 644.1805357933044 and batch: 750, loss is 6.108146858215332 and perplexity is 449.50494646171006
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.783399005268896 and perplexity of 324.86152196540326
Finished 23 epochs...
Completing Train Step...
At time: 648.5826389789581 and batch: 50, loss is 6.160588274002075 and perplexity is 473.70666219792986
At time: 650.28431224823 and batch: 100, loss is 6.149656209945679 and perplexity is 468.55627410260894
At time: 651.9636046886444 and batch: 150, loss is 6.157480869293213 and perplexity is 472.236948565353
At time: 653.6548342704773 and batch: 200, loss is 6.184038658142089 and perplexity is 484.94654004080854
At time: 655.3499600887299 and batch: 250, loss is 6.214411478042603 and perplexity is 499.9016994744656
At time: 657.044269323349 and batch: 300, loss is 6.200773162841797 and perplexity is 493.13016365807755
At time: 658.7389326095581 and batch: 350, loss is 6.141488790512085 and perplexity is 464.7449639547051
At time: 660.4328536987305 and batch: 400, loss is 6.1572114372253415 and perplexity is 472.10972992693564
At time: 662.1259570121765 and batch: 450, loss is 6.133202953338623 and perplexity is 460.9100724399039
At time: 663.8212473392487 and batch: 500, loss is 6.1310645198822025 and perplexity is 459.92550001709753
At time: 665.5678129196167 and batch: 550, loss is 6.157000007629395 and perplexity is 472.0099225089876
At time: 667.2662460803986 and batch: 600, loss is 6.149360694885254 and perplexity is 468.41782912425066
At time: 668.9630959033966 and batch: 650, loss is 6.150857076644898 and perplexity is 469.11928571217646
At time: 670.6795675754547 and batch: 700, loss is 6.128984994888306 and perplexity is 458.97006721198875
At time: 672.3888704776764 and batch: 750, loss is 6.108467359542846 and perplexity is 449.6490364830633
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.783170833144077 and perplexity of 324.78740607757607
Finished 24 epochs...
Completing Train Step...
At time: 676.7777202129364 and batch: 50, loss is 6.159329795837403 and perplexity is 473.11088767034164
At time: 678.4860682487488 and batch: 100, loss is 6.148230447769165 and perplexity is 467.8887003032485
At time: 680.1671991348267 and batch: 150, loss is 6.1560001468658445 and perplexity is 471.5382141680934
At time: 681.8480322360992 and batch: 200, loss is 6.18274881362915 and perplexity is 484.32143763618643
At time: 683.541015625 and batch: 250, loss is 6.213288908004761 and perplexity is 499.3408396658753
At time: 685.234786272049 and batch: 300, loss is 6.200111541748047 and perplexity is 492.80400624804605
At time: 686.931387424469 and batch: 350, loss is 6.1412464427948 and perplexity is 464.6323477202657
At time: 688.6385114192963 and batch: 400, loss is 6.157085876464844 and perplexity is 472.0504551915762
At time: 690.3435614109039 and batch: 450, loss is 6.133141441345215 and perplexity is 460.8817218145269
At time: 692.0506353378296 and batch: 500, loss is 6.130754041671753 and perplexity is 459.7827253362727
At time: 693.7581157684326 and batch: 550, loss is 6.157162551879883 and perplexity is 472.08665124380303
At time: 695.4665081501007 and batch: 600, loss is 6.149514780044556 and perplexity is 468.4900109210002
At time: 697.1785273551941 and batch: 650, loss is 6.150946846008301 and perplexity is 469.1614001420795
At time: 698.8860907554626 and batch: 700, loss is 6.1291014766693115 and perplexity is 459.0235319766209
At time: 700.5942568778992 and batch: 750, loss is 6.107916669845581 and perplexity is 449.4014875588639
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.782892271529796 and perplexity of 324.6969453734658
Finished 25 epochs...
Completing Train Step...
At time: 704.9847941398621 and batch: 50, loss is 6.158682661056519 and perplexity is 472.804820203848
At time: 706.6670517921448 and batch: 100, loss is 6.147826099395752 and perplexity is 467.6995485125295
At time: 708.3256752490997 and batch: 150, loss is 6.155381469726563 and perplexity is 471.24657447939643
At time: 709.9990491867065 and batch: 200, loss is 6.182146797180176 and perplexity is 484.0299559113383
At time: 711.6829071044922 and batch: 250, loss is 6.212812185287476 and perplexity is 499.1028492761583
At time: 713.3797636032104 and batch: 300, loss is 6.199739799499512 and perplexity is 492.6208442253167
At time: 715.0863931179047 and batch: 350, loss is 6.141036577224732 and perplexity is 464.5348476190544
At time: 716.7926957607269 and batch: 400, loss is 6.156970014572144 and perplexity is 471.99576570066273
At time: 718.4976620674133 and batch: 450, loss is 6.132970991134644 and perplexity is 460.80317112267653
At time: 720.2063841819763 and batch: 500, loss is 6.130532350540161 and perplexity is 459.68080688123274
At time: 721.9083135128021 and batch: 550, loss is 6.157082042694092 and perplexity is 472.0486454618164
At time: 723.6112887859344 and batch: 600, loss is 6.149391279220581 and perplexity is 468.4321555912915
At time: 725.3078775405884 and batch: 650, loss is 6.150753860473633 and perplexity is 469.0708675144521
At time: 727.0045027732849 and batch: 700, loss is 6.128895587921143 and perplexity is 458.9290339246189
At time: 728.7019984722137 and batch: 750, loss is 6.107830686569214 and perplexity is 449.36284820775205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.782508317814317 and perplexity of 324.57230070530784
Finished 26 epochs...
Completing Train Step...
At time: 733.0765523910522 and batch: 50, loss is 6.158132658004761 and perplexity is 472.54484760926397
At time: 734.7758338451385 and batch: 100, loss is 6.1472706127166745 and perplexity is 467.43981978812195
At time: 736.458664894104 and batch: 150, loss is 6.154888801574707 and perplexity is 471.0144634820221
At time: 738.1419746875763 and batch: 200, loss is 6.181646404266357 and perplexity is 483.78781134009074
At time: 739.83318400383 and batch: 250, loss is 6.212381792068482 and perplexity is 498.8880850141055
At time: 741.5338208675385 and batch: 300, loss is 6.199426689147949 and perplexity is 492.46662368487796
At time: 743.2638669013977 and batch: 350, loss is 6.140730981826782 and perplexity is 464.3929095963427
At time: 744.9706766605377 and batch: 400, loss is 6.15677020072937 and perplexity is 471.90146383466794
At time: 746.6756491661072 and batch: 450, loss is 6.132726526260376 and perplexity is 460.6905347017708
At time: 748.3821654319763 and batch: 500, loss is 6.130358066558838 and perplexity is 459.60069886104674
At time: 750.1204295158386 and batch: 550, loss is 6.1568513774871825 and perplexity is 471.93977282038765
At time: 751.8148262500763 and batch: 600, loss is 6.1494025707244875 and perplexity is 468.43744492466834
At time: 753.5094978809357 and batch: 650, loss is 6.150730094909668 and perplexity is 469.0597199132112
At time: 755.204304933548 and batch: 700, loss is 6.128885354995727 and perplexity is 458.92433776207156
At time: 756.9000909328461 and batch: 750, loss is 6.107567834854126 and perplexity is 449.2447479345135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.782500510992006 and perplexity of 324.5697668369198
Finished 27 epochs...
Completing Train Step...
At time: 761.2715935707092 and batch: 50, loss is 6.157869596481323 and perplexity is 472.4205555906996
At time: 762.9747173786163 and batch: 100, loss is 6.146814804077149 and perplexity is 467.2268052304301
At time: 764.6601839065552 and batch: 150, loss is 6.15451756477356 and perplexity is 470.8396380321332
At time: 766.3538117408752 and batch: 200, loss is 6.1811457538604735 and perplexity is 483.54566339677274
At time: 768.0494067668915 and batch: 250, loss is 6.211829586029053 and perplexity is 498.61267204991117
At time: 769.7450096607208 and batch: 300, loss is 6.1990595722198485 and perplexity is 492.28586403279286
At time: 771.4401211738586 and batch: 350, loss is 6.140773334503174 and perplexity is 464.41257829546953
At time: 773.1508207321167 and batch: 400, loss is 6.156834754943848 and perplexity is 471.93192804626284
At time: 774.8552362918854 and batch: 450, loss is 6.1327588081359865 and perplexity is 460.70540689635703
At time: 776.5628552436829 and batch: 500, loss is 6.130377912521363 and perplexity is 459.609820169803
At time: 778.2730922698975 and batch: 550, loss is 6.156837711334228 and perplexity is 471.93332326333757
At time: 779.9823553562164 and batch: 600, loss is 6.149249629974365 and perplexity is 468.3658072287574
At time: 781.6890423297882 and batch: 650, loss is 6.150484066009522 and perplexity is 468.944331861198
At time: 783.4044086933136 and batch: 700, loss is 6.128731288909912 and perplexity is 458.8536385319839
At time: 785.1147844791412 and batch: 750, loss is 6.107433557510376 and perplexity is 449.1844285929184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.782811719317769 and perplexity of 324.6707913696729
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 789.4990465641022 and batch: 50, loss is 6.158085765838623 and perplexity is 472.5226894772878
At time: 791.2007851600647 and batch: 100, loss is 6.145639190673828 and perplexity is 466.67784987866946
At time: 792.8675103187561 and batch: 150, loss is 6.152398843765258 and perplexity is 469.8431162480935
At time: 794.5401611328125 and batch: 200, loss is 6.1780353355407716 and perplexity is 482.04397076377137
At time: 796.2246041297913 and batch: 250, loss is 6.206816635131836 and perplexity is 496.1194057402328
At time: 797.9195625782013 and batch: 300, loss is 6.1930018901824955 and perplexity is 489.31276692706933
At time: 799.6253581047058 and batch: 350, loss is 6.13234577178955 and perplexity is 460.5151581108455
At time: 801.3335752487183 and batch: 400, loss is 6.148658399581909 and perplexity is 468.088976972031
At time: 803.0407454967499 and batch: 450, loss is 6.1228314399719235 and perplexity is 456.15444163648226
At time: 804.7627756595612 and batch: 500, loss is 6.117154569625854 and perplexity is 453.5722483364754
At time: 806.4757606983185 and batch: 550, loss is 6.140044822692871 and perplexity is 464.07437145612437
At time: 808.1836111545563 and batch: 600, loss is 6.133270893096924 and perplexity is 460.9413876225849
At time: 809.8956859111786 and batch: 650, loss is 6.1368004512786865 and perplexity is 462.5711816029525
At time: 811.6061873435974 and batch: 700, loss is 6.115005559921265 and perplexity is 452.5985637765781
At time: 813.3298482894897 and batch: 750, loss is 6.098245592117309 and perplexity is 445.07623945832825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.772442839866461 and perplexity of 321.3217121777337
Finished 29 epochs...
Completing Train Step...
At time: 817.7255311012268 and batch: 50, loss is 6.152431049346924 and perplexity is 469.85824806260695
At time: 819.4102821350098 and batch: 100, loss is 6.14033055305481 and perplexity is 464.2069905399942
At time: 821.0774779319763 and batch: 150, loss is 6.148719711303711 and perplexity is 468.1176771929866
At time: 822.7604601383209 and batch: 200, loss is 6.174775676727295 and perplexity is 480.47523005476506
At time: 824.4474272727966 and batch: 250, loss is 6.204443168640137 and perplexity is 494.94327925535464
At time: 826.1434504985809 and batch: 300, loss is 6.191203594207764 and perplexity is 488.43362846042055
At time: 827.8408210277557 and batch: 350, loss is 6.131076211929321 and perplexity is 459.93087751915186
At time: 829.5368986129761 and batch: 400, loss is 6.147933092117309 and perplexity is 467.7495916371732
At time: 831.2350118160248 and batch: 450, loss is 6.123028593063355 and perplexity is 456.2443827606149
At time: 832.932874917984 and batch: 500, loss is 6.1176689338684085 and perplexity is 453.8056096936584
At time: 834.6752371788025 and batch: 550, loss is 6.141223468780518 and perplexity is 464.62167337269005
At time: 836.3740224838257 and batch: 600, loss is 6.13533200263977 and perplexity is 461.89241806759105
At time: 838.0721793174744 and batch: 650, loss is 6.139092073440552 and perplexity is 463.63243550620217
At time: 839.7694594860077 and batch: 700, loss is 6.116425380706787 and perplexity is 453.2416290356072
At time: 841.4704301357269 and batch: 750, loss is 6.098475379943848 and perplexity is 445.1785243114942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771761162336483 and perplexity of 321.10274902625724
Finished 30 epochs...
Completing Train Step...
At time: 845.8768277168274 and batch: 50, loss is 6.1499765014648435 and perplexity is 468.7063727398325
At time: 847.5879743099213 and batch: 100, loss is 6.1379137134552 and perplexity is 463.0864313541913
At time: 849.2646820545197 and batch: 150, loss is 6.146731233596801 and perplexity is 467.1877604934022
At time: 850.9617085456848 and batch: 200, loss is 6.173053512573242 and perplexity is 479.6484849363915
At time: 852.6600432395935 and batch: 250, loss is 6.203456172943115 and perplexity is 494.4550133662629
At time: 854.3578896522522 and batch: 300, loss is 6.190647420883178 and perplexity is 488.1620502347233
At time: 856.0669267177582 and batch: 350, loss is 6.130844058990479 and perplexity is 459.8241156072967
At time: 857.7767963409424 and batch: 400, loss is 6.1479541778564455 and perplexity is 467.7594545870268
At time: 859.4859981536865 and batch: 450, loss is 6.123631248474121 and perplexity is 456.5194237756709
At time: 861.1998171806335 and batch: 500, loss is 6.118446521759033 and perplexity is 454.15862067119724
At time: 862.911993265152 and batch: 550, loss is 6.142141427993774 and perplexity is 465.04837293487526
At time: 864.6362280845642 and batch: 600, loss is 6.136622543334961 and perplexity is 462.48889383524767
At time: 866.3541898727417 and batch: 650, loss is 6.140465526580811 and perplexity is 464.26965042291937
At time: 868.0667865276337 and batch: 700, loss is 6.117298240661621 and perplexity is 453.6374182125888
At time: 869.7862770557404 and batch: 750, loss is 6.098537292480469 and perplexity is 445.20608729642186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771451018577398 and perplexity of 321.0031764543259
Finished 31 epochs...
Completing Train Step...
At time: 874.2446873188019 and batch: 50, loss is 6.148454151153564 and perplexity is 467.9933802972267
At time: 875.9590044021606 and batch: 100, loss is 6.136376619338989 and perplexity is 462.3751707025799
At time: 877.6226913928986 and batch: 150, loss is 6.145473432540894 and perplexity is 466.6005006404016
At time: 879.3038601875305 and batch: 200, loss is 6.172106189727783 and perplexity is 479.19431812408493
At time: 880.9934959411621 and batch: 250, loss is 6.203005409240722 and perplexity is 494.232181219866
At time: 882.6898927688599 and batch: 300, loss is 6.190424032211304 and perplexity is 488.0530125420081
At time: 884.3984684944153 and batch: 350, loss is 6.130850744247437 and perplexity is 459.8271896599403
At time: 886.1098606586456 and batch: 400, loss is 6.1481327629089355 and perplexity is 467.84299689325854
At time: 887.8245613574982 and batch: 450, loss is 6.124163398742676 and perplexity is 456.76242536057833
At time: 889.5377824306488 and batch: 500, loss is 6.119042520523071 and perplexity is 454.4293793256915
At time: 891.2481887340546 and batch: 550, loss is 6.142766780853272 and perplexity is 465.3392832160024
At time: 892.9612801074982 and batch: 600, loss is 6.137487049102783 and perplexity is 462.8888910265493
At time: 894.6713562011719 and batch: 650, loss is 6.14131269454956 and perplexity is 464.66313144834743
At time: 896.3842186927795 and batch: 700, loss is 6.117784070968628 and perplexity is 453.85786256368607
At time: 898.10196185112 and batch: 750, loss is 6.098515338897705 and perplexity is 445.1963135350226
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771337109942769 and perplexity of 320.9666135032417
Finished 32 epochs...
Completing Train Step...
At time: 902.5285584926605 and batch: 50, loss is 6.147294406890869 and perplexity is 467.450942264944
At time: 904.2324907779694 and batch: 100, loss is 6.135235843658447 and perplexity is 461.84800509857587
At time: 905.8964369297028 and batch: 150, loss is 6.1446202278137205 and perplexity is 466.2025646721185
At time: 907.5850148200989 and batch: 200, loss is 6.17147744178772 and perplexity is 478.8931203823267
At time: 909.2805876731873 and batch: 250, loss is 6.202728624343872 and perplexity is 494.0954041463543
At time: 910.9780628681183 and batch: 300, loss is 6.190285100936889 and perplexity is 487.9852114249506
At time: 912.6777946949005 and batch: 350, loss is 6.130883989334106 and perplexity is 459.84247690882506
At time: 914.3755173683167 and batch: 400, loss is 6.148284921646118 and perplexity is 467.9141887089565
At time: 916.0717709064484 and batch: 450, loss is 6.124566450119018 and perplexity is 456.9465611903886
At time: 917.7687940597534 and batch: 500, loss is 6.119467363357544 and perplexity is 454.6224814073855
At time: 919.5082292556763 and batch: 550, loss is 6.143196849822998 and perplexity is 465.5394542426967
At time: 921.2067370414734 and batch: 600, loss is 6.138075914382934 and perplexity is 463.16155049501066
At time: 922.9057402610779 and batch: 650, loss is 6.141921224594117 and perplexity is 464.9459789763124
At time: 924.6136083602905 and batch: 700, loss is 6.118136806488037 and perplexity is 454.0179825909257
At time: 926.3230195045471 and batch: 750, loss is 6.098467388153076 and perplexity is 445.1749665520884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771270042242006 and perplexity of 320.94508773230245
Finished 33 epochs...
Completing Train Step...
At time: 930.887912273407 and batch: 50, loss is 6.1463397598266605 and perplexity is 467.00490453342945
At time: 932.5737447738647 and batch: 100, loss is 6.134366912841797 and perplexity is 461.4468654408822
At time: 934.233327627182 and batch: 150, loss is 6.143969631195068 and perplexity is 465.89935350471103
At time: 935.9131934642792 and batch: 200, loss is 6.171024646759033 and perplexity is 478.67632904286967
At time: 937.6218438148499 and batch: 250, loss is 6.202556180953979 and perplexity is 494.01020800589964
At time: 939.3334875106812 and batch: 300, loss is 6.190202465057373 and perplexity is 487.94488800391724
At time: 941.0429418087006 and batch: 350, loss is 6.130925216674805 and perplexity is 459.8614353820893
At time: 942.7510030269623 and batch: 400, loss is 6.148416042327881 and perplexity is 467.97554595890085
At time: 944.4601802825928 and batch: 450, loss is 6.124882955551147 and perplexity is 457.091210149081
At time: 946.1687319278717 and batch: 500, loss is 6.119793653488159 and perplexity is 454.77084443960473
At time: 947.8749797344208 and batch: 550, loss is 6.143500452041626 and perplexity is 465.68081451142325
At time: 949.5778920650482 and batch: 600, loss is 6.138507642745972 and perplexity is 463.36155364343705
At time: 951.2747900485992 and batch: 650, loss is 6.1423708152771 and perplexity is 465.1550613537732
At time: 952.9713139533997 and batch: 700, loss is 6.118388118743897 and perplexity is 454.13209721293254
At time: 954.674859046936 and batch: 750, loss is 6.098412303924561 and perplexity is 445.15044510788016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771227814430414 and perplexity of 320.93153520975517
Finished 34 epochs...
Completing Train Step...
At time: 959.0854227542877 and batch: 50, loss is 6.145602703094482 and perplexity is 466.6608222442437
At time: 960.7820751667023 and batch: 100, loss is 6.133676633834839 and perplexity is 461.12844826783044
At time: 962.4477806091309 and batch: 150, loss is 6.143454599380493 and perplexity is 465.6594622963711
At time: 964.1357593536377 and batch: 200, loss is 6.170694351196289 and perplexity is 478.5182504831568
At time: 965.844064950943 and batch: 250, loss is 6.202428483963013 and perplexity is 493.94712841645327
At time: 967.5516335964203 and batch: 300, loss is 6.190143709182739 and perplexity is 487.9162192174874
At time: 969.2871046066284 and batch: 350, loss is 6.130960836410522 and perplexity is 459.8778158166159
At time: 971.0173125267029 and batch: 400, loss is 6.148532056808472 and perplexity is 468.029841048242
At time: 972.7461569309235 and batch: 450, loss is 6.125140295028687 and perplexity is 457.2088528987014
At time: 974.4553370475769 and batch: 500, loss is 6.120052099227905 and perplexity is 454.8883932162468
At time: 976.1639935970306 and batch: 550, loss is 6.143715915679931 and perplexity is 465.7811626043013
At time: 977.8757727146149 and batch: 600, loss is 6.138847923278808 and perplexity is 463.5192533893619
At time: 979.5847001075745 and batch: 650, loss is 6.142714681625367 and perplexity is 465.315040030164
At time: 981.2946338653564 and batch: 700, loss is 6.118575077056885 and perplexity is 454.21700892092883
At time: 983.0034935474396 and batch: 750, loss is 6.098348188400268 and perplexity is 445.1219049686458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771194103152253 and perplexity of 320.92071637986044
Finished 35 epochs...
Completing Train Step...
At time: 987.4194288253784 and batch: 50, loss is 6.145012788772583 and perplexity is 466.385613524496
At time: 989.1228425502777 and batch: 100, loss is 6.133102655410767 and perplexity is 460.8638464329352
At time: 990.7840752601624 and batch: 150, loss is 6.143029308319091 and perplexity is 465.46146359592245
At time: 992.4682936668396 and batch: 200, loss is 6.1704191303253175 and perplexity is 478.38657039486856
At time: 994.1702535152435 and batch: 250, loss is 6.202324867248535 and perplexity is 493.8959498894022
At time: 995.8710539340973 and batch: 300, loss is 6.190090894699097 and perplexity is 487.890450854786
At time: 997.5680058002472 and batch: 350, loss is 6.130991258621216 and perplexity is 459.89180652923517
At time: 999.2666652202606 and batch: 400, loss is 6.14862738609314 and perplexity is 468.07446012491096
At time: 1000.9862101078033 and batch: 450, loss is 6.125349960327148 and perplexity is 457.3047237793509
At time: 1002.6988799571991 and batch: 500, loss is 6.120255031585693 and perplexity is 454.98071415754896
At time: 1004.4496932029724 and batch: 550, loss is 6.143876581192017 and perplexity is 465.8560035853322
At time: 1006.1560950279236 and batch: 600, loss is 6.139117212295532 and perplexity is 463.6440908412626
At time: 1007.8628361225128 and batch: 650, loss is 6.142991256713867 and perplexity is 465.44375237703497
At time: 1009.5690226554871 and batch: 700, loss is 6.1187263774871825 and perplexity is 454.2857373490162
At time: 1011.2763645648956 and batch: 750, loss is 6.09829047203064 and perplexity is 445.0962148896251
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.7711699729742 and perplexity of 320.91297259926307
Finished 36 epochs...
Completing Train Step...
At time: 1015.6633992195129 and batch: 50, loss is 6.1445113563537594 and perplexity is 466.1518112811131
At time: 1017.3646805286407 and batch: 100, loss is 6.13261420249939 and perplexity is 460.63879111433414
At time: 1019.0257782936096 and batch: 150, loss is 6.142668333053589 and perplexity is 465.29347384241674
At time: 1020.716965675354 and batch: 200, loss is 6.170181388854981 and perplexity is 478.2728515866074
At time: 1022.4440085887909 and batch: 250, loss is 6.2022359085083005 and perplexity is 493.85201548209676
At time: 1024.1543955802917 and batch: 300, loss is 6.19003999710083 and perplexity is 487.8656190345656
At time: 1025.8629114627838 and batch: 350, loss is 6.131014280319214 and perplexity is 459.90239414138864
At time: 1027.5728693008423 and batch: 400, loss is 6.148705024719238 and perplexity is 468.11080219366244
At time: 1029.2811479568481 and batch: 450, loss is 6.125522871017456 and perplexity is 457.38380349148713
At time: 1030.9890627861023 and batch: 500, loss is 6.120417213439941 and perplexity is 455.05450975740985
At time: 1032.6940052509308 and batch: 550, loss is 6.143999080657959 and perplexity is 465.9130741924656
At time: 1034.398533821106 and batch: 600, loss is 6.139336500167847 and perplexity is 463.74577351593575
At time: 1036.094494819641 and batch: 650, loss is 6.1432192993164065 and perplexity is 465.5499054849181
At time: 1037.7902653217316 and batch: 700, loss is 6.118853693008423 and perplexity is 454.34357865642875
At time: 1039.4855816364288 and batch: 750, loss is 6.098238458633423 and perplexity is 445.0730645254701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771149391351744 and perplexity of 320.9063677575892
Finished 37 epochs...
Completing Train Step...
At time: 1043.8651616573334 and batch: 50, loss is 6.144076356887817 and perplexity is 465.9490795894434
At time: 1045.564962387085 and batch: 100, loss is 6.132190361022949 and perplexity is 460.44359465810146
At time: 1047.2264766693115 and batch: 150, loss is 6.14235408782959 and perplexity is 465.1472805619771
At time: 1048.9127995967865 and batch: 200, loss is 6.169969444274902 and perplexity is 478.1714949892851
At time: 1050.6082553863525 and batch: 250, loss is 6.202156572341919 and perplexity is 493.812836710596
At time: 1052.3143470287323 and batch: 300, loss is 6.189989023208618 and perplexity is 487.840751258896
At time: 1054.0224912166595 and batch: 350, loss is 6.131029844284058 and perplexity is 459.9095521017857
At time: 1055.7305076122284 and batch: 400, loss is 6.148768396377563 and perplexity is 468.1404680914561
At time: 1057.4383623600006 and batch: 450, loss is 6.125666580200195 and perplexity is 457.44953846733193
At time: 1059.1448884010315 and batch: 500, loss is 6.120548734664917 and perplexity is 455.11436301986475
At time: 1060.8501799106598 and batch: 550, loss is 6.144092988967896 and perplexity is 465.95682935629486
At time: 1062.5577189922333 and batch: 600, loss is 6.139518556594848 and perplexity is 463.83020910028625
At time: 1064.2642736434937 and batch: 650, loss is 6.143408498764038 and perplexity is 465.6379956029188
At time: 1065.960113286972 and batch: 700, loss is 6.1189621067047115 and perplexity is 454.3928383933428
At time: 1067.6553616523743 and batch: 750, loss is 6.098190851211548 and perplexity is 445.0518762486857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.7711302291515265 and perplexity of 320.90021854443546
Finished 38 epochs...
Completing Train Step...
At time: 1072.0378761291504 and batch: 50, loss is 6.143692283630371 and perplexity is 465.7701553708448
At time: 1073.7239317893982 and batch: 100, loss is 6.131816177368164 and perplexity is 460.27133642114836
At time: 1075.4045550823212 and batch: 150, loss is 6.142075672149658 and perplexity is 465.01779429193124
At time: 1077.0861427783966 and batch: 200, loss is 6.16977442741394 and perplexity is 478.07825257754746
At time: 1078.7682752609253 and batch: 250, loss is 6.202082529067993 and perplexity is 493.776274545067
At time: 1080.4564559459686 and batch: 300, loss is 6.189937162399292 and perplexity is 487.8154520987366
At time: 1082.1476414203644 and batch: 350, loss is 6.13103627204895 and perplexity is 459.91250830175943
At time: 1083.8478033542633 and batch: 400, loss is 6.148817729949951 and perplexity is 468.16356370281636
At time: 1085.5488541126251 and batch: 450, loss is 6.125784959793091 and perplexity is 457.50369436287906
At time: 1087.2663352489471 and batch: 500, loss is 6.120653314590454 and perplexity is 455.16196133493105
At time: 1088.9929645061493 and batch: 550, loss is 6.144164009094238 and perplexity is 465.9899228443238
At time: 1090.6958911418915 and batch: 600, loss is 6.139669008255005 and perplexity is 463.8999983751008
At time: 1092.4069828987122 and batch: 650, loss is 6.1435582733154295 and perplexity is 465.7077415477745
At time: 1094.117113351822 and batch: 700, loss is 6.1190569210052494 and perplexity is 454.4359233749884
At time: 1095.8351836204529 and batch: 750, loss is 6.098147563934326 and perplexity is 445.03261158170073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771112486373546 and perplexity of 320.89452493361443
Finished 39 epochs...
Completing Train Step...
At time: 1100.3035352230072 and batch: 50, loss is 6.14334698677063 and perplexity is 465.60935416250777
At time: 1102.0144999027252 and batch: 100, loss is 6.131477680206299 and perplexity is 460.1155622461271
At time: 1103.6873273849487 and batch: 150, loss is 6.141825122833252 and perplexity is 464.9012989959726
At time: 1105.35839509964 and batch: 200, loss is 6.169585838317871 and perplexity is 477.9881007331381
At time: 1107.0478067398071 and batch: 250, loss is 6.202008075714112 and perplexity is 493.7395126139017
At time: 1108.7519953250885 and batch: 300, loss is 6.189883728027343 and perplexity is 487.78938668282774
At time: 1110.4561762809753 and batch: 350, loss is 6.131030550003052 and perplexity is 459.9098766688069
At time: 1112.1667845249176 and batch: 400, loss is 6.148852443695068 and perplexity is 468.17981569552177
At time: 1113.8681709766388 and batch: 450, loss is 6.12588059425354 and perplexity is 457.54744957406274
At time: 1115.5710558891296 and batch: 500, loss is 6.120733528137207 and perplexity is 455.1984729545406
At time: 1117.273918390274 and batch: 550, loss is 6.14421799659729 and perplexity is 466.01508115581674
At time: 1118.9698901176453 and batch: 600, loss is 6.139792385101319 and perplexity is 463.95723642475764
At time: 1120.6682760715485 and batch: 650, loss is 6.1436763381958 and perplexity is 465.7627285225195
At time: 1122.3668880462646 and batch: 700, loss is 6.119145221710205 and perplexity is 454.4760521590536
At time: 1124.0663754940033 and batch: 750, loss is 6.098109636306763 and perplexity is 445.015732870642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771101840706759 and perplexity of 320.89110881561163
Finished 40 epochs...
Completing Train Step...
At time: 1128.486171722412 and batch: 50, loss is 6.143035230636596 and perplexity is 465.46422021465895
At time: 1130.188381433487 and batch: 100, loss is 6.13117112159729 and perplexity is 459.9745314775842
At time: 1131.8570408821106 and batch: 150, loss is 6.141599159240723 and perplexity is 464.79626009621063
At time: 1133.5298192501068 and batch: 200, loss is 6.169404783248901 and perplexity is 477.9015663985697
At time: 1135.1933393478394 and batch: 250, loss is 6.201940469741821 and perplexity is 493.70613400240256
At time: 1136.866452217102 and batch: 300, loss is 6.18983097076416 and perplexity is 487.7636529286038
At time: 1138.5532720088959 and batch: 350, loss is 6.131024379730224 and perplexity is 459.9070389081465
At time: 1140.2402110099792 and batch: 400, loss is 6.148881578445435 and perplexity is 468.1934561962842
At time: 1141.931536912918 and batch: 450, loss is 6.125962591171264 and perplexity is 457.58496859284134
At time: 1143.6176507472992 and batch: 500, loss is 6.1208030796051025 and perplexity is 455.2301337775338
At time: 1145.3018605709076 and batch: 550, loss is 6.144262828826904 and perplexity is 466.0359741192743
At time: 1146.9866259098053 and batch: 600, loss is 6.139903631210327 and perplexity is 464.0088527330596
At time: 1148.67747092247 and batch: 650, loss is 6.143800544738769 and perplexity is 465.8205828937445
At time: 1150.3676218986511 and batch: 700, loss is 6.119222049713135 and perplexity is 454.5109699878365
At time: 1152.0539813041687 and batch: 750, loss is 6.098067970275879 and perplexity is 444.9971912176535
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771089065906613 and perplexity of 320.88700952201185
Finished 41 epochs...
Completing Train Step...
At time: 1156.4603235721588 and batch: 50, loss is 6.142747955322266 and perplexity is 465.3305230393556
At time: 1158.1771688461304 and batch: 100, loss is 6.13088586807251 and perplexity is 459.84334083335773
At time: 1159.8453934192657 and batch: 150, loss is 6.141390228271485 and perplexity is 464.6991599070619
At time: 1161.5160839557648 and batch: 200, loss is 6.169233646392822 and perplexity is 477.81978682493
At time: 1163.1830897331238 and batch: 250, loss is 6.201878204345703 and perplexity is 493.6753941514274
At time: 1164.8462817668915 and batch: 300, loss is 6.189779815673828 and perplexity is 487.7387019730669
At time: 1166.5106205940247 and batch: 350, loss is 6.131019468307495 and perplexity is 459.90478011580933
At time: 1168.180298089981 and batch: 400, loss is 6.14890604019165 and perplexity is 468.20490916586857
At time: 1169.8438005447388 and batch: 450, loss is 6.126034364700318 and perplexity is 457.61781225951785
At time: 1171.5335593223572 and batch: 500, loss is 6.120862340927124 and perplexity is 455.2571121164635
At time: 1173.250731229782 and batch: 550, loss is 6.144300146102905 and perplexity is 466.0533656368468
At time: 1174.9368679523468 and batch: 600, loss is 6.14000247001648 and perplexity is 464.05471708065943
At time: 1176.6223528385162 and batch: 650, loss is 6.143915510177612 and perplexity is 465.8741392399842
At time: 1178.3134860992432 and batch: 700, loss is 6.119290952682495 and perplexity is 454.5422882222228
At time: 1179.9988198280334 and batch: 750, loss is 6.0980281639099125 and perplexity is 444.97947784916073
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771077710528706 and perplexity of 320.8833657494414
Finished 42 epochs...
Completing Train Step...
At time: 1184.4076347351074 and batch: 50, loss is 6.142480821609497 and perplexity is 465.206234170684
At time: 1186.0981559753418 and batch: 100, loss is 6.1305899333953855 and perplexity is 459.7072773767009
At time: 1187.7686803340912 and batch: 150, loss is 6.14119441986084 and perplexity is 464.60817681105095
At time: 1189.435292005539 and batch: 200, loss is 6.169072151184082 and perplexity is 477.742627449319
At time: 1191.1044280529022 and batch: 250, loss is 6.201821422576904 and perplexity is 493.64736318516645
At time: 1192.7689764499664 and batch: 300, loss is 6.189732275009155 and perplexity is 487.7155151021526
At time: 1194.446042060852 and batch: 350, loss is 6.131019201278686 and perplexity is 459.90465730800014
At time: 1196.1168775558472 and batch: 400, loss is 6.148926477432251 and perplexity is 468.21447808002864
At time: 1197.7865660190582 and batch: 450, loss is 6.126098833084106 and perplexity is 457.64731509125687
At time: 1199.4567251205444 and batch: 500, loss is 6.120909519195557 and perplexity is 455.27859086536614
At time: 1201.13605427742 and batch: 550, loss is 6.14432734489441 and perplexity is 466.0660418975574
At time: 1202.8207032680511 and batch: 600, loss is 6.140088510513306 and perplexity is 464.0946462968117
At time: 1204.5143308639526 and batch: 650, loss is 6.1440190505981445 and perplexity is 465.922378541592
At time: 1206.2041676044464 and batch: 700, loss is 6.119352149963379 and perplexity is 454.5701058254808
At time: 1207.888305425644 and batch: 750, loss is 6.0979928779602055 and perplexity is 444.963776602703
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.7710691939952765 and perplexity of 320.88063294716704
Finished 43 epochs...
Completing Train Step...
At time: 1212.2904334068298 and batch: 50, loss is 6.142231531143189 and perplexity is 465.09027714572915
At time: 1213.9969420433044 and batch: 100, loss is 6.130296764373779 and perplexity is 459.5725251975144
At time: 1215.6645250320435 and batch: 150, loss is 6.14102391242981 and perplexity is 464.52896441773
At time: 1217.331253528595 and batch: 200, loss is 6.168931007385254 and perplexity is 477.67520179848685
At time: 1218.9959735870361 and batch: 250, loss is 6.201771268844604 and perplexity is 493.6226055483119
At time: 1220.6694388389587 and batch: 300, loss is 6.1896876621246335 and perplexity is 487.6937571915431
At time: 1222.350340604782 and batch: 350, loss is 6.1310210704803465 and perplexity is 459.9055169633525
At time: 1224.0183997154236 and batch: 400, loss is 6.148945121765137 and perplexity is 468.2232077079986
At time: 1225.6805629730225 and batch: 450, loss is 6.126160917282104 and perplexity is 457.67572863978717
At time: 1227.3420870304108 and batch: 500, loss is 6.1209441089630126 and perplexity is 455.2943391183143
At time: 1229.0020122528076 and batch: 550, loss is 6.144342041015625 and perplexity is 466.07289131093313
At time: 1230.6636941432953 and batch: 600, loss is 6.140162763595581 and perplexity is 464.1291080341957
At time: 1232.3248467445374 and batch: 650, loss is 6.144114637374878 and perplexity is 465.96691668856
At time: 1233.9966053962708 and batch: 700, loss is 6.1194024181365965 and perplexity is 454.59295680863386
At time: 1235.670705795288 and batch: 750, loss is 6.0979603672027585 and perplexity is 444.9493107284387
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.77105712890625 and perplexity of 320.87676151711827
Finished 44 epochs...
Completing Train Step...
At time: 1240.0283546447754 and batch: 50, loss is 6.142004919052124 and perplexity is 464.98489400648907
At time: 1241.7160007953644 and batch: 100, loss is 6.130068235397339 and perplexity is 459.46751155851763
At time: 1243.3784816265106 and batch: 150, loss is 6.140851249694824 and perplexity is 464.44876450022156
At time: 1245.0404102802277 and batch: 200, loss is 6.168799867630005 and perplexity is 477.612563696697
At time: 1246.7023041248322 and batch: 250, loss is 6.201719608306885 and perplexity is 493.5971053977602
At time: 1248.365134716034 and batch: 300, loss is 6.189642105102539 and perplexity is 487.6715398223539
At time: 1250.0277569293976 and batch: 350, loss is 6.131018714904785 and perplexity is 459.9044336224322
At time: 1251.6899490356445 and batch: 400, loss is 6.148957424163818 and perplexity is 468.22896801200426
At time: 1253.3696539402008 and batch: 450, loss is 6.126214256286621 and perplexity is 457.7001412586111
At time: 1255.067105293274 and batch: 500, loss is 6.120968189239502 and perplexity is 455.3053028638888
At time: 1256.785662651062 and batch: 550, loss is 6.144354543685913 and perplexity is 466.0787185030512
At time: 1258.4702916145325 and batch: 600, loss is 6.140230369567871 and perplexity is 464.16048699450334
At time: 1260.1512823104858 and batch: 650, loss is 6.14420069694519 and perplexity is 466.00701932677293
At time: 1261.821042060852 and batch: 700, loss is 6.119448127746582 and perplexity is 454.61373655030525
At time: 1263.489425420761 and batch: 750, loss is 6.097929382324219 and perplexity is 444.935524241677
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771047192950581 and perplexity of 320.8735733156796
Finished 45 epochs...
Completing Train Step...
At time: 1267.8210892677307 and batch: 50, loss is 6.141791543960571 and perplexity is 464.8856883965392
At time: 1269.5074124336243 and batch: 100, loss is 6.129853734970093 and perplexity is 459.3689661503766
At time: 1271.1688005924225 and batch: 150, loss is 6.140687074661255 and perplexity is 464.37251986762215
At time: 1272.830881357193 and batch: 200, loss is 6.168676319122315 and perplexity is 477.55355902224295
At time: 1274.4961688518524 and batch: 250, loss is 6.201668958663941 and perplexity is 493.57210551373663
At time: 1276.1605560779572 and batch: 300, loss is 6.1895947265625 and perplexity is 487.6484352041145
At time: 1277.8254599571228 and batch: 350, loss is 6.131012983322144 and perplexity is 459.9017976497179
At time: 1279.4901938438416 and batch: 400, loss is 6.1489661502838135 and perplexity is 468.23305385199126
At time: 1281.154233455658 and batch: 450, loss is 6.1262594413757325 and perplexity is 457.7208229475285
At time: 1282.8196535110474 and batch: 500, loss is 6.120980606079102 and perplexity is 455.3109563519027
At time: 1284.4842100143433 and batch: 550, loss is 6.144363422393798 and perplexity is 466.08285669821527
At time: 1286.1488749980927 and batch: 600, loss is 6.1402916431427 and perplexity is 464.18892863818763
At time: 1287.8141219615936 and batch: 650, loss is 6.14427773475647 and perplexity is 466.04292087045314
At time: 1289.4792666435242 and batch: 700, loss is 6.119487028121949 and perplexity is 454.6314215392778
At time: 1291.1432790756226 and batch: 750, loss is 6.097898302078247 and perplexity is 444.9216957510393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771034773005995 and perplexity of 320.86958810842805
Finished 46 epochs...
Completing Train Step...
At time: 1295.5031895637512 and batch: 50, loss is 6.1415895652770995 and perplexity is 464.79180087918974
At time: 1297.192773103714 and batch: 100, loss is 6.12965202331543 and perplexity is 459.27631542079376
At time: 1298.857637643814 and batch: 150, loss is 6.14053111076355 and perplexity is 464.30010016701266
At time: 1300.5208735466003 and batch: 200, loss is 6.168559236526489 and perplexity is 477.497649085011
At time: 1302.1850452423096 and batch: 250, loss is 6.2016199016571045 and perplexity is 493.54789293748536
At time: 1303.8549318313599 and batch: 300, loss is 6.189546670913696 and perplexity is 487.6250015052377
At time: 1305.5193240642548 and batch: 350, loss is 6.131004915237427 and perplexity is 459.8980871380215
At time: 1307.184016942978 and batch: 400, loss is 6.148971824645996 and perplexity is 468.23571078346293
At time: 1308.848384141922 and batch: 450, loss is 6.126298198699951 and perplexity is 457.73856332564765
At time: 1310.512169122696 and batch: 500, loss is 6.120980215072632 and perplexity is 455.3107783224077
At time: 1312.179556131363 and batch: 550, loss is 6.144369525909424 and perplexity is 466.08570145089533
At time: 1313.8480668067932 and batch: 600, loss is 6.140347957611084 and perplexity is 464.2150699269929
At time: 1315.5209698677063 and batch: 650, loss is 6.144348621368408 and perplexity is 466.07595824507155
At time: 1317.1928520202637 and batch: 700, loss is 6.119524593353272 and perplexity is 454.64850019457447
At time: 1318.8563673496246 and batch: 750, loss is 6.097865352630615 and perplexity is 444.9070360684405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771020578783612 and perplexity of 320.86503364646205
Finished 47 epochs...
Completing Train Step...
At time: 1323.209887266159 and batch: 50, loss is 6.141396474838257 and perplexity is 464.7020626904595
At time: 1324.8995518684387 and batch: 100, loss is 6.129463615417481 and perplexity is 459.18979228670446
At time: 1326.56378865242 and batch: 150, loss is 6.140385484695434 and perplexity is 464.23249089195565
At time: 1328.2268698215485 and batch: 200, loss is 6.168448333740234 and perplexity is 477.44469620166274
At time: 1329.8914821147919 and batch: 250, loss is 6.201571111679077 and perplexity is 493.5238133340598
At time: 1331.554007768631 and batch: 300, loss is 6.189498586654663 and perplexity is 487.60155498206313
At time: 1333.2162301540375 and batch: 350, loss is 6.130994815826416 and perplexity is 459.89344246167036
At time: 1334.8777797222137 and batch: 400, loss is 6.148976249694824 and perplexity is 468.2377827539304
At time: 1336.539380788803 and batch: 450, loss is 6.126334199905395 and perplexity is 457.7550427623437
At time: 1338.201080083847 and batch: 500, loss is 6.120966558456421 and perplexity is 455.3045603603096
At time: 1339.8863971233368 and batch: 550, loss is 6.144371137619019 and perplexity is 466.08645264629786
At time: 1341.547393321991 and batch: 600, loss is 6.140398082733154 and perplexity is 464.2383393472261
At time: 1343.2109951972961 and batch: 650, loss is 6.144413528442382 and perplexity is 466.1062108535645
At time: 1344.8725411891937 and batch: 700, loss is 6.119560441970825 and perplexity is 454.6647990069224
At time: 1346.5338819026947 and batch: 750, loss is 6.097829284667969 and perplexity is 444.8909894674685
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.771006384561228 and perplexity of 320.8604792491423
Finished 48 epochs...
Completing Train Step...
At time: 1350.86483168602 and batch: 50, loss is 6.141210107803345 and perplexity is 464.61546561458914
At time: 1352.550714969635 and batch: 100, loss is 6.12928726196289 and perplexity is 459.1088197206277
At time: 1354.212406873703 and batch: 150, loss is 6.140247859954834 and perplexity is 464.1686054120306
At time: 1355.8738827705383 and batch: 200, loss is 6.168342924118042 and perplexity is 477.39437158901404
At time: 1357.5357398986816 and batch: 250, loss is 6.201521759033203 and perplexity is 493.499457229094
At time: 1359.1973242759705 and batch: 300, loss is 6.189449262619019 and perplexity is 487.5775050987085
At time: 1360.8607716560364 and batch: 350, loss is 6.130982789993286 and perplexity is 459.88791189312894
At time: 1362.5229668617249 and batch: 400, loss is 6.148979549407959 and perplexity is 468.2393278068414
At time: 1364.1963040828705 and batch: 450, loss is 6.126366882324219 and perplexity is 457.770003548846
At time: 1365.861601114273 and batch: 500, loss is 6.120938568115235 and perplexity is 455.29181640867614
At time: 1367.5209965705872 and batch: 550, loss is 6.144368820190429 and perplexity is 466.0853725254789
At time: 1369.180136680603 and batch: 600, loss is 6.140440216064453 and perplexity is 464.2578996670475
At time: 1370.8402495384216 and batch: 650, loss is 6.1444730377197265 and perplexity is 466.13394932267795
At time: 1372.5001442432404 and batch: 700, loss is 6.119591341018677 and perplexity is 454.6788479333516
At time: 1374.1737909317017 and batch: 750, loss is 6.09778977394104 and perplexity is 444.8734118483249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.770992900049964 and perplexity of 320.85615263156694
Finished 49 epochs...
Completing Train Step...
At time: 1378.535075187683 and batch: 50, loss is 6.141031017303467 and perplexity is 464.53226484905696
At time: 1380.236968278885 and batch: 100, loss is 6.129120216369629 and perplexity is 459.0321340206486
At time: 1381.9070246219635 and batch: 150, loss is 6.140115118026733 and perplexity is 464.1069948656261
At time: 1383.5698084831238 and batch: 200, loss is 6.168242120742798 and perplexity is 477.3462510504328
At time: 1385.2334613800049 and batch: 250, loss is 6.201471605300903 and perplexity is 493.4747070100892
At time: 1386.8971633911133 and batch: 300, loss is 6.189398288726807 and perplexity is 487.5526520089533
At time: 1388.567064523697 and batch: 350, loss is 6.130967226028442 and perplexity is 459.8807542695367
At time: 1390.2294056415558 and batch: 400, loss is 6.148979949951172 and perplexity is 468.2395153569637
At time: 1391.8926513195038 and batch: 450, loss is 6.126396064758301 and perplexity is 457.7833625867228
At time: 1393.5645334720612 and batch: 500, loss is 6.120909671783448 and perplexity is 455.27866033537134
At time: 1395.227103471756 and batch: 550, loss is 6.144365015029908 and perplexity is 466.08359899919384
At time: 1396.8945169448853 and batch: 600, loss is 6.140475454330445 and perplexity is 464.2742595986507
At time: 1398.5556018352509 and batch: 650, loss is 6.144528017044068 and perplexity is 466.1595777567747
At time: 1400.2161424160004 and batch: 700, loss is 6.119617471694946 and perplexity is 454.6907291543648
At time: 1401.8782942295074 and batch: 750, loss is 6.097749433517456 and perplexity is 444.8554658284272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.770981189816497 and perplexity of 320.8523953531095
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb44a8f7518>
SETTINGS FOR THIS RUN
{'dropout': 0.19469257568311504, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 29.828116662289073, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 4.336722391562965, 'wordvec_source': 'glove', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.1342852115631104 and batch: 50, loss is 7.964123125076294 and perplexity is 2875.9062618605335
At time: 3.788496255874634 and batch: 100, loss is 6.4580395221710205 and perplexity is 637.809418841678
At time: 5.45095419883728 and batch: 150, loss is 6.2599790668487545 and perplexity is 523.2079876014286
At time: 7.111575365066528 and batch: 200, loss is 6.306898403167724 and perplexity is 548.3415740975287
At time: 8.77091383934021 and batch: 250, loss is 6.476795721054077 and perplexity is 649.885192947019
At time: 10.431096315383911 and batch: 300, loss is 6.791294031143188 and perplexity is 890.064588234637
At time: 12.086908102035522 and batch: 350, loss is 6.755056037902832 and perplexity is 858.3878507956933
At time: 13.770050048828125 and batch: 400, loss is 6.852656307220459 and perplexity is 946.391477286441
At time: 15.432347059249878 and batch: 450, loss is 6.611877040863037 and perplexity is 743.8779984903088
At time: 17.09370446205139 and batch: 500, loss is 6.586590633392334 and perplexity is 725.3038230830342
At time: 18.755662441253662 and batch: 550, loss is 6.5946135997772215 and perplexity is 731.1463170015168
At time: 20.421521186828613 and batch: 600, loss is 6.581890287399292 and perplexity is 721.9026437861315
At time: 22.087685346603394 and batch: 650, loss is 6.961285543441773 and perplexity is 1054.988919954028
At time: 23.749699115753174 and batch: 700, loss is 6.85253529548645 and perplexity is 946.2769597418461
At time: 25.41053581237793 and batch: 750, loss is 6.728166246414185 and perplexity is 835.6135508109452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.177809604378634 and perplexity of 481.9351706982941
Finished 1 epochs...
Completing Train Step...
At time: 29.782485485076904 and batch: 50, loss is 6.874361448287964 and perplexity is 967.1575882305009
At time: 31.436241626739502 and batch: 100, loss is 6.7765449523925785 and perplexity is 877.0332915697571
At time: 33.09035301208496 and batch: 150, loss is 6.94205717086792 and perplexity is 1034.8969865761433
At time: 34.7714364528656 and batch: 200, loss is 6.900090389251709 and perplexity is 992.3644106277308
At time: 36.42680048942566 and batch: 250, loss is 7.038859634399414 and perplexity is 1140.0867493328626
At time: 38.080681800842285 and batch: 300, loss is 6.8065845489501955 and perplexity is 903.7787175437929
At time: 39.73363924026489 and batch: 350, loss is 6.573869886398316 and perplexity is 716.1358520038473
At time: 41.38777160644531 and batch: 400, loss is 6.813421039581299 and perplexity is 909.978560716622
At time: 43.041826486587524 and batch: 450, loss is 6.723355989456177 and perplexity is 831.6036868837535
At time: 44.696282625198364 and batch: 500, loss is 6.675727624893188 and perplexity is 792.9241954072575
At time: 46.35116457939148 and batch: 550, loss is 6.639482069015503 and perplexity is 764.698829218679
At time: 48.005839109420776 and batch: 600, loss is 6.711581935882569 and perplexity is 821.8697568786727
At time: 49.66095447540283 and batch: 650, loss is 6.625300397872925 and perplexity is 753.9306577069455
At time: 51.31524085998535 and batch: 700, loss is 6.727362947463989 and perplexity is 834.9425728568339
At time: 52.96933937072754 and batch: 750, loss is 6.639396677017212 and perplexity is 764.6335328454948
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 6.6357301224109735 and perplexity of 761.835095689971
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 57.304279088974 and batch: 50, loss is 6.0815722846984865 and perplexity is 437.71686950264393
At time: 58.99397826194763 and batch: 100, loss is 5.948332319259643 and perplexity is 383.1138943515252
At time: 60.650421380996704 and batch: 150, loss is 5.931823596954346 and perplexity is 376.8410938828962
At time: 62.30827021598816 and batch: 200, loss is 5.935359945297241 and perplexity is 378.1760943829209
At time: 63.97214078903198 and batch: 250, loss is 5.97191460609436 and perplexity is 392.2559679256573
At time: 65.63549208641052 and batch: 300, loss is 5.9670326709747314 and perplexity is 390.34566651778556
At time: 67.33232498168945 and batch: 350, loss is 5.884424419403076 and perplexity is 359.3958471762629
At time: 68.98870754241943 and batch: 400, loss is 5.915430021286011 and perplexity is 370.7136832081703
At time: 70.65004563331604 and batch: 450, loss is 5.871117105484009 and perplexity is 354.64493486667726
At time: 72.30840134620667 and batch: 500, loss is 5.869196872711182 and perplexity is 353.9645874617745
At time: 73.96689057350159 and batch: 550, loss is 5.865163974761963 and perplexity is 352.5399590243329
At time: 75.62596130371094 and batch: 600, loss is 5.850181016921997 and perplexity is 347.29724146678006
At time: 77.28560471534729 and batch: 650, loss is 5.844998970031738 and perplexity is 345.5021859246116
At time: 78.945645570755 and batch: 700, loss is 5.86964467048645 and perplexity is 354.12312751085506
At time: 80.60586810112 and batch: 750, loss is 5.855686683654785 and perplexity is 349.2146177094609
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.615724785383358 and perplexity of 274.7124146004209
Finished 3 epochs...
Completing Train Step...
At time: 84.94106435775757 and batch: 50, loss is 5.896468839645386 and perplexity is 363.75073519757785
At time: 86.61909699440002 and batch: 100, loss is 5.875168113708496 and perplexity is 356.084518328106
At time: 88.27703785896301 and batch: 150, loss is 5.858017616271972 and perplexity is 350.0295628744286
At time: 89.93535041809082 and batch: 200, loss is 5.871366243362427 and perplexity is 354.733301360605
At time: 91.59325075149536 and batch: 250, loss is 5.912251586914063 and perplexity is 369.5372646702601
At time: 93.25139689445496 and batch: 300, loss is 5.9136458683013915 and perplexity is 370.0528629613367
At time: 94.90981245040894 and batch: 350, loss is 5.838533296585083 and perplexity is 343.2754879236282
At time: 96.56648778915405 and batch: 400, loss is 5.8694680881500245 and perplexity is 354.0606011423053
At time: 98.22484970092773 and batch: 450, loss is 5.82977972984314 and perplexity is 340.2837164684362
At time: 99.883376121521 and batch: 500, loss is 5.821874923706055 and perplexity is 337.604443177522
At time: 101.55040001869202 and batch: 550, loss is 5.816219501495361 and perplexity is 335.70053628432447
At time: 103.20918774604797 and batch: 600, loss is 5.7978000831604 and perplexity is 329.57372702441484
At time: 104.86880660057068 and batch: 650, loss is 5.78151930809021 and perplexity is 324.25145423002914
At time: 106.53354144096375 and batch: 700, loss is 5.792893810272217 and perplexity is 327.96070857285434
At time: 108.21857261657715 and batch: 750, loss is 5.780228233337402 and perplexity is 323.8330914908098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.571435972701671 and perplexity of 262.8112181441992
Finished 4 epochs...
Completing Train Step...
At time: 112.54009437561035 and batch: 50, loss is 5.82027497291565 and perplexity is 337.0647245585642
At time: 114.22271084785461 and batch: 100, loss is 5.799732332229614 and perplexity is 330.21116119415956
At time: 115.88296747207642 and batch: 150, loss is 5.787339382171631 and perplexity is 326.1441241115451
At time: 117.55171275138855 and batch: 200, loss is 5.7957014560699465 and perplexity is 328.88279992532733
At time: 119.20764470100403 and batch: 250, loss is 5.831221237182617 and perplexity is 340.7745916583235
At time: 120.86313009262085 and batch: 300, loss is 5.835324258804321 and perplexity is 342.1756695368697
At time: 122.51954317092896 and batch: 350, loss is 5.761760444641113 and perplexity is 317.9074951290277
At time: 124.17547488212585 and batch: 400, loss is 5.79680212020874 and perplexity is 329.2449887166135
At time: 125.83375859260559 and batch: 450, loss is 5.758588981628418 and perplexity is 316.9008603629739
At time: 127.49159216880798 and batch: 500, loss is 5.74746844291687 and perplexity is 313.3962746347589
At time: 129.14815998077393 and batch: 550, loss is 5.757609272003174 and perplexity is 316.59054157580067
At time: 130.80513262748718 and batch: 600, loss is 5.735768995285034 and perplexity is 309.75107631366853
At time: 132.46302771568298 and batch: 650, loss is 5.726096591949463 and perplexity is 306.7694818075744
At time: 134.1231837272644 and batch: 700, loss is 5.729293403625488 and perplexity is 307.7517352720169
At time: 135.78247213363647 and batch: 750, loss is 5.721422166824341 and perplexity is 305.3388571131584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.535700243572856 and perplexity of 253.5852969543196
Finished 5 epochs...
Completing Train Step...
At time: 140.13947367668152 and batch: 50, loss is 5.770001192092895 and perplexity is 320.5381147583379
At time: 141.8226671218872 and batch: 100, loss is 5.751832513809204 and perplexity is 314.7669468746876
At time: 143.4818160533905 and batch: 150, loss is 5.7385601043701175 and perplexity is 310.6168330055248
At time: 145.1409695148468 and batch: 200, loss is 5.743191223144532 and perplexity is 312.0586725406179
At time: 146.79927253723145 and batch: 250, loss is 5.774514646530151 and perplexity is 321.988118736614
At time: 148.48085474967957 and batch: 300, loss is 5.7836673545837405 and perplexity is 324.9487100301476
At time: 150.13779091835022 and batch: 350, loss is 5.715135650634766 and perplexity is 303.4253603605912
At time: 151.7935380935669 and batch: 400, loss is 5.747954044342041 and perplexity is 313.5484972691377
At time: 153.45330810546875 and batch: 450, loss is 5.7133003520965575 and perplexity is 302.86899494475136
At time: 155.13408207893372 and batch: 500, loss is 5.701822414398193 and perplexity is 299.41255782170276
At time: 156.82977962493896 and batch: 550, loss is 5.704481573104858 and perplexity is 300.20980286248005
At time: 158.52770948410034 and batch: 600, loss is 5.6863837242126465 and perplexity is 294.8255201415182
At time: 160.22632384300232 and batch: 650, loss is 5.6804261302948 and perplexity is 293.07429115117145
At time: 161.92391109466553 and batch: 700, loss is 5.680944108963013 and perplexity is 293.2261367051734
At time: 163.62397408485413 and batch: 750, loss is 5.6732243728637695 and perplexity is 290.9712231794079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.503951671511628 and perplexity of 245.66078743999307
Finished 6 epochs...
Completing Train Step...
At time: 168.03205919265747 and batch: 50, loss is 5.718703393936157 and perplexity is 304.5098375753089
At time: 169.71914196014404 and batch: 100, loss is 5.700164031982422 and perplexity is 298.9164288002786
At time: 171.37984418869019 and batch: 150, loss is 5.690598869323731 and perplexity is 296.07087532387175
At time: 173.03969812393188 and batch: 200, loss is 5.698049535751343 and perplexity is 298.2850389091956
At time: 174.69931888580322 and batch: 250, loss is 5.732667322158814 and perplexity is 308.7918181461235
At time: 176.35876727104187 and batch: 300, loss is 5.740129261016846 and perplexity is 311.1046220822688
At time: 178.01839876174927 and batch: 350, loss is 5.671312656402588 and perplexity is 290.4155000640876
At time: 179.67701077461243 and batch: 400, loss is 5.70244833946228 and perplexity is 299.60002631063816
At time: 181.33657908439636 and batch: 450, loss is 5.670175800323486 and perplexity is 290.0855270388335
At time: 182.99564790725708 and batch: 500, loss is 5.666069097518921 and perplexity is 288.89667479363726
At time: 184.65412521362305 and batch: 550, loss is 5.672551374435425 and perplexity is 290.7754658830798
At time: 186.3153257369995 and batch: 600, loss is 5.651215267181397 and perplexity is 284.6371659287375
At time: 187.97555541992188 and batch: 650, loss is 5.648125305175781 and perplexity is 283.7590053407705
At time: 189.64783310890198 and batch: 700, loss is 5.646898593902588 and perplexity is 283.41112838596433
At time: 191.3618266582489 and batch: 750, loss is 5.638890476226806 and perplexity is 281.15060208016337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.483498861623365 and perplexity of 240.68736763352143
Finished 7 epochs...
Completing Train Step...
At time: 195.7335114479065 and batch: 50, loss is 5.685487623214722 and perplexity is 294.5614450353597
At time: 197.4192566871643 and batch: 100, loss is 5.666672134399414 and perplexity is 289.07094268289075
At time: 199.08123111724854 and batch: 150, loss is 5.657470655441284 and perplexity is 286.42326245429257
At time: 200.75060105323792 and batch: 200, loss is 5.663115034103393 and perplexity is 288.04451498251984
At time: 202.4207010269165 and batch: 250, loss is 5.6951234245300295 and perplexity is 297.41349944225306
At time: 204.0828893184662 and batch: 300, loss is 5.706542081832886 and perplexity is 300.8290255192763
At time: 205.74276685714722 and batch: 350, loss is 5.638186511993408 and perplexity is 280.95275176004554
At time: 207.42081594467163 and batch: 400, loss is 5.668301219940186 and perplexity is 289.5422477697899
At time: 209.10253763198853 and batch: 450, loss is 5.632860097885132 and perplexity is 279.46025940406713
At time: 210.78311777114868 and batch: 500, loss is 5.627216262817383 and perplexity is 277.887474236849
At time: 212.4631006717682 and batch: 550, loss is 5.631983880996704 and perplexity is 279.21549885245696
At time: 214.15747690200806 and batch: 600, loss is 5.613213739395142 and perplexity is 274.02346444858887
At time: 215.84957098960876 and batch: 650, loss is 5.605439796447754 and perplexity is 271.9014804476868
At time: 217.55057954788208 and batch: 700, loss is 5.605035314559936 and perplexity is 271.79152346287856
At time: 219.2582356929779 and batch: 750, loss is 5.594666423797608 and perplexity is 268.98790716048546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.458194821379905 and perplexity of 234.67341434319073
Finished 8 epochs...
Completing Train Step...
At time: 223.83956742286682 and batch: 50, loss is 5.642978954315185 and perplexity is 282.3024331699217
At time: 225.57139348983765 and batch: 100, loss is 5.626675796508789 and perplexity is 277.73732599813087
At time: 227.25271797180176 and batch: 150, loss is 5.615911254882812 and perplexity is 274.7636448631577
At time: 228.91272163391113 and batch: 200, loss is 5.620115156173706 and perplexity is 275.9211554298689
At time: 230.5716803073883 and batch: 250, loss is 5.6521151256561275 and perplexity is 284.89341437109675
At time: 232.26154446601868 and batch: 300, loss is 5.66340124130249 and perplexity is 288.1269671950138
At time: 233.9195830821991 and batch: 350, loss is 5.591258974075317 and perplexity is 268.07290418956114
At time: 235.57594895362854 and batch: 400, loss is 5.62066162109375 and perplexity is 276.0719778678418
At time: 237.2355649471283 and batch: 450, loss is 5.583963069915772 and perplexity is 266.12418744234435
At time: 238.89358258247375 and batch: 500, loss is 5.583041296005249 and perplexity is 265.87899413315984
At time: 240.55399250984192 and batch: 550, loss is 5.5914466667175295 and perplexity is 268.12322422345636
At time: 242.2260181903839 and batch: 600, loss is 5.5718223762512205 and perplexity is 262.9127889541484
At time: 243.89218187332153 and batch: 650, loss is 5.562706604003906 and perplexity is 260.52702640400446
At time: 245.58053517341614 and batch: 700, loss is 5.562982263565064 and perplexity is 260.59885306917147
At time: 247.27184176445007 and batch: 750, loss is 5.554655513763428 and perplexity is 258.437920877549
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.423936355945676 and perplexity of 226.77001541434677
Finished 9 epochs...
Completing Train Step...
At time: 251.65900325775146 and batch: 50, loss is 5.604675550460815 and perplexity is 271.6937602171986
At time: 253.3466546535492 and batch: 100, loss is 5.585837812423706 and perplexity is 266.6235697292692
At time: 255.01554894447327 and batch: 150, loss is 5.580112066268921 and perplexity is 265.10131303664343
At time: 256.6778874397278 and batch: 200, loss is 5.583841075897217 and perplexity is 266.09172386350514
At time: 258.3413062095642 and batch: 250, loss is 5.615100660324097 and perplexity is 274.5410131918997
At time: 260.0095946788788 and batch: 300, loss is 5.624151630401611 and perplexity is 277.03715489898366
At time: 261.68655920028687 and batch: 350, loss is 5.554637622833252 and perplexity is 258.4332972241127
At time: 263.36647772789 and batch: 400, loss is 5.582486982345581 and perplexity is 265.73165461481943
At time: 265.04651522636414 and batch: 450, loss is 5.548933744430542 and perplexity is 256.9634211039681
At time: 266.7257487773895 and batch: 500, loss is 5.544115209579468 and perplexity is 255.72821224235247
At time: 268.40562629699707 and batch: 550, loss is 5.551261043548584 and perplexity is 257.56214828565174
At time: 270.08811020851135 and batch: 600, loss is 5.534702663421631 and perplexity is 253.3324514332405
At time: 271.7712576389313 and batch: 650, loss is 5.526312589645386 and perplexity is 251.2158650498709
At time: 273.45340394973755 and batch: 700, loss is 5.5278465461730955 and perplexity is 251.6015149754185
At time: 275.1695508956909 and batch: 750, loss is 5.51653673171997 and perplexity is 248.77197944670564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.405642487281977 and perplexity of 222.6592302615725
Finished 10 epochs...
Completing Train Step...
At time: 279.5284261703491 and batch: 50, loss is 5.567817344665527 and perplexity is 261.8619207150338
At time: 281.2137050628662 and batch: 100, loss is 5.546677055358887 and perplexity is 256.38418837964605
At time: 282.8761568069458 and batch: 150, loss is 5.542352781295777 and perplexity is 255.2779065404712
At time: 284.53655672073364 and batch: 200, loss is 5.555202369689941 and perplexity is 258.57928783637334
At time: 286.19662833213806 and batch: 250, loss is 5.582605142593383 and perplexity is 265.76305538810266
At time: 287.85657501220703 and batch: 300, loss is 5.592940196990967 and perplexity is 268.5239735669904
At time: 289.51650047302246 and batch: 350, loss is 5.524951410293579 and perplexity is 250.87414782349654
At time: 291.1755201816559 and batch: 400, loss is 5.553060636520386 and perplexity is 258.0260726298006
At time: 292.8369126319885 and batch: 450, loss is 5.52204683303833 and perplexity is 250.14652171447733
At time: 294.5016841888428 and batch: 500, loss is 5.520199756622315 and perplexity is 249.68490842242292
At time: 296.1686019897461 and batch: 550, loss is 5.526438522338867 and perplexity is 251.2475033325071
At time: 297.83775997161865 and batch: 600, loss is 5.50780047416687 and perplexity is 246.6081091888002
At time: 299.5119881629944 and batch: 650, loss is 5.498971748352051 and perplexity is 244.4404566938717
At time: 301.1926233768463 and batch: 700, loss is 5.503848505020142 and perplexity is 245.63544478573593
At time: 302.8752670288086 and batch: 750, loss is 5.494989652633667 and perplexity is 243.46900688463083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.402802933094113 and perplexity of 222.0278741210255
Finished 11 epochs...
Completing Train Step...
At time: 307.25735354423523 and batch: 50, loss is 5.5430037212371825 and perplexity is 255.44413122126943
At time: 308.94310569763184 and batch: 100, loss is 5.526032285690308 and perplexity is 251.14545811745086
At time: 310.6028070449829 and batch: 150, loss is 5.518501310348511 and perplexity is 249.26119195177677
At time: 312.26309156417847 and batch: 200, loss is 5.528918495178223 and perplexity is 251.8713635752143
At time: 313.92369318008423 and batch: 250, loss is 5.559674835205078 and perplexity is 259.7383648179858
At time: 315.6094410419464 and batch: 300, loss is 5.572456769943237 and perplexity is 263.0796320855309
At time: 317.27008605003357 and batch: 350, loss is 5.5049042129516605 and perplexity is 245.8949010039547
At time: 318.9456412792206 and batch: 400, loss is 5.528911457061768 and perplexity is 251.869590881464
At time: 320.625896692276 and batch: 450, loss is 5.505043287277221 and perplexity is 245.92910104958963
At time: 322.3051850795746 and batch: 500, loss is 5.497294025421143 and perplexity is 244.03069716203052
At time: 323.9849133491516 and batch: 550, loss is 5.499556541442871 and perplexity is 244.58344558944742
At time: 325.6654841899872 and batch: 600, loss is 5.482966823577881 and perplexity is 240.55934685585507
At time: 327.3469421863556 and batch: 650, loss is 5.4768323230743405 and perplexity is 239.08815255619677
At time: 329.0275413990021 and batch: 700, loss is 5.479512929916382 and perplexity is 239.729913664109
At time: 330.709477186203 and batch: 750, loss is 5.472538356781006 and perplexity is 238.06371710768468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.392393954964572 and perplexity of 219.72877721812455
Finished 12 epochs...
Completing Train Step...
At time: 335.1038405895233 and batch: 50, loss is 5.520795297622681 and perplexity is 249.83365030911094
At time: 336.7894105911255 and batch: 100, loss is 5.50695613861084 and perplexity is 246.39997707284454
At time: 338.4508719444275 and batch: 150, loss is 5.501290616989135 and perplexity is 245.0079397075282
At time: 340.11359548568726 and batch: 200, loss is 5.508502130508423 and perplexity is 246.7812040519436
At time: 341.77411556243896 and batch: 250, loss is 5.534729480743408 and perplexity is 253.33924522220246
At time: 343.434396982193 and batch: 300, loss is 5.548878612518311 and perplexity is 256.94925460970575
At time: 345.10115790367126 and batch: 350, loss is 5.4833205032348635 and perplexity is 240.6444428506091
At time: 346.7786660194397 and batch: 400, loss is 5.507906427383423 and perplexity is 246.63423949546745
At time: 348.47292041778564 and batch: 450, loss is 5.479888772964477 and perplexity is 239.8200314195853
At time: 350.15511083602905 and batch: 500, loss is 5.475771379470825 and perplexity is 238.83462802146337
At time: 351.83507800102234 and batch: 550, loss is 5.481892108917236 and perplexity is 240.30095307369763
At time: 353.5207476615906 and batch: 600, loss is 5.463861379623413 and perplexity is 236.00697970829208
At time: 355.22092938423157 and batch: 650, loss is 5.456963348388672 and perplexity is 234.38459824277982
At time: 356.91993379592896 and batch: 700, loss is 5.4563414669036865 and perplexity is 234.23888411392244
At time: 358.6530032157898 and batch: 750, loss is 5.454024705886841 and perplexity is 233.69683673814058
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.382810370866642 and perplexity of 217.63304634962225
Finished 13 epochs...
Completing Train Step...
At time: 363.21949005126953 and batch: 50, loss is 5.500105218887329 and perplexity is 244.71767983161308
At time: 364.9502503871918 and batch: 100, loss is 5.484949531555176 and perplexity is 241.03677893967574
At time: 366.63541769981384 and batch: 150, loss is 5.480663919448853 and perplexity is 240.00599914062406
At time: 368.3173930644989 and batch: 200, loss is 5.490381717681885 and perplexity is 242.34969836907533
At time: 370.0028123855591 and batch: 250, loss is 5.518723773956299 and perplexity is 249.31664966425248
At time: 371.6901948451996 and batch: 300, loss is 5.528785848617554 and perplexity is 251.8379559208593
At time: 373.3719596862793 and batch: 350, loss is 5.465944213867187 and perplexity is 236.49905540556114
At time: 375.03681540489197 and batch: 400, loss is 5.491837968826294 and perplexity is 242.7028774910082
At time: 376.7153367996216 and batch: 450, loss is 5.463700180053711 and perplexity is 235.9689385509078
At time: 378.3893024921417 and batch: 500, loss is 5.4594842720031735 and perplexity is 234.9762093011994
At time: 380.06486535072327 and batch: 550, loss is 5.463786487579346 and perplexity is 235.9893053250111
At time: 381.7390058040619 and batch: 600, loss is 5.443939695358276 and perplexity is 231.3518461815849
At time: 383.406366109848 and batch: 650, loss is 5.439137268066406 and perplexity is 230.24345936526097
At time: 385.06909680366516 and batch: 700, loss is 5.437185754776001 and perplexity is 229.7945743392892
At time: 386.7351350784302 and batch: 750, loss is 5.431573905944824 and perplexity is 228.50861360936068
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.373021680255269 and perplexity of 215.51309648625832
Finished 14 epochs...
Completing Train Step...
At time: 391.10522270202637 and batch: 50, loss is 5.480793142318726 and perplexity is 240.03701540858216
At time: 392.7917332649231 and batch: 100, loss is 5.467907209396362 and perplexity is 236.96375794926223
At time: 394.4765627384186 and batch: 150, loss is 5.461654987335205 and perplexity is 235.48682976672498
At time: 396.14899802207947 and batch: 200, loss is 5.473915348052978 and perplexity is 238.39175456884297
At time: 397.81976890563965 and batch: 250, loss is 5.501187582015991 and perplexity is 244.9826966215229
At time: 399.52273869514465 and batch: 300, loss is 5.513129196166992 and perplexity is 247.92572272593253
At time: 401.1849172115326 and batch: 350, loss is 5.449913768768311 and perplexity is 232.7380957496798
At time: 402.85260820388794 and batch: 400, loss is 5.474764099121094 and perplexity is 238.59417571559385
At time: 404.5344669818878 and batch: 450, loss is 5.449788780212402 and perplexity is 232.7090079690448
At time: 406.2211148738861 and batch: 500, loss is 5.442286605834961 and perplexity is 230.96971680251963
At time: 407.9036765098572 and batch: 550, loss is 5.446691055297851 and perplexity is 231.9892548513576
At time: 409.5890784263611 and batch: 600, loss is 5.423145151138305 and perplexity is 226.59066484885446
At time: 411.2888011932373 and batch: 650, loss is 5.4213427734375 and perplexity is 226.18263071359397
At time: 412.9756407737732 and batch: 700, loss is 5.422372970581055 and perplexity is 226.41576347941276
At time: 414.6933481693268 and batch: 750, loss is 5.413829822540283 and perplexity is 224.4896991341927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.363287282544513 and perplexity of 213.42538409025047
Finished 15 epochs...
Completing Train Step...
At time: 419.06953477859497 and batch: 50, loss is 5.462730121612549 and perplexity is 235.7401458792871
At time: 420.75745368003845 and batch: 100, loss is 5.449773721694946 and perplexity is 232.7055037427704
At time: 422.42361521720886 and batch: 150, loss is 5.445300626754761 and perplexity is 231.666914517254
At time: 424.0854015350342 and batch: 200, loss is 5.4602876472473145 and perplexity is 235.1650592192314
At time: 425.7474436759949 and batch: 250, loss is 5.485999259948731 and perplexity is 241.28993493969355
At time: 427.4088644981384 and batch: 300, loss is 5.499333162307739 and perplexity is 244.52881685259112
At time: 429.0852220058441 and batch: 350, loss is 5.435965480804444 and perplexity is 229.51433302178546
At time: 430.7498776912689 and batch: 400, loss is 5.459346542358398 and perplexity is 234.94384833994468
At time: 432.4109916687012 and batch: 450, loss is 5.434964647293091 and perplexity is 229.284742296172
At time: 434.0833303928375 and batch: 500, loss is 5.433928556442261 and perplexity is 229.0473054967322
At time: 435.7498140335083 and batch: 550, loss is 5.435944585800171 and perplexity is 229.50953736891913
At time: 437.41737723350525 and batch: 600, loss is 5.414582786560058 and perplexity is 224.6587954541842
At time: 439.1183865070343 and batch: 650, loss is 5.411627254486084 and perplexity is 223.99578942892782
At time: 440.8209047317505 and batch: 700, loss is 5.40956259727478 and perplexity is 223.53379200425616
At time: 442.5679850578308 and batch: 750, loss is 5.402357521057129 and perplexity is 221.9290022543404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.370354231013808 and perplexity of 214.9389922778384
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 446.9547743797302 and batch: 50, loss is 5.43400502204895 and perplexity is 229.06482040754312
At time: 448.6665184497833 and batch: 100, loss is 5.396123638153076 and perplexity is 220.5498261189178
At time: 450.3286249637604 and batch: 150, loss is 5.369445600509644 and perplexity is 214.7437808538784
At time: 451.99659395217896 and batch: 200, loss is 5.365530767440796 and perplexity is 213.90473822676492
At time: 453.672660112381 and batch: 250, loss is 5.385474424362183 and perplexity is 218.2136054043236
At time: 455.3458092212677 and batch: 300, loss is 5.3886459445953365 and perplexity is 218.9067728853373
At time: 457.0104629993439 and batch: 350, loss is 5.312721157073975 and perplexity is 202.90160499449152
At time: 458.6756103038788 and batch: 400, loss is 5.327820205688477 and perplexity is 205.988471923749
At time: 460.3577928543091 and batch: 450, loss is 5.29726390838623 and perplexity is 199.78941930608883
At time: 462.0207850933075 and batch: 500, loss is 5.258277206420899 and perplexity is 192.1501709002163
At time: 463.6825203895569 and batch: 550, loss is 5.257264432907104 and perplexity is 191.95566480839204
At time: 465.37225246429443 and batch: 600, loss is 5.225367679595947 and perplexity is 185.9295204507016
At time: 467.07905101776123 and batch: 650, loss is 5.215999002456665 and perplexity is 184.19574109388753
At time: 468.806756734848 and batch: 700, loss is 5.2098931884765625 and perplexity is 183.07450268282568
At time: 470.49855852127075 and batch: 750, loss is 5.217249774932862 and perplexity is 184.4262721980062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.195275595021802 and perplexity of 180.41785820960175
Finished 17 epochs...
Completing Train Step...
At time: 475.20259261131287 and batch: 50, loss is 5.321034412384034 and perplexity is 204.5954085956058
At time: 476.9318423271179 and batch: 100, loss is 5.310658521652222 and perplexity is 202.48352427917143
At time: 478.6036431789398 and batch: 150, loss is 5.297499179840088 and perplexity is 199.8364295831056
At time: 480.2739553451538 and batch: 200, loss is 5.302403354644776 and perplexity is 200.8188694254382
At time: 481.9487805366516 and batch: 250, loss is 5.326756677627563 and perplexity is 205.76951385829526
At time: 483.6860463619232 and batch: 300, loss is 5.334404382705689 and perplexity is 207.3492112448576
At time: 485.3640294075012 and batch: 350, loss is 5.267625932693481 and perplexity is 193.95495331496625
At time: 487.04540276527405 and batch: 400, loss is 5.284644231796265 and perplexity is 197.28398359299092
At time: 488.7190852165222 and batch: 450, loss is 5.25772292137146 and perplexity is 192.04369444511892
At time: 490.3997461795807 and batch: 500, loss is 5.229060831069947 and perplexity is 186.6174558761626
At time: 492.0800678730011 and batch: 550, loss is 5.240309972763061 and perplexity is 188.72859406355772
At time: 493.7607727050781 and batch: 600, loss is 5.21782299041748 and perplexity is 184.53201849781337
At time: 495.4380648136139 and batch: 650, loss is 5.21826828956604 and perplexity is 184.61420874679817
At time: 497.1178033351898 and batch: 700, loss is 5.216453218460083 and perplexity is 184.27942475104328
At time: 498.8128092288971 and batch: 750, loss is 5.217195472717285 and perplexity is 184.4162577147221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.188411623932595 and perplexity of 179.1837156517486
Finished 18 epochs...
Completing Train Step...
At time: 503.33023476600647 and batch: 50, loss is 5.297457265853882 and perplexity is 199.82805381728465
At time: 505.0381293296814 and batch: 100, loss is 5.287309083938599 and perplexity is 197.8104173618589
At time: 506.71781754493713 and batch: 150, loss is 5.276131248474121 and perplexity is 195.61163676097487
At time: 508.39227414131165 and batch: 200, loss is 5.282701988220214 and perplexity is 196.90118191059125
At time: 510.069180727005 and batch: 250, loss is 5.309620037078857 and perplexity is 202.27335740926094
At time: 511.7493770122528 and batch: 300, loss is 5.319822244644165 and perplexity is 204.3475548920614
At time: 513.4260296821594 and batch: 350, loss is 5.25484034538269 and perplexity is 191.49091100561924
At time: 515.1055183410645 and batch: 400, loss is 5.27343490600586 and perplexity is 195.08491123281493
At time: 516.7848126888275 and batch: 450, loss is 5.248270339965821 and perplexity is 190.23693849675632
At time: 518.4618339538574 and batch: 500, loss is 5.224480648040771 and perplexity is 185.7646682243956
At time: 520.1389997005463 and batch: 550, loss is 5.239255304336548 and perplexity is 188.52965290114065
At time: 521.8192121982574 and batch: 600, loss is 5.218639354705811 and perplexity is 184.68272535524684
At time: 523.4939339160919 and batch: 650, loss is 5.219797105789184 and perplexity is 184.89666580160207
At time: 525.1772000789642 and batch: 700, loss is 5.216680955886841 and perplexity is 184.3213968521682
At time: 526.9220116138458 and batch: 750, loss is 5.21322416305542 and perplexity is 183.68533596758482
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.184777193291243 and perplexity of 178.53366685932312
Finished 19 epochs...
Completing Train Step...
At time: 531.3818917274475 and batch: 50, loss is 5.28513952255249 and perplexity is 197.3817207285677
At time: 533.0963592529297 and batch: 100, loss is 5.274752397537231 and perplexity is 195.34210333827045
At time: 534.7742047309875 and batch: 150, loss is 5.265659999847412 and perplexity is 193.57402546356116
At time: 536.4501028060913 and batch: 200, loss is 5.273302040100098 and perplexity is 195.05899282125822
At time: 538.1288278102875 and batch: 250, loss is 5.300321006774903 and perplexity is 200.40112977104886
At time: 539.8088569641113 and batch: 300, loss is 5.3117084789276126 and perplexity is 202.6962349776666
At time: 541.4844777584076 and batch: 350, loss is 5.2471592903137205 and perplexity is 190.02569318616798
At time: 543.158774137497 and batch: 400, loss is 5.267251300811767 and perplexity is 193.88230521483501
At time: 544.8337633609772 and batch: 450, loss is 5.243128080368042 and perplexity is 189.26120167005323
At time: 546.5138323307037 and batch: 500, loss is 5.220084629058838 and perplexity is 184.94983553890344
At time: 548.2038943767548 and batch: 550, loss is 5.235621862411499 and perplexity is 187.84588432513377
At time: 549.8958690166473 and batch: 600, loss is 5.217321443557739 and perplexity is 184.43949024897998
At time: 551.5923545360565 and batch: 650, loss is 5.216887559890747 and perplexity is 184.35948232493348
At time: 553.2932381629944 and batch: 700, loss is 5.213112564086914 and perplexity is 183.66483801735743
At time: 554.9901127815247 and batch: 750, loss is 5.2072727394104 and perplexity is 182.59539328827557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.182164036950399 and perplexity of 178.06773951135287
Finished 20 epochs...
Completing Train Step...
At time: 559.4683156013489 and batch: 50, loss is 5.276071405410766 and perplexity is 195.59993111165778
At time: 561.1722860336304 and batch: 100, loss is 5.266545295715332 and perplexity is 193.74547162753333
At time: 562.8489809036255 and batch: 150, loss is 5.257655372619629 and perplexity is 192.030722571384
At time: 564.5311789512634 and batch: 200, loss is 5.265961427688598 and perplexity is 193.63238285899632
At time: 566.2055349349976 and batch: 250, loss is 5.293277006149292 and perplexity is 198.99446418053486
At time: 567.9277818202972 and batch: 300, loss is 5.3058891868591305 and perplexity is 201.5201118063046
At time: 569.6046576499939 and batch: 350, loss is 5.242076721191406 and perplexity is 189.06232473276202
At time: 571.2915818691254 and batch: 400, loss is 5.26254620552063 and perplexity is 192.97221320730495
At time: 572.9657936096191 and batch: 450, loss is 5.238573970794678 and perplexity is 188.4012450742292
At time: 574.6402449607849 and batch: 500, loss is 5.216047277450562 and perplexity is 184.20463335679986
At time: 576.3217940330505 and batch: 550, loss is 5.232197685241699 and perplexity is 187.20376672619983
At time: 578.0035238265991 and batch: 600, loss is 5.214108800888061 and perplexity is 183.84790286089333
At time: 579.6989243030548 and batch: 650, loss is 5.2145343589782716 and perplexity is 183.9261574730847
At time: 581.4018063545227 and batch: 700, loss is 5.21128589630127 and perplexity is 183.32964960653402
At time: 583.1033940315247 and batch: 750, loss is 5.2040791416168215 and perplexity is 182.0131872040154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.179955415947493 and perplexity of 177.6748893501803
Finished 21 epochs...
Completing Train Step...
At time: 587.5807628631592 and batch: 50, loss is 5.268886413574219 and perplexity is 194.19958396908586
At time: 589.2902491092682 and batch: 100, loss is 5.2599014472961425 and perplexity is 192.46252266055635
At time: 590.9692130088806 and batch: 150, loss is 5.251851005554199 and perplexity is 190.91933434263925
At time: 592.6473331451416 and batch: 200, loss is 5.260916538238526 and perplexity is 192.6579888152347
At time: 594.3390617370605 and batch: 250, loss is 5.287590246200562 and perplexity is 197.86604200565316
At time: 596.020124912262 and batch: 300, loss is 5.300969858169555 and perplexity is 200.5312025179689
At time: 597.6925623416901 and batch: 350, loss is 5.23808783531189 and perplexity is 188.3096788026073
At time: 599.3594264984131 and batch: 400, loss is 5.259359092712402 and perplexity is 192.35816803035715
At time: 601.026299238205 and batch: 450, loss is 5.234840402603149 and perplexity is 187.699147658241
At time: 602.7054536342621 and batch: 500, loss is 5.213264627456665 and perplexity is 183.69276883510446
At time: 604.3921556472778 and batch: 550, loss is 5.229573373794556 and perplexity is 186.71312981185233
At time: 606.0831389427185 and batch: 600, loss is 5.21163761138916 and perplexity is 183.39414075094777
At time: 607.7738358974457 and batch: 650, loss is 5.2115593528747555 and perplexity is 183.3797891595165
At time: 609.457980632782 and batch: 700, loss is 5.208617286682129 and perplexity is 182.84106654879685
At time: 611.1699571609497 and batch: 750, loss is 5.200418605804443 and perplexity is 181.34813937224263
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.17853670342024 and perplexity of 177.42299848149565
Finished 22 epochs...
Completing Train Step...
At time: 615.6064875125885 and batch: 50, loss is 5.263005237579346 and perplexity is 193.06081397334873
At time: 617.3068945407867 and batch: 100, loss is 5.254945278167725 and perplexity is 191.51100573449935
At time: 618.9759654998779 and batch: 150, loss is 5.24627685546875 and perplexity is 189.85808185697593
At time: 620.6556072235107 and batch: 200, loss is 5.255258436203003 and perplexity is 191.57098833631602
At time: 622.3389415740967 and batch: 250, loss is 5.282862253189087 and perplexity is 196.9327408012059
At time: 624.0157492160797 and batch: 300, loss is 5.296711483001709 and perplexity is 199.67908103894123
At time: 625.6943299770355 and batch: 350, loss is 5.233885259628296 and perplexity is 187.5199537275045
At time: 627.3750021457672 and batch: 400, loss is 5.255337076187134 and perplexity is 191.5860540681754
At time: 629.0544526576996 and batch: 450, loss is 5.230768480300903 and perplexity is 186.9364052805187
At time: 630.744472026825 and batch: 500, loss is 5.210061664581299 and perplexity is 183.10534896027164
At time: 632.4449074268341 and batch: 550, loss is 5.2261613178253175 and perplexity is 186.07713979652962
At time: 634.1497752666473 and batch: 600, loss is 5.209074802398682 and perplexity is 182.92473834949664
At time: 635.8513798713684 and batch: 650, loss is 5.208688449859619 and perplexity is 182.85407856305085
At time: 637.5471739768982 and batch: 700, loss is 5.2046410274505615 and perplexity is 182.11548657305084
At time: 639.2485342025757 and batch: 750, loss is 5.1962254619598385 and perplexity is 180.58931258467632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.176902948423874 and perplexity of 177.1333694271357
Finished 23 epochs...
Completing Train Step...
At time: 643.7264678478241 and batch: 50, loss is 5.257816429138184 and perplexity is 192.06165286171245
At time: 645.4328718185425 and batch: 100, loss is 5.250184345245361 and perplexity is 190.60140168236845
At time: 647.1184754371643 and batch: 150, loss is 5.241407690048217 and perplexity is 188.93587845247836
At time: 648.7964913845062 and batch: 200, loss is 5.251239318847656 and perplexity is 190.8025872337672
At time: 650.4788331985474 and batch: 250, loss is 5.278082304000854 and perplexity is 195.9936584775773
At time: 652.1869690418243 and batch: 300, loss is 5.291804971694947 and perplexity is 198.70175296638922
At time: 653.8681807518005 and batch: 350, loss is 5.228834733963013 and perplexity is 186.57526697885922
At time: 655.5502297878265 and batch: 400, loss is 5.252053413391113 and perplexity is 190.9579818232732
At time: 657.2260262966156 and batch: 450, loss is 5.226920785903931 and perplexity is 186.2185131218334
At time: 658.90425157547 and batch: 500, loss is 5.206297826766968 and perplexity is 182.41746547685554
At time: 660.5858504772186 and batch: 550, loss is 5.222234296798706 and perplexity is 185.34784387342506
At time: 662.2684552669525 and batch: 600, loss is 5.205555028915406 and perplexity is 182.28201648724894
At time: 663.9673178195953 and batch: 650, loss is 5.2047141456604 and perplexity is 182.12880301824438
At time: 665.6722555160522 and batch: 700, loss is 5.2014241123199465 and perplexity is 181.53057781410575
At time: 667.3717248439789 and batch: 750, loss is 5.192110443115235 and perplexity is 179.84771105886801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.175994518191316 and perplexity of 176.972529186283
Finished 24 epochs...
Completing Train Step...
At time: 671.8571140766144 and batch: 50, loss is 5.2523585796356205 and perplexity is 191.0162646459676
At time: 673.5636270046234 and batch: 100, loss is 5.2451693725585935 and perplexity is 189.64793366520894
At time: 675.2382488250732 and batch: 150, loss is 5.236304216384887 and perplexity is 187.97410545181032
At time: 676.9168746471405 and batch: 200, loss is 5.246200799942017 and perplexity is 189.84364264965342
At time: 678.6001596450806 and batch: 250, loss is 5.272414855957031 and perplexity is 194.88601631821572
At time: 680.282829284668 and batch: 300, loss is 5.287610321044922 and perplexity is 197.8700141755208
At time: 681.9558417797089 and batch: 350, loss is 5.2255110263824465 and perplexity is 185.95617476033277
At time: 683.6333427429199 and batch: 400, loss is 5.24923433303833 and perplexity is 190.42041400795503
At time: 685.3117804527283 and batch: 450, loss is 5.22330717086792 and perplexity is 185.5468054801912
At time: 687.0004577636719 and batch: 500, loss is 5.201197271347046 and perplexity is 181.4894039113638
At time: 688.6900796890259 and batch: 550, loss is 5.2183900070190425 and perplexity is 184.63668088567297
At time: 690.3883240222931 and batch: 600, loss is 5.201693019866943 and perplexity is 181.5793993204322
At time: 692.0907797813416 and batch: 650, loss is 5.201001787185669 and perplexity is 181.45392907493877
At time: 693.7882351875305 and batch: 700, loss is 5.196585350036621 and perplexity is 180.65431622138516
At time: 695.5315370559692 and batch: 750, loss is 5.187160396575928 and perplexity is 178.95965628868996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.173855094022529 and perplexity of 176.59431460510012
Finished 25 epochs...
Completing Train Step...
At time: 699.9729783535004 and batch: 50, loss is 5.24716775894165 and perplexity is 190.02730244987475
At time: 701.680303812027 and batch: 100, loss is 5.2396078777313235 and perplexity is 188.59613516012905
At time: 703.3627095222473 and batch: 150, loss is 5.230085735321045 and perplexity is 186.80881894768086
At time: 705.0432085990906 and batch: 200, loss is 5.241540021896363 and perplexity is 188.9608823408238
At time: 706.723872423172 and batch: 250, loss is 5.267894868850708 and perplexity is 194.0071218294713
At time: 708.4079167842865 and batch: 300, loss is 5.283522701263427 and perplexity is 197.0628476103091
At time: 710.0831959247589 and batch: 350, loss is 5.221407299041748 and perplexity is 185.19462498685502
At time: 711.7572667598724 and batch: 400, loss is 5.244576549530029 and perplexity is 189.5355393210819
At time: 713.4266576766968 and batch: 450, loss is 5.2190092754364015 and perplexity is 184.75105596161242
At time: 715.1005256175995 and batch: 500, loss is 5.197358703613281 and perplexity is 180.79407991938612
At time: 716.7763404846191 and batch: 550, loss is 5.214985666275024 and perplexity is 184.00918342371537
At time: 718.4642758369446 and batch: 600, loss is 5.198680334091186 and perplexity is 181.03318085236933
At time: 720.1571834087372 and batch: 650, loss is 5.198303394317627 and perplexity is 180.96495510547868
At time: 721.8473746776581 and batch: 700, loss is 5.19455620765686 and perplexity is 180.28811455556394
At time: 723.5372018814087 and batch: 750, loss is 5.183935623168946 and perplexity is 178.3834814641048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.172440284906432 and perplexity of 176.34464401870352
Finished 26 epochs...
Completing Train Step...
At time: 728.0269782543182 and batch: 50, loss is 5.242792987823487 and perplexity is 189.19779227700087
At time: 729.7358629703522 and batch: 100, loss is 5.235648279190063 and perplexity is 187.8508466738086
At time: 731.4171125888824 and batch: 150, loss is 5.225811834335327 and perplexity is 186.01212027059333
At time: 733.092981338501 and batch: 200, loss is 5.237662782669068 and perplexity is 188.2296542844845
At time: 734.7725772857666 and batch: 250, loss is 5.263821239471436 and perplexity is 193.21841625597466
At time: 736.5065569877625 and batch: 300, loss is 5.278762645721436 and perplexity is 196.12704650998572
At time: 738.1805202960968 and batch: 350, loss is 5.217248725891113 and perplexity is 184.42607872724858
At time: 739.8539028167725 and batch: 400, loss is 5.240852108001709 and perplexity is 188.83093822462152
At time: 741.5337862968445 and batch: 450, loss is 5.215336208343506 and perplexity is 184.07369769031396
At time: 743.2197072505951 and batch: 500, loss is 5.193228349685669 and perplexity is 180.04887641782665
At time: 744.9028444290161 and batch: 550, loss is 5.211620960235596 and perplexity is 183.39108705237118
At time: 746.5979399681091 and batch: 600, loss is 5.195889797210693 and perplexity is 180.52870529080437
At time: 748.2929828166962 and batch: 650, loss is 5.195100994110107 and perplexity is 180.3863598369746
At time: 750.0021529197693 and batch: 700, loss is 5.1916814708709715 and perplexity is 179.77057792779848
At time: 751.6872839927673 and batch: 750, loss is 5.181051273345947 and perplexity is 177.86970241636814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.170465158861737 and perplexity of 175.99668486417337
Finished 27 epochs...
Completing Train Step...
At time: 756.1775734424591 and batch: 50, loss is 5.239541864395141 and perplexity is 188.5836857109754
At time: 757.8819732666016 and batch: 100, loss is 5.2318456268310545 and perplexity is 187.13787166575526
At time: 759.5481934547424 and batch: 150, loss is 5.221997661590576 and perplexity is 185.30398923679383
At time: 761.2455816268921 and batch: 200, loss is 5.233148822784424 and perplexity is 187.3819079618513
At time: 762.9431834220886 and batch: 250, loss is 5.259276485443115 and perplexity is 192.34227850367532
At time: 764.6311237812042 and batch: 300, loss is 5.275537595748902 and perplexity is 195.49554584198182
At time: 766.3166854381561 and batch: 350, loss is 5.214357433319091 and perplexity is 183.89361909495514
At time: 767.9965305328369 and batch: 400, loss is 5.23795669555664 and perplexity is 188.28498553658778
At time: 769.674565076828 and batch: 450, loss is 5.21129207611084 and perplexity is 183.3307825523578
At time: 771.3546528816223 and batch: 500, loss is 5.189717836380005 and perplexity is 179.41792057890186
At time: 773.0303597450256 and batch: 550, loss is 5.208009862899781 and perplexity is 182.73003826061034
At time: 774.7143943309784 and batch: 600, loss is 5.192197370529175 and perplexity is 179.86334543481146
At time: 776.4041004180908 and batch: 650, loss is 5.191498403549194 and perplexity is 179.73767082176266
At time: 778.1075747013092 and batch: 700, loss is 5.187889213562012 and perplexity is 179.09013266694177
At time: 779.8584892749786 and batch: 750, loss is 5.17740758895874 and perplexity is 177.22278066379576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.170962311500727 and perplexity of 176.0842038338478
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 784.4028651714325 and batch: 50, loss is 5.235677108764649 and perplexity is 187.85626241187006
At time: 786.11780834198 and batch: 100, loss is 5.226329984664917 and perplexity is 186.1085274865777
At time: 787.7997608184814 and batch: 150, loss is 5.20998031616211 and perplexity is 183.09045423542798
At time: 789.4819502830505 and batch: 200, loss is 5.221115007400512 and perplexity is 185.14050205619506
At time: 791.1648168563843 and batch: 250, loss is 5.240621547698975 and perplexity is 188.7874063248957
At time: 792.8515298366547 and batch: 300, loss is 5.248567771911621 and perplexity is 190.2935294550988
At time: 794.5339484214783 and batch: 350, loss is 5.186403026580811 and perplexity is 178.82416892818756
At time: 796.2175076007843 and batch: 400, loss is 5.20564058303833 and perplexity is 182.29761213242082
At time: 797.8997688293457 and batch: 450, loss is 5.1706641578674315 and perplexity is 176.03171151448555
At time: 799.5862736701965 and batch: 500, loss is 5.1371836090087895 and perplexity is 170.23564239789332
At time: 801.2719790935516 and batch: 550, loss is 5.150343742370605 and perplexity is 172.49077251442066
At time: 802.9576861858368 and batch: 600, loss is 5.135433845520019 and perplexity is 169.93803073727938
At time: 804.6499543190002 and batch: 650, loss is 5.128932418823243 and perplexity is 168.83677483375
At time: 806.336635351181 and batch: 700, loss is 5.125057182312012 and perplexity is 168.18375851304023
At time: 808.0279214382172 and batch: 750, loss is 5.131368808746338 and perplexity is 169.24862856501298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.127565605695858 and perplexity of 168.60616415027846
Finished 29 epochs...
Completing Train Step...
At time: 812.5048813819885 and batch: 50, loss is 5.213630332946777 and perplexity is 183.75995857424073
At time: 814.2349648475647 and batch: 100, loss is 5.204839429855347 and perplexity is 182.1516223081254
At time: 815.9038474559784 and batch: 150, loss is 5.190729303359985 and perplexity is 179.5994876902734
At time: 817.5681850910187 and batch: 200, loss is 5.202667102813721 and perplexity is 181.75635888945345
At time: 819.2476480007172 and batch: 250, loss is 5.223403596878052 and perplexity is 185.564697880969
At time: 820.9848771095276 and batch: 300, loss is 5.234261503219605 and perplexity is 187.59052018259283
At time: 822.6689352989197 and batch: 350, loss is 5.175218133926392 and perplexity is 176.83518382258757
At time: 824.3540506362915 and batch: 400, loss is 5.19605673789978 and perplexity is 180.55884539300027
At time: 826.0415272712708 and batch: 450, loss is 5.163871517181397 and perplexity is 174.84004321805506
At time: 827.7410061359406 and batch: 500, loss is 5.1344622611999515 and perplexity is 169.77300179398063
At time: 829.4392418861389 and batch: 550, loss is 5.151546010971069 and perplexity is 172.69827746743437
At time: 831.1307408809662 and batch: 600, loss is 5.1389994812011714 and perplexity is 170.54504940384538
At time: 832.8237583637238 and batch: 650, loss is 5.134294891357422 and perplexity is 169.74458929116904
At time: 834.5281178951263 and batch: 700, loss is 5.130686979293824 and perplexity is 169.13326919743614
At time: 836.2345924377441 and batch: 750, loss is 5.134021549224854 and perplexity is 169.69819728387418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1242555130359735 and perplexity of 168.04898479030604
Finished 30 epochs...
Completing Train Step...
At time: 840.7811503410339 and batch: 50, loss is 5.207041206359864 and perplexity is 182.5531213137221
At time: 842.4982123374939 and batch: 100, loss is 5.197349176406861 and perplexity is 180.79235746507229
At time: 844.1816403865814 and batch: 150, loss is 5.183395586013794 and perplexity is 178.2871737634578
At time: 845.8670825958252 and batch: 200, loss is 5.196065502166748 and perplexity is 180.56042786585934
At time: 847.550235748291 and batch: 250, loss is 5.217177057266236 and perplexity is 184.41286163742566
At time: 849.2361829280853 and batch: 300, loss is 5.228941020965576 and perplexity is 186.59509855863968
At time: 850.9170887470245 and batch: 350, loss is 5.171618776321411 and perplexity is 176.19983486884368
At time: 852.6002678871155 and batch: 400, loss is 5.193281602859497 and perplexity is 180.05846484724495
At time: 854.2813677787781 and batch: 450, loss is 5.162455329895019 and perplexity is 174.59261221737236
At time: 855.9689910411835 and batch: 500, loss is 5.134747409820557 and perplexity is 169.8214192340133
At time: 857.6546788215637 and batch: 550, loss is 5.153429775238037 and perplexity is 173.0239069197646
At time: 859.3378064632416 and batch: 600, loss is 5.141451110839844 and perplexity is 170.96367565027037
At time: 861.0211992263794 and batch: 650, loss is 5.136529016494751 and perplexity is 170.12424388505116
At time: 862.7090697288513 and batch: 700, loss is 5.132937479019165 and perplexity is 169.51433220357399
At time: 864.4773347377777 and batch: 750, loss is 5.134815769195557 and perplexity is 169.83302851689083
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.123266530591388 and perplexity of 167.88286945063493
Finished 31 epochs...
Completing Train Step...
At time: 868.9851076602936 and batch: 50, loss is 5.203115978240967 and perplexity is 181.83796316641173
At time: 870.6975338459015 and batch: 100, loss is 5.193214159011841 and perplexity is 180.04632142107693
At time: 872.3852624893188 and batch: 150, loss is 5.179028272628784 and perplexity is 177.5102356042218
At time: 874.0693006515503 and batch: 200, loss is 5.192233257293701 and perplexity is 179.86980026415677
At time: 875.752911567688 and batch: 250, loss is 5.213360080718994 and perplexity is 183.71030374602492
At time: 877.4413030147552 and batch: 300, loss is 5.225925445556641 and perplexity is 186.03325453527845
At time: 879.121429681778 and batch: 350, loss is 5.169782724380493 and perplexity is 175.8766196308247
At time: 880.8006875514984 and batch: 400, loss is 5.191908760070801 and perplexity is 179.81144248246858
At time: 882.4775428771973 and batch: 450, loss is 5.161809549331665 and perplexity is 174.47990009946173
At time: 884.1548779010773 and batch: 500, loss is 5.134952964782715 and perplexity is 169.85633045738228
At time: 885.8377764225006 and batch: 550, loss is 5.154486274719238 and perplexity is 173.20680318555333
At time: 887.5238020420074 and batch: 600, loss is 5.142827663421631 and perplexity is 171.1991781930074
At time: 889.2178058624268 and batch: 650, loss is 5.137640333175659 and perplexity is 170.31341088786152
At time: 890.9311230182648 and batch: 700, loss is 5.133884763717651 and perplexity is 169.6749866173649
At time: 892.63419008255 and batch: 750, loss is 5.1349178981781005 and perplexity is 169.85037427703293
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.12269769712936 and perplexity of 167.78739921269744
Finished 32 epochs...
Completing Train Step...
At time: 897.1057918071747 and batch: 50, loss is 5.200261459350586 and perplexity is 181.31964339430635
At time: 898.8136403560638 and batch: 100, loss is 5.190265340805054 and perplexity is 179.51617958051636
At time: 900.4905605316162 and batch: 150, loss is 5.176028003692627 and perplexity is 176.97845529936
At time: 902.1702609062195 and batch: 200, loss is 5.189677410125732 and perplexity is 179.41066753103115
At time: 903.8536639213562 and batch: 250, loss is 5.210672655105591 and perplexity is 183.21725877787117
At time: 905.5681841373444 and batch: 300, loss is 5.223729467391967 and perplexity is 185.62517779820766
At time: 907.2459273338318 and batch: 350, loss is 5.168341856002808 and perplexity is 175.6233870524431
At time: 908.920547246933 and batch: 400, loss is 5.190955476760864 and perplexity is 179.64011291119948
At time: 910.611387014389 and batch: 450, loss is 5.161365737915039 and perplexity is 174.40248110881254
At time: 912.28826379776 and batch: 500, loss is 5.134845237731934 and perplexity is 169.83803332141153
At time: 913.9646654129028 and batch: 550, loss is 5.154957799911499 and perplexity is 173.28849381480762
At time: 915.6404883861542 and batch: 600, loss is 5.143542804718018 and perplexity is 171.32165358362573
At time: 917.3263857364655 and batch: 650, loss is 5.13804892539978 and perplexity is 170.38301384185846
At time: 919.0245757102966 and batch: 700, loss is 5.134329872131348 and perplexity is 169.7505271921277
At time: 920.730544090271 and batch: 750, loss is 5.134765539169312 and perplexity is 169.82449801365664
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.122351712958757 and perplexity of 167.72935746988432
Finished 33 epochs...
Completing Train Step...
At time: 925.2516152858734 and batch: 50, loss is 5.198075380325317 and perplexity is 180.92369726745792
At time: 926.9627957344055 and batch: 100, loss is 5.187959108352661 and perplexity is 179.10265057173498
At time: 928.6430814266205 and batch: 150, loss is 5.173675918579102 and perplexity is 176.56267607498253
At time: 930.3243024349213 and batch: 200, loss is 5.187706317901611 and perplexity is 179.05738085402993
At time: 932.009619474411 and batch: 250, loss is 5.208543615341187 and perplexity is 182.82759689841455
At time: 933.6885073184967 and batch: 300, loss is 5.221811981201172 and perplexity is 185.26958511409845
At time: 935.372978925705 and batch: 350, loss is 5.1672252178192135 and perplexity is 175.42738872250325
At time: 937.0550806522369 and batch: 400, loss is 5.1900882911682125 and perplexity is 179.48439911955708
At time: 938.7427315711975 and batch: 450, loss is 5.160897827148437 and perplexity is 174.32089539908412
At time: 940.4249646663666 and batch: 500, loss is 5.134737424850464 and perplexity is 169.81972358068657
At time: 942.1056551933289 and batch: 550, loss is 5.155318784713745 and perplexity is 173.35105961945172
At time: 943.7855203151703 and batch: 600, loss is 5.144011869430542 and perplexity is 171.4020333760014
At time: 945.4667911529541 and batch: 650, loss is 5.138169450759888 and perplexity is 170.40355055353035
At time: 947.1509101390839 and batch: 700, loss is 5.134342651367188 and perplexity is 169.7526964880096
At time: 948.8748092651367 and batch: 750, loss is 5.134380702972412 and perplexity is 169.75915597349822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.122274709302325 and perplexity of 167.71644219333632
Finished 34 epochs...
Completing Train Step...
At time: 953.3472561836243 and batch: 50, loss is 5.196140232086182 and perplexity is 180.57392163627463
At time: 955.0543954372406 and batch: 100, loss is 5.186052379608154 and perplexity is 178.76147576694098
At time: 956.730322599411 and batch: 150, loss is 5.171735496520996 and perplexity is 176.22040214902154
At time: 958.4095683097839 and batch: 200, loss is 5.1860784149169925 and perplexity is 178.76612993775714
At time: 960.0946662425995 and batch: 250, loss is 5.206877632141113 and perplexity is 182.52326277163317
At time: 961.7761497497559 and batch: 300, loss is 5.220208597183228 and perplexity is 184.97276484434354
At time: 963.44970870018 and batch: 350, loss is 5.166161022186279 and perplexity is 175.24079896314083
At time: 965.132566690445 and batch: 400, loss is 5.189291858673096 and perplexity is 179.3415088205235
At time: 966.8169929981232 and batch: 450, loss is 5.160512342453003 and perplexity is 174.2537103120567
At time: 968.4984750747681 and batch: 500, loss is 5.134697322845459 and perplexity is 169.81291360582935
At time: 970.1820640563965 and batch: 550, loss is 5.155514860153199 and perplexity is 173.385052837155
At time: 971.8804566860199 and batch: 600, loss is 5.144212980270385 and perplexity is 171.4365076493434
At time: 973.5872554779053 and batch: 650, loss is 5.1381110572814945 and perplexity is 170.3936003879982
At time: 975.2920753955841 and batch: 700, loss is 5.13435845375061 and perplexity is 169.7553790064015
At time: 976.9928052425385 and batch: 750, loss is 5.133866529464722 and perplexity is 169.67189274895048
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.121871238531068 and perplexity of 167.64878716038405
Finished 35 epochs...
Completing Train Step...
At time: 981.4989008903503 and batch: 50, loss is 5.194456205368042 and perplexity is 180.2700862329135
At time: 983.211713552475 and batch: 100, loss is 5.184404516220093 and perplexity is 178.46714385182608
At time: 984.8972573280334 and batch: 150, loss is 5.169943017959595 and perplexity is 175.90481378327607
At time: 986.5805287361145 and batch: 200, loss is 5.18461015701294 and perplexity is 178.50384775056492
At time: 988.2625594139099 and batch: 250, loss is 5.20534460067749 and perplexity is 182.24366323916058
At time: 989.9690175056458 and batch: 300, loss is 5.218806428909302 and perplexity is 184.71358365222497
At time: 991.6473000049591 and batch: 350, loss is 5.165186395645142 and perplexity is 175.0700878326937
At time: 993.3258626461029 and batch: 400, loss is 5.1885316276550295 and perplexity is 179.20521965488092
At time: 994.9974851608276 and batch: 450, loss is 5.160049209594726 and perplexity is 174.17302637826725
At time: 996.6726262569427 and batch: 500, loss is 5.1344839954376225 and perplexity is 169.77669172085047
At time: 998.3555545806885 and batch: 550, loss is 5.1554710388183596 and perplexity is 173.3774550391726
At time: 1000.0410406589508 and batch: 600, loss is 5.1441903495788575 and perplexity is 171.4326279665223
At time: 1001.7231085300446 and batch: 650, loss is 5.137793493270874 and perplexity is 170.3394981037988
At time: 1003.412430524826 and batch: 700, loss is 5.1340380859375 and perplexity is 169.7010035574025
At time: 1005.1062595844269 and batch: 750, loss is 5.1332743453979495 and perplexity is 169.5714455020504
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.121707650118096 and perplexity of 167.62136400447218
Finished 36 epochs...
Completing Train Step...
At time: 1009.5588374137878 and batch: 50, loss is 5.193087587356567 and perplexity is 180.02353410229338
At time: 1011.270955324173 and batch: 100, loss is 5.182894887924195 and perplexity is 178.19792806059533
At time: 1012.9564020633698 and batch: 150, loss is 5.1684599876403805 and perplexity is 175.64413495621943
At time: 1014.6382477283478 and batch: 200, loss is 5.183431720733642 and perplexity is 178.29361623693205
At time: 1016.3183722496033 and batch: 250, loss is 5.203965520858764 and perplexity is 181.99250790253006
At time: 1018.0014610290527 and batch: 300, loss is 5.217556667327881 and perplexity is 184.4828799041785
At time: 1019.6875221729279 and batch: 350, loss is 5.164326200485229 and perplexity is 174.91955814223198
At time: 1021.3694386482239 and batch: 400, loss is 5.187843828201294 and perplexity is 179.08200478111462
At time: 1023.0509080886841 and batch: 450, loss is 5.159607734680176 and perplexity is 174.09615032700253
At time: 1024.7377967834473 and batch: 500, loss is 5.134152603149414 and perplexity is 169.72043835597714
At time: 1026.4310352802277 and batch: 550, loss is 5.155403089523316 and perplexity is 173.36567456356818
At time: 1028.1196711063385 and batch: 600, loss is 5.1440474224090575 and perplexity is 171.4081273371402
At time: 1029.8079779148102 and batch: 650, loss is 5.137434034347534 and perplexity is 170.27827905473407
At time: 1031.497778892517 and batch: 700, loss is 5.133579387664795 and perplexity is 169.62317985035884
At time: 1033.250074148178 and batch: 750, loss is 5.132651643753052 and perplexity is 169.46588595347183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.121523834938227 and perplexity of 167.5905554849213
Finished 37 epochs...
Completing Train Step...
At time: 1037.756961107254 and batch: 50, loss is 5.191517515182495 and perplexity is 179.74110593504298
At time: 1039.46053981781 and batch: 100, loss is 5.181546859741211 and perplexity is 177.95787406754607
At time: 1041.1371850967407 and batch: 150, loss is 5.167143993377685 and perplexity is 175.41314030949303
At time: 1042.8162384033203 and batch: 200, loss is 5.182472858428955 and perplexity is 178.122739146049
At time: 1044.4984188079834 and batch: 250, loss is 5.202760229110718 and perplexity is 181.77328597427854
At time: 1046.175225019455 and batch: 300, loss is 5.216373262405395 and perplexity is 184.26469108431073
At time: 1047.8498709201813 and batch: 350, loss is 5.163549184799194 and perplexity is 174.7836956922242
At time: 1049.534025669098 and batch: 400, loss is 5.1872182464599605 and perplexity is 178.97000938351252
At time: 1051.2160000801086 and batch: 450, loss is 5.159167671203614 and perplexity is 174.01955382473125
At time: 1052.8958084583282 and batch: 500, loss is 5.133830957412719 and perplexity is 169.6658572789059
At time: 1054.57834649086 and batch: 550, loss is 5.155142917633056 and perplexity is 173.32057555531242
At time: 1056.267261981964 and batch: 600, loss is 5.143668041229248 and perplexity is 171.34311065339506
At time: 1057.9524927139282 and batch: 650, loss is 5.136823558807373 and perplexity is 170.174360053582
At time: 1059.634664773941 and batch: 700, loss is 5.1330970287322994 and perplexity is 169.54138032435185
At time: 1061.327993631363 and batch: 750, loss is 5.131959228515625 and perplexity is 169.34858580670468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.121302050213481 and perplexity of 167.55339058115948
Finished 38 epochs...
Completing Train Step...
At time: 1065.7772433757782 and batch: 50, loss is 5.190177488327026 and perplexity is 179.50040933203204
At time: 1067.4755020141602 and batch: 100, loss is 5.1802614402771 and perplexity is 177.7292705096362
At time: 1069.1559352874756 and batch: 150, loss is 5.165873289108276 and perplexity is 175.19038364208907
At time: 1070.8411667346954 and batch: 200, loss is 5.181390857696533 and perplexity is 177.93011444066062
At time: 1072.5202765464783 and batch: 250, loss is 5.201555051803589 and perplexity is 181.5543488904823
At time: 1074.2274856567383 and batch: 300, loss is 5.215239391326905 and perplexity is 184.0558770867516
At time: 1075.9102578163147 and batch: 350, loss is 5.162711744308472 and perplexity is 174.63738601970599
At time: 1077.5922298431396 and batch: 400, loss is 5.186516647338867 and perplexity is 178.8444882201448
At time: 1079.2704002857208 and batch: 450, loss is 5.15863145828247 and perplexity is 173.9262673043928
At time: 1080.9518542289734 and batch: 500, loss is 5.133380823135376 and perplexity is 169.58950204719252
At time: 1082.6388487815857 and batch: 550, loss is 5.1546071434020995 and perplexity is 173.22773972897775
At time: 1084.324735403061 and batch: 600, loss is 5.14328932762146 and perplexity is 171.2782329715957
At time: 1086.0129177570343 and batch: 650, loss is 5.136290798187256 and perplexity is 170.08372200232776
At time: 1087.697800397873 and batch: 700, loss is 5.132553319931031 and perplexity is 169.44922423899783
At time: 1089.3874833583832 and batch: 750, loss is 5.131255826950073 and perplexity is 169.2295076311215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.12095429176508 and perplexity of 167.49513260446548
Finished 39 epochs...
Completing Train Step...
At time: 1093.8839325904846 and batch: 50, loss is 5.188905897140503 and perplexity is 179.2723032531242
At time: 1095.5959086418152 and batch: 100, loss is 5.179018964767456 and perplexity is 177.5085833712539
At time: 1097.2742137908936 and batch: 150, loss is 5.164719066619873 and perplexity is 174.98829161355548
At time: 1098.9546773433685 and batch: 200, loss is 5.180446348190308 and perplexity is 177.76213709671444
At time: 1100.6365625858307 and batch: 250, loss is 5.200378828048706 and perplexity is 181.34092589372023
At time: 1102.316134929657 and batch: 300, loss is 5.214120454788208 and perplexity is 183.85004541848008
At time: 1103.9954507350922 and batch: 350, loss is 5.161903333663941 and perplexity is 174.49626434773108
At time: 1105.6751084327698 and batch: 400, loss is 5.185886888504029 and perplexity is 178.73189478070378
At time: 1107.3575983047485 and batch: 450, loss is 5.15813910484314 and perplexity is 173.84065518593547
At time: 1109.035816192627 and batch: 500, loss is 5.1328769969940184 and perplexity is 169.5040799435128
At time: 1110.71462225914 and batch: 550, loss is 5.154210300445556 and perplexity is 173.1590091591117
At time: 1112.396282672882 and batch: 600, loss is 5.14291974067688 and perplexity is 171.2149424691908
At time: 1114.0892360210419 and batch: 650, loss is 5.135777730941772 and perplexity is 169.99647999799964
At time: 1115.7876315116882 and batch: 700, loss is 5.132012100219726 and perplexity is 169.3575397917275
At time: 1117.5130128860474 and batch: 750, loss is 5.130515022277832 and perplexity is 169.10418804558572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.120789283929869 and perplexity of 167.46749687534472
Finished 40 epochs...
Completing Train Step...
At time: 1121.969141960144 and batch: 50, loss is 5.18771710395813 and perplexity is 179.05931218747568
At time: 1123.6788637638092 and batch: 100, loss is 5.177904300689697 and perplexity is 177.3108311639921
At time: 1125.3629581928253 and batch: 150, loss is 5.163579216003418 and perplexity is 174.78894473590168
At time: 1127.0432045459747 and batch: 200, loss is 5.179475345611572 and perplexity is 177.58961337723557
At time: 1128.721561908722 and batch: 250, loss is 5.199249439239502 and perplexity is 181.13623708974447
At time: 1130.4018058776855 and batch: 300, loss is 5.213042430877685 and perplexity is 183.65195746452736
At time: 1132.08722615242 and batch: 350, loss is 5.161188850402832 and perplexity is 174.37163421610188
At time: 1133.7661118507385 and batch: 400, loss is 5.185362796783448 and perplexity is 178.63824741649313
At time: 1135.44579744339 and batch: 450, loss is 5.157667121887207 and perplexity is 173.7586247196533
At time: 1137.1206538677216 and batch: 500, loss is 5.132438402175904 and perplexity is 169.4297526333783
At time: 1138.7989740371704 and batch: 550, loss is 5.153864336013794 and perplexity is 173.09911266252956
At time: 1140.4756095409393 and batch: 600, loss is 5.142508134841919 and perplexity is 171.1444839014116
At time: 1142.146317243576 and batch: 650, loss is 5.1351737213134765 and perplexity is 169.8938314907686
At time: 1143.8329710960388 and batch: 700, loss is 5.131411809921264 and perplexity is 169.25590661137716
At time: 1145.537940979004 and batch: 750, loss is 5.129788761138916 and perplexity is 168.98141883203095
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.120643438294876 and perplexity of 167.44307425293198
Finished 41 epochs...
Completing Train Step...
At time: 1150.0495982170105 and batch: 50, loss is 5.186719694137573 and perplexity is 178.88080570789418
At time: 1151.774822473526 and batch: 100, loss is 5.1767357730865475 and perplexity is 177.10375957143773
At time: 1153.459300518036 and batch: 150, loss is 5.16247968673706 and perplexity is 174.59686479383913
At time: 1155.1426792144775 and batch: 200, loss is 5.1785531330108645 and perplexity is 177.42591349267428
At time: 1156.8265993595123 and batch: 250, loss is 5.198159217834473 and perplexity is 180.9388660954335
At time: 1158.5393135547638 and batch: 300, loss is 5.212002391815186 and perplexity is 183.46105154687382
At time: 1160.2243435382843 and batch: 350, loss is 5.160418386459351 and perplexity is 174.23733890066495
At time: 1161.9023158550262 and batch: 400, loss is 5.1847528457641605 and perplexity is 178.52932005895127
At time: 1163.5841064453125 and batch: 450, loss is 5.157148704528809 and perplexity is 173.6685685777778
At time: 1165.2771100997925 and batch: 500, loss is 5.131965131759643 and perplexity is 169.34958551568155
At time: 1166.9708602428436 and batch: 550, loss is 5.153520545959473 and perplexity is 173.0396131374428
At time: 1168.6512792110443 and batch: 600, loss is 5.142066211700439 and perplexity is 171.06886790289934
At time: 1170.3443653583527 and batch: 650, loss is 5.1345696449279785 and perplexity is 169.7912336307154
At time: 1172.0442607402802 and batch: 700, loss is 5.130818653106689 and perplexity is 169.15554108615385
At time: 1173.7383189201355 and batch: 750, loss is 5.129108219146729 and perplexity is 168.86645900254894
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.120351392169331 and perplexity of 167.3941802918405
Finished 42 epochs...
Completing Train Step...
At time: 1178.1742000579834 and batch: 50, loss is 5.185538911819458 and perplexity is 178.66971106839955
At time: 1179.888367176056 and batch: 100, loss is 5.1757608127594 and perplexity is 176.9311745774988
At time: 1181.5657811164856 and batch: 150, loss is 5.161514730453491 and perplexity is 174.4284677130422
At time: 1183.2491161823273 and batch: 200, loss is 5.177753620147705 and perplexity is 177.28411588463817
At time: 1184.9393963813782 and batch: 250, loss is 5.197127857208252 and perplexity is 180.7523490728795
At time: 1186.6170156002045 and batch: 300, loss is 5.211016359329224 and perplexity is 183.28024214676054
At time: 1188.2919220924377 and batch: 350, loss is 5.159715080261231 and perplexity is 174.11483978251616
At time: 1189.9761786460876 and batch: 400, loss is 5.1841457653045655 and perplexity is 178.4209712888169
At time: 1191.6533443927765 and batch: 450, loss is 5.1565917873382565 and perplexity is 173.57187649371923
At time: 1193.3262655735016 and batch: 500, loss is 5.131415748596192 and perplexity is 169.25657325668573
At time: 1194.9993464946747 and batch: 550, loss is 5.1530906772613525 and perplexity is 172.96524480967355
At time: 1196.679589509964 and batch: 600, loss is 5.141460437774658 and perplexity is 170.96527022476494
At time: 1198.3601145744324 and batch: 650, loss is 5.133858842849731 and perplexity is 169.67058855144862
At time: 1200.0441257953644 and batch: 700, loss is 5.130229921340942 and perplexity is 169.05598315509818
At time: 1201.7461280822754 and batch: 750, loss is 5.1284599781036375 and perplexity is 168.75702830558862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1202137082122094 and perplexity of 167.371134385261
Finished 43 epochs...
Completing Train Step...
At time: 1206.2078948020935 and batch: 50, loss is 5.184755840301514 and perplexity is 178.52985467246927
At time: 1207.9164707660675 and batch: 100, loss is 5.1748302459716795 and perplexity is 176.766604886145
At time: 1209.590760231018 and batch: 150, loss is 5.160588054656983 and perplexity is 174.2669039439693
At time: 1211.2628464698792 and batch: 200, loss is 5.177000923156738 and perplexity is 177.15072487187035
At time: 1212.9384384155273 and batch: 250, loss is 5.1961153125762936 and perplexity is 180.5694218787149
At time: 1214.6123614311218 and batch: 300, loss is 5.210047912597656 and perplexity is 183.10283091582195
At time: 1216.2820925712585 and batch: 350, loss is 5.1590206336975095 and perplexity is 173.99396830458514
At time: 1217.962106704712 and batch: 400, loss is 5.183531742095948 and perplexity is 178.3114502991972
At time: 1219.6441266536713 and batch: 450, loss is 5.155978860855103 and perplexity is 173.46552229086893
At time: 1221.3258428573608 and batch: 500, loss is 5.130880670547485 and perplexity is 169.16603200521513
At time: 1223.0065100193024 and batch: 550, loss is 5.152645950317383 and perplexity is 172.88833960711173
At time: 1224.6886687278748 and batch: 600, loss is 5.140894241333008 and perplexity is 170.86849769583404
At time: 1226.3860094547272 and batch: 650, loss is 5.1332502269744875 and perplexity is 169.56735575544005
At time: 1228.0886240005493 and batch: 700, loss is 5.129745588302613 and perplexity is 168.97412358237688
At time: 1229.7895605564117 and batch: 750, loss is 5.127796268463134 and perplexity is 168.64505980039797
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.119936566020167 and perplexity of 167.3247552093029
Finished 44 epochs...
Completing Train Step...
At time: 1234.2933082580566 and batch: 50, loss is 5.183846693038941 and perplexity is 178.3676185032655
At time: 1236.0202629566193 and batch: 100, loss is 5.173868608474732 and perplexity is 176.5967011966508
At time: 1237.6925847530365 and batch: 150, loss is 5.159594049453736 and perplexity is 174.09376779806573
At time: 1239.365070104599 and batch: 200, loss is 5.1761137390136716 and perplexity is 176.9936292545057
At time: 1241.0402998924255 and batch: 250, loss is 5.195109224319458 and perplexity is 180.3878444605894
At time: 1242.7496089935303 and batch: 300, loss is 5.209086112976074 and perplexity is 182.9268073456074
At time: 1244.435203075409 and batch: 350, loss is 5.158354072570801 and perplexity is 173.8780293335298
At time: 1246.1164419651031 and batch: 400, loss is 5.182974700927734 and perplexity is 178.21215114004417
At time: 1247.797089099884 and batch: 450, loss is 5.155432033538818 and perplexity is 173.37069253496014
At time: 1249.4767389297485 and batch: 500, loss is 5.130404052734375 and perplexity is 169.0854236721978
At time: 1251.1578452587128 and batch: 550, loss is 5.152229804992675 and perplexity is 172.8164079009475
At time: 1252.8399333953857 and batch: 600, loss is 5.140433502197266 and perplexity is 170.7897900251245
At time: 1254.5254299640656 and batch: 650, loss is 5.132671012878418 and perplexity is 169.46916839125097
At time: 1256.2084729671478 and batch: 700, loss is 5.129283008575439 and perplexity is 168.8959776541453
At time: 1257.8878438472748 and batch: 750, loss is 5.127198905944824 and perplexity is 168.54434764660277
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.119600872660792 and perplexity of 167.2685948269812
Finished 45 epochs...
Completing Train Step...
At time: 1262.3699753284454 and batch: 50, loss is 5.182663278579712 and perplexity is 178.15666053444585
At time: 1264.0840423107147 and batch: 100, loss is 5.172848787307739 and perplexity is 176.41669594493334
At time: 1265.7636466026306 and batch: 150, loss is 5.1585510635375975 and perplexity is 173.91228510856124
At time: 1267.4473280906677 and batch: 200, loss is 5.175253362655639 and perplexity is 176.84141361113302
At time: 1269.1161127090454 and batch: 250, loss is 5.194153909683227 and perplexity is 180.21559959969608
At time: 1270.7902655601501 and batch: 300, loss is 5.208173217773438 and perplexity is 182.75989054111517
At time: 1272.4660851955414 and batch: 350, loss is 5.15768196105957 and perplexity is 173.76120317296608
At time: 1274.143723487854 and batch: 400, loss is 5.182542161941528 and perplexity is 178.13508410531057
At time: 1275.8283817768097 and batch: 450, loss is 5.154819822311401 and perplexity is 173.26458553374798
At time: 1277.5018215179443 and batch: 500, loss is 5.129791421890259 and perplexity is 168.9818684501662
At time: 1279.1763544082642 and batch: 550, loss is 5.1516775703430175 and perplexity is 172.72099903893962
At time: 1280.8508970737457 and batch: 600, loss is 5.139782333374024 and perplexity is 170.67861323988873
At time: 1282.5285954475403 and batch: 650, loss is 5.132046432495117 and perplexity is 169.36335432123556
At time: 1284.2061262130737 and batch: 700, loss is 5.128653345108032 and perplexity is 168.78966350180593
At time: 1285.9383771419525 and batch: 750, loss is 5.12651442527771 and perplexity is 168.42902177275553
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.119644519894622 and perplexity of 167.27589579778441
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1290.3609807491302 and batch: 50, loss is 5.182408037185669 and perplexity is 178.11119338284948
At time: 1292.0604310035706 and batch: 100, loss is 5.172866401672363 and perplexity is 176.41980344030964
At time: 1293.7346415519714 and batch: 150, loss is 5.157335958480835 and perplexity is 173.70109174853778
At time: 1295.41392660141 and batch: 200, loss is 5.172784767150879 and perplexity is 176.40540208190748
At time: 1297.0967214107513 and batch: 250, loss is 5.189272880554199 and perplexity is 179.3381052883424
At time: 1298.773592710495 and batch: 300, loss is 5.200922079086304 and perplexity is 181.4394663035375
At time: 1300.447515964508 and batch: 350, loss is 5.148588466644287 and perplexity is 172.1882692144295
At time: 1302.1166734695435 and batch: 400, loss is 5.171734247207642 and perplexity is 176.2201819946574
At time: 1303.791955947876 and batch: 450, loss is 5.143048343658447 and perplexity is 171.23696263718043
At time: 1305.474594116211 and batch: 500, loss is 5.113931694030762 and perplexity is 166.32300218309598
At time: 1307.1625392436981 and batch: 550, loss is 5.131902990341186 and perplexity is 169.33906221919216
At time: 1308.8507709503174 and batch: 600, loss is 5.120661067962646 and perplexity is 167.4460262447227
At time: 1310.5485079288483 and batch: 650, loss is 5.1134685611724855 and perplexity is 166.24599037042483
At time: 1312.2531645298004 and batch: 700, loss is 5.110451021194458 and perplexity is 165.74509256794624
At time: 1313.9535830020905 and batch: 750, loss is 5.112188558578492 and perplexity is 166.03333120268795
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.113588200059048 and perplexity of 166.26588104543353
Finished 47 epochs...
Completing Train Step...
At time: 1318.4727787971497 and batch: 50, loss is 5.177773599624634 and perplexity is 177.28765796392565
At time: 1320.1853787899017 and batch: 100, loss is 5.168439989089966 and perplexity is 175.64062236325495
At time: 1321.8633351325989 and batch: 150, loss is 5.153819799423218 and perplexity is 173.09140358988898
At time: 1323.5366015434265 and batch: 200, loss is 5.1692795753479 and perplexity is 175.7881497384386
At time: 1325.2154932022095 and batch: 250, loss is 5.186682062149048 and perplexity is 178.8740741941273
At time: 1326.925586938858 and batch: 300, loss is 5.199111623764038 and perplexity is 181.11127543319705
At time: 1328.6001796722412 and batch: 350, loss is 5.147428922653198 and perplexity is 171.98872505399999
At time: 1330.2770357131958 and batch: 400, loss is 5.1707367324829105 and perplexity is 176.04448741185814
At time: 1331.9576668739319 and batch: 450, loss is 5.142479362487793 and perplexity is 171.13955974255424
At time: 1333.636880159378 and batch: 500, loss is 5.113935537338257 and perplexity is 166.3236414147653
At time: 1335.310527563095 and batch: 550, loss is 5.132447834014893 and perplexity is 169.4313506750613
At time: 1336.9853172302246 and batch: 600, loss is 5.12155312538147 and perplexity is 167.59546435846605
At time: 1338.6654562950134 and batch: 650, loss is 5.115136346817017 and perplexity is 166.52348438256942
At time: 1340.339616060257 and batch: 700, loss is 5.111817588806153 and perplexity is 165.97174927883336
At time: 1342.0158367156982 and batch: 750, loss is 5.112529201507568 and perplexity is 166.08989891710215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.112921781318132 and perplexity of 166.1551152586275
Finished 48 epochs...
Completing Train Step...
At time: 1346.4450092315674 and batch: 50, loss is 5.1757096004486085 and perplexity is 176.92211375521248
At time: 1348.1622738838196 and batch: 100, loss is 5.166405305862427 and perplexity is 175.28361265885323
At time: 1349.8433723449707 and batch: 150, loss is 5.152167339324951 and perplexity is 172.8056131457885
At time: 1351.5231130123138 and batch: 200, loss is 5.167693014144898 and perplexity is 175.5094722080516
At time: 1353.1972975730896 and batch: 250, loss is 5.185669841766358 and perplexity is 178.69310581568536
At time: 1354.8751101493835 and batch: 300, loss is 5.198441371917725 and perplexity is 180.98992593835422
At time: 1356.5580694675446 and batch: 350, loss is 5.14712438583374 and perplexity is 171.936356129227
At time: 1358.2312717437744 and batch: 400, loss is 5.170489234924316 and perplexity is 176.00092222238112
At time: 1359.907138824463 and batch: 450, loss is 5.142529611587524 and perplexity is 171.14815956742459
At time: 1361.5910584926605 and batch: 500, loss is 5.114224300384522 and perplexity is 166.3716764711669
At time: 1363.2652261257172 and batch: 550, loss is 5.132973260879517 and perplexity is 169.52039785025596
At time: 1364.943086862564 and batch: 600, loss is 5.122204704284668 and perplexity is 167.70470161179375
At time: 1366.6243410110474 and batch: 650, loss is 5.116130962371826 and perplexity is 166.68919362521171
At time: 1368.307177066803 and batch: 700, loss is 5.112473564147949 and perplexity is 166.0806583707292
At time: 1370.009643316269 and batch: 750, loss is 5.112472887039185 and perplexity is 166.08054591609786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.112582184547602 and perplexity of 166.09869909799374
Finished 49 epochs...
Completing Train Step...
At time: 1374.457640171051 and batch: 50, loss is 5.1742997741699215 and perplexity is 176.67286005345386
At time: 1376.156706571579 and batch: 100, loss is 5.164957904815674 and perplexity is 175.03009049279655
At time: 1377.8262915611267 and batch: 150, loss is 5.151015062332153 and perplexity is 172.60660789014202
At time: 1379.5012392997742 and batch: 200, loss is 5.166626253128052 and perplexity is 175.32234537256502
At time: 1381.1861026287079 and batch: 250, loss is 5.185030288696289 and perplexity is 178.57885862872465
At time: 1382.8702065944672 and batch: 300, loss is 5.198073387145996 and perplexity is 180.9233366544452
At time: 1384.5504937171936 and batch: 350, loss is 5.147037115097046 and perplexity is 171.9213517714933
At time: 1386.2321145534515 and batch: 400, loss is 5.17040246963501 and perplexity is 175.98565211391363
At time: 1387.9115016460419 and batch: 450, loss is 5.14266881942749 and perplexity is 171.17198639143393
At time: 1389.5930349826813 and batch: 500, loss is 5.114454288482666 and perplexity is 166.40994437703625
At time: 1391.2700510025024 and batch: 550, loss is 5.133327417373657 and perplexity is 169.5804452325014
At time: 1392.948481798172 and batch: 600, loss is 5.122624626159668 and perplexity is 167.7751392726632
At time: 1394.6328792572021 and batch: 650, loss is 5.116780118942261 and perplexity is 166.79743613984266
At time: 1396.3098731040955 and batch: 700, loss is 5.112851638793945 and perplexity is 166.14346112816648
At time: 1398.0070142745972 and batch: 750, loss is 5.112367744445801 and perplexity is 166.0630846947648
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.112390207689862 and perplexity of 166.06681505226368
Finished Training.
Improved accuracyfrom -192.02307654891396 to -166.06681505226368
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb44d84ce10>
SETTINGS FOR THIS RUN
{'dropout': 0.06293036160504895, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 20.233557798071583, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 7.777238875493282, 'wordvec_source': 'glove', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.2336933612823486 and batch: 50, loss is 7.628243770599365 and perplexity is 2055.4370292039566
At time: 3.8964898586273193 and batch: 100, loss is 6.724504890441895 and perplexity is 832.5596662369959
At time: 5.567298650741577 and batch: 150, loss is 6.315114412307739 and perplexity is 552.8653115693193
At time: 7.245765447616577 and batch: 200, loss is 6.076114387512207 and perplexity is 435.3343634829474
At time: 8.91754698753357 and batch: 250, loss is 6.085414609909058 and perplexity is 439.4019553161364
At time: 10.619678020477295 and batch: 300, loss is 6.088734073638916 and perplexity is 440.86295770030324
At time: 12.296983480453491 and batch: 350, loss is 6.041469774246216 and perplexity is 420.5106366118912
At time: 13.97412633895874 and batch: 400, loss is 6.094859647750854 and perplexity is 443.57178451040244
At time: 15.65227222442627 and batch: 450, loss is 6.066457099914551 and perplexity is 431.15044948110153
At time: 17.327241897583008 and batch: 500, loss is 6.105516338348389 and perplexity is 448.3240686120122
At time: 19.006521940231323 and batch: 550, loss is 6.1363121509552006 and perplexity is 462.3453630834555
At time: 20.687142372131348 and batch: 600, loss is 6.147529640197754 and perplexity is 467.56091523004443
At time: 22.3573215007782 and batch: 650, loss is 6.14971170425415 and perplexity is 468.5822770305207
At time: 24.02814245223999 and batch: 700, loss is 6.171192188262939 and perplexity is 478.75653391355644
At time: 25.713335752487183 and batch: 750, loss is 6.146073637008667 and perplexity is 466.8806404077111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.807312366574309 and perplexity of 332.72368360997615
Finished 1 epochs...
Completing Train Step...
At time: 30.239999771118164 and batch: 50, loss is 6.160999937057495 and perplexity is 473.90170987406975
At time: 31.930936574935913 and batch: 100, loss is 6.160671176910401 and perplexity is 473.74593548583175
At time: 33.60598278045654 and batch: 150, loss is 6.1612379837036135 and perplexity is 474.01453401486646
At time: 35.2794725894928 and batch: 200, loss is 6.157246894836426 and perplexity is 472.1264701069103
At time: 36.95040535926819 and batch: 250, loss is 6.175762300491333 and perplexity is 480.9495122653454
At time: 38.63331365585327 and batch: 300, loss is 6.201659803390503 and perplexity is 493.5675867468346
At time: 40.30607843399048 and batch: 350, loss is 6.135198268890381 and perplexity is 461.8306515929316
At time: 41.97951316833496 and batch: 400, loss is 6.210474081039429 and perplexity is 497.93725795206456
At time: 43.656214475631714 and batch: 450, loss is 6.1652631664276125 and perplexity is 475.9263743000334
At time: 45.33181428909302 and batch: 500, loss is 6.159446401596069 and perplexity is 473.1660583408783
At time: 47.0112566947937 and batch: 550, loss is 6.1681069564819335 and perplexity is 477.281735257446
At time: 48.68989896774292 and batch: 600, loss is 6.126339502334595 and perplexity is 457.7574699824838
At time: 50.36485004425049 and batch: 650, loss is 6.132769746780395 and perplexity is 460.71044641654305
At time: 52.03443145751953 and batch: 700, loss is 6.188462438583374 and perplexity is 487.0965892261359
At time: 53.70667290687561 and batch: 750, loss is 6.162581529617309 and perplexity is 474.6518223222603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.841484247251999 and perplexity of 344.28997306526776
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 58.17177867889404 and batch: 50, loss is 6.08885124206543 and perplexity is 440.91461594566385
At time: 59.877798318862915 and batch: 100, loss is 5.993127269744873 and perplexity is 400.6656422906533
At time: 61.58532476425171 and batch: 150, loss is 5.94238127708435 and perplexity is 380.84073794176174
At time: 63.2647602558136 and batch: 200, loss is 5.900532646179199 and perplexity is 365.23195546979326
At time: 64.9450204372406 and batch: 250, loss is 5.919877614974975 and perplexity is 372.36613904815516
At time: 66.61558389663696 and batch: 300, loss is 5.8948490428924565 and perplexity is 363.1620098742346
At time: 68.2893123626709 and batch: 350, loss is 5.7887591361999515 and perplexity is 326.6074974057811
At time: 69.96342158317566 and batch: 400, loss is 5.78946364402771 and perplexity is 326.83767601610396
At time: 71.64483094215393 and batch: 450, loss is 5.752057514190674 and perplexity is 314.8377775259714
At time: 73.3187186717987 and batch: 500, loss is 5.702995662689209 and perplexity is 299.76404924651314
At time: 74.99183559417725 and batch: 550, loss is 5.64784607887268 and perplexity is 283.67978342367394
At time: 76.67003560066223 and batch: 600, loss is 5.566171321868897 and perplexity is 261.43124457258085
At time: 78.34793162345886 and batch: 650, loss is 5.512395257949829 and perplexity is 247.74382732116584
At time: 80.02280855178833 and batch: 700, loss is 5.490535268783569 and perplexity is 242.38691428945242
At time: 81.69747471809387 and batch: 750, loss is 5.498170242309571 and perplexity is 244.2446146855658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.331067462300146 and perplexity of 206.65845656843373
Finished 3 epochs...
Completing Train Step...
At time: 86.1377341747284 and batch: 50, loss is 5.686494903564453 and perplexity is 294.8583004739579
At time: 87.83081936836243 and batch: 100, loss is 5.690519943237304 and perplexity is 296.04750853051445
At time: 89.50962162017822 and batch: 150, loss is 5.675021953582764 and perplexity is 291.4947378289283
At time: 91.18623614311218 and batch: 200, loss is 5.645520963668823 and perplexity is 283.0209614614972
At time: 92.85809230804443 and batch: 250, loss is 5.669096946716309 and perplexity is 289.7727359798176
At time: 94.53203821182251 and batch: 300, loss is 5.67043246269226 and perplexity is 290.1599906329429
At time: 96.20530533790588 and batch: 350, loss is 5.587641201019287 and perplexity is 267.1048294529509
At time: 97.87772035598755 and batch: 400, loss is 5.594984874725342 and perplexity is 269.0735802496834
At time: 99.54076790809631 and batch: 450, loss is 5.573148775100708 and perplexity is 263.26174755291345
At time: 101.25191926956177 and batch: 500, loss is 5.551295824050904 and perplexity is 257.5711065823338
At time: 102.92528295516968 and batch: 550, loss is 5.515800495147705 and perplexity is 248.58889182345428
At time: 104.60403609275818 and batch: 600, loss is 5.45680004119873 and perplexity is 234.34632467793443
At time: 106.28051424026489 and batch: 650, loss is 5.445612173080445 and perplexity is 231.7391007373464
At time: 107.95323538780212 and batch: 700, loss is 5.439307413101196 and perplexity is 230.2826374795512
At time: 109.62902355194092 and batch: 750, loss is 5.450652847290039 and perplexity is 232.91017105818574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.283977064975472 and perplexity of 197.15240616179264
Finished 4 epochs...
Completing Train Step...
At time: 114.06031250953674 and batch: 50, loss is 5.593690423965454 and perplexity is 268.7255030822437
At time: 115.75763010978699 and batch: 100, loss is 5.593407735824585 and perplexity is 268.64954830563636
At time: 117.4330186843872 and batch: 150, loss is 5.572334394454956 and perplexity is 263.0474395569261
At time: 119.110107421875 and batch: 200, loss is 5.543214797973633 and perplexity is 255.49805522568423
At time: 120.78349900245667 and batch: 250, loss is 5.562953042984009 and perplexity is 260.59123833051655
At time: 122.45801949501038 and batch: 300, loss is 5.567872619628906 and perplexity is 261.8763955231552
At time: 124.136789560318 and batch: 350, loss is 5.496305742263794 and perplexity is 243.7896448677302
At time: 125.8123083114624 and batch: 400, loss is 5.507027683258056 and perplexity is 246.41760630290935
At time: 127.48391699790955 and batch: 450, loss is 5.479170150756836 and perplexity is 239.64775332801523
At time: 129.15859508514404 and batch: 500, loss is 5.462417383193969 and perplexity is 235.6664324059874
At time: 130.83717370033264 and batch: 550, loss is 5.450412464141846 and perplexity is 232.85419010672885
At time: 132.51285672187805 and batch: 600, loss is 5.408678913116455 and perplexity is 223.33634598621964
At time: 134.19119429588318 and batch: 650, loss is 5.407023115158081 and perplexity is 222.96685210852812
At time: 135.86953020095825 and batch: 700, loss is 5.404384317398072 and perplexity is 222.37926328362607
At time: 137.54236721992493 and batch: 750, loss is 5.408703985214234 and perplexity is 223.34194556712
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.240246794944586 and perplexity of 188.71667097934204
Finished 5 epochs...
Completing Train Step...
At time: 141.98469591140747 and batch: 50, loss is 5.524007530212402 and perplexity is 250.63746443042575
At time: 143.6821596622467 and batch: 100, loss is 5.520483036041259 and perplexity is 249.75564903740712
At time: 145.35109996795654 and batch: 150, loss is 5.4962512969970705 and perplexity is 243.77637203681564
At time: 147.02274322509766 and batch: 200, loss is 5.472195014953614 and perplexity is 237.98199390631663
At time: 148.69199466705322 and batch: 250, loss is 5.489426326751709 and perplexity is 242.11827023509625
At time: 150.3667013645172 and batch: 300, loss is 5.49690881729126 and perplexity is 243.93671265650272
At time: 152.0431933403015 and batch: 350, loss is 5.429334745407105 and perplexity is 227.99751856471084
At time: 153.72671127319336 and batch: 400, loss is 5.444810752868652 and perplexity is 231.5534547383171
At time: 155.39905500411987 and batch: 450, loss is 5.420660648345947 and perplexity is 226.02839847473118
At time: 157.06966066360474 and batch: 500, loss is 5.40840365409851 and perplexity is 223.2748791029971
At time: 158.74368262290955 and batch: 550, loss is 5.408295440673828 and perplexity is 223.2507190709273
At time: 160.4149568080902 and batch: 600, loss is 5.374485177993774 and perplexity is 215.8287303239521
At time: 162.08953332901 and batch: 650, loss is 5.368500843048095 and perplexity is 214.54099587099174
At time: 163.77021050453186 and batch: 700, loss is 5.368074293136597 and perplexity is 214.44950294272758
At time: 165.44095826148987 and batch: 750, loss is 5.371999006271363 and perplexity is 215.2928095093701
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.208896370821221 and perplexity of 182.8921017116663
Finished 6 epochs...
Completing Train Step...
At time: 169.85500502586365 and batch: 50, loss is 5.469771776199341 and perplexity is 237.4060048771872
At time: 171.55545663833618 and batch: 100, loss is 5.4570113468170165 and perplexity is 234.39584860512142
At time: 173.22693061828613 and batch: 150, loss is 5.435970888137818 and perplexity is 229.51557408565364
At time: 174.90299344062805 and batch: 200, loss is 5.420702304840088 and perplexity is 226.03781422149993
At time: 176.5804841518402 and batch: 250, loss is 5.435475292205811 and perplexity is 229.4018552824203
At time: 178.24658942222595 and batch: 300, loss is 5.44920693397522 and perplexity is 232.5736464920015
At time: 179.9129719734192 and batch: 350, loss is 5.389089298248291 and perplexity is 219.00384752035944
At time: 181.5866358280182 and batch: 400, loss is 5.409575672149658 and perplexity is 223.53671469972448
At time: 183.26856398582458 and batch: 450, loss is 5.382552947998047 and perplexity is 217.57702983680474
At time: 184.94934749603271 and batch: 500, loss is 5.3732474994659425 and perplexity is 215.56176897898183
At time: 186.66725301742554 and batch: 550, loss is 5.378207893371582 and perplexity is 216.63369665793417
At time: 188.34490299224854 and batch: 600, loss is 5.346506214141845 and perplexity is 209.8737614775809
At time: 190.0249125957489 and batch: 650, loss is 5.343411664962769 and perplexity is 209.22530066571161
At time: 191.69854950904846 and batch: 700, loss is 5.341543893814087 and perplexity is 208.83488040690227
At time: 193.3729772567749 and batch: 750, loss is 5.341895971298218 and perplexity is 208.9084194111488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.191330310910247 and perplexity of 179.7074607814698
Finished 7 epochs...
Completing Train Step...
At time: 197.79950261116028 and batch: 50, loss is 5.430118284225464 and perplexity is 228.17623347688811
At time: 199.50708317756653 and batch: 100, loss is 5.417837228775024 and perplexity is 225.3911255386786
At time: 201.18360924720764 and batch: 150, loss is 5.3983644676208495 and perplexity is 221.04459480749335
At time: 202.8597002029419 and batch: 200, loss is 5.386517515182495 and perplexity is 218.44134076667234
At time: 204.54242777824402 and batch: 250, loss is 5.4038998413085935 and perplexity is 222.27155194166104
At time: 206.22111773490906 and batch: 300, loss is 5.419907646179199 and perplexity is 225.85826266528875
At time: 207.899245262146 and batch: 350, loss is 5.3606197547912595 and perplexity is 212.85682461617094
At time: 209.573397397995 and batch: 400, loss is 5.382722473144531 and perplexity is 217.61391774128478
At time: 211.24275708198547 and batch: 450, loss is 5.358655681610108 and perplexity is 212.43916852334453
At time: 212.91439008712769 and batch: 500, loss is 5.349966707229615 and perplexity is 210.60128624969104
At time: 214.59387922286987 and batch: 550, loss is 5.360381689071655 and perplexity is 212.80615673442816
At time: 216.2772216796875 and batch: 600, loss is 5.325362606048584 and perplexity is 205.48285628421849
At time: 217.95251441001892 and batch: 650, loss is 5.324818716049195 and perplexity is 205.3711266007226
At time: 219.6326413154602 and batch: 700, loss is 5.3196930408477785 and perplexity is 204.32115411776394
At time: 221.31670331954956 and batch: 750, loss is 5.318737993240356 and perplexity is 204.12611084101061
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.176429216251817 and perplexity of 177.0494755244826
Finished 8 epochs...
Completing Train Step...
At time: 225.7835237979889 and batch: 50, loss is 5.398561220169068 and perplexity is 221.08809017356333
At time: 227.48989939689636 and batch: 100, loss is 5.384642581939698 and perplexity is 218.0321615469964
At time: 229.16965746879578 and batch: 150, loss is 5.36482943534851 and perplexity is 213.75477256316782
At time: 230.8495512008667 and batch: 200, loss is 5.355262908935547 and perplexity is 211.71963201947614
At time: 232.52410078048706 and batch: 250, loss is 5.375129251480103 and perplexity is 215.96778466254102
At time: 234.20815777778625 and batch: 300, loss is 5.395135078430176 and perplexity is 220.3319071746163
At time: 235.8837730884552 and batch: 350, loss is 5.34018651008606 and perplexity is 208.55160363952518
At time: 237.55634212493896 and batch: 400, loss is 5.360378341674805 and perplexity is 212.8054443889616
At time: 239.22819900512695 and batch: 450, loss is 5.335762405395508 and perplexity is 207.6309874643892
At time: 240.8967673778534 and batch: 500, loss is 5.326734342575073 and perplexity is 205.76491803672653
At time: 242.57195210456848 and batch: 550, loss is 5.335962200164795 and perplexity is 207.67247519400416
At time: 244.2511022090912 and batch: 600, loss is 5.302398796081543 and perplexity is 200.81795398201018
At time: 245.92916703224182 and batch: 650, loss is 5.299606380462646 and perplexity is 200.25796901002442
At time: 247.60461354255676 and batch: 700, loss is 5.295693645477295 and perplexity is 199.47594357542314
At time: 249.279381275177 and batch: 750, loss is 5.293830261230469 and perplexity is 199.10458933981172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.159333339957303 and perplexity of 174.04838581554506
Finished 9 epochs...
Completing Train Step...
At time: 253.67311644554138 and batch: 50, loss is 5.365496997833252 and perplexity is 213.89751486966884
At time: 255.36316347122192 and batch: 100, loss is 5.352874727249145 and perplexity is 211.21461035341142
At time: 257.03673219680786 and batch: 150, loss is 5.3302272510528566 and perplexity is 206.48489273423104
At time: 258.71495294570923 and batch: 200, loss is 5.321748218536377 and perplexity is 204.7415021920534
At time: 260.39418601989746 and batch: 250, loss is 5.338331108093262 and perplexity is 208.1650153277962
At time: 262.06853008270264 and batch: 300, loss is 5.361038980484008 and perplexity is 212.94607837335388
At time: 263.74490237236023 and batch: 350, loss is 5.30727370262146 and perplexity is 201.79931281202386
At time: 265.4243702888489 and batch: 400, loss is 5.324291868209839 and perplexity is 205.26295576369486
At time: 267.09747099876404 and batch: 450, loss is 5.299048051834107 and perplexity is 200.1461904603189
At time: 268.7685532569885 and batch: 500, loss is 5.2902850151062015 and perplexity is 198.39996433868035
At time: 270.473886013031 and batch: 550, loss is 5.300549211502076 and perplexity is 200.446867474775
At time: 272.1538281440735 and batch: 600, loss is 5.260162782669068 and perplexity is 192.51282649848977
At time: 273.8299512863159 and batch: 650, loss is 5.259380979537964 and perplexity is 192.3623781860995
At time: 275.50309681892395 and batch: 700, loss is 5.252150039672852 and perplexity is 190.9764342745066
At time: 277.1777482032776 and batch: 750, loss is 5.251521434783935 and perplexity is 190.85642327795517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.125841362531795 and perplexity of 168.3156966146186
Finished 10 epochs...
Completing Train Step...
At time: 281.6087956428528 and batch: 50, loss is 5.321546220779419 and perplexity is 204.70014904461658
At time: 283.3117024898529 and batch: 100, loss is 5.311549472808838 and perplexity is 202.66400759829568
At time: 284.99119424819946 and batch: 150, loss is 5.287015266418457 and perplexity is 197.75230573309688
At time: 286.66395449638367 and batch: 200, loss is 5.280854558944702 and perplexity is 196.5377567072906
At time: 288.3368046283722 and batch: 250, loss is 5.293007326126099 and perplexity is 198.94080658433535
At time: 290.01872539520264 and batch: 300, loss is 5.320728626251221 and perplexity is 204.53285572120822
At time: 291.6900005340576 and batch: 350, loss is 5.264189138412475 and perplexity is 193.28951418432894
At time: 293.35968494415283 and batch: 400, loss is 5.2837006378173825 and perplexity is 197.097915414155
At time: 295.03206181526184 and batch: 450, loss is 5.2572916316986085 and perplexity is 191.96088584149985
At time: 296.71150159835815 and batch: 500, loss is 5.243815155029297 and perplexity is 189.39128292873048
At time: 298.38391971588135 and batch: 550, loss is 5.254399766921997 and perplexity is 191.40656281717364
At time: 300.0548896789551 and batch: 600, loss is 5.214885292053222 and perplexity is 183.99071457203866
At time: 301.72917127609253 and batch: 650, loss is 5.213215217590332 and perplexity is 183.68369282417413
At time: 303.40757060050964 and batch: 700, loss is 5.21227840423584 and perplexity is 183.51169606474352
At time: 305.07934403419495 and batch: 750, loss is 5.212856321334839 and perplexity is 183.6177812630381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.099986586459847 and perplexity of 164.01970720021785
Finished 11 epochs...
Completing Train Step...
At time: 309.51075506210327 and batch: 50, loss is 5.284201135635376 and perplexity is 197.1965871811932
At time: 311.2204773426056 and batch: 100, loss is 5.272086763381958 and perplexity is 194.82208615135625
At time: 312.89334774017334 and batch: 150, loss is 5.25200382232666 and perplexity is 190.94851224849384
At time: 314.56798911094666 and batch: 200, loss is 5.243570051193237 and perplexity is 189.34486808722866
At time: 316.24811840057373 and batch: 250, loss is 5.254277467727661 and perplexity is 191.38315538013495
At time: 317.9216685295105 and batch: 300, loss is 5.287600793838501 and perplexity is 197.86812903603126
At time: 319.59545373916626 and batch: 350, loss is 5.233355121612549 and perplexity is 187.4205686175626
At time: 321.27056193351746 and batch: 400, loss is 5.255829668045044 and perplexity is 191.68045104618108
At time: 322.9413197040558 and batch: 450, loss is 5.227633543014527 and perplexity is 186.35128900404152
At time: 324.6076099872589 and batch: 500, loss is 5.211290054321289 and perplexity is 183.33041189647196
At time: 326.271497964859 and batch: 550, loss is 5.224247560501099 and perplexity is 185.72137384080787
At time: 327.94104766845703 and batch: 600, loss is 5.18567569732666 and perplexity is 178.69415216700557
At time: 329.626788854599 and batch: 650, loss is 5.17624864578247 and perplexity is 177.01750850382638
At time: 331.3051815032959 and batch: 700, loss is 5.1781602954864505 and perplexity is 177.35622762455907
At time: 332.97498321533203 and batch: 750, loss is 5.181083726882934 and perplexity is 177.87547501100454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.093071871025618 and perplexity of 162.88946973789592
Finished 12 epochs...
Completing Train Step...
At time: 337.37656331062317 and batch: 50, loss is 5.253398847579956 and perplexity is 191.21507613360555
At time: 339.07332396507263 and batch: 100, loss is 5.24422402381897 and perplexity is 189.4687349461335
At time: 340.7476191520691 and batch: 150, loss is 5.225969343185425 and perplexity is 186.04142113327342
At time: 342.43009305000305 and batch: 200, loss is 5.223210601806641 and perplexity is 185.52888826450095
At time: 344.10119342803955 and batch: 250, loss is 5.232063455581665 and perplexity is 187.17864011464104
At time: 345.7768738269806 and batch: 300, loss is 5.264504537582398 and perplexity is 193.35048715156358
At time: 347.4585704803467 and batch: 350, loss is 5.211128492355346 and perplexity is 183.3007950672499
At time: 349.1408860683441 and batch: 400, loss is 5.233573398590088 and perplexity is 187.46148267794536
At time: 350.816997051239 and batch: 450, loss is 5.203555812835694 and perplexity is 181.91795938450943
At time: 352.49443340301514 and batch: 500, loss is 5.190750789642334 and perplexity is 179.60334665703292
At time: 354.21698355674744 and batch: 550, loss is 5.206031284332275 and perplexity is 182.36884996081372
At time: 355.89277386665344 and batch: 600, loss is 5.162504749298096 and perplexity is 174.60124069325502
At time: 357.5639977455139 and batch: 650, loss is 5.150879955291748 and perplexity is 172.5832890974976
At time: 359.2385597229004 and batch: 700, loss is 5.15833607673645 and perplexity is 173.87490028147172
At time: 360.91984248161316 and batch: 750, loss is 5.158727550506592 and perplexity is 173.94298106926982
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.080150160678597 and perplexity of 160.79819968262225
Finished 13 epochs...
Completing Train Step...
At time: 365.36928701400757 and batch: 50, loss is 5.236393079757691 and perplexity is 187.9908102070299
At time: 367.07359051704407 and batch: 100, loss is 5.222862758636475 and perplexity is 185.46436453057026
At time: 368.75576853752136 and batch: 150, loss is 5.20731819152832 and perplexity is 182.6036928242373
At time: 370.43669605255127 and batch: 200, loss is 5.201429538726806 and perplexity is 181.53156287555115
At time: 372.1101040840149 and batch: 250, loss is 5.21211519241333 and perplexity is 183.48174723044536
At time: 373.7907419204712 and batch: 300, loss is 5.244958457946777 and perplexity is 189.60793836284324
At time: 375.470849275589 and batch: 350, loss is 5.188857870101929 and perplexity is 179.2636935420516
At time: 377.14472675323486 and batch: 400, loss is 5.213568496704101 and perplexity is 183.74859590016445
At time: 378.8175241947174 and batch: 450, loss is 5.188086004257202 and perplexity is 179.12537940665243
At time: 380.496209859848 and batch: 500, loss is 5.171500291824341 and perplexity is 176.17895915677389
At time: 382.1805782318115 and batch: 550, loss is 5.18622727394104 and perplexity is 178.7927428701309
At time: 383.85588788986206 and batch: 600, loss is 5.145170812606811 and perplexity is 171.6007937467219
At time: 385.5297932624817 and batch: 650, loss is 5.136277160644531 and perplexity is 170.08140249411846
At time: 387.20889115333557 and batch: 700, loss is 5.141202964782715 and perplexity is 170.92125695147956
At time: 388.8869845867157 and batch: 750, loss is 5.143736915588379 and perplexity is 171.3549122067403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.077830469885538 and perplexity of 160.42562986972578
Finished 14 epochs...
Completing Train Step...
At time: 393.27404737472534 and batch: 50, loss is 5.223757381439209 and perplexity is 185.63035942050965
At time: 394.9836416244507 and batch: 100, loss is 5.208842678070068 and perplexity is 182.88228199518744
At time: 396.64854669570923 and batch: 150, loss is 5.19233904838562 and perplexity is 179.88882989329494
At time: 398.3154125213623 and batch: 200, loss is 5.183571949005127 and perplexity is 178.3186197956156
At time: 399.98923230171204 and batch: 250, loss is 5.197680368423462 and perplexity is 180.85224436701168
At time: 401.6716113090515 and batch: 300, loss is 5.231522932052612 and perplexity is 187.07749299418484
At time: 403.345255613327 and batch: 350, loss is 5.1755963325500485 and perplexity is 176.90207529405725
At time: 405.0204863548279 and batch: 400, loss is 5.202273445129395 and perplexity is 181.68482318331544
At time: 406.69869232177734 and batch: 450, loss is 5.173999166488647 and perplexity is 176.61975881636883
At time: 408.38170289993286 and batch: 500, loss is 5.1609390926361085 and perplexity is 174.32808898426646
At time: 410.0617094039917 and batch: 550, loss is 5.17170654296875 and perplexity is 176.21530001626397
At time: 411.73521971702576 and batch: 600, loss is 5.128111495971679 and perplexity is 168.69822974230115
At time: 413.4121284484863 and batch: 650, loss is 5.120321941375733 and perplexity is 167.38925047298102
At time: 415.0936646461487 and batch: 700, loss is 5.126273250579834 and perplexity is 168.3884058522789
At time: 416.76686549186707 and batch: 750, loss is 5.130506629943848 and perplexity is 169.1027688727166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.074868845385175 and perplexity of 159.951212263988
Finished 15 epochs...
Completing Train Step...
At time: 421.1990809440613 and batch: 50, loss is 5.207509155273438 and perplexity is 182.63856683902154
At time: 422.8997631072998 and batch: 100, loss is 5.196684045791626 and perplexity is 180.672146915412
At time: 424.5780339241028 and batch: 150, loss is 5.177373399734497 and perplexity is 177.21672165798364
At time: 426.255895614624 and batch: 200, loss is 5.171737728118896 and perplexity is 176.22079540253975
At time: 427.9285624027252 and batch: 250, loss is 5.182849292755127 and perplexity is 178.18980328116461
At time: 429.6040997505188 and batch: 300, loss is 5.217988214492798 and perplexity is 184.56251014884418
At time: 431.2792966365814 and batch: 350, loss is 5.1621683406829835 and perplexity is 174.54251321044595
At time: 432.96260690689087 and batch: 400, loss is 5.188471460342408 and perplexity is 179.19443768277085
At time: 434.6376850605011 and batch: 450, loss is 5.1607190608978275 and perplexity is 174.28973549146363
At time: 436.3106014728546 and batch: 500, loss is 5.148495445251465 and perplexity is 172.17225276674722
At time: 438.0113787651062 and batch: 550, loss is 5.159152193069458 and perplexity is 174.01686034757648
At time: 439.68669152259827 and batch: 600, loss is 5.116265707015991 and perplexity is 166.7116556145753
At time: 441.3647174835205 and batch: 650, loss is 5.106854810714721 and perplexity is 165.15010881372478
At time: 443.04581475257874 and batch: 700, loss is 5.11574914932251 and perplexity is 166.6255616644352
At time: 444.72445583343506 and batch: 750, loss is 5.119697780609131 and perplexity is 167.28480526877502
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.079592682594477 and perplexity of 160.70858319226878
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 449.15674901008606 and batch: 50, loss is 5.209526739120483 and perplexity is 183.00742743979347
At time: 450.85539841651917 and batch: 100, loss is 5.189114322662354 and perplexity is 179.3096720706554
At time: 452.5284101963043 and batch: 150, loss is 5.170610790252685 and perplexity is 176.02231737259604
At time: 454.2080981731415 and batch: 200, loss is 5.162320785522461 and perplexity is 174.56912334409148
At time: 455.88066267967224 and batch: 250, loss is 5.165442094802857 and perplexity is 175.11485883040288
At time: 457.5558075904846 and batch: 300, loss is 5.186922054290772 and perplexity is 178.91700771794015
At time: 459.2370879650116 and batch: 350, loss is 5.129280490875244 and perplexity is 168.89555242524472
At time: 460.9095392227173 and batch: 400, loss is 5.1424336338043215 and perplexity is 171.1317339347306
At time: 462.5798690319061 and batch: 450, loss is 5.1060779094696045 and perplexity is 165.02185331593518
At time: 464.2506959438324 and batch: 500, loss is 5.071832065582275 and perplexity is 159.46621244468687
At time: 465.9272437095642 and batch: 550, loss is 5.074505558013916 and perplexity is 159.89311456227466
At time: 467.5907070636749 and batch: 600, loss is 5.025388250350952 and perplexity is 152.22934773843969
At time: 469.2603180408478 and batch: 650, loss is 4.9979509925842285 and perplexity is 148.10937077752897
At time: 470.9353814125061 and batch: 700, loss is 4.993969345092774 and perplexity is 147.52082394346897
At time: 472.61291241645813 and batch: 750, loss is 5.015577211380005 and perplexity is 150.74312230463195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.00790795614553 and perplexity of 149.59145667848523
Finished 17 epochs...
Completing Train Step...
At time: 477.07931113243103 and batch: 50, loss is 5.155227499008179 and perplexity is 173.33523586791586
At time: 478.7958414554596 and batch: 100, loss is 5.147382316589355 and perplexity is 171.98070952328746
At time: 480.472275018692 and batch: 150, loss is 5.131107110977172 and perplexity is 169.20434237153503
At time: 482.1466348171234 and batch: 200, loss is 5.124371433258057 and perplexity is 168.0684661950664
At time: 483.8237781524658 and batch: 250, loss is 5.12833987236023 and perplexity is 168.73676083439292
At time: 485.4989969730377 and batch: 300, loss is 5.157311325073242 and perplexity is 173.6968129514463
At time: 487.1710743904114 and batch: 350, loss is 5.10311261177063 and perplexity is 164.53323919521242
At time: 488.8458433151245 and batch: 400, loss is 5.118470277786255 and perplexity is 167.07958867582468
At time: 490.5226764678955 and batch: 450, loss is 5.0861626148223875 and perplexity is 161.76790371392897
At time: 492.1999170780182 and batch: 500, loss is 5.055590991973877 and perplexity is 156.89722795214226
At time: 493.87649154663086 and batch: 550, loss is 5.0633367252349855 and perplexity is 158.11723083809238
At time: 495.55101323127747 and batch: 600, loss is 5.018611755371094 and perplexity is 151.20125369899753
At time: 497.22662711143494 and batch: 650, loss is 5.001775302886963 and perplexity is 148.67687142775895
At time: 498.90446066856384 and batch: 700, loss is 5.002427043914795 and perplexity is 148.7738018280865
At time: 500.5781316757202 and batch: 750, loss is 5.022507991790771 and perplexity is 151.79151868961995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.99976508561955 and perplexity of 148.37829881200744
Finished 18 epochs...
Completing Train Step...
At time: 505.0039596557617 and batch: 50, loss is 5.1437074661254885 and perplexity is 171.349865970917
At time: 506.7032587528229 and batch: 100, loss is 5.133430910110474 and perplexity is 169.59799648508707
At time: 508.37985944747925 and batch: 150, loss is 5.116394920349121 and perplexity is 166.73319837504488
At time: 510.0580837726593 and batch: 200, loss is 5.109774703979492 and perplexity is 165.63303420631732
At time: 511.7307183742523 and batch: 250, loss is 5.114070615768433 and perplexity is 166.34610966860274
At time: 513.4073934555054 and batch: 300, loss is 5.145143995285034 and perplexity is 171.59619193472312
At time: 515.0864906311035 and batch: 350, loss is 5.093436393737793 and perplexity is 162.948857472621
At time: 516.760155916214 and batch: 400, loss is 5.109633445739746 and perplexity is 165.6096388278954
At time: 518.4382035732269 and batch: 450, loss is 5.078084592819214 and perplexity is 160.46640288188433
At time: 520.1205017566681 and batch: 500, loss is 5.050992956161499 and perplexity is 156.17746489540926
At time: 521.8216049671173 and batch: 550, loss is 5.061222047805786 and perplexity is 157.7832171889059
At time: 523.4940505027771 and batch: 600, loss is 5.019013795852661 and perplexity is 151.26205494531052
At time: 525.1684577465057 and batch: 650, loss is 5.005305938720703 and perplexity is 149.2027230667447
At time: 526.8452877998352 and batch: 700, loss is 5.007019386291504 and perplexity is 149.45859325761805
At time: 528.5283639431 and batch: 750, loss is 5.024746112823486 and perplexity is 152.13162693992902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.996742780818495 and perplexity of 147.93053135284424
Finished 19 epochs...
Completing Train Step...
At time: 532.9644808769226 and batch: 50, loss is 5.136128263473511 and perplexity is 170.05607973973395
At time: 534.6649594306946 and batch: 100, loss is 5.124926958084107 and perplexity is 168.16185833893928
At time: 536.3393623828888 and batch: 150, loss is 5.107566890716552 and perplexity is 165.26775078378742
At time: 538.0114986896515 and batch: 200, loss is 5.101350212097168 and perplexity is 164.24352124258365
At time: 539.6833205223083 and batch: 250, loss is 5.106151161193847 and perplexity is 165.03394189397747
At time: 541.3590679168701 and batch: 300, loss is 5.138273057937622 and perplexity is 170.4212064991046
At time: 543.0268034934998 and batch: 350, loss is 5.0880842113494875 and perplexity is 162.07905521397808
At time: 544.6965792179108 and batch: 400, loss is 5.104804620742798 and perplexity is 164.81186656568258
At time: 546.372832775116 and batch: 450, loss is 5.073645706176758 and perplexity is 159.7556892651335
At time: 548.0489616394043 and batch: 500, loss is 5.048581447601318 and perplexity is 155.80129535224052
At time: 549.7225472927094 and batch: 550, loss is 5.059957838058471 and perplexity is 157.58387214129903
At time: 551.3964946269989 and batch: 600, loss is 5.019537420272827 and perplexity is 151.34128019145092
At time: 553.0782995223999 and batch: 650, loss is 5.006958284378052 and perplexity is 149.44946133058005
At time: 554.7509093284607 and batch: 700, loss is 5.008505582809448 and perplexity is 149.68088324087807
At time: 556.425662279129 and batch: 750, loss is 5.024848699569702 and perplexity is 152.1472344290804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.994414928347566 and perplexity of 147.58657139924614
Finished 20 epochs...
Completing Train Step...
At time: 560.8527898788452 and batch: 50, loss is 5.129743604660034 and perplexity is 168.97378839844316
At time: 562.5522434711456 and batch: 100, loss is 5.1184305000305175 and perplexity is 167.0729427569384
At time: 564.2275140285492 and batch: 150, loss is 5.100940999984741 and perplexity is 164.17632455409057
At time: 565.9000058174133 and batch: 200, loss is 5.094971866607666 and perplexity is 163.1992532111826
At time: 567.5842158794403 and batch: 250, loss is 5.100490379333496 and perplexity is 164.1023599780353
At time: 569.2618656158447 and batch: 300, loss is 5.13348949432373 and perplexity is 169.6079325413262
At time: 570.9334101676941 and batch: 350, loss is 5.084116888046265 and perplexity is 161.43730905025674
At time: 572.605682849884 and batch: 400, loss is 5.101088123321533 and perplexity is 164.20048049968938
At time: 574.2785520553589 and batch: 450, loss is 5.070459632873535 and perplexity is 159.2475059131168
At time: 575.9539823532104 and batch: 500, loss is 5.046851720809936 and perplexity is 155.53203461842497
At time: 577.6349506378174 and batch: 550, loss is 5.058923902511597 and perplexity is 157.4210247756254
At time: 579.3124816417694 and batch: 600, loss is 5.019765110015869 and perplexity is 151.37574297191128
At time: 580.9855527877808 and batch: 650, loss is 5.0076813125610355 and perplexity is 149.55755657629928
At time: 582.6595692634583 and batch: 700, loss is 5.008900241851807 and perplexity is 149.73996781328077
At time: 584.3395171165466 and batch: 750, loss is 5.0242404365539555 and perplexity is 152.05471703373652
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.992944406908612 and perplexity of 147.3697016767706
Finished 21 epochs...
Completing Train Step...
At time: 588.784752368927 and batch: 50, loss is 5.124809713363647 and perplexity is 168.14214340462496
At time: 590.4994511604309 and batch: 100, loss is 5.113305234909058 and perplexity is 166.21884025123194
At time: 592.1792013645172 and batch: 150, loss is 5.09606032371521 and perplexity is 163.3769853076208
At time: 593.8526494503021 and batch: 200, loss is 5.090240707397461 and perplexity is 162.4289551993741
At time: 595.5297017097473 and batch: 250, loss is 5.096560745239258 and perplexity is 163.45876312758693
At time: 597.2114698886871 and batch: 300, loss is 5.130024433135986 and perplexity is 169.02124771356657
At time: 598.8880677223206 and batch: 350, loss is 5.0811402797698975 and perplexity is 160.95748789411132
At time: 600.5598838329315 and batch: 400, loss is 5.098514490127563 and perplexity is 163.77843202454832
At time: 602.2366359233856 and batch: 450, loss is 5.068303117752075 and perplexity is 158.90445628736063
At time: 603.916107416153 and batch: 500, loss is 5.045576725006104 and perplexity is 155.33385829076454
At time: 605.6150505542755 and batch: 550, loss is 5.057963285446167 and perplexity is 157.2698760624083
At time: 607.2937660217285 and batch: 600, loss is 5.019486818313599 and perplexity is 151.33362221991695
At time: 608.9714529514313 and batch: 650, loss is 5.007701807022094 and perplexity is 149.5606217092276
At time: 610.654703617096 and batch: 700, loss is 5.008625946044922 and perplexity is 149.69890040055355
At time: 612.329562664032 and batch: 750, loss is 5.0227524948120115 and perplexity is 151.82863671208762
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.991803546284521 and perplexity of 147.2016692559551
Finished 22 epochs...
Completing Train Step...
At time: 616.7569379806519 and batch: 50, loss is 5.120797252655029 and perplexity is 167.46883138311765
At time: 618.4553601741791 and batch: 100, loss is 5.108960342407227 and perplexity is 165.4982039359089
At time: 620.1264817714691 and batch: 150, loss is 5.091751518249512 and perplexity is 162.67454009702755
At time: 621.8009719848633 and batch: 200, loss is 5.085874691009521 and perplexity is 161.72133358693824
At time: 623.4827117919922 and batch: 250, loss is 5.093191738128662 and perplexity is 162.9089959970045
At time: 625.1652166843414 and batch: 300, loss is 5.127161273956299 and perplexity is 168.53800510698807
At time: 626.8433125019073 and batch: 350, loss is 5.07873272895813 and perplexity is 160.57044066835655
At time: 628.5222206115723 and batch: 400, loss is 5.096118822097778 and perplexity is 163.38654287655862
At time: 630.1984345912933 and batch: 450, loss is 5.0663657474517825 and perplexity is 158.59689753695903
At time: 631.8837795257568 and batch: 500, loss is 5.044186487197876 and perplexity is 155.11805733015566
At time: 633.5648896694183 and batch: 550, loss is 5.057219352722168 and perplexity is 157.15292136371383
At time: 635.2458915710449 and batch: 600, loss is 5.018878955841064 and perplexity is 151.24166014311604
At time: 636.9228253364563 and batch: 650, loss is 5.007459669113159 and perplexity is 149.52441179710104
At time: 638.5978314876556 and batch: 700, loss is 5.008009376525879 and perplexity is 149.6066290703004
At time: 640.2753031253815 and batch: 750, loss is 5.021406517028809 and perplexity is 151.62441620918054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.991083189498546 and perplexity of 147.0956697179268
Finished 23 epochs...
Completing Train Step...
At time: 644.7519252300262 and batch: 50, loss is 5.117193927764893 and perplexity is 166.86647267348795
At time: 646.4575574398041 and batch: 100, loss is 5.105109100341797 and perplexity is 164.86205605717507
At time: 648.1357746124268 and batch: 150, loss is 5.088040342330933 and perplexity is 162.07194512085513
At time: 649.8192238807678 and batch: 200, loss is 5.0824064445495605 and perplexity is 161.16141567214666
At time: 651.5015316009521 and batch: 250, loss is 5.090091152191162 and perplexity is 162.40466491988445
At time: 653.1793208122253 and batch: 300, loss is 5.124660511016845 and perplexity is 168.11705807367434
At time: 654.8545014858246 and batch: 350, loss is 5.076515235900879 and perplexity is 160.21477132386946
At time: 656.5357351303101 and batch: 400, loss is 5.09406907081604 and perplexity is 163.05198409915693
At time: 658.2158043384552 and batch: 450, loss is 5.064583196640014 and perplexity is 158.3144423286825
At time: 659.8905911445618 and batch: 500, loss is 5.042660579681397 and perplexity is 154.88154201667302
At time: 661.5676376819611 and batch: 550, loss is 5.05624153137207 and perplexity is 156.99932898716455
At time: 663.247082233429 and batch: 600, loss is 5.018008108139038 and perplexity is 151.11000902328752
At time: 664.9325556755066 and batch: 650, loss is 5.006867437362671 and perplexity is 149.43588490976592
At time: 666.6113324165344 and batch: 700, loss is 5.006629104614258 and perplexity is 149.40027368843252
At time: 668.2908647060394 and batch: 750, loss is 5.019901428222656 and perplexity is 151.39637964828975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.990342605945676 and perplexity of 146.98677341261873
Finished 24 epochs...
Completing Train Step...
At time: 672.7347521781921 and batch: 50, loss is 5.113836126327515 and perplexity is 166.30710783528545
At time: 674.4444036483765 and batch: 100, loss is 5.10158745765686 and perplexity is 164.2824919113329
At time: 676.128479719162 and batch: 150, loss is 5.08470272064209 and perplexity is 161.53191199611973
At time: 677.803460597992 and batch: 200, loss is 5.079155607223511 and perplexity is 160.6383567768888
At time: 679.4830806255341 and batch: 250, loss is 5.087164611816406 and perplexity is 161.93007590163293
At time: 681.164826631546 and batch: 300, loss is 5.122429342269897 and perplexity is 167.7423786897703
At time: 682.8483917713165 and batch: 350, loss is 5.074542083740234 and perplexity is 159.8989548810776
At time: 684.5265369415283 and batch: 400, loss is 5.092116060256958 and perplexity is 162.73385261072949
At time: 686.2039382457733 and batch: 450, loss is 5.062787580490112 and perplexity is 158.03042542825247
At time: 687.8802738189697 and batch: 500, loss is 5.041052665710449 and perplexity is 154.63270592854525
At time: 689.5918619632721 and batch: 550, loss is 5.054884529113769 and perplexity is 156.78642503142274
At time: 691.275030374527 and batch: 600, loss is 5.016962699890136 and perplexity is 150.95211991702945
At time: 692.9670460224152 and batch: 650, loss is 5.005831451416015 and perplexity is 149.28115159768092
At time: 694.6764664649963 and batch: 700, loss is 5.005057325363159 and perplexity is 149.16563388743222
At time: 696.3880908489227 and batch: 750, loss is 5.018188047409057 and perplexity is 151.1372020944811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.989721253860829 and perplexity of 146.89547124284806
Finished 25 epochs...
Completing Train Step...
At time: 700.8683276176453 and batch: 50, loss is 5.1106800365447995 and perplexity is 165.78305508522175
At time: 702.5796315670013 and batch: 100, loss is 5.09838623046875 and perplexity is 163.75742720580018
At time: 704.258780002594 and batch: 150, loss is 5.081773529052734 and perplexity is 161.05944638705157
At time: 705.929211139679 and batch: 200, loss is 5.076206960678101 and perplexity is 160.16538869165927
At time: 707.597864151001 and batch: 250, loss is 5.084469938278199 and perplexity is 161.49431459197743
At time: 709.2781178951263 and batch: 300, loss is 5.120301065444946 and perplexity is 167.38575610304795
At time: 710.9804766178131 and batch: 350, loss is 5.072851858139038 and perplexity is 159.6289178499729
At time: 712.6763718128204 and batch: 400, loss is 5.0904389762878415 and perplexity is 162.46116300088414
At time: 714.3798539638519 and batch: 450, loss is 5.061329517364502 and perplexity is 157.80017499283755
At time: 716.0846989154816 and batch: 500, loss is 5.0395174312591555 and perplexity is 154.395490608265
At time: 717.7910959720612 and batch: 550, loss is 5.053597497940063 and perplexity is 156.5847658134799
At time: 719.4906079769135 and batch: 600, loss is 5.01577094078064 and perplexity is 150.77232850832476
At time: 721.1901307106018 and batch: 650, loss is 5.004580316543579 and perplexity is 149.09449753217282
At time: 722.8937649726868 and batch: 700, loss is 5.003683166503906 and perplexity is 148.96079738131243
At time: 724.5971686840057 and batch: 750, loss is 5.016520700454712 and perplexity is 150.88541390834507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.988846534906432 and perplexity of 146.76703517075944
Finished 26 epochs...
Completing Train Step...
At time: 729.0831701755524 and batch: 50, loss is 5.107699089050293 and perplexity is 165.2896003492683
At time: 730.8145558834076 and batch: 100, loss is 5.095415210723877 and perplexity is 163.2716226809677
At time: 732.4996905326843 and batch: 150, loss is 5.079097557067871 and perplexity is 160.62903196593217
At time: 734.1783208847046 and batch: 200, loss is 5.073668212890625 and perplexity is 159.75928488118296
At time: 735.8553133010864 and batch: 250, loss is 5.082021837234497 and perplexity is 161.09944373096803
At time: 737.5302472114563 and batch: 300, loss is 5.11840253829956 and perplexity is 167.06827117357585
At time: 739.2249343395233 and batch: 350, loss is 5.071229248046875 and perplexity is 159.37011238383243
At time: 740.9333214759827 and batch: 400, loss is 5.088841695785522 and perplexity is 162.20187408652944
At time: 742.6314580440521 and batch: 450, loss is 5.05996262550354 and perplexity is 157.58462656723654
At time: 744.3268773555756 and batch: 500, loss is 5.038063383102417 and perplexity is 154.17115526645756
At time: 746.0266206264496 and batch: 550, loss is 5.052212839126587 and perplexity is 156.36809937660334
At time: 747.7297070026398 and batch: 600, loss is 5.0144753170013425 and perplexity is 150.57711078591936
At time: 749.4363865852356 and batch: 650, loss is 5.003291473388672 and perplexity is 148.90246188808976
At time: 751.1373245716095 and batch: 700, loss is 5.002279806137085 and perplexity is 148.75189831667552
At time: 752.8410696983337 and batch: 750, loss is 5.01470838546753 and perplexity is 150.6122096522337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.988087144008903 and perplexity of 146.6556239278893
Finished 27 epochs...
Completing Train Step...
At time: 757.3490605354309 and batch: 50, loss is 5.104857425689698 and perplexity is 164.82056967732663
At time: 759.0587575435638 and batch: 100, loss is 5.092723588943482 and perplexity is 162.83274813235036
At time: 760.7366380691528 and batch: 150, loss is 5.076599063873291 and perplexity is 160.22820236624074
At time: 762.4210519790649 and batch: 200, loss is 5.071281576156617 and perplexity is 159.37845213876264
At time: 764.1257503032684 and batch: 250, loss is 5.079750671386718 and perplexity is 160.73397535301575
At time: 765.8266031742096 and batch: 300, loss is 5.116679639816284 and perplexity is 166.78067732121397
At time: 767.5287930965424 and batch: 350, loss is 5.069753837585449 and perplexity is 159.13514942880664
At time: 769.2400276660919 and batch: 400, loss is 5.0871854019165035 and perplexity is 161.93344247911534
At time: 770.940670967102 and batch: 450, loss is 5.058632698059082 and perplexity is 157.37518974629745
At time: 772.6400945186615 and batch: 500, loss is 5.0366185283660885 and perplexity is 153.94856118934297
At time: 774.3870134353638 and batch: 550, loss is 5.050890140533447 and perplexity is 156.1614082367203
At time: 776.0898680686951 and batch: 600, loss is 5.013109655380249 and perplexity is 150.3716137563453
At time: 777.7867736816406 and batch: 650, loss is 5.001940412521362 and perplexity is 148.7014214383099
At time: 779.4908061027527 and batch: 700, loss is 5.000798273086548 and perplexity is 148.53168063315783
At time: 781.2028665542603 and batch: 750, loss is 5.012785205841064 and perplexity is 150.3228336693219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.987424273823583 and perplexity of 146.55844250017202
Finished 28 epochs...
Completing Train Step...
At time: 785.6360142230988 and batch: 50, loss is 5.10225206375122 and perplexity is 164.39171134658477
At time: 787.3355340957642 and batch: 100, loss is 5.090100202560425 and perplexity is 162.40613474872325
At time: 789.036333322525 and batch: 150, loss is 5.074184989929199 and perplexity is 159.8418661475219
At time: 790.7361414432526 and batch: 200, loss is 5.068997735977173 and perplexity is 159.01487256288618
At time: 792.4341435432434 and batch: 250, loss is 5.0775463581085205 and perplexity is 160.38005753306965
At time: 794.137265920639 and batch: 300, loss is 5.115071086883545 and perplexity is 166.5126174256495
At time: 795.842545747757 and batch: 350, loss is 5.068331537246704 and perplexity is 158.90897233587418
At time: 797.5616884231567 and batch: 400, loss is 5.085535402297974 and perplexity is 161.66647267140678
At time: 799.2834866046906 and batch: 450, loss is 5.057243404388427 and perplexity is 157.15670119878567
At time: 800.9963052272797 and batch: 500, loss is 5.035128755569458 and perplexity is 153.71938356448507
At time: 802.7121117115021 and batch: 550, loss is 5.049584579467774 and perplexity is 155.9576630120222
At time: 804.426659822464 and batch: 600, loss is 5.011691637039185 and perplexity is 150.1585351604464
At time: 806.1405253410339 and batch: 650, loss is 5.000540561676026 and perplexity is 148.49340725618947
At time: 807.8642907142639 and batch: 700, loss is 4.99930923461914 and perplexity is 148.3106758302279
At time: 809.580527305603 and batch: 750, loss is 5.010741348266602 and perplexity is 150.01590896914323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.986687948537427 and perplexity of 146.45056753346628
Finished 29 epochs...
Completing Train Step...
At time: 814.1205606460571 and batch: 50, loss is 5.0997122859954835 and perplexity is 163.97472268828633
At time: 815.8404955863953 and batch: 100, loss is 5.08740647315979 and perplexity is 161.96924526391112
At time: 817.5437932014465 and batch: 150, loss is 5.071675987243652 and perplexity is 159.4413251654151
At time: 819.2576117515564 and batch: 200, loss is 5.066705198287964 and perplexity is 158.65074252478973
At time: 820.9807431697845 and batch: 250, loss is 5.075298528671265 and perplexity is 160.01995539431715
At time: 822.6990885734558 and batch: 300, loss is 5.11332799911499 and perplexity is 166.22262413420955
At time: 824.4193468093872 and batch: 350, loss is 5.06691692352295 and perplexity is 158.6843364467465
At time: 826.1612095832825 and batch: 400, loss is 5.083828535079956 and perplexity is 161.39076483422306
At time: 827.8984425067902 and batch: 450, loss is 5.055670957565308 and perplexity is 156.90977483342206
At time: 829.6388120651245 and batch: 500, loss is 5.033462839126587 and perplexity is 153.46351310439633
At time: 831.3799483776093 and batch: 550, loss is 5.047919158935547 and perplexity is 155.6981440819452
At time: 833.102995634079 and batch: 600, loss is 5.009910078048706 and perplexity is 149.89125702870743
At time: 834.8212621212006 and batch: 650, loss is 4.998902616500854 and perplexity is 148.25038228135548
At time: 836.536915063858 and batch: 700, loss is 4.9977690124511716 and perplexity is 148.08242026683104
At time: 838.2536723613739 and batch: 750, loss is 5.008458232879638 and perplexity is 149.67379602935355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.985615220180778 and perplexity of 146.29355009041484
Finished 30 epochs...
Completing Train Step...
At time: 842.7221314907074 and batch: 50, loss is 5.097037000656128 and perplexity is 163.5366297896993
At time: 844.4253478050232 and batch: 100, loss is 5.084678106307983 and perplexity is 161.52793604460183
At time: 846.1053681373596 and batch: 150, loss is 5.069247035980225 and perplexity is 159.0545199129383
At time: 847.802805185318 and batch: 200, loss is 5.064283800125122 and perplexity is 158.26705063120076
At time: 849.5065243244171 and batch: 250, loss is 5.072970066070557 and perplexity is 159.6477883694631
At time: 851.2129487991333 and batch: 300, loss is 5.11151050567627 and perplexity is 165.92078997936375
At time: 852.9260487556458 and batch: 350, loss is 5.065408601760864 and perplexity is 158.4451698242352
At time: 854.6445605754852 and batch: 400, loss is 5.082146253585815 and perplexity is 161.11948838287182
At time: 856.3564031124115 and batch: 450, loss is 5.0539293956756595 and perplexity is 156.63674456802056
At time: 858.0694875717163 and batch: 500, loss is 5.0314521217346195 and perplexity is 153.1552513670413
At time: 859.8077917098999 and batch: 550, loss is 5.045989484786987 and perplexity is 155.39798709406134
At time: 861.5264670848846 and batch: 600, loss is 5.00788646697998 and perplexity is 149.588242117447
At time: 863.2576959133148 and batch: 650, loss is 4.997164888381958 and perplexity is 147.99298712959583
At time: 864.9844632148743 and batch: 700, loss is 4.995895986557007 and perplexity is 147.80531765040016
At time: 866.7081081867218 and batch: 750, loss is 5.006160287857056 and perplexity is 149.3302487523111
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984469391578852 and perplexity of 146.12601875587103
Finished 31 epochs...
Completing Train Step...
At time: 871.183961391449 and batch: 50, loss is 5.094151382446289 and perplexity is 163.06540572615384
At time: 872.8808436393738 and batch: 100, loss is 5.0818037509918215 and perplexity is 161.06431398938355
At time: 874.5742163658142 and batch: 150, loss is 5.066754302978516 and perplexity is 158.65853321168532
At time: 876.2790606021881 and batch: 200, loss is 5.061978759765625 and perplexity is 157.90265882217142
At time: 877.9835231304169 and batch: 250, loss is 5.070638093948364 and perplexity is 159.2759279302223
At time: 879.6946113109589 and batch: 300, loss is 5.109715995788574 and perplexity is 165.62331047595683
At time: 881.4105739593506 and batch: 350, loss is 5.063797082901001 and perplexity is 158.1900380748368
At time: 883.1327793598175 and batch: 400, loss is 5.080334577560425 and perplexity is 160.82785631971944
At time: 884.8523550033569 and batch: 450, loss is 5.052079725265503 and perplexity is 156.3472860004501
At time: 886.5709428787231 and batch: 500, loss is 5.029474420547485 and perplexity is 152.85265536546132
At time: 888.2833688259125 and batch: 550, loss is 5.044122238159179 and perplexity is 155.10809146423983
At time: 889.9979360103607 and batch: 600, loss is 5.0059781265258785 and perplexity is 149.3030490328571
At time: 891.7132835388184 and batch: 650, loss is 4.9954421043396 and perplexity is 147.73824666739515
At time: 893.4432179927826 and batch: 700, loss is 4.9941162014007565 and perplexity is 147.54248989787595
At time: 895.172200679779 and batch: 750, loss is 5.004002828598022 and perplexity is 149.0084221132497
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983192266419876 and perplexity of 145.93951665954617
Finished 32 epochs...
Completing Train Step...
At time: 899.701244354248 and batch: 50, loss is 5.091134281158447 and perplexity is 162.57416231873847
At time: 901.4078648090363 and batch: 100, loss is 5.079114084243774 and perplexity is 160.6316867321365
At time: 903.1062126159668 and batch: 150, loss is 5.064235458374023 and perplexity is 158.2593999097581
At time: 904.8141739368439 and batch: 200, loss is 5.05964282989502 and perplexity is 157.53423975286037
At time: 906.5368783473969 and batch: 250, loss is 5.068077526092529 and perplexity is 158.8686128105043
At time: 908.2600939273834 and batch: 300, loss is 5.107758102416992 and perplexity is 165.29935493288787
At time: 909.9857788085938 and batch: 350, loss is 5.061851100921631 and perplexity is 157.88250243788025
At time: 911.7128026485443 and batch: 400, loss is 5.078231773376465 and perplexity is 160.4900221545868
At time: 913.4438610076904 and batch: 450, loss is 5.049993515014648 and perplexity is 156.02145268627905
At time: 915.1619865894318 and batch: 500, loss is 5.027318115234375 and perplexity is 152.5234134731663
At time: 916.891480922699 and batch: 550, loss is 5.041946506500244 and perplexity is 154.77098473895455
At time: 918.6224546432495 and batch: 600, loss is 5.003781929016113 and perplexity is 148.97550985039035
At time: 920.3546726703644 and batch: 650, loss is 4.993146009445191 and perplexity is 147.39941477746106
At time: 922.0858879089355 and batch: 700, loss is 4.9918227386474605 and perplexity is 147.2044944309276
At time: 923.8163342475891 and batch: 750, loss is 5.001692657470703 and perplexity is 148.6645844735688
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.981731326081032 and perplexity of 145.72646339957188
Finished 33 epochs...
Completing Train Step...
At time: 928.3310260772705 and batch: 50, loss is 5.08771131515503 and perplexity is 162.01862781836064
At time: 930.0448575019836 and batch: 100, loss is 5.076261196136475 and perplexity is 160.17407557049694
At time: 931.7209134101868 and batch: 150, loss is 5.061584796905517 and perplexity is 157.84046329125223
At time: 933.4068677425385 and batch: 200, loss is 5.057239933013916 and perplexity is 157.15615564996574
At time: 935.1080603599548 and batch: 250, loss is 5.065430517196655 and perplexity is 158.44864225723074
At time: 936.8187837600708 and batch: 300, loss is 5.1056232929229735 and perplexity is 164.94684870132326
At time: 938.5343255996704 and batch: 350, loss is 5.059800777435303 and perplexity is 157.55912386368004
At time: 940.2589378356934 and batch: 400, loss is 5.076004362106323 and perplexity is 160.13294269952894
At time: 941.9865868091583 and batch: 450, loss is 5.04783187866211 and perplexity is 155.6845552983811
At time: 943.7193109989166 and batch: 500, loss is 5.025099744796753 and perplexity is 152.18543506093536
At time: 945.4632709026337 and batch: 550, loss is 5.039751844406128 and perplexity is 154.43168718340763
At time: 947.1717534065247 and batch: 600, loss is 5.001628389358521 and perplexity is 148.6550303883911
At time: 948.8861691951752 and batch: 650, loss is 4.991113691329956 and perplexity is 147.10015647367246
At time: 950.6001238822937 and batch: 700, loss is 4.989599304199219 and perplexity is 146.87755848208775
At time: 952.3115227222443 and batch: 750, loss is 4.999373226165772 and perplexity is 148.32016676342275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980577335801235 and perplexity of 145.5583934714827
Finished 34 epochs...
Completing Train Step...
At time: 956.7896773815155 and batch: 50, loss is 5.084702081680298 and perplexity is 161.53180878343275
At time: 958.5191602706909 and batch: 100, loss is 5.07357123374939 and perplexity is 159.74379231417097
At time: 960.2106456756592 and batch: 150, loss is 5.059217081069947 and perplexity is 157.4671840108414
At time: 961.9161050319672 and batch: 200, loss is 5.055074644088745 and perplexity is 156.8162353122948
At time: 963.628509759903 and batch: 250, loss is 5.063052806854248 and perplexity is 158.07234482223018
At time: 965.3303489685059 and batch: 300, loss is 5.1036562252044675 and perplexity is 164.6227059898242
At time: 967.0468883514404 and batch: 350, loss is 5.057914438247681 and perplexity is 157.26219405718012
At time: 968.7647647857666 and batch: 400, loss is 5.0739422798156735 and perplexity is 159.80307561766048
At time: 970.4816744327545 and batch: 450, loss is 5.045786199569702 and perplexity is 155.3664001911732
At time: 972.1936407089233 and batch: 500, loss is 5.023030824661255 and perplexity is 151.87090103509303
At time: 973.9205756187439 and batch: 550, loss is 5.037655820846558 and perplexity is 154.1083337253429
At time: 975.6516420841217 and batch: 600, loss is 4.999645051956176 and perplexity is 148.36048949011638
At time: 977.3787379264832 and batch: 650, loss is 4.9892373752594 and perplexity is 146.82440886183613
At time: 979.1047766208649 and batch: 700, loss is 4.987523994445801 and perplexity is 146.57305812797702
At time: 980.8472592830658 and batch: 750, loss is 4.997206783294677 and perplexity is 147.99918741275397
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.979440733443859 and perplexity of 145.39304544360687
Finished 35 epochs...
Completing Train Step...
At time: 985.319961309433 and batch: 50, loss is 5.08205325126648 and perplexity is 161.10450459353643
At time: 987.027801990509 and batch: 100, loss is 5.070961561203003 and perplexity is 159.32745681086337
At time: 988.717182636261 and batch: 150, loss is 5.056943082809449 and perplexity is 157.1095107366463
At time: 990.4262857437134 and batch: 200, loss is 5.053034057617188 and perplexity is 156.49656449289623
At time: 992.1302914619446 and batch: 250, loss is 5.060779790878296 and perplexity is 157.71345189629162
At time: 993.8377151489258 and batch: 300, loss is 5.101828889846802 and perplexity is 164.32215978148182
At time: 995.5478122234344 and batch: 350, loss is 5.056168880462646 and perplexity is 156.98792325745808
At time: 997.2748544216156 and batch: 400, loss is 5.071850719451905 and perplexity is 159.46918713436875
At time: 999.0061960220337 and batch: 450, loss is 5.043770208358764 and perplexity is 155.05349840351488
At time: 1000.7347943782806 and batch: 500, loss is 5.020964097976685 and perplexity is 151.55734951556784
At time: 1002.4720888137817 and batch: 550, loss is 5.035710544586181 and perplexity is 153.8088418338964
At time: 1004.2009289264679 and batch: 600, loss is 4.997761936187744 and perplexity is 148.08137240032372
At time: 1005.9334716796875 and batch: 650, loss is 4.987304639816284 and perplexity is 146.54091017514432
At time: 1007.6672711372375 and batch: 700, loss is 4.9854940414428714 and perplexity is 146.27582349671587
At time: 1009.4037125110626 and batch: 750, loss is 4.9950635051727295 and perplexity is 147.68232367715873
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978247354196948 and perplexity of 145.2196398904032
Finished 36 epochs...
Completing Train Step...
At time: 1013.9247851371765 and batch: 50, loss is 5.079481325149536 and perplexity is 160.69068809145926
At time: 1015.633852481842 and batch: 100, loss is 5.0684246158599855 and perplexity is 158.92376405104272
At time: 1017.3165304660797 and batch: 150, loss is 5.054765691757202 and perplexity is 156.76779405417642
At time: 1019.0032019615173 and batch: 200, loss is 5.050984497070313 and perplexity is 156.17614378158015
At time: 1020.7035934925079 and batch: 250, loss is 5.058526678085327 and perplexity is 157.3585057172467
At time: 1022.4056959152222 and batch: 300, loss is 5.100042028427124 and perplexity is 164.0288010275447
At time: 1024.10444688797 and batch: 350, loss is 5.054432907104492 and perplexity is 156.71563281798677
At time: 1025.8162927627563 and batch: 400, loss is 5.0697971439361575 and perplexity is 159.14204114062417
At time: 1027.5316090583801 and batch: 450, loss is 5.041834745407105 and perplexity is 154.75368833106464
At time: 1029.2497279644012 and batch: 500, loss is 5.018903436660767 and perplexity is 151.24536270825033
At time: 1031.0211923122406 and batch: 550, loss is 5.033917188644409 and perplexity is 153.5332550199817
At time: 1032.742825269699 and batch: 600, loss is 4.995987548828125 and perplexity is 147.81885166056043
At time: 1034.4622378349304 and batch: 650, loss is 4.985381088256836 and perplexity is 146.25930210950116
At time: 1036.1772682666779 and batch: 700, loss is 4.983573656082154 and perplexity is 145.99518709790792
At time: 1037.8948771953583 and batch: 750, loss is 4.993023023605347 and perplexity is 147.38128785134265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.977035167605378 and perplexity of 145.04371323960126
Finished 37 epochs...
Completing Train Step...
At time: 1042.437912940979 and batch: 50, loss is 5.076956338882447 and perplexity is 160.28545812612506
At time: 1044.1478703022003 and batch: 100, loss is 5.065948972702026 and perplexity is 158.53081212699763
At time: 1045.829871892929 and batch: 150, loss is 5.052619571685791 and perplexity is 156.43171230968926
At time: 1047.5140218734741 and batch: 200, loss is 5.0489802742004395 and perplexity is 155.86344544573595
At time: 1049.2214395999908 and batch: 250, loss is 5.056345233917236 and perplexity is 157.0156110614016
At time: 1050.9384784698486 and batch: 300, loss is 5.0983084869384765 and perplexity is 163.7446966201674
At time: 1052.648378610611 and batch: 350, loss is 5.052695684432983 and perplexity is 156.4436192101888
At time: 1054.361686706543 and batch: 400, loss is 5.06776388168335 and perplexity is 158.81879237166163
At time: 1056.0910725593567 and batch: 450, loss is 5.039941148757935 and perplexity is 154.46092454114049
At time: 1057.8249418735504 and batch: 500, loss is 5.016871700286865 and perplexity is 150.93838395899687
At time: 1059.5584874153137 and batch: 550, loss is 5.032195491790771 and perplexity is 153.26914472206005
At time: 1061.2863748073578 and batch: 600, loss is 4.994249620437622 and perplexity is 147.5621761880086
At time: 1063.0145647525787 and batch: 650, loss is 4.983479976654053 and perplexity is 145.98151099286994
At time: 1064.7482426166534 and batch: 700, loss is 4.981652898788452 and perplexity is 145.71503491574853
At time: 1066.4781959056854 and batch: 750, loss is 4.9910350513458255 and perplexity is 147.08858897454166
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.975846401480741 and perplexity of 144.87139263145428
Finished 38 epochs...
Completing Train Step...
At time: 1071.0108172893524 and batch: 50, loss is 5.074474334716797 and perplexity is 159.88812224998998
At time: 1072.7558012008667 and batch: 100, loss is 5.063516283035279 and perplexity is 158.1456245693318
At time: 1074.4502601623535 and batch: 150, loss is 5.050463266372681 and perplexity is 156.09476119259776
At time: 1076.1433067321777 and batch: 200, loss is 5.046938209533692 and perplexity is 155.54548696733204
At time: 1077.8316309452057 and batch: 250, loss is 5.054222030639648 and perplexity is 156.68258866359204
At time: 1079.5383975505829 and batch: 300, loss is 5.09657585144043 and perplexity is 163.46123238719653
At time: 1081.2626595497131 and batch: 350, loss is 5.0509563827514645 and perplexity is 156.1717530573988
At time: 1082.9905903339386 and batch: 400, loss is 5.06575834274292 and perplexity is 158.5005942850693
At time: 1084.718175649643 and batch: 450, loss is 5.0380355072021485 and perplexity is 154.16685766660913
At time: 1086.4420778751373 and batch: 500, loss is 5.014876766204834 and perplexity is 150.63757198234597
At time: 1088.1670083999634 and batch: 550, loss is 5.030516729354859 and perplexity is 153.0120580933814
At time: 1089.892949104309 and batch: 600, loss is 4.992463865280151 and perplexity is 147.29890141294052
At time: 1091.6164064407349 and batch: 650, loss is 4.981567935943604 and perplexity is 145.70265507776548
At time: 1093.3413662910461 and batch: 700, loss is 4.979677534103393 and perplexity is 145.42747868940637
At time: 1095.0753259658813 and batch: 750, loss is 4.989038209915162 and perplexity is 146.79516943973906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.974682830100836 and perplexity of 144.70292245776517
Finished 39 epochs...
Completing Train Step...
At time: 1099.6193056106567 and batch: 50, loss is 5.072056398391724 and perplexity is 159.50198996102205
At time: 1101.3360979557037 and batch: 100, loss is 5.061131725311279 and perplexity is 157.76896645872853
At time: 1103.014538526535 and batch: 150, loss is 5.048284711837769 and perplexity is 155.75507039454044
At time: 1104.7084283828735 and batch: 200, loss is 5.044873361587524 and perplexity is 155.22464055155993
At time: 1106.4162058830261 and batch: 250, loss is 5.052122611999511 and perplexity is 156.3539913687024
At time: 1108.1337485313416 and batch: 300, loss is 5.094856128692627 and perplexity is 163.18036596288655
At time: 1109.8523836135864 and batch: 350, loss is 5.0491663932800295 and perplexity is 155.89245730649068
At time: 1111.5707666873932 and batch: 400, loss is 5.063740091323853 and perplexity is 158.18102283197666
At time: 1113.2855229377747 and batch: 450, loss is 5.0361347675323485 and perplexity is 153.87410491599542
At time: 1115.0036270618439 and batch: 500, loss is 5.012881307601929 and perplexity is 150.33728065251486
At time: 1116.732236623764 and batch: 550, loss is 5.028853836059571 and perplexity is 152.7578268062148
At time: 1118.4336206912994 and batch: 600, loss is 4.990588073730469 and perplexity is 147.02285835894688
At time: 1120.1434893608093 and batch: 650, loss is 4.979613904953003 and perplexity is 145.41822555688154
At time: 1121.857810974121 and batch: 700, loss is 4.977648916244507 and perplexity is 145.13276094487497
At time: 1123.5777826309204 and batch: 750, loss is 4.986983156204223 and perplexity is 146.4938072458421
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.973572487054869 and perplexity of 144.5423417404224
Finished 40 epochs...
Completing Train Step...
At time: 1128.0473101139069 and batch: 50, loss is 5.069655323028565 and perplexity is 159.11947307226325
At time: 1129.7729408740997 and batch: 100, loss is 5.058789186477661 and perplexity is 157.39981906791593
At time: 1131.4613780975342 and batch: 150, loss is 5.0460591506958 and perplexity is 155.4088134131683
At time: 1133.1586887836456 and batch: 200, loss is 5.04281907081604 and perplexity is 154.90609131337408
At time: 1134.8569419384003 and batch: 250, loss is 5.050077743530274 and perplexity is 156.0345946951027
At time: 1136.5584893226624 and batch: 300, loss is 5.093113431930542 and perplexity is 162.89623971234295
At time: 1138.2769615650177 and batch: 350, loss is 5.047330846786499 and perplexity is 155.60657191134908
At time: 1140.0026009082794 and batch: 400, loss is 5.0617434024810795 and perplexity is 157.8654996541804
At time: 1141.7023487091064 and batch: 450, loss is 5.034244251251221 and perplexity is 153.58347821921836
At time: 1143.4018313884735 and batch: 500, loss is 5.010898551940918 and perplexity is 150.03949387500745
At time: 1145.124490737915 and batch: 550, loss is 5.027190570831299 and perplexity is 152.50396120597995
At time: 1146.837734222412 and batch: 600, loss is 4.988645792007446 and perplexity is 146.73757568763372
At time: 1148.550729751587 and batch: 650, loss is 4.977641839981079 and perplexity is 145.13173395086005
At time: 1150.259919643402 and batch: 700, loss is 4.975598859786987 and perplexity is 144.8355353598138
At time: 1151.9756925106049 and batch: 750, loss is 4.984872121810913 and perplexity is 146.18487997311496
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.972560084143351 and perplexity of 144.39608070284945
Finished 41 epochs...
Completing Train Step...
At time: 1156.4842839241028 and batch: 50, loss is 5.067228002548218 and perplexity is 158.7337074942042
At time: 1158.2058305740356 and batch: 100, loss is 5.056441144943237 and perplexity is 157.0306713119672
At time: 1159.888530254364 and batch: 150, loss is 5.043802566528321 and perplexity is 155.05851573208184
At time: 1161.5922169685364 and batch: 200, loss is 5.040706281661987 and perplexity is 154.57915290133374
At time: 1163.314957857132 and batch: 250, loss is 5.047974653244019 and perplexity is 155.7067846825313
At time: 1165.033977508545 and batch: 300, loss is 5.091282987594605 and perplexity is 162.59833994066767
At time: 1166.7487947940826 and batch: 350, loss is 5.045447158813476 and perplexity is 155.31373357792907
At time: 1168.4637656211853 and batch: 400, loss is 5.059723653793335 and perplexity is 157.54697279879562
At time: 1170.18909907341 and batch: 450, loss is 5.032277193069458 and perplexity is 153.2816675187243
At time: 1171.9093296527863 and batch: 500, loss is 5.0088558101654055 and perplexity is 149.73331476179342
At time: 1173.6277623176575 and batch: 550, loss is 5.025500688552857 and perplexity is 152.24646509488088
At time: 1175.3521559238434 and batch: 600, loss is 4.986643362045288 and perplexity is 146.4440379619546
At time: 1177.0686795711517 and batch: 650, loss is 4.975603981018066 and perplexity is 144.83627709795817
At time: 1178.784213066101 and batch: 700, loss is 4.973430099487305 and perplexity is 144.521762173144
At time: 1180.4997119903564 and batch: 750, loss is 4.982648859024048 and perplexity is 145.86023359030378
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.971683236055596 and perplexity of 144.26952276975553
Finished 42 epochs...
Completing Train Step...
At time: 1185.008059978485 and batch: 50, loss is 5.064656457901001 and perplexity is 158.3260410692239
At time: 1186.7053651809692 and batch: 100, loss is 5.053977880477905 and perplexity is 156.64433925371733
At time: 1188.382999420166 and batch: 150, loss is 5.041478986740112 and perplexity is 154.69864315717126
At time: 1190.0793476104736 and batch: 200, loss is 5.038526411056519 and perplexity is 154.24255735036687
At time: 1191.7695960998535 and batch: 250, loss is 5.045821409225464 and perplexity is 155.37187068494754
At time: 1193.460036277771 and batch: 300, loss is 5.089357271194458 and perplexity is 162.28552294588624
At time: 1195.1723828315735 and batch: 350, loss is 5.043503894805908 and perplexity is 155.01221105340667
At time: 1196.89386844635 and batch: 400, loss is 5.0576785373687745 and perplexity is 157.22510014279877
At time: 1198.6092541217804 and batch: 450, loss is 5.030176410675049 and perplexity is 152.95999409145472
At time: 1200.3233850002289 and batch: 500, loss is 5.006843767166138 and perplexity is 149.43234777486344
At time: 1202.0867702960968 and batch: 550, loss is 5.023890514373779 and perplexity is 152.00151902377056
At time: 1203.8057417869568 and batch: 600, loss is 4.984608955383301 and perplexity is 146.14641408216917
At time: 1205.5201106071472 and batch: 650, loss is 4.973601045608521 and perplexity is 144.5464697195882
At time: 1207.2358567714691 and batch: 700, loss is 4.971143980026245 and perplexity is 144.19174553255712
At time: 1208.951946735382 and batch: 750, loss is 4.980474433898926 and perplexity is 145.54341600651543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.97107820732649 and perplexity of 144.18226196405442
Finished 43 epochs...
Completing Train Step...
At time: 1213.4467771053314 and batch: 50, loss is 5.062087516784668 and perplexity is 157.91983277852108
At time: 1215.1621582508087 and batch: 100, loss is 5.051508283615112 and perplexity is 156.25796817169814
At time: 1216.844290971756 and batch: 150, loss is 5.039140195846557 and perplexity is 154.3372581460261
At time: 1218.5287108421326 and batch: 200, loss is 5.0363452339172365 and perplexity is 153.9064936508354
At time: 1220.2356634140015 and batch: 250, loss is 5.043632192611694 and perplexity is 155.03210005578532
At time: 1221.9478950500488 and batch: 300, loss is 5.087491598129272 and perplexity is 161.9830334778236
At time: 1223.65274643898 and batch: 350, loss is 5.041585655212402 and perplexity is 154.71514550522662
At time: 1225.3568058013916 and batch: 400, loss is 5.055786819458008 and perplexity is 156.92795575013469
At time: 1227.060640335083 and batch: 450, loss is 5.028242864608765 and perplexity is 152.66452464053117
At time: 1228.7662889957428 and batch: 500, loss is 5.00487865447998 and perplexity is 149.13898471267223
At time: 1230.4692039489746 and batch: 550, loss is 5.022223224639893 and perplexity is 151.74829960529502
At time: 1232.1790564060211 and batch: 600, loss is 4.982599334716797 and perplexity is 145.8530101421494
At time: 1233.8870024681091 and batch: 650, loss is 4.971655855178833 and perplexity is 144.2655725978117
At time: 1235.5929627418518 and batch: 700, loss is 4.968948163986206 and perplexity is 143.87547434862287
At time: 1237.3154244422913 and batch: 750, loss is 4.978339729309082 and perplexity is 145.2330551905325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.970300363939862 and perplexity of 144.07015435185153
Finished 44 epochs...
Completing Train Step...
At time: 1241.8495676517487 and batch: 50, loss is 5.059564657211304 and perplexity is 157.52192535989263
At time: 1243.55393242836 and batch: 100, loss is 5.049113550186157 and perplexity is 155.88421968438792
At time: 1245.2388427257538 and batch: 150, loss is 5.036807117462158 and perplexity is 153.97759694716646
At time: 1246.9307720661163 and batch: 200, loss is 5.034169139862061 and perplexity is 153.571942784044
At time: 1248.6448562145233 and batch: 250, loss is 5.041471862792969 and perplexity is 154.6975410961398
At time: 1250.3657100200653 and batch: 300, loss is 5.085633430480957 and perplexity is 161.68232131876644
At time: 1252.084870815277 and batch: 350, loss is 5.03966383934021 and perplexity is 154.418097010608
At time: 1253.796495437622 and batch: 400, loss is 5.053892803192139 and perplexity is 156.63101294539402
At time: 1255.5078947544098 and batch: 450, loss is 5.026360235214233 and perplexity is 152.37738429317903
At time: 1257.221229314804 and batch: 500, loss is 5.002913265228272 and perplexity is 148.84615641016674
At time: 1258.9390187263489 and batch: 550, loss is 5.020428733825684 and perplexity is 151.47623285922882
At time: 1260.6508049964905 and batch: 600, loss is 4.9805753135681154 and perplexity is 145.5580991187762
At time: 1262.3591949939728 and batch: 650, loss is 4.969671421051025 and perplexity is 143.9795709416601
At time: 1264.068739414215 and batch: 700, loss is 4.966864213943482 and perplexity is 143.5759572454435
At time: 1265.78240609169 and batch: 750, loss is 4.976176290512085 and perplexity is 144.91919199864896
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.969449775163517 and perplexity of 143.94766199824946
Finished 45 epochs...
Completing Train Step...
At time: 1270.2093484401703 and batch: 50, loss is 5.056937427520752 and perplexity is 157.1086222395184
At time: 1271.9251408576965 and batch: 100, loss is 5.0468199253082275 and perplexity is 155.52708947796953
At time: 1273.6032662391663 and batch: 150, loss is 5.034432582855224 and perplexity is 153.61240556590113
At time: 1275.273787021637 and batch: 200, loss is 5.0318540096282955 and perplexity is 153.21681497842505
At time: 1276.973522901535 and batch: 250, loss is 5.039306602478027 and perplexity is 154.36294302627243
At time: 1278.683786869049 and batch: 300, loss is 5.083588762283325 and perplexity is 161.35207235807536
At time: 1280.4078414440155 and batch: 350, loss is 5.037503976821899 and perplexity is 154.08493507223437
At time: 1282.1192545890808 and batch: 400, loss is 5.0518037223815915 and perplexity is 156.30413965315432
At time: 1283.8304150104523 and batch: 450, loss is 5.024185009002686 and perplexity is 152.0462892466803
At time: 1285.5484817028046 and batch: 500, loss is 5.000612478256226 and perplexity is 148.50408677823438
At time: 1287.304410457611 and batch: 550, loss is 5.018367643356323 and perplexity is 151.16434816104717
At time: 1289.0115802288055 and batch: 600, loss is 4.978351497650147 and perplexity is 145.2347643527169
At time: 1290.720771074295 and batch: 650, loss is 4.967470293045044 and perplexity is 143.66300200795794
At time: 1292.4366014003754 and batch: 700, loss is 4.964771203994751 and perplexity is 143.27576560016746
At time: 1294.143857717514 and batch: 750, loss is 4.974002695083618 and perplexity is 144.60453839412386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.968037095180778 and perplexity of 143.74445358560544
Finished 46 epochs...
Completing Train Step...
At time: 1298.6347765922546 and batch: 50, loss is 5.054065885543824 and perplexity is 156.65812535573357
At time: 1300.3522083759308 and batch: 100, loss is 5.043992300033569 and perplexity is 155.08793831892729
At time: 1302.0336389541626 and batch: 150, loss is 5.031660890579223 and perplexity is 153.18722874973867
At time: 1303.7275972366333 and batch: 200, loss is 5.029132900238037 and perplexity is 152.80046199235665
At time: 1305.444827079773 and batch: 250, loss is 5.036929922103882 and perplexity is 153.99650727190715
At time: 1307.1639087200165 and batch: 300, loss is 5.081069993972778 and perplexity is 160.94617526633556
At time: 1308.8822989463806 and batch: 350, loss is 5.034898433685303 and perplexity is 153.68398270338437
At time: 1310.5995037555695 and batch: 400, loss is 5.049336681365967 and perplexity is 155.919006195073
At time: 1312.3122675418854 and batch: 450, loss is 5.021452102661133 and perplexity is 151.63132826161313
At time: 1314.0277709960938 and batch: 500, loss is 4.997796907424926 and perplexity is 148.08655107967212
At time: 1315.746672153473 and batch: 550, loss is 5.015845670700073 and perplexity is 150.78359613329602
At time: 1317.46022772789 and batch: 600, loss is 4.975721607208252 and perplexity is 144.85331463944473
At time: 1319.1747629642487 and batch: 650, loss is 4.9647633075714115 and perplexity is 143.2746342385349
At time: 1320.8879652023315 and batch: 700, loss is 4.962021980285645 and perplexity is 142.88240942825252
At time: 1322.6060247421265 and batch: 750, loss is 4.971284923553466 and perplexity is 144.21206985802615
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9660765182140265 and perplexity of 143.46290760727268
Finished 47 epochs...
Completing Train Step...
At time: 1327.0800650119781 and batch: 50, loss is 5.0507806777954105 and perplexity is 156.14431531693506
At time: 1328.7877976894379 and batch: 100, loss is 5.040874748229981 and perplexity is 154.60519651438383
At time: 1330.4632380008698 and batch: 150, loss is 5.028496627807617 and perplexity is 152.70327019454515
At time: 1332.1534304618835 and batch: 200, loss is 5.026157741546631 and perplexity is 152.34653196158013
At time: 1333.8528203964233 and batch: 250, loss is 5.034187107086182 and perplexity is 153.57470207034697
At time: 1335.5587775707245 and batch: 300, loss is 5.078211765289307 and perplexity is 160.48681108835933
At time: 1337.2604644298553 and batch: 350, loss is 5.031966352462769 and perplexity is 153.23402875661293
At time: 1338.9590656757355 and batch: 400, loss is 5.04633731842041 and perplexity is 155.4520491422932
At time: 1340.6620666980743 and batch: 450, loss is 5.018536319732666 and perplexity is 151.18984816609097
At time: 1342.3617279529572 and batch: 500, loss is 4.995096740722656 and perplexity is 147.68723206196654
At time: 1344.0624737739563 and batch: 550, loss is 5.01324875831604 and perplexity is 150.39253234416324
At time: 1345.760217666626 and batch: 600, loss is 4.973189287185669 and perplexity is 144.48696374507148
At time: 1347.4597644805908 and batch: 650, loss is 4.962249488830566 and perplexity is 142.91492009540346
At time: 1349.148987531662 and batch: 700, loss is 4.959276895523072 and perplexity is 142.4907229555377
At time: 1350.8505799770355 and batch: 750, loss is 4.968693332672119 and perplexity is 143.83881504359758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.964681581009266 and perplexity of 143.2629253737051
Finished 48 epochs...
Completing Train Step...
At time: 1355.3454308509827 and batch: 50, loss is 5.047648248672485 and perplexity is 155.65596956978575
At time: 1357.0583019256592 and batch: 100, loss is 5.038140230178833 and perplexity is 154.18300332424596
At time: 1358.7417562007904 and batch: 150, loss is 5.025610637664795 and perplexity is 152.26320537878664
At time: 1360.4383187294006 and batch: 200, loss is 5.023361015319824 and perplexity is 151.92105566776317
At time: 1362.1393547058105 and batch: 250, loss is 5.031476078033447 and perplexity is 153.1589204439586
At time: 1363.8464770317078 and batch: 300, loss is 5.075529403686524 and perplexity is 160.0569042690824
At time: 1365.5638823509216 and batch: 350, loss is 5.029073286056518 and perplexity is 152.79135318938896
At time: 1367.2843816280365 and batch: 400, loss is 5.0433026313781735 and perplexity is 154.98101590379605
At time: 1369.0025615692139 and batch: 450, loss is 5.015674648284912 and perplexity is 150.75781096350246
At time: 1370.7248058319092 and batch: 500, loss is 4.992384614944458 and perplexity is 147.28722838810802
At time: 1372.475090265274 and batch: 550, loss is 5.010497007369995 and perplexity is 149.97925842523622
At time: 1374.1986336708069 and batch: 600, loss is 4.97060531616211 and perplexity is 144.11409556522548
At time: 1375.911949634552 and batch: 650, loss is 4.959379940032959 and perplexity is 142.5054065987696
At time: 1377.627517938614 and batch: 700, loss is 4.956417770385742 and perplexity is 142.08390599512848
At time: 1379.3438878059387 and batch: 750, loss is 4.966083660125732 and perplexity is 143.46393221035066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.96309998978016 and perplexity of 143.03652107419964
Finished 49 epochs...
Completing Train Step...
At time: 1383.8305668830872 and batch: 50, loss is 5.0443197441101075 and perplexity is 155.13872926081513
At time: 1385.5354869365692 and batch: 100, loss is 5.035064086914063 and perplexity is 153.70944306006479
At time: 1387.2127344608307 and batch: 150, loss is 5.022571830749512 and perplexity is 151.80120921143188
At time: 1388.890293121338 and batch: 200, loss is 5.0201803398132325 and perplexity is 151.43861174258143
At time: 1390.58651304245 and batch: 250, loss is 5.0283069896698 and perplexity is 152.67431457637844
At time: 1392.2904558181763 and batch: 300, loss is 5.072489652633667 and perplexity is 159.57110984693173
At time: 1393.996808052063 and batch: 350, loss is 5.025820293426514 and perplexity is 152.29513158373152
At time: 1395.698118686676 and batch: 400, loss is 5.039951763153076 and perplexity is 154.46256405912874
At time: 1397.4002814292908 and batch: 450, loss is 5.012537002563477 and perplexity is 150.28552767923495
At time: 1399.1082301139832 and batch: 500, loss is 4.989112997055054 and perplexity is 146.80614824114295
At time: 1400.813924074173 and batch: 550, loss is 5.007182836532593 and perplexity is 149.48302429729404
At time: 1402.5129702091217 and batch: 600, loss is 4.967409572601318 and perplexity is 143.65427899156447
At time: 1404.2204174995422 and batch: 650, loss is 4.956118803024292 and perplexity is 142.04143389385106
At time: 1405.931988477707 and batch: 700, loss is 4.9530822372436525 and perplexity is 141.61076993689298
At time: 1407.652857542038 and batch: 750, loss is 4.963046054840088 and perplexity is 143.02880661604877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.960988954056141 and perplexity of 142.73488436332843
Finished Training.
Improved accuracyfrom -166.06681505226368 to -142.73488436332843
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb41f33acf8>
SETTINGS FOR THIS RUN
{'dropout': 0.0, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 21.23703938645875, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 8.0, 'wordvec_source': 'glove', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.124983787536621 and batch: 50, loss is 7.605015859603882 and perplexity is 2008.2437434072037
At time: 3.805161714553833 and batch: 100, loss is 6.671826257705688 and perplexity is 789.8367335464202
At time: 5.450737237930298 and batch: 150, loss is 6.991581134796142 and perplexity is 1087.4395060513166
At time: 7.105569362640381 and batch: 200, loss is 6.531971654891968 and perplexity is 686.7509133208217
At time: 8.757425785064697 and batch: 250, loss is 6.406006317138672 and perplexity is 605.4707878398041
At time: 10.416786670684814 and batch: 300, loss is 6.182351427078247 and perplexity is 484.1290130465601
At time: 12.075112581253052 and batch: 350, loss is 6.0074278926849365 and perplexity is 406.4365761911964
At time: 13.735477209091187 and batch: 400, loss is 6.033519191741943 and perplexity is 417.18058755796136
At time: 15.40218472480774 and batch: 450, loss is 5.941526517868042 and perplexity is 380.51534989508747
At time: 17.062244653701782 and batch: 500, loss is 5.998925333023071 and perplexity is 402.9954747691912
At time: 18.722680807113647 and batch: 550, loss is 6.032122535705566 and perplexity is 416.59833646898556
At time: 20.382819414138794 and batch: 600, loss is 6.003094816207886 and perplexity is 404.6792654539378
At time: 22.04334831237793 and batch: 650, loss is 5.9977171421051025 and perplexity is 402.5088733095168
At time: 23.70458436012268 and batch: 700, loss is 5.940233631134033 and perplexity is 380.02370453648666
At time: 25.364774227142334 and batch: 750, loss is 5.9207004737854 and perplexity is 372.672669904887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.669731672420058 and perplexity of 289.9567205672862
Finished 1 epochs...
Completing Train Step...
At time: 29.760806798934937 and batch: 50, loss is 5.952613372802734 and perplexity is 384.7575412061249
At time: 31.42005157470703 and batch: 100, loss is 5.8698507595062255 and perplexity is 354.1961159198799
At time: 33.078802824020386 and batch: 150, loss is 5.950719127655029 and perplexity is 384.02940595171106
At time: 34.738712787628174 and batch: 200, loss is 5.989224052429199 and perplexity is 399.1048053423962
At time: 36.39855217933655 and batch: 250, loss is 6.061556739807129 and perplexity is 429.0428252945327
At time: 38.059144258499146 and batch: 300, loss is 6.091373023986816 and perplexity is 442.0279096049537
At time: 39.71761155128479 and batch: 350, loss is 6.027808351516724 and perplexity is 414.80492584617605
At time: 41.375847578048706 and batch: 400, loss is 6.081689624786377 and perplexity is 437.76823425209574
At time: 43.03484511375427 and batch: 450, loss is 6.045340042114258 and perplexity is 422.141278892707
At time: 44.69317960739136 and batch: 500, loss is 6.073716554641724 and perplexity is 434.2917549366076
At time: 46.35138654708862 and batch: 550, loss is 6.093720083236694 and perplexity is 443.0665937486945
At time: 48.009708404541016 and batch: 600, loss is 6.056112337112427 and perplexity is 426.7132905970916
At time: 49.66826844215393 and batch: 650, loss is 6.0627760887146 and perplexity is 429.5662972774503
At time: 51.36519813537598 and batch: 700, loss is 6.070675859451294 and perplexity is 432.97321174601586
At time: 53.02521109580994 and batch: 750, loss is 6.074423828125 and perplexity is 434.5990266286307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.796771027321039 and perplexity of 329.2347516983032
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 57.51094889640808 and batch: 50, loss is 6.0307636737823485 and perplexity is 416.03262130373037
At time: 59.20975923538208 and batch: 100, loss is 5.9414413547515865 and perplexity is 380.48294540188454
At time: 60.87207746505737 and batch: 150, loss is 5.8804052734375 and perplexity is 357.95428167864753
At time: 62.534687519073486 and batch: 200, loss is 5.837604513168335 and perplexity is 342.9568073585897
At time: 64.19732189178467 and batch: 250, loss is 5.852495975494385 and perplexity is 348.10215150030973
At time: 65.86232852935791 and batch: 300, loss is 5.829203205108643 and perplexity is 340.087591030161
At time: 67.5257682800293 and batch: 350, loss is 5.722983484268188 and perplexity is 305.81596035494067
At time: 69.188072681427 and batch: 400, loss is 5.728727149963379 and perplexity is 307.57751905483923
At time: 70.84946727752686 and batch: 450, loss is 5.698097887039185 and perplexity is 298.29946172364885
At time: 72.51217293739319 and batch: 500, loss is 5.663608694076538 and perplexity is 288.18674613407546
At time: 74.17251133918762 and batch: 550, loss is 5.621266555786133 and perplexity is 276.2390339087517
At time: 75.83361744880676 and batch: 600, loss is 5.557363777160645 and perplexity is 259.1387874764059
At time: 77.49433398246765 and batch: 650, loss is 5.488165187835693 and perplexity is 241.81311792239737
At time: 79.15479445457458 and batch: 700, loss is 5.465577249526977 and perplexity is 236.41228460760198
At time: 80.81675148010254 and batch: 750, loss is 5.458367080688476 and perplexity is 234.71384250524915
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.309890392214753 and perplexity of 202.3280504432297
Finished 3 epochs...
Completing Train Step...
At time: 85.15757393836975 and batch: 50, loss is 5.635102291107177 and perplexity is 280.0875663123711
At time: 86.83887195587158 and batch: 100, loss is 5.637661514282226 and perplexity is 280.80529092011005
At time: 88.5013518333435 and batch: 150, loss is 5.620381183624268 and perplexity is 275.9945677958233
At time: 90.16303706169128 and batch: 200, loss is 5.608886470794678 and perplexity is 272.84025319786366
At time: 91.82605028152466 and batch: 250, loss is 5.636519956588745 and perplexity is 280.4849183765521
At time: 93.49082064628601 and batch: 300, loss is 5.623365602493286 and perplexity is 276.8194815234714
At time: 95.15463185310364 and batch: 350, loss is 5.539394130706787 and perplexity is 254.52374461288164
At time: 96.81803154945374 and batch: 400, loss is 5.557369356155395 and perplexity is 259.14023321437384
At time: 98.48173546791077 and batch: 450, loss is 5.523402166366577 and perplexity is 250.48578348676222
At time: 100.1439962387085 and batch: 500, loss is 5.499630737304687 and perplexity is 244.60159334221467
At time: 101.80534934997559 and batch: 550, loss is 5.489050626754761 and perplexity is 242.027323487124
At time: 103.46674585342407 and batch: 600, loss is 5.449337606430054 and perplexity is 232.60403944703597
At time: 105.14594888687134 and batch: 650, loss is 5.4225237178802494 and perplexity is 226.44989761700225
At time: 106.8388261795044 and batch: 700, loss is 5.412981357574463 and perplexity is 224.29930827070035
At time: 108.53983807563782 and batch: 750, loss is 5.424216470718384 and perplexity is 226.83354594317595
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.253931089889171 and perplexity of 191.31687597597207
Finished 4 epochs...
Completing Train Step...
At time: 112.92862010002136 and batch: 50, loss is 5.553855867385864 and perplexity is 258.2313445353137
At time: 114.6210584640503 and batch: 100, loss is 5.544891738891602 and perplexity is 255.92686981683053
At time: 116.28462505340576 and batch: 150, loss is 5.526695432662964 and perplexity is 251.31205970226074
At time: 117.94760513305664 and batch: 200, loss is 5.523641710281372 and perplexity is 250.54579301911124
At time: 119.62755918502808 and batch: 250, loss is 5.5555171871185305 and perplexity is 258.66070591809955
At time: 121.3044023513794 and batch: 300, loss is 5.5450419998168945 and perplexity is 255.96532851444383
At time: 122.97673726081848 and batch: 350, loss is 5.4647611808776855 and perplexity is 236.2194346539295
At time: 124.66251373291016 and batch: 400, loss is 5.479877481460571 and perplexity is 239.817323506052
At time: 126.35202026367188 and batch: 450, loss is 5.444510431289673 and perplexity is 231.4839246803829
At time: 128.0361773967743 and batch: 500, loss is 5.42599048614502 and perplexity is 227.23630930153413
At time: 129.7466893196106 and batch: 550, loss is 5.431333131790161 and perplexity is 228.45360126412695
At time: 131.4421682357788 and batch: 600, loss is 5.403804588317871 and perplexity is 222.25038091990365
At time: 133.14028000831604 and batch: 650, loss is 5.3860885047912594 and perplexity is 218.34764726078876
At time: 134.8432960510254 and batch: 700, loss is 5.377619485855103 and perplexity is 216.50626525697174
At time: 136.5596981048584 and batch: 750, loss is 5.3902303409576415 and perplexity is 219.2538828872801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.221981669581214 and perplexity of 185.30102587735823
Finished 5 epochs...
Completing Train Step...
At time: 140.99470806121826 and batch: 50, loss is 5.496646690368652 and perplexity is 243.87277865648045
At time: 142.68988728523254 and batch: 100, loss is 5.4751588726043705 and perplexity is 238.68838496386363
At time: 144.36032009124756 and batch: 150, loss is 5.461886663436889 and perplexity is 235.54139275766977
At time: 146.03461122512817 and batch: 200, loss is 5.462358770370483 and perplexity is 235.65261973578689
At time: 147.71853685379028 and batch: 250, loss is 5.4929938220977785 and perplexity is 242.98356859356923
At time: 149.41168022155762 and batch: 300, loss is 5.487258367538452 and perplexity is 241.59393627313014
At time: 151.09978699684143 and batch: 350, loss is 5.407648324966431 and perplexity is 223.1062967579422
At time: 152.80372381210327 and batch: 400, loss is 5.425254259109497 and perplexity is 227.0690733565399
At time: 154.5119457244873 and batch: 450, loss is 5.3905307579040525 and perplexity is 219.31976036412559
At time: 156.2212085723877 and batch: 500, loss is 5.377076864242554 and perplexity is 216.3888161462857
At time: 157.92486453056335 and batch: 550, loss is 5.392741050720215 and perplexity is 219.80505738158428
At time: 159.63040232658386 and batch: 600, loss is 5.368575639724732 and perplexity is 214.55704342462948
At time: 161.33247017860413 and batch: 650, loss is 5.358848552703858 and perplexity is 212.48014584967686
At time: 163.04610061645508 and batch: 700, loss is 5.3521486759185795 and perplexity is 211.0613133620059
At time: 164.76193737983704 and batch: 750, loss is 5.3662537002563475 and perplexity is 214.05943289161848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.204626393872638 and perplexity of 182.11282159138614
Finished 6 epochs...
Completing Train Step...
At time: 169.3266339302063 and batch: 50, loss is 5.455839757919311 and perplexity is 234.1213938367008
At time: 171.04239916801453 and batch: 100, loss is 5.426184329986572 and perplexity is 227.28036193019682
At time: 172.72225379943848 and batch: 150, loss is 5.417703876495361 and perplexity is 225.3610711222292
At time: 174.4028811454773 and batch: 200, loss is 5.4157984256744385 and perplexity is 224.93206553857684
At time: 176.08582663536072 and batch: 250, loss is 5.4480464649200435 and perplexity is 232.30390851375745
At time: 177.75484538078308 and batch: 300, loss is 5.444776153564453 and perplexity is 231.54544328849428
At time: 179.4313793182373 and batch: 350, loss is 5.36668776512146 and perplexity is 214.15236873911653
At time: 181.10305285453796 and batch: 400, loss is 5.3877122116088865 and perplexity is 218.7024678085754
At time: 182.79081201553345 and batch: 450, loss is 5.352893085479736 and perplexity is 211.21848791552492
At time: 184.47946310043335 and batch: 500, loss is 5.343832473754883 and perplexity is 209.3133630391755
At time: 186.17291498184204 and batch: 550, loss is 5.3614582920074465 and perplexity is 213.03538784082266
At time: 187.8771092891693 and batch: 600, loss is 5.336091556549072 and perplexity is 207.69934069208352
At time: 189.58979630470276 and batch: 650, loss is 5.332678022384644 and perplexity is 206.99156059973197
At time: 191.29109621047974 and batch: 700, loss is 5.323305025100708 and perplexity is 205.0604933459913
At time: 192.9990692138672 and batch: 750, loss is 5.335773735046387 and perplexity is 207.6333398643147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.178018614303234 and perplexity of 177.33110136437955
Finished 7 epochs...
Completing Train Step...
At time: 197.5480296611786 and batch: 50, loss is 5.415284471511841 and perplexity is 224.8164904698795
At time: 199.27724814414978 and batch: 100, loss is 5.381618947982788 and perplexity is 217.37390776039464
At time: 200.96442532539368 and batch: 150, loss is 5.376292943954468 and perplexity is 216.21925103464292
At time: 202.66060638427734 and batch: 200, loss is 5.3758737754821775 and perplexity is 216.12863773395313
At time: 204.34687566757202 and batch: 250, loss is 5.40428840637207 and perplexity is 222.3579356831155
At time: 206.0257053375244 and batch: 300, loss is 5.405629425048828 and perplexity is 222.65632185378936
At time: 207.70798420906067 and batch: 350, loss is 5.331757125854492 and perplexity is 206.80103053250872
At time: 209.40622401237488 and batch: 400, loss is 5.352421312332154 and perplexity is 211.11886420639658
At time: 211.10873651504517 and batch: 450, loss is 5.320676908493042 and perplexity is 204.52227801396654
At time: 212.8205337524414 and batch: 500, loss is 5.315147495269775 and perplexity is 203.3945106447658
At time: 214.59359312057495 and batch: 550, loss is 5.3310325241088865 and perplexity is 206.651236421883
At time: 216.28765177726746 and batch: 600, loss is 5.306218452453614 and perplexity is 201.58647637090314
At time: 217.98793768882751 and batch: 650, loss is 5.304518728256226 and perplexity is 201.24412599218337
At time: 219.6751389503479 and batch: 700, loss is 5.294041433334351 and perplexity is 199.14663911454852
At time: 221.371808052063 and batch: 750, loss is 5.306797876358032 and perplexity is 201.70331424017826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.162194540334302 and perplexity of 174.54708622333774
Finished 8 epochs...
Completing Train Step...
At time: 225.944406747818 and batch: 50, loss is 5.381046266555786 and perplexity is 217.2494573993119
At time: 227.6595778465271 and batch: 100, loss is 5.346315956115722 and perplexity is 209.8338351082637
At time: 229.34251737594604 and batch: 150, loss is 5.339465970993042 and perplexity is 208.40138818074564
At time: 231.0307035446167 and batch: 200, loss is 5.342331132888794 and perplexity is 208.99934811412908
At time: 232.714035987854 and batch: 250, loss is 5.374451961517334 and perplexity is 215.8215613730804
At time: 234.39743280410767 and batch: 300, loss is 5.372520799636841 and perplexity is 215.40517718281097
At time: 236.0804741382599 and batch: 350, loss is 5.301335411071777 and perplexity is 200.60452068105263
At time: 237.78385734558105 and batch: 400, loss is 5.322769384384156 and perplexity is 204.9506840082006
At time: 239.48253798484802 and batch: 450, loss is 5.2925416851043705 and perplexity is 198.84819314787495
At time: 241.18563985824585 and batch: 500, loss is 5.281544094085693 and perplexity is 196.67332313060697
At time: 242.88802790641785 and batch: 550, loss is 5.2992713928222654 and perplexity is 200.19089630040992
At time: 244.58990383148193 and batch: 600, loss is 5.270189914703369 and perplexity is 194.4528884015109
At time: 246.28208136558533 and batch: 650, loss is 5.2673095607757565 and perplexity is 193.89360112000128
At time: 247.97184824943542 and batch: 700, loss is 5.257506103515625 and perplexity is 192.00206045672175
At time: 249.65912294387817 and batch: 750, loss is 5.269501895904541 and perplexity is 194.31914717231984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.145613204601199 and perplexity of 171.67672535863625
Finished 9 epochs...
Completing Train Step...
At time: 254.12217211723328 and batch: 50, loss is 5.33953631401062 and perplexity is 208.41604827886943
At time: 255.86638355255127 and batch: 100, loss is 5.305095233917236 and perplexity is 201.36017781911633
At time: 257.53831005096436 and batch: 150, loss is 5.299276428222656 and perplexity is 200.19190434426537
At time: 259.23062658309937 and batch: 200, loss is 5.305350437164306 and perplexity is 201.41157214804724
At time: 260.9618799686432 and batch: 250, loss is 5.338499565124511 and perplexity is 208.20008514208385
At time: 262.6891665458679 and batch: 300, loss is 5.3390360355377195 and perplexity is 208.31180829319302
At time: 264.38371539115906 and batch: 350, loss is 5.269374885559082 and perplexity is 194.29446819758397
At time: 266.12249517440796 and batch: 400, loss is 5.288437232971192 and perplexity is 198.03370291885338
At time: 267.8196704387665 and batch: 450, loss is 5.257006578445434 and perplexity is 191.9061745646927
At time: 269.5293958187103 and batch: 500, loss is 5.2425682067871096 and perplexity is 189.15526898056694
At time: 271.24646520614624 and batch: 550, loss is 5.2630441284179685 and perplexity is 193.06832241631315
At time: 272.9548535346985 and batch: 600, loss is 5.2333988571167 and perplexity is 187.4287657298704
At time: 274.6491069793701 and batch: 650, loss is 5.230509271621704 and perplexity is 186.88795602131725
At time: 276.34287095069885 and batch: 700, loss is 5.222820043563843 and perplexity is 185.45644257596337
At time: 278.03914642333984 and batch: 750, loss is 5.235635366439819 and perplexity is 187.84842101840331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.121671454851017 and perplexity of 167.6152970142328
Finished 10 epochs...
Completing Train Step...
At time: 282.4974253177643 and batch: 50, loss is 5.305926713943482 and perplexity is 201.52767441043918
At time: 284.20781540870667 and batch: 100, loss is 5.273591938018799 and perplexity is 195.11554821455044
At time: 285.889591217041 and batch: 150, loss is 5.267811069488525 and perplexity is 193.99086483757537
At time: 287.58484625816345 and batch: 200, loss is 5.274407377243042 and perplexity is 195.27471797363683
At time: 289.2606725692749 and batch: 250, loss is 5.3043718338012695 and perplexity is 201.21456651709724
At time: 290.9440588951111 and batch: 300, loss is 5.305903453826904 and perplexity is 201.5229869077549
At time: 292.6327295303345 and batch: 350, loss is 5.239196605682373 and perplexity is 188.51858678902948
At time: 294.3354091644287 and batch: 400, loss is 5.2589288997650145 and perplexity is 192.27543470002163
At time: 296.02840781211853 and batch: 450, loss is 5.227376441955567 and perplexity is 186.3033840487713
At time: 297.72203159332275 and batch: 500, loss is 5.2132398128509525 and perplexity is 183.68821062802886
At time: 299.4673881530762 and batch: 550, loss is 5.235605478286743 and perplexity is 187.84280665994265
At time: 301.1692407131195 and batch: 600, loss is 5.2044658184051515 and perplexity is 182.08358108764048
At time: 302.8665614128113 and batch: 650, loss is 5.200736379623413 and perplexity is 181.40577622030963
At time: 304.56377029418945 and batch: 700, loss is 5.193861036300659 and perplexity is 180.16282697567547
At time: 306.2582790851593 and batch: 750, loss is 5.207516345977783 and perplexity is 182.6398801436796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.113260313521984 and perplexity of 166.21137363807657
Finished 11 epochs...
Completing Train Step...
At time: 310.6877272129059 and batch: 50, loss is 5.278014650344849 and perplexity is 195.9803992385505
At time: 312.3897068500519 and batch: 100, loss is 5.246406726837158 and perplexity is 189.88274058706685
At time: 314.06434535980225 and batch: 150, loss is 5.237388458251953 and perplexity is 188.17802537614807
At time: 315.74025321006775 and batch: 200, loss is 5.244047088623047 and perplexity is 189.43521422397973
At time: 317.4169411659241 and batch: 250, loss is 5.27483455657959 and perplexity is 195.3581531177213
At time: 319.0999994277954 and batch: 300, loss is 5.281564493179321 and perplexity is 196.67733512906
At time: 320.79056763648987 and batch: 350, loss is 5.214395179748535 and perplexity is 183.90056055348023
At time: 322.49320006370544 and batch: 400, loss is 5.231709051132202 and perplexity is 187.1123149254053
At time: 324.1964821815491 and batch: 450, loss is 5.202894411087036 and perplexity is 181.79767830950175
At time: 325.89230704307556 and batch: 500, loss is 5.187633619308472 and perplexity is 179.04436410750597
At time: 327.585973739624 and batch: 550, loss is 5.209157028198242 and perplexity is 182.93978010076833
At time: 329.2822721004486 and batch: 600, loss is 5.178297243118286 and perplexity is 177.3805178031268
At time: 330.9789581298828 and batch: 650, loss is 5.173728637695312 and perplexity is 176.57198454858602
At time: 332.6673753261566 and batch: 700, loss is 5.166222009658814 and perplexity is 175.25148678246273
At time: 334.354288816452 and batch: 750, loss is 5.182269678115845 and perplexity is 178.08655178854107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.098209114961846 and perplexity of 163.72842579444693
Finished 12 epochs...
Completing Train Step...
At time: 338.81999254226685 and batch: 50, loss is 5.2487257289886475 and perplexity is 190.323590038867
At time: 340.5245020389557 and batch: 100, loss is 5.2183440399169925 and perplexity is 184.62819386758383
At time: 342.2097613811493 and batch: 150, loss is 5.208669605255127 and perplexity is 182.85063278272784
At time: 343.8908886909485 and batch: 200, loss is 5.216361179351806 and perplexity is 184.2624646176251
At time: 345.5688989162445 and batch: 250, loss is 5.244882335662842 and perplexity is 189.5935055228602
At time: 347.24715089797974 and batch: 300, loss is 5.252081003189087 and perplexity is 190.9632503880921
At time: 348.9258644580841 and batch: 350, loss is 5.185508460998535 and perplexity is 178.6642705118585
At time: 350.61920952796936 and batch: 400, loss is 5.204544296264649 and perplexity is 182.0978711780543
At time: 352.3201446533203 and batch: 450, loss is 5.174385566711425 and perplexity is 176.68801791733915
At time: 354.02362537384033 and batch: 500, loss is 5.158211393356323 and perplexity is 173.85322232265403
At time: 355.72962975502014 and batch: 550, loss is 5.18102991104126 and perplexity is 177.8659027501755
At time: 357.4349412918091 and batch: 600, loss is 5.151887292861939 and perplexity is 172.75722632063423
At time: 359.13324880599976 and batch: 650, loss is 5.1449395942687985 and perplexity is 171.5611210830905
At time: 360.8297119140625 and batch: 700, loss is 5.1374031448364255 and perplexity is 170.2730193231773
At time: 362.52859830856323 and batch: 750, loss is 5.15139554977417 and perplexity is 172.67229503262737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.076208779978198 and perplexity of 160.16568008083155
Finished 13 epochs...
Completing Train Step...
At time: 366.99244260787964 and batch: 50, loss is 5.222526588439941 and perplexity is 185.402027417223
At time: 368.698429107666 and batch: 100, loss is 5.196488208770752 and perplexity is 180.6367680847607
At time: 370.38130283355713 and batch: 150, loss is 5.182276039123535 and perplexity is 178.08768460206946
At time: 372.0572552680969 and batch: 200, loss is 5.189751043319702 and perplexity is 179.42387859789426
At time: 373.7358410358429 and batch: 250, loss is 5.216216583251953 and perplexity is 184.2358229100819
At time: 375.4159185886383 and batch: 300, loss is 5.224656419754028 and perplexity is 185.79732326822452
At time: 377.09507513046265 and batch: 350, loss is 5.158597574234009 and perplexity is 173.9203740781665
At time: 378.7844412326813 and batch: 400, loss is 5.178227806091309 and perplexity is 177.36820145493712
At time: 380.4908654689789 and batch: 450, loss is 5.1464824199676515 and perplexity is 171.82601427909265
At time: 382.19993782043457 and batch: 500, loss is 5.1325203227996825 and perplexity is 169.44363299293684
At time: 383.9200658798218 and batch: 550, loss is 5.155431938171387 and perplexity is 173.37067600104334
At time: 385.6107621192932 and batch: 600, loss is 5.126229133605957 and perplexity is 168.38097722924215
At time: 387.31284642219543 and batch: 650, loss is 5.118301801681518 and perplexity is 167.0514421286205
At time: 389.0151250362396 and batch: 700, loss is 5.113514347076416 and perplexity is 166.2536022676262
At time: 390.7169666290283 and batch: 750, loss is 5.12751540184021 and perplexity is 168.59769968322834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.06468803938045 and perplexity of 158.33104131879344
Finished 14 epochs...
Completing Train Step...
At time: 395.2356643676758 and batch: 50, loss is 5.196590242385864 and perplexity is 180.65520004755436
At time: 396.9475836753845 and batch: 100, loss is 5.1720660781860355 and perplexity is 176.27866701309517
At time: 398.6131703853607 and batch: 150, loss is 5.159001026153565 and perplexity is 173.99055674365172
At time: 400.29637575149536 and batch: 200, loss is 5.168431053161621 and perplexity is 175.63905285825157
At time: 401.9694492816925 and batch: 250, loss is 5.1952196311950685 and perplexity is 180.40776161836982
At time: 403.6388852596283 and batch: 300, loss is 5.204324703216553 and perplexity is 182.05788814162972
At time: 405.30695819854736 and batch: 350, loss is 5.137673492431641 and perplexity is 170.31905844748422
At time: 406.97419023513794 and batch: 400, loss is 5.158087186813354 and perplexity is 173.8316299559097
At time: 408.66312050819397 and batch: 450, loss is 5.125152635574341 and perplexity is 168.1998129676739
At time: 410.3544855117798 and batch: 500, loss is 5.112631664276123 and perplexity is 166.1069178198613
At time: 412.06548714637756 and batch: 550, loss is 5.137513046264648 and perplexity is 170.29173359953273
At time: 413.7650144100189 and batch: 600, loss is 5.107547731399536 and perplexity is 165.26458439689057
At time: 415.47668409347534 and batch: 650, loss is 5.098916254043579 and perplexity is 163.8442455086137
At time: 417.1724066734314 and batch: 700, loss is 5.09730746269226 and perplexity is 163.5808662214425
At time: 418.87159299850464 and batch: 750, loss is 5.109280395507812 and perplexity is 165.55118062644473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.055175426394441 and perplexity of 156.8320404104838
Finished 15 epochs...
Completing Train Step...
At time: 423.328941822052 and batch: 50, loss is 5.173245878219604 and perplexity is 176.48676332223303
At time: 425.03747296333313 and batch: 100, loss is 5.154401264190674 and perplexity is 173.19207940951048
At time: 426.7253737449646 and batch: 150, loss is 5.14375394821167 and perplexity is 171.3578308552651
At time: 428.40760684013367 and batch: 200, loss is 5.15285026550293 and perplexity is 172.92366692912879
At time: 430.09149646759033 and batch: 250, loss is 5.175346450805664 and perplexity is 176.85787621739874
At time: 431.77592253685 and batch: 300, loss is 5.187157020568848 and perplexity is 178.95905212064312
At time: 433.46064019203186 and batch: 350, loss is 5.127074012756347 and perplexity is 168.52329892007413
At time: 435.145307302475 and batch: 400, loss is 5.144759292602539 and perplexity is 171.53019111554005
At time: 436.8338770866394 and batch: 450, loss is 5.110292453765869 and perplexity is 165.7188128784229
At time: 438.51561760902405 and batch: 500, loss is 5.095284519195556 and perplexity is 163.2502858573696
At time: 440.1947021484375 and batch: 550, loss is 5.1247841739654545 and perplexity is 168.1378492103074
At time: 441.8758499622345 and batch: 600, loss is 5.0907976436614994 and perplexity is 162.51944297047612
At time: 443.5846371650696 and batch: 650, loss is 5.078014049530029 and perplexity is 160.45508345328105
At time: 445.30436635017395 and batch: 700, loss is 5.0761964225769045 and perplexity is 160.16370086147842
At time: 447.02497148513794 and batch: 750, loss is 5.088514575958252 and perplexity is 162.14882331494803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.047574065452398 and perplexity of 155.64442293702584
Finished 16 epochs...
Completing Train Step...
At time: 451.50422763824463 and batch: 50, loss is 5.153448209762574 and perplexity is 173.02709656262175
At time: 453.21336674690247 and batch: 100, loss is 5.136736612319947 and perplexity is 170.15956463393948
At time: 454.8905510902405 and batch: 150, loss is 5.124654684066773 and perplexity is 168.11607846682466
At time: 456.5753057003021 and batch: 200, loss is 5.132080764770508 and perplexity is 169.36916905037307
At time: 458.26238346099854 and batch: 250, loss is 5.154259443283081 and perplexity is 173.16751889325926
At time: 459.94177317619324 and batch: 300, loss is 5.1698665428161625 and perplexity is 175.89136195178352
At time: 461.6178545951843 and batch: 350, loss is 5.108712797164917 and perplexity is 165.45724071324835
At time: 463.29909014701843 and batch: 400, loss is 5.129236125946045 and perplexity is 168.888059552231
At time: 464.9849588871002 and batch: 450, loss is 5.0933290958404545 and perplexity is 162.93137434080887
At time: 466.6793706417084 and batch: 500, loss is 5.079742136001587 and perplexity is 160.7326034324874
At time: 468.42571210861206 and batch: 550, loss is 5.107549505233765 and perplexity is 165.2648775491272
At time: 470.12914848327637 and batch: 600, loss is 5.068782644271851 and perplexity is 158.98067346087845
At time: 471.827023267746 and batch: 650, loss is 5.059516344070435 and perplexity is 157.51431516476043
At time: 473.5266442298889 and batch: 700, loss is 5.058753499984741 and perplexity is 157.39420212061236
At time: 475.2185299396515 and batch: 750, loss is 5.0734077739715575 and perplexity is 159.71768276335803
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.047548160996548 and perplexity of 155.64039110516492
Finished 17 epochs...
Completing Train Step...
At time: 479.6657786369324 and batch: 50, loss is 5.1414214038848876 and perplexity is 170.9585969154959
At time: 481.3601772785187 and batch: 100, loss is 5.120934057235718 and perplexity is 167.4917434535758
At time: 483.034019947052 and batch: 150, loss is 5.110135021209717 and perplexity is 165.69272539567248
At time: 484.71740555763245 and batch: 200, loss is 5.118160438537598 and perplexity is 167.02782888062555
At time: 486.4010465145111 and batch: 250, loss is 5.142304677963256 and perplexity is 171.10966692091384
At time: 488.0792908668518 and batch: 300, loss is 5.156039056777954 and perplexity is 173.47596452235274
At time: 489.7599141597748 and batch: 350, loss is 5.095947189331055 and perplexity is 163.35850279852627
At time: 491.4493200778961 and batch: 400, loss is 5.1165039920806885 and perplexity is 166.7513852455201
At time: 493.13706851005554 and batch: 450, loss is 5.081234302520752 and perplexity is 160.97262227137048
At time: 494.8236210346222 and batch: 500, loss is 5.066812543869019 and perplexity is 158.6677738950357
At time: 496.51747274398804 and batch: 550, loss is 5.09174446105957 and perplexity is 162.6733920759503
At time: 498.2266128063202 and batch: 600, loss is 5.056830930709839 and perplexity is 157.0918915631806
At time: 499.93222737312317 and batch: 650, loss is 5.047448654174804 and perplexity is 155.62490459503064
At time: 501.63160490989685 and batch: 700, loss is 5.044045162200928 and perplexity is 155.09613682017144
At time: 503.3295476436615 and batch: 750, loss is 5.055036268234253 and perplexity is 156.81021747073706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.035009339798329 and perplexity of 153.7010281417445
Finished 18 epochs...
Completing Train Step...
At time: 507.8195834159851 and batch: 50, loss is 5.12302300453186 and perplexity is 167.84199057471324
At time: 509.5385437011719 and batch: 100, loss is 5.104213218688965 and perplexity is 164.71442530562032
At time: 511.227947473526 and batch: 150, loss is 5.099202737808228 and perplexity is 163.89119094911524
At time: 512.9114091396332 and batch: 200, loss is 5.10410701751709 and perplexity is 164.6969333694766
At time: 514.5887765884399 and batch: 250, loss is 5.124536724090576 and perplexity is 168.09624866779473
At time: 516.2653584480286 and batch: 300, loss is 5.141860227584839 and perplexity is 171.033634062368
At time: 517.9468991756439 and batch: 350, loss is 5.0821543884277345 and perplexity is 161.12079906977104
At time: 519.618426322937 and batch: 400, loss is 5.104618015289307 and perplexity is 164.78111464190894
At time: 521.302277803421 and batch: 450, loss is 5.0682351016998295 and perplexity is 158.893648601112
At time: 523.0007305145264 and batch: 500, loss is 5.052119951248169 and perplexity is 156.3535753501635
At time: 524.6999864578247 and batch: 550, loss is 5.0804353618621825 and perplexity is 160.84406605975173
At time: 526.3942716121674 and batch: 600, loss is 5.044720754623413 and perplexity is 155.20095399782264
At time: 528.0919964313507 and batch: 650, loss is 5.035273599624634 and perplexity is 153.74165051593852
At time: 529.7845430374146 and batch: 700, loss is 5.031285581588745 and perplexity is 153.12974699294716
At time: 531.4894871711731 and batch: 750, loss is 5.044155645370483 and perplexity is 155.11327327958085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.038025257199309 and perplexity of 154.16527746397887
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 535.9413509368896 and batch: 50, loss is 5.115125141143799 and perplexity is 166.52161838527525
At time: 537.6418449878693 and batch: 100, loss is 5.095286674499512 and perplexity is 163.25063771173566
At time: 539.3187086582184 and batch: 150, loss is 5.0764228630065915 and perplexity is 160.19997250525006
At time: 541.003945350647 and batch: 200, loss is 5.08221170425415 and perplexity is 161.1300341061768
At time: 542.6804087162018 and batch: 250, loss is 5.0918887424468995 and perplexity is 162.69686451191765
At time: 544.3540093898773 and batch: 300, loss is 5.10305344581604 and perplexity is 164.52350471703136
At time: 546.0318443775177 and batch: 350, loss is 5.039135589599609 and perplexity is 154.33654723213908
At time: 547.7118680477142 and batch: 400, loss is 5.050337076187134 and perplexity is 156.07506480849125
At time: 549.398365020752 and batch: 450, loss is 5.007471446990967 and perplexity is 149.52617288772333
At time: 551.0912871360779 and batch: 500, loss is 4.972966175079346 and perplexity is 144.45473055019588
At time: 552.8133721351624 and batch: 550, loss is 4.987402534484863 and perplexity is 146.55525645117962
At time: 554.5121750831604 and batch: 600, loss is 4.945156574249268 and perplexity is 140.49284669140832
At time: 556.2221190929413 and batch: 650, loss is 4.919594593048096 and perplexity is 136.94708263165123
At time: 557.9226384162903 and batch: 700, loss is 4.902962512969971 and perplexity is 134.688204777341
At time: 559.6123008728027 and batch: 750, loss is 4.932024745941162 and perplexity is 138.65997953750409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.975331860919331 and perplexity of 144.7968695979634
Finished 20 epochs...
Completing Train Step...
At time: 563.9999537467957 and batch: 50, loss is 5.070169878005982 and perplexity is 159.20136985752882
At time: 565.7107124328613 and batch: 100, loss is 5.054948768615723 and perplexity is 156.79649723679307
At time: 567.393944978714 and batch: 150, loss is 5.041279239654541 and perplexity is 154.667745640006
At time: 569.0682926177979 and batch: 200, loss is 5.048179292678833 and perplexity is 155.73865169145765
At time: 570.7419455051422 and batch: 250, loss is 5.059432888031006 and perplexity is 157.5011701923848
At time: 572.4200587272644 and batch: 300, loss is 5.074663467407227 and perplexity is 159.91836518059216
At time: 574.1024589538574 and batch: 350, loss is 5.014750137329101 and perplexity is 150.61849812363886
At time: 575.7875027656555 and batch: 400, loss is 5.027786226272583 and perplexity is 152.59482808027704
At time: 577.4798882007599 and batch: 450, loss is 4.987399053573609 and perplexity is 146.55474630622595
At time: 579.1800858974457 and batch: 500, loss is 4.956842756271362 and perplexity is 142.1443024826685
At time: 580.883508682251 and batch: 550, loss is 4.976034669876099 and perplexity is 144.89866990372187
At time: 582.5806624889374 and batch: 600, loss is 4.939711036682129 and perplexity is 139.72986691978232
At time: 584.2789342403412 and batch: 650, loss is 4.92185564994812 and perplexity is 137.25707810440719
At time: 585.9793810844421 and batch: 700, loss is 4.910453977584839 and perplexity is 135.70100564168922
At time: 587.6793520450592 and batch: 750, loss is 4.939708042144775 and perplexity is 139.72944849410285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.968475341796875 and perplexity of 143.80746291177562
Finished 21 epochs...
Completing Train Step...
At time: 592.1134383678436 and batch: 50, loss is 5.0590189170837405 and perplexity is 157.43598277754367
At time: 593.8235673904419 and batch: 100, loss is 5.041696481704712 and perplexity is 154.73229299229047
At time: 595.4974675178528 and batch: 150, loss is 5.027410545349121 and perplexity is 152.53751188132378
At time: 597.1703288555145 and batch: 200, loss is 5.035208492279053 and perplexity is 153.73164113101402
At time: 598.8497939109802 and batch: 250, loss is 5.046358804702759 and perplexity is 155.45538926479605
At time: 600.5303342342377 and batch: 300, loss is 5.062603197097778 and perplexity is 158.00128992845373
At time: 602.2052795886993 and batch: 350, loss is 5.004568386077881 and perplexity is 149.09271877599497
At time: 603.8858284950256 and batch: 400, loss is 5.0183545684814455 and perplexity is 151.16237171902986
At time: 605.5759725570679 and batch: 450, loss is 4.979591798782349 and perplexity is 145.41501095230262
At time: 607.2755670547485 and batch: 500, loss is 4.951077098846436 and perplexity is 141.32710523316914
At time: 608.9706566333771 and batch: 550, loss is 4.973116426467896 and perplexity is 144.47643670469267
At time: 610.6687524318695 and batch: 600, loss is 4.939641265869141 and perplexity is 139.7201181934608
At time: 612.3720846176147 and batch: 650, loss is 4.924758081436157 and perplexity is 137.65603606385326
At time: 614.0743505954742 and batch: 700, loss is 4.914866018295288 and perplexity is 136.30104673251802
At time: 615.7730810642242 and batch: 750, loss is 4.942853384017944 and perplexity is 140.16963728858448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.965547783430233 and perplexity of 143.38707382758173
Finished 22 epochs...
Completing Train Step...
At time: 620.2195649147034 and batch: 50, loss is 5.051717777252197 and perplexity is 156.29070665090575
At time: 621.928474187851 and batch: 100, loss is 5.0330143070220945 and perplexity is 153.39469522656833
At time: 623.616534948349 and batch: 150, loss is 5.018834791183472 and perplexity is 151.23498075447966
At time: 625.3066756725311 and batch: 200, loss is 5.025930995941162 and perplexity is 152.31199197099298
At time: 626.9884598255157 and batch: 250, loss is 5.03818208694458 and perplexity is 154.1894570611637
At time: 628.6698784828186 and batch: 300, loss is 5.055112619400024 and perplexity is 156.82219057072
At time: 630.3409667015076 and batch: 350, loss is 4.998573789596557 and perplexity is 148.20164158115506
At time: 632.0234432220459 and batch: 400, loss is 5.013176603317261 and perplexity is 150.38168116266374
At time: 633.700441122055 and batch: 450, loss is 4.975379600524902 and perplexity is 144.80378230840964
At time: 635.3952975273132 and batch: 500, loss is 4.948154554367066 and perplexity is 140.91467345065658
At time: 637.124080657959 and batch: 550, loss is 4.9718578433990475 and perplexity is 144.2947154872201
At time: 638.8135964870453 and batch: 600, loss is 4.939919776916504 and perplexity is 139.75903720935295
At time: 640.5158548355103 and batch: 650, loss is 4.926325674057007 and perplexity is 137.87199387291275
At time: 642.2237586975098 and batch: 700, loss is 4.916890268325806 and perplexity is 136.57723357182653
At time: 643.9244301319122 and batch: 750, loss is 4.943891515731812 and perplexity is 140.31522739215154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.963522622751635 and perplexity of 143.09698580042422
Finished 23 epochs...
Completing Train Step...
At time: 648.3706467151642 and batch: 50, loss is 5.046054592132569 and perplexity is 155.4081049738803
At time: 650.0729486942291 and batch: 100, loss is 5.026686153411865 and perplexity is 152.42705494947342
At time: 651.7537879943848 and batch: 150, loss is 5.012699785232544 and perplexity is 150.30999354980867
At time: 653.4339692592621 and batch: 200, loss is 5.019734630584717 and perplexity is 151.37112919568824
At time: 655.1220772266388 and batch: 250, loss is 5.032402095794677 and perplexity is 153.3008140124233
At time: 656.808025598526 and batch: 300, loss is 5.049910888671875 and perplexity is 156.00856173682277
At time: 658.4848854541779 and batch: 350, loss is 4.99456280708313 and perplexity is 147.60839792861097
At time: 660.1667430400848 and batch: 400, loss is 5.009908866882324 and perplexity is 149.891075485566
At time: 661.8497431278229 and batch: 450, loss is 4.972548608779907 and perplexity is 144.39442371485086
At time: 663.5350520610809 and batch: 500, loss is 4.946222200393676 and perplexity is 140.64263933925113
At time: 665.2377395629883 and batch: 550, loss is 4.971007328033448 and perplexity is 144.17204278942543
At time: 666.9355726242065 and batch: 600, loss is 4.939794998168946 and perplexity is 139.7415993396904
At time: 668.6391758918762 and batch: 650, loss is 4.9269826793670655 and perplexity is 137.96260626814856
At time: 670.3435862064362 and batch: 700, loss is 4.917555618286133 and perplexity is 136.66813546619758
At time: 672.0498294830322 and batch: 750, loss is 4.943646621704102 and perplexity is 140.28086923819149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.961366520371548 and perplexity of 142.78878642285812
Finished 24 epochs...
Completing Train Step...
At time: 676.4962470531464 and batch: 50, loss is 5.041094522476197 and perplexity is 154.6391784889535
At time: 678.1960122585297 and batch: 100, loss is 5.021453199386596 and perplexity is 151.63149455964307
At time: 679.8766658306122 and batch: 150, loss is 5.007511940002441 and perplexity is 149.5322277753473
At time: 681.561176776886 and batch: 200, loss is 5.0144819736480715 and perplexity is 150.57811312788743
At time: 683.24276304245 and batch: 250, loss is 5.02763072013855 and perplexity is 152.57110049342936
At time: 684.9257071018219 and batch: 300, loss is 5.045640125274658 and perplexity is 155.34370681129187
At time: 686.6021337509155 and batch: 350, loss is 4.991280879974365 and perplexity is 147.12475200541513
At time: 688.2833416461945 and batch: 400, loss is 5.007084627151489 and perplexity is 149.4683443828567
At time: 689.9627225399017 and batch: 450, loss is 4.970043821334839 and perplexity is 144.03319895966698
At time: 691.6389093399048 and batch: 500, loss is 4.944285898208618 and perplexity is 140.3705761726413
At time: 693.3171274662018 and batch: 550, loss is 4.9698804759979245 and perplexity is 144.00967372967668
At time: 695.0050117969513 and batch: 600, loss is 4.939301176071167 and perplexity is 139.6726088858648
At time: 696.7063360214233 and batch: 650, loss is 4.926911211013794 and perplexity is 137.95274666019466
At time: 698.4004776477814 and batch: 700, loss is 4.917547245025634 and perplexity is 136.6669911130885
At time: 700.0971946716309 and batch: 750, loss is 4.9428315925598145 and perplexity is 140.16658282108324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.959526239439499 and perplexity of 142.52625658031644
Finished 25 epochs...
Completing Train Step...
At time: 704.5427911281586 and batch: 50, loss is 5.0370006275177 and perplexity is 154.00739604362337
At time: 706.2433602809906 and batch: 100, loss is 5.017181272506714 and perplexity is 150.98511752291282
At time: 707.9154717922211 and batch: 150, loss is 5.003125085830688 and perplexity is 148.87768843213297
At time: 709.5900435447693 and batch: 200, loss is 5.01052077293396 and perplexity is 149.98282280925034
At time: 711.2642676830292 and batch: 250, loss is 5.0236775588989255 and perplexity is 151.96915291450082
At time: 712.9314222335815 and batch: 300, loss is 5.042111749649048 and perplexity is 154.79656169696668
At time: 714.5987598896027 and batch: 350, loss is 4.98870285987854 and perplexity is 146.74594992763542
At time: 716.2767405509949 and batch: 400, loss is 5.004824275970459 and perplexity is 149.13087497747173
At time: 717.9687657356262 and batch: 450, loss is 4.968075504302979 and perplexity is 143.74997478992054
At time: 719.6608736515045 and batch: 500, loss is 4.942755537033081 and perplexity is 140.1559227831789
At time: 721.3868691921234 and batch: 550, loss is 4.968958139419556 and perplexity is 143.8769095759864
At time: 723.1010286808014 and batch: 600, loss is 4.938695011138916 and perplexity is 139.58796990354526
At time: 724.8083438873291 and batch: 650, loss is 4.926433744430542 and perplexity is 137.88689455593735
At time: 726.5030174255371 and batch: 700, loss is 4.917015886306762 and perplexity is 136.59439120578358
At time: 728.2030472755432 and batch: 750, loss is 4.941731052398682 and perplexity is 140.01240872039585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.958369410315226 and perplexity of 142.36147338710606
Finished 26 epochs...
Completing Train Step...
At time: 732.6436269283295 and batch: 50, loss is 5.033320560455322 and perplexity is 153.44168007288835
At time: 734.3503589630127 and batch: 100, loss is 5.013522634506225 and perplexity is 150.43372691880307
At time: 736.0335676670074 and batch: 150, loss is 4.999364881515503 and perplexity is 148.31892908866732
At time: 737.7096793651581 and batch: 200, loss is 5.007076349258423 and perplexity is 149.46710710500616
At time: 739.3839313983917 and batch: 250, loss is 5.020212230682373 and perplexity is 151.44344132854073
At time: 741.0652441978455 and batch: 300, loss is 5.038946542739868 and perplexity is 154.3073731502628
At time: 742.7494783401489 and batch: 350, loss is 4.98623122215271 and perplexity is 146.38369496761197
At time: 744.4344606399536 and batch: 400, loss is 5.002931671142578 and perplexity is 148.84889608497951
At time: 746.1156840324402 and batch: 450, loss is 4.966253004074097 and perplexity is 143.48822901627952
At time: 747.804247379303 and batch: 500, loss is 4.941280698776245 and perplexity is 139.94936782135662
At time: 749.4835894107819 and batch: 550, loss is 4.9678033924102785 and perplexity is 143.71086403369713
At time: 751.1593713760376 and batch: 600, loss is 4.937814111709595 and perplexity is 139.4650610836198
At time: 752.8419976234436 and batch: 650, loss is 4.925538635253906 and perplexity is 137.76352595370352
At time: 754.5271668434143 and batch: 700, loss is 4.916139373779297 and perplexity is 136.47471696635708
At time: 756.212583065033 and batch: 750, loss is 4.940413331985473 and perplexity is 139.8280330158163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.957620665084484 and perplexity of 142.2549208082117
Finished 27 epochs...
Completing Train Step...
At time: 760.6724870204926 and batch: 50, loss is 5.0300446891784665 and perplexity is 152.93984729902775
At time: 762.3882758617401 and batch: 100, loss is 5.01035629272461 and perplexity is 149.95815563184257
At time: 764.064293384552 and batch: 150, loss is 4.995979709625244 and perplexity is 147.81769288313464
At time: 765.7437539100647 and batch: 200, loss is 5.00395676612854 and perplexity is 149.0015585754305
At time: 767.4274706840515 and batch: 250, loss is 5.016965780258179 and perplexity is 150.95258490583177
At time: 769.1064789295197 and batch: 300, loss is 5.035911912918091 and perplexity is 153.8398171824319
At time: 770.7825977802277 and batch: 350, loss is 4.983948802947998 and perplexity is 146.0499670093896
At time: 772.4609200954437 and batch: 400, loss is 5.001034870147705 and perplexity is 148.56682694987805
At time: 774.1453778743744 and batch: 450, loss is 4.964461574554443 and perplexity is 143.23141007229805
At time: 775.8499150276184 and batch: 500, loss is 4.939737091064453 and perplexity is 139.73350754258396
At time: 777.5488059520721 and batch: 550, loss is 4.96663462638855 and perplexity is 143.54299777615927
At time: 779.2437388896942 and batch: 600, loss is 4.936849679946899 and perplexity is 139.33062138830115
At time: 780.9395527839661 and batch: 650, loss is 4.92447151184082 and perplexity is 137.6165936810665
At time: 782.6344563961029 and batch: 700, loss is 4.915143985748291 and perplexity is 136.33893925351964
At time: 784.3313503265381 and batch: 750, loss is 4.939100313186645 and perplexity is 139.64455666017514
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.956733171329942 and perplexity of 142.12872646107206
Finished 28 epochs...
Completing Train Step...
At time: 788.7188770771027 and batch: 50, loss is 5.0271171188354495 and perplexity is 152.49275989703875
At time: 790.4135279655457 and batch: 100, loss is 5.007517013549805 and perplexity is 149.53298643611186
At time: 792.0941910743713 and batch: 150, loss is 4.992799568176269 and perplexity is 147.3483583816992
At time: 793.7809987068176 and batch: 200, loss is 5.000999765396118 and perplexity is 148.5616116398656
At time: 795.457998752594 and batch: 250, loss is 5.013821992874146 and perplexity is 150.47876725503806
At time: 797.1360332965851 and batch: 300, loss is 5.033159093856812 and perplexity is 153.4169063668541
At time: 798.8171048164368 and batch: 350, loss is 4.981806888580322 and perplexity is 145.73747527139602
At time: 800.4990396499634 and batch: 400, loss is 4.999234018325805 and perplexity is 148.29952087045257
At time: 802.1833665370941 and batch: 450, loss is 4.962773180007934 and perplexity is 142.98978297897656
At time: 803.8717572689056 and batch: 500, loss is 4.938181161880493 and perplexity is 139.51626115404736
At time: 805.6061408519745 and batch: 550, loss is 4.965423917770385 and perplexity is 143.36931419299566
At time: 807.3180251121521 and batch: 600, loss is 4.935789728164673 and perplexity is 139.1830158890175
At time: 809.013857126236 and batch: 650, loss is 4.923280458450318 and perplexity is 137.45278254384382
At time: 810.7083508968353 and batch: 700, loss is 4.914007034301758 and perplexity is 136.18401658578068
At time: 812.412918806076 and batch: 750, loss is 4.937676906585693 and perplexity is 139.4459270753059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.955877969431323 and perplexity of 142.00722966390498
Finished 29 epochs...
Completing Train Step...
At time: 816.8592932224274 and batch: 50, loss is 5.024292192459106 and perplexity is 152.06258696690506
At time: 818.5720336437225 and batch: 100, loss is 5.004723739624024 and perplexity is 149.11588265780992
At time: 820.2583646774292 and batch: 150, loss is 4.989851322174072 and perplexity is 146.91457893164122
At time: 821.9345252513885 and batch: 200, loss is 4.998181467056274 and perplexity is 148.14351014053867
At time: 823.6073091030121 and batch: 250, loss is 5.0108849048614506 and perplexity is 150.03744628808312
At time: 825.2823100090027 and batch: 300, loss is 5.030488901138305 and perplexity is 153.00780009993997
At time: 826.9606382846832 and batch: 350, loss is 4.979697008132934 and perplexity is 145.4303107759984
At time: 828.6412971019745 and batch: 400, loss is 4.9975074100494385 and perplexity is 148.04368661666365
At time: 830.3209357261658 and batch: 450, loss is 4.9611196708679195 and perplexity is 142.75354343184236
At time: 831.9969816207886 and batch: 500, loss is 4.936672096252441 and perplexity is 139.3058807386369
At time: 833.6814239025116 and batch: 550, loss is 4.9643003940582275 and perplexity is 143.2083258229643
At time: 835.3557198047638 and batch: 600, loss is 4.934791240692139 and perplexity is 139.04411274932548
At time: 837.0423936843872 and batch: 650, loss is 4.922086372375488 and perplexity is 137.28875004421377
At time: 838.7173869609833 and batch: 700, loss is 4.912883911132813 and perplexity is 136.03115102101074
At time: 840.4081678390503 and batch: 750, loss is 4.936228380203247 and perplexity is 139.2440821951228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.955125675644985 and perplexity of 141.9004386815465
Finished 30 epochs...
Completing Train Step...
At time: 844.8547625541687 and batch: 50, loss is 5.021487884521484 and perplexity is 151.63675400969697
At time: 846.5599541664124 and batch: 100, loss is 5.001910047531128 and perplexity is 148.6969061896532
At time: 848.2330129146576 and batch: 150, loss is 4.987019453048706 and perplexity is 146.4991246052825
At time: 849.9110331535339 and batch: 200, loss is 4.9955065822601314 and perplexity is 147.7477728294335
At time: 851.5933628082275 and batch: 250, loss is 5.008136186599732 and perplexity is 149.62560190092918
At time: 853.2727723121643 and batch: 300, loss is 5.027919054031372 and perplexity is 152.61509825548495
At time: 854.9465148448944 and batch: 350, loss is 4.977694387435913 and perplexity is 145.13936045446982
At time: 856.6312258243561 and batch: 400, loss is 4.995746736526489 and perplexity is 147.78325934837224
At time: 858.3275470733643 and batch: 450, loss is 4.959540882110596 and perplexity is 142.52834356069368
At time: 860.0379405021667 and batch: 500, loss is 4.935168619155884 and perplexity is 139.09659490518243
At time: 861.736578464508 and batch: 550, loss is 4.963091697692871 and perplexity is 143.0353350077989
At time: 863.4363722801208 and batch: 600, loss is 4.933748826980591 and perplexity is 138.899246777941
At time: 865.1422765254974 and batch: 650, loss is 4.92080961227417 and perplexity is 137.11357709650878
At time: 866.8462138175964 and batch: 700, loss is 4.911707572937011 and perplexity is 135.87122646337033
At time: 868.5468738079071 and batch: 750, loss is 4.934780569076538 and perplexity is 139.04262893192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.954379059547602 and perplexity of 141.79453307013787
Finished 31 epochs...
Completing Train Step...
At time: 872.9283838272095 and batch: 50, loss is 5.018750982284546 and perplexity is 151.22230644838083
At time: 874.6396753787994 and batch: 100, loss is 4.999211568832397 and perplexity is 148.29619165870602
At time: 876.3208804130554 and batch: 150, loss is 4.984404220581054 and perplexity is 146.1164958877353
At time: 877.994677066803 and batch: 200, loss is 4.992955923080444 and perplexity is 147.37139882135025
At time: 879.6737825870514 and batch: 250, loss is 5.005376462936401 and perplexity is 149.21324584282
At time: 881.3586256504059 and batch: 300, loss is 5.025418300628662 and perplexity is 152.23392234134852
At time: 883.0328242778778 and batch: 350, loss is 4.975708665847779 and perplexity is 144.85144005261415
At time: 884.7059133052826 and batch: 400, loss is 4.994020357131958 and perplexity is 147.52834947346508
At time: 886.381995677948 and batch: 450, loss is 4.95796199798584 and perplexity is 142.30348538093506
At time: 888.0652730464935 and batch: 500, loss is 4.93363166809082 and perplexity is 138.882974449641
At time: 889.7805187702179 and batch: 550, loss is 4.961718683242798 and perplexity is 142.83908018714558
At time: 891.4575638771057 and batch: 600, loss is 4.932476387023926 and perplexity is 138.72261822483594
At time: 893.141401052475 and batch: 650, loss is 4.919452629089355 and perplexity is 136.92764246159726
At time: 894.833000421524 and batch: 700, loss is 4.9104946327209475 and perplexity is 135.70652269669117
At time: 896.5290949344635 and batch: 750, loss is 4.9332248401641845 and perplexity is 138.82648446873074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.953461048214934 and perplexity of 141.66442381189336
Finished 32 epochs...
Completing Train Step...
At time: 900.9589421749115 and batch: 50, loss is 5.01617862701416 and perplexity is 150.83380884254473
At time: 902.6723518371582 and batch: 100, loss is 4.9966863250732425 and perplexity is 147.9221800601925
At time: 904.3501324653625 and batch: 150, loss is 4.981999206542969 and perplexity is 145.7655059010319
At time: 906.0321600437164 and batch: 200, loss is 4.990559349060058 and perplexity is 147.01863523645193
At time: 907.7093989849091 and batch: 250, loss is 5.0027187633514405 and perplexity is 148.8172083687013
At time: 909.3826060295105 and batch: 300, loss is 5.022960596084594 and perplexity is 151.86023573238606
At time: 911.0536241531372 and batch: 350, loss is 4.973605651855468 and perplexity is 144.54713553785658
At time: 912.7356791496277 and batch: 400, loss is 4.992106504440308 and perplexity is 147.24627195823675
At time: 914.4247968196869 and batch: 450, loss is 4.95633282661438 and perplexity is 142.07183736488818
At time: 916.108559846878 and batch: 500, loss is 4.931927967071533 and perplexity is 138.64656083075573
At time: 917.8030440807343 and batch: 550, loss is 4.960373163223267 and perplexity is 142.64701658686832
At time: 919.5003147125244 and batch: 600, loss is 4.931093683242798 and perplexity is 138.53093848477803
At time: 921.210455417633 and batch: 650, loss is 4.918130655288696 and perplexity is 136.7467473013528
At time: 922.9067738056183 and batch: 700, loss is 4.909184370040894 and perplexity is 135.52882794338566
At time: 924.6097364425659 and batch: 750, loss is 4.931744470596313 and perplexity is 138.62112200958032
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.952233602834302 and perplexity of 141.49064514301597
Finished 33 epochs...
Completing Train Step...
At time: 929.0591492652893 and batch: 50, loss is 5.01378399848938 and perplexity is 150.47305001546812
At time: 930.7699162960052 and batch: 100, loss is 4.99442681312561 and perplexity is 147.58832544331275
At time: 932.4553384780884 and batch: 150, loss is 4.979717226028442 and perplexity is 145.43325110054886
At time: 934.1364710330963 and batch: 200, loss is 4.988300390243531 and perplexity is 146.68690102222172
At time: 935.8128614425659 and batch: 250, loss is 5.000007858276367 and perplexity is 148.41432537877984
At time: 937.4915478229523 and batch: 300, loss is 5.020575647354126 and perplexity is 151.49848840185342
At time: 939.1808984279633 and batch: 350, loss is 4.9715958595275875 and perplexity is 144.25691755046662
At time: 940.8639965057373 and batch: 400, loss is 4.990077209472656 and perplexity is 146.94776881744443
At time: 942.5387344360352 and batch: 450, loss is 4.954588098526001 and perplexity is 141.82417675270804
At time: 944.2172985076904 and batch: 500, loss is 4.9301713275909425 and perplexity is 138.40322259949846
At time: 945.9045028686523 and batch: 550, loss is 4.959076328277588 and perplexity is 142.46214684953875
At time: 947.5858392715454 and batch: 600, loss is 4.92982292175293 and perplexity is 138.35501050791981
At time: 949.2620768547058 and batch: 650, loss is 4.916732234954834 and perplexity is 136.55565151659457
At time: 950.9367744922638 and batch: 700, loss is 4.907848873138428 and perplexity is 135.34795042104156
At time: 952.6217205524445 and batch: 750, loss is 4.930291738510132 and perplexity is 138.4198888621303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.951056901798692 and perplexity of 141.32425087170222
Finished 34 epochs...
Completing Train Step...
At time: 957.0603911876678 and batch: 50, loss is 5.011522092819214 and perplexity is 150.13307880678045
At time: 958.7644143104553 and batch: 100, loss is 4.99221116065979 and perplexity is 147.26168300280943
At time: 960.4460346698761 and batch: 150, loss is 4.977452135086059 and perplexity is 145.10420436183838
At time: 962.122406244278 and batch: 200, loss is 4.986081848144531 and perplexity is 146.36183068138138
At time: 963.8005666732788 and batch: 250, loss is 4.997197208404541 and perplexity is 147.9977703435783
At time: 965.4876563549042 and batch: 300, loss is 5.018214864730835 and perplexity is 151.14125524380927
At time: 967.1732132434845 and batch: 350, loss is 4.969585151672363 and perplexity is 143.96715044929675
At time: 968.8471441268921 and batch: 400, loss is 4.988272514343262 and perplexity is 146.68281204979021
At time: 970.5246512889862 and batch: 450, loss is 4.952952785491943 and perplexity is 141.59243936115192
At time: 972.2066164016724 and batch: 500, loss is 4.928563385009766 and perplexity is 138.1808569879865
At time: 973.947783946991 and batch: 550, loss is 4.9577466487884525 and perplexity is 142.2728437390178
At time: 975.6436634063721 and batch: 600, loss is 4.928464879989624 and perplexity is 138.16724615026462
At time: 977.3368813991547 and batch: 650, loss is 4.915270528793335 and perplexity is 136.35619308970436
At time: 979.0319397449493 and batch: 700, loss is 4.906427211761475 and perplexity is 135.1556681799815
At time: 980.7334108352661 and batch: 750, loss is 4.928907127380371 and perplexity is 138.22836376791855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.950028175531432 and perplexity of 141.17894165714733
Finished 35 epochs...
Completing Train Step...
At time: 985.1534171104431 and batch: 50, loss is 5.009226150512696 and perplexity is 149.78877731896065
At time: 986.8621680736542 and batch: 100, loss is 4.990022087097168 and perplexity is 146.93966893059908
At time: 988.5435242652893 and batch: 150, loss is 4.975175838470459 and perplexity is 144.77427979808354
At time: 990.2193303108215 and batch: 200, loss is 4.983808546066284 and perplexity is 146.02948393292235
At time: 991.8989925384521 and batch: 250, loss is 4.994299440383911 and perplexity is 147.56952791083035
At time: 993.5828723907471 and batch: 300, loss is 5.01578579902649 and perplexity is 150.77456873729196
At time: 995.2582378387451 and batch: 350, loss is 4.967635707855225 and perplexity is 143.68676796173125
At time: 996.9382011890411 and batch: 400, loss is 4.986568984985351 and perplexity is 146.43314629001603
At time: 998.6080255508423 and batch: 450, loss is 4.951447248458862 and perplexity is 141.37942708925655
At time: 1000.2914271354675 and batch: 500, loss is 4.92705888748169 and perplexity is 137.9731205388922
At time: 1001.9665541648865 and batch: 550, loss is 4.956349201202393 and perplexity is 142.07416375174
At time: 1003.648273229599 and batch: 600, loss is 4.926935033798218 and perplexity is 137.9560331178852
At time: 1005.3385665416718 and batch: 650, loss is 4.9138827419281 and perplexity is 136.16709100298726
At time: 1007.0385856628418 and batch: 700, loss is 4.904858922958374 and perplexity is 134.9438711817397
At time: 1008.733546257019 and batch: 750, loss is 4.927508153915405 and perplexity is 138.0351211570608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9490001589752906 and perplexity of 141.03388194241464
Finished 36 epochs...
Completing Train Step...
At time: 1013.2819392681122 and batch: 50, loss is 5.00734435081482 and perplexity is 149.50716989054513
At time: 1015.1410953998566 and batch: 100, loss is 4.9877602481842045 and perplexity is 146.60769065177954
At time: 1016.8805196285248 and batch: 150, loss is 4.972939224243164 and perplexity is 144.45083742687873
At time: 1018.5985662937164 and batch: 200, loss is 4.981385917663574 and perplexity is 145.67613694455818
At time: 1020.2944767475128 and batch: 250, loss is 4.9914211082458495 and perplexity is 147.14538450167726
At time: 1021.9784886837006 and batch: 300, loss is 5.013601312637329 and perplexity is 150.44556322891526
At time: 1023.6467356681824 and batch: 350, loss is 4.965872888565063 and perplexity is 143.43369727990267
At time: 1025.3140017986298 and batch: 400, loss is 4.984844236373902 and perplexity is 146.1808036006885
At time: 1026.980639219284 and batch: 450, loss is 4.949919548034668 and perplexity is 141.1636065750402
At time: 1028.6509373188019 and batch: 500, loss is 4.925491008758545 and perplexity is 137.75696491601465
At time: 1030.3188560009003 and batch: 550, loss is 4.954838514328003 and perplexity is 141.85969621480453
At time: 1031.9869389533997 and batch: 600, loss is 4.925346307754516 and perplexity is 137.73703278701367
At time: 1033.6560597419739 and batch: 650, loss is 4.912476263046265 and perplexity is 135.97570948367218
At time: 1035.3317093849182 and batch: 700, loss is 4.903307723999023 and perplexity is 134.73470865747774
At time: 1037.0251331329346 and batch: 750, loss is 4.926124973297119 and perplexity is 137.8443256355862
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9481932174327765 and perplexity of 140.9201217492591
Finished 37 epochs...
Completing Train Step...
At time: 1041.623366355896 and batch: 50, loss is 5.005853214263916 and perplexity is 149.2844004160292
At time: 1043.3419647216797 and batch: 100, loss is 4.985791616439819 and perplexity is 146.3193580015152
At time: 1045.033995628357 and batch: 150, loss is 4.970844755172729 and perplexity is 144.14860623311918
At time: 1046.7236795425415 and batch: 200, loss is 4.979253578186035 and perplexity is 145.3658369168899
At time: 1048.4209842681885 and batch: 250, loss is 4.989016675949097 and perplexity is 146.79200839157699
At time: 1050.1262786388397 and batch: 300, loss is 5.011587963104248 and perplexity is 150.1429684411875
At time: 1051.834946155548 and batch: 350, loss is 4.9640318965911865 and perplexity is 143.16987991176748
At time: 1053.5221173763275 and batch: 400, loss is 4.9831467628479 and perplexity is 145.93287604133275
At time: 1055.2160165309906 and batch: 450, loss is 4.9483347988128665 and perplexity is 140.94007482703762
At time: 1056.905600309372 and batch: 500, loss is 4.923823976516724 and perplexity is 137.527510920706
At time: 1058.6520102024078 and batch: 550, loss is 4.953100900650025 and perplexity is 141.6134129009023
At time: 1060.3521900177002 and batch: 600, loss is 4.923638534545899 and perplexity is 137.50200991258984
At time: 1062.0437998771667 and batch: 650, loss is 4.910730171203613 and perplexity is 135.73849056981933
At time: 1063.7280702590942 and batch: 700, loss is 4.901676731109619 and perplexity is 134.51513641475913
At time: 1065.4191942214966 and batch: 750, loss is 4.924703292846679 and perplexity is 137.64849429040768
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9474395042242 and perplexity of 140.81394840929318
Finished 38 epochs...
Completing Train Step...
At time: 1069.996192932129 and batch: 50, loss is 5.004115238189697 and perplexity is 149.02517303059983
At time: 1071.7309732437134 and batch: 100, loss is 4.983931541442871 and perplexity is 146.0474459888937
At time: 1073.4184126853943 and batch: 150, loss is 4.968780755996704 and perplexity is 143.85139046069742
At time: 1075.105192899704 and batch: 200, loss is 4.976962776184082 and perplexity is 145.03321369909264
At time: 1076.7865324020386 and batch: 250, loss is 4.986532049179077 and perplexity is 146.4277377635773
At time: 1078.4634139537811 and batch: 300, loss is 5.009628601074219 and perplexity is 149.8490720285086
At time: 1080.1449749469757 and batch: 350, loss is 4.962039384841919 and perplexity is 142.88489625482902
At time: 1081.8288922309875 and batch: 400, loss is 4.981561002731323 and perplexity is 145.70164489382987
At time: 1083.5168976783752 and batch: 450, loss is 4.946804084777832 and perplexity is 140.724500909504
At time: 1085.201599597931 and batch: 500, loss is 4.9220992469787594 and perplexity is 137.29051759378245
At time: 1086.8841111660004 and batch: 550, loss is 4.951204643249512 and perplexity is 141.3451318640181
At time: 1088.5659675598145 and batch: 600, loss is 4.921899003982544 and perplexity is 137.26302888149038
At time: 1090.2468860149384 and batch: 650, loss is 4.908982038497925 and perplexity is 135.50140896046722
At time: 1091.92804479599 and batch: 700, loss is 4.899877367019653 and perplexity is 134.27331233876308
At time: 1093.6214287281036 and batch: 750, loss is 4.923243894577026 and perplexity is 137.44775682959963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.94678550542787 and perplexity of 140.72188636404178
Finished 39 epochs...
Completing Train Step...
At time: 1098.041977405548 and batch: 50, loss is 5.00225902557373 and perplexity is 148.74880720054614
At time: 1099.7375311851501 and batch: 100, loss is 4.982240571975708 and perplexity is 145.80069290173412
At time: 1101.4197254180908 and batch: 150, loss is 4.966748380661011 and perplexity is 143.55932733419908
At time: 1103.1048698425293 and batch: 200, loss is 4.974771203994751 and perplexity is 144.71571098356134
At time: 1104.7917084693909 and batch: 250, loss is 4.984332103729248 and perplexity is 146.10595880601
At time: 1106.4675419330597 and batch: 300, loss is 5.008372755050659 and perplexity is 149.661002784991
At time: 1108.1477363109589 and batch: 350, loss is 4.960444478988648 and perplexity is 142.65718993079116
At time: 1109.83398604393 and batch: 400, loss is 4.979511222839355 and perplexity is 145.40329447270926
At time: 1111.5152788162231 and batch: 450, loss is 4.944950037002563 and perplexity is 140.46383268201097
At time: 1113.2051351070404 and batch: 500, loss is 4.920071697235107 and perplexity is 137.01243624717586
At time: 1114.910087108612 and batch: 550, loss is 4.949488172531128 and perplexity is 141.1027251854865
At time: 1116.6122696399689 and batch: 600, loss is 4.920075769424439 and perplexity is 137.01299418889303
At time: 1118.307695388794 and batch: 650, loss is 4.907083425521851 and perplexity is 135.2443882957307
At time: 1120.0051503181458 and batch: 700, loss is 4.898134508132935 and perplexity is 134.03949671610795
At time: 1121.712769985199 and batch: 750, loss is 4.921692714691162 and perplexity is 137.23471590896148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.945607030114462 and perplexity of 140.5561467740947
Finished 40 epochs...
Completing Train Step...
At time: 1126.1729452610016 and batch: 50, loss is 5.000712423324585 and perplexity is 148.51892977106866
At time: 1127.8825328350067 and batch: 100, loss is 4.98088677406311 and perplexity is 145.60344177723513
At time: 1129.5594532489777 and batch: 150, loss is 4.964864826202392 and perplexity is 143.28918002157855
At time: 1131.2372677326202 and batch: 200, loss is 4.973078842163086 and perplexity is 144.47100676029874
At time: 1132.9199380874634 and batch: 250, loss is 4.982084789276123 and perplexity is 145.77798144526452
At time: 1134.6066529750824 and batch: 300, loss is 5.005758600234985 and perplexity is 149.2702766856114
At time: 1136.283277273178 and batch: 350, loss is 4.958607063293457 and perplexity is 142.3953100357729
At time: 1137.9578309059143 and batch: 400, loss is 4.977544231414795 and perplexity is 145.11756854173072
At time: 1139.6372447013855 and batch: 450, loss is 4.943295850753784 and perplexity is 140.2316714134657
At time: 1141.3238401412964 and batch: 500, loss is 4.918593158721924 and perplexity is 136.81000776942662
At time: 1143.040539264679 and batch: 550, loss is 4.948477382659912 and perplexity is 140.96017203784413
At time: 1144.7237441539764 and batch: 600, loss is 4.918842191696167 and perplexity is 136.84408221522185
At time: 1146.4049117565155 and batch: 650, loss is 4.905690221786499 and perplexity is 135.05609650364224
At time: 1148.0910987854004 and batch: 700, loss is 4.896590757369995 and perplexity is 133.83273277784485
At time: 1149.7731461524963 and batch: 750, loss is 4.920425186157226 and perplexity is 137.06087718674513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.944646436114644 and perplexity of 140.42119421055455
Finished 41 epochs...
Completing Train Step...
At time: 1154.2167675495148 and batch: 50, loss is 4.9992530822753904 and perplexity is 148.30234807199068
At time: 1155.93043923378 and batch: 100, loss is 4.978911485671997 and perplexity is 145.31611685719875
At time: 1157.6105704307556 and batch: 150, loss is 4.96255446434021 and perplexity is 142.95851229293368
At time: 1159.2950925827026 and batch: 200, loss is 4.970969924926758 and perplexity is 144.1666504079738
At time: 1160.9861061573029 and batch: 250, loss is 4.979601964950562 and perplexity is 145.41648927327907
At time: 1162.6654195785522 and batch: 300, loss is 5.003565320968628 and perplexity is 148.94324405072064
At time: 1164.3456184864044 and batch: 350, loss is 4.956560153961181 and perplexity is 142.10413784998417
At time: 1166.037546634674 and batch: 400, loss is 4.9756646251678465 and perplexity is 144.8450608371786
At time: 1167.715342283249 and batch: 450, loss is 4.9418044090271 and perplexity is 140.0226799353625
At time: 1169.389131307602 and batch: 500, loss is 4.917306957244873 and perplexity is 136.6341556502288
At time: 1171.0632553100586 and batch: 550, loss is 4.947267675399781 and perplexity is 140.78975459273315
At time: 1172.7445800304413 and batch: 600, loss is 4.917567739486694 and perplexity is 136.66979205811785
At time: 1174.43510055542 and batch: 650, loss is 4.904257040023804 and perplexity is 134.86267520623784
At time: 1176.1230926513672 and batch: 700, loss is 4.895074510574341 and perplexity is 133.6299630889098
At time: 1177.8210859298706 and batch: 750, loss is 4.919233293533325 and perplexity is 136.89761265442462
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.943675906159157 and perplexity of 140.28497734715341
Finished 42 epochs...
Completing Train Step...
At time: 1182.2915680408478 and batch: 50, loss is 4.997775831222534 and perplexity is 148.08343001044025
At time: 1184.0011885166168 and batch: 100, loss is 4.977242288589477 and perplexity is 145.07375794756211
At time: 1185.6801900863647 and batch: 150, loss is 4.960344343185425 and perplexity is 142.64290555369266
At time: 1187.3617489337921 and batch: 200, loss is 4.968860607147217 and perplexity is 143.86287761835388
At time: 1189.0471093654633 and batch: 250, loss is 4.977348661422729 and perplexity is 145.08919067502245
At time: 1190.7323927879333 and batch: 300, loss is 5.002455615997315 and perplexity is 148.7780526661564
At time: 1192.4134359359741 and batch: 350, loss is 4.954646635055542 and perplexity is 141.83247889080695
At time: 1194.0965321063995 and batch: 400, loss is 4.9737098598480225 and perplexity is 144.56219928954846
At time: 1195.7763254642487 and batch: 450, loss is 4.940294809341431 and perplexity is 139.81146120971854
At time: 1197.4614124298096 and batch: 500, loss is 4.916027355194092 and perplexity is 136.4594301178678
At time: 1199.1431663036346 and batch: 550, loss is 4.945850248336792 and perplexity is 140.59033674789447
At time: 1200.8243613243103 and batch: 600, loss is 4.915988340377807 and perplexity is 136.45410628212642
At time: 1202.5063841342926 and batch: 650, loss is 4.902588109970093 and perplexity is 134.63778654838126
At time: 1204.1875677108765 and batch: 700, loss is 4.893542900085449 and perplexity is 133.42545069245756
At time: 1205.8686645030975 and batch: 750, loss is 4.9177654838562015 and perplexity is 136.69682041224343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.942640082780705 and perplexity of 140.13974211997902
Finished 43 epochs...
Completing Train Step...
At time: 1210.337585926056 and batch: 50, loss is 4.996281986236572 and perplexity is 147.86238146825048
At time: 1212.0418589115143 and batch: 100, loss is 4.975654277801514 and perplexity is 144.84356208002674
At time: 1213.724189043045 and batch: 150, loss is 4.9583705711364745 and perplexity is 142.36163864342532
At time: 1215.4047138690948 and batch: 200, loss is 4.967231178283692 and perplexity is 143.62865417021885
At time: 1217.0840365886688 and batch: 250, loss is 4.975044240951538 and perplexity is 144.7552291155973
At time: 1218.7676830291748 and batch: 300, loss is 4.999539098739624 and perplexity is 148.3447710517699
At time: 1220.4491350650787 and batch: 350, loss is 4.952637710571289 and perplexity is 141.54783416191378
At time: 1222.1215693950653 and batch: 400, loss is 4.972071533203125 and perplexity is 144.3255530914254
At time: 1223.795655965805 and batch: 450, loss is 4.938993434906006 and perplexity is 139.62963248760786
At time: 1225.4684808254242 and batch: 500, loss is 4.9148329067230225 and perplexity is 136.29653366527705
At time: 1227.166254043579 and batch: 550, loss is 4.944582843780518 and perplexity is 140.41226478297855
At time: 1228.8492345809937 and batch: 600, loss is 4.914648914337159 and perplexity is 136.27145844775828
At time: 1230.5339353084564 and batch: 650, loss is 4.901120681762695 and perplexity is 134.4403601525768
At time: 1232.217990398407 and batch: 700, loss is 4.892105903625488 and perplexity is 133.23385648521213
At time: 1233.896072626114 and batch: 750, loss is 4.916629838943481 and perplexity is 136.5416694784163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.94156309615734 and perplexity of 139.9888947372014
Finished 44 epochs...
Completing Train Step...
At time: 1238.3344864845276 and batch: 50, loss is 4.995242738723755 and perplexity is 147.7087956767195
At time: 1240.03906083107 and batch: 100, loss is 4.974410953521729 and perplexity is 144.66358646972984
At time: 1241.7184481620789 and batch: 150, loss is 4.956414813995361 and perplexity is 142.0834859402565
At time: 1243.4001021385193 and batch: 200, loss is 4.965166053771973 and perplexity is 143.3323491745667
At time: 1245.0831463336945 and batch: 250, loss is 4.972554264068603 and perplexity is 144.3952403093121
At time: 1246.7664158344269 and batch: 300, loss is 4.9974039363861085 and perplexity is 148.02836878658619
At time: 1248.4489324092865 and batch: 350, loss is 4.950700731277466 and perplexity is 141.2739243025595
At time: 1250.1339762210846 and batch: 400, loss is 4.970403833389282 and perplexity is 144.08506198262904
At time: 1251.8117496967316 and batch: 450, loss is 4.937626838684082 and perplexity is 139.43894548512722
At time: 1253.490662097931 and batch: 500, loss is 4.913563947677613 and perplexity is 136.12368863585002
At time: 1255.1768250465393 and batch: 550, loss is 4.943236761093139 and perplexity is 140.22338541640107
At time: 1256.8591465950012 and batch: 600, loss is 4.9132142162323 and perplexity is 136.07609022529766
At time: 1258.5440254211426 and batch: 650, loss is 4.899541864395141 and perplexity is 134.22827084626354
At time: 1260.2389118671417 and batch: 700, loss is 4.890713634490967 and perplexity is 133.04848817038956
At time: 1261.9375958442688 and batch: 750, loss is 4.915373506546021 and perplexity is 136.37023546704737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.940598243890807 and perplexity of 139.85389127449653
Finished 45 epochs...
Completing Train Step...
At time: 1266.3438456058502 and batch: 50, loss is 4.994164333343506 and perplexity is 147.54959157546025
At time: 1268.0373756885529 and batch: 100, loss is 4.972914409637451 and perplexity is 144.44725298077665
At time: 1269.7196741104126 and batch: 150, loss is 4.954358377456665 and perplexity is 141.7916004930366
At time: 1271.394791841507 and batch: 200, loss is 4.963132572174072 and perplexity is 143.04118162239868
At time: 1273.0679037570953 and batch: 250, loss is 4.970125017166137 and perplexity is 144.0448943297923
At time: 1274.736064195633 and batch: 300, loss is 4.995233736038208 and perplexity is 147.70746590686522
At time: 1276.4044365882874 and batch: 350, loss is 4.948833866119385 and perplexity is 141.01043096532595
At time: 1278.0722408294678 and batch: 400, loss is 4.968772840499878 and perplexity is 143.8502518099793
At time: 1279.747045993805 and batch: 450, loss is 4.936240072250366 and perplexity is 139.2457102530105
At time: 1281.422307252884 and batch: 500, loss is 4.912257833480835 and perplexity is 135.94601161210534
At time: 1283.1003074645996 and batch: 550, loss is 4.941831979751587 and perplexity is 140.02654051531223
At time: 1284.7861936092377 and batch: 600, loss is 4.9117111968994145 and perplexity is 135.8717188564789
At time: 1286.4687287807465 and batch: 650, loss is 4.897975101470947 and perplexity is 134.01813163027566
At time: 1288.166096687317 and batch: 700, loss is 4.88928376197815 and perplexity is 132.85838174065194
At time: 1289.8707392215729 and batch: 750, loss is 4.914109363555908 and perplexity is 136.19795290765964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.939568453056868 and perplexity of 139.7099451490925
Finished 46 epochs...
Completing Train Step...
At time: 1294.3459112644196 and batch: 50, loss is 4.993178815841675 and perplexity is 147.4042505004246
At time: 1296.0624675750732 and batch: 100, loss is 4.971533346176147 and perplexity is 144.2478998489484
At time: 1297.7390656471252 and batch: 150, loss is 4.952471752166748 and perplexity is 141.52434505835095
At time: 1299.4197673797607 and batch: 200, loss is 4.961166534423828 and perplexity is 142.76023352726577
At time: 1301.108719587326 and batch: 250, loss is 4.9677351951599125 and perplexity is 143.70106368210463
At time: 1302.795027256012 and batch: 300, loss is 4.993247051239013 and perplexity is 147.41430903119695
At time: 1304.4736623764038 and batch: 350, loss is 4.946945276260376 and perplexity is 140.74437141315755
At time: 1306.1540777683258 and batch: 400, loss is 4.967219114303589 and perplexity is 143.62692144744452
At time: 1307.8353338241577 and batch: 450, loss is 4.934837703704834 and perplexity is 139.05057330778865
At time: 1309.524071931839 and batch: 500, loss is 4.910906867980957 and perplexity is 135.76247724278605
At time: 1311.2268300056458 and batch: 550, loss is 4.94036298751831 and perplexity is 139.8209936251983
At time: 1312.904271364212 and batch: 600, loss is 4.910169982910157 and perplexity is 135.66247275056483
At time: 1314.59117436409 and batch: 650, loss is 4.896467447280884 and perplexity is 133.81623086908687
At time: 1316.269945859909 and batch: 700, loss is 4.887929525375366 and perplexity is 132.67858183040073
At time: 1317.9521565437317 and batch: 750, loss is 4.912888622283935 and perplexity is 136.0317918858302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.938683798146802 and perplexity of 139.58640471351484
Finished 47 epochs...
Completing Train Step...
At time: 1322.3645985126495 and batch: 50, loss is 4.992108898162842 and perplexity is 147.24662442537783
At time: 1324.0671989917755 and batch: 100, loss is 4.9700258922576905 and perplexity is 144.03061660048064
At time: 1325.7592389583588 and batch: 150, loss is 4.950478410720825 and perplexity is 141.2425196961439
At time: 1327.4407215118408 and batch: 200, loss is 4.959164619445801 and perplexity is 142.4747255541963
At time: 1329.128663301468 and batch: 250, loss is 4.965443210601807 and perplexity is 143.37208021968752
At time: 1330.8165800571442 and batch: 300, loss is 4.991601238250732 and perplexity is 147.17189218784915
At time: 1332.4939436912537 and batch: 350, loss is 4.945094728469849 and perplexity is 140.484158070482
At time: 1334.1719944477081 and batch: 400, loss is 4.965473489761353 and perplexity is 143.37642147150333
At time: 1335.8485424518585 and batch: 450, loss is 4.9332589912414555 and perplexity is 138.8312256236864
At time: 1337.5277588367462 and batch: 500, loss is 4.9093786811828615 and perplexity is 135.5551652634474
At time: 1339.1947782039642 and batch: 550, loss is 4.938675403594971 and perplexity is 139.58523295312372
At time: 1340.8621590137482 and batch: 600, loss is 4.908592042922973 and perplexity is 135.4485743138818
At time: 1342.5378108024597 and batch: 650, loss is 4.894798879623413 and perplexity is 133.59313561074066
At time: 1344.2316427230835 and batch: 700, loss is 4.886354694366455 and perplexity is 132.4697999267737
At time: 1345.9398770332336 and batch: 750, loss is 4.910915298461914 and perplexity is 135.76362179058964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.938988264216933 and perplexity of 139.62891050805942
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1350.3739376068115 and batch: 50, loss is 4.992323913574219 and perplexity is 147.278288122872
At time: 1352.0913543701172 and batch: 100, loss is 4.968923854827881 and perplexity is 143.871976899448
At time: 1353.7726392745972 and batch: 150, loss is 4.950949697494507 and perplexity is 141.30910111579706
At time: 1355.4536023139954 and batch: 200, loss is 4.959803524017334 and perplexity is 142.56578239289868
At time: 1357.135081768036 and batch: 250, loss is 4.962881631851197 and perplexity is 143.0052913254475
At time: 1358.8096072673798 and batch: 300, loss is 4.988349876403809 and perplexity is 146.69416017332867
At time: 1360.4888412952423 and batch: 350, loss is 4.938827934265137 and perplexity is 139.60652560610143
At time: 1362.1709280014038 and batch: 400, loss is 4.95757493019104 and perplexity is 142.2484149433403
At time: 1363.8526396751404 and batch: 450, loss is 4.923376531600952 and perplexity is 137.46598870009458
At time: 1365.5284311771393 and batch: 500, loss is 4.894361581802368 and perplexity is 133.53472839523633
At time: 1367.2074332237244 and batch: 550, loss is 4.918193578720093 and perplexity is 136.75535214664572
At time: 1368.8893823623657 and batch: 600, loss is 4.888648977279663 and perplexity is 132.77407203494394
At time: 1370.5756831169128 and batch: 650, loss is 4.873549489974976 and perplexity is 130.7843116054284
At time: 1372.258941411972 and batch: 700, loss is 4.863810014724732 and perplexity is 129.51672387905953
At time: 1373.9427349567413 and batch: 750, loss is 4.890824165344238 and perplexity is 133.0631949460747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.930482021597928 and perplexity of 138.4462303320841
Finished 49 epochs...
Completing Train Step...
At time: 1378.3844575881958 and batch: 50, loss is 4.987877311706543 and perplexity is 146.62485406903465
At time: 1380.1003873348236 and batch: 100, loss is 4.965083274841309 and perplexity is 143.32048476704077
At time: 1381.7779400348663 and batch: 150, loss is 4.947738590240479 and perplexity is 140.85607019086328
At time: 1383.4556238651276 and batch: 200, loss is 4.95562557220459 and perplexity is 141.97139195582108
At time: 1385.1378667354584 and batch: 250, loss is 4.959816293716431 and perplexity is 142.5676029266651
At time: 1386.8207256793976 and batch: 300, loss is 4.985537004470825 and perplexity is 146.2821080840112
At time: 1388.494862794876 and batch: 350, loss is 4.935274658203125 and perplexity is 139.11134535762875
At time: 1390.1790072917938 and batch: 400, loss is 4.954981498718261 and perplexity is 141.87998138716702
At time: 1391.8556928634644 and batch: 450, loss is 4.921460781097412 and perplexity is 137.20289025897478
At time: 1393.542695760727 and batch: 500, loss is 4.89323802947998 and perplexity is 133.38477939457667
At time: 1395.2434132099152 and batch: 550, loss is 4.917575559616089 and perplexity is 136.67086083775507
At time: 1396.9163689613342 and batch: 600, loss is 4.888272476196289 and perplexity is 132.72409186236376
At time: 1398.5924434661865 and batch: 650, loss is 4.874215221405029 and perplexity is 130.8714078203491
At time: 1400.2843298912048 and batch: 700, loss is 4.865184669494629 and perplexity is 129.69488708971912
At time: 1401.9861602783203 and batch: 750, loss is 4.891998891830444 and perplexity is 133.21959965390718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.929470328397529 and perplexity of 138.30623604980855
Finished Training.
Improved accuracyfrom -142.73488436332843 to -138.30623604980855
<pretraining.langmodel.trainer.TrainLangModel object at 0x7fb441508f28>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'dropout': 0.8854947775984927, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 18.411270472382746, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 6.977251598321425, 'wordvec_source': 'glove', 'num_layers': 1}, 'best_accuracy': -225.01869140826716}, {'params': {'dropout': 0.029713015436989565, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 29.533192484499416, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 5.136763059877755, 'wordvec_source': 'glove', 'num_layers': 1}, 'best_accuracy': -192.02307654891396}, {'params': {'dropout': 0.8004002862614646, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 26.058052406139705, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 2.8482811379792228, 'wordvec_source': 'glove', 'num_layers': 1}, 'best_accuracy': -320.8523953531095}, {'params': {'dropout': 0.19469257568311504, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 29.828116662289073, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 4.336722391562965, 'wordvec_source': 'glove', 'num_layers': 1}, 'best_accuracy': -166.06681505226368}, {'params': {'dropout': 0.06293036160504895, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 20.233557798071583, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 7.777238875493282, 'wordvec_source': 'glove', 'num_layers': 1}, 'best_accuracy': -142.73488436332843}, {'params': {'dropout': 0.0, 'batch_size': 80, 'tune_wordvecs': True, 'data': 'wikitext', 'lr': 21.23703938645875, 'wordvec_dim': 200, 'seq_len': 35, 'anneal': 8.0, 'wordvec_source': 'glove', 'num_layers': 1}, 'best_accuracy': -138.30623604980855}]
