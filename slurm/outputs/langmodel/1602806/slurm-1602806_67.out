Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'type': 'continuous', 'name': 'lr', 'domain': [0, 30]}, {'type': 'continuous', 'name': 'dropout', 'domain': [0, 1]}, {'type': 'continuous', 'name': 'anneal', 'domain': [2, 8]}]
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 3.3014077963985886, 'num_layers': 1, 'lr': 14.212458493693168, 'dropout': 0.16697668162061952}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 2.075449228286743 and batch: 50, loss is 6.94985034942627 and perplexity is 1042.9936319052342
At time: 3.3064115047454834 and batch: 100, loss is 5.920422382354737 and perplexity is 372.56904723789955
At time: 4.508394956588745 and batch: 150, loss is 5.7988705062866215 and perplexity is 329.92669924472744
At time: 5.711919546127319 and batch: 200, loss is 5.7280340576171875 and perplexity is 307.36441329004003
At time: 6.913750410079956 and batch: 250, loss is 5.722985277175903 and perplexity is 305.8165086552268
At time: 8.113319635391235 and batch: 300, loss is 5.709296045303344 and perplexity is 301.65863950537835
At time: 9.319301128387451 and batch: 350, loss is 5.626056804656982 and perplexity is 277.56546205307365
At time: 10.52029800415039 and batch: 400, loss is 5.65012565612793 and perplexity is 284.3271910332185
At time: 11.722279071807861 and batch: 450, loss is 5.597783060073852 and perplexity is 269.8275523845429
At time: 12.924817323684692 and batch: 500, loss is 5.591656951904297 and perplexity is 268.1796124943402
At time: 14.127609729766846 and batch: 550, loss is 5.595896043777466 and perplexity is 269.31886349903385
At time: 15.333481550216675 and batch: 600, loss is 5.553579158782959 and perplexity is 258.1598995859131
At time: 16.535213470458984 and batch: 650, loss is 5.540208702087402 and perplexity is 254.73115683549187
At time: 17.73753070831299 and batch: 700, loss is 5.535258617401123 and perplexity is 253.47333177561413
At time: 18.944926261901855 and batch: 750, loss is 5.522788372039795 and perplexity is 250.33208390870834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.166194117346476 and perplexity of 175.24659868142624
Finished 1 epochs...
Completing Train Step...
At time: 22.37582802772522 and batch: 50, loss is 5.441465263366699 and perplexity is 230.7800894503863
At time: 23.571547269821167 and batch: 100, loss is 5.405239534378052 and perplexity is 222.56952715243085
At time: 24.772509336471558 and batch: 150, loss is 5.394362716674805 and perplexity is 220.16179693780376
At time: 25.97175097465515 and batch: 200, loss is 5.379197435379028 and perplexity is 216.84817089914023
At time: 27.17538857460022 and batch: 250, loss is 5.394441680908203 and perplexity is 220.17918253173355
At time: 28.378137350082397 and batch: 300, loss is 5.418930892944336 and perplexity is 225.63776258122562
At time: 29.580567836761475 and batch: 350, loss is 5.351558313369751 and perplexity is 210.9367474402359
At time: 30.785420894622803 and batch: 400, loss is 5.387124824523926 and perplexity is 218.57404252490454
At time: 31.98894500732422 and batch: 450, loss is 5.328217792510986 and perplexity is 206.07038650877544
At time: 33.19039511680603 and batch: 500, loss is 5.3308033275604245 and perplexity is 206.60387809914906
At time: 34.39017605781555 and batch: 550, loss is 5.343770380020142 and perplexity is 209.3003663942424
At time: 35.589879274368286 and batch: 600, loss is 5.301954956054687 and perplexity is 200.72884271295857
At time: 36.84123229980469 and batch: 650, loss is 5.299186191558838 and perplexity is 200.1738405097162
At time: 38.040562868118286 and batch: 700, loss is 5.307803802490234 and perplexity is 201.9063149596707
At time: 39.238091230392456 and batch: 750, loss is 5.295502119064331 and perplexity is 199.43774232186885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.149999574173329 and perplexity of 172.43141689094236
Finished 2 epochs...
Completing Train Step...
At time: 42.70710897445679 and batch: 50, loss is 5.307585515975952 and perplexity is 201.86224634393392
At time: 43.91420102119446 and batch: 100, loss is 5.3008497524261475 and perplexity is 200.50711901512648
At time: 45.114174365997314 and batch: 150, loss is 5.296050224304199 and perplexity is 199.54708515641593
At time: 46.31978416442871 and batch: 200, loss is 5.292638187408447 and perplexity is 198.86738338261122
At time: 47.52588605880737 and batch: 250, loss is 5.301819305419922 and perplexity is 200.70161556476063
At time: 48.72738742828369 and batch: 300, loss is 5.318721809387207 and perplexity is 204.12280732074075
At time: 49.925877809524536 and batch: 350, loss is 5.260382280349732 and perplexity is 192.55508725530368
At time: 51.127923250198364 and batch: 400, loss is 5.304272203445435 and perplexity is 201.19452043685163
At time: 52.3323757648468 and batch: 450, loss is 5.254343318939209 and perplexity is 191.3957586077509
At time: 53.537163734436035 and batch: 500, loss is 5.252868442535401 and perplexity is 191.11368158509632
At time: 54.7385733127594 and batch: 550, loss is 5.271046237945557 and perplexity is 194.6194742448447
At time: 55.94547748565674 and batch: 600, loss is 5.233375577926636 and perplexity is 187.42440259079495
At time: 57.151034355163574 and batch: 650, loss is 5.1984865093231205 and perplexity is 180.9980955383897
At time: 58.35328769683838 and batch: 700, loss is 5.195557641983032 and perplexity is 180.46875169510045
At time: 59.55210542678833 and batch: 750, loss is 5.189456796646118 and perplexity is 179.37109148505232
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.125001419422238 and perplexity of 168.17438036213144
Finished 3 epochs...
Completing Train Step...
At time: 63.07011413574219 and batch: 50, loss is 5.223797101974487 and perplexity is 185.63773290418803
At time: 64.34043550491333 and batch: 100, loss is 5.233102331161499 and perplexity is 187.37319647535057
At time: 65.54249811172485 and batch: 150, loss is 5.2254547595977785 and perplexity is 185.94571189864843
At time: 66.74540185928345 and batch: 200, loss is 5.201893119812012 and perplexity is 181.6157369837228
At time: 67.95355296134949 and batch: 250, loss is 5.217612390518188 and perplexity is 184.49316016522565
At time: 69.151850938797 and batch: 300, loss is 5.244353628158569 and perplexity is 189.49329250774977
At time: 70.35516834259033 and batch: 350, loss is 5.18553729057312 and perplexity is 178.66942140101958
At time: 71.55440759658813 and batch: 400, loss is 5.224257287979126 and perplexity is 185.72318045017784
At time: 72.75429058074951 and batch: 450, loss is 5.16606146812439 and perplexity is 175.2233538981733
At time: 73.95327425003052 and batch: 500, loss is 5.166085691452026 and perplexity is 175.22759844229256
At time: 75.15055751800537 and batch: 550, loss is 5.1803138732910154 and perplexity is 177.73858963526283
At time: 76.350266456604 and batch: 600, loss is 5.163284683227539 and perplexity is 174.7374712435031
At time: 77.5481789112091 and batch: 650, loss is 5.159292306900024 and perplexity is 174.04124422468223
At time: 78.74388146400452 and batch: 700, loss is 5.17205662727356 and perplexity is 176.27700102671446
At time: 79.94251871109009 and batch: 750, loss is 5.1423861598968506 and perplexity is 171.1236098354713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1115264892578125 and perplexity of 165.92344200903443
Finished 4 epochs...
Completing Train Step...
At time: 83.40782117843628 and batch: 50, loss is 5.171530694961548 and perplexity is 176.1843156312685
At time: 84.6579282283783 and batch: 100, loss is 5.1721478366851805 and perplexity is 176.29307988152067
At time: 85.86030387878418 and batch: 150, loss is 5.169355068206787 and perplexity is 175.8014209893568
At time: 87.06427717208862 and batch: 200, loss is 5.170842714309693 and perplexity is 176.06314591694272
At time: 88.27706623077393 and batch: 250, loss is 5.158795089721679 and perplexity is 173.95472943841455
At time: 89.48067736625671 and batch: 300, loss is 5.178089275360107 and perplexity is 177.34363221013464
At time: 90.69458985328674 and batch: 350, loss is 5.13685435295105 and perplexity is 170.17960050796285
At time: 91.89567065238953 and batch: 400, loss is 5.165094347000122 and perplexity is 175.05397360998273
At time: 93.09711956977844 and batch: 450, loss is 5.147874851226806 and perplexity is 172.06543684355827
At time: 94.354243516922 and batch: 500, loss is 5.130235595703125 and perplexity is 169.05694244269742
At time: 95.55586886405945 and batch: 550, loss is 5.12037371635437 and perplexity is 167.3979172722082
At time: 96.76058101654053 and batch: 600, loss is 5.097287902832031 and perplexity is 163.57766663385488
At time: 97.96854162216187 and batch: 650, loss is 5.091781940460205 and perplexity is 162.67948909144002
At time: 99.17933821678162 and batch: 700, loss is 5.107394781112671 and perplexity is 165.2393090642824
At time: 100.38877725601196 and batch: 750, loss is 5.1069842720031735 and perplexity is 165.17149074363604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.141356002452762 and perplexity of 170.94741634404005
Annealing...
Finished 5 epochs...
Completing Train Step...
At time: 103.82527232170105 and batch: 50, loss is 5.093913145065308 and perplexity is 163.02656207815323
At time: 105.05764508247375 and batch: 100, loss is 5.04519474029541 and perplexity is 155.2745344629634
At time: 106.2690920829773 and batch: 150, loss is 5.0098013496398925 and perplexity is 149.87496047680082
At time: 107.4717628955841 and batch: 200, loss is 4.989211750030518 and perplexity is 146.8206465009595
At time: 108.68145990371704 and batch: 250, loss is 4.991889934539795 and perplexity is 147.21438630062937
At time: 109.8871214389801 and batch: 300, loss is 5.001909198760987 and perplexity is 148.69677998021268
At time: 111.09806942939758 and batch: 350, loss is 4.941493730545044 and perplexity is 139.97918465858
At time: 112.30047655105591 and batch: 400, loss is 4.951954202651978 and perplexity is 141.45111815314704
At time: 113.50085639953613 and batch: 450, loss is 4.90877028465271 and perplexity is 135.47271905379404
At time: 114.70237135887146 and batch: 500, loss is 4.902059564590454 and perplexity is 134.5666431712841
At time: 115.90658760070801 and batch: 550, loss is 4.8793235111236575 and perplexity is 131.5416473186794
At time: 117.10862255096436 and batch: 600, loss is 4.829544229507446 and perplexity is 125.15390619641138
At time: 118.30976152420044 and batch: 650, loss is 4.813711366653442 and perplexity is 123.18796586403914
At time: 119.51843619346619 and batch: 700, loss is 4.80258430480957 and perplexity is 121.82484358654642
At time: 120.71769571304321 and batch: 750, loss is 4.811163301467896 and perplexity is 122.87447446481828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.985217072242914 and perplexity of 146.2353152089419
Finished 6 epochs...
Completing Train Step...
At time: 124.2114155292511 and batch: 50, loss is 4.947086868286132 and perplexity is 140.76430110472918
At time: 125.48050856590271 and batch: 100, loss is 4.94267014503479 and perplexity is 140.1439550998396
At time: 126.6827940940857 and batch: 150, loss is 4.904721069335937 and perplexity is 134.9252699624239
At time: 127.88967704772949 and batch: 200, loss is 4.895923461914062 and perplexity is 133.743456593516
At time: 129.09399390220642 and batch: 250, loss is 4.896870841979981 and perplexity is 133.8702225165126
At time: 130.2982680797577 and batch: 300, loss is 4.920371208190918 and perplexity is 137.0534791190023
At time: 131.50344276428223 and batch: 350, loss is 4.8660154628753665 and perplexity is 129.80268151467598
At time: 132.7088496685028 and batch: 400, loss is 4.880787305831909 and perplexity is 131.73433828153853
At time: 133.91229820251465 and batch: 450, loss is 4.842119121551514 and perplexity is 126.73763982611563
At time: 135.11793518066406 and batch: 500, loss is 4.834346361160279 and perplexity is 125.75635709737028
At time: 136.32066369056702 and batch: 550, loss is 4.826950778961182 and perplexity is 124.82974625809497
At time: 137.52443027496338 and batch: 600, loss is 4.785865144729614 and perplexity is 119.80496689694289
At time: 138.7270007133484 and batch: 650, loss is 4.777811412811279 and perplexity is 118.84396483199053
At time: 139.9246108531952 and batch: 700, loss is 4.770616426467895 and perplexity is 117.991952910664
At time: 141.14024209976196 and batch: 750, loss is 4.779852294921875 and perplexity is 119.08675902665883
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.981967305028161 and perplexity of 145.76085583475944
Finished 7 epochs...
Completing Train Step...
At time: 144.62476110458374 and batch: 50, loss is 4.886611089706421 and perplexity is 132.50376892072097
At time: 145.89615273475647 and batch: 100, loss is 4.882174158096314 and perplexity is 131.91716109169548
At time: 147.0970275402069 and batch: 150, loss is 4.848131341934204 and perplexity is 127.50190962460522
At time: 148.30136132240295 and batch: 200, loss is 4.846430778503418 and perplexity is 127.28526879768508
At time: 149.50798106193542 and batch: 250, loss is 4.847160167694092 and perplexity is 127.37814316353993
At time: 150.71304154396057 and batch: 300, loss is 4.872544574737549 and perplexity is 130.6529504723364
At time: 151.9123854637146 and batch: 350, loss is 4.824684104919434 and perplexity is 124.54711834629668
At time: 153.11153960227966 and batch: 400, loss is 4.8389441108703615 and perplexity is 126.33588459204088
At time: 154.31017327308655 and batch: 450, loss is 4.801429309844971 and perplexity is 121.68421773235067
At time: 155.5811448097229 and batch: 500, loss is 4.796948089599609 and perplexity is 121.1401439200758
At time: 156.78318738937378 and batch: 550, loss is 4.795006399154663 and perplexity is 120.90515547136826
At time: 157.9865107536316 and batch: 600, loss is 4.7542586994171145 and perplexity is 116.07757289021892
At time: 159.1899287700653 and batch: 650, loss is 4.7478045463562015 and perplexity is 115.33080294390489
At time: 160.3927755355835 and batch: 700, loss is 4.742145299911499 and perplexity is 114.6799608822378
At time: 161.59122014045715 and batch: 750, loss is 4.752103796005249 and perplexity is 115.82770624828659
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980411973110465 and perplexity of 145.53432553389842
Finished 8 epochs...
Completing Train Step...
At time: 165.0156750679016 and batch: 50, loss is 4.8428124332427975 and perplexity is 126.82553898076759
At time: 166.24692678451538 and batch: 100, loss is 4.841076078414917 and perplexity is 126.60551591819278
At time: 167.45063591003418 and batch: 150, loss is 4.808538789749146 and perplexity is 122.55241178016077
At time: 168.65487146377563 and batch: 200, loss is 4.811357316970825 and perplexity is 122.89831633054973
At time: 169.85654997825623 and batch: 250, loss is 4.810070171356201 and perplexity is 122.74023006348858
At time: 171.06172609329224 and batch: 300, loss is 4.8387669086456295 and perplexity is 126.31349957562362
At time: 172.26399183273315 and batch: 350, loss is 4.7882971096038816 and perplexity is 120.0966829460129
At time: 173.46695518493652 and batch: 400, loss is 4.804246015548706 and perplexity is 122.02744952704035
At time: 174.67567801475525 and batch: 450, loss is 4.765647382736206 and perplexity is 117.40710001986132
At time: 175.8788661956787 and batch: 500, loss is 4.767062196731567 and perplexity is 117.5733267903769
At time: 177.0806930065155 and batch: 550, loss is 4.7694003295898435 and perplexity is 117.84855047839227
At time: 178.2826006412506 and batch: 600, loss is 4.727406311035156 and perplexity is 113.00208965179935
At time: 179.48615765571594 and batch: 650, loss is 4.719738531112671 and perplexity is 112.13892799350253
At time: 180.69205474853516 and batch: 700, loss is 4.716722602844238 and perplexity is 111.80123451619401
At time: 181.89537620544434 and batch: 750, loss is 4.723887014389038 and perplexity is 112.60510074728393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.994882982830669 and perplexity of 147.6556661244219
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 185.3259997367859 and batch: 50, loss is 4.804743204116821 and perplexity is 122.08813526481886
At time: 186.5589039325714 and batch: 100, loss is 4.788741512298584 and perplexity is 120.15006609646832
At time: 187.76802229881287 and batch: 150, loss is 4.7489855766296385 and perplexity is 115.46709257889209
At time: 188.97134017944336 and batch: 200, loss is 4.739421062469482 and perplexity is 114.36797059971015
At time: 190.1850187778473 and batch: 250, loss is 4.7355351066589355 and perplexity is 113.92440411786998
At time: 191.40347409248352 and batch: 300, loss is 4.752270727157593 and perplexity is 115.84704311468174
At time: 192.611008644104 and batch: 350, loss is 4.695168409347534 and perplexity is 109.41723397525283
At time: 193.8153202533722 and batch: 400, loss is 4.698478317260742 and perplexity is 109.77999496537939
At time: 195.0221402645111 and batch: 450, loss is 4.659919595718383 and perplexity is 105.62758890481238
At time: 196.22766876220703 and batch: 500, loss is 4.648664541244507 and perplexity is 104.44540986198142
At time: 197.43355226516724 and batch: 550, loss is 4.64120114326477 and perplexity is 103.66879390295307
At time: 198.63822388648987 and batch: 600, loss is 4.592140455245971 and perplexity is 98.70547887126644
At time: 199.84878587722778 and batch: 650, loss is 4.5785737991333 and perplexity is 97.37541822091316
At time: 201.05797624588013 and batch: 700, loss is 4.570399084091187 and perplexity is 96.58264668037465
At time: 202.26320958137512 and batch: 750, loss is 4.587765951156616 and perplexity is 98.27463440281022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.932734999545785 and perplexity of 138.75849827028844
Finished 10 epochs...
Completing Train Step...
At time: 205.7513725757599 and batch: 50, loss is 4.744569835662841 and perplexity is 114.95834388576169
At time: 206.99551224708557 and batch: 100, loss is 4.737293100357055 and perplexity is 114.12485864964697
At time: 208.1952772140503 and batch: 150, loss is 4.706367874145508 and perplexity is 110.64953611727331
At time: 209.39581084251404 and batch: 200, loss is 4.700028886795044 and perplexity is 109.9503485194328
At time: 210.59331679344177 and batch: 250, loss is 4.699443712234497 and perplexity is 109.88602719399303
At time: 211.79329204559326 and batch: 300, loss is 4.719822587966919 and perplexity is 112.14835443520144
At time: 212.99094367027283 and batch: 350, loss is 4.669014616012573 and perplexity is 106.59265596932542
At time: 214.2035994529724 and batch: 400, loss is 4.675786476135254 and perplexity is 107.31693611974903
At time: 215.409277677536 and batch: 450, loss is 4.640334281921387 and perplexity is 103.57896637263528
At time: 216.66049432754517 and batch: 500, loss is 4.632342128753662 and perplexity is 102.75444664123381
At time: 217.85976243019104 and batch: 550, loss is 4.630798587799072 and perplexity is 102.59596328881513
At time: 219.05606985092163 and batch: 600, loss is 4.58837441444397 and perplexity is 98.33444910560239
At time: 220.25495290756226 and batch: 650, loss is 4.579806079864502 and perplexity is 97.49548603591319
At time: 221.46189618110657 and batch: 700, loss is 4.573476390838623 and perplexity is 96.88031889013085
At time: 222.65859842300415 and batch: 750, loss is 4.587197732925415 and perplexity is 98.2188088259353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.931151634038881 and perplexity of 138.5389666955637
Finished 11 epochs...
Completing Train Step...
At time: 226.10997200012207 and batch: 50, loss is 4.72232232093811 and perplexity is 112.42904605533438
At time: 227.34173250198364 and batch: 100, loss is 4.716576061248779 and perplexity is 111.78485218528948
At time: 228.5488519668579 and batch: 150, loss is 4.686616544723511 and perplexity is 108.48550230278768
At time: 229.75180387496948 and batch: 200, loss is 4.681836366653442 and perplexity is 107.96815976360028
At time: 230.95201230049133 and batch: 250, loss is 4.682479448318482 and perplexity is 108.03761443767242
At time: 232.15547394752502 and batch: 300, loss is 4.70388198852539 and perplexity is 110.37481562989376
At time: 233.3605010509491 and batch: 350, loss is 4.654845418930054 and perplexity is 105.09297335895488
At time: 234.56631922721863 and batch: 400, loss is 4.663244352340699 and perplexity is 105.97935938209541
At time: 235.76966428756714 and batch: 450, loss is 4.630239124298096 and perplexity is 102.5385806452516
At time: 236.9771158695221 and batch: 500, loss is 4.6234187889099125 and perplexity is 101.84161261461952
At time: 238.18048191070557 and batch: 550, loss is 4.624084272384644 and perplexity is 101.90940908106647
At time: 239.38644766807556 and batch: 600, loss is 4.584333238601684 and perplexity is 97.93786417984533
At time: 240.58926105499268 and batch: 650, loss is 4.57718934059143 and perplexity is 97.24069926930896
At time: 241.79548335075378 and batch: 700, loss is 4.570313625335693 and perplexity is 96.57439320025817
At time: 243.00459265708923 and batch: 750, loss is 4.581658983230591 and perplexity is 97.67630321681663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.930913880813954 and perplexity of 138.50603252471154
Finished 12 epochs...
Completing Train Step...
At time: 246.43087363243103 and batch: 50, loss is 4.7055388355255126 and perplexity is 110.55784139303317
At time: 247.65855407714844 and batch: 100, loss is 4.70072998046875 and perplexity is 110.0274610415936
At time: 248.8596441745758 and batch: 150, loss is 4.672179079055786 and perplexity is 106.93049875339565
At time: 250.0609335899353 and batch: 200, loss is 4.6688093566894535 and perplexity is 106.57077907820637
At time: 251.26032662391663 and batch: 250, loss is 4.669903326034546 and perplexity is 106.68742803715965
At time: 252.4648199081421 and batch: 300, loss is 4.691828060150146 and perplexity is 109.0523519615114
At time: 253.66659903526306 and batch: 350, loss is 4.64352575302124 and perplexity is 103.91006371313067
At time: 254.8693380355835 and batch: 400, loss is 4.652496280670166 and perplexity is 104.84638518265041
At time: 256.07571506500244 and batch: 450, loss is 4.620972328186035 and perplexity is 101.59276563060376
At time: 257.275799036026 and batch: 500, loss is 4.6152889919281 and perplexity is 101.01701741723609
At time: 258.4747037887573 and batch: 550, loss is 4.617415018081665 and perplexity is 101.23201069790848
At time: 259.6811475753784 and batch: 600, loss is 4.579081573486328 and perplexity is 97.42487551641155
At time: 260.88467931747437 and batch: 650, loss is 4.572099170684814 and perplexity is 96.74698519844652
At time: 262.0867953300476 and batch: 700, loss is 4.565369386672973 and perplexity is 96.09808481303965
At time: 263.2876763343811 and batch: 750, loss is 4.574831819534301 and perplexity is 97.01172228825153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.931183571039244 and perplexity of 138.54339128524708
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 266.71763610839844 and batch: 50, loss is 4.695006351470948 and perplexity is 109.39950348737423
At time: 267.96146178245544 and batch: 100, loss is 4.68947099685669 and perplexity is 108.79561136142549
At time: 269.1708290576935 and batch: 150, loss is 4.657048807144165 and perplexity is 105.32478927428045
At time: 270.3726418018341 and batch: 200, loss is 4.651669778823853 and perplexity is 104.75976525241462
At time: 271.5762691497803 and batch: 250, loss is 4.64656289100647 and perplexity is 104.22613264420262
At time: 272.77999925613403 and batch: 300, loss is 4.66385347366333 and perplexity is 106.04393333434294
At time: 273.9874265193939 and batch: 350, loss is 4.613698568344116 and perplexity is 100.85648526125354
At time: 275.1913466453552 and batch: 400, loss is 4.6193101024627685 and perplexity is 101.42403579466155
At time: 276.3939700126648 and batch: 450, loss is 4.583615064620972 and perplexity is 97.86755300491505
At time: 277.64774465560913 and batch: 500, loss is 4.569058895111084 and perplexity is 96.4532943792496
At time: 278.85161685943604 and batch: 550, loss is 4.567678689956665 and perplexity is 96.32026087307084
At time: 280.0523798465729 and batch: 600, loss is 4.528202934265137 and perplexity is 92.59201754004252
At time: 281.2530708312988 and batch: 650, loss is 4.5146427249908445 and perplexity is 91.34492493292097
At time: 282.4607708454132 and batch: 700, loss is 4.5043113422393795 and perplexity is 90.40606376900978
At time: 283.67154145240784 and batch: 750, loss is 4.524299240112304 and perplexity is 92.23127120236953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.904920356218205 and perplexity of 134.952161478288
Finished 14 epochs...
Completing Train Step...
At time: 287.14727997779846 and batch: 50, loss is 4.675497026443481 and perplexity is 107.28587776080019
At time: 288.3800461292267 and batch: 100, loss is 4.670235118865967 and perplexity is 106.72283203405652
At time: 289.58499789237976 and batch: 150, loss is 4.640547895431519 and perplexity is 103.60109460257827
At time: 290.79451727867126 and batch: 200, loss is 4.635908813476562 and perplexity is 103.12159371599668
At time: 292.0186560153961 and batch: 250, loss is 4.631721801757813 and perplexity is 102.6907250501936
At time: 293.2377882003784 and batch: 300, loss is 4.652232809066772 and perplexity is 104.81876477619242
At time: 294.4425356388092 and batch: 350, loss is 4.604418544769287 and perplexity is 99.92486411926951
At time: 295.6553325653076 and batch: 400, loss is 4.6115586471557615 and perplexity is 100.64089109099838
At time: 296.8596558570862 and batch: 450, loss is 4.578503742218017 and perplexity is 97.3685966384405
At time: 298.06418466567993 and batch: 500, loss is 4.565477952957154 and perplexity is 96.10851839138182
At time: 299.2709481716156 and batch: 550, loss is 4.566449842453003 and perplexity is 96.20197065614927
At time: 300.47263646125793 and batch: 600, loss is 4.528120241165161 and perplexity is 92.58436113564942
At time: 301.67549443244934 and batch: 650, loss is 4.516860609054565 and perplexity is 91.5477422157028
At time: 302.8828423023224 and batch: 700, loss is 4.508179292678833 and perplexity is 90.75642710027843
At time: 304.0865890979767 and batch: 750, loss is 4.5272453498840335 and perplexity is 92.50339530863563
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.903475739235102 and perplexity of 134.75734804318841
Finished 15 epochs...
Completing Train Step...
At time: 307.57380652427673 and batch: 50, loss is 4.667300891876221 and perplexity is 106.41014199600156
At time: 308.839674949646 and batch: 100, loss is 4.661336688995362 and perplexity is 105.77737915922724
At time: 310.0483863353729 and batch: 150, loss is 4.632308502197265 and perplexity is 102.75099142113267
At time: 311.2565884590149 and batch: 200, loss is 4.628231935501098 and perplexity is 102.33297277086099
At time: 312.46289229393005 and batch: 250, loss is 4.624430007934571 and perplexity is 101.94464887813213
At time: 313.6738269329071 and batch: 300, loss is 4.646044692993164 and perplexity is 104.17213686079398
At time: 314.87790966033936 and batch: 350, loss is 4.599664306640625 and perplexity is 99.45092502262897
At time: 316.08039259910583 and batch: 400, loss is 4.60758412361145 and perplexity is 100.24168535165728
At time: 317.2888572216034 and batch: 450, loss is 4.575855102539062 and perplexity is 97.1110435431411
At time: 318.4972310066223 and batch: 500, loss is 4.563568773269654 and perplexity is 95.92520500505103
At time: 319.71197748184204 and batch: 550, loss is 4.565595836639404 and perplexity is 96.11984868524068
At time: 320.91458654403687 and batch: 600, loss is 4.527595682144165 and perplexity is 92.53580790944228
At time: 322.1221385002136 and batch: 650, loss is 4.517324094772339 and perplexity is 91.59018312133124
At time: 323.33414363861084 and batch: 700, loss is 4.509113531112671 and perplexity is 90.84125486110072
At time: 324.54416012763977 and batch: 750, loss is 4.527588491439819 and perplexity is 92.53514251419855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.902978941451671 and perplexity of 134.6904175182244
Finished 16 epochs...
Completing Train Step...
At time: 327.99711513519287 and batch: 50, loss is 4.660849723815918 and perplexity is 105.7258817985323
At time: 329.2573208808899 and batch: 100, loss is 4.654925422668457 and perplexity is 105.1013815260414
At time: 330.46686601638794 and batch: 150, loss is 4.626264410018921 and perplexity is 102.13182798290389
At time: 331.6718807220459 and batch: 200, loss is 4.622701807022095 and perplexity is 101.76862019319762
At time: 332.8813238143921 and batch: 250, loss is 4.618786573410034 and perplexity is 101.37095126213745
At time: 334.0908668041229 and batch: 300, loss is 4.641153898239136 and perplexity is 103.66389618382495
At time: 335.2945148944855 and batch: 350, loss is 4.596055240631103 and perplexity is 99.09264698295335
At time: 336.50260305404663 and batch: 400, loss is 4.604455900192261 and perplexity is 99.92859692455409
At time: 337.71228981018066 and batch: 450, loss is 4.573661289215088 and perplexity is 96.89823355995065
At time: 338.9785487651825 and batch: 500, loss is 4.5615214824676515 and perplexity is 95.72901910847924
At time: 340.18300127983093 and batch: 550, loss is 4.564230995178223 and perplexity is 95.98874981544992
At time: 341.38874101638794 and batch: 600, loss is 4.526569099426269 and perplexity is 92.4408609920267
At time: 342.59040784835815 and batch: 650, loss is 4.517018156051636 and perplexity is 91.56216642379297
At time: 343.8025772571564 and batch: 700, loss is 4.5089802551269536 and perplexity is 90.82914871006278
At time: 345.00911378860474 and batch: 750, loss is 4.52701626777649 and perplexity is 92.48220686292292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9028508385946585 and perplexity of 134.67316439603877
Finished 17 epochs...
Completing Train Step...
At time: 348.4439151287079 and batch: 50, loss is 4.655514707565308 and perplexity is 105.16333443497682
At time: 349.67442989349365 and batch: 100, loss is 4.649460487365722 and perplexity is 104.52857587428157
At time: 350.8772852420807 and batch: 150, loss is 4.621421890258789 and perplexity is 101.63844815269276
At time: 352.07762145996094 and batch: 200, loss is 4.618046913146973 and perplexity is 101.2959989207116
At time: 353.2780773639679 and batch: 250, loss is 4.614263343811035 and perplexity is 100.91346261800089
At time: 354.4775445461273 and batch: 300, loss is 4.637159776687622 and perplexity is 103.25067575761044
At time: 355.6801960468292 and batch: 350, loss is 4.592834548950195 and perplexity is 98.77401350470073
At time: 356.8832085132599 and batch: 400, loss is 4.601528806686401 and perplexity is 99.6365242480008
At time: 358.08651542663574 and batch: 450, loss is 4.571524209976197 and perplexity is 96.69137547151614
At time: 359.29188537597656 and batch: 500, loss is 4.559408531188965 and perplexity is 95.52696189884108
At time: 360.4938750267029 and batch: 550, loss is 4.562644834518433 and perplexity is 95.83661692221139
At time: 361.69380021095276 and batch: 600, loss is 4.525241889953613 and perplexity is 92.31825398624488
At time: 362.89481949806213 and batch: 650, loss is 4.515991086959839 and perplexity is 91.46817402928525
At time: 364.0924782752991 and batch: 700, loss is 4.5081090259552 and perplexity is 90.75005016754312
At time: 365.2943642139435 and batch: 750, loss is 4.5258088874816895 and perplexity is 92.37061305037864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.902893776117369 and perplexity of 134.67894705223912
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 368.76813435554504 and batch: 50, loss is 4.652067356109619 and perplexity is 104.80142363620602
At time: 370.02137088775635 and batch: 100, loss is 4.646613454818725 and perplexity is 104.23140284804542
At time: 371.24702882766724 and batch: 150, loss is 4.617792129516602 and perplexity is 101.27019364588489
At time: 372.4643352031708 and batch: 200, loss is 4.613141050338745 and perplexity is 100.80027162627486
At time: 373.6586525440216 and batch: 250, loss is 4.606487274169922 and perplexity is 100.13179559233922
At time: 374.8646240234375 and batch: 300, loss is 4.62730788230896 and perplexity is 102.23845533700482
At time: 376.07654452323914 and batch: 350, loss is 4.582372226715088 and perplexity is 97.74599505434583
At time: 377.299453496933 and batch: 400, loss is 4.59001856803894 and perplexity is 98.49625902734012
At time: 378.5214993953705 and batch: 450, loss is 4.558651132583618 and perplexity is 95.45463730385697
At time: 379.74040031433105 and batch: 500, loss is 4.5427163791656495 and perplexity is 93.94564582901666
At time: 380.9520733356476 and batch: 550, loss is 4.542634153366089 and perplexity is 93.93792139075157
At time: 382.1682777404785 and batch: 600, loss is 4.504619331359863 and perplexity is 90.4339121413541
At time: 383.378781080246 and batch: 650, loss is 4.494065170288086 and perplexity is 89.48447712445027
At time: 384.5889382362366 and batch: 700, loss is 4.485124244689941 and perplexity is 88.68796913768928
At time: 385.7953383922577 and batch: 750, loss is 4.506383533477783 and perplexity is 90.5935966571492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.895620301712391 and perplexity of 133.70291704554216
Finished 19 epochs...
Completing Train Step...
At time: 389.23605847358704 and batch: 50, loss is 4.646304054260254 and perplexity is 104.19915858224813
At time: 390.4841101169586 and batch: 100, loss is 4.641037092208863 and perplexity is 103.65178832277844
At time: 391.68538665771484 and batch: 150, loss is 4.612541370391845 and perplexity is 100.73984184581326
At time: 392.88992500305176 and batch: 200, loss is 4.6082480525970455 and perplexity is 100.30826081037002
At time: 394.095091342926 and batch: 250, loss is 4.602400608062744 and perplexity is 99.72342538173424
At time: 395.3052730560303 and batch: 300, loss is 4.623908128738403 and perplexity is 101.89145996702233
At time: 396.5100145339966 and batch: 350, loss is 4.57998948097229 and perplexity is 97.51336845583417
At time: 397.7170670032501 and batch: 400, loss is 4.588055391311645 and perplexity is 98.3030831451328
At time: 398.92387890815735 and batch: 450, loss is 4.5575697708129885 and perplexity is 95.35147209775519
At time: 400.1839826107025 and batch: 500, loss is 4.542168064117432 and perplexity is 93.89414813746389
At time: 401.3874087333679 and batch: 550, loss is 4.542433385848999 and perplexity is 93.91906360059251
At time: 402.5957534313202 and batch: 600, loss is 4.505066747665405 and perplexity is 90.47438280116154
At time: 403.80442571640015 and batch: 650, loss is 4.495478849411011 and perplexity is 89.61106892059398
At time: 405.01089668273926 and batch: 700, loss is 4.48713773727417 and perplexity is 88.86672160385007
At time: 406.21375155448914 and batch: 750, loss is 4.507660245895385 and perplexity is 90.7093324919246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.89502840263899 and perplexity of 133.62380182925324
Finished 20 epochs...
Completing Train Step...
At time: 409.6819097995758 and batch: 50, loss is 4.643237714767456 and perplexity is 103.88013794991731
At time: 410.92037653923035 and batch: 100, loss is 4.638027248382568 and perplexity is 103.34028165598536
At time: 412.13006949424744 and batch: 150, loss is 4.609630947113037 and perplexity is 100.44707251300257
At time: 413.3340723514557 and batch: 200, loss is 4.605407638549805 and perplexity is 100.02374807558043
At time: 414.539443731308 and batch: 250, loss is 4.599937829971314 and perplexity is 99.47813089143156
At time: 415.7415978908539 and batch: 300, loss is 4.621723642349243 and perplexity is 101.66912239466892
At time: 416.9433870315552 and batch: 350, loss is 4.5785808277130124 and perplexity is 97.37610263420734
At time: 418.14986276626587 and batch: 400, loss is 4.586914892196655 and perplexity is 98.19103247479592
At time: 419.35292959213257 and batch: 450, loss is 4.55694881439209 and perplexity is 95.29228136824767
At time: 420.5548369884491 and batch: 500, loss is 4.541780672073364 and perplexity is 93.85778133605054
At time: 421.7586724758148 and batch: 550, loss is 4.542266712188721 and perplexity is 93.90341107096077
At time: 422.9621696472168 and batch: 600, loss is 4.5051939487457275 and perplexity is 90.48589197236932
At time: 424.1644332408905 and batch: 650, loss is 4.496268968582154 and perplexity is 89.68190032303427
At time: 425.37896633148193 and batch: 700, loss is 4.488178672790528 and perplexity is 88.95927429292327
At time: 426.5841143131256 and batch: 750, loss is 4.508154697418213 and perplexity is 90.75419494975121
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894767583802689 and perplexity of 133.58895476934038
Finished 21 epochs...
Completing Train Step...
At time: 430.0148673057556 and batch: 50, loss is 4.640804090499878 and perplexity is 103.62764009235865
At time: 431.25302720069885 and batch: 100, loss is 4.6356773853302 and perplexity is 103.09773123804409
At time: 432.4569890499115 and batch: 150, loss is 4.607352094650269 and perplexity is 100.21842907570718
At time: 433.66622948646545 and batch: 200, loss is 4.603181056976318 and perplexity is 99.80128479942822
At time: 434.87157011032104 and batch: 250, loss is 4.597987976074219 and perplexity is 99.28435205184441
At time: 436.0748288631439 and batch: 300, loss is 4.619954261779785 and perplexity is 101.48939007931396
At time: 437.2749207019806 and batch: 350, loss is 4.577470703125 and perplexity is 97.26806300820638
At time: 438.47659850120544 and batch: 400, loss is 4.58602819442749 and perplexity is 98.10400529445376
At time: 439.6797397136688 and batch: 450, loss is 4.556427326202392 and perplexity is 95.24260052405744
At time: 440.8825786113739 and batch: 500, loss is 4.541392669677735 and perplexity is 93.82137135608018
At time: 442.0843594074249 and batch: 550, loss is 4.541998453140259 and perplexity is 93.87822400973913
At time: 443.2893569469452 and batch: 600, loss is 4.505142002105713 and perplexity is 90.48119165639652
At time: 444.4916582107544 and batch: 650, loss is 4.496676473617554 and perplexity is 89.7184535963158
At time: 445.6950387954712 and batch: 700, loss is 4.4888063621521 and perplexity is 89.01513061138462
At time: 446.9009268283844 and batch: 750, loss is 4.5082871627807615 and perplexity is 90.76621753335839
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894633093545603 and perplexity of 133.57098956456923
Finished 22 epochs...
Completing Train Step...
At time: 450.35174036026 and batch: 50, loss is 4.6387081050872805 and perplexity is 103.4106655375676
At time: 451.5822217464447 and batch: 100, loss is 4.633637247085571 and perplexity is 102.88761202261375
At time: 452.803683757782 and batch: 150, loss is 4.605367040634155 and perplexity is 100.01968740232108
At time: 454.02320623397827 and batch: 200, loss is 4.601303510665893 and perplexity is 99.61407906409072
At time: 455.22433590888977 and batch: 250, loss is 4.596317348480224 and perplexity is 99.11862334767358
At time: 456.4377303123474 and batch: 300, loss is 4.618433618545533 and perplexity is 101.33517820527892
At time: 457.6392819881439 and batch: 350, loss is 4.576524162292481 and perplexity is 97.17603837427659
At time: 458.84257340431213 and batch: 400, loss is 4.585228652954101 and perplexity is 98.0255984224652
At time: 460.05103874206543 and batch: 450, loss is 4.555941915512085 and perplexity is 95.19637996647428
At time: 461.3033998012543 and batch: 500, loss is 4.5409885692596434 and perplexity is 93.78346576003872
At time: 462.5126917362213 and batch: 550, loss is 4.541602592468262 and perplexity is 93.84106866755133
At time: 463.71446228027344 and batch: 600, loss is 4.50499059677124 and perplexity is 90.4674933583341
At time: 464.91974329948425 and batch: 650, loss is 4.496850948333741 and perplexity is 89.73410856370201
At time: 466.1248161792755 and batch: 700, loss is 4.489179019927978 and perplexity is 89.04830897368119
At time: 467.3264949321747 and batch: 750, loss is 4.508227224349976 and perplexity is 90.76077731175187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894568509833757 and perplexity of 133.56236333282823
Finished 23 epochs...
Completing Train Step...
At time: 470.7603259086609 and batch: 50, loss is 4.636806688308716 and perplexity is 103.21422557933205
At time: 472.00094652175903 and batch: 100, loss is 4.631791477203369 and perplexity is 102.69788032148644
At time: 473.2190155982971 and batch: 150, loss is 4.603621711730957 and perplexity is 99.8452724010553
At time: 474.4244501590729 and batch: 200, loss is 4.59961727142334 and perplexity is 99.44624743676786
At time: 475.6298403739929 and batch: 250, loss is 4.594789838790893 and perplexity is 98.96733426730165
At time: 476.839013338089 and batch: 300, loss is 4.6170276927947995 and perplexity is 101.19280857280218
At time: 478.04710698127747 and batch: 350, loss is 4.575637245178223 and perplexity is 97.08988949185299
At time: 479.2590718269348 and batch: 400, loss is 4.584473199844361 and perplexity is 97.9515726443255
At time: 480.46337366104126 and batch: 450, loss is 4.555446443557739 and perplexity is 95.14922451311266
At time: 481.6724989414215 and batch: 500, loss is 4.540571098327637 and perplexity is 93.74432206042793
At time: 482.8780343532562 and batch: 550, loss is 4.541127157211304 and perplexity is 93.79646391913134
At time: 484.0844204425812 and batch: 600, loss is 4.504766902923584 and perplexity is 90.44725859993686
At time: 485.29872822761536 and batch: 650, loss is 4.496884660720825 and perplexity is 89.73713376569769
At time: 486.50966000556946 and batch: 700, loss is 4.489371814727783 and perplexity is 89.06547867964456
At time: 487.713760137558 and batch: 750, loss is 4.508059873580932 and perplexity is 90.74558969673475
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894562477289244 and perplexity of 133.56155761435645
Finished 24 epochs...
Completing Train Step...
At time: 491.17878818511963 and batch: 50, loss is 4.635074825286865 and perplexity is 103.03562737717316
At time: 492.4113211631775 and batch: 100, loss is 4.630128507614136 and perplexity is 102.52723879479315
At time: 493.60717129707336 and batch: 150, loss is 4.602054672241211 and perplexity is 99.68893344299124
At time: 494.80627512931824 and batch: 200, loss is 4.598071069717407 and perplexity is 99.29260229313455
At time: 496.0053668022156 and batch: 250, loss is 4.5933756923675535 and perplexity is 98.82747887682721
At time: 497.20934295654297 and batch: 300, loss is 4.615720882415771 and perplexity is 101.06065512882944
At time: 498.40980529785156 and batch: 350, loss is 4.57480788230896 and perplexity is 97.00940012458754
At time: 499.6094672679901 and batch: 400, loss is 4.583752126693725 and perplexity is 97.88096785389774
At time: 500.8051526546478 and batch: 450, loss is 4.554957599639892 and perplexity is 95.10272276039714
At time: 502.0049571990967 and batch: 500, loss is 4.540142831802368 and perplexity is 93.70418310105525
At time: 503.20334029197693 and batch: 550, loss is 4.540650100708008 and perplexity is 93.75172837757145
At time: 504.4058828353882 and batch: 600, loss is 4.504489231109619 and perplexity is 90.42214743206635
At time: 505.6076691150665 and batch: 650, loss is 4.496844139099121 and perplexity is 89.7334975451837
At time: 506.8087079524994 and batch: 700, loss is 4.4894678497314455 and perplexity is 89.07403249394196
At time: 508.0087640285492 and batch: 750, loss is 4.507825183868408 and perplexity is 90.7242951392805
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894572768100472 and perplexity of 133.56293207820534
Annealing...
Finished 25 epochs...
Completing Train Step...
At time: 511.4376564025879 and batch: 50, loss is 4.633875675201416 and perplexity is 102.91214624680033
At time: 512.6714596748352 and batch: 100, loss is 4.629417934417725 and perplexity is 102.4544115646067
At time: 513.8755326271057 and batch: 150, loss is 4.600764331817627 and perplexity is 99.56038373666303
At time: 515.0784642696381 and batch: 200, loss is 4.596298589706421 and perplexity is 99.11676402127787
At time: 516.2839643955231 and batch: 250, loss is 4.591204814910888 and perplexity is 98.61316923498448
At time: 517.4894535541534 and batch: 300, loss is 4.612864589691162 and perplexity is 100.77240816965656
At time: 518.6993782520294 and batch: 350, loss is 4.571070146560669 and perplexity is 96.6474814214128
At time: 519.9105741977692 and batch: 400, loss is 4.579328546524048 and perplexity is 97.44893980536023
At time: 521.1136803627014 and batch: 450, loss is 4.550241470336914 and perplexity is 94.65526199357457
At time: 522.3713166713715 and batch: 500, loss is 4.534715013504028 and perplexity is 93.19695164677672
At time: 523.5753471851349 and batch: 550, loss is 4.533576593399048 and perplexity is 93.09091473202898
At time: 524.7785089015961 and batch: 600, loss is 4.497098217010498 and perplexity is 89.75629974146504
At time: 525.9808962345123 and batch: 650, loss is 4.489150886535644 and perplexity is 89.04580377790815
At time: 527.1861083507538 and batch: 700, loss is 4.481584739685059 and perplexity is 88.37461251786242
At time: 528.3904728889465 and batch: 750, loss is 4.50040524482727 and perplexity is 90.05361766980394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8937938601471656 and perplexity of 133.4589393537804
Finished 26 epochs...
Completing Train Step...
At time: 531.9237263202667 and batch: 50, loss is 4.6323857021331785 and perplexity is 102.75892409728252
At time: 533.2023854255676 and batch: 100, loss is 4.628024206161499 and perplexity is 102.31171741776475
At time: 534.4170393943787 and batch: 150, loss is 4.599524993896484 and perplexity is 99.43707120638571
At time: 535.6254830360413 and batch: 200, loss is 4.595244092941284 and perplexity is 99.01230080198998
At time: 536.8412322998047 and batch: 250, loss is 4.590378274917603 and perplexity is 98.53169518216703
At time: 538.0509366989136 and batch: 300, loss is 4.612105083465576 and perplexity is 100.69589995619451
At time: 539.259601354599 and batch: 350, loss is 4.570547218322754 and perplexity is 96.59695493626847
At time: 540.4645030498505 and batch: 400, loss is 4.578778095245362 and perplexity is 97.39531367247852
At time: 541.6737389564514 and batch: 450, loss is 4.549932184219361 and perplexity is 94.6259909618818
At time: 542.8784582614899 and batch: 500, loss is 4.534526014328003 and perplexity is 93.17933916413222
At time: 544.0868384838104 and batch: 550, loss is 4.5335976505279545 and perplexity is 93.09287498005902
At time: 545.2911186218262 and batch: 600, loss is 4.497217350006103 and perplexity is 89.7669933152938
At time: 546.4940218925476 and batch: 650, loss is 4.489517517089844 and perplexity is 89.07845667570564
At time: 547.7022261619568 and batch: 700, loss is 4.4821309471130375 and perplexity is 88.42289657301878
At time: 548.9057013988495 and batch: 750, loss is 4.500616779327393 and perplexity is 90.07266913175165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8936583053234015 and perplexity of 133.44084957688253
Finished 27 epochs...
Completing Train Step...
At time: 552.3521370887756 and batch: 50, loss is 4.6314785861968994 and perplexity is 102.66575210492748
At time: 553.5847947597504 and batch: 100, loss is 4.6271233654022215 and perplexity is 102.21959235379947
At time: 554.7914969921112 and batch: 150, loss is 4.598641510009766 and perplexity is 99.34925895229996
At time: 555.9942502975464 and batch: 200, loss is 4.594527988433838 and perplexity is 98.94142302806866
At time: 557.2025966644287 and batch: 250, loss is 4.58980435371399 and perplexity is 98.47516197742827
At time: 558.4085683822632 and batch: 300, loss is 4.611597118377685 and perplexity is 100.6447629435311
At time: 559.6143317222595 and batch: 350, loss is 4.570137786865234 and perplexity is 96.55741319958477
At time: 560.8183414936066 and batch: 400, loss is 4.578357286453247 and perplexity is 97.35433749034794
At time: 562.0238163471222 and batch: 450, loss is 4.549706134796143 and perplexity is 94.60460322863688
At time: 563.2321057319641 and batch: 500, loss is 4.534413766860962 and perplexity is 93.16888060631477
At time: 564.4359307289124 and batch: 550, loss is 4.533585958480835 and perplexity is 93.09178654014129
At time: 565.6389644145966 and batch: 600, loss is 4.497263498306275 and perplexity is 89.77113600503512
At time: 566.8512268066406 and batch: 650, loss is 4.48971981048584 and perplexity is 89.0964784820014
At time: 568.0559475421906 and batch: 700, loss is 4.48248743057251 and perplexity is 88.45442349216442
At time: 569.2673234939575 and batch: 750, loss is 4.5006832504272465 and perplexity is 90.07865656012886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.893604367278343 and perplexity of 133.43365223243245
Finished 28 epochs...
Completing Train Step...
At time: 572.700350522995 and batch: 50, loss is 4.63072250366211 and perplexity is 102.58815766043851
At time: 573.9302890300751 and batch: 100, loss is 4.626375951766968 and perplexity is 102.14322058089165
At time: 575.1339182853699 and batch: 150, loss is 4.597929792404175 and perplexity is 99.27857549191647
At time: 576.3355989456177 and batch: 200, loss is 4.593957176208496 and perplexity is 98.88496217001857
At time: 577.5424399375916 and batch: 250, loss is 4.589349641799926 and perplexity is 98.4303943270012
At time: 578.749523639679 and batch: 300, loss is 4.611169767379761 and perplexity is 100.60176149266225
At time: 579.950366973877 and batch: 350, loss is 4.569790849685669 and perplexity is 96.52391965339734
At time: 581.1595370769501 and batch: 400, loss is 4.57797833442688 and perplexity is 97.31745185626461
At time: 582.3623156547546 and batch: 450, loss is 4.54949423789978 and perplexity is 94.58455893056838
At time: 583.6262159347534 and batch: 500, loss is 4.534312400817871 and perplexity is 93.15943692419093
At time: 584.8325910568237 and batch: 550, loss is 4.533578071594238 and perplexity is 93.0910523386731
At time: 586.043808221817 and batch: 600, loss is 4.497280082702637 and perplexity is 89.77262481748198
At time: 587.2470753192902 and batch: 650, loss is 4.489828147888184 and perplexity is 89.10613148591933
At time: 588.4515206813812 and batch: 700, loss is 4.482730550765991 and perplexity is 88.47593116308673
At time: 589.6522579193115 and batch: 750, loss is 4.500697174072266 and perplexity is 90.07991079209833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.893587689067042 and perplexity of 133.4314268163439
Finished 29 epochs...
Completing Train Step...
At time: 593.093549489975 and batch: 50, loss is 4.630033369064331 and perplexity is 102.51748496596868
At time: 594.3590035438538 and batch: 100, loss is 4.625693073272705 and perplexity is 102.07349298267053
At time: 595.569757938385 and batch: 150, loss is 4.597238273620605 and perplexity is 99.20994622410655
At time: 596.778733253479 and batch: 200, loss is 4.593394937515259 and perplexity is 98.82938084455735
At time: 597.9899055957794 and batch: 250, loss is 4.588948020935058 and perplexity is 98.39087056421744
At time: 599.1978552341461 and batch: 300, loss is 4.610700998306275 and perplexity is 100.55461354974838
At time: 600.4070315361023 and batch: 350, loss is 4.569446744918824 and perplexity is 96.49071102648112
At time: 601.6138722896576 and batch: 400, loss is 4.577621870040893 and perplexity is 97.28276783271974
At time: 602.8258969783783 and batch: 450, loss is 4.549256210327148 and perplexity is 94.56204787682972
At time: 604.0304284095764 and batch: 500, loss is 4.534183626174927 and perplexity is 93.14744112335816
At time: 605.2379817962646 and batch: 550, loss is 4.5334626579284665 and perplexity is 93.08030897904929
At time: 606.4465637207031 and batch: 600, loss is 4.497181606292725 and perplexity is 89.76378476695695
At time: 607.6633567810059 and batch: 650, loss is 4.489958543777465 and perplexity is 89.11775131674752
At time: 608.8863327503204 and batch: 700, loss is 4.483022212982178 and perplexity is 88.50174001279912
At time: 610.1262013912201 and batch: 750, loss is 4.50061803817749 and perplexity is 90.0727825198113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8935699462890625 and perplexity of 133.42905939316483
Finished 30 epochs...
Completing Train Step...
At time: 613.601556301117 and batch: 50, loss is 4.6293988704681395 and perplexity is 102.45245839748746
At time: 614.8678913116455 and batch: 100, loss is 4.625093669891357 and perplexity is 102.01232811887509
At time: 616.0779395103455 and batch: 150, loss is 4.596646814346314 and perplexity is 99.15128493090337
At time: 617.2851250171661 and batch: 200, loss is 4.592898483276367 and perplexity is 98.78032875657598
At time: 618.4894845485687 and batch: 250, loss is 4.588554782867432 and perplexity is 98.35218713480165
At time: 619.6940245628357 and batch: 300, loss is 4.6103445911407475 and perplexity is 100.51878155072234
At time: 620.9025337696075 and batch: 350, loss is 4.569184350967407 and perplexity is 96.46539576897037
At time: 622.1162941455841 and batch: 400, loss is 4.577416458129883 and perplexity is 97.26278684570745
At time: 623.3228259086609 and batch: 450, loss is 4.54912371635437 and perplexity is 94.5495198053978
At time: 624.5321617126465 and batch: 500, loss is 4.534044790267944 and perplexity is 93.13450981157267
At time: 625.737717628479 and batch: 550, loss is 4.533416776657105 and perplexity is 93.07603843410436
At time: 626.9419343471527 and batch: 600, loss is 4.497123508453369 and perplexity is 89.75856983649919
At time: 628.1464488506317 and batch: 650, loss is 4.489957427978515 and perplexity is 89.11765187930965
At time: 629.3601958751678 and batch: 700, loss is 4.483083438873291 and perplexity is 88.50715877657913
At time: 630.5703585147858 and batch: 750, loss is 4.500506391525269 and perplexity is 90.06272675654331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.893560720044513 and perplexity of 133.4278283497118
Finished 31 epochs...
Completing Train Step...
At time: 634.0382223129272 and batch: 50, loss is 4.628806142807007 and perplexity is 102.3917499849993
At time: 635.286447763443 and batch: 100, loss is 4.624569578170776 and perplexity is 101.95887830983543
At time: 636.484780550003 and batch: 150, loss is 4.596142101287842 and perplexity is 99.10125460917484
At time: 637.6899774074554 and batch: 200, loss is 4.592500743865966 and perplexity is 98.74104773917934
At time: 638.8869190216064 and batch: 250, loss is 4.588171682357788 and perplexity is 98.31451557824275
At time: 640.0848252773285 and batch: 300, loss is 4.610084114074707 and perplexity is 100.49260212314027
At time: 641.2855987548828 and batch: 350, loss is 4.568996105194092 and perplexity is 96.44723827503505
At time: 642.4869155883789 and batch: 400, loss is 4.57707221031189 and perplexity is 97.22931010604181
At time: 643.689483165741 and batch: 450, loss is 4.548933639526367 and perplexity is 94.53154984047501
At time: 644.9417006969452 and batch: 500, loss is 4.5339439010620115 and perplexity is 93.12511401880786
At time: 646.1416738033295 and batch: 550, loss is 4.533373756408691 and perplexity is 93.07203436593821
At time: 647.346563577652 and batch: 600, loss is 4.497128553390503 and perplexity is 89.75902266398347
At time: 648.5491585731506 and batch: 650, loss is 4.489987373352051 and perplexity is 89.1203205806412
At time: 649.7515115737915 and batch: 700, loss is 4.483203630447388 and perplexity is 88.5177972306248
At time: 650.9549188613892 and batch: 750, loss is 4.500419731140137 and perplexity is 90.05492222413334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.893587689067042 and perplexity of 133.4314268163439
Annealing...
Finished 32 epochs...
Completing Train Step...
At time: 654.3828048706055 and batch: 50, loss is 4.628428287506104 and perplexity is 102.35306802805017
At time: 655.6148126125336 and batch: 100, loss is 4.624310750961303 and perplexity is 101.9324919927772
At time: 656.8183286190033 and batch: 150, loss is 4.595684423446655 and perplexity is 99.05590853864382
At time: 658.0296339988708 and batch: 200, loss is 4.592081708908081 and perplexity is 98.69968045617253
At time: 659.2310225963593 and batch: 250, loss is 4.587781839370727 and perplexity is 98.2761958236474
At time: 660.4377181529999 and batch: 300, loss is 4.609489498138427 and perplexity is 100.43286538240878
At time: 661.6383237838745 and batch: 350, loss is 4.567595567703247 and perplexity is 96.31225484868125
At time: 662.8437027931213 and batch: 400, loss is 4.575552949905395 and perplexity is 97.08170561806524
At time: 664.0464012622833 and batch: 450, loss is 4.547344474792481 and perplexity is 94.38144293911645
At time: 665.2494807243347 and batch: 500, loss is 4.531947946548462 and perplexity is 92.93942590132116
At time: 666.4560468196869 and batch: 550, loss is 4.531148519515991 and perplexity is 92.86515730199166
At time: 667.6592919826508 and batch: 600, loss is 4.494677991867065 and perplexity is 89.5393319494093
At time: 668.8622825145721 and batch: 650, loss is 4.487371139526367 and perplexity is 88.887465717585
At time: 670.0661816596985 and batch: 700, loss is 4.480528650283813 and perplexity is 88.28133029209808
At time: 671.2723760604858 and batch: 750, loss is 4.497977895736694 and perplexity is 89.83529118734312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894300948741824 and perplexity of 133.52663202142125
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 674.6969463825226 and batch: 50, loss is 4.6281461429595945 and perplexity is 102.32419374164033
At time: 675.9464509487152 and batch: 100, loss is 4.623983783721924 and perplexity is 101.89916885535125
At time: 677.144451379776 and batch: 150, loss is 4.595424728393555 and perplexity is 99.03018754916741
At time: 678.3467454910278 and batch: 200, loss is 4.59185604095459 and perplexity is 98.67740961427593
At time: 679.5549473762512 and batch: 250, loss is 4.587656440734864 and perplexity is 98.2638728954086
At time: 680.7629859447479 and batch: 300, loss is 4.609246063232422 and perplexity is 100.40841949287666
At time: 681.9735164642334 and batch: 350, loss is 4.567159967422485 and perplexity is 96.2703103396116
At time: 683.177992105484 and batch: 400, loss is 4.575083808898926 and perplexity is 97.03617129082701
At time: 684.383421421051 and batch: 450, loss is 4.546896381378174 and perplexity is 94.33916071000434
At time: 685.5915200710297 and batch: 500, loss is 4.531242876052857 and perplexity is 92.87392015003965
At time: 686.7944023609161 and batch: 550, loss is 4.5303876590728756 and perplexity is 92.79452675066055
At time: 687.9931199550629 and batch: 600, loss is 4.493938093185425 and perplexity is 89.47310641885761
At time: 689.192334651947 and batch: 650, loss is 4.486570043563843 and perplexity is 88.81628684204115
At time: 690.3999164104462 and batch: 700, loss is 4.479723539352417 and perplexity is 88.2102826325032
At time: 691.6021149158478 and batch: 750, loss is 4.497268505096436 and perplexity is 89.77158547140081
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.894770777502726 and perplexity of 133.58938141307144
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 695.0223295688629 and batch: 50, loss is 4.628012914657592 and perplexity is 102.31056217113004
At time: 696.2564966678619 and batch: 100, loss is 4.623854932785034 and perplexity is 101.88603989783333
At time: 697.4608867168427 and batch: 150, loss is 4.595337800979614 and perplexity is 99.02157948520548
At time: 698.6659848690033 and batch: 200, loss is 4.591745557785035 and perplexity is 98.66650802353048
At time: 699.8758115768433 and batch: 250, loss is 4.587525291442871 and perplexity is 98.250986503089
At time: 701.0802989006042 and batch: 300, loss is 4.609150037765503 and perplexity is 100.398778190425
At time: 702.2906925678253 and batch: 350, loss is 4.567015829086304 and perplexity is 96.25643509725678
At time: 703.4940929412842 and batch: 400, loss is 4.574941883087158 and perplexity is 97.02240033069619
At time: 704.6980171203613 and batch: 450, loss is 4.5467621612548825 and perplexity is 94.32649934594657
At time: 705.9553008079529 and batch: 500, loss is 4.530996904373169 and perplexity is 92.85107860520293
At time: 707.1638760566711 and batch: 550, loss is 4.5301438999176025 and perplexity is 92.77190999183885
At time: 708.3774704933167 and batch: 600, loss is 4.493705558776855 and perplexity is 89.45230326179228
At time: 709.5851526260376 and batch: 650, loss is 4.486336507797241 and perplexity is 88.79554748419199
At time: 710.7936205863953 and batch: 700, loss is 4.479457502365112 and perplexity is 88.18681855595685
At time: 712.002347946167 and batch: 750, loss is 4.49704158782959 and perplexity is 89.75121705964476
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8948584268259445 and perplexity of 133.6010909451003
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 715.4550669193268 and batch: 50, loss is 4.627974433898926 and perplexity is 102.30662525882627
At time: 716.7080745697021 and batch: 100, loss is 4.623815059661865 and perplexity is 101.88197746420683
At time: 717.9101021289825 and batch: 150, loss is 4.595305919647217 and perplexity is 99.0184225956386
At time: 719.1105086803436 and batch: 200, loss is 4.591708106994629 and perplexity is 98.66281295401049
At time: 720.314537525177 and batch: 250, loss is 4.587470779418945 and perplexity is 98.24563078893873
At time: 721.5146572589874 and batch: 300, loss is 4.609117660522461 and perplexity is 100.39552760740514
At time: 722.7216758728027 and batch: 350, loss is 4.566965970993042 and perplexity is 96.25163605457519
At time: 723.9272637367249 and batch: 400, loss is 4.574896612167358 and perplexity is 97.01800813681213
At time: 725.1326773166656 and batch: 450, loss is 4.546719312667847 and perplexity is 94.32245767532012
At time: 726.335923910141 and batch: 500, loss is 4.530921087265015 and perplexity is 92.84403917179219
At time: 727.5412018299103 and batch: 550, loss is 4.530069398880005 and perplexity is 92.76499864573896
At time: 728.7474286556244 and batch: 600, loss is 4.493634901046753 and perplexity is 89.44598298838208
At time: 729.9484283924103 and batch: 650, loss is 4.486264925003052 and perplexity is 88.78919147828456
At time: 731.1551542282104 and batch: 700, loss is 4.479372749328613 and perplexity is 88.17934477202245
At time: 732.3578386306763 and batch: 750, loss is 4.496971101760864 and perplexity is 89.74489107214049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.8948822021484375 and perplexity of 133.6042673918833
Annealing...
Model not improving. Stopping early with 133.4278283497118loss at 35 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -133.4278283497118
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f819b44d860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 4.728900084303893, 'num_layers': 1, 'lr': 20.865679133274845, 'dropout': 0.19327484176991094}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7641849517822266 and batch: 50, loss is 6.953674449920654 and perplexity is 1046.9897803353495
At time: 2.9371273517608643 and batch: 100, loss is 5.997577438354492 and perplexity is 402.4526452379891
At time: 4.133670330047607 and batch: 150, loss is 5.897010717391968 and perplexity is 363.94789704026743
At time: 5.2997260093688965 and batch: 200, loss is 5.852923593521118 and perplexity is 348.25103808646116
At time: 6.4674928188323975 and batch: 250, loss is 5.858261680603027 and perplexity is 350.1150030315642
At time: 7.6352550983428955 and batch: 300, loss is 5.853533620834351 and perplexity is 348.46354554264167
At time: 8.80277705192566 and batch: 350, loss is 5.768067388534546 and perplexity is 319.9188559668405
At time: 9.980631351470947 and batch: 400, loss is 5.788645658493042 and perplexity is 326.5704368387299
At time: 11.148910284042358 and batch: 450, loss is 5.749310989379882 and perplexity is 313.9742541456413
At time: 12.321641683578491 and batch: 500, loss is 5.757231483459472 and perplexity is 316.47095988586756
At time: 13.496243238449097 and batch: 550, loss is 5.756437177658081 and perplexity is 316.2196849742296
At time: 14.66673469543457 and batch: 600, loss is 5.722684602737427 and perplexity is 305.7245712705136
At time: 15.843569278717041 and batch: 650, loss is 5.719140119552613 and perplexity is 304.64285386553723
At time: 17.01816964149475 and batch: 700, loss is 5.714042100906372 and perplexity is 303.0937309998281
At time: 18.196281671524048 and batch: 750, loss is 5.6959929847717286 and perplexity is 297.67223087169174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.319288741710574 and perplexity of 204.2385639481337
Finished 1 epochs...
Completing Train Step...
At time: 21.57720446586609 and batch: 50, loss is 5.622079572677612 and perplexity is 276.46371223066495
At time: 22.747477293014526 and batch: 100, loss is 5.621459531784057 and perplexity is 276.29234655585117
At time: 23.913190603256226 and batch: 150, loss is 5.6242578125 and perplexity is 277.0665728472245
At time: 25.07986283302307 and batch: 200, loss is 5.604480142593384 and perplexity is 271.6406743057915
At time: 26.24905514717102 and batch: 250, loss is 5.593470020294189 and perplexity is 268.6662815213663
At time: 27.416414976119995 and batch: 300, loss is 5.606145296096802 and perplexity is 272.09337452936273
At time: 28.586283922195435 and batch: 350, loss is 5.535863447189331 and perplexity is 253.62668636922652
At time: 29.755417346954346 and batch: 400, loss is 5.554395408630371 and perplexity is 258.3707085892617
At time: 30.924784898757935 and batch: 450, loss is 5.525906829833985 and perplexity is 251.1139524252692
At time: 32.091739654541016 and batch: 500, loss is 5.487412786483764 and perplexity is 241.63124583454075
At time: 33.26292443275452 and batch: 550, loss is 5.50535662651062 and perplexity is 246.00617235968502
At time: 34.43260145187378 and batch: 600, loss is 5.524297924041748 and perplexity is 250.71025857247247
At time: 35.603638887405396 and batch: 650, loss is 5.532022399902344 and perplexity is 252.65436283941125
At time: 36.77290463447571 and batch: 700, loss is 5.533635759353638 and perplexity is 253.06231414116414
At time: 37.99653935432434 and batch: 750, loss is 5.506873636245728 and perplexity is 246.3796493305274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.322866395462391 and perplexity of 204.97056745948325
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 41.333575963974 and batch: 50, loss is 5.457443876266479 and perplexity is 234.49725364122628
At time: 42.525747299194336 and batch: 100, loss is 5.4127379322052 and perplexity is 224.24471477374985
At time: 43.69298505783081 and batch: 150, loss is 5.392971105575562 and perplexity is 219.85563041932815
At time: 44.855841875076294 and batch: 200, loss is 5.398365411758423 and perplexity is 221.04480350409924
At time: 46.01975989341736 and batch: 250, loss is 5.406559648513794 and perplexity is 222.86353835288335
At time: 47.18006467819214 and batch: 300, loss is 5.421898946762085 and perplexity is 226.30846244816485
At time: 48.33893799781799 and batch: 350, loss is 5.371967420578003 and perplexity is 215.2860094440994
At time: 49.50046348571777 and batch: 400, loss is 5.35170503616333 and perplexity is 210.96769893967846
At time: 50.66305327415466 and batch: 450, loss is 5.353690929412842 and perplexity is 211.38707454863646
At time: 51.82797312736511 and batch: 500, loss is 5.304433145523071 and perplexity is 201.2269037068253
At time: 52.98883938789368 and batch: 550, loss is 5.256204309463501 and perplexity is 191.7522759357008
At time: 54.151344299316406 and batch: 600, loss is 5.236469869613647 and perplexity is 188.00524654854206
At time: 55.31574320793152 and batch: 650, loss is 5.201451253890991 and perplexity is 181.5355049060445
At time: 56.475858211517334 and batch: 700, loss is 5.200819463729858 and perplexity is 181.4208487832675
At time: 57.63662528991699 and batch: 750, loss is 5.199469795227051 and perplexity is 181.1761559421669
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.129144003224927 and perplexity of 168.87250184124102
Finished 3 epochs...
Completing Train Step...
At time: 61.02823328971863 and batch: 50, loss is 5.29767032623291 and perplexity is 199.8706337940634
At time: 62.24363970756531 and batch: 100, loss is 5.2816778182983395 and perplexity is 196.69962487444195
At time: 63.41436147689819 and batch: 150, loss is 5.2730115795135495 and perplexity is 195.00234409930064
At time: 64.63352632522583 and batch: 200, loss is 5.285539712905884 and perplexity is 197.46072679681822
At time: 65.80364990234375 and batch: 250, loss is 5.296406192779541 and perplexity is 199.6181302722385
At time: 66.98045706748962 and batch: 300, loss is 5.309174976348877 and perplexity is 202.18335351124279
At time: 68.14603638648987 and batch: 350, loss is 5.258388547897339 and perplexity is 192.17156637502237
At time: 69.31387543678284 and batch: 400, loss is 5.251658945083618 and perplexity is 190.88266980645918
At time: 70.48318099975586 and batch: 450, loss is 5.251010417938232 and perplexity is 190.75891734625822
At time: 71.65555596351624 and batch: 500, loss is 5.212477283477783 and perplexity is 183.54819636119996
At time: 72.82291603088379 and batch: 550, loss is 5.184333257675171 and perplexity is 178.45442699593642
At time: 74.01349806785583 and batch: 600, loss is 5.166572780609131 and perplexity is 175.31297069578082
At time: 75.21700859069824 and batch: 650, loss is 5.147001419067383 and perplexity is 171.91521497135116
At time: 76.40612864494324 and batch: 700, loss is 5.145107297897339 and perplexity is 171.58989491828348
At time: 77.57449793815613 and batch: 750, loss is 5.147416086196899 and perplexity is 171.98651734241656
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.113248958144077 and perplexity of 166.20948625583253
Finished 4 epochs...
Completing Train Step...
At time: 80.94677138328552 and batch: 50, loss is 5.23040189743042 and perplexity is 186.86789015547606
At time: 82.16637516021729 and batch: 100, loss is 5.2184772872924805 and perplexity is 184.65279672895474
At time: 83.33975410461426 and batch: 150, loss is 5.210582475662232 and perplexity is 183.20073709243022
At time: 84.50483703613281 and batch: 200, loss is 5.225877275466919 and perplexity is 186.02429351254466
At time: 85.67380738258362 and batch: 250, loss is 5.24613169670105 and perplexity is 189.83052429193512
At time: 86.83800339698792 and batch: 300, loss is 5.264842176437378 and perplexity is 193.41578081087326
At time: 88.0042495727539 and batch: 350, loss is 5.203133926391602 and perplexity is 181.84122685085427
At time: 89.16789412498474 and batch: 400, loss is 5.202514905929565 and perplexity is 181.72869824294042
At time: 90.33694219589233 and batch: 450, loss is 5.197945880889892 and perplexity is 180.90026926779592
At time: 91.5066785812378 and batch: 500, loss is 5.15122088432312 and perplexity is 172.64213778212317
At time: 92.67881894111633 and batch: 550, loss is 5.129927406311035 and perplexity is 169.0048489141262
At time: 93.85173034667969 and batch: 600, loss is 5.101369304656982 and perplexity is 164.24665710177274
At time: 95.07110905647278 and batch: 650, loss is 5.08528564453125 and perplexity is 161.62610025611033
At time: 96.23748183250427 and batch: 700, loss is 5.079868650436401 and perplexity is 160.75293971335623
At time: 97.40943217277527 and batch: 750, loss is 5.089893503189087 and perplexity is 162.37256897190122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.09945288369822 and perplexity of 163.9321927849436
Finished 5 epochs...
Completing Train Step...
At time: 100.78963112831116 and batch: 50, loss is 5.1717518711090085 and perplexity is 176.223287709131
At time: 101.98428320884705 and batch: 100, loss is 5.16335711479187 and perplexity is 174.75012821026883
At time: 103.15078806877136 and batch: 150, loss is 5.1532949924469 and perplexity is 173.0005878461966
At time: 104.32003855705261 and batch: 200, loss is 5.165324783325195 and perplexity is 175.09431705246965
At time: 105.48917055130005 and batch: 250, loss is 5.173493137359619 and perplexity is 176.5304066829437
At time: 106.66172504425049 and batch: 300, loss is 5.203305835723877 and perplexity is 181.87248974185667
At time: 107.83352661132812 and batch: 350, loss is 5.141565933227539 and perplexity is 170.98330723476874
At time: 109.00412964820862 and batch: 400, loss is 5.155222063064575 and perplexity is 173.33429362991012
At time: 110.17269968986511 and batch: 450, loss is 5.155146627426148 and perplexity is 173.32121853997887
At time: 111.34434771537781 and batch: 500, loss is 5.128322668075562 and perplexity is 168.7338578640974
At time: 112.51349449157715 and batch: 550, loss is 5.119287567138672 and perplexity is 167.21619686125928
At time: 113.68609809875488 and batch: 600, loss is 5.083580770492554 and perplexity is 161.3507828712252
At time: 114.86156725883484 and batch: 650, loss is 5.073760604858398 and perplexity is 159.77404603777887
At time: 116.02869629859924 and batch: 700, loss is 5.057891569137573 and perplexity is 157.25859765187187
At time: 117.20568466186523 and batch: 750, loss is 5.060919876098633 and perplexity is 157.73554676749686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.084366909293241 and perplexity of 161.47767685376795
Finished 6 epochs...
Completing Train Step...
At time: 120.6289529800415 and batch: 50, loss is 5.125701913833618 and perplexity is 168.29222684629525
At time: 121.85264205932617 and batch: 100, loss is 5.120969924926758 and perplexity is 167.49775110342142
At time: 123.01809024810791 and batch: 150, loss is 5.126096363067627 and perplexity is 168.35862268029652
At time: 124.22273373603821 and batch: 200, loss is 5.141333904266357 and perplexity is 170.94363875790742
At time: 125.39061284065247 and batch: 250, loss is 5.142932271957397 and perplexity is 171.21708802510688
At time: 126.56105208396912 and batch: 300, loss is 5.1729704284667966 and perplexity is 176.43815678153922
At time: 127.7298994064331 and batch: 350, loss is 5.109792575836182 and perplexity is 165.63599440261976
At time: 128.89237451553345 and batch: 400, loss is 5.117102556228637 and perplexity is 166.8512265240729
At time: 130.05957126617432 and batch: 450, loss is 5.107444705963135 and perplexity is 165.2475588180104
At time: 131.22751665115356 and batch: 500, loss is 5.074888105392456 and perplexity is 159.95429295516786
At time: 132.39470863342285 and batch: 550, loss is 5.075525331497192 and perplexity is 160.05625248839146
At time: 133.5681984424591 and batch: 600, loss is 5.040739755630494 and perplexity is 154.58432736563415
At time: 134.73736333847046 and batch: 650, loss is 5.035785245895386 and perplexity is 153.82033198490788
At time: 135.9190149307251 and batch: 700, loss is 5.02918625831604 and perplexity is 152.80861534884832
At time: 137.0943992137909 and batch: 750, loss is 5.041183433532715 and perplexity is 154.65292823293512
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.090875315111737 and perplexity of 162.5320665815442
Annealing...
Finished 7 epochs...
Completing Train Step...
At time: 140.49490189552307 and batch: 50, loss is 5.092796211242676 and perplexity is 162.844573850304
At time: 141.69599509239197 and batch: 100, loss is 5.0819553565979 and perplexity is 161.08873409338915
At time: 142.8665270805359 and batch: 150, loss is 5.060239219665528 and perplexity is 157.628219583473
At time: 144.03759717941284 and batch: 200, loss is 5.070866422653198 and perplexity is 159.31229934871928
At time: 145.21143126487732 and batch: 250, loss is 5.072080917358399 and perplexity is 159.50590083294355
At time: 146.39251899719238 and batch: 300, loss is 5.091465482711792 and perplexity is 162.62801605155985
At time: 147.57626080513 and batch: 350, loss is 5.013764715194702 and perplexity is 150.4701484272797
At time: 148.7492072582245 and batch: 400, loss is 5.013627510070801 and perplexity is 150.4495045681754
At time: 149.92657160758972 and batch: 450, loss is 4.996616907119751 and perplexity is 147.91191196157607
At time: 151.09377717971802 and batch: 500, loss is 4.952792663574218 and perplexity is 141.56976912327548
At time: 152.26740503311157 and batch: 550, loss is 4.94085391998291 and perplexity is 139.88965314241554
At time: 153.4414701461792 and batch: 600, loss is 4.896647891998291 and perplexity is 133.84037947972962
At time: 154.67192840576172 and batch: 650, loss is 4.878274965286255 and perplexity is 131.40379215829597
At time: 155.84327507019043 and batch: 700, loss is 4.865105218887329 and perplexity is 129.6845831615083
At time: 157.014098405838 and batch: 750, loss is 4.89527928352356 and perplexity is 133.6573296924299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0126190185546875 and perplexity of 150.29785400122307
Finished 8 epochs...
Completing Train Step...
At time: 160.42843770980835 and batch: 50, loss is 5.0322479724884035 and perplexity is 153.27718860477296
At time: 161.65256547927856 and batch: 100, loss is 5.0239588642120365 and perplexity is 152.01190865807163
At time: 162.8235056400299 and batch: 150, loss is 5.009176759719849 and perplexity is 149.78137931518745
At time: 164.00351119041443 and batch: 200, loss is 5.025131731033325 and perplexity is 152.19030297811685
At time: 165.179340839386 and batch: 250, loss is 5.028469181060791 and perplexity is 152.69907904406557
At time: 166.35080122947693 and batch: 300, loss is 5.052421998977661 and perplexity is 156.40080872560378
At time: 167.5245006084442 and batch: 350, loss is 4.981125326156616 and perplexity is 145.63817992635708
At time: 168.70138549804688 and batch: 400, loss is 4.985044803619385 and perplexity is 146.2101256222294
At time: 169.874183177948 and batch: 450, loss is 4.973052158355713 and perplexity is 144.46715177521637
At time: 171.0516655445099 and batch: 500, loss is 4.9328631401062015 and perplexity is 138.77628000127555
At time: 172.22344160079956 and batch: 550, loss is 4.928354425430298 and perplexity is 138.1519857907814
At time: 173.39332556724548 and batch: 600, loss is 4.890322685241699 and perplexity is 132.9964831301512
At time: 174.57418751716614 and batch: 650, loss is 4.88087760925293 and perplexity is 131.746234880095
At time: 175.74513721466064 and batch: 700, loss is 4.873766689300537 and perplexity is 130.8127209548372
At time: 176.91623067855835 and batch: 750, loss is 4.899246234893798 and perplexity is 134.18859487446997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.008233003838118 and perplexity of 149.64008893978337
Finished 9 epochs...
Completing Train Step...
At time: 180.3137035369873 and batch: 50, loss is 5.01191517829895 and perplexity is 150.1921055406034
At time: 181.51336145401 and batch: 100, loss is 5.001442098617554 and perplexity is 148.62733991195142
At time: 182.68388843536377 and batch: 150, loss is 4.987125425338745 and perplexity is 146.51465027563657
At time: 183.88889598846436 and batch: 200, loss is 5.005541353225708 and perplexity is 149.23785168667203
At time: 185.0506112575531 and batch: 250, loss is 5.010554685592651 and perplexity is 149.987909211776
At time: 186.21746039390564 and batch: 300, loss is 5.035264844894409 and perplexity is 153.74030455515577
At time: 187.38374066352844 and batch: 350, loss is 4.966434755325317 and perplexity is 143.5143105515427
At time: 188.54788279533386 and batch: 400, loss is 4.9725329875946045 and perplexity is 144.39216812041892
At time: 189.72089624404907 and batch: 450, loss is 4.962571783065796 and perplexity is 142.96098817361778
At time: 190.88622212409973 and batch: 500, loss is 4.923816318511963 and perplexity is 137.52645773840524
At time: 192.0566794872284 and batch: 550, loss is 4.922364377975464 and perplexity is 137.32692239135707
At time: 193.22360014915466 and batch: 600, loss is 4.8867959308624265 and perplexity is 132.52826333426145
At time: 194.38658046722412 and batch: 650, loss is 4.880390138626098 and perplexity is 131.68202811112369
At time: 195.55176258087158 and batch: 700, loss is 4.8743648433685305 and perplexity is 130.89099052231802
At time: 196.71544313430786 and batch: 750, loss is 4.8964013671875 and perplexity is 133.80738857220788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.006232683048692 and perplexity of 149.34105993562073
Finished 10 epochs...
Completing Train Step...
At time: 200.12349462509155 and batch: 50, loss is 4.997643594741821 and perplexity is 148.06384927347617
At time: 201.34428477287292 and batch: 100, loss is 4.986760349273681 and perplexity is 146.461171046227
At time: 202.51515460014343 and batch: 150, loss is 4.971815404891967 and perplexity is 144.28859196485274
At time: 203.68333292007446 and batch: 200, loss is 4.991984901428222 and perplexity is 147.22836745669096
At time: 204.853440284729 and batch: 250, loss is 4.997868146896362 and perplexity is 148.09710106308094
At time: 206.02684426307678 and batch: 300, loss is 5.023165979385376 and perplexity is 151.89142849198456
At time: 207.20644998550415 and batch: 350, loss is 4.9562319564819335 and perplexity is 142.05750728258704
At time: 208.38468670845032 and batch: 400, loss is 4.964092779159546 and perplexity is 143.17859672711654
At time: 209.56326127052307 and batch: 450, loss is 4.954568538665772 and perplexity is 141.82140271876358
At time: 210.7314808368683 and batch: 500, loss is 4.917289724349976 and perplexity is 136.63180106847332
At time: 211.90402126312256 and batch: 550, loss is 4.917065324783326 and perplexity is 136.6011443913241
At time: 213.07950735092163 and batch: 600, loss is 4.882622623443604 and perplexity is 131.976334634808
At time: 214.3090090751648 and batch: 650, loss is 4.87768702507019 and perplexity is 131.32655729130977
At time: 215.48014330863953 and batch: 700, loss is 4.8715082454681395 and perplexity is 130.51762113061793
At time: 216.64804100990295 and batch: 750, loss is 4.891221141815185 and perplexity is 133.1160283897405
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0048675537109375 and perplexity of 149.13732916443664
Finished 11 epochs...
Completing Train Step...
At time: 220.03975224494934 and batch: 50, loss is 4.985526971817016 and perplexity is 146.28064049362428
At time: 221.26050233840942 and batch: 100, loss is 4.974115161895752 and perplexity is 144.6208025201473
At time: 222.42845129966736 and batch: 150, loss is 4.959908685684204 and perplexity is 142.58077563655715
At time: 223.6213357448578 and batch: 200, loss is 4.9803657054901125 and perplexity is 145.52759216274606
At time: 224.8112998008728 and batch: 250, loss is 4.9868246364593505 and perplexity is 146.4705869253803
At time: 226.00978255271912 and batch: 300, loss is 5.013084545135498 and perplexity is 150.36783793572636
At time: 227.1805648803711 and batch: 350, loss is 4.9468729782104495 and perplexity is 140.7341962373934
At time: 228.38097071647644 and batch: 400, loss is 4.955694990158081 and perplexity is 141.98124766138247
At time: 229.55951738357544 and batch: 450, loss is 4.946560201644897 and perplexity is 140.69018476207634
At time: 230.73046112060547 and batch: 500, loss is 4.9101957511901855 and perplexity is 135.66596858419268
At time: 231.89849400520325 and batch: 550, loss is 4.911383991241455 and perplexity is 135.82726813397292
At time: 233.06870937347412 and batch: 600, loss is 4.877550945281983 and perplexity is 131.30868761708587
At time: 234.23528265953064 and batch: 650, loss is 4.874074668884277 and perplexity is 130.8530148067081
At time: 235.40745329856873 and batch: 700, loss is 4.868108682632446 and perplexity is 130.07467162036394
At time: 236.58444452285767 and batch: 750, loss is 4.886327209472657 and perplexity is 132.46615905842592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.003540393918059 and perplexity of 148.93953138121748
Finished 12 epochs...
Completing Train Step...
At time: 239.9734935760498 and batch: 50, loss is 4.974758701324463 and perplexity is 144.71390166205205
At time: 241.19955778121948 and batch: 100, loss is 4.963330574035645 and perplexity is 143.06950684677247
At time: 242.37077236175537 and batch: 150, loss is 4.948842334747314 and perplexity is 141.01162513525648
At time: 243.57251739501953 and batch: 200, loss is 4.970173921585083 and perplexity is 144.0519389339063
At time: 244.74581122398376 and batch: 250, loss is 4.976990604400635 and perplexity is 145.03724977092895
At time: 245.9203760623932 and batch: 300, loss is 5.003386173248291 and perplexity is 148.9165635980319
At time: 247.09491205215454 and batch: 350, loss is 4.938354291915894 and perplexity is 139.5404177003314
At time: 248.26621437072754 and batch: 400, loss is 4.948157615661621 and perplexity is 140.9151048326395
At time: 249.43453764915466 and batch: 450, loss is 4.939160299301148 and perplexity is 139.6529336457895
At time: 250.61208200454712 and batch: 500, loss is 4.903455801010132 and perplexity is 134.75466124765185
At time: 251.77992129325867 and batch: 550, loss is 4.906304044723511 and perplexity is 135.1390224817894
At time: 252.9527153968811 and batch: 600, loss is 4.872674713134765 and perplexity is 130.66995454431986
At time: 254.1236217021942 and batch: 650, loss is 4.86950945854187 and perplexity is 130.2570047609173
At time: 255.3016860485077 and batch: 700, loss is 4.863990316390991 and perplexity is 129.54007806551945
At time: 256.4755787849426 and batch: 750, loss is 4.880609092712402 and perplexity is 131.7108635859787
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.002661061841388 and perplexity of 148.80862163879263
Finished 13 epochs...
Completing Train Step...
At time: 259.85843086242676 and batch: 50, loss is 4.9652303695678714 and perplexity is 143.34156800513696
At time: 261.0586338043213 and batch: 100, loss is 4.953133716583252 and perplexity is 141.61806015345562
At time: 262.2333195209503 and batch: 150, loss is 4.939097204208374 and perplexity is 139.64412250895768
At time: 263.41211676597595 and batch: 200, loss is 4.96111421585083 and perplexity is 142.75276471094733
At time: 264.59020042419434 and batch: 250, loss is 4.967964782714843 and perplexity is 143.73405944551976
At time: 265.77169609069824 and batch: 300, loss is 4.9946170425415035 and perplexity is 147.61640375483023
At time: 266.9478099346161 and batch: 350, loss is 4.930560846328735 and perplexity is 138.45714374903912
At time: 268.11109924316406 and batch: 400, loss is 4.941250848770141 and perplexity is 139.9451903942214
At time: 269.2833058834076 and batch: 450, loss is 4.932526712417602 and perplexity is 138.72959967088045
At time: 270.4533851146698 and batch: 500, loss is 4.896762027740478 and perplexity is 133.85565632257646
At time: 271.62817001342773 and batch: 550, loss is 4.900836009979248 and perplexity is 134.4020942223586
At time: 272.80148220062256 and batch: 600, loss is 4.8674284362792966 and perplexity is 129.98621888759237
At time: 274.0598213672638 and batch: 650, loss is 4.864991836547851 and perplexity is 129.66988005362734
At time: 275.27243185043335 and batch: 700, loss is 4.85983452796936 and perplexity is 129.00285397696175
At time: 276.4455773830414 and batch: 750, loss is 4.874998626708984 and perplexity is 130.97397334531416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.002333175304324 and perplexity of 148.75983729346035
Finished 14 epochs...
Completing Train Step...
At time: 279.8559820652008 and batch: 50, loss is 4.956103076934815 and perplexity is 142.03920015511613
At time: 281.0690920352936 and batch: 100, loss is 4.944135837554931 and perplexity is 140.3495136525897
At time: 282.2406334877014 and batch: 150, loss is 4.930034170150757 and perplexity is 138.38424086954575
At time: 283.4095220565796 and batch: 200, loss is 4.952877340316772 and perplexity is 141.58175729772154
At time: 284.5858988761902 and batch: 250, loss is 4.959653444290161 and perplexity is 142.5443877646631
At time: 285.7571802139282 and batch: 300, loss is 4.986621017456055 and perplexity is 146.44076576663878
At time: 286.9309182167053 and batch: 350, loss is 4.923107080459594 and perplexity is 137.428953322378
At time: 288.10113167762756 and batch: 400, loss is 4.934631443023681 and perplexity is 139.02189559946703
At time: 289.27120876312256 and batch: 450, loss is 4.92607048034668 and perplexity is 137.83681429624042
At time: 290.44641304016113 and batch: 500, loss is 4.890692977905274 and perplexity is 133.04573987127762
At time: 291.6206729412079 and batch: 550, loss is 4.895411443710327 and perplexity is 133.67499503738685
At time: 292.79387307167053 and batch: 600, loss is 4.862121839523315 and perplexity is 129.29826141098235
At time: 293.96234798431396 and batch: 650, loss is 4.8603018951416015 and perplexity is 129.0631597674117
At time: 295.13334012031555 and batch: 700, loss is 4.855534896850586 and perplexity is 128.44938001119965
At time: 296.30541491508484 and batch: 750, loss is 4.869892444610596 and perplexity is 130.30690093325418
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.001739501953125 and perplexity of 148.6715487522007
Finished 15 epochs...
Completing Train Step...
At time: 299.74589109420776 and batch: 50, loss is 4.948022537231445 and perplexity is 140.89607152701404
At time: 300.95205664634705 and batch: 100, loss is 4.936241998672485 and perplexity is 139.24597849928503
At time: 302.1296875476837 and batch: 150, loss is 4.921875486373901 and perplexity is 137.25980082125432
At time: 303.33155632019043 and batch: 200, loss is 4.945154495239258 and perplexity is 140.4925546056774
At time: 304.5057771205902 and batch: 250, loss is 4.95229287147522 and perplexity is 141.49903134977325
At time: 305.68402767181396 and batch: 300, loss is 4.979296636581421 and perplexity is 145.3720962713294
At time: 306.8611102104187 and batch: 350, loss is 4.916027202606201 and perplexity is 136.45940929581278
At time: 308.037926197052 and batch: 400, loss is 4.928513479232788 and perplexity is 138.1739611370279
At time: 309.21055269241333 and batch: 450, loss is 4.919615669250488 and perplexity is 136.9499689864984
At time: 310.38296914100647 and batch: 500, loss is 4.8849403667449955 and perplexity is 132.28257665845686
At time: 311.55921268463135 and batch: 550, loss is 4.890058908462525 and perplexity is 132.96140637261
At time: 312.7467157840729 and batch: 600, loss is 4.856947317123413 and perplexity is 128.63093270372914
At time: 313.943909406662 and batch: 650, loss is 4.8556340885162355 and perplexity is 128.46212175108133
At time: 315.15636682510376 and batch: 700, loss is 4.850864419937134 and perplexity is 127.85085892474727
At time: 316.35613894462585 and batch: 750, loss is 4.864456005096436 and perplexity is 129.60041746537505
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.001375775004542 and perplexity of 148.6174827366614
Finished 16 epochs...
Completing Train Step...
At time: 319.75153136253357 and batch: 50, loss is 4.940170831680298 and perplexity is 139.79412878619706
At time: 320.94403100013733 and batch: 100, loss is 4.928308839797974 and perplexity is 138.1456881886935
At time: 322.11073565483093 and batch: 150, loss is 4.913834753036499 and perplexity is 136.1605566520069
At time: 323.27578926086426 and batch: 200, loss is 4.937779512405395 and perplexity is 139.46023577302284
At time: 324.4404146671295 and batch: 250, loss is 4.945163316726685 and perplexity is 140.49379396444792
At time: 325.60579800605774 and batch: 300, loss is 4.972579021453857 and perplexity is 144.39881520215758
At time: 326.7724356651306 and batch: 350, loss is 4.909490060806275 and perplexity is 135.5702641875468
At time: 327.9368932247162 and batch: 400, loss is 4.922316913604736 and perplexity is 137.32040441008894
At time: 329.108402967453 and batch: 450, loss is 4.913396244049072 and perplexity is 136.1008621134343
At time: 330.28098702430725 and batch: 500, loss is 4.879532794952393 and perplexity is 131.56917973921313
At time: 331.44671511650085 and batch: 550, loss is 4.8849881076812744 and perplexity is 132.28889210327134
At time: 332.61349844932556 and batch: 600, loss is 4.851540822982788 and perplexity is 127.93736688894043
At time: 333.8401777744293 and batch: 650, loss is 4.850804347991943 and perplexity is 127.84317890563652
At time: 335.007107257843 and batch: 700, loss is 4.846322631835937 and perplexity is 127.27150406436364
At time: 336.1716923713684 and batch: 750, loss is 4.858767900466919 and perplexity is 128.865329341834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.001306578170421 and perplexity of 148.60719923315813
Finished 17 epochs...
Completing Train Step...
At time: 339.56162190437317 and batch: 50, loss is 4.9325879859924315 and perplexity is 138.73810038981895
At time: 340.7911911010742 and batch: 100, loss is 4.921122970581055 and perplexity is 137.156549507408
At time: 341.9594142436981 and batch: 150, loss is 4.9067658519744874 and perplexity is 135.20144507475507
At time: 343.1284804344177 and batch: 200, loss is 4.9317663955688475 and perplexity is 138.6241613071912
At time: 344.29799580574036 and batch: 250, loss is 4.937257490158081 and perplexity is 139.3874534259886
At time: 345.4687988758087 and batch: 300, loss is 4.9653394985198975 and perplexity is 143.35721157380274
At time: 346.64159297943115 and batch: 350, loss is 4.903074035644531 and perplexity is 134.7032264037799
At time: 347.81464099884033 and batch: 400, loss is 4.916571941375732 and perplexity is 136.53376427670528
At time: 348.9938254356384 and batch: 450, loss is 4.906707267761231 and perplexity is 135.1935246364728
At time: 350.1641390323639 and batch: 500, loss is 4.873458518981933 and perplexity is 130.77241456787812
At time: 351.33369541168213 and batch: 550, loss is 4.879593544006347 and perplexity is 131.57717268519133
At time: 352.50739908218384 and batch: 600, loss is 4.845931072235107 and perplexity is 127.22167944032668
At time: 353.6775014400482 and batch: 650, loss is 4.845598888397217 and perplexity is 127.17942547301844
At time: 354.84840989112854 and batch: 700, loss is 4.841555528640747 and perplexity is 126.6662315152813
At time: 356.0311367511749 and batch: 750, loss is 4.85421459197998 and perplexity is 128.27989957668606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.001440003860829 and perplexity of 148.62702857415763
Annealing...
Finished 18 epochs...
Completing Train Step...
At time: 359.4149401187897 and batch: 50, loss is 4.927117233276367 and perplexity is 137.98117092510498
At time: 360.6434106826782 and batch: 100, loss is 4.916972999572754 and perplexity is 136.58853324407622
At time: 361.81243658065796 and batch: 150, loss is 4.900003690719604 and perplexity is 134.2902753117726
At time: 363.01542377471924 and batch: 200, loss is 4.922945909500122 and perplexity is 137.40680555094843
At time: 364.1896035671234 and batch: 250, loss is 4.924166889190674 and perplexity is 137.57467893401002
At time: 365.3603928089142 and batch: 300, loss is 4.944416570663452 and perplexity is 140.38891993890257
At time: 366.5330128669739 and batch: 350, loss is 4.881769351959228 and perplexity is 131.86377102234158
At time: 367.7169167995453 and batch: 400, loss is 4.89326813697815 and perplexity is 133.3887953370328
At time: 368.88556933403015 and batch: 450, loss is 4.877211751937867 and perplexity is 131.2641561370393
At time: 370.05413246154785 and batch: 500, loss is 4.834589395523071 and perplexity is 125.7869239277292
At time: 371.225305557251 and batch: 550, loss is 4.834786672592163 and perplexity is 125.8117412512756
At time: 372.39692997932434 and batch: 600, loss is 4.79987756729126 and perplexity is 121.49554157984723
At time: 373.56619787216187 and batch: 650, loss is 4.795075807571411 and perplexity is 120.91354759802498
At time: 374.7342722415924 and batch: 700, loss is 4.787878084182739 and perplexity is 120.04636992481726
At time: 375.90828466415405 and batch: 750, loss is 4.811955251693726 and perplexity is 122.97182347531687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9865332315134445 and perplexity of 146.42791089022634
Finished 19 epochs...
Completing Train Step...
At time: 379.34019947052 and batch: 50, loss is 4.91387713432312 and perplexity is 136.1663274338705
At time: 380.5610496997833 and batch: 100, loss is 4.904115114212036 and perplexity is 134.84353606978289
At time: 381.73184037208557 and batch: 150, loss is 4.887983779907227 and perplexity is 132.68578044002274
At time: 382.90587615966797 and batch: 200, loss is 4.911355257034302 and perplexity is 135.82336530118596
At time: 384.07794427871704 and batch: 250, loss is 4.913622837066651 and perplexity is 136.1317051127478
At time: 385.24784445762634 and batch: 300, loss is 4.935718727111817 and perplexity is 139.17313409916252
At time: 386.42411375045776 and batch: 350, loss is 4.874550733566284 and perplexity is 130.91532413605046
At time: 387.60875487327576 and batch: 400, loss is 4.887484731674195 and perplexity is 132.61958035563714
At time: 388.80243706703186 and batch: 450, loss is 4.872855606079102 and perplexity is 130.69359395517057
At time: 389.9795618057251 and batch: 500, loss is 4.831643476486206 and perplexity is 125.41691111542998
At time: 391.15777921676636 and batch: 550, loss is 4.834242649078369 and perplexity is 125.74331532006903
At time: 392.33505272865295 and batch: 600, loss is 4.8010784530639645 and perplexity is 121.64153148823485
At time: 393.5652720928192 and batch: 650, loss is 4.797815389633179 and perplexity is 121.24525434551336
At time: 394.74160623550415 and batch: 700, loss is 4.791642379760742 and perplexity is 120.49911153629434
At time: 395.909948348999 and batch: 750, loss is 4.814717664718628 and perplexity is 123.31199206894114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984786277593568 and perplexity of 146.17233138512566
Finished 20 epochs...
Completing Train Step...
At time: 399.280597448349 and batch: 50, loss is 4.908423519134521 and perplexity is 135.4257499302778
At time: 400.49714040756226 and batch: 100, loss is 4.898123416900635 and perplexity is 134.03801006115697
At time: 401.66802167892456 and batch: 150, loss is 4.882372045516968 and perplexity is 131.94326842151483
At time: 402.83886909484863 and batch: 200, loss is 4.905746822357178 and perplexity is 135.06374097211653
At time: 404.01356267929077 and batch: 250, loss is 4.908599243164063 and perplexity is 135.44954957978317
At time: 405.1842370033264 and batch: 300, loss is 4.931185083389282 and perplexity is 138.54360081150858
At time: 406.3586423397064 and batch: 350, loss is 4.871134929656982 and perplexity is 130.4689059326578
At time: 407.5278470516205 and batch: 400, loss is 4.884721536636352 and perplexity is 132.25363241488654
At time: 408.6978018283844 and batch: 450, loss is 4.870829048156739 and perplexity is 130.42900401091697
At time: 409.87011337280273 and batch: 500, loss is 4.830431003570556 and perplexity is 125.26493865740052
At time: 411.048113822937 and batch: 550, loss is 4.83429856300354 and perplexity is 125.7503463189561
At time: 412.21618008613586 and batch: 600, loss is 4.8021116542816165 and perplexity is 121.7672766155087
At time: 413.3875799179077 and batch: 650, loss is 4.799363765716553 and perplexity is 121.43313301344644
At time: 414.55972838401794 and batch: 700, loss is 4.793229122161865 and perplexity is 120.69046435953743
At time: 415.73139357566833 and batch: 750, loss is 4.815528173446655 and perplexity is 123.41197802905651
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.984198281931323 and perplexity of 146.08640795210945
Finished 21 epochs...
Completing Train Step...
At time: 419.12992906570435 and batch: 50, loss is 4.904446287155151 and perplexity is 134.88819999581685
At time: 420.3339915275574 and batch: 100, loss is 4.893916330337524 and perplexity is 133.47528509640082
At time: 421.50766921043396 and batch: 150, loss is 4.878152980804443 and perplexity is 131.38776391241996
At time: 422.7079107761383 and batch: 200, loss is 4.901963481903076 and perplexity is 134.5537142677086
At time: 423.8905167579651 and batch: 250, loss is 4.905259485244751 and perplexity is 134.99793543464037
At time: 425.0645933151245 and batch: 300, loss is 4.927995138168335 and perplexity is 138.10235845784
At time: 426.2378144264221 and batch: 350, loss is 4.868770904541016 and perplexity is 130.16083844529166
At time: 427.4116418361664 and batch: 400, loss is 4.882773227691651 and perplexity is 131.99621232824063
At time: 428.58960914611816 and batch: 450, loss is 4.869427194595337 and perplexity is 130.24628974637784
At time: 429.75817608833313 and batch: 500, loss is 4.829516983032226 and perplexity is 125.15049624006237
At time: 430.9332447052002 and batch: 550, loss is 4.834259243011474 and perplexity is 125.7454019135442
At time: 432.1040041446686 and batch: 600, loss is 4.802644958496094 and perplexity is 121.83223293651355
At time: 433.2814157009125 and batch: 650, loss is 4.800040082931519 and perplexity is 121.51528811009223
At time: 434.4524531364441 and batch: 700, loss is 4.793742475509643 and perplexity is 120.75243711906212
At time: 435.6250693798065 and batch: 750, loss is 4.8155042171478275 and perplexity is 123.40902157024503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983840587527253 and perplexity of 146.03416300589384
Finished 22 epochs...
Completing Train Step...
At time: 439.00454568862915 and batch: 50, loss is 4.901153392791748 and perplexity is 134.44475790703086
At time: 440.2170207500458 and batch: 100, loss is 4.890529546737671 and perplexity is 133.0239978273781
At time: 441.395831823349 and batch: 150, loss is 4.874904308319092 and perplexity is 130.96162067358168
At time: 442.57149386405945 and batch: 200, loss is 4.898865776062012 and perplexity is 134.13755134902067
At time: 443.74973607063293 and batch: 250, loss is 4.902471437454223 and perplexity is 134.6220789354347
At time: 444.92765736579895 and batch: 300, loss is 4.925323209762573 and perplexity is 137.73385137488236
At time: 446.1049163341522 and batch: 350, loss is 4.866872444152832 and perplexity is 129.9139676607598
At time: 447.2825810909271 and batch: 400, loss is 4.88124605178833 and perplexity is 131.7947847402712
At time: 448.459105014801 and batch: 450, loss is 4.868247375488282 and perplexity is 130.09271329913992
At time: 449.63311433792114 and batch: 500, loss is 4.828655261993408 and perplexity is 125.04269787716296
At time: 450.80992317199707 and batch: 550, loss is 4.8339848136901855 and perplexity is 125.71089842284317
At time: 451.9932556152344 and batch: 600, loss is 4.802879009246826 and perplexity is 121.86075119932596
At time: 453.2327103614807 and batch: 650, loss is 4.800352668762207 and perplexity is 121.55327800460945
At time: 454.40534496307373 and batch: 700, loss is 4.793713884353638 and perplexity is 120.74898471664875
At time: 455.5778317451477 and batch: 750, loss is 4.815175285339356 and perplexity is 123.3684350930558
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9836376101471656 and perplexity of 146.00452438217044
Finished 23 epochs...
Completing Train Step...
At time: 458.9494378566742 and batch: 50, loss is 4.8983156108856205 and perplexity is 134.06377383619215
At time: 460.1647183895111 and batch: 100, loss is 4.887700738906861 and perplexity is 132.64823023836664
At time: 461.3340847492218 and batch: 150, loss is 4.872115602493286 and perplexity is 130.59691600242826
At time: 462.4972641468048 and batch: 200, loss is 4.896228094100952 and perplexity is 133.78420536155826
At time: 463.660653591156 and batch: 250, loss is 4.9001793384552 and perplexity is 134.31386516623533
At time: 464.8261001110077 and batch: 300, loss is 4.923027801513672 and perplexity is 137.41805853168879
At time: 466.0135157108307 and batch: 350, loss is 4.865178699493408 and perplexity is 129.69411281339612
At time: 467.204430103302 and batch: 400, loss is 4.879857044219971 and perplexity is 131.6118478665601
At time: 468.3946216106415 and batch: 450, loss is 4.867154331207275 and perplexity is 129.95059388842193
At time: 469.56987142562866 and batch: 500, loss is 4.827652196884156 and perplexity is 124.91733479393409
At time: 470.75158858299255 and batch: 550, loss is 4.833448839187622 and perplexity is 125.64353863976986
At time: 471.92218708992004 and batch: 600, loss is 4.802843608856201 and perplexity is 121.85643735748793
At time: 473.08937406539917 and batch: 650, loss is 4.800112218856811 and perplexity is 121.52405404400282
At time: 474.25596737861633 and batch: 700, loss is 4.793354253768921 and perplexity is 120.70556749621932
At time: 475.43062949180603 and batch: 750, loss is 4.814476871490479 and perplexity is 123.28230295091414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983395243799964 and perplexity of 145.9691420868331
Finished 24 epochs...
Completing Train Step...
At time: 478.8397924900055 and batch: 50, loss is 4.895655107498169 and perplexity is 133.7075707616184
At time: 480.0610020160675 and batch: 100, loss is 4.885281257629394 and perplexity is 132.32767826993185
At time: 481.23415994644165 and batch: 150, loss is 4.869673900604248 and perplexity is 130.2784262526682
At time: 482.4363934993744 and batch: 200, loss is 4.893946542739868 and perplexity is 133.4793177663351
At time: 483.6124589443207 and batch: 250, loss is 4.898093814849854 and perplexity is 134.03404231990345
At time: 484.78684878349304 and batch: 300, loss is 4.920908460617065 and perplexity is 137.1271312162833
At time: 485.95849990844727 and batch: 350, loss is 4.863650894165039 and perplexity is 129.49611674501912
At time: 487.1314010620117 and batch: 400, loss is 4.878601999282837 and perplexity is 131.44677269327653
At time: 488.30448269844055 and batch: 450, loss is 4.8660674571990965 and perplexity is 129.80943069277765
At time: 489.4774212837219 and batch: 500, loss is 4.826669549942016 and perplexity is 124.79464544690681
At time: 490.6497440338135 and batch: 550, loss is 4.8328310489654545 and perplexity is 125.56594126203828
At time: 491.8226659297943 and batch: 600, loss is 4.802558126449585 and perplexity is 121.82165445367926
At time: 492.99611139297485 and batch: 650, loss is 4.799767990112304 and perplexity is 121.4822291705267
At time: 494.1749620437622 and batch: 700, loss is 4.7928080749511714 and perplexity is 120.63965867270849
At time: 495.34867095947266 and batch: 750, loss is 4.81375862121582 and perplexity is 123.1937871949974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9833253372547235 and perplexity of 145.9589382450599
Finished 25 epochs...
Completing Train Step...
At time: 498.7694809436798 and batch: 50, loss is 4.893388862609863 and perplexity is 133.40489975570188
At time: 499.9900450706482 and batch: 100, loss is 4.883080520629883 and perplexity is 132.03678006493274
At time: 501.16073203086853 and batch: 150, loss is 4.867506895065308 and perplexity is 129.9964178486183
At time: 502.3347136974335 and batch: 200, loss is 4.891840524673462 and perplexity is 133.19850371515173
At time: 503.5073218345642 and batch: 250, loss is 4.8961709690093995 and perplexity is 133.776563144862
At time: 504.6838502883911 and batch: 300, loss is 4.91896011352539 and perplexity is 136.86022007119908
At time: 505.8561131954193 and batch: 350, loss is 4.8622125720977785 and perplexity is 129.30999350734743
At time: 507.0268909931183 and batch: 400, loss is 4.8775042533874515 and perplexity is 131.3025567088256
At time: 508.1972076892853 and batch: 450, loss is 4.864926137924194 and perplexity is 129.66136120081958
At time: 509.3699462413788 and batch: 500, loss is 4.825567770004272 and perplexity is 124.65722492770253
At time: 510.54091119766235 and batch: 550, loss is 4.832054777145386 and perplexity is 125.4685057833772
At time: 511.71249198913574 and batch: 600, loss is 4.802072772979736 and perplexity is 121.76254223730747
At time: 512.9420204162598 and batch: 650, loss is 4.799206056594849 and perplexity is 121.41398341076629
At time: 514.1157529354095 and batch: 700, loss is 4.792230291366577 and perplexity is 120.56997519120077
At time: 515.2906534671783 and batch: 750, loss is 4.812836313247681 and perplexity is 123.08021696485706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.983195460119913 and perplexity of 145.93998274733016
Finished 26 epochs...
Completing Train Step...
At time: 518.6744930744171 and batch: 50, loss is 4.891180372238159 and perplexity is 133.1106014161961
At time: 519.8781979084015 and batch: 100, loss is 4.8809592819213865 and perplexity is 131.7569953860704
At time: 521.0493731498718 and batch: 150, loss is 4.8653812599182125 and perplexity is 129.72038636888362
At time: 522.2198872566223 and batch: 200, loss is 4.889894142150879 and perplexity is 132.9395006168056
At time: 523.3899214267731 and batch: 250, loss is 4.894329538345337 and perplexity is 133.53044954945972
At time: 524.5650115013123 and batch: 300, loss is 4.917064895629883 and perplexity is 136.60108576848532
At time: 525.7410111427307 and batch: 350, loss is 4.860855970382691 and perplexity is 129.13469028358955
At time: 526.9132699966431 and batch: 400, loss is 4.876318035125732 and perplexity is 131.14689556054822
At time: 528.0886001586914 and batch: 450, loss is 4.863767099380493 and perplexity is 129.51116574353514
At time: 529.2606735229492 and batch: 500, loss is 4.824401273727417 and perplexity is 124.51189751736185
At time: 530.4302296638489 and batch: 550, loss is 4.831224737167358 and perplexity is 125.36440511742765
At time: 531.6054775714874 and batch: 600, loss is 4.801473789215088 and perplexity is 121.68963029008127
At time: 532.7854151725769 and batch: 650, loss is 4.798548564910889 and perplexity is 121.33418096395472
At time: 533.9616405963898 and batch: 700, loss is 4.791647911071777 and perplexity is 120.49977805620304
At time: 535.1351525783539 and batch: 750, loss is 4.811827688217163 and perplexity is 122.95613776247846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.98311064964117 and perplexity of 145.92760603237062
Finished 27 epochs...
Completing Train Step...
At time: 538.5041773319244 and batch: 50, loss is 4.889066572189331 and perplexity is 132.82952939011028
At time: 539.7162907123566 and batch: 100, loss is 4.878995361328125 and perplexity is 131.49848903558598
At time: 540.8883061408997 and batch: 150, loss is 4.863584394454956 and perplexity is 129.48750557712202
At time: 542.1252739429474 and batch: 200, loss is 4.888061866760254 and perplexity is 132.69614185959884
At time: 543.3031578063965 and batch: 250, loss is 4.892649974822998 and perplexity is 133.3063649121436
At time: 544.4880945682526 and batch: 300, loss is 4.915418930053711 and perplexity is 136.376430022175
At time: 545.7016777992249 and batch: 350, loss is 4.859544668197632 and perplexity is 128.96546665795736
At time: 546.9125480651855 and batch: 400, loss is 4.8751390933990475 and perplexity is 130.99237211801156
At time: 548.1134576797485 and batch: 450, loss is 4.862669286727905 and perplexity is 129.36906476152998
At time: 549.2874674797058 and batch: 500, loss is 4.8233599281311035 and perplexity is 124.38230508815711
At time: 550.4614264965057 and batch: 550, loss is 4.830393590927124 and perplexity is 125.26025225258192
At time: 551.639762878418 and batch: 600, loss is 4.800895395278931 and perplexity is 121.61926609690131
At time: 552.8098313808441 and batch: 650, loss is 4.797822341918946 and perplexity is 121.24609728009958
At time: 553.9788691997528 and batch: 700, loss is 4.790975570678711 and perplexity is 120.418788417414
At time: 555.1480965614319 and batch: 750, loss is 4.810826091766358 and perplexity is 122.83304698522394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982968352561773 and perplexity of 145.9068424375633
Finished 28 epochs...
Completing Train Step...
At time: 558.5250248908997 and batch: 50, loss is 4.8870635604858395 and perplexity is 132.56373657010184
At time: 559.7472231388092 and batch: 100, loss is 4.877063283920288 and perplexity is 131.2446690546385
At time: 560.9185712337494 and batch: 150, loss is 4.861731519699097 and perplexity is 129.24780358430232
At time: 562.0905957221985 and batch: 200, loss is 4.88628098487854 and perplexity is 132.46003600550813
At time: 563.2579700946808 and batch: 250, loss is 4.891020832061767 and perplexity is 133.08936662131305
At time: 564.426915884018 and batch: 300, loss is 4.9137374210357665 and perplexity is 136.1473045175459
At time: 565.6031949520111 and batch: 350, loss is 4.8582541942596436 and perplexity is 128.79914742273525
At time: 566.7734532356262 and batch: 400, loss is 4.87397367477417 and perplexity is 130.83980009023838
At time: 567.9458467960358 and batch: 450, loss is 4.8615518283844 and perplexity is 129.2245809630687
At time: 569.1249985694885 and batch: 500, loss is 4.822183570861816 and perplexity is 124.23607308679806
At time: 570.294585943222 and batch: 550, loss is 4.829433012008667 and perplexity is 125.13998766600817
At time: 571.4671092033386 and batch: 600, loss is 4.8001787471771244 and perplexity is 121.53213910413577
At time: 572.6905989646912 and batch: 650, loss is 4.797096910476685 and perplexity is 121.1581734440946
At time: 573.8601400852203 and batch: 700, loss is 4.790233917236328 and perplexity is 120.32951251843778
At time: 575.0242726802826 and batch: 750, loss is 4.809756078720093 and perplexity is 122.70168431485624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982942802961483 and perplexity of 145.90311462368163
Finished 29 epochs...
Completing Train Step...
At time: 578.4289917945862 and batch: 50, loss is 4.885114831924438 and perplexity is 132.3056573752629
At time: 579.6384143829346 and batch: 100, loss is 4.875148868560791 and perplexity is 130.99365259589456
At time: 580.8125922679901 and batch: 150, loss is 4.85997727394104 and perplexity is 129.02126992907245
At time: 581.988956451416 and batch: 200, loss is 4.884650707244873 and perplexity is 132.24426530231906
At time: 583.1641707420349 and batch: 250, loss is 4.889401330947876 and perplexity is 132.87400268196365
At time: 584.3342123031616 and batch: 300, loss is 4.912068071365357 and perplexity is 135.92021665688105
At time: 585.5120193958282 and batch: 350, loss is 4.857081871032715 and perplexity is 128.64824166304962
At time: 586.6822535991669 and batch: 400, loss is 4.872928609848023 and perplexity is 130.70313542838062
At time: 587.8552215099335 and batch: 450, loss is 4.86037386894226 and perplexity is 129.07244926784153
At time: 589.0312848091125 and batch: 500, loss is 4.821077919006347 and perplexity is 124.09878715128373
At time: 590.2038474082947 and batch: 550, loss is 4.828597679138183 and perplexity is 125.03549776889761
At time: 591.3774347305298 and batch: 600, loss is 4.7994781017303465 and perplexity is 121.44701798757883
At time: 592.5510621070862 and batch: 650, loss is 4.796359958648682 and perplexity is 121.06891859899761
At time: 593.7252688407898 and batch: 700, loss is 4.789520931243897 and perplexity is 120.24374983896368
At time: 594.9061136245728 and batch: 750, loss is 4.808826303482055 and perplexity is 122.58765234737952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982937480128089 and perplexity of 145.9023380077778
Finished 30 epochs...
Completing Train Step...
At time: 598.2799799442291 and batch: 50, loss is 4.883320264816284 and perplexity is 132.06843891020483
At time: 599.5147037506104 and batch: 100, loss is 4.8734527111053465 and perplexity is 130.7716550600389
At time: 600.6826264858246 and batch: 150, loss is 4.858311233520507 and perplexity is 128.80649424043068
At time: 601.8739280700684 and batch: 200, loss is 4.883146305084228 and perplexity is 132.0454663181699
At time: 603.0452947616577 and batch: 250, loss is 4.887835321426391 and perplexity is 132.66608357274978
At time: 604.2138519287109 and batch: 300, loss is 4.910520420074463 and perplexity is 135.7100222538883
At time: 605.37859416008 and batch: 350, loss is 4.855911226272583 and perplexity is 128.49772838902302
At time: 606.5429978370667 and batch: 400, loss is 4.871784191131592 and perplexity is 130.55364187182818
At time: 607.7085959911346 and batch: 450, loss is 4.859330615997314 and perplexity is 128.93786427033555
At time: 608.8753688335419 and batch: 500, loss is 4.819984035491943 and perplexity is 123.96311175393055
At time: 610.0447280406952 and batch: 550, loss is 4.827747735977173 and perplexity is 124.92926985292571
At time: 611.2148933410645 and batch: 600, loss is 4.798716621398926 and perplexity is 121.35457367380195
At time: 612.383287191391 and batch: 650, loss is 4.795631561279297 and perplexity is 120.98076442670703
At time: 613.5542068481445 and batch: 700, loss is 4.788769359588623 and perplexity is 120.15341199679405
At time: 614.724684715271 and batch: 750, loss is 4.807749443054199 and perplexity is 122.45571360817638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982881058094113 and perplexity of 145.89410613333735
Finished 31 epochs...
Completing Train Step...
At time: 618.1171989440918 and batch: 50, loss is 4.88150387763977 and perplexity is 131.82876922371315
At time: 619.3558101654053 and batch: 100, loss is 4.871695156097412 and perplexity is 130.5420185413113
At time: 620.5355169773102 and batch: 150, loss is 4.8564941024780275 and perplexity is 128.57264848980557
At time: 621.7101526260376 and batch: 200, loss is 4.881576738357544 and perplexity is 131.83837471238917
At time: 622.8851892948151 and batch: 250, loss is 4.886409788131714 and perplexity is 132.47709838788282
At time: 624.0780282020569 and batch: 300, loss is 4.909081020355225 and perplexity is 135.51482180539608
At time: 625.2466804981232 and batch: 350, loss is 4.854606857299805 and perplexity is 128.33022920315872
At time: 626.4216196537018 and batch: 400, loss is 4.87067135810852 and perplexity is 130.40843827653384
At time: 627.5962061882019 and batch: 450, loss is 4.8581304931640625 and perplexity is 128.7832158124883
At time: 628.7724833488464 and batch: 500, loss is 4.818838834762573 and perplexity is 123.8212303647764
At time: 629.9442093372345 and batch: 550, loss is 4.826806735992432 and perplexity is 124.81176670580078
At time: 631.1133685112 and batch: 600, loss is 4.797927312850952 and perplexity is 121.25882526395566
At time: 632.3429653644562 and batch: 650, loss is 4.794717741012573 and perplexity is 120.87026025045797
At time: 633.5146629810333 and batch: 700, loss is 4.787971935272217 and perplexity is 120.05763693612492
At time: 634.6920175552368 and batch: 750, loss is 4.806900939941406 and perplexity is 122.35185362299218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982837765715843 and perplexity of 145.88779016722472
Finished 32 epochs...
Completing Train Step...
At time: 638.0966796875 and batch: 50, loss is 4.879866704940796 and perplexity is 131.61311933802128
At time: 639.3212056159973 and batch: 100, loss is 4.8701065635681156 and perplexity is 130.33480509833845
At time: 640.4897828102112 and batch: 150, loss is 4.854895114898682 and perplexity is 128.3672266990406
At time: 641.6648986339569 and batch: 200, loss is 4.880165119171142 and perplexity is 131.65240042645834
At time: 642.8370594978333 and batch: 250, loss is 4.884936685562134 and perplexity is 132.28208970299906
At time: 644.0086500644684 and batch: 300, loss is 4.907570571899414 and perplexity is 135.31028815971146
At time: 645.1747987270355 and batch: 350, loss is 4.853522272109985 and perplexity is 128.191119588909
At time: 646.347850561142 and batch: 400, loss is 4.869585018157959 and perplexity is 130.26684730203462
At time: 647.5176246166229 and batch: 450, loss is 4.857057380676269 and perplexity is 128.64509106033498
At time: 648.688145160675 and batch: 500, loss is 4.817743167877198 and perplexity is 123.68563783864438
At time: 649.8652987480164 and batch: 550, loss is 4.826067104339599 and perplexity is 124.71948610353328
At time: 651.0327489376068 and batch: 600, loss is 4.797151126861572 and perplexity is 121.16474238032868
At time: 652.2031352519989 and batch: 650, loss is 4.793851537704468 and perplexity is 120.76560736305954
At time: 653.3722174167633 and batch: 700, loss is 4.787121028900146 and perplexity is 119.95552257888012
At time: 654.5401031970978 and batch: 750, loss is 4.805835981369018 and perplexity is 122.22162332487324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.982853734216024 and perplexity of 145.8901197950288
Annealing...
Finished 33 epochs...
Completing Train Step...
At time: 657.9240956306458 and batch: 50, loss is 4.878957443237304 and perplexity is 131.49350295846776
At time: 659.1459832191467 and batch: 100, loss is 4.869465417861939 and perplexity is 130.251268280182
At time: 660.3170433044434 and batch: 150, loss is 4.853470573425293 and perplexity is 128.18449244794573
At time: 661.518196105957 and batch: 200, loss is 4.87779203414917 and perplexity is 131.3403484962247
At time: 662.6940360069275 and batch: 250, loss is 4.881354598999024 and perplexity is 131.80909147300264
At time: 663.8647484779358 and batch: 300, loss is 4.902680978775025 and perplexity is 134.65029077933414
At time: 665.0344624519348 and batch: 350, loss is 4.846729717254639 and perplexity is 127.32332498493939
At time: 666.2109897136688 and batch: 400, loss is 4.862541875839233 and perplexity is 129.35258278403612
At time: 667.3825454711914 and batch: 450, loss is 4.849238386154175 and perplexity is 127.64313803531478
At time: 668.5524215698242 and batch: 500, loss is 4.807955055236817 and perplexity is 122.48089458339429
At time: 669.7259604930878 and batch: 550, loss is 4.813412141799927 and perplexity is 123.15111047729746
At time: 670.894638299942 and batch: 600, loss is 4.784157114028931 and perplexity is 119.60051099357457
At time: 672.0646004676819 and batch: 650, loss is 4.779948654174805 and perplexity is 119.09823469067679
At time: 673.2352063655853 and batch: 700, loss is 4.772792625427246 and perplexity is 118.24900647420161
At time: 674.4058456420898 and batch: 750, loss is 4.793267745971679 and perplexity is 120.69512597510337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9791075240734015 and perplexity of 145.34460718895406
Finished 34 epochs...
Completing Train Step...
At time: 677.77490234375 and batch: 50, loss is 4.87673791885376 and perplexity is 131.20197357034627
At time: 678.9712152481079 and batch: 100, loss is 4.867404994964599 and perplexity is 129.98317187544225
At time: 680.1433708667755 and batch: 150, loss is 4.8517436599731445 and perplexity is 127.96331995142592
At time: 681.3204431533813 and batch: 200, loss is 4.876244478225708 and perplexity is 131.13724915624712
At time: 682.5008525848389 and batch: 250, loss is 4.880069341659546 and perplexity is 131.6397916909761
At time: 683.6707496643066 and batch: 300, loss is 4.901480932235717 and perplexity is 134.48880108082486
At time: 684.842453956604 and batch: 350, loss is 4.845874662399292 and perplexity is 127.21450308868768
At time: 686.0140285491943 and batch: 400, loss is 4.861761798858643 and perplexity is 129.2517171584176
At time: 687.1747174263 and batch: 450, loss is 4.848735771179199 and perplexity is 127.57899880270497
At time: 688.3458743095398 and batch: 500, loss is 4.807636804580689 and perplexity is 122.44192116031768
At time: 689.5204842090607 and batch: 550, loss is 4.81325140953064 and perplexity is 123.13131771055598
At time: 690.7011826038361 and batch: 600, loss is 4.784213800430297 and perplexity is 119.60729090830709
At time: 691.9225282669067 and batch: 650, loss is 4.780445203781128 and perplexity is 119.15738755717712
At time: 693.0901145935059 and batch: 700, loss is 4.773503570556641 and perplexity is 118.3331049205584
At time: 694.2600436210632 and batch: 750, loss is 4.7935514068603515 and perplexity is 120.72936731803117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978892126748728 and perplexity of 145.3133037208725
Finished 35 epochs...
Completing Train Step...
At time: 697.6390087604523 and batch: 50, loss is 4.87565845489502 and perplexity is 131.06042218211766
At time: 698.8800568580627 and batch: 100, loss is 4.866345205307007 and perplexity is 129.8454900240152
At time: 700.0922751426697 and batch: 150, loss is 4.850746593475342 and perplexity is 127.83579559784981
At time: 701.2770338058472 and batch: 200, loss is 4.875354270935059 and perplexity is 131.02056176666102
At time: 702.4663252830505 and batch: 250, loss is 4.879365348815918 and perplexity is 131.54715083276577
At time: 703.6461906433105 and batch: 300, loss is 4.9008055019378665 and perplexity is 134.39799394025246
At time: 704.8231103420258 and batch: 350, loss is 4.845376129150391 and perplexity is 127.1510982351825
At time: 706.0041871070862 and batch: 400, loss is 4.861317920684814 and perplexity is 129.19435787345023
At time: 707.1763508319855 and batch: 450, loss is 4.848553981781006 and perplexity is 127.55580840123893
At time: 708.3464002609253 and batch: 500, loss is 4.807528734207153 and perplexity is 122.42868953114791
At time: 709.5227696895599 and batch: 550, loss is 4.81318736076355 and perplexity is 123.12343155401845
At time: 710.6947867870331 and batch: 600, loss is 4.784272871017456 and perplexity is 119.61435638988851
At time: 711.8646929264069 and batch: 650, loss is 4.780756826400757 and perplexity is 119.19452548064376
At time: 713.0351786613464 and batch: 700, loss is 4.7738934421539305 and perplexity is 118.3792486316627
At time: 714.2061319351196 and batch: 750, loss is 4.79360933303833 and perplexity is 120.73636091140379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978799509447675 and perplexity of 145.29984581810166
Finished 36 epochs...
Completing Train Step...
At time: 717.5927288532257 and batch: 50, loss is 4.874800071716309 and perplexity is 130.94797039058963
At time: 718.8195509910583 and batch: 100, loss is 4.865540409088135 and perplexity is 129.74103290359213
At time: 719.9949040412903 and batch: 150, loss is 4.849985389709473 and perplexity is 127.73852353545486
At time: 721.2207984924316 and batch: 200, loss is 4.874693212509155 and perplexity is 130.93397814190888
At time: 722.4009292125702 and batch: 250, loss is 4.878840160369873 and perplexity is 131.47808192772126
At time: 723.5802853107452 and batch: 300, loss is 4.900310020446778 and perplexity is 134.33141871657318
At time: 724.7585833072662 and batch: 350, loss is 4.84501085281372 and perplexity is 127.104661429465
At time: 725.9321205615997 and batch: 400, loss is 4.861022205352783 and perplexity is 129.15615876932384
At time: 727.1091241836548 and batch: 450, loss is 4.848415803909302 and perplexity is 127.53818422877396
At time: 728.2873723506927 and batch: 500, loss is 4.807452392578125 and perplexity is 122.41934348230016
At time: 729.4600603580475 and batch: 550, loss is 4.813119411468506 and perplexity is 123.11506568787154
At time: 730.6357836723328 and batch: 600, loss is 4.7842777061462405 and perplexity is 119.61493474210432
At time: 731.8148641586304 and batch: 650, loss is 4.7809554386138915 and perplexity is 119.21820132022062
At time: 732.9889707565308 and batch: 700, loss is 4.774125661849975 and perplexity is 118.40674181690399
At time: 734.17018866539 and batch: 750, loss is 4.793563117980957 and perplexity is 120.73078120249154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978753023369368 and perplexity of 145.29309155508184
Finished 37 epochs...
Completing Train Step...
At time: 737.5578608512878 and batch: 50, loss is 4.874057502746582 and perplexity is 130.8507685851176
At time: 738.7801389694214 and batch: 100, loss is 4.8648694896697995 and perplexity is 129.6540163190844
At time: 739.9507346153259 and batch: 150, loss is 4.849345417022705 and perplexity is 127.65680052238146
At time: 741.1223866939545 and batch: 200, loss is 4.874150409698486 and perplexity is 130.86292609593107
At time: 742.2897837162018 and batch: 250, loss is 4.878403549194336 and perplexity is 131.42068965777835
At time: 743.4584422111511 and batch: 300, loss is 4.899898252487183 and perplexity is 134.2761167289534
At time: 744.6216928958893 and batch: 350, loss is 4.844694185256958 and perplexity is 127.06441787912668
At time: 745.7908427715302 and batch: 400, loss is 4.860766677856446 and perplexity is 129.1231600356584
At time: 746.9653327465057 and batch: 450, loss is 4.848291397094727 and perplexity is 127.52231859645417
At time: 748.1381967067719 and batch: 500, loss is 4.807372188568115 and perplexity is 122.40952535378194
At time: 749.3093583583832 and batch: 550, loss is 4.813030242919922 and perplexity is 123.10408818558658
At time: 750.4736199378967 and batch: 600, loss is 4.784236192703247 and perplexity is 119.60996921739847
At time: 751.6904137134552 and batch: 650, loss is 4.781077032089233 and perplexity is 119.23269835699782
At time: 752.855895280838 and batch: 700, loss is 4.774262390136719 and perplexity is 118.42293247468669
At time: 754.0257399082184 and batch: 750, loss is 4.793454313278199 and perplexity is 120.71764584033653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978721086369005 and perplexity of 145.2884514036607
Finished 38 epochs...
Completing Train Step...
At time: 757.4137346744537 and batch: 50, loss is 4.873385810852051 and perplexity is 130.7629066958287
At time: 758.6372983455658 and batch: 100, loss is 4.864268054962158 and perplexity is 129.5760613384504
At time: 759.8106861114502 and batch: 150, loss is 4.848771419525146 and perplexity is 127.58354686405482
At time: 760.9701535701752 and batch: 200, loss is 4.873663454055786 and perplexity is 130.7992171686182
At time: 762.146196603775 and batch: 250, loss is 4.87800594329834 and perplexity is 131.36844640351578
At time: 763.3171446323395 and batch: 300, loss is 4.899517765045166 and perplexity is 134.225036071166
At time: 764.4945623874664 and batch: 350, loss is 4.844401168823242 and perplexity is 127.02719137080327
At time: 765.6688885688782 and batch: 400, loss is 4.860528421401978 and perplexity is 129.09239927397917
At time: 766.8384168148041 and batch: 450, loss is 4.848176336288452 and perplexity is 127.50764661975936
At time: 768.0070645809174 and batch: 500, loss is 4.807277822494507 and perplexity is 122.3979745925107
At time: 769.1902923583984 and batch: 550, loss is 4.812919416427612 and perplexity is 123.09044574728975
At time: 770.3612139225006 and batch: 600, loss is 4.784157848358154 and perplexity is 119.60059881975715
At time: 771.533935546875 and batch: 650, loss is 4.781136226654053 and perplexity is 119.23975649358893
At time: 772.7020108699799 and batch: 700, loss is 4.7743445491790775 and perplexity is 118.43266238910688
At time: 773.8728015422821 and batch: 750, loss is 4.793310117721558 and perplexity is 120.70024014714019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978693407635356 and perplexity of 145.28443005896526
Finished 39 epochs...
Completing Train Step...
At time: 777.2540006637573 and batch: 50, loss is 4.87276912689209 and perplexity is 130.68229216810934
At time: 778.5167095661163 and batch: 100, loss is 4.86371563911438 and perplexity is 129.50450123596147
At time: 779.7169809341431 and batch: 150, loss is 4.848244333267212 and perplexity is 127.51631704927652
At time: 780.9448218345642 and batch: 200, loss is 4.873227481842041 and perplexity is 130.7422047731662
At time: 782.1239242553711 and batch: 250, loss is 4.877647590637207 and perplexity is 131.32137860509732
At time: 783.3019506931305 and batch: 300, loss is 4.899175910949707 and perplexity is 134.17915853503052
At time: 784.4748673439026 and batch: 350, loss is 4.84412091255188 and perplexity is 126.99159619190709
At time: 785.6498470306396 and batch: 400, loss is 4.860301609039307 and perplexity is 129.06312284215082
At time: 786.8343975543976 and batch: 450, loss is 4.848054981231689 and perplexity is 127.49217386093389
At time: 788.0189809799194 and batch: 500, loss is 4.807172336578369 and perplexity is 122.38506401098161
At time: 789.1902487277985 and batch: 550, loss is 4.8127860260009765 and perplexity is 123.07402775524255
At time: 790.3767099380493 and batch: 600, loss is 4.784051036834716 and perplexity is 119.58782477981266
At time: 791.5534343719482 and batch: 650, loss is 4.781160154342651 and perplexity is 119.24260965948557
At time: 792.740905046463 and batch: 700, loss is 4.774380922317505 and perplexity is 118.43697023507475
At time: 793.9103827476501 and batch: 750, loss is 4.7931391048431395 and perplexity is 120.67960061650993
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978671406590661 and perplexity of 145.2812336848879
Finished 40 epochs...
Completing Train Step...
At time: 797.3334648609161 and batch: 50, loss is 4.872189912796021 and perplexity is 130.60662105938096
At time: 798.5376472473145 and batch: 100, loss is 4.863209419250488 and perplexity is 129.43896007549068
At time: 799.7077705860138 and batch: 150, loss is 4.8477613067626955 and perplexity is 127.45473816169803
At time: 800.8845620155334 and batch: 200, loss is 4.872824382781983 and perplexity is 130.68951333396046
At time: 802.0547616481781 and batch: 250, loss is 4.877318935394287 and perplexity is 131.27822623702568
At time: 803.2308533191681 and batch: 300, loss is 4.898862628936768 and perplexity is 134.13712920201093
At time: 804.4024441242218 and batch: 350, loss is 4.843845539093017 and perplexity is 126.95663089129611
At time: 805.5714247226715 and batch: 400, loss is 4.860070819854736 and perplexity is 129.0333399061939
At time: 806.7472093105316 and batch: 450, loss is 4.847919588088989 and perplexity is 127.47491346334132
At time: 807.9174606800079 and batch: 500, loss is 4.807057018280029 and perplexity is 122.37095158738363
At time: 809.0895059108734 and batch: 550, loss is 4.812633724212646 and perplexity is 123.05528478804837
At time: 810.2635006904602 and batch: 600, loss is 4.783927221298217 and perplexity is 119.5730188657488
At time: 811.4863483905792 and batch: 650, loss is 4.781147022247314 and perplexity is 119.241043764449
At time: 812.6587126255035 and batch: 700, loss is 4.77439603805542 and perplexity is 118.43876051080687
At time: 813.8375325202942 and batch: 750, loss is 4.792940559387207 and perplexity is 120.65564260863931
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9786497604015265 and perplexity of 145.2780889338619
Finished 41 epochs...
Completing Train Step...
At time: 817.2215573787689 and batch: 50, loss is 4.871635541915894 and perplexity is 130.53423661768102
At time: 818.4208142757416 and batch: 100, loss is 4.862726993560791 and perplexity is 129.37653045593956
At time: 819.5894992351532 and batch: 150, loss is 4.847300691604614 and perplexity is 127.39604409605738
At time: 820.7588040828705 and batch: 200, loss is 4.872420864105225 and perplexity is 130.6367883129546
At time: 821.9273614883423 and batch: 250, loss is 4.876985816955567 and perplexity is 131.23450232228905
At time: 823.1013312339783 and batch: 300, loss is 4.8985443210601805 and perplexity is 134.0944390919027
At time: 824.2705795764923 and batch: 350, loss is 4.84348837852478 and perplexity is 126.91129508542855
At time: 825.4390079975128 and batch: 400, loss is 4.859771661758423 and perplexity is 128.99474431124676
At time: 826.6092519760132 and batch: 450, loss is 4.847755069732666 and perplexity is 127.45394322514777
At time: 827.7836399078369 and batch: 500, loss is 4.8068994140625 and perplexity is 122.35166692902207
At time: 828.959130525589 and batch: 550, loss is 4.812427635192871 and perplexity is 123.02992705809287
At time: 830.1317837238312 and batch: 600, loss is 4.783762607574463 and perplexity is 119.55333712583946
At time: 831.3031585216522 and batch: 650, loss is 4.781083450317383 and perplexity is 119.23346362211461
At time: 832.4834954738617 and batch: 700, loss is 4.774367046356201 and perplexity is 118.43532681966087
At time: 833.6523957252502 and batch: 750, loss is 4.792694978713989 and perplexity is 120.62601555276494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978644792423692 and perplexity of 145.27736719732906
Finished 42 epochs...
Completing Train Step...
At time: 837.0399131774902 and batch: 50, loss is 4.871087350845337 and perplexity is 130.4626985248286
At time: 838.260128736496 and batch: 100, loss is 4.862286491394043 and perplexity is 129.31955236435508
At time: 839.4363310337067 and batch: 150, loss is 4.8469181156158445 and perplexity is 127.34731475045513
At time: 840.6394126415253 and batch: 200, loss is 4.872044944763184 and perplexity is 130.58768864678098
At time: 841.8128607273102 and batch: 250, loss is 4.876697072982788 and perplexity is 131.1966146209088
At time: 842.9826955795288 and batch: 300, loss is 4.8983109664916995 and perplexity is 134.06315119266182
At time: 844.1546356678009 and batch: 350, loss is 4.843341922760009 and perplexity is 126.89270955566096
At time: 845.3336026668549 and batch: 400, loss is 4.859501876831055 and perplexity is 128.95994816747051
At time: 846.504397392273 and batch: 450, loss is 4.847596349716187 and perplexity is 127.43371533850645
At time: 847.6773791313171 and batch: 500, loss is 4.806737623214722 and perplexity is 122.33187315037179
At time: 848.8482224941254 and batch: 550, loss is 4.812226209640503 and perplexity is 123.00514818271049
At time: 850.0212609767914 and batch: 600, loss is 4.783601350784302 and perplexity is 119.53405989277564
At time: 851.201194524765 and batch: 650, loss is 4.781017408370972 and perplexity is 119.22558947211462
At time: 852.373884677887 and batch: 700, loss is 4.774318027496338 and perplexity is 118.42952139726135
At time: 853.5444207191467 and batch: 750, loss is 4.792468109130859 and perplexity is 120.59865228296607
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.978634501612464 and perplexity of 145.27587218306
Finished 43 epochs...
Completing Train Step...
At time: 856.9516997337341 and batch: 50, loss is 4.8705845546722415 and perplexity is 130.39711886726062
At time: 858.1885097026825 and batch: 100, loss is 4.8618801498413085 and perplexity is 129.2670151314017
At time: 859.3622341156006 and batch: 150, loss is 4.846557855606079 and perplexity is 127.30144486863767
At time: 860.5366714000702 and batch: 200, loss is 4.87169472694397 and perplexity is 130.54196251876667
At time: 861.7178750038147 and batch: 250, loss is 4.876423225402832 and perplexity is 131.16069166442836
At time: 862.8984200954437 and batch: 300, loss is 4.898070640563965 and perplexity is 134.03093621267897
At time: 864.0938177108765 and batch: 350, loss is 4.843225460052491 and perplexity is 126.8779321476669
At time: 865.2692408561707 and batch: 400, loss is 4.85927885055542 and perplexity is 128.93118991756654
At time: 866.469277381897 and batch: 450, loss is 4.847444639205933 and perplexity is 127.41438377096695
At time: 867.6457142829895 and batch: 500, loss is 4.8065598201751705 and perplexity is 122.31012410507213
At time: 868.8164222240448 and batch: 550, loss is 4.811970796585083 and perplexity is 122.97373507381056
At time: 870.002500295639 and batch: 600, loss is 4.783365106582641 and perplexity is 119.50582399963935
At time: 871.2357702255249 and batch: 650, loss is 4.780861892700195 and perplexity is 119.20704946626222
At time: 872.410317659378 and batch: 700, loss is 4.7741805267333985 and perplexity is 118.41323836720488
At time: 873.5870056152344 and batch: 750, loss is 4.792186841964722 and perplexity is 120.56473661170442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9786529541015625 and perplexity of 145.2785529092407
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 877.010823726654 and batch: 50, loss is 4.870347919464112 and perplexity is 130.36626596848296
At time: 878.2306177616119 and batch: 100, loss is 4.861708183288574 and perplexity is 129.2447874396918
At time: 879.4015245437622 and batch: 150, loss is 4.846253776550293 and perplexity is 127.26274105029036
At time: 880.5715146064758 and batch: 200, loss is 4.871420602798462 and perplexity is 130.5061827191159
At time: 881.7385170459747 and batch: 250, loss is 4.8761865997314455 and perplexity is 131.1296593493697
At time: 882.9011964797974 and batch: 300, loss is 4.897606763839722 and perplexity is 133.96877679930853
At time: 884.0671029090881 and batch: 350, loss is 4.8419216537475585 and perplexity is 126.71261569351222
At time: 885.2294526100159 and batch: 400, loss is 4.857731533050537 and perplexity is 128.73184669388365
At time: 886.3942124843597 and batch: 450, loss is 4.84552903175354 and perplexity is 127.1705414555176
At time: 887.5586185455322 and batch: 500, loss is 4.80390588760376 and perplexity is 121.98595163910136
At time: 888.7288091182709 and batch: 550, loss is 4.809032726287842 and perplexity is 122.61295984646443
At time: 889.8973205089569 and batch: 600, loss is 4.780245504379272 and perplexity is 119.13359427398653
At time: 891.0599317550659 and batch: 650, loss is 4.777498207092285 and perplexity is 118.80674805109577
At time: 892.2255291938782 and batch: 700, loss is 4.77086184501648 and perplexity is 118.02091387812595
At time: 893.3946695327759 and batch: 750, loss is 4.78919264793396 and perplexity is 120.2042823013887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.97984952704851 and perplexity of 145.45249334087725
Annealing...
Finished 45 epochs...
Completing Train Step...
At time: 896.8126094341278 and batch: 50, loss is 4.870120115280152 and perplexity is 130.33657137005338
At time: 898.0131611824036 and batch: 100, loss is 4.861410140991211 and perplexity is 129.2062727661071
At time: 899.1826117038727 and batch: 150, loss is 4.846033887863159 and perplexity is 127.23476048965684
At time: 900.3897471427917 and batch: 200, loss is 4.871132822036743 and perplexity is 130.46863095404086
At time: 901.5591232776642 and batch: 250, loss is 4.876009426116943 and perplexity is 131.10642869164488
At time: 902.7365863323212 and batch: 300, loss is 4.897334861755371 and perplexity is 133.9323553614157
At time: 903.9100832939148 and batch: 350, loss is 4.841475610733032 and perplexity is 126.65610901957022
At time: 905.0868148803711 and batch: 400, loss is 4.8573018741607665 and perplexity is 128.6765477922381
At time: 906.2565948963165 and batch: 450, loss is 4.845154657363891 and perplexity is 127.12294097243573
At time: 907.4282932281494 and batch: 500, loss is 4.803227443695068 and perplexity is 121.90321908113924
At time: 908.6013059616089 and batch: 550, loss is 4.808308238983154 and perplexity is 122.52416048455484
At time: 909.7760293483734 and batch: 600, loss is 4.7795878028869625 and perplexity is 119.05526569248431
At time: 910.9457035064697 and batch: 650, loss is 4.776785879135132 and perplexity is 118.72214881773729
At time: 912.124240398407 and batch: 700, loss is 4.7701209545135494 and perplexity is 117.93350568783384
At time: 913.2960221767426 and batch: 750, loss is 4.788576440811157 and perplexity is 120.1302343832136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.9802026083303055 and perplexity of 145.50385896125223
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 916.6754245758057 and batch: 50, loss is 4.870024223327636 and perplexity is 130.32407374096115
At time: 917.8914070129395 and batch: 100, loss is 4.861344976425171 and perplexity is 129.19785336973877
At time: 919.063259601593 and batch: 150, loss is 4.8459624004364015 and perplexity is 127.22566512914112
At time: 920.2285845279694 and batch: 200, loss is 4.871052942276001 and perplexity is 130.4582095672504
At time: 921.4031286239624 and batch: 250, loss is 4.875869426727295 and perplexity is 131.08807515642093
At time: 922.5721478462219 and batch: 300, loss is 4.897250385284424 and perplexity is 133.92104170656464
At time: 923.7385776042938 and batch: 350, loss is 4.841368741989136 and perplexity is 126.64257416353195
At time: 924.9064536094666 and batch: 400, loss is 4.857208204269409 and perplexity is 128.6644952384757
At time: 926.0726799964905 and batch: 450, loss is 4.845065631866455 and perplexity is 127.11162429312408
At time: 927.2416541576385 and batch: 500, loss is 4.803066825866699 and perplexity is 121.88364082316996
At time: 928.4081070423126 and batch: 550, loss is 4.80815541267395 and perplexity is 122.50543700007661
At time: 929.5790138244629 and batch: 600, loss is 4.779450006484986 and perplexity is 119.03886143548525
At time: 930.7975964546204 and batch: 650, loss is 4.776642446517944 and perplexity is 118.70512141038618
At time: 931.9675872325897 and batch: 700, loss is 4.769940042495728 and perplexity is 117.91217202916688
At time: 933.1333622932434 and batch: 750, loss is 4.78843276977539 and perplexity is 120.11297638777957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980254062386447 and perplexity of 145.51134591759546
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 936.5433540344238 and batch: 50, loss is 4.870006561279297 and perplexity is 130.321771971198
At time: 937.7322719097137 and batch: 100, loss is 4.861330337524414 and perplexity is 129.19596206902867
At time: 938.897792339325 and batch: 150, loss is 4.845948991775512 and perplexity is 127.22395921477805
At time: 940.0644755363464 and batch: 200, loss is 4.871035642623902 and perplexity is 130.45595270513292
At time: 941.2488391399384 and batch: 250, loss is 4.875840282440185 and perplexity is 131.08425474359376
At time: 942.4392952919006 and batch: 300, loss is 4.897233180999756 and perplexity is 133.91873771065943
At time: 943.6317050457001 and batch: 350, loss is 4.841345405578613 and perplexity is 126.63961881491531
At time: 944.8097457885742 and batch: 400, loss is 4.8571883392333985 and perplexity is 128.66193933903105
At time: 945.9912574291229 and batch: 450, loss is 4.8450456142425535 and perplexity is 127.10907984590244
At time: 947.168215751648 and batch: 500, loss is 4.803033056259156 and perplexity is 121.87952492994987
At time: 948.3387565612793 and batch: 550, loss is 4.808122873306274 and perplexity is 122.50145081547409
At time: 949.5113549232483 and batch: 600, loss is 4.779420595169068 and perplexity is 119.03536039741039
At time: 950.6830971240997 and batch: 650, loss is 4.776611204147339 and perplexity is 118.70141283892292
At time: 951.8606498241425 and batch: 700, loss is 4.769901332855224 and perplexity is 117.90760777971742
At time: 953.0129992961884 and batch: 750, loss is 4.78840217590332 and perplexity is 120.10930172295735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.980264353197675 and perplexity of 145.51284335509277
Annealing...
Model not improving. Stopping early with 145.27587218306loss at 47 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f819b44d860>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 2.5838881158966025, 'num_layers': 1, 'lr': 5.332471187800479, 'dropout': 0.6610078554862018}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7782680988311768 and batch: 50, loss is 7.3504808807373045 and perplexity is 1556.9450527320282
At time: 2.9492948055267334 and batch: 100, loss is 6.519188947677613 and perplexity is 678.0282458901872
At time: 4.116584300994873 and batch: 150, loss is 6.334912843704224 and perplexity is 563.9202516429295
At time: 5.2866926193237305 and batch: 200, loss is 6.279853458404541 and perplexity is 533.7104473142525
At time: 6.488133430480957 and batch: 250, loss is 6.25352261543274 and perplexity is 519.8408023798711
At time: 7.658579587936401 and batch: 300, loss is 6.189096632003785 and perplexity is 487.4056006542776
At time: 8.828151941299438 and batch: 350, loss is 6.086217765808105 and perplexity is 439.75500534673085
At time: 10.002769470214844 and batch: 400, loss is 6.08963623046875 and perplexity is 441.2608646888797
At time: 11.179847240447998 and batch: 450, loss is 6.041619338989258 and perplexity is 420.57353488076694
At time: 12.360971689224243 and batch: 500, loss is 6.017296285629272 and perplexity is 410.46730774232833
At time: 13.535924911499023 and batch: 550, loss is 6.025468130111694 and perplexity is 413.835325462098
At time: 14.723116874694824 and batch: 600, loss is 5.989687032699585 and perplexity is 399.28962577389143
At time: 15.890885353088379 and batch: 650, loss is 5.973971796035767 and perplexity is 393.06374354639394
At time: 17.063227653503418 and batch: 700, loss is 5.94366042137146 and perplexity is 381.3281998966201
At time: 18.23472285270691 and batch: 750, loss is 5.9185428714752195 and perplexity is 371.8694573096306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.356459151866824 and perplexity of 211.9730516782095
Finished 1 epochs...
Completing Train Step...
At time: 21.686967849731445 and batch: 50, loss is 5.627576808929444 and perplexity is 277.98768354922896
At time: 22.854963302612305 and batch: 100, loss is 5.491740531921387 and perplexity is 242.67923042588055
At time: 24.022732734680176 and batch: 150, loss is 5.429044637680054 and perplexity is 227.93138431631886
At time: 25.189769983291626 and batch: 200, loss is 5.377443122863769 and perplexity is 216.46808493128574
At time: 26.356689929962158 and batch: 250, loss is 5.366145496368408 and perplexity is 214.03627208179694
At time: 27.53498601913452 and batch: 300, loss is 5.35301812171936 and perplexity is 211.24489953216323
At time: 28.718075037002563 and batch: 350, loss is 5.25792308807373 and perplexity is 192.0821390456639
At time: 29.890151977539062 and batch: 400, loss is 5.269358196258545 and perplexity is 194.29122558587005
At time: 31.06047224998474 and batch: 450, loss is 5.195578842163086 and perplexity is 180.47257770568643
At time: 32.23534178733826 and batch: 500, loss is 5.1673561954498295 and perplexity is 175.45036729102966
At time: 33.40700435638428 and batch: 550, loss is 5.170093793869018 and perplexity is 175.93133799110174
At time: 34.5751314163208 and batch: 600, loss is 5.120410652160644 and perplexity is 167.40410036343926
At time: 35.74806451797485 and batch: 650, loss is 5.086459827423096 and perplexity is 161.81599031892142
At time: 36.945358991622925 and batch: 700, loss is 5.075371141433716 and perplexity is 160.03157530719747
At time: 38.14402961730957 and batch: 750, loss is 5.065110788345337 and perplexity is 158.39798975281394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.875947464344113 and perplexity of 131.09830535656448
Finished 2 epochs...
Completing Train Step...
At time: 41.57972955703735 and batch: 50, loss is 5.050486745834351 and perplexity is 156.09842625658678
At time: 42.78253984451294 and batch: 100, loss is 4.997366323471069 and perplexity is 148.02280111283648
At time: 43.95437717437744 and batch: 150, loss is 4.974570112228394 and perplexity is 144.68661277142343
At time: 45.13944149017334 and batch: 200, loss is 4.954453802108764 and perplexity is 141.8051315527723
At time: 46.31256985664368 and batch: 250, loss is 4.951971235275269 and perplexity is 141.453527457275
At time: 47.48635125160217 and batch: 300, loss is 4.979856405258179 and perplexity is 145.4534937970639
At time: 48.668174028396606 and batch: 350, loss is 4.909447984695435 and perplexity is 135.5645600380893
At time: 49.844430446624756 and batch: 400, loss is 4.9332849884033205 and perplexity is 138.83483488844598
At time: 51.02099061012268 and batch: 450, loss is 4.87390172958374 and perplexity is 130.83038713451757
At time: 52.20115089416504 and batch: 500, loss is 4.853542747497559 and perplexity is 128.1937443786378
At time: 53.37496566772461 and batch: 550, loss is 4.858385248184204 and perplexity is 128.81602816260448
At time: 54.553184032440186 and batch: 600, loss is 4.814533710479736 and perplexity is 123.28931039155324
At time: 55.73154878616333 and batch: 650, loss is 4.785435333251953 and perplexity is 119.75348441176381
At time: 56.90227961540222 and batch: 700, loss is 4.788373975753784 and perplexity is 120.10591467044608
At time: 58.07526421546936 and batch: 750, loss is 4.796536560058594 and perplexity is 121.09030142878207
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.762100574582122 and perplexity of 116.99141716959376
Finished 3 epochs...
Completing Train Step...
At time: 61.46121168136597 and batch: 50, loss is 4.815392122268677 and perplexity is 123.3951888261904
At time: 62.68803024291992 and batch: 100, loss is 4.774020299911499 and perplexity is 118.39426691025923
At time: 63.847832679748535 and batch: 150, loss is 4.7502223491668705 and perplexity is 115.60998745396188
At time: 65.0108892917633 and batch: 200, loss is 4.737297811508179 and perplexity is 114.12539631036951
At time: 66.17589330673218 and batch: 250, loss is 4.73058931350708 and perplexity is 113.36234863094042
At time: 67.34048700332642 and batch: 300, loss is 4.773419771194458 and perplexity is 118.32318909731659
At time: 68.50095319747925 and batch: 350, loss is 4.7114729404449465 and perplexity is 111.21585364949566
At time: 69.72349166870117 and batch: 400, loss is 4.732893314361572 and perplexity is 113.623836697744
At time: 70.89112997055054 and batch: 450, loss is 4.684304323196411 and perplexity is 108.23494956732819
At time: 72.0536859035492 and batch: 500, loss is 4.667600126266479 and perplexity is 106.44198833448124
At time: 73.21472263336182 and batch: 550, loss is 4.670826549530029 and perplexity is 106.78596985845935
At time: 74.38180685043335 and batch: 600, loss is 4.6312133407592775 and perplexity is 102.63852409379412
At time: 75.54582858085632 and batch: 650, loss is 4.607345428466797 and perplexity is 100.21776100349844
At time: 76.71505498886108 and batch: 700, loss is 4.608889493942261 and perplexity is 100.37262331629982
At time: 77.89037680625916 and batch: 750, loss is 4.626396570205689 and perplexity is 102.14532663633766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.703719737917878 and perplexity of 110.35690870174804
Finished 4 epochs...
Completing Train Step...
At time: 81.26579117774963 and batch: 50, loss is 4.656962785720825 and perplexity is 105.31572947566805
At time: 82.46885776519775 and batch: 100, loss is 4.62454249382019 and perplexity is 101.95611685722614
At time: 83.63879466056824 and batch: 150, loss is 4.59466516494751 and perplexity is 98.9549963984899
At time: 84.81110906600952 and batch: 200, loss is 4.589855165481567 and perplexity is 98.48016580159631
At time: 85.98830032348633 and batch: 250, loss is 4.581950426101685 and perplexity is 97.70477442772813
At time: 87.16013479232788 and batch: 300, loss is 4.628442573547363 and perplexity is 102.35453025864776
At time: 88.3352563381195 and batch: 350, loss is 4.572929334640503 and perplexity is 96.82733440527296
At time: 89.5125560760498 and batch: 400, loss is 4.592861461639404 and perplexity is 98.77667181479906
At time: 90.68621253967285 and batch: 450, loss is 4.547121248245239 and perplexity is 94.36037684682849
At time: 91.86274981498718 and batch: 500, loss is 4.530270147323608 and perplexity is 92.78362294417488
At time: 93.03384375572205 and batch: 550, loss is 4.534256658554077 and perplexity is 93.1542441510128
At time: 94.20697569847107 and batch: 600, loss is 4.497283973693848 and perplexity is 89.77297412265571
At time: 95.39019107818604 and batch: 650, loss is 4.476595935821533 and perplexity is 87.93482682390143
At time: 96.56044864654541 and batch: 700, loss is 4.476100597381592 and perplexity is 87.89128011003706
At time: 97.73314547538757 and batch: 750, loss is 4.500955324172974 and perplexity is 90.10316793192845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.673883837322856 and perplexity of 107.11294487422846
Finished 5 epochs...
Completing Train Step...
At time: 101.11771607398987 and batch: 50, loss is 4.534247388839722 and perplexity is 93.1533806417808
At time: 102.31733632087708 and batch: 100, loss is 4.511703062057495 and perplexity is 91.07679594046532
At time: 103.49154305458069 and batch: 150, loss is 4.480809068679809 and perplexity is 88.30608947243758
At time: 104.66440439224243 and batch: 200, loss is 4.481823625564576 and perplexity is 88.3957264867148
At time: 105.85244584083557 and batch: 250, loss is 4.466555042266846 and perplexity is 87.05630056338518
At time: 107.02879238128662 and batch: 300, loss is 4.51528920173645 and perplexity is 91.40399639482509
At time: 108.2092547416687 and batch: 350, loss is 4.464351825714111 and perplexity is 86.86470781861928
At time: 109.38210988044739 and batch: 400, loss is 4.479757528305054 and perplexity is 88.21328085857469
At time: 110.56860828399658 and batch: 450, loss is 4.43766448020935 and perplexity is 84.5771791488369
At time: 111.7538526058197 and batch: 500, loss is 4.4247653198242185 and perplexity is 83.49321072130739
At time: 112.92533135414124 and batch: 550, loss is 4.429835996627808 and perplexity is 83.91765300355706
At time: 114.11283111572266 and batch: 600, loss is 4.390742835998535 and perplexity is 80.70034384104888
At time: 115.2900743484497 and batch: 650, loss is 4.371353712081909 and perplexity is 79.15070647665229
At time: 116.46329617500305 and batch: 700, loss is 4.372774677276611 and perplexity is 79.26325682181535
At time: 117.6383044719696 and batch: 750, loss is 4.394534893035889 and perplexity is 81.00694510516618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.660600174305051 and perplexity of 105.69950124820708
Finished 6 epochs...
Completing Train Step...
At time: 121.0000650882721 and batch: 50, loss is 4.434287853240967 and perplexity is 84.29207518057413
At time: 122.20358371734619 and batch: 100, loss is 4.416316051483154 and perplexity is 82.79072609900672
At time: 123.37956643104553 and batch: 150, loss is 4.386133050918579 and perplexity is 80.32918872996707
At time: 124.5528883934021 and batch: 200, loss is 4.388578033447265 and perplexity is 80.5258324902742
At time: 125.72143507003784 and batch: 250, loss is 4.371103200912476 and perplexity is 79.13088082398875
At time: 126.89552354812622 and batch: 300, loss is 4.422223119735718 and perplexity is 83.28122384431994
At time: 128.07175302505493 and batch: 350, loss is 4.373860340118409 and perplexity is 79.34935672374182
At time: 129.2449975013733 and batch: 400, loss is 4.385341968536377 and perplexity is 80.2656668528179
At time: 130.46140933036804 and batch: 450, loss is 4.343946027755737 and perplexity is 77.01082741563954
At time: 131.63136625289917 and batch: 500, loss is 4.336700944900513 and perplexity is 76.45489391420425
At time: 132.80410265922546 and batch: 550, loss is 4.339691696166992 and perplexity is 76.68389375509624
At time: 133.97952508926392 and batch: 600, loss is 4.303357801437378 and perplexity is 73.94767891457506
At time: 135.15869975090027 and batch: 650, loss is 4.285098514556885 and perplexity is 72.6096994797929
At time: 136.32912158966064 and batch: 700, loss is 4.285826768875122 and perplexity is 72.66259706609492
At time: 137.49786233901978 and batch: 750, loss is 4.308523082733155 and perplexity is 74.33062764486907
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.654153158498365 and perplexity of 105.02024682760423
Finished 7 epochs...
Completing Train Step...
At time: 140.88181710243225 and batch: 50, loss is 4.350600547790528 and perplexity is 77.52500641938153
At time: 142.08254313468933 and batch: 100, loss is 4.334218444824219 and perplexity is 76.26533002776426
At time: 143.2517809867859 and batch: 150, loss is 4.303113956451416 and perplexity is 73.92964934214795
At time: 144.42827320098877 and batch: 200, loss is 4.308029251098633 and perplexity is 74.29392989152572
At time: 145.6027991771698 and batch: 250, loss is 4.288697910308838 and perplexity is 72.87152144140653
At time: 146.7742280960083 and batch: 300, loss is 4.341563110351562 and perplexity is 76.82753544622504
At time: 147.94495248794556 and batch: 350, loss is 4.296314296722412 and perplexity is 73.42865809802198
At time: 149.12012362480164 and batch: 400, loss is 4.309657554626465 and perplexity is 74.41500150358064
At time: 150.29453229904175 and batch: 450, loss is 4.270227584838867 and perplexity is 71.53791471150419
At time: 151.46950221061707 and batch: 500, loss is 4.261144037246704 and perplexity is 70.89103906926746
At time: 152.64925479888916 and batch: 550, loss is 4.2677084255218505 and perplexity is 71.35792611230347
At time: 153.82793927192688 and batch: 600, loss is 4.2308541202545165 and perplexity is 68.77595002616428
At time: 154.99795389175415 and batch: 650, loss is 4.21177565574646 and perplexity is 67.47624808598317
At time: 156.1666395664215 and batch: 700, loss is 4.213968877792358 and perplexity is 67.6244008874677
At time: 157.34087347984314 and batch: 750, loss is 4.23298638343811 and perplexity is 68.92275491002499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.656987744708394 and perplexity of 105.31835808245432
Annealing...
Finished 8 epochs...
Completing Train Step...
At time: 160.74169945716858 and batch: 50, loss is 4.274154291152954 and perplexity is 71.81937533785391
At time: 161.94203901290894 and batch: 100, loss is 4.258260431289673 and perplexity is 70.68691169960314
At time: 163.11644577980042 and batch: 150, loss is 4.223532738685608 and perplexity is 68.27425384884893
At time: 164.29101419448853 and batch: 200, loss is 4.213670167922974 and perplexity is 67.60420382819206
At time: 165.47035241127014 and batch: 250, loss is 4.182093629837036 and perplexity is 65.50284847326245
At time: 166.64113664627075 and batch: 300, loss is 4.223839626312256 and perplexity is 68.2952095879378
At time: 167.81110310554504 and batch: 350, loss is 4.172906312942505 and perplexity is 64.90380904296052
At time: 168.99491333961487 and batch: 400, loss is 4.168203520774841 and perplexity is 64.59929650894453
At time: 170.1706645488739 and batch: 450, loss is 4.122408938407898 and perplexity is 61.707713444646664
At time: 171.35354208946228 and batch: 500, loss is 4.105167922973632 and perplexity is 60.65292871265338
At time: 172.52808618545532 and batch: 550, loss is 4.097447848320007 and perplexity is 60.186486376502934
At time: 173.70483422279358 and batch: 600, loss is 4.051997218132019 and perplexity is 57.512206839430156
At time: 174.87574100494385 and batch: 650, loss is 4.020875382423401 and perplexity is 55.749886943751505
At time: 176.04904127120972 and batch: 700, loss is 4.01125286102295 and perplexity is 55.21600522715
At time: 177.22245836257935 and batch: 750, loss is 4.02378427028656 and perplexity is 55.91229320954399
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.605647952057595 and perplexity of 100.047788021789
Finished 9 epochs...
Completing Train Step...
At time: 180.63219022750854 and batch: 50, loss is 4.184322538375855 and perplexity is 65.64901116266294
At time: 181.83612632751465 and batch: 100, loss is 4.17304407119751 and perplexity is 64.91275069431646
At time: 183.0143346786499 and batch: 150, loss is 4.141864953041076 and perplexity is 62.92005504564895
At time: 184.19217729568481 and batch: 200, loss is 4.137226648330689 and perplexity is 62.628888439682726
At time: 185.36216807365417 and batch: 250, loss is 4.11162380695343 and perplexity is 61.045763667508425
At time: 186.53145503997803 and batch: 300, loss is 4.159381561279297 and perplexity is 64.03191053954491
At time: 187.7064244747162 and batch: 350, loss is 4.113753476142883 and perplexity is 61.175909484116055
At time: 188.9652681350708 and batch: 400, loss is 4.114476752281189 and perplexity is 61.220172564970554
At time: 190.17335104942322 and batch: 450, loss is 4.0735722923278805 and perplexity is 58.766519256084486
At time: 191.3653826713562 and batch: 500, loss is 4.061554245948791 and perplexity is 58.06448747708534
At time: 192.55018401145935 and batch: 550, loss is 4.058981966972351 and perplexity is 57.91532134736796
At time: 193.74751019477844 and batch: 600, loss is 4.018693237304688 and perplexity is 55.62836523730907
At time: 194.92573380470276 and batch: 650, loss is 3.992747826576233 and perplexity is 54.20362708403776
At time: 196.10491061210632 and batch: 700, loss is 3.987606177330017 and perplexity is 53.925646297952866
At time: 197.2875735759735 and batch: 750, loss is 4.004154005050659 and perplexity is 54.82542274331806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.605296999909157 and perplexity of 100.01268219622911
Finished 10 epochs...
Completing Train Step...
At time: 200.6878900527954 and batch: 50, loss is 4.138744201660156 and perplexity is 62.72400327051544
At time: 201.91339445114136 and batch: 100, loss is 4.129021573066711 and perplexity is 62.117116132662886
At time: 203.08749318122864 and batch: 150, loss is 4.0995712041854855 and perplexity is 60.31441948105997
At time: 204.25405406951904 and batch: 200, loss is 4.095922732353211 and perplexity is 60.09476496581724
At time: 205.4219889640808 and batch: 250, loss is 4.071795692443848 and perplexity is 58.66220735248374
At time: 206.5886948108673 and batch: 300, loss is 4.121393074989319 and perplexity is 61.64505866565128
At time: 207.75909948349 and batch: 350, loss is 4.0778283548355105 and perplexity is 59.01716624166027
At time: 208.93102431297302 and batch: 400, loss is 4.08054744720459 and perplexity is 59.17785773653586
At time: 210.0989682674408 and batch: 450, loss is 4.0406717014312745 and perplexity is 56.86452596367093
At time: 211.26703643798828 and batch: 500, loss is 4.03028814792633 and perplexity is 56.27712504791151
At time: 212.43305921554565 and batch: 550, loss is 4.029642510414123 and perplexity is 56.240802151875286
At time: 213.6062035560608 and batch: 600, loss is 3.989654531478882 and perplexity is 54.036218325952575
At time: 214.77506637573242 and batch: 650, loss is 3.966448006629944 and perplexity is 52.79666398584865
At time: 215.9443483352661 and batch: 700, loss is 3.96239625453949 and perplexity is 52.58317778082515
At time: 217.11067390441895 and batch: 750, loss is 3.9806812477111815 and perplexity is 53.55350500597454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.608291980832122 and perplexity of 100.31266727201431
Annealing...
Finished 11 epochs...
Completing Train Step...
At time: 220.48951148986816 and batch: 50, loss is 4.112373285293579 and perplexity is 61.091533294664686
At time: 221.7208263874054 and batch: 100, loss is 4.11160861492157 and perplexity is 61.04483626536645
At time: 222.8989474773407 and batch: 150, loss is 4.079668083190918 and perplexity is 59.12584173188938
At time: 224.07092428207397 and batch: 200, loss is 4.073770642280579 and perplexity is 58.77817674849248
At time: 225.24681687355042 and batch: 250, loss is 4.042701368331909 and perplexity is 56.9800592171879
At time: 226.42728066444397 and batch: 300, loss is 4.084628577232361 and perplexity is 59.41986376185536
At time: 227.59908866882324 and batch: 350, loss is 4.041662158966065 and perplexity is 56.92087576330912
At time: 228.7777168750763 and batch: 400, loss is 4.035806827545166 and perplexity is 56.58855903269779
At time: 229.95287013053894 and batch: 450, loss is 3.988280873298645 and perplexity is 53.96204199074871
At time: 231.12895846366882 and batch: 500, loss is 3.9703819656372072 and perplexity is 53.0047729754145
At time: 232.30226945877075 and batch: 550, loss is 3.9639336967468264 and perplexity is 52.664083555781275
At time: 233.47643566131592 and batch: 600, loss is 3.920507302284241 and perplexity is 50.426019525343534
At time: 234.65046501159668 and batch: 650, loss is 3.8906668615341187 and perplexity is 48.943514190413
At time: 235.82707023620605 and batch: 700, loss is 3.879341378211975 and perplexity is 48.39233232871818
At time: 237.0059187412262 and batch: 750, loss is 3.895188126564026 and perplexity is 49.16530179201443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.580611118050509 and perplexity of 97.57400522636813
Finished 12 epochs...
Completing Train Step...
At time: 240.40452003479004 and batch: 50, loss is 4.081657285690308 and perplexity is 59.24357205995154
At time: 241.62038111686707 and batch: 100, loss is 4.074643301963806 and perplexity is 58.82949248092334
At time: 242.79463076591492 and batch: 150, loss is 4.042401056289673 and perplexity is 56.96294998841997
At time: 243.97046613693237 and batch: 200, loss is 4.037809252738953 and perplexity is 56.70198691651003
At time: 245.13948249816895 and batch: 250, loss is 4.009424452781677 and perplexity is 55.11514006758703
At time: 246.31304025650024 and batch: 300, loss is 4.055637707710266 and perplexity is 57.721961001312444
At time: 247.48865127563477 and batch: 350, loss is 4.016310691833496 and perplexity is 55.49598589017131
At time: 248.6591136455536 and batch: 400, loss is 4.013143000602722 and perplexity is 55.32046987935254
At time: 249.8627429008484 and batch: 450, loss is 3.969526572227478 and perplexity is 52.95945242813927
At time: 251.0359148979187 and batch: 500, loss is 3.954295611381531 and perplexity is 52.15894083743888
At time: 252.21245503425598 and batch: 550, loss is 3.9514439344406127 and perplexity is 52.01041226699335
At time: 253.38490414619446 and batch: 600, loss is 3.9112502241134646 and perplexity is 49.961375860194856
At time: 254.56229734420776 and batch: 650, loss is 3.8844304275512695 and perplexity is 48.63923100249191
At time: 255.73694968223572 and batch: 700, loss is 3.8763821744918823 and perplexity is 48.249341233104495
At time: 256.9102747440338 and batch: 750, loss is 3.8939003944396973 and perplexity is 49.102030800282115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.58115972474564 and perplexity of 97.62754966498187
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 260.3006739616394 and batch: 50, loss is 4.071184043884277 and perplexity is 58.62633766879382
At time: 261.528626203537 and batch: 100, loss is 4.075451250076294 and perplexity is 58.87704286486675
At time: 262.6972351074219 and batch: 150, loss is 4.040830416679382 and perplexity is 56.873551947279445
At time: 263.8673689365387 and batch: 200, loss is 4.035176134109497 and perplexity is 56.55288025235085
At time: 265.0407829284668 and batch: 250, loss is 4.004755272865295 and perplexity is 54.85839741774922
At time: 266.219651222229 and batch: 300, loss is 4.049164505004883 and perplexity is 57.349521784975806
At time: 267.3949365615845 and batch: 350, loss is 4.009199471473694 and perplexity is 55.10274158605052
At time: 268.6036059856415 and batch: 400, loss is 4.004494915008545 and perplexity is 54.84411646213294
At time: 269.8094274997711 and batch: 450, loss is 3.954012703895569 and perplexity is 52.14418676973263
At time: 271.0229229927063 and batch: 500, loss is 3.9321822929382324 and perplexity is 51.01819291506087
At time: 272.19882893562317 and batch: 550, loss is 3.9265529823303225 and perplexity is 50.731802507056976
At time: 273.37880277633667 and batch: 600, loss is 3.884176731109619 and perplexity is 48.6268929677859
At time: 274.55206537246704 and batch: 650, loss is 3.8547171115875245 and perplexity is 47.215258402381814
At time: 275.72768092155457 and batch: 700, loss is 3.8444680976867676 and perplexity is 46.73381991185477
At time: 276.90435242652893 and batch: 750, loss is 3.8611238193511963 and perplexity is 47.5187238341205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.568219561909521 and perplexity of 96.37237189209274
Finished 14 epochs...
Completing Train Step...
At time: 280.31188797950745 and batch: 50, loss is 4.05845374584198 and perplexity is 57.884737329133515
At time: 281.5427851676941 and batch: 100, loss is 4.0571193027496335 and perplexity is 57.8075449571211
At time: 282.7173640727997 and batch: 150, loss is 4.020933604240417 and perplexity is 55.75313289795959
At time: 283.8876118659973 and batch: 200, loss is 4.01520945072174 and perplexity is 55.43490506750106
At time: 285.0610764026642 and batch: 250, loss is 3.9862570905685426 and perplexity is 53.852944973652015
At time: 286.23602509498596 and batch: 300, loss is 4.0323872566223145 and perplexity is 56.39538092303032
At time: 287.4089217185974 and batch: 350, loss is 3.994256296157837 and perplexity is 54.28545330737492
At time: 288.5827865600586 and batch: 400, loss is 3.9923926639556884 and perplexity is 54.18437940003085
At time: 289.7574439048767 and batch: 450, loss is 3.9449468898773192 and perplexity is 51.67359364833026
At time: 290.92766070365906 and batch: 500, loss is 3.925418944358826 and perplexity is 50.67430332594106
At time: 292.08637142181396 and batch: 550, loss is 3.922402548789978 and perplexity is 50.521679883993386
At time: 293.263475894928 and batch: 600, loss is 3.882020659446716 and perplexity is 48.52216284519891
At time: 294.44733476638794 and batch: 650, loss is 3.8543914365768432 and perplexity is 47.1998840762501
At time: 295.6174132823944 and batch: 700, loss is 3.846516089439392 and perplexity is 46.829628463677636
At time: 296.7866859436035 and batch: 750, loss is 3.864588737487793 and perplexity is 47.683657898665274
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.567892030228016 and perplexity of 96.34081205578161
Finished 15 epochs...
Completing Train Step...
At time: 300.19074749946594 and batch: 50, loss is 4.048808808326721 and perplexity is 57.32912637808703
At time: 301.3964638710022 and batch: 100, loss is 4.047018446922302 and perplexity is 57.226578349279315
At time: 302.5689113140106 and batch: 150, loss is 4.010846943855285 and perplexity is 55.19359665101902
At time: 303.7428398132324 and batch: 200, loss is 4.0052204513549805 and perplexity is 54.883922300559504
At time: 304.913845539093 and batch: 250, loss is 3.9766879320144652 and perplexity is 53.34007538337352
At time: 306.09225583076477 and batch: 300, loss is 4.02366512298584 and perplexity is 55.90563180758288
At time: 307.26403856277466 and batch: 350, loss is 3.9864964437484742 and perplexity is 53.865836390019346
At time: 308.4911918640137 and batch: 400, loss is 3.985624418258667 and perplexity is 53.81888448226293
At time: 309.6672420501709 and batch: 450, loss is 3.9393154096603396 and perplexity is 51.38341266907061
At time: 310.84231090545654 and batch: 500, loss is 3.9205035257339476 and perplexity is 50.425829089304294
At time: 312.01293110847473 and batch: 550, loss is 3.9187302350997926 and perplexity is 50.336488675531584
At time: 313.19008469581604 and batch: 600, loss is 3.8791527938842774 and perplexity is 48.38320715371978
At time: 314.36346101760864 and batch: 650, loss is 3.852236452102661 and perplexity is 47.098278577339876
At time: 315.5389919281006 and batch: 700, loss is 3.8454949378967287 and perplexity is 46.78183272382959
At time: 316.71570348739624 and batch: 750, loss is 3.864513554573059 and perplexity is 47.68007303704114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.568241917809774 and perplexity of 96.37452640730895
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 320.0784945487976 and batch: 50, loss is 4.045659899711609 and perplexity is 57.14888612711063
At time: 321.28374099731445 and batch: 100, loss is 4.050385322570801 and perplexity is 57.41957784269991
At time: 322.45815682411194 and batch: 150, loss is 4.017319030761719 and perplexity is 55.55197287528905
At time: 323.63357973098755 and batch: 200, loss is 4.010342745780945 and perplexity is 55.16577516023209
At time: 324.81363224983215 and batch: 250, loss is 3.97997896194458 and perplexity is 53.5159083450024
At time: 325.9871723651886 and batch: 300, loss is 4.025586109161377 and perplexity is 56.01312897064497
At time: 327.16092109680176 and batch: 350, loss is 3.9869658088684083 and perplexity is 53.891125069124584
At time: 328.3414361476898 and batch: 400, loss is 3.9861716508865355 and perplexity is 53.84834399171433
At time: 329.516117811203 and batch: 450, loss is 3.9371446657180784 and perplexity is 51.2719934123766
At time: 330.68900060653687 and batch: 500, loss is 3.912343153953552 and perplexity is 50.01600998892073
At time: 331.8618404865265 and batch: 550, loss is 3.9073564910888674 and perplexity is 49.767217846094404
At time: 333.0345058441162 and batch: 600, loss is 3.8671102142333984 and perplexity is 47.804042843339026
At time: 334.2082633972168 and batch: 650, loss is 3.8376087665557863 and perplexity is 46.4143540796105
At time: 335.3892340660095 and batch: 700, loss is 3.8293938732147215 and perplexity is 46.034626955734076
At time: 336.5672197341919 and batch: 750, loss is 3.8503248357772826 and perplexity is 47.00833073938313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.561543309411337 and perplexity of 95.73110860319193
Finished 17 epochs...
Completing Train Step...
At time: 339.93893361091614 and batch: 50, loss is 4.041170115470886 and perplexity is 56.892875105985496
At time: 341.1424369812012 and batch: 100, loss is 4.040972642898559 and perplexity is 56.88164143279891
At time: 342.3172070980072 and batch: 150, loss is 4.006288428306579 and perplexity is 54.94256837534373
At time: 343.4907605648041 and batch: 200, loss is 3.999575514793396 and perplexity is 54.57497884440858
At time: 344.6619427204132 and batch: 250, loss is 3.969738826751709 and perplexity is 52.970694504566524
At time: 345.8312382698059 and batch: 300, loss is 4.0165160322189335 and perplexity is 55.50738262736939
At time: 347.01248931884766 and batch: 350, loss is 3.9787904834747314 and perplexity is 53.452343620275364
At time: 348.180983543396 and batch: 400, loss is 3.9795835971832276 and perplexity is 53.49475422274383
At time: 349.3476972579956 and batch: 450, loss is 3.9325419569015505 and perplexity is 51.03654562073162
At time: 350.51144790649414 and batch: 500, loss is 3.9099401473999023 and perplexity is 49.89596548076567
At time: 351.68314266204834 and batch: 550, loss is 3.906651487350464 and perplexity is 49.7321441364639
At time: 352.8510890007019 and batch: 600, loss is 3.8678674793243406 and perplexity is 47.84025688627487
At time: 354.02344703674316 and batch: 650, loss is 3.839246253967285 and perplexity is 46.49041926102067
At time: 355.1883180141449 and batch: 700, loss is 3.832135715484619 and perplexity is 46.16101983729095
At time: 356.3566174507141 and batch: 750, loss is 3.853206214904785 and perplexity is 47.14397488965898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.561248424441316 and perplexity of 95.70288309994426
Finished 18 epochs...
Completing Train Step...
At time: 359.8025119304657 and batch: 50, loss is 4.037282104492188 and perplexity is 56.67210444045832
At time: 361.0046226978302 and batch: 100, loss is 4.036143207550049 and perplexity is 56.607597494360945
At time: 362.17463207244873 and batch: 150, loss is 4.001178169250489 and perplexity is 54.66251380290886
At time: 363.3503053188324 and batch: 200, loss is 3.9944908905029295 and perplexity is 54.29818986164501
At time: 364.524480342865 and batch: 250, loss is 3.9648897743225096 and perplexity is 52.71445858249232
At time: 365.6982867717743 and batch: 300, loss is 4.012107462882995 and perplexity is 55.26321309701495
At time: 366.87876319885254 and batch: 350, loss is 3.9748226737976076 and perplexity is 53.24067510189212
At time: 368.0556607246399 and batch: 400, loss is 3.976257281303406 and perplexity is 53.317109387497865
At time: 369.29010224342346 and batch: 450, loss is 3.930111985206604 and perplexity is 50.91267881682944
At time: 370.46596097946167 and batch: 500, loss is 3.908281545639038 and perplexity is 49.81327653752884
At time: 371.64100909233093 and batch: 550, loss is 3.905696439743042 and perplexity is 49.68467024471623
At time: 372.8116936683655 and batch: 600, loss is 3.867525691986084 and perplexity is 47.823908486209426
At time: 373.98519682884216 and batch: 650, loss is 3.8392016744613646 and perplexity is 46.48834678729525
At time: 375.15607047080994 and batch: 700, loss is 3.8326666450500486 and perplexity is 46.18553459472184
At time: 376.33126163482666 and batch: 750, loss is 3.853899154663086 and perplexity is 47.17665414529449
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.561320460119913 and perplexity of 95.70977737038582
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 379.7125794887543 and batch: 50, loss is 4.036261186599732 and perplexity is 56.614276398895804
At time: 380.9164791107178 and batch: 100, loss is 4.0379715204238895 and perplexity is 56.71118856320305
At time: 382.0931005477905 and batch: 150, loss is 4.004881615638733 and perplexity is 54.86532881768226
At time: 383.26447105407715 and batch: 200, loss is 3.9977583122253417 and perplexity is 54.47589510757791
At time: 384.4339292049408 and batch: 250, loss is 3.9678961277008056 and perplexity is 52.87317533291398
At time: 385.60747361183167 and batch: 300, loss is 4.013436360359192 and perplexity is 55.33670105959529
At time: 386.77655148506165 and batch: 350, loss is 3.9749293661117555 and perplexity is 53.24635577576227
At time: 387.9474413394928 and batch: 400, loss is 3.9760581970214846 and perplexity is 53.30649584559108
At time: 389.1167256832123 and batch: 450, loss is 3.9290099573135375 and perplexity is 50.856602529154365
At time: 390.2851574420929 and batch: 500, loss is 3.9045983123779298 and perplexity is 49.63014009469867
At time: 391.4554512500763 and batch: 550, loss is 3.9006046724319456 and perplexity is 49.432330437873375
At time: 392.6366605758667 and batch: 600, loss is 3.861158056259155 and perplexity is 47.520350756144964
At time: 393.81107878685 and batch: 650, loss is 3.8306333208084107 and perplexity is 46.09171983784943
At time: 394.98281264305115 and batch: 700, loss is 3.8227275705337522 and perplexity is 45.728766809488924
At time: 396.15531826019287 and batch: 750, loss is 3.84460657119751 and perplexity is 46.74029175604752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558834342069404 and perplexity of 95.47212710097273
Finished 20 epochs...
Completing Train Step...
At time: 399.52389311790466 and batch: 50, loss is 4.034302449226379 and perplexity is 56.50349243361451
At time: 400.7572455406189 and batch: 100, loss is 4.033736801147461 and perplexity is 56.47154037932825
At time: 401.9312241077423 and batch: 150, loss is 3.9998127126693728 and perplexity is 54.58792544886478
At time: 403.1092767715454 and batch: 200, loss is 3.993113331794739 and perplexity is 54.223442413681525
At time: 404.2833921909332 and batch: 250, loss is 3.963286280632019 and perplexity is 52.62999901404703
At time: 405.455792427063 and batch: 300, loss is 4.009362392425537 and perplexity is 55.111719708501155
At time: 406.626193523407 and batch: 350, loss is 3.971375708580017 and perplexity is 53.05747227493572
At time: 407.7978057861328 and batch: 400, loss is 3.973156633377075 and perplexity is 53.152047833961454
At time: 408.9750483036041 and batch: 450, loss is 3.927063307762146 and perplexity is 50.75769884329604
At time: 410.1502785682678 and batch: 500, loss is 3.9037461423873903 and perplexity is 49.58786479411329
At time: 411.3204674720764 and batch: 550, loss is 3.900804891586304 and perplexity is 49.442228728152195
At time: 412.4916055202484 and batch: 600, loss is 3.8621465730667115 and perplexity is 47.56734864684376
At time: 413.66331911087036 and batch: 650, loss is 3.832180151939392 and perplexity is 46.16307111493663
At time: 414.8360266685486 and batch: 700, loss is 3.8248349475860595 and perplexity is 45.82523617625643
At time: 416.0083692073822 and batch: 750, loss is 3.846676745414734 and perplexity is 46.837152527690094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.55856749068859 and perplexity of 95.4466536309931
Finished 21 epochs...
Completing Train Step...
At time: 419.362096786499 and batch: 50, loss is 4.032736783027649 and perplexity is 56.41509604307861
At time: 420.63077425956726 and batch: 100, loss is 4.031327447891235 and perplexity is 56.3356442662421
At time: 421.84480571746826 and batch: 150, loss is 3.9970893049240113 and perplexity is 54.43946252419543
At time: 423.06672501564026 and batch: 200, loss is 3.9904758739471435 and perplexity is 54.080618798388684
At time: 424.25538992881775 and batch: 250, loss is 3.9607533597946167 and perplexity is 52.496860079244485
At time: 425.4469816684723 and batch: 300, loss is 4.007104725837707 and perplexity is 54.98743616850531
At time: 426.6359567642212 and batch: 350, loss is 3.9694052028656004 and perplexity is 52.95302516323709
At time: 427.8955137729645 and batch: 400, loss is 3.971552724838257 and perplexity is 53.06686514147002
At time: 429.077223777771 and batch: 450, loss is 3.925971059799194 and perplexity is 50.70228911621977
At time: 430.25753951072693 and batch: 500, loss is 3.9032142257690428 and perplexity is 49.56149519859571
At time: 431.4350643157959 and batch: 550, loss is 3.9008120250701905 and perplexity is 49.44258142475212
At time: 432.6156060695648 and batch: 600, loss is 3.8625254011154175 and perplexity is 47.58537190635634
At time: 433.7971160411835 and batch: 650, loss is 3.8327599477767946 and perplexity is 46.18984403207375
At time: 434.9722545146942 and batch: 700, loss is 3.82573326587677 and perplexity is 45.86642031955475
At time: 436.14410972595215 and batch: 750, loss is 3.847533664703369 and perplexity is 46.87730538853924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558523133743641 and perplexity of 95.44242000292851
Finished 22 epochs...
Completing Train Step...
At time: 439.58023047447205 and batch: 50, loss is 4.031210012435913 and perplexity is 56.32902885265649
At time: 440.7848415374756 and batch: 100, loss is 4.029421019554138 and perplexity is 56.228346707674845
At time: 441.9635932445526 and batch: 150, loss is 3.9950881481170653 and perplexity is 54.33062955543641
At time: 443.1377754211426 and batch: 200, loss is 3.988517198562622 and perplexity is 53.974796091565686
At time: 444.3143537044525 and batch: 250, loss is 3.9588865566253664 and perplexity is 52.39895019215091
At time: 445.49269247055054 and batch: 300, loss is 4.005423517227173 and perplexity is 54.895068483777195
At time: 446.66656947135925 and batch: 350, loss is 3.9679384088516234 and perplexity is 52.87541091887569
At time: 447.8415949344635 and batch: 400, loss is 3.970328297615051 and perplexity is 53.00192839041638
At time: 449.01808857917786 and batch: 450, loss is 3.9250767707824705 and perplexity is 50.656966884547934
At time: 450.198175907135 and batch: 500, loss is 3.902670450210571 and perplexity is 49.53455219500272
At time: 451.3788356781006 and batch: 550, loss is 3.900630679130554 and perplexity is 49.43361602630947
At time: 452.5545184612274 and batch: 600, loss is 3.862580065727234 and perplexity is 47.587973213338806
At time: 453.73421454429626 and batch: 650, loss is 3.8329136753082276 and perplexity is 46.19694522858479
At time: 454.9112780094147 and batch: 700, loss is 3.826129503250122 and perplexity is 45.88459791054985
At time: 456.0884132385254 and batch: 750, loss is 3.8479192733764647 and perplexity is 46.895385169704596
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558549038199491 and perplexity of 95.44489241890682
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 459.48681831359863 and batch: 50, loss is 4.030882539749146 and perplexity is 56.3105856542218
At time: 460.73125171661377 and batch: 100, loss is 4.030277438163758 and perplexity is 56.27652233649142
At time: 461.91043162345886 and batch: 150, loss is 3.9968460273742674 and perplexity is 54.42622023598419
At time: 463.1015348434448 and batch: 200, loss is 3.990328826904297 and perplexity is 54.072666987978494
At time: 464.28733587265015 and batch: 250, loss is 3.960736036300659 and perplexity is 52.49595065808331
At time: 465.46283316612244 and batch: 300, loss is 4.006234946250916 and perplexity is 54.939630012419144
At time: 466.63414669036865 and batch: 350, loss is 3.9676168394088744 and perplexity is 52.8584105359997
At time: 467.8106951713562 and batch: 400, loss is 3.969362692832947 and perplexity is 52.950774176253404
At time: 468.9893000125885 and batch: 450, loss is 3.9237586069107055 and perplexity is 50.59023669127478
At time: 470.17201948165894 and batch: 500, loss is 3.900018348693848 and perplexity is 49.40335558426047
At time: 471.3571071624756 and batch: 550, loss is 3.897429494857788 and perplexity is 49.2756229295689
At time: 472.5458753108978 and batch: 600, loss is 3.8592185831069945 and perplexity is 47.4282756291527
At time: 473.7353837490082 and batch: 650, loss is 3.8285577201843264 and perplexity is 45.9961510510158
At time: 474.9114010334015 and batch: 700, loss is 3.820921998023987 and perplexity is 45.64627470038546
At time: 476.10402822494507 and batch: 750, loss is 3.842776827812195 and perplexity is 46.6548472110135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558299219885538 and perplexity of 95.42105151487854
Finished 24 epochs...
Completing Train Step...
At time: 479.51031851768494 and batch: 50, loss is 4.0298376607894895 and perplexity is 56.25177863652383
At time: 480.7317452430725 and batch: 100, loss is 4.028697538375854 and perplexity is 56.18768126925926
At time: 481.89932346343994 and batch: 150, loss is 3.994953303337097 and perplexity is 54.3233038475763
At time: 483.06812715530396 and batch: 200, loss is 3.988615846633911 and perplexity is 53.98012086373327
At time: 484.24092841148376 and batch: 250, loss is 3.9590729093551635 and perplexity is 52.40871578945232
At time: 485.4071092605591 and batch: 300, loss is 4.004852066040039 and perplexity is 54.86370759318686
At time: 486.57745456695557 and batch: 350, loss is 3.9665376710891724 and perplexity is 52.801398182415426
At time: 487.7437002658844 and batch: 400, loss is 3.968541111946106 and perplexity is 52.90728869811165
At time: 488.9669072628021 and batch: 450, loss is 3.9232704305648802 and perplexity is 50.56554576164665
At time: 490.1380605697632 and batch: 500, loss is 3.899916729927063 and perplexity is 49.39833553126108
At time: 491.3129336833954 and batch: 550, loss is 3.897742495536804 and perplexity is 49.29104864700887
At time: 492.4819803237915 and batch: 600, loss is 3.8597984075546266 and perplexity is 47.45578367702017
At time: 493.65264678001404 and batch: 650, loss is 3.8292197799682617 and perplexity is 46.02661333565659
At time: 494.81875920295715 and batch: 700, loss is 3.8216665029525756 and perplexity is 45.68027123059345
At time: 495.9873778820038 and batch: 750, loss is 3.8434332275390624 and perplexity is 46.6854814930471
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558162955350654 and perplexity of 95.40804989552564
Finished 25 epochs...
Completing Train Step...
At time: 499.38250517845154 and batch: 50, loss is 4.029099888801575 and perplexity is 56.210292955343924
At time: 500.5906620025635 and batch: 100, loss is 4.027602591514587 and perplexity is 56.12619241366963
At time: 501.76413011550903 and batch: 150, loss is 3.9937002515792845 and perplexity is 54.255276565953345
At time: 502.938752412796 and batch: 200, loss is 3.987449359893799 and perplexity is 53.917190479381254
At time: 504.1171085834503 and batch: 250, loss is 3.957946915626526 and perplexity is 52.349737115187516
At time: 505.29805541038513 and batch: 300, loss is 4.0038847064971925 and perplexity is 54.810660324118366
At time: 506.4704520702362 and batch: 350, loss is 3.96575644493103 and perplexity is 52.76016445749821
At time: 507.6516807079315 and batch: 400, loss is 3.967928376197815 and perplexity is 52.87488044084401
At time: 508.82940459251404 and batch: 450, loss is 3.922888059616089 and perplexity is 50.546214661998604
At time: 510.0042040348053 and batch: 500, loss is 3.899794702529907 and perplexity is 49.392307948724955
At time: 511.18184876441956 and batch: 550, loss is 3.897903628349304 and perplexity is 49.29899169223386
At time: 512.3652350902557 and batch: 600, loss is 3.8601413011550902 and perplexity is 47.47205875169802
At time: 513.5623819828033 and batch: 650, loss is 3.8296211528778077 and perplexity is 46.04509087931234
At time: 514.777069568634 and batch: 700, loss is 3.8221553993225097 and perplexity is 45.70260960950692
At time: 515.9763770103455 and batch: 750, loss is 3.843873267173767 and perplexity is 46.706029475901175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.55811434013899 and perplexity of 95.4034117257292
Finished 26 epochs...
Completing Train Step...
At time: 519.3701922893524 and batch: 50, loss is 4.028450975418091 and perplexity is 56.173829176152324
At time: 520.6120264530182 and batch: 100, loss is 4.026714420318603 and perplexity is 56.0763648771944
At time: 521.792562007904 and batch: 150, loss is 3.992723503112793 and perplexity is 54.202308680130216
At time: 522.9693560600281 and batch: 200, loss is 3.9865259027481077 and perplexity is 53.86742324704731
At time: 524.1455950737 and batch: 250, loss is 3.9570641136169433 and perplexity is 52.30354305516569
At time: 525.3192925453186 and batch: 300, loss is 4.00310860157013 and perplexity is 54.76813800361089
At time: 526.4952187538147 and batch: 350, loss is 3.9651167154312135 and perplexity is 52.726423017727505
At time: 527.6743633747101 and batch: 400, loss is 3.967410387992859 and perplexity is 52.84749896868923
At time: 528.8443462848663 and batch: 450, loss is 3.922545428276062 and perplexity is 50.52889891136451
At time: 530.0203433036804 and batch: 500, loss is 3.899642252922058 and perplexity is 49.384778684678665
At time: 531.1970756053925 and batch: 550, loss is 3.8979628801345827 and perplexity is 49.301912832044586
At time: 532.3705849647522 and batch: 600, loss is 3.860336241722107 and perplexity is 47.481313883819574
At time: 533.5409710407257 and batch: 650, loss is 3.8298617362976075 and perplexity is 46.05616989740167
At time: 534.7104029655457 and batch: 700, loss is 3.82248459815979 and perplexity is 45.71765733216155
At time: 535.884838104248 and batch: 750, loss is 3.844171004295349 and perplexity is 46.71993766506723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5581005007721656 and perplexity of 95.40209141205423
Finished 27 epochs...
Completing Train Step...
At time: 539.2786388397217 and batch: 50, loss is 4.027841238975525 and perplexity is 56.13958838537499
At time: 540.498929977417 and batch: 100, loss is 4.025937743186951 and perplexity is 56.03282855598916
At time: 541.6799519062042 and batch: 150, loss is 3.9918973445892334 and perplexity is 54.15754747328973
At time: 542.8585405349731 and batch: 200, loss is 3.9857375907897947 and perplexity is 53.82497564631199
At time: 544.0354840755463 and batch: 250, loss is 3.9563143253326416 and perplexity is 52.264341169749216
At time: 545.2135915756226 and batch: 300, loss is 4.002439951896667 and perplexity is 54.73152954649901
At time: 546.3971259593964 and batch: 350, loss is 3.9645580673217773 and perplexity is 52.696975727293726
At time: 547.642590045929 and batch: 400, loss is 3.9669464874267577 and perplexity is 52.82298866961099
At time: 548.8171763420105 and batch: 450, loss is 3.92222252368927 and perplexity is 50.5125855321147
At time: 549.9950511455536 and batch: 500, loss is 3.8994677543640135 and perplexity is 49.376161863842114
At time: 551.1675879955292 and batch: 550, loss is 3.8979576444625854 and perplexity is 49.301654704075894
At time: 552.3455460071564 and batch: 600, loss is 3.860439338684082 and perplexity is 47.486209315379305
At time: 553.520416021347 and batch: 650, loss is 3.8300000476837157 and perplexity is 46.0625404306475
At time: 554.6969857215881 and batch: 700, loss is 3.82270845413208 and perplexity is 45.72789264837005
At time: 555.8668148517609 and batch: 750, loss is 3.844373188018799 and perplexity is 46.72938463100287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558104049327762 and perplexity of 95.40242995228026
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 559.2725236415863 and batch: 50, loss is 4.027708320617676 and perplexity is 56.1321268993717
At time: 560.4921040534973 and batch: 100, loss is 4.026228075027466 and perplexity is 56.049099032037404
At time: 561.6688013076782 and batch: 150, loss is 3.9925179290771484 and perplexity is 54.191167238028434
At time: 562.8382675647736 and batch: 200, loss is 3.986464805603027 and perplexity is 53.86413220181183
At time: 564.0087997913361 and batch: 250, loss is 3.957124319076538 and perplexity is 52.306692108807916
At time: 565.1796109676361 and batch: 300, loss is 4.002877230644226 and perplexity is 54.75546771463575
At time: 566.351024389267 and batch: 350, loss is 3.9644291734695436 and perplexity is 52.69018384881627
At time: 567.5247383117676 and batch: 400, loss is 3.9663820457458496 and perplexity is 52.79318158606726
At time: 568.6968524456024 and batch: 450, loss is 3.9214178037643435 and perplexity is 50.4719533990137
At time: 569.8719570636749 and batch: 500, loss is 3.898026123046875 and perplexity is 49.30503092719134
At time: 571.0393934249878 and batch: 550, loss is 3.8961852264404295 and perplexity is 49.21434895675195
At time: 572.210821390152 and batch: 600, loss is 3.8586822986602782 and perplexity is 47.40284740158912
At time: 573.3825206756592 and batch: 650, loss is 3.827986454963684 and perplexity is 45.96988255349085
At time: 574.552738904953 and batch: 700, loss is 3.8204415798187257 and perplexity is 45.6243506657914
At time: 575.7254211902618 and batch: 750, loss is 3.8421805047988893 and perplexity is 46.627034145549004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558322285496911 and perplexity of 95.42325248515284
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 579.1138141155243 and batch: 50, loss is 4.027507495880127 and perplexity is 56.12085531156379
At time: 580.3464109897614 and batch: 100, loss is 4.026125121116638 and perplexity is 56.04332885512992
At time: 581.522528886795 and batch: 150, loss is 3.9924775886535646 and perplexity is 54.18898118748095
At time: 582.6994071006775 and batch: 200, loss is 3.986520471572876 and perplexity is 53.867130684426854
At time: 583.8750121593475 and batch: 250, loss is 3.9572478342056274 and perplexity is 52.313153175647514
At time: 585.0477435588837 and batch: 300, loss is 4.002951393127441 and perplexity is 54.75952866667435
At time: 586.2318768501282 and batch: 350, loss is 3.964328546524048 and perplexity is 52.684882063313665
At time: 587.4281659126282 and batch: 400, loss is 3.9661224842071534 and perplexity is 52.77948028486445
At time: 588.6071343421936 and batch: 450, loss is 3.92108615398407 and perplexity is 50.45521716219697
At time: 589.7897236347198 and batch: 500, loss is 3.8974486923217775 and perplexity is 49.27656890564579
At time: 590.9757273197174 and batch: 550, loss is 3.8954863500595094 and perplexity is 49.17996622670214
At time: 592.1511089801788 and batch: 600, loss is 3.858020710945129 and perplexity is 47.37149663187071
At time: 593.3209400177002 and batch: 650, loss is 3.8272465515136718 and perplexity is 45.93588185895961
At time: 594.4949979782104 and batch: 700, loss is 3.819598898887634 and perplexity is 45.58592009012271
At time: 595.6694285869598 and batch: 750, loss is 3.841339259147644 and perplexity is 46.58782585005992
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558374804119731 and perplexity of 95.42826411455918
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 599.0527131557465 and batch: 50, loss is 4.027411012649536 and perplexity is 56.11544085134632
At time: 600.2858633995056 and batch: 100, loss is 4.026057891845703 and perplexity is 56.03956122963899
At time: 601.4629755020142 and batch: 150, loss is 3.9924261474609377 and perplexity is 54.18619371335754
At time: 602.6450982093811 and batch: 200, loss is 3.9865099477767942 and perplexity is 53.866563800710914
At time: 603.824978351593 and batch: 250, loss is 3.95726176738739 and perplexity is 52.31388206939718
At time: 605.0028667449951 and batch: 300, loss is 4.002967305183411 and perplexity is 54.76040001029178
At time: 606.1874203681946 and batch: 350, loss is 3.9642723560333253 and perplexity is 52.68192175710817
At time: 607.3706176280975 and batch: 400, loss is 3.9660128259658816 and perplexity is 52.77369289720434
At time: 608.6082119941711 and batch: 450, loss is 3.9209599542617797 and perplexity is 50.44885012957034
At time: 609.7869415283203 and batch: 500, loss is 3.897220482826233 and perplexity is 49.26532480776743
At time: 610.9670579433441 and batch: 550, loss is 3.895209002494812 and perplexity is 49.16632817416501
At time: 612.1486778259277 and batch: 600, loss is 3.857760968208313 and perplexity is 47.35919382753967
At time: 613.3248960971832 and batch: 650, loss is 3.826964678764343 and perplexity is 45.922935610330256
At time: 614.5032966136932 and batch: 700, loss is 3.8192756032943724 and perplexity is 45.571184745107196
At time: 615.6874558925629 and batch: 750, loss is 3.841012225151062 and perplexity is 46.5725925382213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558385449786519 and perplexity of 95.42928001746851
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 619.0967144966125 and batch: 50, loss is 4.027369346618652 and perplexity is 56.11310279236388
At time: 620.3212852478027 and batch: 100, loss is 4.026026568412781 and perplexity is 56.03780590569336
At time: 621.5009653568268 and batch: 150, loss is 3.992400870323181 and perplexity is 54.18482405878508
At time: 622.6707227230072 and batch: 200, loss is 3.9864993953704833 and perplexity is 53.86599538184221
At time: 623.8440566062927 and batch: 250, loss is 3.957261571884155 and perplexity is 52.313871841865
At time: 625.0136315822601 and batch: 300, loss is 4.00297176361084 and perplexity is 54.760644156105464
At time: 626.178797006607 and batch: 350, loss is 3.9642479753494264 and perplexity is 52.68063735148404
At time: 627.3526103496552 and batch: 400, loss is 3.9659687995910646 and perplexity is 52.77136951396582
At time: 628.5183148384094 and batch: 450, loss is 3.9209102964401246 and perplexity is 50.44634501176776
At time: 629.6869974136353 and batch: 500, loss is 3.897131862640381 and perplexity is 49.26095909897471
At time: 630.8602323532104 and batch: 550, loss is 3.895100975036621 and perplexity is 49.16101714757729
At time: 632.027012348175 and batch: 600, loss is 3.857659101486206 and perplexity is 47.35436974741362
At time: 633.194079875946 and batch: 650, loss is 3.8268548583984376 and perplexity is 45.91789261365501
At time: 634.3563275337219 and batch: 700, loss is 3.819150071144104 and perplexity is 45.56546445534351
At time: 635.5217664241791 and batch: 750, loss is 3.840885052680969 and perplexity is 46.566670163179104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.558388288630995 and perplexity of 95.42955092673755
Annealing...
Model not improving. Stopping early with 95.40209141205423loss at 31 epochs.
Finished Training.
Improved accuracyfrom -133.4278283497118 to -95.40209141205423
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8188e94d30>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 7.920087375621255, 'num_layers': 1, 'lr': 17.959793212236615, 'dropout': 0.6065585298813164}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8788506984710693 and batch: 50, loss is 7.120015392303467 and perplexity is 1236.4694654323996
At time: 3.065441370010376 and batch: 100, loss is 6.447798137664795 and perplexity is 631.310702072156
At time: 4.280465126037598 and batch: 150, loss is 6.293655824661255 and perplexity is 541.127986422522
At time: 5.455930948257446 and batch: 200, loss is 6.258618316650391 and perplexity is 522.4965164055823
At time: 6.623053312301636 and batch: 250, loss is 6.271705226898193 and perplexity is 529.379320507392
At time: 7.797192573547363 and batch: 300, loss is 6.23258334159851 and perplexity is 509.068885110261
At time: 8.97186827659607 and batch: 350, loss is 6.140762014389038 and perplexity is 464.40732112183304
At time: 10.14130711555481 and batch: 400, loss is 6.160584840774536 and perplexity is 473.7050358579637
At time: 11.308749914169312 and batch: 450, loss is 6.127503032684326 and perplexity is 458.2903946685249
At time: 12.477943181991577 and batch: 500, loss is 6.119337177276611 and perplexity is 454.56329974062515
At time: 13.648111581802368 and batch: 550, loss is 6.121339302062989 and perplexity is 455.47430385766154
At time: 14.821655511856079 and batch: 600, loss is 6.092109832763672 and perplexity is 442.35371966348146
At time: 16.00057077407837 and batch: 650, loss is 6.088439693450928 and perplexity is 440.7331954805889
At time: 17.17620277404785 and batch: 700, loss is 6.069691371917725 and perplexity is 432.54716477006184
At time: 18.347923040390015 and batch: 750, loss is 6.0518276977539065 and perplexity is 424.8888892792912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.47785239995912 and perplexity of 239.33216528875903
Finished 1 epochs...
Completing Train Step...
At time: 21.807961225509644 and batch: 50, loss is 5.821332054138184 and perplexity is 337.42121773760033
At time: 22.97010111808777 and batch: 100, loss is 5.777596654891968 and perplexity is 322.9820196296609
At time: 24.1364426612854 and batch: 150, loss is 5.7527295684814455 and perplexity is 315.0494367205232
At time: 25.302154779434204 and batch: 200, loss is 5.730541229248047 and perplexity is 308.1359954676753
At time: 26.46905827522278 and batch: 250, loss is 5.7521640586853025 and perplexity is 314.87132354490797
At time: 27.633224487304688 and batch: 300, loss is 5.750027742385864 and perplexity is 314.1993768051347
At time: 28.795706510543823 and batch: 350, loss is 5.702369194030762 and perplexity is 299.57631527559545
At time: 29.960200309753418 and batch: 400, loss is 5.71239972114563 and perplexity is 302.5963445509492
At time: 31.126301050186157 and batch: 450, loss is 5.655942354202271 and perplexity is 285.98585575713764
At time: 32.29802346229553 and batch: 500, loss is 5.663706588745117 and perplexity is 288.2149594610216
At time: 33.46141982078552 and batch: 550, loss is 5.648699636459351 and perplexity is 283.9220238233585
At time: 34.62324285507202 and batch: 600, loss is 5.584845390319824 and perplexity is 266.35909786082465
At time: 35.790271520614624 and batch: 650, loss is 5.601546392440796 and perplexity is 270.8449162844562
At time: 36.95467495918274 and batch: 700, loss is 5.609157314300537 and perplexity is 272.9141602167456
At time: 38.17749309539795 and batch: 750, loss is 5.585280838012696 and perplexity is 266.4751085719287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.37776361509811 and perplexity of 216.53747238996576
Finished 2 epochs...
Completing Train Step...
At time: 41.55139231681824 and batch: 50, loss is 5.640664148330688 and perplexity is 281.6497135592694
At time: 42.768654346466064 and batch: 100, loss is 5.595192661285401 and perplexity is 269.1294959323886
At time: 43.94368815422058 and batch: 150, loss is 5.619246759414673 and perplexity is 275.68165040049166
At time: 45.11519455909729 and batch: 200, loss is 5.637848873138427 and perplexity is 280.8579072071418
At time: 46.29095458984375 and batch: 250, loss is 5.607062091827393 and perplexity is 272.3429429576076
At time: 47.46520924568176 and batch: 300, loss is 5.6084355449676515 and perplexity is 272.7172502157393
At time: 48.63650107383728 and batch: 350, loss is 5.587266483306885 and perplexity is 267.0047592924885
At time: 49.80852150917053 and batch: 400, loss is 5.604732074737549 and perplexity is 271.7091179445261
At time: 50.97984433174133 and batch: 450, loss is 5.525984554290772 and perplexity is 251.1334708793339
At time: 52.16253352165222 and batch: 500, loss is 5.602739639282227 and perplexity is 271.16829402169213
At time: 53.3377149105072 and batch: 550, loss is 5.584756908416748 and perplexity is 266.33553094358246
At time: 54.51254630088806 and batch: 600, loss is 5.582092065811157 and perplexity is 265.62673350959534
At time: 55.681447982788086 and batch: 650, loss is 5.581598405838013 and perplexity is 265.49563658477916
At time: 56.86018657684326 and batch: 700, loss is 5.556228446960449 and perplexity is 258.844746333421
At time: 58.029157876968384 and batch: 750, loss is 5.513770456314087 and perplexity is 248.08475859758977
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.354767555414244 and perplexity of 211.614781925302
Finished 3 epochs...
Completing Train Step...
At time: 61.434467792510986 and batch: 50, loss is 5.5725681018829345 and perplexity is 263.1089228817362
At time: 62.62903642654419 and batch: 100, loss is 5.556558856964111 and perplexity is 258.93028535770145
At time: 63.80065035820007 and batch: 150, loss is 5.550732698440552 and perplexity is 257.426102527311
At time: 65.0036404132843 and batch: 200, loss is 5.519041595458984 and perplexity is 249.39590044962802
At time: 66.16970086097717 and batch: 250, loss is 5.532411375045776 and perplexity is 252.75265822242662
At time: 67.34785771369934 and batch: 300, loss is 5.562969217300415 and perplexity is 260.5954532497446
At time: 68.51946997642517 and batch: 350, loss is 5.534327745437622 and perplexity is 253.23749034368763
At time: 69.69343185424805 and batch: 400, loss is 5.5439354610443115 and perplexity is 255.68224960178608
At time: 70.86160397529602 and batch: 450, loss is 5.4734477519989015 and perplexity is 238.2803095827306
At time: 72.04055404663086 and batch: 500, loss is 5.50516794204712 and perplexity is 245.95975919589466
At time: 73.20991110801697 and batch: 550, loss is 5.49508674621582 and perplexity is 243.49264731030092
At time: 74.38418531417847 and batch: 600, loss is 5.477763385772705 and perplexity is 239.3108622789319
At time: 75.55896973609924 and batch: 650, loss is 5.456044921875 and perplexity is 234.16943203565657
At time: 76.75696015357971 and batch: 700, loss is 5.506753311157227 and perplexity is 246.35000546090328
At time: 77.95089435577393 and batch: 750, loss is 5.489780082702636 and perplexity is 242.20393616554307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.363765982694404 and perplexity of 213.52757531112997
Annealing...
Finished 4 epochs...
Completing Train Step...
At time: 81.40952348709106 and batch: 50, loss is 5.465287675857544 and perplexity is 236.3438357458214
At time: 82.60898947715759 and batch: 100, loss is 5.440909538269043 and perplexity is 230.6518747919909
At time: 83.78005719184875 and batch: 150, loss is 5.414101066589356 and perplexity is 224.5505988881356
At time: 84.94860100746155 and batch: 200, loss is 5.408878107070922 and perplexity is 223.38083766724267
At time: 86.11817717552185 and batch: 250, loss is 5.395578060150147 and perplexity is 220.42953180318744
At time: 87.2889928817749 and batch: 300, loss is 5.381735210418701 and perplexity is 217.39918164958834
At time: 88.45980381965637 and batch: 350, loss is 5.351715965270996 and perplexity is 210.97000464097383
At time: 89.63773989677429 and batch: 400, loss is 5.37850754737854 and perplexity is 216.69862154019458
At time: 90.8073000907898 and batch: 450, loss is 5.320782165527344 and perplexity is 204.5438065553943
At time: 91.97407221794128 and batch: 500, loss is 5.303940315246582 and perplexity is 201.1277574293847
At time: 93.15108609199524 and batch: 550, loss is 5.29737190246582 and perplexity is 199.8109965456247
At time: 94.3208372592926 and batch: 600, loss is 5.246572647094727 and perplexity is 189.91424859412896
At time: 95.54265642166138 and batch: 650, loss is 5.193387804031372 and perplexity is 180.0775882826653
At time: 96.71054267883301 and batch: 700, loss is 5.172888412475586 and perplexity is 176.4236866246236
At time: 97.88060164451599 and batch: 750, loss is 5.137590436935425 and perplexity is 170.3049131010024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.154112350109012 and perplexity of 173.14204900652868
Finished 5 epochs...
Completing Train Step...
At time: 101.2472505569458 and batch: 50, loss is 5.314532985687256 and perplexity is 203.26956116420138
At time: 102.46969246864319 and batch: 100, loss is 5.314641256332397 and perplexity is 203.2915704821862
At time: 103.64598178863525 and batch: 150, loss is 5.299100465774536 and perplexity is 200.15668118575064
At time: 104.81640076637268 and batch: 200, loss is 5.300032501220703 and perplexity is 200.3433212714796
At time: 105.99405980110168 and batch: 250, loss is 5.296986389160156 and perplexity is 199.73398159393648
At time: 107.1676824092865 and batch: 300, loss is 5.298707494735718 and perplexity is 200.0780408595159
At time: 108.33775043487549 and batch: 350, loss is 5.277019634246826 and perplexity is 195.78549257012824
At time: 109.5146267414093 and batch: 400, loss is 5.305586242675782 and perplexity is 201.45907170694343
At time: 110.68714451789856 and batch: 450, loss is 5.254221954345703 and perplexity is 191.3725313488207
At time: 111.85951662063599 and batch: 500, loss is 5.241762619018555 and perplexity is 189.0029491712438
At time: 113.03075647354126 and batch: 550, loss is 5.238807344436646 and perplexity is 188.44521808981526
At time: 114.20692873001099 and batch: 600, loss is 5.193768310546875 and perplexity is 180.14612201624212
At time: 115.38476204872131 and batch: 650, loss is 5.157706594467163 and perplexity is 173.76548355622765
At time: 116.55902647972107 and batch: 700, loss is 5.145325393676758 and perplexity is 171.62732203135536
At time: 117.7306489944458 and batch: 750, loss is 5.12313515663147 and perplexity is 167.86081546196448
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.139519979787427 and perplexity of 170.63384096688992
Finished 6 epochs...
Completing Train Step...
At time: 121.12235188484192 and batch: 50, loss is 5.2638146114349365 and perplexity is 193.21713560150351
At time: 122.3218469619751 and batch: 100, loss is 5.265185708999634 and perplexity is 193.48223684388796
At time: 123.48308634757996 and batch: 150, loss is 5.248094253540039 and perplexity is 190.20344330331574
At time: 124.69695401191711 and batch: 200, loss is 5.250556383132935 and perplexity is 190.67232581763358
At time: 125.86355829238892 and batch: 250, loss is 5.251950750350952 and perplexity is 190.93837850260158
At time: 127.03226399421692 and batch: 300, loss is 5.25766263961792 and perplexity is 192.0321180633873
At time: 128.1971321105957 and batch: 350, loss is 5.235869560241699 and perplexity is 187.89241910613742
At time: 129.36446976661682 and batch: 400, loss is 5.265043106079101 and perplexity is 193.45464767903735
At time: 130.52864503860474 and batch: 450, loss is 5.215141248703003 and perplexity is 184.03781424641159
At time: 131.68953657150269 and batch: 500, loss is 5.205589761734009 and perplexity is 182.2883477654132
At time: 132.85697770118713 and batch: 550, loss is 5.204122648239136 and perplexity is 182.0211061552695
At time: 134.0245807170868 and batch: 600, loss is 5.1622772312164305 and perplexity is 174.5615202726444
At time: 135.1975154876709 and batch: 650, loss is 5.133769807815551 and perplexity is 169.65548259728715
At time: 136.36137866973877 and batch: 700, loss is 5.124164924621582 and perplexity is 168.03376218873993
At time: 137.52268242835999 and batch: 750, loss is 5.104009952545166 and perplexity is 164.68094784209302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1316986083984375 and perplexity of 169.30445590924546
Finished 7 epochs...
Completing Train Step...
At time: 140.90064358711243 and batch: 50, loss is 5.226977586746216 and perplexity is 186.22909079063515
At time: 142.1284966468811 and batch: 100, loss is 5.226961946487426 and perplexity is 186.22617814223844
At time: 143.29864192008972 and batch: 150, loss is 5.210063943862915 and perplexity is 183.10576630940292
At time: 144.46630549430847 and batch: 200, loss is 5.215722570419311 and perplexity is 184.14483052688513
At time: 145.63577508926392 and batch: 250, loss is 5.219505624771118 and perplexity is 184.84277978697392
At time: 146.80529808998108 and batch: 300, loss is 5.228307075500489 and perplexity is 186.47684492923736
At time: 147.97511076927185 and batch: 350, loss is 5.205421209335327 and perplexity is 182.25762521639797
At time: 149.15649843215942 and batch: 400, loss is 5.234211349487305 and perplexity is 187.58111205378998
At time: 150.34675240516663 and batch: 450, loss is 5.187076253890991 and perplexity is 178.94459877621307
At time: 151.53369641304016 and batch: 500, loss is 5.176735305786133 and perplexity is 177.10367681079686
At time: 152.69974565505981 and batch: 550, loss is 5.176931705474853 and perplexity is 177.13846333371302
At time: 153.87299489974976 and batch: 600, loss is 5.137826471328736 and perplexity is 170.34511566225092
At time: 155.11504340171814 and batch: 650, loss is 5.11370774269104 and perplexity is 166.28575809451794
At time: 156.2820782661438 and batch: 700, loss is 5.105076837539673 and perplexity is 164.85673723108332
At time: 157.45806765556335 and batch: 750, loss is 5.085086688995362 and perplexity is 161.59394704735575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.1264222610828485 and perplexity of 168.4134993628887
Finished 8 epochs...
Completing Train Step...
At time: 160.85212898254395 and batch: 50, loss is 5.1969070148468015 and perplexity is 180.71243570471597
At time: 162.0560142993927 and batch: 100, loss is 5.197944593429566 and perplexity is 180.90003636602606
At time: 163.22374081611633 and batch: 150, loss is 5.180481204986572 and perplexity is 177.76833342330218
At time: 164.39701056480408 and batch: 200, loss is 5.186819458007813 and perplexity is 178.8986524395979
At time: 165.56599044799805 and batch: 250, loss is 5.192647914886475 and perplexity is 179.94440010817405
At time: 166.73843789100647 and batch: 300, loss is 5.202915439605713 and perplexity is 181.80150128557105
At time: 167.90915632247925 and batch: 350, loss is 5.180131540298462 and perplexity is 177.70618498063104
At time: 169.07815289497375 and batch: 400, loss is 5.208300304412842 and perplexity is 182.78311835736318
At time: 170.24433207511902 and batch: 450, loss is 5.162792978286743 and perplexity is 174.65157308555536
At time: 171.4139723777771 and batch: 500, loss is 5.15206389427185 and perplexity is 172.7877381845147
At time: 172.58157896995544 and batch: 550, loss is 5.152853555679322 and perplexity is 172.92423587943125
At time: 173.74756622314453 and batch: 600, loss is 5.1161436843872075 and perplexity is 166.6913142611863
At time: 174.92232084274292 and batch: 650, loss is 5.09527403831482 and perplexity is 163.2485748595597
At time: 176.08919382095337 and batch: 700, loss is 5.087711133956909 and perplexity is 162.01859846089243
At time: 177.26171207427979 and batch: 750, loss is 5.067399005889893 and perplexity is 158.76085380961138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.12336908384811 and perplexity of 167.90008726850152
Finished 9 epochs...
Completing Train Step...
At time: 180.6368043422699 and batch: 50, loss is 5.171698036193848 and perplexity is 176.21380099874833
At time: 181.8398084640503 and batch: 100, loss is 5.173077278137207 and perplexity is 176.45701014765913
At time: 183.01088738441467 and batch: 150, loss is 5.154912881851196 and perplexity is 173.2807102066062
At time: 184.23608112335205 and batch: 200, loss is 5.161712713241577 and perplexity is 174.4630049661797
At time: 185.41123247146606 and batch: 250, loss is 5.1694051456451415 and perplexity is 175.8102248946157
At time: 186.5778295993805 and batch: 300, loss is 5.181994333267212 and perplexity is 178.0375233240725
At time: 187.74499773979187 and batch: 350, loss is 5.157990875244141 and perplexity is 173.81488876504787
At time: 188.91773796081543 and batch: 400, loss is 5.186786575317383 and perplexity is 178.89276986730928
At time: 190.0872495174408 and batch: 450, loss is 5.1422105979919435 and perplexity is 171.09356968558416
At time: 191.25757241249084 and batch: 500, loss is 5.1315538024902345 and perplexity is 169.27994139871052
At time: 192.42588424682617 and batch: 550, loss is 5.133213529586792 and perplexity is 169.561133190622
At time: 193.5993137359619 and batch: 600, loss is 5.098166990280151 and perplexity is 163.72152893189104
At time: 194.77469563484192 and batch: 650, loss is 5.079397773742675 and perplexity is 160.677262719262
At time: 195.94442415237427 and batch: 700, loss is 5.0728249263763425 and perplexity is 159.62461881972854
At time: 197.1137228012085 and batch: 750, loss is 5.051066579818726 and perplexity is 156.18896367483717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.120818027230197 and perplexity of 167.47231051308208
Finished 10 epochs...
Completing Train Step...
At time: 200.45579934120178 and batch: 50, loss is 5.150302248001099 and perplexity is 172.48361526706321
At time: 201.65634083747864 and batch: 100, loss is 5.15275408744812 and perplexity is 172.90703626697808
At time: 202.82993626594543 and batch: 150, loss is 5.133357095718384 and perplexity is 169.58547817409823
At time: 204.00105476379395 and batch: 200, loss is 5.140446043014526 and perplexity is 170.7919318821014
At time: 205.1674234867096 and batch: 250, loss is 5.148844299316406 and perplexity is 172.23232623482252
At time: 206.3449854850769 and batch: 300, loss is 5.163024883270264 and perplexity is 174.69208035247073
At time: 207.51757860183716 and batch: 350, loss is 5.138847875595093 and perplexity is 170.5191957780936
At time: 208.6881115436554 and batch: 400, loss is 5.1669094944000244 and perplexity is 175.3720109300094
At time: 209.8589789867401 and batch: 450, loss is 5.123770971298217 and perplexity is 167.9675777673305
At time: 211.03494930267334 and batch: 500, loss is 5.112739324569702 and perplexity is 166.12480190208476
At time: 212.20045471191406 and batch: 550, loss is 5.115907917022705 and perplexity is 166.65201852184597
At time: 213.37438774108887 and batch: 600, loss is 5.08166732788086 and perplexity is 161.04234259334137
At time: 214.60022807121277 and batch: 650, loss is 5.064510192871094 and perplexity is 158.30288519958006
At time: 215.77694439888 and batch: 700, loss is 5.0576863098144536 and perplexity is 157.2263221710981
At time: 216.94706106185913 and batch: 750, loss is 5.036498165130615 and perplexity is 153.93003255752714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.119467092114825 and perplexity of 167.24621903979806
Finished 11 epochs...
Completing Train Step...
At time: 220.3343300819397 and batch: 50, loss is 5.130769996643067 and perplexity is 169.14731077595295
At time: 221.57370352745056 and batch: 100, loss is 5.133605451583862 and perplexity is 169.6276009528058
At time: 222.74407315254211 and batch: 150, loss is 5.113627138137818 and perplexity is 166.2723552454521
At time: 223.92160487174988 and batch: 200, loss is 5.121609802246094 and perplexity is 167.6049634130968
At time: 225.092050075531 and batch: 250, loss is 5.1312488842010495 and perplexity is 169.2283327172012
At time: 226.26467895507812 and batch: 300, loss is 5.145956802368164 and perplexity is 171.73572323329785
At time: 227.43962836265564 and batch: 350, loss is 5.120945930480957 and perplexity is 167.49373213592744
At time: 228.62534284591675 and batch: 400, loss is 5.149201822280884 and perplexity is 172.2939142555868
At time: 229.84224915504456 and batch: 450, loss is 5.106587018966675 and perplexity is 165.10588889852392
At time: 231.04855751991272 and batch: 500, loss is 5.095595293045044 and perplexity is 163.3010276613399
At time: 232.24308228492737 and batch: 550, loss is 5.099242448806763 and perplexity is 163.8976993611858
At time: 233.43979358673096 and batch: 600, loss is 5.065906591415406 and perplexity is 158.5240935295831
At time: 234.64403462409973 and batch: 650, loss is 5.050131206512451 and perplexity is 156.042936992858
At time: 235.81592178344727 and batch: 700, loss is 5.044132194519043 and perplexity is 155.1096357839041
At time: 236.99034667015076 and batch: 750, loss is 5.022844095230102 and perplexity is 151.84254491567748
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.118082090865734 and perplexity of 167.01474315172115
Finished 12 epochs...
Completing Train Step...
At time: 240.37605237960815 and batch: 50, loss is 5.1130452156066895 and perplexity is 166.17562576289333
At time: 241.58324027061462 and batch: 100, loss is 5.115916595458985 and perplexity is 166.65346480704534
At time: 242.76471161842346 and batch: 150, loss is 5.095525255203247 and perplexity is 163.28959081031024
At time: 243.99801182746887 and batch: 200, loss is 5.104444208145142 and perplexity is 164.75247699574652
At time: 245.1743597984314 and batch: 250, loss is 5.114012317657471 and perplexity is 166.33641228731545
At time: 246.34648394584656 and batch: 300, loss is 5.129761905670166 and perplexity is 168.97688081775368
At time: 247.52499198913574 and batch: 350, loss is 5.10391131401062 and perplexity is 164.66470475584057
At time: 248.7050359249115 and batch: 400, loss is 5.132239160537719 and perplexity is 169.3959985346291
At time: 249.88032841682434 and batch: 450, loss is 5.091020593643188 and perplexity is 162.5556807167628
At time: 251.05490970611572 and batch: 500, loss is 5.080476102828979 and perplexity is 160.85061913599492
At time: 252.22953391075134 and batch: 550, loss is 5.084893360137939 and perplexity is 161.5627092938836
At time: 253.40659952163696 and batch: 600, loss is 5.052425718307495 and perplexity is 156.40139043287942
At time: 254.57760047912598 and batch: 650, loss is 5.037582082748413 and perplexity is 154.0969704888626
At time: 255.75822734832764 and batch: 700, loss is 5.03105788230896 and perplexity is 153.09488342919803
At time: 256.94259428977966 and batch: 750, loss is 5.010141048431397 and perplexity is 149.92588146816146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.11820806458939 and perplexity of 167.03578394608724
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 260.3491475582123 and batch: 50, loss is 5.101615085601806 and perplexity is 164.2870307616743
At time: 261.5519902706146 and batch: 100, loss is 5.103238878250122 and perplexity is 164.55401553973527
At time: 262.7167971134186 and batch: 150, loss is 5.07779390335083 and perplexity is 160.4197637676153
At time: 263.88691306114197 and batch: 200, loss is 5.084091758728027 and perplexity is 161.43325229171415
At time: 265.0562860965729 and batch: 250, loss is 5.087259426116943 and perplexity is 161.94542991639403
At time: 266.2286813259125 and batch: 300, loss is 5.093057861328125 and perplexity is 162.88718772168536
At time: 267.39726662635803 and batch: 350, loss is 5.062625827789307 and perplexity is 158.0048656473676
At time: 268.5721526145935 and batch: 400, loss is 5.081416206359863 and perplexity is 161.00190647272706
At time: 269.7402057647705 and batch: 450, loss is 5.033142576217651 and perplexity is 153.41437230268207
At time: 270.90749311447144 and batch: 500, loss is 5.001713008880615 and perplexity is 148.66761003825397
At time: 272.0853056907654 and batch: 550, loss is 4.996600046157837 and perplexity is 147.90941804548672
At time: 273.2547233104706 and batch: 600, loss is 4.962509069442749 and perplexity is 142.95202285322188
At time: 274.49778008461 and batch: 650, loss is 4.931232080459595 and perplexity is 138.55011210786205
At time: 275.6728575229645 and batch: 700, loss is 4.915096940994263 and perplexity is 136.332525372529
At time: 276.84223437309265 and batch: 750, loss is 4.907319669723511 and perplexity is 135.27634277265392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0787573526071945 and perplexity of 160.574394547217
Finished 14 epochs...
Completing Train Step...
At time: 280.22586464881897 and batch: 50, loss is 5.070147380828858 and perplexity is 159.1977883164002
At time: 281.4542739391327 and batch: 100, loss is 5.07275634765625 and perplexity is 159.6136723430265
At time: 282.6224842071533 and batch: 150, loss is 5.051122465133667 and perplexity is 156.19769258816922
At time: 283.79461121559143 and batch: 200, loss is 5.057360906600952 and perplexity is 157.17516854384527
At time: 284.9713578224182 and batch: 250, loss is 5.062102394104004 and perplexity is 157.92218221977942
At time: 286.1490650177002 and batch: 300, loss is 5.071115427017212 and perplexity is 159.35197374584212
At time: 287.32314801216125 and batch: 350, loss is 5.043758134841919 and perplexity is 155.05162637379104
At time: 288.4975838661194 and batch: 400, loss is 5.064475841522217 and perplexity is 158.29744737534122
At time: 289.67383909225464 and batch: 450, loss is 5.018971300125122 and perplexity is 151.25562709081564
At time: 290.85174107551575 and batch: 500, loss is 4.990345058441162 and perplexity is 146.98713389745905
At time: 292.0229551792145 and batch: 550, loss is 4.989662656784057 and perplexity is 146.8868638498288
At time: 293.1953818798065 and batch: 600, loss is 4.958883762359619 and perplexity is 142.43471613664985
At time: 294.369215965271 and batch: 650, loss is 4.933506174087524 and perplexity is 138.86554656275823
At time: 295.5428144931793 and batch: 700, loss is 4.9233342552185055 and perplexity is 137.46017725822716
At time: 296.71845626831055 and batch: 750, loss is 4.915464057922363 and perplexity is 136.38258453866527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.074850747751635 and perplexity of 159.9483175517581
Finished 15 epochs...
Completing Train Step...
At time: 300.12744307518005 and batch: 50, loss is 5.060654964447021 and perplexity is 157.69376631759317
At time: 301.32492566108704 and batch: 100, loss is 5.060826292037964 and perplexity is 157.7207859252191
At time: 302.4991497993469 and batch: 150, loss is 5.039433031082154 and perplexity is 154.38246015142352
At time: 303.7256352901459 and batch: 200, loss is 5.045790367126465 and perplexity is 155.36704769081427
At time: 304.8999488353729 and batch: 250, loss is 5.0508571434021 and perplexity is 156.1562554432355
At time: 306.0740966796875 and batch: 300, loss is 5.060694084167481 and perplexity is 157.6999353743148
At time: 307.25809955596924 and batch: 350, loss is 5.035341339111328 and perplexity is 153.7520652491684
At time: 308.45690155029297 and batch: 400, loss is 5.056693992614746 and perplexity is 157.0703811716251
At time: 309.66154623031616 and batch: 450, loss is 5.0126510334014895 and perplexity is 150.30266584101835
At time: 310.88152861595154 and batch: 500, loss is 4.98562518119812 and perplexity is 146.29500733026217
At time: 312.0605204105377 and batch: 550, loss is 4.987519044876098 and perplexity is 146.57233265620468
At time: 313.2359516620636 and batch: 600, loss is 4.95863263130188 and perplexity is 142.39895084680313
At time: 314.415278673172 and batch: 650, loss is 4.936189985275268 and perplexity is 139.2387360312488
At time: 315.59679079055786 and batch: 700, loss is 4.928011589050293 and perplexity is 138.10463038212464
At time: 316.76224660873413 and batch: 750, loss is 4.918995590209961 and perplexity is 136.8650755041836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.073509038880814 and perplexity of 159.73385737895632
Finished 16 epochs...
Completing Train Step...
At time: 320.12150526046753 and batch: 50, loss is 5.0541489219665525 and perplexity is 156.67113422615208
At time: 321.3903651237488 and batch: 100, loss is 5.052780752182007 and perplexity is 156.456928082794
At time: 322.570903301239 and batch: 150, loss is 5.031674337387085 and perplexity is 153.18928864282
At time: 323.77417373657227 and batch: 200, loss is 5.038331308364868 and perplexity is 154.21246714769597
At time: 324.9786744117737 and batch: 250, loss is 5.043970851898194 and perplexity is 155.08461200750287
At time: 326.1834406852722 and batch: 300, loss is 5.053987512588501 and perplexity is 156.64584807658372
At time: 327.3704764842987 and batch: 350, loss is 5.030214624404907 and perplexity is 152.96583937503246
At time: 328.55259132385254 and batch: 400, loss is 5.052149124145508 and perplexity is 156.35813670349938
At time: 329.7361488342285 and batch: 450, loss is 5.008983707427978 and perplexity is 149.75246646756665
At time: 330.90730834007263 and batch: 500, loss is 4.982839164733886 and perplexity is 145.88799426701829
At time: 332.085732460022 and batch: 550, loss is 4.986512250900269 and perplexity is 146.42483877509738
At time: 333.2563548088074 and batch: 600, loss is 4.958691272735596 and perplexity is 142.40730157028727
At time: 334.4851996898651 and batch: 650, loss is 4.937881698608399 and perplexity is 139.4744874131345
At time: 335.66451716423035 and batch: 700, loss is 4.930640525817871 and perplexity is 138.46817638305183
At time: 336.83617758750916 and batch: 750, loss is 4.920541543960571 and perplexity is 137.07682621721816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.072890880496003 and perplexity of 159.63514706812583
Finished 17 epochs...
Completing Train Step...
At time: 340.21104884147644 and batch: 50, loss is 5.048977546691894 and perplexity is 155.86302032743635
At time: 341.44045758247375 and batch: 100, loss is 5.046788301467895 and perplexity is 155.5221711918925
At time: 342.61904191970825 and batch: 150, loss is 5.025951871871948 and perplexity is 152.31517165878458
At time: 343.79227352142334 and batch: 200, loss is 5.032867755889892 and perplexity is 153.37221670747553
At time: 344.9654984474182 and batch: 250, loss is 5.038847789764405 and perplexity is 154.2921355904178
At time: 346.13426089286804 and batch: 300, loss is 5.049121780395508 and perplexity is 155.88550264942987
At time: 347.30239033699036 and batch: 350, loss is 5.026375818252563 and perplexity is 152.37975881430015
At time: 348.47245717048645 and batch: 400, loss is 5.048796625137329 and perplexity is 155.83482389824727
At time: 349.6369643211365 and batch: 450, loss is 5.0062908267974855 and perplexity is 149.3497434371374
At time: 350.8077881336212 and batch: 500, loss is 4.98081371307373 and perplexity is 145.59280423432116
At time: 351.9838762283325 and batch: 550, loss is 4.98558557510376 and perplexity is 146.28921327113815
At time: 353.15706157684326 and batch: 600, loss is 4.958429927825928 and perplexity is 142.37008900979075
At time: 354.3262777328491 and batch: 650, loss is 4.938715381622314 and perplexity is 139.59081340693052
At time: 355.49538230895996 and batch: 700, loss is 4.931831388473511 and perplexity is 138.63317118687706
At time: 356.6676290035248 and batch: 750, loss is 4.921145629882813 and perplexity is 137.15965741426268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0723962118459305 and perplexity of 159.55620009332802
Finished 18 epochs...
Completing Train Step...
At time: 360.0490245819092 and batch: 50, loss is 5.044691314697266 and perplexity is 155.1963849604553
At time: 361.255455493927 and batch: 100, loss is 5.042017269134521 and perplexity is 154.78193712904982
At time: 362.4245846271515 and batch: 150, loss is 5.0212436962127684 and perplexity is 151.5997306077211
At time: 363.6486840248108 and batch: 200, loss is 5.028501415252686 and perplexity is 152.704001254813
At time: 364.8235332965851 and batch: 250, loss is 5.034840936660767 and perplexity is 153.6751465856877
At time: 365.99396657943726 and batch: 300, loss is 5.045330076217652 and perplexity is 155.29555010733927
At time: 367.16441011428833 and batch: 350, loss is 5.023340158462524 and perplexity is 151.91788710502752
At time: 368.3362307548523 and batch: 400, loss is 5.0461403465271 and perplexity is 155.42143247326524
At time: 369.51163816452026 and batch: 450, loss is 5.004028930664062 and perplexity is 149.01231159168574
At time: 370.6840510368347 and batch: 500, loss is 4.979168033599853 and perplexity is 145.3534021883953
At time: 371.8567419052124 and batch: 550, loss is 4.9846662902832035 and perplexity is 146.15479361240915
At time: 373.0305366516113 and batch: 600, loss is 4.958039035797119 and perplexity is 142.3144485522702
At time: 374.2038655281067 and batch: 650, loss is 4.939006996154785 and perplexity is 139.63152605263048
At time: 375.3775475025177 and batch: 700, loss is 4.932401008605957 and perplexity is 138.7121619274322
At time: 376.5501570701599 and batch: 750, loss is 4.921248521804809 and perplexity is 137.1737707610967
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.071990611941316 and perplexity of 159.49149723641284
Finished 19 epochs...
Completing Train Step...
At time: 379.9973511695862 and batch: 50, loss is 5.040805473327636 and perplexity is 154.59448662546092
At time: 381.23626613616943 and batch: 100, loss is 5.037859649658203 and perplexity is 154.1397486453949
At time: 382.4123492240906 and batch: 150, loss is 5.0171874904632565 and perplexity is 150.9860563447309
At time: 383.58293628692627 and batch: 200, loss is 5.024658336639404 and perplexity is 152.11827399228198
At time: 384.7571337223053 and batch: 250, loss is 5.031271085739136 and perplexity is 153.12752726325263
At time: 385.93019104003906 and batch: 300, loss is 5.04193642616272 and perplexity is 154.76942460305287
At time: 387.10315132141113 and batch: 350, loss is 5.020416650772095 and perplexity is 151.4744025748475
At time: 388.27955412864685 and batch: 400, loss is 5.0436381912231445 and perplexity is 155.03303003590517
At time: 389.4576139450073 and batch: 450, loss is 5.002042999267578 and perplexity is 148.7166770157896
At time: 390.6299750804901 and batch: 500, loss is 4.977656564712524 and perplexity is 145.13387099240038
At time: 391.8006672859192 and batch: 550, loss is 4.98345064163208 and perplexity is 145.9772286848485
At time: 392.976402759552 and batch: 600, loss is 4.9574051856994625 and perplexity is 142.2242711076676
At time: 394.2068946361542 and batch: 650, loss is 4.938867902755737 and perplexity is 139.61210557971862
At time: 395.3826549053192 and batch: 700, loss is 4.932436962127685 and perplexity is 138.71714920781457
At time: 396.5699682235718 and batch: 750, loss is 4.920801954269409 and perplexity is 137.11252708410308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.071617303892624 and perplexity of 159.43196888869352
Finished 20 epochs...
Completing Train Step...
At time: 399.90108728408813 and batch: 50, loss is 5.037086124420166 and perplexity is 154.0205637618335
At time: 401.1371328830719 and batch: 100, loss is 5.034245653152466 and perplexity is 153.5836935282386
At time: 402.3030164241791 and batch: 150, loss is 5.013487377166748 and perplexity is 150.42842311932375
At time: 403.46754336357117 and batch: 200, loss is 5.0213235664367675 and perplexity is 151.6118393957223
At time: 404.6287772655487 and batch: 250, loss is 5.0282637405395505 and perplexity is 152.66771168784717
At time: 405.7935574054718 and batch: 300, loss is 5.038980598449707 and perplexity is 154.31262828687198
At time: 406.95820355415344 and batch: 350, loss is 5.017988500595092 and perplexity is 151.1070461561923
At time: 408.1196641921997 and batch: 400, loss is 5.041439847946167 and perplexity is 154.69258855733875
At time: 409.2816119194031 and batch: 450, loss is 5.000185079574585 and perplexity is 148.44062988898847
At time: 410.44989109039307 and batch: 500, loss is 4.976130065917968 and perplexity is 144.91249332264206
At time: 411.61254954338074 and batch: 550, loss is 4.982480993270874 and perplexity is 145.83575070731038
At time: 412.77311611175537 and batch: 600, loss is 4.956659641265869 and perplexity is 142.11827611092093
At time: 413.936646938324 and batch: 650, loss is 4.938633880615234 and perplexity is 139.57943707865613
At time: 415.10035395622253 and batch: 700, loss is 4.932107982635498 and perplexity is 138.67152161619296
At time: 416.26438307762146 and batch: 750, loss is 4.920295152664185 and perplexity is 137.04305584083625
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.071340161700581 and perplexity of 159.3877896855974
Finished 21 epochs...
Completing Train Step...
At time: 419.6370527744293 and batch: 50, loss is 5.033879528045654 and perplexity is 153.52747297454718
At time: 420.8381230831146 and batch: 100, loss is 5.031038904190064 and perplexity is 153.0919780038678
At time: 422.0071232318878 and batch: 150, loss is 5.010298557281494 and perplexity is 149.9494979812075
At time: 423.2190771102905 and batch: 200, loss is 5.018402118682861 and perplexity is 151.16955969114494
At time: 424.3905463218689 and batch: 250, loss is 5.025399417877197 and perplexity is 152.2310477731684
At time: 425.56195759773254 and batch: 300, loss is 5.036299695968628 and perplexity is 153.89948522441293
At time: 426.7367215156555 and batch: 350, loss is 5.015680208206176 and perplexity is 150.75864916739164
At time: 427.91421246528625 and batch: 400, loss is 5.039397382736206 and perplexity is 154.37695677016967
At time: 429.0889849662781 and batch: 450, loss is 4.998403539657593 and perplexity is 148.17641240841496
At time: 430.2607898712158 and batch: 500, loss is 4.974699687957764 and perplexity is 144.70536185949055
At time: 431.42755341529846 and batch: 550, loss is 4.981173257827759 and perplexity is 145.64516077500375
At time: 432.5979218482971 and batch: 600, loss is 4.9556756114959715 and perplexity is 141.97849628141725
At time: 433.77606558799744 and batch: 650, loss is 4.938018417358398 and perplexity is 139.49355749429958
At time: 434.9472162723541 and batch: 700, loss is 4.931689872741699 and perplexity is 138.61355380032026
At time: 436.11883783340454 and batch: 750, loss is 4.919591588973999 and perplexity is 136.94667123308557
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.071143571720567 and perplexity of 159.35645872298616
Finished 22 epochs...
Completing Train Step...
At time: 439.49158000946045 and batch: 50, loss is 5.030816450119018 and perplexity is 153.0579258577761
At time: 440.71760416030884 and batch: 100, loss is 5.028119115829468 and perplexity is 152.64563376085076
At time: 441.887335062027 and batch: 150, loss is 5.007500619888305 and perplexity is 149.53053506304278
At time: 443.0563876628876 and batch: 200, loss is 5.015704011917114 and perplexity is 150.76223782540927
At time: 444.2325277328491 and batch: 250, loss is 5.022787113189697 and perplexity is 151.83389286415596
At time: 445.41181564331055 and batch: 300, loss is 5.03377926826477 and perplexity is 153.51208111535223
At time: 446.58453726768494 and batch: 350, loss is 5.013596754074097 and perplexity is 150.4448774148655
At time: 447.7549777030945 and batch: 400, loss is 5.037719078063965 and perplexity is 154.1180824980505
At time: 448.923700094223 and batch: 450, loss is 4.996829996109009 and perplexity is 147.9434337197456
At time: 450.1000545024872 and batch: 500, loss is 4.973336277008056 and perplexity is 144.5082034191794
At time: 451.26622676849365 and batch: 550, loss is 4.980031480789185 and perplexity is 145.47896137403683
At time: 452.4366192817688 and batch: 600, loss is 4.954740743637085 and perplexity is 141.84582717229625
At time: 453.6641356945038 and batch: 650, loss is 4.937271461486817 and perplexity is 139.38940086752618
At time: 454.8374128341675 and batch: 700, loss is 4.93109314918518 and perplexity is 138.53086450129481
At time: 456.0061843395233 and batch: 750, loss is 4.918787431716919 and perplexity is 136.8365888412899
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0711176672647165 and perplexity of 159.35233073410356
Finished 23 epochs...
Completing Train Step...
At time: 459.40001916885376 and batch: 50, loss is 5.028049573898316 and perplexity is 152.63501885779178
At time: 460.62418127059937 and batch: 100, loss is 5.025451078414917 and perplexity is 152.23891231409544
At time: 461.79390001296997 and batch: 150, loss is 5.0049520683288575 and perplexity is 149.14993398146672
At time: 462.96510648727417 and batch: 200, loss is 5.013247041702271 and perplexity is 150.392274178493
At time: 464.13958764076233 and batch: 250, loss is 5.02035159111023 and perplexity is 151.46454802200518
At time: 465.31315994262695 and batch: 300, loss is 5.031345376968384 and perplexity is 153.13890371806482
At time: 466.48430609703064 and batch: 350, loss is 5.01154070854187 and perplexity is 150.13587366855108
At time: 467.6669485569 and batch: 400, loss is 5.035583753585815 and perplexity is 153.78934149323504
At time: 468.8424093723297 and batch: 450, loss is 4.995224466323853 and perplexity is 147.70609670719418
At time: 470.0148124694824 and batch: 500, loss is 4.971785135269165 and perplexity is 144.2842244697009
At time: 471.1852512359619 and batch: 550, loss is 4.978726005554199 and perplexity is 145.2891661062176
At time: 472.3493824005127 and batch: 600, loss is 4.953512334823609 and perplexity is 141.67168948607483
At time: 473.5265951156616 and batch: 650, loss is 4.936469478607178 and perplexity is 139.27765776844268
At time: 474.7278425693512 and batch: 700, loss is 4.9302758121490475 and perplexity is 138.41768435455398
At time: 475.9435203075409 and batch: 750, loss is 4.917723798751831 and perplexity is 136.69112230978132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.071059470952943 and perplexity of 159.34305728602524
Finished 24 epochs...
Completing Train Step...
At time: 479.3873088359833 and batch: 50, loss is 5.025277328491211 and perplexity is 152.21246311253577
At time: 480.63696551322937 and batch: 100, loss is 5.022751207351685 and perplexity is 151.82844123886719
At time: 481.8075909614563 and batch: 150, loss is 5.002426309585571 and perplexity is 148.77369257917616
At time: 483.0476953983307 and batch: 200, loss is 5.010756883621216 and perplexity is 150.01823953758992
At time: 484.2198688983917 and batch: 250, loss is 5.018085079193115 and perplexity is 151.121640567604
At time: 485.398957490921 and batch: 300, loss is 5.029151391983032 and perplexity is 152.80328756565984
At time: 486.5727324485779 and batch: 350, loss is 5.0095415210723875 and perplexity is 149.8360237391727
At time: 487.75473737716675 and batch: 400, loss is 5.033730335235596 and perplexity is 153.50456948799322
At time: 488.92855882644653 and batch: 450, loss is 4.993480405807495 and perplexity is 147.4487128476567
At time: 490.0977680683136 and batch: 500, loss is 4.970227069854737 and perplexity is 144.05959524865906
At time: 491.2677516937256 and batch: 550, loss is 4.977315187454224 and perplexity is 145.08433404530908
At time: 492.4432632923126 and batch: 600, loss is 4.95239013671875 and perplexity is 141.51279495686626
At time: 493.6168382167816 and batch: 650, loss is 4.9355878734588625 and perplexity is 139.154923977629
At time: 494.79297399520874 and batch: 700, loss is 4.929476013183594 and perplexity is 138.30702229340676
At time: 495.9612033367157 and batch: 750, loss is 4.916712532043457 and perplexity is 136.5529609992003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.070920367573583 and perplexity of 159.32089366983186
Finished 25 epochs...
Completing Train Step...
At time: 499.34710144996643 and batch: 50, loss is 5.0227352809906005 and perplexity is 151.82602318354463
At time: 500.56936597824097 and batch: 100, loss is 5.020293350219727 and perplexity is 151.45572684872772
At time: 501.74008560180664 and batch: 150, loss is 5.000025062561035 and perplexity is 148.41687876304698
At time: 502.9200077056885 and batch: 200, loss is 5.008634128570557 and perplexity is 149.70012532065508
At time: 504.09548592567444 and batch: 250, loss is 5.0159282398223874 and perplexity is 150.79604671649795
At time: 505.26870346069336 and batch: 300, loss is 5.027050151824951 and perplexity is 152.48254825471116
At time: 506.43835711479187 and batch: 350, loss is 5.007668085098267 and perplexity is 149.55557832237156
At time: 507.6098618507385 and batch: 400, loss is 5.032012138366699 and perplexity is 153.2410448757509
At time: 508.7820656299591 and batch: 450, loss is 4.991922235488891 and perplexity is 147.21914154182645
At time: 509.9593720436096 and batch: 500, loss is 4.968639736175537 and perplexity is 143.83110599362956
At time: 511.1298599243164 and batch: 550, loss is 4.9759597492218015 and perplexity is 144.88781440722138
At time: 512.3022201061249 and batch: 600, loss is 4.951230249404907 and perplexity is 141.3487512157676
At time: 513.5344390869141 and batch: 650, loss is 4.934732151031494 and perplexity is 139.0358969226261
At time: 514.7101271152496 and batch: 700, loss is 4.928613929748535 and perplexity is 138.18784147981907
At time: 515.8899958133698 and batch: 750, loss is 4.915746908187867 and perplexity is 136.42116584502128
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.070831298828125 and perplexity of 159.30670378965422
Finished 26 epochs...
Completing Train Step...
At time: 519.299736738205 and batch: 50, loss is 5.020337171554566 and perplexity is 151.46236398627028
At time: 520.5298843383789 and batch: 100, loss is 5.017963933944702 and perplexity is 151.10333400781556
At time: 521.708099603653 and batch: 150, loss is 4.9977052211761475 and perplexity is 148.07297420172503
At time: 522.8812348842621 and batch: 200, loss is 5.00637131690979 and perplexity is 149.36176509856517
At time: 524.0578844547272 and batch: 250, loss is 5.013653955459595 and perplexity is 150.45348331642705
At time: 525.2402834892273 and batch: 300, loss is 5.024774045944214 and perplexity is 152.13587651038242
At time: 526.417338848114 and batch: 350, loss is 5.00585976600647 and perplexity is 149.28537849219208
At time: 527.5994293689728 and batch: 400, loss is 5.030140600204468 and perplexity is 152.95451662016225
At time: 528.7817420959473 and batch: 450, loss is 4.990230846405029 and perplexity is 146.9703471562535
At time: 529.9541463851929 and batch: 500, loss is 4.967164859771729 and perplexity is 143.6191292474419
At time: 531.137622833252 and batch: 550, loss is 4.974648008346557 and perplexity is 144.69788373588509
At time: 532.3095088005066 and batch: 600, loss is 4.950041885375977 and perplexity is 141.1808772117585
At time: 533.4878897666931 and batch: 650, loss is 4.933916301727295 and perplexity is 138.9225108421297
At time: 534.6603806018829 and batch: 700, loss is 4.927646322250366 and perplexity is 138.05419455755683
At time: 535.8355858325958 and batch: 750, loss is 4.914679584503173 and perplexity is 136.27563798010513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.070770618527434 and perplexity of 159.29703730425177
Finished 27 epochs...
Completing Train Step...
At time: 539.2141864299774 and batch: 50, loss is 5.018018407821655 and perplexity is 151.11156541643587
At time: 540.4132580757141 and batch: 100, loss is 5.015698709487915 and perplexity is 150.76143842143674
At time: 541.5817697048187 and batch: 150, loss is 4.995463800430298 and perplexity is 147.7414520445664
At time: 542.8095524311066 and batch: 200, loss is 5.00418909072876 and perplexity is 149.03617932442882
At time: 543.9757463932037 and batch: 250, loss is 5.011519060134888 and perplexity is 150.13262350123594
At time: 545.143880367279 and batch: 300, loss is 5.022621526718139 and perplexity is 151.80875330701647
At time: 546.3057639598846 and batch: 350, loss is 5.004019727706909 and perplexity is 149.01094024407718
At time: 547.48149061203 and batch: 400, loss is 5.028360300064087 and perplexity is 152.68245392123984
At time: 548.6486461162567 and batch: 450, loss is 4.98868688583374 and perplexity is 146.74360581997956
At time: 549.8157708644867 and batch: 500, loss is 4.965576095581055 and perplexity is 143.39113348151037
At time: 550.9842748641968 and batch: 550, loss is 4.973222255706787 and perplexity is 144.49172734511103
At time: 552.1513130664825 and batch: 600, loss is 4.9487066650390625 and perplexity is 140.99249542690458
At time: 553.318207025528 and batch: 650, loss is 4.93278733253479 and perplexity is 138.765760107268
At time: 554.4975299835205 and batch: 700, loss is 4.926708526611328 and perplexity is 137.92478862359408
At time: 555.6902692317963 and batch: 750, loss is 4.913550186157226 and perplexity is 136.12181537982326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.070700357126635 and perplexity of 159.2858452644565
Finished 28 epochs...
Completing Train Step...
At time: 559.1440999507904 and batch: 50, loss is 5.01587233543396 and perplexity is 150.78761679136605
At time: 560.3730292320251 and batch: 100, loss is 5.013487548828125 and perplexity is 150.42844894207616
At time: 561.5482604503632 and batch: 150, loss is 4.993409805297851 and perplexity is 147.43830326084873
At time: 562.7235839366913 and batch: 200, loss is 5.002113094329834 and perplexity is 148.72710168587818
At time: 563.8961975574493 and batch: 250, loss is 5.009418706893921 and perplexity is 149.8176228809788
At time: 565.0697829723358 and batch: 300, loss is 5.020565748214722 and perplexity is 151.4969887046201
At time: 566.2401578426361 and batch: 350, loss is 5.002287654876709 and perplexity is 148.75306583617584
At time: 567.4144897460938 and batch: 400, loss is 5.026641721725464 and perplexity is 152.42028250882544
At time: 568.5861134529114 and batch: 450, loss is 4.987134037017822 and perplexity is 146.5159120182176
At time: 569.7563223838806 and batch: 500, loss is 4.964104833602906 and perplexity is 143.1803226758038
At time: 570.9294312000275 and batch: 550, loss is 4.971702375411987 and perplexity is 144.27228402199242
At time: 572.0988299846649 and batch: 600, loss is 4.947397499084473 and perplexity is 140.8080336239412
At time: 573.337281703949 and batch: 650, loss is 4.931662998199463 and perplexity is 138.6098286745699
At time: 574.508024930954 and batch: 700, loss is 4.925685844421387 and perplexity is 137.78380750044658
At time: 575.6781141757965 and batch: 750, loss is 4.912474288940429 and perplexity is 135.9754410534956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.070529671602471 and perplexity of 159.25865979661447
Finished 29 epochs...
Completing Train Step...
At time: 579.0943710803986 and batch: 50, loss is 5.013588008880615 and perplexity is 150.44356175105696
At time: 580.3198516368866 and batch: 100, loss is 5.011426372528076 and perplexity is 150.11870871253342
At time: 581.4910244941711 and batch: 150, loss is 4.99135853767395 and perplexity is 147.13617781885327
At time: 582.6665613651276 and batch: 200, loss is 5.000185289382935 and perplexity is 148.4406610330753
At time: 583.8427658081055 and batch: 250, loss is 5.007505807876587 and perplexity is 149.53131082771876
At time: 585.0127651691437 and batch: 300, loss is 5.018651247024536 and perplexity is 151.20722500441627
At time: 586.1821253299713 and batch: 350, loss is 5.00056224822998 and perplexity is 148.49662760139677
At time: 587.3563725948334 and batch: 400, loss is 5.02495246887207 and perplexity is 152.16302346064853
At time: 588.5244393348694 and batch: 450, loss is 4.9856191253662105 and perplexity is 146.29412139497109
At time: 589.694591999054 and batch: 500, loss is 4.96239294052124 and perplexity is 142.93542295286326
At time: 590.8636972904205 and batch: 550, loss is 4.969888544082641 and perplexity is 144.01083561661144
At time: 592.0349113941193 and batch: 600, loss is 4.94583872795105 and perplexity is 140.588717102313
At time: 593.2064027786255 and batch: 650, loss is 4.930337734222412 and perplexity is 138.42625572993552
At time: 594.374502658844 and batch: 700, loss is 4.924432201385498 and perplexity is 137.6111840164379
At time: 595.5459225177765 and batch: 750, loss is 4.911131448745728 and perplexity is 135.79297030770215
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.070686162904251 and perplexity of 159.28358434179222
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 598.9419147968292 and batch: 50, loss is 5.013042154312134 and perplexity is 150.3614638543711
At time: 600.166827917099 and batch: 100, loss is 5.011018409729004 and perplexity is 150.05747835463723
At time: 601.3382484912872 and batch: 150, loss is 4.990334577560425 and perplexity is 146.98559335091198
At time: 602.5777180194855 and batch: 200, loss is 4.997839307785034 and perplexity is 148.09283013588114
At time: 603.753298997879 and batch: 250, loss is 5.0041757106781 and perplexity is 149.03418522613995
At time: 604.9280872344971 and batch: 300, loss is 5.0125504493713375 and perplexity is 150.28754855343706
At time: 606.1039476394653 and batch: 350, loss is 4.992750148773194 and perplexity is 147.34107669371363
At time: 607.2841551303864 and batch: 400, loss is 5.016576433181763 and perplexity is 150.89382339827415
At time: 608.462025642395 and batch: 450, loss is 4.977586154937744 and perplexity is 145.12365250897557
At time: 609.6491134166718 and batch: 500, loss is 4.94822476387024 and perplexity is 140.92456734718817
At time: 610.8266084194183 and batch: 550, loss is 4.951942958831787 and perplexity is 141.4495277111501
At time: 611.9963119029999 and batch: 600, loss is 4.927265005111694 and perplexity is 138.00156216256136
At time: 613.1691269874573 and batch: 650, loss is 4.90920163154602 and perplexity is 135.53116739513518
At time: 614.3428378105164 and batch: 700, loss is 4.901777200698852 and perplexity is 134.52865177419028
At time: 615.5181992053986 and batch: 750, loss is 4.892719058990479 and perplexity is 133.31557458950124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.064185918763626 and perplexity of 158.25155999493967
Finished 31 epochs...
Completing Train Step...
At time: 618.9131045341492 and batch: 50, loss is 5.009230136871338 and perplexity is 149.78937443193777
At time: 620.1360869407654 and batch: 100, loss is 5.007729349136352 and perplexity is 149.56474098168567
At time: 621.3088879585266 and batch: 150, loss is 4.987416610717774 and perplexity is 146.55731941162307
At time: 622.4816157817841 and batch: 200, loss is 4.995387229919434 and perplexity is 147.7301398392037
At time: 623.6550388336182 and batch: 250, loss is 5.001777572631836 and perplexity is 148.67720888670857
At time: 624.8290350437164 and batch: 300, loss is 5.010655555725098 and perplexity is 150.00303927511655
At time: 625.9967339038849 and batch: 350, loss is 4.991126070022583 and perplexity is 147.10197739256503
At time: 627.176659822464 and batch: 400, loss is 5.015091457366943 and perplexity is 150.66991600959636
At time: 628.3734126091003 and batch: 450, loss is 4.976326789855957 and perplexity is 144.94100388325518
At time: 629.5488846302032 and batch: 500, loss is 4.9473925304412845 and perplexity is 140.80733400080217
At time: 630.7237932682037 and batch: 550, loss is 4.951534156799316 and perplexity is 141.3917146745783
At time: 631.9003946781158 and batch: 600, loss is 4.9272517967224125 and perplexity is 137.99973939624476
At time: 633.1331415176392 and batch: 650, loss is 4.909868516921997 and perplexity is 135.6215812931624
At time: 634.3103175163269 and batch: 700, loss is 4.90296763420105 and perplexity is 134.68889454852754
At time: 635.4780938625336 and batch: 750, loss is 4.8937087059021 and perplexity is 133.44757524241047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.063516306322675 and perplexity of 158.14562825213164
Finished 32 epochs...
Completing Train Step...
At time: 638.8818237781525 and batch: 50, loss is 5.007790927886963 and perplexity is 149.57395127514698
At time: 640.0818076133728 and batch: 100, loss is 5.006081800460816 and perplexity is 149.31852866985204
At time: 641.2571189403534 and batch: 150, loss is 4.985873222351074 and perplexity is 146.33129901328115
At time: 642.4265675544739 and batch: 200, loss is 4.99401858329773 and perplexity is 147.5280877828613
At time: 643.5956790447235 and batch: 250, loss is 5.0005346775054935 and perplexity is 148.49253349822897
At time: 644.7707998752594 and batch: 300, loss is 5.009633989334106 and perplexity is 149.84987945642786
At time: 645.949889421463 and batch: 350, loss is 4.990166568756104 and perplexity is 146.9609005514826
At time: 647.1254773139954 and batch: 400, loss is 5.014257936477661 and perplexity is 150.5443818121719
At time: 648.2980396747589 and batch: 450, loss is 4.975733194351196 and perplexity is 144.85499308523165
At time: 649.4729096889496 and batch: 500, loss is 4.947162733078003 and perplexity is 140.77498056422772
At time: 650.6576247215271 and batch: 550, loss is 4.9515533447265625 and perplexity is 141.39442771454142
At time: 651.8285582065582 and batch: 600, loss is 4.927491950988769 and perplexity is 138.0328846022389
At time: 653.0005874633789 and batch: 650, loss is 4.910503377914429 and perplexity is 135.70770948167817
At time: 654.1809961795807 and batch: 700, loss is 4.903762140274048 and perplexity is 134.7959482149744
At time: 655.3597631454468 and batch: 750, loss is 4.8942158412933345 and perplexity is 133.51526839403604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.063160741051962 and perplexity of 158.0894071547373
Finished 33 epochs...
Completing Train Step...
At time: 658.7698397636414 and batch: 50, loss is 5.006637258529663 and perplexity is 149.4014918905922
At time: 660.0048344135284 and batch: 100, loss is 5.004743585586548 and perplexity is 149.11884203539464
At time: 661.1790156364441 and batch: 150, loss is 4.9844843196868895 and perplexity is 146.1282001571482
At time: 662.3810429573059 and batch: 200, loss is 4.992877168655395 and perplexity is 147.35979312857324
At time: 663.5597534179688 and batch: 250, loss is 4.99954197883606 and perplexity is 148.34519829963156
At time: 664.7318398952484 and batch: 300, loss is 5.008757667541504 and perplexity is 149.71862026248738
At time: 665.9114644527435 and batch: 350, loss is 4.989346199035644 and perplexity is 146.84038771787732
At time: 667.0876607894897 and batch: 400, loss is 5.013532123565674 and perplexity is 150.43515440015358
At time: 668.2629263401031 and batch: 450, loss is 4.97521481513977 and perplexity is 144.77992272728252
At time: 669.4413115978241 and batch: 500, loss is 4.9469114112854005 and perplexity is 140.73960518924622
At time: 670.6150496006012 and batch: 550, loss is 4.951466360092163 and perplexity is 141.38212910684328
At time: 671.7885200977325 and batch: 600, loss is 4.927512674331665 and perplexity is 138.03574513467723
At time: 672.9656195640564 and batch: 650, loss is 4.91084436416626 and perplexity is 135.75399183525354
At time: 674.1407108306885 and batch: 700, loss is 4.904113368988037 and perplexity is 134.84330073781297
At time: 675.31378698349 and batch: 750, loss is 4.894421501159668 and perplexity is 133.5427299500606
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062838177348292 and perplexity of 158.0384214735616
Finished 34 epochs...
Completing Train Step...
At time: 678.6954817771912 and batch: 50, loss is 5.005646553039551 and perplexity is 149.2535523067244
At time: 679.8906168937683 and batch: 100, loss is 5.0036960792541505 and perplexity is 148.96272088730404
At time: 681.0557110309601 and batch: 150, loss is 4.98356538772583 and perplexity is 145.99397996266993
At time: 682.2224292755127 and batch: 200, loss is 4.992094926834106 and perplexity is 147.2445672087538
At time: 683.3915929794312 and batch: 250, loss is 4.99889217376709 and perplexity is 148.2488341501663
At time: 684.5572464466095 and batch: 300, loss is 5.008163661956787 and perplexity is 149.62971297424238
At time: 685.7288041114807 and batch: 350, loss is 4.988810062408447 and perplexity is 146.76168230798174
At time: 686.9000160694122 and batch: 400, loss is 5.013045320510864 and perplexity is 150.36193992940065
At time: 688.0691194534302 and batch: 450, loss is 4.974927644729615 and perplexity is 144.73835218669092
At time: 689.2370598316193 and batch: 500, loss is 4.946832208633423 and perplexity is 140.72845868069933
At time: 690.4032766819 and batch: 550, loss is 4.951569910049439 and perplexity is 141.3967699782895
At time: 691.5693018436432 and batch: 600, loss is 4.927685632705688 and perplexity is 138.05962163747378
At time: 692.7984383106232 and batch: 650, loss is 4.911208457946778 and perplexity is 135.80342801851856
At time: 693.9647214412689 and batch: 700, loss is 4.9045473575592045 and perplexity is 134.90183388967242
At time: 695.1337745189667 and batch: 750, loss is 4.894616651535034 and perplexity is 133.56879340700175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062724978424782 and perplexity of 158.02053270689134
Finished 35 epochs...
Completing Train Step...
At time: 698.5174646377563 and batch: 50, loss is 5.0048308944702145 and perplexity is 149.1318620033976
At time: 699.7440595626831 and batch: 100, loss is 5.0028419685363765 and perplexity is 148.83554454991315
At time: 700.9139280319214 and batch: 150, loss is 4.982809839248657 and perplexity is 145.8837160935273
At time: 702.083110332489 and batch: 200, loss is 4.991448163986206 and perplexity is 147.14936568285165
At time: 703.2605307102203 and batch: 250, loss is 4.998334054946899 and perplexity is 148.16611677096597
At time: 704.4315316677094 and batch: 300, loss is 5.007667789459228 and perplexity is 149.55553410791072
At time: 705.6017441749573 and batch: 350, loss is 4.9883520221710205 and perplexity is 146.69447494518545
At time: 706.7721745967865 and batch: 400, loss is 5.012620582580566 and perplexity is 150.29808907114005
At time: 707.9693009853363 and batch: 450, loss is 4.974686059951782 and perplexity is 144.70338982739102
At time: 709.1695952415466 and batch: 500, loss is 4.946752605438232 and perplexity is 140.71725669159702
At time: 710.3697080612183 and batch: 550, loss is 4.951645755767823 and perplexity is 141.40749472459362
At time: 711.5727043151855 and batch: 600, loss is 4.927802457809448 and perplexity is 138.0757514092594
At time: 712.7544076442719 and batch: 650, loss is 4.911493015289307 and perplexity is 135.84207737982084
At time: 713.9463186264038 and batch: 700, loss is 4.9048826694488525 and perplexity is 134.94707566313934
At time: 715.1239864826202 and batch: 750, loss is 4.894742240905762 and perplexity is 133.5855692811283
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062628812568132 and perplexity of 158.00533725764865
Finished 36 epochs...
Completing Train Step...
At time: 718.505175113678 and batch: 50, loss is 5.004103317260742 and perplexity is 149.02339652268847
At time: 719.7050096988678 and batch: 100, loss is 5.002103271484375 and perplexity is 148.72564076971798
At time: 720.881843328476 and batch: 150, loss is 4.982146606445313 and perplexity is 145.78699330594864
At time: 722.1146280765533 and batch: 200, loss is 4.990872602462769 and perplexity is 147.06469653825474
At time: 723.2864127159119 and batch: 250, loss is 4.997864618301391 and perplexity is 148.0965784893169
At time: 724.4592730998993 and batch: 300, loss is 5.007220697402954 and perplexity is 149.48868396183744
At time: 725.6333582401276 and batch: 350, loss is 4.987939424514771 and perplexity is 146.6339616333221
At time: 726.8043251037598 and batch: 400, loss is 5.012231130599975 and perplexity is 150.23956657926215
At time: 727.979573726654 and batch: 450, loss is 4.974469957351684 and perplexity is 144.67212242721124
At time: 729.1518185138702 and batch: 500, loss is 4.9466957759857175 and perplexity is 140.70926003416486
At time: 730.3251669406891 and batch: 550, loss is 4.951684255599975 and perplexity is 141.41293899420668
At time: 731.4915721416473 and batch: 600, loss is 4.9278928089141845 and perplexity is 138.08822726953196
At time: 732.6576709747314 and batch: 650, loss is 4.911725568771362 and perplexity is 135.87367160145587
At time: 733.8339102268219 and batch: 700, loss is 4.905137424468994 and perplexity is 134.98145848753055
At time: 735.0108332633972 and batch: 750, loss is 4.894805669784546 and perplexity is 133.59404273273745
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062547195789426 and perplexity of 157.99244189724936
Finished 37 epochs...
Completing Train Step...
At time: 738.4376854896545 and batch: 50, loss is 5.003435134887695 and perplexity is 148.92385497561756
At time: 739.6613037586212 and batch: 100, loss is 5.0014464473724365 and perplexity is 148.62798625722692
At time: 740.8412969112396 and batch: 150, loss is 4.9815389347076415 and perplexity is 145.69842958195787
At time: 742.0171644687653 and batch: 200, loss is 4.990360383987427 and perplexity is 146.98938657284162
At time: 743.1883189678192 and batch: 250, loss is 4.997444648742675 and perplexity is 148.03439549299813
At time: 744.3594624996185 and batch: 300, loss is 5.006816482543945 and perplexity is 149.42827062533306
At time: 745.5368604660034 and batch: 350, loss is 4.987567596435547 and perplexity is 146.57944914428404
At time: 746.7144696712494 and batch: 400, loss is 5.011876077651977 and perplexity is 150.18623304691647
At time: 747.8933005332947 and batch: 450, loss is 4.974273319244385 and perplexity is 144.64367717168042
At time: 749.0773487091064 and batch: 500, loss is 4.946641569137573 and perplexity is 140.70163283539875
At time: 750.251681804657 and batch: 550, loss is 4.951711273193359 and perplexity is 141.41675968310432
At time: 751.4269394874573 and batch: 600, loss is 4.927967977523804 and perplexity is 138.0986075597117
At time: 752.6566369533539 and batch: 650, loss is 4.9119228267669675 and perplexity is 135.90047641321632
At time: 753.8297791481018 and batch: 700, loss is 4.905338039398194 and perplexity is 135.00854049970542
At time: 755.0037441253662 and batch: 750, loss is 4.8948402595520015 and perplexity is 133.5986637995294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062489354333212 and perplexity of 157.98330364862656
Finished 38 epochs...
Completing Train Step...
At time: 758.3725457191467 and batch: 50, loss is 5.002821426391602 and perplexity is 148.83248718001198
At time: 759.5748686790466 and batch: 100, loss is 5.00084508895874 and perplexity is 148.53863443610777
At time: 760.7424101829529 and batch: 150, loss is 4.98099063873291 and perplexity is 145.61856561603972
At time: 761.9098007678986 and batch: 200, loss is 4.98989387512207 and perplexity is 146.92083071309358
At time: 763.0797262191772 and batch: 250, loss is 4.997063140869141 and perplexity is 147.97792997726793
At time: 764.2513089179993 and batch: 300, loss is 5.006433572769165 and perplexity is 149.37106403306277
At time: 765.4311504364014 and batch: 350, loss is 4.987228908538818 and perplexity is 146.52981286502776
At time: 766.6041567325592 and batch: 400, loss is 5.011537446975708 and perplexity is 150.13538399126438
At time: 767.7730753421783 and batch: 450, loss is 4.974090585708618 and perplexity is 144.61724833591538
At time: 768.9450016021729 and batch: 500, loss is 4.946581592559815 and perplexity is 140.69319428603643
At time: 770.1170501708984 and batch: 550, loss is 4.9517223072052 and perplexity is 141.41832008591393
At time: 771.2892253398895 and batch: 600, loss is 4.928021717071533 and perplexity is 138.10602911583774
At time: 772.4577062129974 and batch: 650, loss is 4.912070016860962 and perplexity is 135.9204810893225
At time: 773.6340053081512 and batch: 700, loss is 4.90548680305481 and perplexity is 135.0286263578504
At time: 774.8070702552795 and batch: 750, loss is 4.894854965209961 and perplexity is 133.60062847022897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0624272546102835 and perplexity of 157.97349323385785
Finished 39 epochs...
Completing Train Step...
At time: 778.1940455436707 and batch: 50, loss is 5.002236824035645 and perplexity is 148.7455047848974
At time: 779.3943190574646 and batch: 100, loss is 5.000291652679444 and perplexity is 148.4564505108143
At time: 780.5678701400757 and batch: 150, loss is 4.980476560592652 and perplexity is 145.54372553311424
At time: 781.7713923454285 and batch: 200, loss is 4.989468717575074 and perplexity is 146.85837948985005
At time: 782.9545741081238 and batch: 250, loss is 4.996707162857056 and perplexity is 147.92526246271686
At time: 784.1275062561035 and batch: 300, loss is 5.0060666847229 and perplexity is 149.3162716271652
At time: 785.3024725914001 and batch: 350, loss is 4.986915063858032 and perplexity is 146.48383247841082
At time: 786.4770374298096 and batch: 400, loss is 5.011221418380737 and perplexity is 150.08794441333265
At time: 787.6494219303131 and batch: 450, loss is 4.973921394348144 and perplexity is 144.59278241669088
At time: 788.8554027080536 and batch: 500, loss is 4.946518478393554 and perplexity is 140.68431483259317
At time: 790.0619592666626 and batch: 550, loss is 4.951726198196411 and perplexity is 141.418870344425
At time: 791.2557756900787 and batch: 600, loss is 4.928063764572143 and perplexity is 138.11183625126816
At time: 792.4351000785828 and batch: 650, loss is 4.912193984985351 and perplexity is 135.93733194089182
At time: 793.6128549575806 and batch: 700, loss is 4.905611944198609 and perplexity is 135.0455250519373
At time: 794.7851197719574 and batch: 750, loss is 4.8948438930511475 and perplexity is 133.59914923104213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062389285065407 and perplexity of 157.96749516608992
Finished 40 epochs...
Completing Train Step...
At time: 798.1888518333435 and batch: 50, loss is 5.001698331832886 and perplexity is 148.66542805265826
At time: 799.3908939361572 and batch: 100, loss is 4.999744968414307 and perplexity is 148.37531388534092
At time: 800.564469575882 and batch: 150, loss is 4.979992523193359 and perplexity is 145.47329397385334
At time: 801.7417004108429 and batch: 200, loss is 4.989066047668457 and perplexity is 146.79925594433016
At time: 802.9209821224213 and batch: 250, loss is 4.9963663387298585 and perplexity is 147.87485455485114
At time: 804.1031172275543 and batch: 300, loss is 5.005729341506958 and perplexity is 149.26590929107587
At time: 805.275294303894 and batch: 350, loss is 4.986600723266601 and perplexity is 146.43779390015052
At time: 806.4529514312744 and batch: 400, loss is 5.010940046310425 and perplexity is 150.04571979837635
At time: 807.6471915245056 and batch: 450, loss is 4.973756170272827 and perplexity is 144.56889418142865
At time: 808.8215501308441 and batch: 500, loss is 4.946441040039063 and perplexity is 140.67342089255962
At time: 809.9951205253601 and batch: 550, loss is 4.951708126068115 and perplexity is 141.41631462755032
At time: 811.1681714057922 and batch: 600, loss is 4.928081884384155 and perplexity is 138.11433883445073
At time: 812.3972399234772 and batch: 650, loss is 4.912277364730835 and perplexity is 135.9486668335745
At time: 813.5742573738098 and batch: 700, loss is 4.905697450637818 and perplexity is 135.0570728076123
At time: 814.7461361885071 and batch: 750, loss is 4.894776315689087 and perplexity is 133.59012125801024
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0623424441315406 and perplexity of 157.96009599438915
Finished 41 epochs...
Completing Train Step...
At time: 818.1211574077606 and batch: 50, loss is 5.00118049621582 and perplexity is 148.58846372814037
At time: 819.3232460021973 and batch: 100, loss is 4.999178056716919 and perplexity is 148.29122202287815
At time: 820.4903457164764 and batch: 150, loss is 4.979522314071655 and perplexity is 145.40490718336886
At time: 821.6567664146423 and batch: 200, loss is 4.9886686229705814 and perplexity is 146.7409258860588
At time: 822.8273370265961 and batch: 250, loss is 4.996034698486328 and perplexity is 147.82582143320218
At time: 824.0025813579559 and batch: 300, loss is 5.005399751663208 and perplexity is 149.21672086980266
At time: 825.1708915233612 and batch: 350, loss is 4.986307802200318 and perplexity is 146.39490546718648
At time: 826.3402173519135 and batch: 400, loss is 5.010666313171387 and perplexity is 150.0046529334342
At time: 827.5116968154907 and batch: 450, loss is 4.973589601516724 and perplexity is 144.5448155259852
At time: 828.6836643218994 and batch: 500, loss is 4.94635269165039 and perplexity is 140.66099317148735
At time: 829.8554222583771 and batch: 550, loss is 4.9516717720031735 and perplexity is 141.4111736631126
At time: 831.0203084945679 and batch: 600, loss is 4.928071641921997 and perplexity is 138.11292421080645
At time: 832.187178850174 and batch: 650, loss is 4.91234130859375 and perplexity is 135.95736019443055
At time: 833.3567371368408 and batch: 700, loss is 4.905765609741211 and perplexity is 135.06627849032375
At time: 834.5261116027832 and batch: 750, loss is 4.894729204177857 and perplexity is 133.58382777376133
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062319733375727 and perplexity of 157.95650864195662
Finished 42 epochs...
Completing Train Step...
At time: 837.9018304347992 and batch: 50, loss is 5.000707197189331 and perplexity is 148.51815359308208
At time: 839.1361231803894 and batch: 100, loss is 4.998433980941773 and perplexity is 148.18092315734992
At time: 840.304833650589 and batch: 150, loss is 4.9790260696411135 and perplexity is 145.33276870864424
At time: 841.5067944526672 and batch: 200, loss is 4.988293571472168 and perplexity is 146.68590080119188
At time: 842.6759440898895 and batch: 250, loss is 4.995710296630859 and perplexity is 147.77787423994275
At time: 843.8501355648041 and batch: 300, loss is 5.005084972381592 and perplexity is 149.1697579294703
At time: 845.0190682411194 and batch: 350, loss is 4.986009731292724 and perplexity is 146.35127590752106
At time: 846.1893901824951 and batch: 400, loss is 5.010369062423706 and perplexity is 149.9600705645936
At time: 847.3708307743073 and batch: 450, loss is 4.973396730422974 and perplexity is 144.51693969762596
At time: 848.5464675426483 and batch: 500, loss is 4.9462285900115965 and perplexity is 140.64353799485082
At time: 849.7210509777069 and batch: 550, loss is 4.951575765609741 and perplexity is 141.3975979380268
At time: 850.8937077522278 and batch: 600, loss is 4.928015956878662 and perplexity is 138.10523360076454
At time: 852.063896894455 and batch: 650, loss is 4.912343168258667 and perplexity is 135.9576130297986
At time: 853.2365288734436 and batch: 700, loss is 4.905783519744873 and perplexity is 135.0686975495288
At time: 854.4057931900024 and batch: 750, loss is 4.89466812133789 and perplexity is 133.57566834339045
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.06229950660883 and perplexity of 157.953313724788
Finished 43 epochs...
Completing Train Step...
At time: 857.7850258350372 and batch: 50, loss is 5.00032133102417 and perplexity is 148.46085651791063
At time: 858.9819445610046 and batch: 100, loss is 4.9978054428100585 and perplexity is 148.08781506081274
At time: 860.1498680114746 and batch: 150, loss is 4.978580198287964 and perplexity is 145.26798343442536
At time: 861.3418164253235 and batch: 200, loss is 4.987944822311402 and perplexity is 146.63475313576237
At time: 862.520555973053 and batch: 250, loss is 4.995386228561402 and perplexity is 147.72999190851567
At time: 863.6988739967346 and batch: 300, loss is 5.004790210723877 and perplexity is 149.12579488397049
At time: 864.8678541183472 and batch: 350, loss is 4.985708456039429 and perplexity is 146.3071905310519
At time: 866.0690984725952 and batch: 400, loss is 5.010134687423706 and perplexity is 149.92492779150962
At time: 867.2411048412323 and batch: 450, loss is 4.973194618225097 and perplexity is 144.48773401282526
At time: 868.4116778373718 and batch: 500, loss is 4.946094026565552 and perplexity is 140.62461378899607
At time: 869.5845866203308 and batch: 550, loss is 4.951472387313843 and perplexity is 141.382981250845
At time: 870.7593364715576 and batch: 600, loss is 4.927954874038696 and perplexity is 138.09679799851997
At time: 872.0053915977478 and batch: 650, loss is 4.912327375411987 and perplexity is 135.9554658890158
At time: 873.17378282547 and batch: 700, loss is 4.9057767295837404 and perplexity is 135.06778041442217
At time: 874.3498384952545 and batch: 750, loss is 4.894598655700683 and perplexity is 133.56638974674917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062287086664244 and perplexity of 157.95135196556691
Finished 44 epochs...
Completing Train Step...
At time: 877.7440824508667 and batch: 50, loss is 4.999816198348999 and perplexity is 148.385883025674
At time: 878.953861951828 and batch: 100, loss is 4.997245235443115 and perplexity is 148.00487840889184
At time: 880.1234047412872 and batch: 150, loss is 4.978160200119018 and perplexity is 145.20698395810734
At time: 881.298047542572 and batch: 200, loss is 4.98763222694397 and perplexity is 146.58892295475192
At time: 882.4710879325867 and batch: 250, loss is 4.995097894668579 and perplexity is 147.68740248514413
At time: 883.6484320163727 and batch: 300, loss is 5.0045115089416505 and perplexity is 149.08423905027178
At time: 884.824919462204 and batch: 350, loss is 4.9854519748687744 and perplexity is 146.26967030337093
At time: 885.9996752738953 and batch: 400, loss is 5.009885683059692 and perplexity is 149.8876004777399
At time: 887.1734385490417 and batch: 450, loss is 4.973018522262573 and perplexity is 144.46229254636688
At time: 888.3409435749054 and batch: 500, loss is 4.94598876953125 and perplexity is 140.60981283816454
At time: 889.5145926475525 and batch: 550, loss is 4.95139910697937 and perplexity is 141.37262103829465
At time: 890.7042942047119 and batch: 600, loss is 4.9279016017913815 and perplexity is 138.08944146769483
At time: 891.877678155899 and batch: 650, loss is 4.912342586517334 and perplexity is 135.95753393765855
At time: 893.050023317337 and batch: 700, loss is 4.905800838470459 and perplexity is 135.07103678749306
At time: 894.2233672142029 and batch: 750, loss is 4.894551258087159 and perplexity is 133.5600591686564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062259762786155 and perplexity of 157.94703618104404
Finished 45 epochs...
Completing Train Step...
At time: 897.6344046592712 and batch: 50, loss is 4.999365234375 and perplexity is 148.3189814244193
At time: 898.860002040863 and batch: 100, loss is 4.996812934875488 and perplexity is 147.94090964380712
At time: 900.0351960659027 and batch: 150, loss is 4.977759294509887 and perplexity is 145.14878133141286
At time: 901.2655832767487 and batch: 200, loss is 4.9873061370849605 and perplexity is 146.54112958642315
At time: 902.4350726604462 and batch: 250, loss is 4.994819555282593 and perplexity is 147.64630098456718
At time: 903.6087069511414 and batch: 300, loss is 5.004233322143555 and perplexity is 149.04277155128662
At time: 904.7769877910614 and batch: 350, loss is 4.985204753875732 and perplexity is 146.2335138397291
At time: 905.9475309848785 and batch: 400, loss is 5.009639072418213 and perplexity is 149.8506411579044
At time: 907.1216697692871 and batch: 450, loss is 4.972846803665161 and perplexity is 144.4374878138897
At time: 908.29044008255 and batch: 500, loss is 4.945882081985474 and perplexity is 140.5948123225191
At time: 909.4597675800323 and batch: 550, loss is 4.951325330734253 and perplexity is 141.36219148188366
At time: 910.6302800178528 and batch: 600, loss is 4.9278427028656 and perplexity is 138.08130838744788
At time: 911.7989573478699 and batch: 650, loss is 4.9123496818542485 and perplexity is 135.95849860559022
At time: 912.9719362258911 and batch: 700, loss is 4.905816669464111 and perplexity is 135.073175113145
At time: 914.1543576717377 and batch: 750, loss is 4.894499263763428 and perplexity is 133.55311498423313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062232084052507 and perplexity of 157.9426644676011
Finished 46 epochs...
Completing Train Step...
At time: 917.5242009162903 and batch: 50, loss is 4.9989353466033934 and perplexity is 148.25523461097725
At time: 918.7266261577606 and batch: 100, loss is 4.99640513420105 and perplexity is 147.88059154079474
At time: 919.8991775512695 and batch: 150, loss is 4.9773802566528325 and perplexity is 145.09377487380635
At time: 921.0722887516022 and batch: 200, loss is 4.986986789703369 and perplexity is 146.49433953193264
At time: 922.2476949691772 and batch: 250, loss is 4.994546566009522 and perplexity is 147.60600062922236
At time: 923.4192237854004 and batch: 300, loss is 5.003952264785767 and perplexity is 149.00088786985114
At time: 924.5918092727661 and batch: 350, loss is 4.984958820343017 and perplexity is 146.19755453704963
At time: 925.7652986049652 and batch: 400, loss is 5.00939757347107 and perplexity is 149.81445675525947
At time: 926.9388155937195 and batch: 450, loss is 4.972673082351685 and perplexity is 144.41239812316164
At time: 928.1121337413788 and batch: 500, loss is 4.945767488479614 and perplexity is 140.57870199315659
At time: 929.2830038070679 and batch: 550, loss is 4.951231317520142 and perplexity is 141.3489021926028
At time: 930.4568574428558 and batch: 600, loss is 4.927773303985596 and perplexity is 138.07172603180246
At time: 931.683210849762 and batch: 650, loss is 4.912345132827759 and perplexity is 135.95788012818528
At time: 932.8539514541626 and batch: 700, loss is 4.905821418762207 and perplexity is 135.07381661744162
At time: 934.0236191749573 and batch: 750, loss is 4.894435844421387 and perplexity is 133.5446454021238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062196953352108 and perplexity of 157.937115928638
Finished 47 epochs...
Completing Train Step...
At time: 937.3897383213043 and batch: 50, loss is 4.9984627532958985 and perplexity is 148.18518673268198
At time: 938.6205401420593 and batch: 100, loss is 4.995981693267822 and perplexity is 147.81798610089487
At time: 939.7921590805054 and batch: 150, loss is 4.976989049911499 and perplexity is 145.03702431227504
At time: 940.9884901046753 and batch: 200, loss is 4.98649109840393 and perplexity is 146.4217415569884
At time: 942.1931142807007 and batch: 250, loss is 4.994209461212158 and perplexity is 147.55625032429495
At time: 943.3875060081482 and batch: 300, loss is 5.003632135391236 and perplexity is 148.9531959400344
At time: 944.582124710083 and batch: 350, loss is 4.984624338150025 and perplexity is 146.14866223565585
At time: 945.7756037712097 and batch: 400, loss is 5.009152812957764 and perplexity is 149.77779257907773
At time: 946.9766099452972 and batch: 450, loss is 4.972440795898438 and perplexity is 144.37885697512465
At time: 948.1539008617401 and batch: 500, loss is 4.945587558746338 and perplexity is 140.55340998026344
At time: 949.3387343883514 and batch: 550, loss is 4.9511207008361815 and perplexity is 141.33326751050512
At time: 950.5173692703247 and batch: 600, loss is 4.92770770072937 and perplexity is 138.06266837409123
At time: 951.6953091621399 and batch: 650, loss is 4.912329235076904 and perplexity is 135.9557187208611
At time: 952.8730990886688 and batch: 700, loss is 4.905791864395142 and perplexity is 135.06982465527472
At time: 954.0522491931915 and batch: 750, loss is 4.894297685623169 and perplexity is 133.52619630888822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062146918718205 and perplexity of 157.92921380055455
Finished 48 epochs...
Completing Train Step...
At time: 957.4309968948364 and batch: 50, loss is 4.998056135177612 and perplexity is 148.1249441995776
At time: 958.628114938736 and batch: 100, loss is 4.995601119995118 and perplexity is 147.7617412294861
At time: 959.791562795639 and batch: 150, loss is 4.976622610092163 and perplexity is 144.98388670775083
At time: 960.986670255661 and batch: 200, loss is 4.986194715499878 and perplexity is 146.37835108642383
At time: 962.1566295623779 and batch: 250, loss is 4.9939661693573 and perplexity is 147.52035545709916
At time: 963.3226397037506 and batch: 300, loss is 5.003345680236817 and perplexity is 148.91053363999978
At time: 964.488032579422 and batch: 350, loss is 4.984386529922485 and perplexity is 146.11391101355937
At time: 965.6597166061401 and batch: 400, loss is 5.00888994216919 and perplexity is 149.73842554707966
At time: 966.8275949954987 and batch: 450, loss is 4.972264909744263 and perplexity is 144.353464966344
At time: 967.9938468933105 and batch: 500, loss is 4.945462055206299 and perplexity is 140.5357711366382
At time: 969.1596391201019 and batch: 550, loss is 4.951025657653808 and perplexity is 141.3198353853118
At time: 970.3242709636688 and batch: 600, loss is 4.927631616592407 and perplexity is 138.05216439471934
At time: 971.4891383647919 and batch: 650, loss is 4.912302131652832 and perplexity is 135.95203390529736
At time: 972.6679825782776 and batch: 700, loss is 4.905788555145263 and perplexity is 135.06937767621346
At time: 973.8314921855927 and batch: 750, loss is 4.894255199432373 and perplexity is 133.5205234099464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.0621337890625 and perplexity of 157.92714025796408
Finished 49 epochs...
Completing Train Step...
At time: 977.201233625412 and batch: 50, loss is 4.9976576232910155 and perplexity is 148.06592640903918
At time: 978.4048070907593 and batch: 100, loss is 4.995244388580322 and perplexity is 147.70903937524707
At time: 979.5874953269958 and batch: 150, loss is 4.976293258666992 and perplexity is 144.9361439205458
At time: 980.7591826915741 and batch: 200, loss is 4.985919008255005 and perplexity is 146.33799907746288
At time: 981.9350261688232 and batch: 250, loss is 4.9937364292144775 and perplexity is 147.48646800237057
At time: 983.1255211830139 and batch: 300, loss is 5.0030802059173585 and perplexity is 148.87100696431236
At time: 984.3035409450531 and batch: 350, loss is 4.984140901565552 and perplexity is 146.07802570107762
At time: 985.4722762107849 and batch: 400, loss is 5.008636980056763 and perplexity is 149.7005521891061
At time: 986.6615109443665 and batch: 450, loss is 4.972090730667114 and perplexity is 144.32832380262883
At time: 987.8457009792328 and batch: 500, loss is 4.94535551071167 and perplexity is 140.52079862155878
At time: 989.024534702301 and batch: 550, loss is 4.9509368419647215 and perplexity is 141.30728452411535
At time: 990.204532623291 and batch: 600, loss is 4.927565498352051 and perplexity is 138.0430369302814
At time: 991.4626648426056 and batch: 650, loss is 4.912262840270996 and perplexity is 135.94669226696266
At time: 992.6484756469727 and batch: 700, loss is 4.905733089447022 and perplexity is 135.06188616663243
At time: 993.8362991809845 and batch: 750, loss is 4.894109382629394 and perplexity is 133.50105529351444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.062100077784339 and perplexity of 157.92181642194686
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8188e94d30>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 3.715717601761124, 'num_layers': 1, 'lr': 4.9817027769186435, 'dropout': 0.8872043072898154}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8129136562347412 and batch: 50, loss is 8.243636169433593 and perplexity is 3803.34480133465
At time: 2.982203960418701 and batch: 100, loss is 7.1227611064910885 and perplexity is 1239.8691222934178
At time: 4.14871072769165 and batch: 150, loss is 6.925876455307007 and perplexity is 1018.2863611128939
At time: 5.315798282623291 and batch: 200, loss is 6.8572992420196535 and perplexity is 950.7957276217596
At time: 6.484294414520264 and batch: 250, loss is 6.834508075714111 and perplexity is 929.3710580640106
At time: 7.657843112945557 and batch: 300, loss is 6.76039716720581 and perplexity is 862.984877026123
At time: 8.82371187210083 and batch: 350, loss is 6.660558662414551 and perplexity is 780.9871234271204
At time: 10.014143228530884 and batch: 400, loss is 6.660274753570556 and perplexity is 780.7654257481888
At time: 11.214670181274414 and batch: 450, loss is 6.616703605651855 and perplexity is 747.4770523915358
At time: 12.41146206855774 and batch: 500, loss is 6.596199111938477 and perplexity is 732.3064778603158
At time: 13.589976787567139 and batch: 550, loss is 6.608214864730835 and perplexity is 741.15876842777
At time: 14.758073568344116 and batch: 600, loss is 6.586704292297363 and perplexity is 725.3862650064198
At time: 15.929481267929077 and batch: 650, loss is 6.576390409469605 and perplexity is 717.9431656720717
At time: 17.103557109832764 and batch: 700, loss is 6.531345691680908 and perplexity is 686.3211670306374
At time: 18.274336099624634 and batch: 750, loss is 6.499482326507568 and perplexity is 664.797395961116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 5.860001320062682 and perplexity of 350.7246069980731
Finished 1 epochs...
Completing Train Step...
At time: 21.81006693840027 and batch: 50, loss is 6.051693420410157 and perplexity is 424.83184015813794
At time: 22.985971689224243 and batch: 100, loss is 5.796488647460937 and perplexity is 329.1417955602663
At time: 24.15695023536682 and batch: 150, loss is 5.67341724395752 and perplexity is 291.0273485297772
At time: 25.32988977432251 and batch: 200, loss is 5.596627702713013 and perplexity is 269.51598515614154
At time: 26.499943733215332 and batch: 250, loss is 5.573228721618652 and perplexity is 263.2827952542723
At time: 27.667263746261597 and batch: 300, loss is 5.528891000747681 and perplexity is 251.86443861070236
At time: 28.836947917938232 and batch: 350, loss is 5.4164489650726315 and perplexity is 225.07844031524402
At time: 30.014860153198242 and batch: 400, loss is 5.422447919845581 and perplexity is 226.43273381031165
At time: 31.182544469833374 and batch: 450, loss is 5.345140781402588 and perplexity is 209.58738852854137
At time: 32.35145282745361 and batch: 500, loss is 5.319183397293091 and perplexity is 204.21704968881036
At time: 33.51752066612244 and batch: 550, loss is 5.314147491455078 and perplexity is 203.19121702237274
At time: 34.74047923088074 and batch: 600, loss is 5.254391136169434 and perplexity is 191.40491084161985
At time: 35.908395528793335 and batch: 650, loss is 5.2223600578308105 and perplexity is 185.37115487534604
At time: 37.07452440261841 and batch: 700, loss is 5.201442766189575 and perplexity is 181.5339640934214
At time: 38.24263024330139 and batch: 750, loss is 5.178853073120117 and perplexity is 177.47913862232812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.943444185478743 and perplexity of 140.25247418272377
Finished 2 epochs...
Completing Train Step...
At time: 41.62162923812866 and batch: 50, loss is 5.184794225692749 and perplexity is 178.5367077423161
At time: 42.817867040634155 and batch: 100, loss is 5.131655187606811 and perplexity is 169.2971047353421
At time: 43.98780345916748 and batch: 150, loss is 5.101232051849365 and perplexity is 164.22411533393964
At time: 45.15215611457825 and batch: 200, loss is 5.074999504089355 and perplexity is 159.9721126474933
At time: 46.32812452316284 and batch: 250, loss is 5.075584487915039 and perplexity is 160.06572112300518
At time: 47.49617910385132 and batch: 300, loss is 5.094669933319092 and perplexity is 163.14998536214387
At time: 48.66441774368286 and batch: 350, loss is 5.013836593627929 and perplexity is 150.4809643745082
At time: 49.839996337890625 and batch: 400, loss is 5.031802453994751 and perplexity is 153.20891599207954
At time: 51.01356029510498 and batch: 450, loss is 4.972934198379517 and perplexity is 144.4501114384905
At time: 52.18300437927246 and batch: 500, loss is 4.957242765426636 and perplexity is 142.2011728786128
At time: 53.35335731506348 and batch: 550, loss is 4.957271461486816 and perplexity is 142.2052535505768
At time: 54.52728605270386 and batch: 600, loss is 4.90812087059021 and perplexity is 135.38476972581157
At time: 55.692986726760864 and batch: 650, loss is 4.883717699050903 and perplexity is 132.1209378608896
At time: 56.85909414291382 and batch: 700, loss is 4.881297874450683 and perplexity is 131.80161487387707
At time: 58.025797843933105 and batch: 750, loss is 4.876354827880859 and perplexity is 131.15172090493056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.807715482490007 and perplexity of 122.45155501366823
Finished 3 epochs...
Completing Train Step...
At time: 61.44246506690979 and batch: 50, loss is 4.923190317153931 and perplexity is 137.4403929302496
At time: 62.63884663581848 and batch: 100, loss is 4.880647859573364 and perplexity is 131.71596970168795
At time: 63.808841705322266 and batch: 150, loss is 4.8590602111816406 and perplexity is 128.90300356437794
At time: 64.98543357849121 and batch: 200, loss is 4.842504787445068 and perplexity is 126.78652763780094
At time: 66.15648770332336 and batch: 250, loss is 4.839709997177124 and perplexity is 126.43268057873148
At time: 67.33235120773315 and batch: 300, loss is 4.8744504928588865 and perplexity is 130.90220174905951
At time: 68.51482844352722 and batch: 350, loss is 4.804875621795654 and perplexity is 122.10430296272452
At time: 69.68737006187439 and batch: 400, loss is 4.824969539642334 and perplexity is 124.58267349260515
At time: 70.85965728759766 and batch: 450, loss is 4.7743074989318846 and perplexity is 118.42827451097614
At time: 72.02913928031921 and batch: 500, loss is 4.758689508438111 and perplexity is 116.59303155347074
At time: 73.20200228691101 and batch: 550, loss is 4.758976354598999 and perplexity is 116.62648061409611
At time: 74.3722825050354 and batch: 600, loss is 4.720763816833496 and perplexity is 112.25396139609842
At time: 75.53981137275696 and batch: 650, loss is 4.69738883972168 and perplexity is 109.66045725527528
At time: 76.70985698699951 and batch: 700, loss is 4.697109861373901 and perplexity is 109.62986862907414
At time: 77.88566637039185 and batch: 750, loss is 4.701258506774902 and perplexity is 110.08562881939957
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.74211830316588 and perplexity of 114.67686493829659
Finished 4 epochs...
Completing Train Step...
At time: 81.2989501953125 and batch: 50, loss is 4.755948057174683 and perplexity is 116.27383517038692
At time: 82.52851390838623 and batch: 100, loss is 4.720343809127808 and perplexity is 112.20682376709473
At time: 83.7101399898529 and batch: 150, loss is 4.699284496307373 and perplexity is 109.86853298101107
At time: 84.89713096618652 and batch: 200, loss is 4.6931888866424565 and perplexity is 109.20085431115815
At time: 86.07522130012512 and batch: 250, loss is 4.682818784713745 and perplexity is 108.074281753234
At time: 87.26277995109558 and batch: 300, loss is 4.72614333152771 and perplexity is 112.85946041613062
At time: 88.44644212722778 and batch: 350, loss is 4.664540405273438 and perplexity is 106.11680328968282
At time: 89.61751222610474 and batch: 400, loss is 4.677873725891113 and perplexity is 107.5411673002956
At time: 90.85318875312805 and batch: 450, loss is 4.629912919998169 and perplexity is 102.50513757427008
At time: 92.02751445770264 and batch: 500, loss is 4.61920425415039 and perplexity is 101.41330079978867
At time: 93.2045636177063 and batch: 550, loss is 4.619598188400269 and perplexity is 101.45325884227093
At time: 94.38134455680847 and batch: 600, loss is 4.586194286346435 and perplexity is 98.1203009301986
At time: 95.55658984184265 and batch: 650, loss is 4.558901405334472 and perplexity is 95.478529988236
At time: 96.73333764076233 and batch: 700, loss is 4.5605522155761715 and perplexity is 95.63627709284688
At time: 97.91234302520752 and batch: 750, loss is 4.565329008102417 and perplexity is 96.09420458808117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.705892518509266 and perplexity of 110.59695073600129
Finished 5 epochs...
Completing Train Step...
At time: 101.33862352371216 and batch: 50, loss is 4.631246137619018 and perplexity is 102.64189037027418
At time: 102.5390374660492 and batch: 100, loss is 4.599633398056031 and perplexity is 99.44785118280421
At time: 103.70172071456909 and batch: 150, loss is 4.576428508758545 and perplexity is 97.16674358733886
At time: 104.86717081069946 and batch: 200, loss is 4.574682950973511 and perplexity is 96.99728136770086
At time: 106.02768421173096 and batch: 250, loss is 4.559070482254028 and perplexity is 95.49467456876964
At time: 107.19502854347229 and batch: 300, loss is 4.61064380645752 and perplexity is 100.54886280994795
At time: 108.35362911224365 and batch: 350, loss is 4.547039375305176 and perplexity is 94.35265160159916
At time: 109.51598405838013 and batch: 400, loss is 4.564898099899292 and perplexity is 96.05280572724637
At time: 110.68371415138245 and batch: 450, loss is 4.520923233032226 and perplexity is 91.92042278625175
At time: 111.85628390312195 and batch: 500, loss is 4.509436693191528 and perplexity is 90.87061605382411
At time: 113.02651500701904 and batch: 550, loss is 4.513490047454834 and perplexity is 91.23969435003806
At time: 114.19277477264404 and batch: 600, loss is 4.475189752578736 and perplexity is 87.81126124224959
At time: 115.3590407371521 and batch: 650, loss is 4.449938745498657 and perplexity is 85.62169912708194
At time: 116.52307987213135 and batch: 700, loss is 4.453547248840332 and perplexity is 85.93122343839049
At time: 117.69019794464111 and batch: 750, loss is 4.463181400299073 and perplexity is 86.76309863150547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.691678424214208 and perplexity of 109.03603503158938
Finished 6 epochs...
Completing Train Step...
At time: 121.09637641906738 and batch: 50, loss is 4.530093116760254 and perplexity is 92.76719886096026
At time: 122.32542157173157 and batch: 100, loss is 4.5043430519104 and perplexity is 90.40893056100242
At time: 123.49848461151123 and batch: 150, loss is 4.47567421913147 and perplexity is 87.85381316793499
At time: 124.66809916496277 and batch: 200, loss is 4.4823517227172855 and perplexity is 88.44242034654619
At time: 125.84255290031433 and batch: 250, loss is 4.461388387680054 and perplexity is 86.60767068456947
At time: 127.01530599594116 and batch: 300, loss is 4.516505937576294 and perplexity is 91.5152785999384
At time: 128.18419218063354 and batch: 350, loss is 4.453017044067383 and perplexity is 85.88567436981192
At time: 129.3549530506134 and batch: 400, loss is 4.473343572616577 and perplexity is 87.64929540633612
At time: 130.5313527584076 and batch: 450, loss is 4.426567831039429 and perplexity is 83.64384388822833
At time: 131.7019829750061 and batch: 500, loss is 4.420283584594727 and perplexity is 83.11985352637716
At time: 132.87295794487 and batch: 550, loss is 4.424956693649292 and perplexity is 83.50919066543351
At time: 134.04858326911926 and batch: 600, loss is 4.389495010375977 and perplexity is 80.59970668612327
At time: 135.22312712669373 and batch: 650, loss is 4.364248294830322 and perplexity is 78.59030099652281
At time: 136.40448474884033 and batch: 700, loss is 4.3680029964447025 and perplexity is 78.88593879508915
At time: 137.5800609588623 and batch: 750, loss is 4.374765634536743 and perplexity is 79.421223778996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.683104404183322 and perplexity of 108.10515428095088
Finished 7 epochs...
Completing Train Step...
At time: 140.99762535095215 and batch: 50, loss is 4.446691961288452 and perplexity is 85.34415475346376
At time: 142.20015454292297 and batch: 100, loss is 4.418981599807739 and perplexity is 83.0117031619315
At time: 143.37229347229004 and batch: 150, loss is 4.394010190963745 and perplexity is 80.96445174236412
At time: 144.54451751708984 and batch: 200, loss is 4.402921686172485 and perplexity is 81.68919052299083
At time: 145.71491074562073 and batch: 250, loss is 4.3803958988189695 and perplexity is 79.8696474457862
At time: 146.8894658088684 and batch: 300, loss is 4.432660961151123 and perplexity is 84.15505256095672
At time: 148.0590798854828 and batch: 350, loss is 4.374473323822022 and perplexity is 79.39801149707374
At time: 149.28981971740723 and batch: 400, loss is 4.393187627792359 and perplexity is 80.8978807493465
At time: 150.45671772956848 and batch: 450, loss is 4.345336046218872 and perplexity is 77.11794832037843
At time: 151.62755298614502 and batch: 500, loss is 4.343143062591553 and perplexity is 76.94901522379928
At time: 152.79853415489197 and batch: 550, loss is 4.349523677825927 and perplexity is 77.441567003227
At time: 153.97182369232178 and batch: 600, loss is 4.313805112838745 and perplexity is 74.72428299231181
At time: 155.1456286907196 and batch: 650, loss is 4.2906585311889645 and perplexity is 73.01453501980268
At time: 156.31865453720093 and batch: 700, loss is 4.295112133026123 and perplexity is 73.34043786919217
At time: 157.4877004623413 and batch: 750, loss is 4.298252611160279 and perplexity is 73.5711239533743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.679465005564135 and perplexity of 107.71243160239574
Finished 8 epochs...
Completing Train Step...
At time: 160.86314511299133 and batch: 50, loss is 4.3718455791473385 and perplexity is 79.18964767853588
At time: 162.1193232536316 and batch: 100, loss is 4.346381187438965 and perplexity is 77.19858960039944
At time: 163.32845973968506 and batch: 150, loss is 4.322105503082275 and perplexity is 75.34710495865319
At time: 164.53922843933105 and batch: 200, loss is 4.330302009582519 and perplexity is 75.96722593524863
At time: 165.7428731918335 and batch: 250, loss is 4.309197111129761 and perplexity is 74.38074548716625
At time: 166.9237983226776 and batch: 300, loss is 4.3592728424072265 and perplexity is 78.20024983813525
At time: 168.12976360321045 and batch: 350, loss is 4.305044193267822 and perplexity is 74.07248888585576
At time: 169.31816244125366 and batch: 400, loss is 4.323097181320191 and perplexity is 75.42186210432136
At time: 170.49849200248718 and batch: 450, loss is 4.275896587371826 and perplexity is 71.9446150345904
At time: 171.67820811271667 and batch: 500, loss is 4.274181671142578 and perplexity is 71.8213417785259
At time: 172.85204672813416 and batch: 550, loss is 4.28454888343811 and perplexity is 72.56980189491546
At time: 174.02616262435913 and batch: 600, loss is 4.247084865570068 and perplexity is 69.90134323439419
At time: 175.2029139995575 and batch: 650, loss is 4.223224058151245 and perplexity is 68.25318216806376
At time: 176.38397693634033 and batch: 700, loss is 4.227712459564209 and perplexity is 68.5602183827802
At time: 177.5632598400116 and batch: 750, loss is 4.230590324401856 and perplexity is 68.75780960857324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.68030707780705 and perplexity of 107.80317145065683
Annealing...
Finished 9 epochs...
Completing Train Step...
At time: 181.00117349624634 and batch: 50, loss is 4.31278281211853 and perplexity is 74.64793133781652
At time: 182.2262122631073 and batch: 100, loss is 4.293327884674072 and perplexity is 73.20969698557339
At time: 183.39873123168945 and batch: 150, loss is 4.264997148513794 and perplexity is 71.16471704789666
At time: 184.57667541503906 and batch: 200, loss is 4.25904411315918 and perplexity is 70.74232946282542
At time: 185.7538435459137 and batch: 250, loss is 4.22506959438324 and perplexity is 68.37926219554755
At time: 186.9222128391266 and batch: 300, loss is 4.2622285747528075 and perplexity is 70.96796476684668
At time: 188.10826539993286 and batch: 350, loss is 4.196014614105224 and perplexity is 66.42108917449316
At time: 189.29033184051514 and batch: 400, loss is 4.205418162345886 and perplexity is 67.04862901706676
At time: 190.47839450836182 and batch: 450, loss is 4.147342829704285 and perplexity is 63.265669098167564
At time: 191.66387963294983 and batch: 500, loss is 4.120088577270508 and perplexity is 61.5646952555848
At time: 192.8479814529419 and batch: 550, loss is 4.1156044197082515 and perplexity is 61.28924749890789
At time: 194.02952671051025 and batch: 600, loss is 4.070227952003479 and perplexity is 58.57031229032027
At time: 195.20479822158813 and batch: 650, loss is 4.02601080417633 and perplexity is 56.03692251943387
At time: 196.3776113986969 and batch: 700, loss is 4.014683661460876 and perplexity is 55.40576565100698
At time: 197.55295586585999 and batch: 750, loss is 4.008099002838135 and perplexity is 55.042136100536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.623393568881722 and perplexity of 101.83904419866633
Finished 10 epochs...
Completing Train Step...
At time: 200.9754936695099 and batch: 50, loss is 4.230010433197021 and perplexity is 68.7179491180085
At time: 202.1886341571808 and batch: 100, loss is 4.207851600646973 and perplexity is 67.21198639839479
At time: 203.36345434188843 and batch: 150, loss is 4.183898715972901 and perplexity is 65.62119353628344
At time: 204.53405904769897 and batch: 200, loss is 4.185455284118652 and perplexity is 65.72341693403078
At time: 205.70824885368347 and batch: 250, loss is 4.1542203235626225 and perplexity is 63.702278015444755
At time: 206.89057183265686 and batch: 300, loss is 4.198238620758056 and perplexity is 66.56897450673102
At time: 208.06060647964478 and batch: 350, loss is 4.137896747589111 and perplexity is 62.670870075742634
At time: 209.23111033439636 and batch: 400, loss is 4.152649703025818 and perplexity is 63.602304440179346
At time: 210.46493792533875 and batch: 450, loss is 4.103594355583191 and perplexity is 60.557562294317556
At time: 211.64261269569397 and batch: 500, loss is 4.081547288894654 and perplexity is 59.23705581525143
At time: 212.81451511383057 and batch: 550, loss is 4.083169984817505 and perplexity is 59.33325757619777
At time: 213.99379777908325 and batch: 600, loss is 4.043796834945678 and perplexity is 57.042513171576246
At time: 215.16860008239746 and batch: 650, loss is 4.006970062255859 and perplexity is 54.98003186195062
At time: 216.33950757980347 and batch: 700, loss is 4.002892632484436 and perplexity is 54.75631105609457
At time: 217.5134084224701 and batch: 750, loss is 4.0036185932159425 and perplexity is 54.79607642002339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.6212108523346656 and perplexity of 101.61700084876033
Finished 11 epochs...
Completing Train Step...
At time: 220.93577694892883 and batch: 50, loss is 4.192604217529297 and perplexity is 66.19495274603277
At time: 222.1444878578186 and batch: 100, loss is 4.1708987426757815 and perplexity is 64.77364079040402
At time: 223.32213521003723 and batch: 150, loss is 4.146971335411072 and perplexity is 63.2421706281853
At time: 224.49981451034546 and batch: 200, loss is 4.150745992660522 and perplexity is 63.481339251833944
At time: 225.6749348640442 and batch: 250, loss is 4.119872136116028 and perplexity is 61.551371563818094
At time: 226.84841966629028 and batch: 300, loss is 4.166063199043274 and perplexity is 64.46118108922491
At time: 228.03095364570618 and batch: 350, loss is 4.1069668197631835 and perplexity is 60.76213526760235
At time: 229.21312832832336 and batch: 400, loss is 4.1248410987854 and perplexity is 61.8579791613606
At time: 230.38360500335693 and batch: 450, loss is 4.0783728408813475 and perplexity is 59.04930901499537
At time: 231.55545616149902 and batch: 500, loss is 4.057631177902222 and perplexity is 57.83714277756494
At time: 232.734792470932 and batch: 550, loss is 4.061508150100708 and perplexity is 58.06181100697911
At time: 233.91547393798828 and batch: 600, loss is 4.0245930862426755 and perplexity is 55.95753425780939
At time: 235.09599924087524 and batch: 650, loss is 3.9900620365142823 and perplexity is 54.05824284426066
At time: 236.27027559280396 and batch: 700, loss is 3.9887004852294923 and perplexity is 53.9846898587064
At time: 237.4561150074005 and batch: 750, loss is 3.992188491821289 and perplexity is 54.17331758893286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.622414877248365 and perplexity of 101.73942393487097
Annealing...
Finished 12 epochs...
Completing Train Step...
At time: 240.84719276428223 and batch: 50, loss is 4.177441663742066 and perplexity is 65.19883911191164
At time: 242.07042479515076 and batch: 100, loss is 4.169325246810913 and perplexity is 64.67179987860685
At time: 243.24463939666748 and batch: 150, loss is 4.145023078918457 and perplexity is 63.119078605003594
At time: 244.42057061195374 and batch: 200, loss is 4.1466956615448 and perplexity is 63.224738817354506
At time: 245.6116065979004 and batch: 250, loss is 4.109254837036133 and perplexity is 60.901319249594096
At time: 246.81677842140198 and batch: 300, loss is 4.151158995628357 and perplexity is 63.507562648144585
At time: 248.01936173439026 and batch: 350, loss is 4.088976502418518 and perplexity is 59.67877934897672
At time: 249.19410157203674 and batch: 400, loss is 4.100225248336792 and perplexity is 60.35388067763672
At time: 250.3608193397522 and batch: 450, loss is 4.050740637779236 and perplexity is 57.43998351697787
At time: 251.52556157112122 and batch: 500, loss is 4.016728863716126 and perplexity is 55.519197603974185
At time: 252.69886350631714 and batch: 550, loss is 4.009732599258423 and perplexity is 55.13212622079143
At time: 253.86756348609924 and batch: 600, loss is 3.9705577659606934 and perplexity is 53.01409205077412
At time: 255.037588596344 and batch: 650, loss is 3.931136384010315 and perplexity is 50.964860426929356
At time: 256.20481634140015 and batch: 700, loss is 3.9214344120025633 and perplexity is 50.472791656200144
At time: 257.3720073699951 and batch: 750, loss is 3.921307702064514 and perplexity is 50.46639665705975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.594069370003634 and perplexity of 98.8960570715844
Finished 13 epochs...
Completing Train Step...
At time: 260.81875443458557 and batch: 50, loss is 4.155056648254394 and perplexity is 63.7555760876115
At time: 262.0486538410187 and batch: 100, loss is 4.140789284706115 and perplexity is 62.852410322967884
At time: 263.222279548645 and batch: 150, loss is 4.1157338380813595 and perplexity is 61.29717996690069
At time: 264.39895033836365 and batch: 200, loss is 4.118963103294373 and perplexity is 61.49544477034537
At time: 265.5797789096832 and batch: 250, loss is 4.082990212440491 and perplexity is 59.32259205415915
At time: 266.7548363208771 and batch: 300, loss is 4.128149251937867 and perplexity is 62.062953686755
At time: 267.93550086021423 and batch: 350, loss is 4.068072018623352 and perplexity is 58.444174620033856
At time: 269.1870038509369 and batch: 400, loss is 4.082787337303162 and perplexity is 59.31055819587608
At time: 270.35977816581726 and batch: 450, loss is 4.036418552398682 and perplexity is 56.62318625076789
At time: 271.5333902835846 and batch: 500, loss is 4.005811977386474 and perplexity is 54.91639717323013
At time: 272.7033727169037 and batch: 550, loss is 4.002536973953247 and perplexity is 54.7368399696653
At time: 273.87910532951355 and batch: 600, loss is 3.9665343618392943 and perplexity is 52.801223449684045
At time: 275.0553367137909 and batch: 650, loss is 3.9311620712280275 and perplexity is 50.96616958920912
At time: 276.23478055000305 and batch: 700, loss is 3.9246112394332884 and perplexity is 50.63338996673168
At time: 277.41434049606323 and batch: 750, loss is 3.926620798110962 and perplexity is 50.73524304050716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.592784083166788 and perplexity of 98.76902892250541
Finished 14 epochs...
Completing Train Step...
At time: 280.8276455402374 and batch: 50, loss is 4.1425864696502686 and perplexity is 62.96546929201203
At time: 282.0603458881378 and batch: 100, loss is 4.127241058349609 and perplexity is 62.006614097647656
At time: 283.23457527160645 and batch: 150, loss is 4.101672019958496 and perplexity is 60.44126215473963
At time: 284.4057614803314 and batch: 200, loss is 4.105468702316284 and perplexity is 60.67117460453761
At time: 285.5770568847656 and batch: 250, loss is 4.070016822814941 and perplexity is 58.557947693123644
At time: 286.7493727207184 and batch: 300, loss is 4.1164862298965454 and perplexity is 61.34331681771975
At time: 287.92471957206726 and batch: 350, loss is 4.0574396181106565 and perplexity is 57.82606456765342
At time: 289.0941834449768 and batch: 400, loss is 4.0733115673065186 and perplexity is 58.751199351324104
At time: 290.2618610858917 and batch: 450, loss is 4.028411040306091 and perplexity is 56.17158591278552
At time: 291.4319269657135 and batch: 500, loss is 3.999223189353943 and perplexity is 54.5557540778901
At time: 292.606125831604 and batch: 550, loss is 3.997626142501831 and perplexity is 54.468695519377825
At time: 293.77887439727783 and batch: 600, loss is 3.9628877019882203 and perplexity is 52.60902600039221
At time: 294.9473249912262 and batch: 650, loss is 3.9292228174209596 and perplexity is 50.86742902325531
At time: 296.11965918540955 and batch: 700, loss is 3.9239092302322387 and perplexity is 50.597857334671275
At time: 297.29411792755127 and batch: 750, loss is 3.926925621032715 and perplexity is 50.750710662849755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.592740081077398 and perplexity of 98.76468297448191
Finished 15 epochs...
Completing Train Step...
At time: 300.676664352417 and batch: 50, loss is 4.132111101150513 and perplexity is 62.30932547262699
At time: 301.88289880752563 and batch: 100, loss is 4.116740627288818 and perplexity is 61.35892438272929
At time: 303.05516934394836 and batch: 150, loss is 4.090959105491638 and perplexity is 59.79721604800072
At time: 304.2336881160736 and batch: 200, loss is 4.095324563980102 and perplexity is 60.05882892699775
At time: 305.40577697753906 and batch: 250, loss is 4.060135278701782 and perplexity is 57.98215429899713
At time: 306.57590436935425 and batch: 300, loss is 4.107535424232483 and perplexity is 60.79669471367634
At time: 307.75194573402405 and batch: 350, loss is 4.049239821434021 and perplexity is 57.353841308832976
At time: 308.9301829338074 and batch: 400, loss is 4.065839977264404 and perplexity is 58.313870281526896
At time: 310.10182642936707 and batch: 450, loss is 4.021867246627807 and perplexity is 55.80521069329855
At time: 311.28120613098145 and batch: 500, loss is 3.993541932106018 and perplexity is 54.24668757906538
At time: 312.4625999927521 and batch: 550, loss is 3.992953910827637 and perplexity is 54.21479874906133
At time: 313.63530230522156 and batch: 600, loss is 3.958984251022339 and perplexity is 52.40406952605298
At time: 314.80842304229736 and batch: 650, loss is 3.9262928104400636 and perplexity is 50.718605234956534
At time: 315.9840204715729 and batch: 700, loss is 3.921713948249817 and perplexity is 50.48690260313682
At time: 317.1603090763092 and batch: 750, loss is 3.9253980541229248 and perplexity is 50.673244738847565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.593059451081032 and perplexity of 98.79623048903916
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 320.6154565811157 and batch: 50, loss is 4.1286756038665775 and perplexity is 62.095629240795574
At time: 321.83615946769714 and batch: 100, loss is 4.121595253944397 and perplexity is 61.65752325919482
At time: 323.0323791503906 and batch: 150, loss is 4.099072413444519 and perplexity is 60.28434270870741
At time: 324.20317220687866 and batch: 200, loss is 4.101738352775573 and perplexity is 60.44527152690107
At time: 325.3766276836395 and batch: 250, loss is 4.064621930122375 and perplexity is 58.24288447929305
At time: 326.5461645126343 and batch: 300, loss is 4.111988968849182 and perplexity is 61.068059324811394
At time: 327.71590209007263 and batch: 350, loss is 4.052625522613526 and perplexity is 57.54835337105348
At time: 328.89229345321655 and batch: 400, loss is 4.062011032104492 and perplexity is 58.091016589707166
At time: 330.1170506477356 and batch: 450, loss is 4.0157833003997805 and perplexity is 55.46672549912524
At time: 331.2871763706207 and batch: 500, loss is 3.9863249540328978 and perplexity is 53.85659974507497
At time: 332.45748949050903 and batch: 550, loss is 3.9806833934783934 and perplexity is 53.553619919452956
At time: 333.62649846076965 and batch: 600, loss is 3.9421125555038454 and perplexity is 51.52734076834396
At time: 334.80233883857727 and batch: 650, loss is 3.904187641143799 and perplexity is 49.609762608326015
At time: 335.97538805007935 and batch: 700, loss is 3.8991079425811765 and perplexity is 49.35839893485945
At time: 337.15376687049866 and batch: 750, loss is 3.902312369346619 and perplexity is 49.516817995085546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.579779691474382 and perplexity of 97.49291332093777
Finished 17 epochs...
Completing Train Step...
At time: 340.5883512496948 and batch: 50, loss is 4.123532905578613 and perplexity is 61.777109881090766
At time: 341.82564306259155 and batch: 100, loss is 4.111173362731933 and perplexity is 61.01827214818039
At time: 343.0048370361328 and batch: 150, loss is 4.087662296295166 and perplexity is 59.60040064588879
At time: 344.181542634964 and batch: 200, loss is 4.09056028842926 and perplexity is 59.77337265285049
At time: 345.3633482456207 and batch: 250, loss is 4.054459018707275 and perplexity is 57.65396484168422
At time: 346.53787755966187 and batch: 300, loss is 4.102501864433289 and perplexity is 60.49143981913748
At time: 347.71304845809937 and batch: 350, loss is 4.044285144805908 and perplexity is 57.07037439511168
At time: 348.8854100704193 and batch: 400, loss is 4.055286931991577 and perplexity is 57.701717089701944
At time: 350.06014108657837 and batch: 450, loss is 4.010872588157654 and perplexity is 55.195012070449
At time: 351.23894929885864 and batch: 500, loss is 3.982974638938904 and perplexity is 53.676465088462464
At time: 352.41211104393005 and batch: 550, loss is 3.9791707944869996 and perplexity is 53.472676001254825
At time: 353.5917992591858 and batch: 600, loss is 3.9422680711746216 and perplexity is 51.53535470043673
At time: 354.76431226730347 and batch: 650, loss is 3.9059453535079958 and perplexity is 49.697038982358
At time: 355.9339368343353 and batch: 700, loss is 3.9023630905151365 and perplexity is 49.51932960965101
At time: 357.10348200798035 and batch: 750, loss is 3.9062485122680664 and perplexity is 49.71210735901486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.578859551008358 and perplexity of 97.4032474051857
Finished 18 epochs...
Completing Train Step...
At time: 360.53485345840454 and batch: 50, loss is 4.119929738044739 and perplexity is 61.55491714365014
At time: 361.7430808544159 and batch: 100, loss is 4.1063999700546265 and perplexity is 60.72770202909223
At time: 362.92184233665466 and batch: 150, loss is 4.082624320983887 and perplexity is 59.30089039501088
At time: 364.0958020687103 and batch: 200, loss is 4.085421919822693 and perplexity is 59.46702277464259
At time: 365.27859592437744 and batch: 250, loss is 4.049631500244141 and perplexity is 57.37630999311913
At time: 366.44940972328186 and batch: 300, loss is 4.098132796287537 and perplexity is 60.22772510960022
At time: 367.6271960735321 and batch: 350, loss is 4.040538425445557 and perplexity is 56.85694779292605
At time: 368.8085880279541 and batch: 400, loss is 4.052016897201538 and perplexity is 57.513338637283056
At time: 369.9884283542633 and batch: 450, loss is 4.008399333953857 and perplexity is 55.058669449298094
At time: 371.16589307785034 and batch: 500, loss is 3.9813374853134156 and perplexity is 53.58866036356305
At time: 372.34331345558167 and batch: 550, loss is 3.9783675193786623 and perplexity is 53.42973997867463
At time: 373.5261421203613 and batch: 600, loss is 3.942176094055176 and perplexity is 51.53061484494424
At time: 374.7027404308319 and batch: 650, loss is 3.9064155340194704 and perplexity is 49.72041105568162
At time: 375.87884402275085 and batch: 700, loss is 3.9034230136871337 and perplexity is 49.571844120323014
At time: 377.0583620071411 and batch: 750, loss is 3.9076184940338137 and perplexity is 49.7802587120299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.578673961550691 and perplexity of 97.3851720666728
Finished 19 epochs...
Completing Train Step...
At time: 380.4455602169037 and batch: 50, loss is 4.116614661216736 and perplexity is 61.35119572682203
At time: 381.6665234565735 and batch: 100, loss is 4.102678446769715 and perplexity is 60.50212248207155
At time: 382.83571672439575 and batch: 150, loss is 4.078836898803711 and perplexity is 59.07671767376464
At time: 384.0024960041046 and batch: 200, loss is 4.08164806842804 and perplexity is 59.24302599892676
At time: 385.1679136753082 and batch: 250, loss is 4.046072387695313 and perplexity is 57.17246421842549
At time: 386.33564805984497 and batch: 300, loss is 4.094953374862671 and perplexity is 60.03653988027608
At time: 387.4996829032898 and batch: 350, loss is 4.037809748649597 and perplexity is 56.70201503563585
At time: 388.69471168518066 and batch: 400, loss is 4.049510822296143 and perplexity is 57.36938635553905
At time: 389.85705065727234 and batch: 450, loss is 4.006403493881225 and perplexity is 54.94889073728251
At time: 391.03021144866943 and batch: 500, loss is 3.9799195241928103 and perplexity is 53.512727574256324
At time: 392.19912552833557 and batch: 550, loss is 3.9775184106826784 and perplexity is 53.38439157742014
At time: 393.3614892959595 and batch: 600, loss is 3.9417733764648437 and perplexity is 51.50986673799977
At time: 394.52274227142334 and batch: 650, loss is 3.9063154363632204 and perplexity is 49.7154344081467
At time: 395.6816849708557 and batch: 700, loss is 3.903666787147522 and perplexity is 49.58392989333756
At time: 396.8661186695099 and batch: 750, loss is 3.908110852241516 and perplexity is 49.80477446575936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5786792843840844 and perplexity of 97.3856904330983
Annealing...
Finished 20 epochs...
Completing Train Step...
At time: 400.3796212673187 and batch: 50, loss is 4.115799117088318 and perplexity is 61.30118151654544
At time: 401.6142020225525 and batch: 100, loss is 4.104345235824585 and perplexity is 60.603050847401946
At time: 402.80503010749817 and batch: 150, loss is 4.083105230331421 and perplexity is 59.32941560598951
At time: 403.9843089580536 and batch: 200, loss is 4.085370817184448 and perplexity is 59.46398393053739
At time: 405.16048645973206 and batch: 250, loss is 4.048781056404113 and perplexity is 57.327535406684966
At time: 406.3350076675415 and batch: 300, loss is 4.098101849555969 and perplexity is 60.225861287198086
At time: 407.50797724723816 and batch: 350, loss is 4.041447267532349 and perplexity is 56.908645268869286
At time: 408.6830997467041 and batch: 400, loss is 4.049982671737671 and perplexity is 57.39646245587076
At time: 409.86029171943665 and batch: 450, loss is 4.00458836555481 and perplexity is 54.84924191426016
At time: 411.032621383667 and batch: 500, loss is 3.9770103883743286 and perplexity is 53.35727800331452
At time: 412.21561336517334 and batch: 550, loss is 3.972461543083191 and perplexity is 53.115115198657165
At time: 413.39463782310486 and batch: 600, loss is 3.935773687362671 and perplexity is 51.20174878210603
At time: 414.568550825119 and batch: 650, loss is 3.897426881790161 and perplexity is 49.27549416920206
At time: 415.74118280410767 and batch: 700, loss is 3.892875142097473 and perplexity is 49.05171462599138
At time: 416.92294239997864 and batch: 750, loss is 3.897883930206299 and perplexity is 49.29802060320986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.575472099836483 and perplexity of 97.07385687276593
Finished 21 epochs...
Completing Train Step...
At time: 420.3302495479584 and batch: 50, loss is 4.114415612220764 and perplexity is 61.216429674342145
At time: 421.5318353176117 and batch: 100, loss is 4.101044702529907 and perplexity is 60.40335818773663
At time: 422.7045855522156 and batch: 150, loss is 4.078857402801514 and perplexity is 59.07792899507244
At time: 423.87913036346436 and batch: 200, loss is 4.081468667984009 and perplexity is 59.23239872705397
At time: 425.0529205799103 and batch: 250, loss is 4.04517858505249 and perplexity is 57.121386149067504
At time: 426.2317621707916 and batch: 300, loss is 4.09465087890625 and perplexity is 60.018381816233706
At time: 427.4009482860565 and batch: 350, loss is 4.038339676856995 and perplexity is 56.732070995866756
At time: 428.57771253585815 and batch: 400, loss is 4.047622075080872 and perplexity is 57.26113235122461
At time: 429.7548382282257 and batch: 450, loss is 4.002807912826538 and perplexity is 54.75167231665313
At time: 430.9292731285095 and batch: 500, loss is 3.9761123275756836 and perplexity is 53.30938143385215
At time: 432.10565066337585 and batch: 550, loss is 3.972510328292847 and perplexity is 53.11770649389596
At time: 433.27929615974426 and batch: 600, loss is 3.9364698219299314 and perplexity is 51.23740449848588
At time: 434.4524736404419 and batch: 650, loss is 3.898709969520569 and perplexity is 49.33875953000489
At time: 435.6313977241516 and batch: 700, loss is 3.8945259189605714 and perplexity is 49.13275493291977
At time: 436.8122892379761 and batch: 750, loss is 3.899526982307434 and perplexity is 49.37908639896955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574903266374455 and perplexity of 97.01865371687826
Finished 22 epochs...
Completing Train Step...
At time: 440.24424028396606 and batch: 50, loss is 4.113499736785888 and perplexity is 61.16038871737793
At time: 441.4774043560028 and batch: 100, loss is 4.099356346130371 and perplexity is 60.301461834272146
At time: 442.6545832157135 and batch: 150, loss is 4.076902561187744 and perplexity is 58.962553807867366
At time: 443.840185880661 and batch: 200, loss is 4.079487462043762 and perplexity is 59.11516331893182
At time: 445.0178818702698 and batch: 250, loss is 4.04334367275238 and perplexity is 57.01666951732196
At time: 446.2074704170227 and batch: 300, loss is 4.092955365180969 and perplexity is 59.9167060467843
At time: 447.3870143890381 and batch: 350, loss is 4.036923680305481 and perplexity is 56.65179542735348
At time: 448.5715157985687 and batch: 400, loss is 4.046454243659973 and perplexity is 57.19430003370443
At time: 449.8058285713196 and batch: 450, loss is 4.0019575881958005 and perplexity is 54.705135409651405
At time: 450.98689365386963 and batch: 500, loss is 3.975661654472351 and perplexity is 53.28536174240561
At time: 452.16747069358826 and batch: 550, loss is 3.9725497007369994 and perplexity is 53.11979790900019
At time: 453.34090995788574 and batch: 600, loss is 3.9368513441085815 and perplexity is 51.2569564341899
At time: 454.52062940597534 and batch: 650, loss is 3.8993715143203733 and perplexity is 49.371410128526826
At time: 455.693496465683 and batch: 700, loss is 3.895411605834961 and perplexity is 49.176290445636624
At time: 456.86772656440735 and batch: 750, loss is 3.900389246940613 and perplexity is 49.42168260075003
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5746953210165335 and perplexity of 96.99848123566566
Finished 23 epochs...
Completing Train Step...
At time: 460.3211236000061 and batch: 50, loss is 4.112559118270874 and perplexity is 61.10288717111403
At time: 461.54879999160767 and batch: 100, loss is 4.098036451339722 and perplexity is 60.2219227520859
At time: 462.7226483821869 and batch: 150, loss is 4.07548002243042 and perplexity is 58.87873692036481
At time: 463.89519929885864 and batch: 200, loss is 4.078041625022888 and perplexity is 59.02975418603205
At time: 465.07272601127625 and batch: 250, loss is 4.0419990968704225 and perplexity is 56.940057795298365
At time: 466.2565565109253 and batch: 300, loss is 4.091746277809143 and perplexity is 59.84430529238085
At time: 467.42580699920654 and batch: 350, loss is 4.035957164764405 and perplexity is 56.597067038822466
At time: 468.5980443954468 and batch: 400, loss is 4.045605387687683 and perplexity is 57.145770910571876
At time: 469.77230310440063 and batch: 450, loss is 4.00133481502533 and perplexity is 54.671077125424915
At time: 470.9469356536865 and batch: 500, loss is 3.975295305252075 and perplexity is 53.265844267003395
At time: 472.12342500686646 and batch: 550, loss is 3.9724980926513673 and perplexity is 53.117056568659216
At time: 473.2956910133362 and batch: 600, loss is 3.9370331954956055 and perplexity is 51.26627843039537
At time: 474.46442675590515 and batch: 650, loss is 3.8997255086898805 and perplexity is 49.38889042350741
At time: 475.65339946746826 and batch: 700, loss is 3.8959254789352418 and perplexity is 49.20156731246296
At time: 476.83540296554565 and batch: 750, loss is 3.900894117355347 and perplexity is 49.446640445850186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574608736259993 and perplexity of 96.99008300936751
Finished 24 epochs...
Completing Train Step...
At time: 480.3602213859558 and batch: 50, loss is 4.111631078720093 and perplexity is 61.04620757967157
At time: 481.556245803833 and batch: 100, loss is 4.096887392997742 and perplexity is 60.15276399071278
At time: 482.7292878627777 and batch: 150, loss is 4.074285135269165 and perplexity is 58.8084254890267
At time: 483.8893482685089 and batch: 200, loss is 4.076840410232544 and perplexity is 58.95888934270335
At time: 485.05091285705566 and batch: 250, loss is 4.04087613105774 and perplexity is 56.8761519457799
At time: 486.2273542881012 and batch: 300, loss is 4.090746545791626 and perplexity is 59.784506920468026
At time: 487.39558362960815 and batch: 350, loss is 4.0351706314086915 and perplexity is 56.552569059627345
At time: 488.58141255378723 and batch: 400, loss is 4.044887838363647 and perplexity is 57.10478070928784
At time: 489.76768231391907 and batch: 450, loss is 4.000794830322266 and perplexity is 54.64156354923414
At time: 490.9456853866577 and batch: 500, loss is 3.9749495649337767 and perplexity is 53.24743130028799
At time: 492.1117069721222 and batch: 550, loss is 3.972382402420044 and perplexity is 53.110911799549314
At time: 493.293016910553 and batch: 600, loss is 3.937097029685974 and perplexity is 51.26955107622443
At time: 494.4930725097656 and batch: 650, loss is 3.8999141788482667 and perplexity is 49.39820951237548
At time: 495.6849546432495 and batch: 700, loss is 3.896237139701843 and perplexity is 49.216903900431795
At time: 496.8577389717102 and batch: 750, loss is 3.9012103509902953 and perplexity is 49.462279609378804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574578218681868 and perplexity of 96.98712315209599
Finished 25 epochs...
Completing Train Step...
At time: 500.2412316799164 and batch: 50, loss is 4.1107288646698 and perplexity is 60.991155671512914
At time: 501.4428958892822 and batch: 100, loss is 4.095840830802917 and perplexity is 60.08984331295025
At time: 502.62102794647217 and batch: 150, loss is 4.073220000267029 and perplexity is 58.74581992422592
At time: 503.79937505722046 and batch: 200, loss is 4.075779933929443 and perplexity is 58.89639797886006
At time: 504.97390604019165 and batch: 250, loss is 4.039879612922668 and perplexity is 56.81950205992016
At time: 506.15530252456665 and batch: 300, loss is 4.0898622083663945 and perplexity is 59.73166061397658
At time: 507.3315227031708 and batch: 350, loss is 4.034477710723877 and perplexity is 56.513396188160364
At time: 508.57024097442627 and batch: 400, loss is 4.044239039421082 and perplexity is 57.06774320419451
At time: 509.74636030197144 and batch: 450, loss is 4.000292706489563 and perplexity is 54.61413360511057
At time: 510.9253852367401 and batch: 500, loss is 3.9746076536178587 and perplexity is 53.22922851302966
At time: 512.1051850318909 and batch: 550, loss is 3.9722238636016844 and perplexity is 53.10249232577495
At time: 513.2842395305634 and batch: 600, loss is 3.9370873546600342 and perplexity is 51.2690550443874
At time: 514.4603266716003 and batch: 650, loss is 3.900003457069397 and perplexity is 49.402619893520324
At time: 515.6423587799072 and batch: 700, loss is 3.8964266633987426 and perplexity is 49.22623255398148
At time: 516.8278841972351 and batch: 750, loss is 3.9014143466949465 and perplexity is 49.47237073119913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574574670126272 and perplexity of 96.98677898850802
Finished 26 epochs...
Completing Train Step...
At time: 520.2227094173431 and batch: 50, loss is 4.109853968620301 and perplexity is 60.93781808618511
At time: 521.4453344345093 and batch: 100, loss is 4.094864463806152 and perplexity is 60.03120220537826
At time: 522.6099236011505 and batch: 150, loss is 4.07224024772644 and perplexity is 58.68829174419829
At time: 523.7785234451294 and batch: 200, loss is 4.074811534881592 and perplexity is 58.83939037064736
At time: 524.9468891620636 and batch: 250, loss is 4.038965182304382 and perplexity is 56.76756831604242
At time: 526.1125299930573 and batch: 300, loss is 4.089051313400269 and perplexity is 59.68324414405527
At time: 527.2811784744263 and batch: 350, loss is 4.033841881752014 and perplexity is 56.47747475471981
At time: 528.4462230205536 and batch: 400, loss is 4.043631749153137 and perplexity is 57.03309704033691
At time: 529.6123447418213 and batch: 450, loss is 3.999811153411865 and perplexity is 54.58784033229856
At time: 530.7773532867432 and batch: 500, loss is 3.9742647123336794 and perplexity is 53.21097714280057
At time: 531.9429507255554 and batch: 550, loss is 3.9720360088348388 and perplexity is 53.092517706379404
At time: 533.1075954437256 and batch: 600, loss is 3.9370284271240235 and perplexity is 51.26603397431302
At time: 534.2717926502228 and batch: 650, loss is 3.90002769947052 and perplexity is 49.403817546165236
At time: 535.4431672096252 and batch: 700, loss is 3.8965360927581787 and perplexity is 49.231619643824835
At time: 536.6064231395721 and batch: 750, loss is 3.9015444326400757 and perplexity is 49.478806809916094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574586380359738 and perplexity of 96.98791473298304
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 540.0081624984741 and batch: 50, loss is 4.109703617095947 and perplexity is 60.92865668107796
At time: 541.2168436050415 and batch: 100, loss is 4.095353269577027 and perplexity is 60.06055297627749
At time: 542.3879890441895 and batch: 150, loss is 4.073362488746643 and perplexity is 58.75419112317611
At time: 543.562331199646 and batch: 200, loss is 4.075877466201782 and perplexity is 58.90214255852394
At time: 544.7343847751617 and batch: 250, loss is 4.039663825035095 and perplexity is 56.807242422385926
At time: 545.9054002761841 and batch: 300, loss is 4.089634079933166 and perplexity is 59.71803567800332
At time: 547.0797066688538 and batch: 350, loss is 4.034369678497314 and perplexity is 56.5072912499105
At time: 548.2505955696106 and batch: 400, loss is 4.043676471710205 and perplexity is 57.03564776331103
At time: 549.4217686653137 and batch: 450, loss is 3.999087953567505 and perplexity is 54.548376686437564
At time: 550.5969262123108 and batch: 500, loss is 3.972555537223816 and perplexity is 53.120107942905136
At time: 551.7727375030518 and batch: 550, loss is 3.9695940685272215 and perplexity is 52.963027115852356
At time: 552.9443771839142 and batch: 600, loss is 3.9340280151367186 and perplexity is 51.112445281335816
At time: 554.119106054306 and batch: 650, loss is 3.896529211997986 and perplexity is 49.231280894021594
At time: 555.2936964035034 and batch: 700, loss is 3.8928305387496946 and perplexity is 49.04952680409725
At time: 556.4625766277313 and batch: 750, loss is 3.8980462551116943 and perplexity is 49.30602354926161
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574865651685138 and perplexity of 97.01500445899403
Annealing...
Finished 28 epochs...
Completing Train Step...
At time: 559.8664362430573 and batch: 50, loss is 4.109552850723267 and perplexity is 60.919471380952444
At time: 561.0649447441101 and batch: 100, loss is 4.095109758377075 and perplexity is 60.04592933953439
At time: 562.2355768680573 and batch: 150, loss is 4.073096866607666 and perplexity is 58.73858678177716
At time: 563.4093656539917 and batch: 200, loss is 4.0756793785095216 and perplexity is 58.89047592458176
At time: 564.5979373455048 and batch: 250, loss is 4.039399666786194 and perplexity is 56.792238302520964
At time: 565.7881898880005 and batch: 300, loss is 4.089382491111755 and perplexity is 59.70301317761595
At time: 566.979309797287 and batch: 350, loss is 4.034109191894531 and perplexity is 56.49257377451615
At time: 568.1443798542023 and batch: 400, loss is 4.043474087715149 and perplexity is 57.02410582904687
At time: 569.3818078041077 and batch: 450, loss is 3.9988155460357664 and perplexity is 54.533519321505004
At time: 570.5648317337036 and batch: 500, loss is 3.9720420122146605 and perplexity is 53.09283644188563
At time: 571.7362835407257 and batch: 550, loss is 3.968937382698059 and perplexity is 52.92825846376584
At time: 572.9089105129242 and batch: 600, loss is 3.9332375812530516 and perplexity is 51.07206023566528
At time: 574.0793969631195 and batch: 650, loss is 3.8956226301193237 and perplexity is 49.18666893215336
At time: 575.2521064281464 and batch: 700, loss is 3.891837000846863 and perplexity is 49.000818440910656
At time: 576.4239058494568 and batch: 750, loss is 3.8971073484420775 and perplexity is 49.25975152085621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.574979560319767 and perplexity of 97.02605593510786
Annealing...
Finished 29 epochs...
Completing Train Step...
At time: 579.822808265686 and batch: 50, loss is 4.109509353637695 and perplexity is 60.916821619121684
At time: 581.0508623123169 and batch: 100, loss is 4.095037603378296 and perplexity is 60.041596881882356
At time: 582.229752779007 and batch: 150, loss is 4.073009939193725 and perplexity is 58.73348101024865
At time: 583.4044055938721 and batch: 200, loss is 4.075601534843445 and perplexity is 58.88589185246159
At time: 584.5760056972504 and batch: 250, loss is 4.039290971755982 and perplexity is 56.78606560393977
At time: 585.7509443759918 and batch: 300, loss is 4.089296164512635 and perplexity is 59.6978594419865
At time: 586.923858165741 and batch: 350, loss is 4.033999733924865 and perplexity is 56.48639055049734
At time: 588.1023046970367 and batch: 400, loss is 4.043394846916199 and perplexity is 57.019587372366985
At time: 589.2729818820953 and batch: 450, loss is 3.9987367820739745 and perplexity is 54.529224214624826
At time: 590.4413638114929 and batch: 500, loss is 3.971899929046631 and perplexity is 53.08529337936809
At time: 591.6146900653839 and batch: 550, loss is 3.968756742477417 and perplexity is 52.91869835497482
At time: 592.7885572910309 and batch: 600, loss is 3.9330232429504393 and perplexity is 51.06111471002805
At time: 593.9623537063599 and batch: 650, loss is 3.8953802013397216 and perplexity is 49.17474611330705
At time: 595.1345920562744 and batch: 700, loss is 3.891566138267517 and perplexity is 48.98754775018559
At time: 596.3065679073334 and batch: 750, loss is 3.896851806640625 and perplexity is 49.24716520344706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.5750004967977835 and perplexity of 97.0280873402601
Annealing...
Finished 30 epochs...
Completing Train Step...
At time: 599.7137839794159 and batch: 50, loss is 4.109496874809265 and perplexity is 60.91606145329919
At time: 600.9164881706238 and batch: 100, loss is 4.095016918182373 and perplexity is 60.04035492253244
At time: 602.0935940742493 and batch: 150, loss is 4.072986612319946 and perplexity is 58.73211095773009
At time: 603.2665278911591 and batch: 200, loss is 4.075578045845032 and perplexity is 58.88450869808584
At time: 604.4446363449097 and batch: 250, loss is 4.039260020256043 and perplexity is 56.784308017233805
At time: 605.6159746646881 and batch: 300, loss is 4.089271450042725 and perplexity is 59.69638405926738
At time: 606.792557477951 and batch: 350, loss is 4.033968844413757 and perplexity is 56.48464574045728
At time: 607.9686069488525 and batch: 400, loss is 4.043371753692627 and perplexity is 57.0182706214919
At time: 609.1388685703278 and batch: 450, loss is 3.998714442253113 and perplexity is 54.52800605513093
At time: 610.3079972267151 and batch: 500, loss is 3.9718612957000734 and perplexity is 53.083242556447196
At time: 611.4805245399475 and batch: 550, loss is 3.9687081670761106 and perplexity is 52.9161278703973
At time: 612.6710319519043 and batch: 600, loss is 3.932965340614319 and perplexity is 51.058158237795574
At time: 613.8448238372803 and batch: 650, loss is 3.895314784049988 and perplexity is 49.17152933991042
At time: 615.017817735672 and batch: 700, loss is 3.8914928436279297 and perplexity is 48.983957357108864
At time: 616.1918587684631 and batch: 750, loss is 3.8967828607559203 and perplexity is 49.243769931119296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 4.575006884197856 and perplexity of 97.02870709945157
Annealing...
Model not improving. Stopping early with 96.98677898850802loss at 30 epochs.
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8188e94d30>
SETTINGS FOR THIS RUN
{'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 2.0, 'num_layers': 1, 'lr': 0.0, 'dropout': 0.0}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 786 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.769430160522461 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 2.9523966312408447 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 4.121293067932129 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 5.335796594619751 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 6.502365827560425 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 7.668569087982178 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 8.840946435928345 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 10.007053852081299 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 11.172582626342773 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 12.347934484481812 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 13.516835927963257 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 14.688240051269531 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 15.85720157623291 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 17.03309392929077 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 18.20037603378296 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 1 epochs...
Completing Train Step...
At time: 21.65176510810852 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 22.820926427841187 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 23.988115310668945 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 25.163787841796875 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 26.332885265350342 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 27.50381565093994 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 28.674544095993042 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 29.843306064605713 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 31.015409231185913 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 32.19348168373108 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 33.364234924316406 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 34.537023067474365 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 35.71216154098511 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 36.88150334358215 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 38.057288646698 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 2 epochs...
Completing Train Step...
At time: 41.44875645637512 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 42.6473126411438 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 43.810479402542114 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 44.97199249267578 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 46.13249850273132 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 47.301313161849976 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 48.46111989021301 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 49.62082481384277 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 50.7834575176239 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 51.95042705535889 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 53.113181352615356 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 54.27713942527771 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 55.44436693191528 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 56.60618591308594 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 57.76573467254639 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 3 epochs...
Completing Train Step...
At time: 61.144129514694214 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 62.336228370666504 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 63.50616645812988 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 64.67975401878357 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 65.8530158996582 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 67.0547444820404 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 68.22917437553406 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 69.40247511863708 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 70.57032036781311 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 71.74162864685059 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 72.91170048713684 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 74.08448767662048 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 75.25954842567444 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 76.44139504432678 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 77.62931776046753 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 4 epochs...
Completing Train Step...
At time: 81.30760455131531 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 82.54298543930054 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 83.71932864189148 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 84.8837080001831 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 86.05359816551208 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 87.2287654876709 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 88.39958643913269 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 89.57006525993347 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 90.73787474632263 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 91.90422749519348 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 93.07640719413757 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 94.24637222290039 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 95.41600441932678 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 96.59091472625732 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 97.8216495513916 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 5 epochs...
Completing Train Step...
At time: 101.18738317489624 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 102.38783645629883 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 103.56038928031921 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 104.73912167549133 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 105.9157886505127 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 107.08518481254578 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 108.2554018497467 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 109.42796993255615 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 110.59792923927307 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 111.77287578582764 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 112.94980764389038 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 114.11737275123596 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 115.29350972175598 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 116.46218037605286 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 117.63229131698608 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 6 epochs...
Completing Train Step...
At time: 121.00442218780518 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 122.20432662963867 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 123.37641310691833 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 124.55354380607605 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 125.72132682800293 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 126.92043280601501 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 128.08814024925232 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 129.26030325889587 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 130.42618036270142 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 131.5993206501007 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 132.76764607429504 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 133.93618822097778 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 135.10317826271057 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 136.27993154525757 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 137.4510476589203 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 7 epochs...
Completing Train Step...
At time: 140.8575267791748 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 142.0797688961029 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 143.24670934677124 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 144.4137990474701 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 145.581148147583 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 146.74920105934143 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 147.9178147315979 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 149.08496928215027 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 150.26053142547607 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 151.4345257282257 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 152.61748790740967 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 153.80463361740112 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 154.99625945091248 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 156.16893339157104 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 157.42058086395264 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 8 epochs...
Completing Train Step...
At time: 160.77993822097778 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 161.97827816009521 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 163.14906525611877 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 164.31744146347046 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 165.49391841888428 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 166.66564083099365 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 167.83567333221436 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 169.00596618652344 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 170.17715096473694 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 171.34705805778503 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 172.52009868621826 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 173.68971800804138 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 174.8620536327362 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 176.03697800636292 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 177.21076488494873 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 9 epochs...
Completing Train Step...
At time: 180.578795671463 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 181.78900599479675 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 182.94958424568176 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 184.12509965896606 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 185.29070901870728 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 186.5102024078369 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 187.67233657836914 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 188.83506417274475 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 189.99686670303345 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 191.16210770606995 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 192.32437086105347 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 193.48495817184448 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 194.64963102340698 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 195.8104248046875 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 196.97341966629028 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 10 epochs...
Completing Train Step...
At time: 200.34327912330627 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 201.54607224464417 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 202.71472835540771 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 203.88566160202026 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 205.06240391731262 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 206.23189902305603 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 207.40387201309204 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 208.57420825958252 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 209.74636268615723 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 210.91499614715576 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 212.0894021987915 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 213.26001262664795 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 214.4334421157837 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 215.60127544403076 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 216.82897686958313 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 11 epochs...
Completing Train Step...
At time: 220.17470026016235 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 221.37341451644897 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 222.54356217384338 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 223.7139754295349 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 224.88196563720703 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 226.04561591148376 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 227.2062726020813 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 228.37695598602295 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 229.5481517314911 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 230.74733448028564 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 231.973792552948 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 233.17977476119995 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 234.39481353759766 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 235.56905913352966 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 236.7604992389679 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 12 epochs...
Completing Train Step...
At time: 240.15597295761108 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 241.3880615234375 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 242.55764174461365 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 243.73407220840454 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 244.90448594093323 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 246.12657737731934 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 247.29671335220337 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 248.4675590991974 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 249.6447250843048 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 250.81263709068298 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 251.98145055770874 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 253.15347003936768 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 254.3211386203766 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 255.49208283424377 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 256.6639325618744 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 13 epochs...
Completing Train Step...
At time: 260.0592052936554 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 261.25799584388733 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 262.4259376525879 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 263.5945625305176 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 264.76838278770447 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 265.9419569969177 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 267.1082754135132 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 268.2787890434265 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 269.4477450847626 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 270.616574048996 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 271.7892084121704 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 272.95524406433105 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 274.1238491535187 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 275.2932457923889 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 276.5150785446167 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 14 epochs...
Completing Train Step...
At time: 279.87852358818054 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 281.07980251312256 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 282.24717235565186 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 283.42281889915466 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 284.59242248535156 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 285.76117992401123 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 286.9304642677307 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 288.10788893699646 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 289.27990913391113 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 290.4488205909729 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 291.61859703063965 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 292.788809299469 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 293.96784353256226 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 295.1414439678192 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 296.30766582489014 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 15 epochs...
Completing Train Step...
At time: 299.729012966156 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 300.9590630531311 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 302.1320548057556 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 303.30578207969666 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 304.4795889854431 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 305.7120223045349 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 306.8878927230835 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 308.06297993659973 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 309.238751411438 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 310.420512676239 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 311.62163949012756 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 312.8435060977936 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 314.05909538269043 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 315.2453188896179 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 316.4222047328949 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 16 epochs...
Completing Train Step...
At time: 319.8283793926239 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 321.01860666275024 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 322.1797435283661 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 323.34347438812256 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 324.504771232605 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 325.6751244068146 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 326.8348054885864 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 327.9976153373718 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 329.1617765426636 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 330.3224115371704 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 331.4840908050537 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 332.64487290382385 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 333.8113992214203 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 334.97285056114197 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 336.184366941452 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 17 epochs...
Completing Train Step...
At time: 339.5591652393341 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 340.76114773750305 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 341.9293212890625 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 343.0986053943634 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 344.2702271938324 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 345.44049644470215 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 346.6157491207123 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 347.7836329936981 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 348.95542073249817 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 350.1239459514618 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 351.29261565208435 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 352.461386680603 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 353.630300283432 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 354.7973189353943 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 355.9712870121002 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 18 epochs...
Completing Train Step...
At time: 359.33095359802246 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 360.5257601737976 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 361.70146107673645 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 362.86299228668213 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 364.02844977378845 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 365.2281427383423 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 366.39198207855225 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 367.55559635162354 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 368.7269287109375 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 369.8930995464325 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 371.06070828437805 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 372.22729229927063 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 373.39253640174866 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 374.5589532852173 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 375.728435754776 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 19 epochs...
Completing Train Step...
At time: 379.09876680374146 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 380.29322504997253 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 381.4646270275116 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 382.6339690685272 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 383.8231520652771 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 385.00838685035706 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 386.16968870162964 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 387.33203053474426 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 388.5196440219879 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 389.68642473220825 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 390.85855436325073 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 392.0240890979767 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 393.19299149513245 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 394.36028265953064 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 395.58513617515564 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 20 epochs...
Completing Train Step...
At time: 398.955322265625 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 400.14864921569824 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 401.31176257133484 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 402.47901463508606 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 403.646164894104 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 404.810341835022 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 405.9811282157898 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 407.14330983161926 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 408.30826807022095 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 409.47496485710144 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 410.6391797065735 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 411.805645942688 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 412.97206830978394 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 414.14297699928284 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 415.30824398994446 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 21 epochs...
Completing Train Step...
At time: 418.64760971069336 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 419.8452618122101 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 421.0129714012146 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 422.18282175064087 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 423.35374450683594 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 424.5888614654541 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 425.76492166519165 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 426.93211030960083 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 428.0988337993622 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 429.2684738636017 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 430.4410481452942 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 431.608145236969 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 432.7748239040375 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 433.9424021244049 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 435.1158618927002 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 22 epochs...
Completing Train Step...
At time: 438.4842782020569 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 439.6862623691559 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 440.85615968704224 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 442.0258569717407 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 443.2025444507599 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 444.3710799217224 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 445.54468870162964 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 446.72944951057434 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 447.8990361690521 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 449.0718586444855 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 450.2510232925415 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 451.42335510253906 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 452.5929596424103 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 453.761732339859 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 454.9841310977936 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 23 epochs...
Completing Train Step...
At time: 458.32869696617126 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 459.5407238006592 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 460.6955327987671 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 461.8498728275299 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 463.0232801437378 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 464.20722484588623 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 465.411833524704 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 466.65030312538147 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 467.8584940433502 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 469.0331964492798 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 470.2186782360077 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 471.3975830078125 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 472.54137659072876 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 473.710129737854 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 474.8735394477844 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 24 epochs...
Completing Train Step...
At time: 478.25662302970886 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 479.47785568237305 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 480.647723197937 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 481.8198058605194 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 482.99641489982605 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 484.19952058792114 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 485.36835193634033 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 486.53791522979736 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 487.710351228714 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 488.8873200416565 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 490.05819368362427 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 491.2261793613434 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 492.3951439857483 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 493.5681653022766 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 494.73695969581604 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 25 epochs...
Completing Train Step...
At time: 498.1171975135803 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 499.31572341918945 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 500.48259830474854 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 501.648277759552 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 502.8178586959839 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 503.98171949386597 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 505.1470470428467 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 506.3161106109619 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 507.4934072494507 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 508.68106508255005 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 509.8568570613861 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 511.0394170284271 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 512.2030804157257 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 513.3879318237305 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 514.6382460594177 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 26 epochs...
Completing Train Step...
At time: 517.9557197093964 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 519.1919782161713 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 520.370356798172 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 521.5428495407104 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 522.7152805328369 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 523.9093050956726 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 525.0813505649567 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 526.2480299472809 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 527.4165098667145 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 528.5912289619446 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 529.7590157985687 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 530.9292666912079 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 532.1007874011993 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 533.2697956562042 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 534.4362387657166 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 27 epochs...
Completing Train Step...
At time: 537.8273856639862 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 539.0471484661102 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 540.2108030319214 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 541.3784780502319 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 542.5437200069427 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 543.7396793365479 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 544.9063763618469 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 546.0769186019897 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 547.241338968277 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 548.4080879688263 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 549.577641248703 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 550.7444658279419 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 551.9144492149353 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 553.0874350070953 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 554.2560360431671 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 28 epochs...
Completing Train Step...
At time: 557.6174464225769 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 558.8185107707977 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 559.9860413074493 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 561.1576523780823 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 562.3334810733795 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 563.5230040550232 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 564.7164144515991 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 565.92107462883 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 567.108321428299 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 568.3061227798462 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 569.4764876365662 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 570.642219543457 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 571.8072030544281 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 572.9790453910828 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 574.2065296173096 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 29 epochs...
Completing Train Step...
At time: 577.6299583911896 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 578.8615407943726 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 580.0324084758759 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 581.2097156047821 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 582.3789279460907 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 583.5508856773376 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 584.7232475280762 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 585.892153263092 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 587.0617661476135 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 588.2324090003967 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 589.410341501236 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 590.5864315032959 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 591.7612180709839 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 592.9346559047699 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 594.1040132045746 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 30 epochs...
Completing Train Step...
At time: 597.4987072944641 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 598.7187986373901 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 599.8844513893127 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 601.0506801605225 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 602.210967540741 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 603.426260471344 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 604.5916187763214 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 605.754056930542 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 606.9139187335968 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 608.0719006061554 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 609.2341198921204 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 610.3974413871765 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 611.5574014186859 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 612.723509311676 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 613.882157087326 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 31 epochs...
Completing Train Step...
At time: 617.2552888393402 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 618.4600696563721 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 619.6345889568329 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 620.8052158355713 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 621.9739103317261 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 623.142196893692 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 624.3112151622772 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 625.4775052070618 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 626.6459672451019 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 627.8154399394989 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 628.9888942241669 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 630.1532714366913 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 631.3183944225311 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 632.4865057468414 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 633.710550069809 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 32 epochs...
Completing Train Step...
At time: 637.0809094905853 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 638.2893092632294 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 639.4640655517578 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 640.6264355182648 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 641.8028140068054 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 642.9808986186981 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 644.1481714248657 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 645.318118095398 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 646.4813585281372 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 647.6461265087128 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 648.8124494552612 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 649.9786984920502 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 651.1473934650421 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 652.3125710487366 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 653.4837427139282 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 33 epochs...
Completing Train Step...
At time: 656.8772399425507 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 658.0760328769684 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 659.2443833351135 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 660.4165694713593 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 661.582311630249 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 662.7814204692841 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 663.9491295814514 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 665.1159706115723 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 666.2808179855347 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 667.4464523792267 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 668.6139869689941 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 669.7790896892548 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 670.950813293457 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 672.1169822216034 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 673.2851061820984 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 34 epochs...
Completing Train Step...
At time: 676.6303832530975 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 677.8454267978668 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 679.0145828723907 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 680.1808836460114 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 681.3442671298981 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 682.5082876682281 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 683.6776039600372 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 684.8418705463409 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 686.0116431713104 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 687.1752316951752 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 688.3398089408875 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 689.5031833648682 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 690.6769442558289 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 691.8437309265137 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 693.0385947227478 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 35 epochs...
Completing Train Step...
At time: 696.3928837776184 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 697.6037147045135 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 698.7710242271423 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 699.9412715435028 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 701.1090152263641 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 702.2730762958527 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 703.4417991638184 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 704.6148979663849 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 705.7819447517395 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 706.9481041431427 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 708.1137299537659 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 709.2794735431671 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 710.4460790157318 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 711.6146535873413 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 712.7824327945709 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 36 epochs...
Completing Train Step...
At time: 716.1226344108582 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 717.3504874706268 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 718.5556852817535 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 719.7548089027405 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 720.9452219009399 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 722.1762728691101 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 723.3818757534027 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 724.5598292350769 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 725.7283675670624 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 726.8992373943329 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 728.0715043544769 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 729.2447860240936 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 730.4316685199738 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 731.610354423523 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 732.783668756485 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 37 epochs...
Completing Train Step...
At time: 736.1857922077179 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 737.3770084381104 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 738.5376591682434 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 739.7018637657166 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 740.8612813949585 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 742.0258507728577 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 743.191739320755 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 744.3550944328308 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 745.5147936344147 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 746.6711719036102 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 747.8348715305328 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 748.9958181381226 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 750.1547994613647 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 751.313839673996 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 752.5265669822693 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 38 epochs...
Completing Train Step...
At time: 755.8712210655212 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 757.0739727020264 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 758.2458975315094 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 759.4139201641083 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 760.5812652111053 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 761.747401714325 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 762.9193761348724 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 764.0940907001495 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 765.278534412384 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 766.4478757381439 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 767.6178457736969 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 768.7870354652405 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 769.9546213150024 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 771.1224920749664 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 772.2938320636749 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 39 epochs...
Completing Train Step...
At time: 775.667492389679 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 776.8736951351166 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 778.040164232254 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 779.2106246948242 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 780.4026567935944 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 781.6279783248901 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 782.7980952262878 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 783.9766545295715 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 785.1487987041473 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 786.3185465335846 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 787.483668088913 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 788.6594657897949 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 789.8250803947449 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 790.9890830516815 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 792.1591765880585 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 40 epochs...
Completing Train Step...
At time: 795.5316548347473 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 796.7724933624268 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 797.9534406661987 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 799.1471679210663 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 800.3456881046295 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 801.5403480529785 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 802.7270722389221 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 803.8967611789703 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 805.0676617622375 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 806.2334952354431 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 807.4031476974487 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 808.5744562149048 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 809.7416768074036 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 810.908673286438 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 812.1505706310272 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 41 epochs...
Completing Train Step...
At time: 815.5196511745453 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 816.7135472297668 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 817.8820209503174 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 819.0484523773193 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 820.2133791446686 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 821.3801600933075 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 822.547580242157 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 823.7156071662903 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 824.879551410675 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 826.046660900116 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 827.2198348045349 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 828.3840045928955 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 829.5451877117157 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 830.709557056427 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 831.8745312690735 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 42 epochs...
Completing Train Step...
At time: 835.2467153072357 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 836.4466888904572 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 837.6124765872955 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 838.7772986888885 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 839.9468684196472 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 841.1650347709656 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 842.3340821266174 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 843.5070824623108 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 844.672162771225 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 845.8378267288208 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 847.0059537887573 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 848.1734471321106 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 849.3418626785278 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 850.5093948841095 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 851.6812691688538 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 43 epochs...
Completing Train Step...
At time: 855.0321645736694 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 856.2395012378693 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 857.4076449871063 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 858.5775816440582 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 859.748085975647 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 860.9197626113892 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 862.0877597332001 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 863.2573025226593 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 864.4287679195404 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 865.6027429103851 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 866.7736966609955 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 867.9435698986053 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 869.1156830787659 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 870.2909548282623 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 871.5190136432648 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 44 epochs...
Completing Train Step...
At time: 874.8790948390961 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 876.0694568157196 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 877.2454569339752 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 878.4102611541748 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 879.5734751224518 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 880.7328984737396 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 881.8921563625336 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 883.0566608905792 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 884.2145211696625 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 885.3769805431366 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 886.5348038673401 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 887.6918876171112 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 888.859447479248 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 890.0217428207397 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 891.1796519756317 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 45 epochs...
Completing Train Step...
At time: 894.5551073551178 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 895.7526137828827 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 896.9229123592377 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 898.0914795398712 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 899.2583482265472 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 900.48002576828 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 901.6520013809204 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 902.8199653625488 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 903.9935832023621 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 905.1574237346649 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 906.3281128406525 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 907.504887342453 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 908.6749203205109 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 909.83997631073 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 911.0095722675323 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 46 epochs...
Completing Train Step...
At time: 914.3967645168304 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 915.5882375240326 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 916.7511403560638 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 917.9137353897095 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 919.0827877521515 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 920.2458620071411 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 921.4139919281006 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 922.5850217342377 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 923.7682409286499 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 924.9311261177063 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 926.0931625366211 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 927.2590034008026 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 928.425299167633 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 929.5868744850159 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 930.8044590950012 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 47 epochs...
Completing Train Step...
At time: 934.1476981639862 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 935.3435802459717 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 936.5157723426819 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 937.680600643158 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 938.8470153808594 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 940.0112679004669 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 941.1739535331726 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 942.3377785682678 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 943.5022585391998 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 944.6658160686493 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 945.8305897712708 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 946.9960355758667 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 948.1644861698151 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 949.3226020336151 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 950.4875178337097 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 48 epochs...
Completing Train Step...
At time: 953.9243173599243 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 955.1366231441498 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 956.3060035705566 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 957.4972760677338 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 958.6787738800049 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 959.8979797363281 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 961.0628654956818 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 962.2261788845062 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 963.3917362689972 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 964.5628156661987 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 965.728889465332 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 966.8918759822845 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 968.0584597587585 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 969.2235953807831 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 970.3885416984558 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished 49 epochs...
Completing Train Step...
At time: 973.7429370880127 and batch: 50, loss is 9.935351161956786 and perplexity is 20647.533784951993
At time: 974.9416677951813 and batch: 100, loss is 9.936036796569825 and perplexity is 20661.695303047552
At time: 976.1074185371399 and batch: 150, loss is 9.935789051055908 and perplexity is 20656.57709475905
At time: 977.2707200050354 and batch: 200, loss is 9.935535125732422 and perplexity is 20651.332532629895
At time: 978.4378714561462 and batch: 250, loss is 9.934673862457275 and perplexity is 20633.553955453488
At time: 979.604964017868 and batch: 300, loss is 9.935772781372071 and perplexity is 20656.241021514485
At time: 980.7751233577728 and batch: 350, loss is 9.935514450073242 and perplexity is 20650.905557130867
At time: 981.9454073905945 and batch: 400, loss is 9.93622055053711 and perplexity is 20665.492320378136
At time: 983.1130881309509 and batch: 450, loss is 9.93607675552368 and perplexity is 20662.520939272425
At time: 984.2775368690491 and batch: 500, loss is 9.93548963546753 and perplexity is 20650.39311940986
At time: 985.4408552646637 and batch: 550, loss is 9.936130695343017 and perplexity is 20663.63550197831
At time: 986.6059193611145 and batch: 600, loss is 9.936207084655761 and perplexity is 20665.214043184165
At time: 987.7731864452362 and batch: 650, loss is 9.93576644897461 and perplexity is 20656.110218400423
At time: 988.9386637210846 and batch: 700, loss is 9.935971717834473 and perplexity is 20660.350709799695
At time: 990.1645882129669 and batch: 750, loss is 9.936061248779296 and perplexity is 20662.20053332611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 87 batches
Done Evaluating: Achieved loss of 10.052695340888445 and perplexity of 23218.283771908704
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f8188e94d30>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 3.3014077963985886, 'num_layers': 1, 'lr': 14.212458493693168, 'dropout': 0.16697668162061952}, 'best_accuracy': -133.4278283497118}, {'params': {'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 4.728900084303893, 'num_layers': 1, 'lr': 20.865679133274845, 'dropout': 0.19327484176991094}, 'best_accuracy': -145.27587218306}, {'params': {'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 2.5838881158966025, 'num_layers': 1, 'lr': 5.332471187800479, 'dropout': 0.6610078554862018}, 'best_accuracy': -95.40209141205423}, {'params': {'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 7.920087375621255, 'num_layers': 1, 'lr': 17.959793212236615, 'dropout': 0.6065585298813164}, 'best_accuracy': -157.92181642194686}, {'params': {'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 3.715717601761124, 'num_layers': 1, 'lr': 4.9817027769186435, 'dropout': 0.8872043072898154}, 'best_accuracy': -96.98677898850802}, {'params': {'wordvec_source': '', 'seq_len': 35, 'data': 'wikitext', 'tune_wordvecs': True, 'wordvec_dim': 200, 'batch_size': 80, 'anneal': 2.0, 'num_layers': 1, 'lr': 0.0, 'dropout': 0.0}, 'best_accuracy': -23218.283771908704}]
