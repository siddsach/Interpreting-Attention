Building Bayesian Optimizer for 
 data:gigasmall 
 choices:[{'type': 'continuous', 'domain': [0, 30], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [2, 8], 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'batch_size': 80, 'wordvec_dim': 200, 'num_layers': 1, 'dropout': 0.04963365650531826, 'data': 'gigasmall', 'anneal': 7.620066222937851, 'lr': 12.32942957478652, 'tune_wordvecs': True, 'wordvec_source': '', 'seq_len': 50}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_train.txt...
Got Train Dataset with 21438304 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_val.txt...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/gigaword/gigaword_small_test.txt...
Loading Vectors From Memory...
Building Vocab...
Found 55221 tokens
Getting Batches...
Created Iterator with 5360 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 11.425426959991455 and batch: 50, loss is 8.133621253967284 and perplexity is 3407.1152822459762
At time: 15.376985549926758 and batch: 100, loss is 6.715688610076905 and perplexity is 825.2518480203769
At time: 19.335697650909424 and batch: 150, loss is 6.239349918365479 and perplexity is 512.5252193888537
At time: 23.29373073577881 and batch: 200, loss is 6.035272827148438 and perplexity is 417.91281204695287
At time: 27.261930227279663 and batch: 250, loss is 5.847942714691162 and perplexity is 346.5207546067606
At time: 31.238257884979248 and batch: 300, loss is 5.747399969100952 and perplexity is 313.3748159306283
At time: 35.21657657623291 and batch: 350, loss is 5.676459074020386 and perplexity is 291.91395203262056
At time: 39.68843078613281 and batch: 400, loss is 5.598892765045166 and perplexity is 270.1271475612054
At time: 43.67212700843811 and batch: 450, loss is 5.601966381072998 and perplexity is 270.9586919609583
At time: 47.64381766319275 and batch: 500, loss is 5.523256444931031 and perplexity is 250.44928499817917
At time: 51.61622905731201 and batch: 550, loss is 5.506438837051392 and perplexity is 246.27254694320092
At time: 55.58660840988159 and batch: 600, loss is 5.475628290176392 and perplexity is 238.80045578795216
At time: 59.575724840164185 and batch: 650, loss is 5.448406286239624 and perplexity is 232.38751145282055
At time: 63.55786061286926 and batch: 700, loss is 5.48899585723877 and perplexity is 242.01406813075806
At time: 67.53794264793396 and batch: 750, loss is 5.463323907852173 and perplexity is 235.88016670114172
At time: 71.50236701965332 and batch: 800, loss is 5.420745258331299 and perplexity is 226.04752354328957
At time: 75.50535249710083 and batch: 850, loss is 5.409387445449829 and perplexity is 223.49464308125104
At time: 79.4849579334259 and batch: 900, loss is 5.349890031814575 and perplexity is 210.58513892771924
At time: 83.46460747718811 and batch: 950, loss is 5.38519510269165 and perplexity is 218.1526621273143
At time: 87.42599105834961 and batch: 1000, loss is 5.410746908187866 and perplexity is 223.79868233886617
At time: 91.40133500099182 and batch: 1050, loss is 5.375637559890747 and perplexity is 216.0775908092429
At time: 95.37874865531921 and batch: 1100, loss is 5.3661886405944825 and perplexity is 214.04550671031677
At time: 99.35230803489685 and batch: 1150, loss is 5.335174016952514 and perplexity is 207.50885572493328
At time: 103.32349348068237 and batch: 1200, loss is 5.3419250392913815 and perplexity is 208.9144920479153
At time: 107.30852890014648 and batch: 1250, loss is 5.265848941802979 and perplexity is 193.61060317390238
At time: 111.27702498435974 and batch: 1300, loss is 5.306583957672119 and perplexity is 201.66017074701597
At time: 115.2507004737854 and batch: 1350, loss is 5.275042018890381 and perplexity is 195.3986867760568
At time: 119.23144054412842 and batch: 1400, loss is 5.233088445663452 and perplexity is 187.3705947232603
At time: 123.21995902061462 and batch: 1450, loss is 5.2728854751586915 and perplexity is 194.97775500493074
At time: 127.20651912689209 and batch: 1500, loss is 5.245884351730346 and perplexity is 189.78357647285836
At time: 131.17948150634766 and batch: 1550, loss is 5.262327499389649 and perplexity is 192.93001361599033
At time: 135.16590404510498 and batch: 1600, loss is 5.249398107528687 and perplexity is 190.4516025680883
At time: 139.15921902656555 and batch: 1650, loss is 5.268361206054688 and perplexity is 194.09761566689897
At time: 143.14488458633423 and batch: 1700, loss is 5.244627676010132 and perplexity is 189.54522985378108
At time: 147.12675046920776 and batch: 1750, loss is 5.26793176651001 and perplexity is 194.01428037022063
At time: 151.107164144516 and batch: 1800, loss is 5.213342657089234 and perplexity is 183.70710287359478
At time: 155.087384223938 and batch: 1850, loss is 5.2354552364349365 and perplexity is 187.81458692876691
At time: 159.09131503105164 and batch: 1900, loss is 5.1946274948120115 and perplexity is 180.30096724046842
At time: 163.08601760864258 and batch: 1950, loss is 5.1572984600067135 and perplexity is 173.69457834476611
At time: 167.077401638031 and batch: 2000, loss is 5.175514545440674 and perplexity is 176.88760757632102
At time: 171.05862736701965 and batch: 2050, loss is 5.204799861907959 and perplexity is 182.14441508490575
At time: 175.04896521568298 and batch: 2100, loss is 5.167160005569458 and perplexity is 175.41594908082232
At time: 179.03462481498718 and batch: 2150, loss is 5.185140085220337 and perplexity is 178.59846704311866
At time: 183.01397609710693 and batch: 2200, loss is 5.16282901763916 and perplexity is 174.65786752857113
At time: 186.9962968826294 and batch: 2250, loss is 5.161722946166992 and perplexity is 174.4647902422314
At time: 190.97311329841614 and batch: 2300, loss is 5.144122896194458 and perplexity is 171.42106464556633
At time: 194.9581618309021 and batch: 2350, loss is 5.137426977157593 and perplexity is 170.27707737281608
At time: 198.9455316066742 and batch: 2400, loss is 5.163282585144043 and perplexity is 174.7371046300831
At time: 202.9304711818695 and batch: 2450, loss is 5.1444941425323485 and perplexity is 171.48471590247033
At time: 206.90046429634094 and batch: 2500, loss is 5.147803430557251 and perplexity is 172.05314825368643
At time: 210.8702495098114 and batch: 2550, loss is 5.1667546463012695 and perplexity is 175.34485700996274
At time: 214.83880352973938 and batch: 2600, loss is 5.178129043579101 and perplexity is 177.35068499077494
At time: 218.81263160705566 and batch: 2650, loss is 5.170885705947876 and perplexity is 176.07071532271888
At time: 222.78863286972046 and batch: 2700, loss is 5.153535623550415 and perplexity is 173.04222217761546
At time: 226.76218700408936 and batch: 2750, loss is 5.082676029205322 and perplexity is 161.20486817371332
At time: 230.74101567268372 and batch: 2800, loss is 5.100570468902588 and perplexity is 164.11550339165112
At time: 234.7225625514984 and batch: 2850, loss is 5.105061349868774 and perplexity is 164.85418400396347
At time: 238.69814109802246 and batch: 2900, loss is 5.104363374710083 and perplexity is 164.73916002533193
At time: 242.68660521507263 and batch: 2950, loss is 5.118643665313721 and perplexity is 167.10856070421687
At time: 246.66399478912354 and batch: 3000, loss is 5.139851179122925 and perplexity is 170.69036414133376
At time: 250.64327383041382 and batch: 3050, loss is 5.079001970291138 and perplexity is 160.61367868831368
At time: 254.63104820251465 and batch: 3100, loss is 5.105981969833374 and perplexity is 165.00602193879843
At time: 258.623215675354 and batch: 3150, loss is 5.0868774509429935 and perplexity is 161.88358259545396
At time: 262.6036288738251 and batch: 3200, loss is 5.097317304611206 and perplexity is 163.5824761789914
At time: 266.59682869911194 and batch: 3250, loss is 5.097444152832031 and perplexity is 163.60322764116808
At time: 270.58658480644226 and batch: 3300, loss is 5.083779582977295 and perplexity is 161.3828646103022
At time: 274.5652732849121 and batch: 3350, loss is 5.082194986343384 and perplexity is 161.12734037116178
At time: 278.55459356307983 and batch: 3400, loss is 5.0817027759552005 and perplexity is 161.048051335455
At time: 282.5420653820038 and batch: 3450, loss is 5.078287162780762 and perplexity is 160.49891184750467
At time: 286.5351016521454 and batch: 3500, loss is 5.089453296661377 and perplexity is 162.30110723723382
At time: 290.5218036174774 and batch: 3550, loss is 5.046310634613037 and perplexity is 155.44790114510047
At time: 294.5108768939972 and batch: 3600, loss is 5.057509880065918 and perplexity is 157.19858521749816
At time: 298.513769865036 and batch: 3650, loss is 5.084864978790283 and perplexity is 161.55812399153172
At time: 302.4946804046631 and batch: 3700, loss is 5.066063718795776 and perplexity is 158.54900396212932
At time: 306.48795342445374 and batch: 3750, loss is 5.0649034118652345 and perplexity is 158.36514514096137
At time: 310.47383189201355 and batch: 3800, loss is 5.015692310333252 and perplexity is 150.76047367876183
At time: 314.4793817996979 and batch: 3850, loss is 5.0702980041503904 and perplexity is 159.2217690220385
At time: 318.4814429283142 and batch: 3900, loss is 5.094348392486572 and perplexity is 163.09753441303778
At time: 322.4794383049011 and batch: 3950, loss is 5.046704225540161 and perplexity is 155.50909607072543
At time: 326.4638783931732 and batch: 4000, loss is 5.02978518486023 and perplexity is 152.90016389745293
At time: 330.4563946723938 and batch: 4050, loss is 5.06732572555542 and perplexity is 158.749220187406
At time: 334.44234704971313 and batch: 4100, loss is 5.0236013221740725 and perplexity is 151.957567725618
At time: 338.43645691871643 and batch: 4150, loss is 5.087276706695556 and perplexity is 161.94822845130685
At time: 342.4163398742676 and batch: 4200, loss is 5.004398288726807 and perplexity is 149.06736065620245
At time: 346.39259099960327 and batch: 4250, loss is 5.0789878463745115 and perplexity is 160.61141021012673
At time: 350.378710269928 and batch: 4300, loss is 5.059194974899292 and perplexity is 157.4637030528746
At time: 354.3815016746521 and batch: 4350, loss is 5.097883586883545 and perplexity is 163.6751362687238
At time: 358.37649369239807 and batch: 4400, loss is 5.05081353187561 and perplexity is 156.14944537906445
At time: 362.3583405017853 and batch: 4450, loss is 5.0855531406402585 and perplexity is 161.66934039206922
At time: 366.3555734157562 and batch: 4500, loss is 5.047488250732422 and perplexity is 155.63106692753493
At time: 370.35263299942017 and batch: 4550, loss is 5.070031967163086 and perplexity is 159.1794157763093
At time: 374.3353524208069 and batch: 4600, loss is 5.063477783203125 and perplexity is 158.13953610653326
At time: 378.3236870765686 and batch: 4650, loss is 5.001659898757935 and perplexity is 148.65971449291482
At time: 382.3234169483185 and batch: 4700, loss is 5.117427263259888 and perplexity is 166.90541308740097
At time: 386.3012454509735 and batch: 4750, loss is 5.0434459400177 and perplexity is 155.00322761386465
At time: 390.2933087348938 and batch: 4800, loss is 5.049338636398315 and perplexity is 155.9193110220718
At time: 394.27476501464844 and batch: 4850, loss is 5.0279475021362305 and perplexity is 152.619439927559
At time: 398.2654654979706 and batch: 4900, loss is 5.0442274379730225 and perplexity is 155.12440966490826
At time: 402.244003534317 and batch: 4950, loss is 5.067107181549073 and perplexity is 158.71453028759473
At time: 406.2232484817505 and batch: 5000, loss is 5.009018039703369 and perplexity is 149.75760789874386
At time: 410.19584441185 and batch: 5050, loss is 5.030869045257568 and perplexity is 153.06597617229463
At time: 414.1984193325043 and batch: 5100, loss is 5.019239234924316 and perplexity is 151.29615916662155
At time: 418.16840386390686 and batch: 5150, loss is 5.032023305892944 and perplexity is 153.24275620869705
At time: 422.12591767311096 and batch: 5200, loss is 5.071487140655518 and perplexity is 159.4112180580517
At time: 426.13283944129944 and batch: 5250, loss is 5.056850070953369 and perplexity is 157.09489836901722
At time: 430.11931920051575 and batch: 5300, loss is 5.025608758926392 and perplexity is 152.26291931632406
At time: 434.0913896560669 and batch: 5350, loss is 5.002187519073487 and perplexity is 148.73817107420842
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 356 batches
Traceback (most recent call last):
  File "tune_models.py", line 172, in <module>
    seq_len = args.seq_len)
  File "tune_models.py", line 147, in tuneModels
    opt = Optimizer(dataset, vectors, tune_wordvecs, wordvec_dim, choices, trainerclass, max_time, num_layers, batch_size, seq_len)
  File "tune_models.py", line 35, in __init__
    exact_feval = True
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 117, in __init__
    self._init_design_chooser()
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/methods/bayesian_optimization.py", line 192, in _init_design_chooser
    self.Y, _ = self.objective.evaluate(self.X)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 50, in evaluate
    f_evals, cost_evals = self._eval_func(x)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/GPyOpt/core/task/objective.py", line 74, in _eval_func
    rlt = self.func(np.atleast_2d(x[i]))
  File "tune_models.py", line 73, in getError
    trainer.train()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 408, in train
    this_perplexity = self.evaluate()
  File "/home-nfs/siddsach/Interpreting-Attention/interpreting_language/pretraining/langmodel/trainer.py", line 348, in evaluate
    for i, batch in enumerate(self.valid_iterator):
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torchtext/data/iterator.py", line 246, in __iter__
    text=data[i:i + seq_len],
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/variable.py", line 76, in __getitem__
    return Index.apply(self, key)
  File "/home-nfs/siddsach/anaconda3/lib/python3.5/site-packages/torch/autograd/_functions/tensor.py", line 16, in forward
    result = i.index(ctx.index)
ValueError: result of slicing is an empty tensor
