Building Bayesian Optimizer for 
 data:wikitext 
 choices:[{'domain': [0, 30], 'type': 'continuous', 'name': 'lr'}, {'domain': [0, 1], 'type': 'continuous', 'name': 'dropout'}, {'domain': [2, 8], 'type': 'continuous', 'name': 'anneal'}]
SETTINGS FOR THIS RUN
{'seq_len': 35, 'anneal': 7.763089457080676, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 8.650584662440988, 'dropout': 0.05567825749440558, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.8462700843811035 and batch: 50, loss is 7.426407976150513 and perplexity is 1679.7629750274498
At time: 2.9999279975891113 and batch: 100, loss is 6.543595914840698 and perplexity is 694.7804628278496
At time: 4.156979560852051 and batch: 150, loss is 6.068299083709717 and perplexity is 431.94535349773014
At time: 5.355043649673462 and batch: 200, loss is 5.841692085266113 and perplexity is 344.36153704614946
At time: 6.521058559417725 and batch: 250, loss is 5.708081693649292 and perplexity is 301.2925421679781
At time: 7.678917646408081 and batch: 300, loss is 5.601160907745362 and perplexity is 270.74052983537416
At time: 8.833410739898682 and batch: 350, loss is 5.5329881763458255 and perplexity is 252.8984883377371
At time: 9.98819613456726 and batch: 400, loss is 5.448662815093994 and perplexity is 232.44713320192585
At time: 11.143685102462769 and batch: 450, loss is 5.365105657577515 and perplexity is 213.81382453826419
At time: 12.299362421035767 and batch: 500, loss is 5.33818754196167 and perplexity is 208.1351320269791
At time: 13.456290483474731 and batch: 550, loss is 5.312565317153931 and perplexity is 202.86998728830676
At time: 14.612467050552368 and batch: 600, loss is 5.291293268203735 and perplexity is 198.6001025953294
At time: 15.76814579963684 and batch: 650, loss is 5.245761585235596 and perplexity is 189.76027883852726
At time: 16.923560857772827 and batch: 700, loss is 5.247787389755249 and perplexity is 190.1450857091977
At time: 18.079819440841675 and batch: 750, loss is 5.2174512481689455 and perplexity is 184.46343289920245
At time: 19.235764026641846 and batch: 800, loss is 5.224633703231811 and perplexity is 185.79310264714172
At time: 20.394901990890503 and batch: 850, loss is 5.255684442520142 and perplexity is 191.65261617328042
At time: 21.559847354888916 and batch: 900, loss is 5.217910261154175 and perplexity is 184.54812344574643
At time: 22.73292112350464 and batch: 950, loss is 5.187425041198731 and perplexity is 179.00702326685698
At time: 23.896814823150635 and batch: 1000, loss is 5.173321781158447 and perplexity is 176.50015969464158
At time: 25.060450792312622 and batch: 1050, loss is 5.137073612213134 and perplexity is 170.21691805255116
At time: 26.223461866378784 and batch: 1100, loss is 5.110195617675782 and perplexity is 165.7027660934964
At time: 27.389281034469604 and batch: 1150, loss is 5.12569766998291 and perplexity is 168.29151264072468
At time: 28.552521228790283 and batch: 1200, loss is 5.105053834915161 and perplexity is 164.85294513707277
At time: 29.717057704925537 and batch: 1250, loss is 5.106533145904541 and perplexity is 165.09699437830227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.8383263358234485 and perplexity of 126.25786153783966
Finished 1 epochs...
Completing Train Step...
At time: 32.591330766677856 and batch: 50, loss is 5.022705879211426 and perplexity is 151.82155929396149
At time: 33.777209520339966 and batch: 100, loss is 5.0219410800933835 and perplexity is 151.70549068955435
At time: 34.93758797645569 and batch: 150, loss is 4.905211896896362 and perplexity is 134.99151125871626
At time: 36.0974338054657 and batch: 200, loss is 4.932141427993774 and perplexity is 138.6761596124755
At time: 37.257758140563965 and batch: 250, loss is 4.932069578170776 and perplexity is 138.66619611289542
At time: 38.44349145889282 and batch: 300, loss is 4.908403921127319 and perplexity is 135.42309588146244
At time: 39.6035361289978 and batch: 350, loss is 4.888160305023193 and perplexity is 132.7092048802422
At time: 40.76304912567139 and batch: 400, loss is 4.879484548568725 and perplexity is 131.56283215521378
At time: 41.92303919792175 and batch: 450, loss is 4.811150388717651 and perplexity is 122.87288782766208
At time: 43.0830283164978 and batch: 500, loss is 4.821846189498902 and perplexity is 124.19416522100083
At time: 44.24378252029419 and batch: 550, loss is 4.809639263153076 and perplexity is 122.6873516851825
At time: 45.40418291091919 and batch: 600, loss is 4.817824506759644 and perplexity is 123.69569869936487
At time: 46.564074754714966 and batch: 650, loss is 4.812006340026856 and perplexity is 122.97810606128226
At time: 47.724159717559814 and batch: 700, loss is 4.801341753005982 and perplexity is 121.67356391330537
At time: 48.88452672958374 and batch: 750, loss is 4.7937959289550784 and perplexity is 120.75889192538513
At time: 50.04475998878479 and batch: 800, loss is 4.818431625366211 and perplexity is 123.77081946089615
At time: 51.205212354660034 and batch: 850, loss is 4.857126531600952 and perplexity is 128.65398729492554
At time: 52.365636110305786 and batch: 900, loss is 4.809636106491089 and perplexity is 122.68696440329443
At time: 53.525261640548706 and batch: 950, loss is 4.794840335845947 and perplexity is 120.88507922821972
At time: 54.68560075759888 and batch: 1000, loss is 4.788959236145019 and perplexity is 120.17622847899223
At time: 55.845693826675415 and batch: 1050, loss is 4.760960817337036 and perplexity is 116.85815131405063
At time: 57.00658130645752 and batch: 1100, loss is 4.733312282562256 and perplexity is 113.67145144599624
At time: 58.16764831542969 and batch: 1150, loss is 4.7396009731292725 and perplexity is 114.38854846779303
At time: 59.32796049118042 and batch: 1200, loss is 4.738801441192627 and perplexity is 114.29712772187959
At time: 60.488436698913574 and batch: 1250, loss is 4.758483963012695 and perplexity is 116.56906885199582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.708967445540602 and perplexity of 110.93755168256136
Finished 2 epochs...
Completing Train Step...
At time: 63.4180383682251 and batch: 50, loss is 4.711188888549804 and perplexity is 111.1842670618241
At time: 64.58042526245117 and batch: 100, loss is 4.707248592376709 and perplexity is 110.74703010707098
At time: 65.75948882102966 and batch: 150, loss is 4.616204652786255 and perplexity is 101.1095571071128
At time: 66.91565275192261 and batch: 200, loss is 4.668649435043335 and perplexity is 106.55373746648566
At time: 68.07290506362915 and batch: 250, loss is 4.672605934143067 and perplexity is 106.97615232381024
At time: 69.22908186912537 and batch: 300, loss is 4.665640830993652 and perplexity is 106.2336412233131
At time: 70.38593292236328 and batch: 350, loss is 4.643961515426636 and perplexity is 103.95535367955476
At time: 71.54372358322144 and batch: 400, loss is 4.657655372619629 and perplexity is 105.38869503471929
At time: 72.70148730278015 and batch: 450, loss is 4.5871274948120115 and perplexity is 98.2119103643729
At time: 73.85861444473267 and batch: 500, loss is 4.601022806167602 and perplexity is 99.58612086818358
At time: 75.01666569709778 and batch: 550, loss is 4.59334280014038 and perplexity is 98.82422827440112
At time: 76.17390251159668 and batch: 600, loss is 4.612667074203491 and perplexity is 100.75250602386893
At time: 77.3317985534668 and batch: 650, loss is 4.6142949295043945 and perplexity is 100.91665009002597
At time: 78.48961067199707 and batch: 700, loss is 4.594415655136109 and perplexity is 98.930309235974
At time: 79.64817118644714 and batch: 750, loss is 4.602705945968628 and perplexity is 99.75387937275016
At time: 80.8053195476532 and batch: 800, loss is 4.63821439743042 and perplexity is 103.35962350114951
At time: 81.96255207061768 and batch: 850, loss is 4.6706227207183835 and perplexity is 106.76420601924674
At time: 83.12097573280334 and batch: 900, loss is 4.618894309997558 and perplexity is 101.3818732108367
At time: 84.27905249595642 and batch: 950, loss is 4.611082715988159 and perplexity is 100.59300435049296
At time: 85.43803906440735 and batch: 1000, loss is 4.611118373870849 and perplexity is 100.59659134799352
At time: 86.59565138816833 and batch: 1050, loss is 4.586488647460937 and perplexity is 98.14918798273959
At time: 87.75797009468079 and batch: 1100, loss is 4.555346441268921 and perplexity is 95.13970984863415
At time: 88.91425561904907 and batch: 1150, loss is 4.560994367599488 and perplexity is 95.678572216014
At time: 90.07070708274841 and batch: 1200, loss is 4.563226613998413 and perplexity is 95.89238892129671
At time: 91.22838187217712 and batch: 1250, loss is 4.597700290679931 and perplexity is 99.25579350201362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.66148343051437 and perplexity of 105.79290223143357
Finished 3 epochs...
Completing Train Step...
At time: 94.11811876296997 and batch: 50, loss is 4.541668682098389 and perplexity is 93.84727079401613
At time: 95.28352355957031 and batch: 100, loss is 4.540805749893188 and perplexity is 93.76632189340766
At time: 96.44605445861816 and batch: 150, loss is 4.465881948471069 and perplexity is 86.9977232238262
At time: 97.60587477684021 and batch: 200, loss is 4.523526334762574 and perplexity is 92.16001270103058
At time: 98.76605343818665 and batch: 250, loss is 4.5288526058197025 and perplexity is 92.65219148455171
At time: 99.93471074104309 and batch: 300, loss is 4.528135414123535 and perplexity is 92.5857659249644
At time: 101.09552574157715 and batch: 350, loss is 4.50229853630066 and perplexity is 90.22427691915878
At time: 102.2573401927948 and batch: 400, loss is 4.5225052070617675 and perplexity is 92.06595359050952
At time: 103.41894745826721 and batch: 450, loss is 4.446685619354248 and perplexity is 85.34361350816593
At time: 104.58069610595703 and batch: 500, loss is 4.471846370697022 and perplexity is 87.51816490192286
At time: 105.74190402030945 and batch: 550, loss is 4.456211576461792 and perplexity is 86.16047763907675
At time: 106.90351223945618 and batch: 600, loss is 4.486727695465088 and perplexity is 88.8302900023068
At time: 108.0659339427948 and batch: 650, loss is 4.494984655380249 and perplexity is 89.56679460619526
At time: 109.2271876335144 and batch: 700, loss is 4.467228260040283 and perplexity is 87.11492814456471
At time: 110.38861060142517 and batch: 750, loss is 4.476723079681396 and perplexity is 87.94600790798829
At time: 111.55004978179932 and batch: 800, loss is 4.516348028182984 and perplexity is 91.50082861873993
At time: 112.71146273612976 and batch: 850, loss is 4.549667854309082 and perplexity is 94.60098178766276
At time: 113.87387681007385 and batch: 900, loss is 4.500494775772094 and perplexity is 90.06168061621496
At time: 115.03579330444336 and batch: 950, loss is 4.492408771514892 and perplexity is 89.3363778359225
At time: 116.19732880592346 and batch: 1000, loss is 4.494334650039673 and perplexity is 89.50859462856023
At time: 117.35948085784912 and batch: 1050, loss is 4.475916166305542 and perplexity is 87.87507172138214
At time: 118.52122354507446 and batch: 1100, loss is 4.440830926895142 and perplexity is 84.84541272699092
At time: 119.68301200866699 and batch: 1150, loss is 4.44818078994751 and perplexity is 85.47131221128929
At time: 120.8449776172638 and batch: 1200, loss is 4.451117210388183 and perplexity is 85.7226607714769
At time: 122.00729250907898 and batch: 1250, loss is 4.493598890304566 and perplexity is 89.4427620301439
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.644368638087363 and perplexity of 103.99768487615168
Finished 4 epochs...
Completing Train Step...
At time: 124.88307809829712 and batch: 50, loss is 4.430080270767212 and perplexity is 83.93815441990628
At time: 126.0636990070343 and batch: 100, loss is 4.4332807254791256 and perplexity is 84.20722502620158
At time: 127.22286057472229 and batch: 150, loss is 4.362657556533813 and perplexity is 78.46538377663767
At time: 128.38149976730347 and batch: 200, loss is 4.4202238464355466 and perplexity is 83.11488824764595
At time: 129.5400242805481 and batch: 250, loss is 4.429453907012939 and perplexity is 83.88559506473304
At time: 130.6985912322998 and batch: 300, loss is 4.4297691917419435 and perplexity is 83.91204708157976
At time: 131.85741567611694 and batch: 350, loss is 4.408167352676392 and perplexity is 82.11883066269252
At time: 133.01735401153564 and batch: 400, loss is 4.425956497192383 and perplexity is 83.59272520224593
At time: 134.17623233795166 and batch: 450, loss is 4.349986267089844 and perplexity is 77.47739892779639
At time: 135.3348536491394 and batch: 500, loss is 4.3883822154998775 and perplexity is 80.51006563081168
At time: 136.49331259727478 and batch: 550, loss is 4.366447620391845 and perplexity is 78.76333686574907
At time: 137.65181946754456 and batch: 600, loss is 4.395838375091553 and perplexity is 81.1126050524573
At time: 138.81062579154968 and batch: 650, loss is 4.41210205078125 and perplexity is 82.44257998030102
At time: 139.96889996528625 and batch: 700, loss is 4.383865995407104 and perplexity is 80.14728427158578
At time: 141.12760090827942 and batch: 750, loss is 4.390892705917358 and perplexity is 80.71243930137956
At time: 142.28650283813477 and batch: 800, loss is 4.427217416763305 and perplexity is 83.69819538617567
At time: 143.44541811943054 and batch: 850, loss is 4.470339937210083 and perplexity is 87.38642386206998
At time: 144.60351157188416 and batch: 900, loss is 4.41343276977539 and perplexity is 82.55236091499408
At time: 145.76218628883362 and batch: 950, loss is 4.403589591979981 and perplexity is 81.74376943250711
At time: 146.92093110084534 and batch: 1000, loss is 4.40573748588562 and perplexity is 81.9195350720944
At time: 148.0921232700348 and batch: 1050, loss is 4.394233837127685 and perplexity is 80.98256115638684
At time: 149.26267886161804 and batch: 1100, loss is 4.36141900062561 and perplexity is 78.36826017089537
At time: 150.42786502838135 and batch: 1150, loss is 4.370060844421387 and perplexity is 79.04844120991396
At time: 151.58735251426697 and batch: 1200, loss is 4.3777072143554685 and perplexity is 79.6551915968402
At time: 152.79387092590332 and batch: 1250, loss is 4.420988302230835 and perplexity is 83.17845019769675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.6421673628535585 and perplexity of 103.76900912952907
Finished 5 epochs...
Completing Train Step...
At time: 155.68842220306396 and batch: 50, loss is 4.349400453567505 and perplexity is 77.43202491148257
At time: 156.8534860610962 and batch: 100, loss is 4.356644868850708 and perplexity is 77.99501144809727
At time: 158.01244282722473 and batch: 150, loss is 4.28934531211853 and perplexity is 72.91871387084652
At time: 159.17141127586365 and batch: 200, loss is 4.341961431503296 and perplexity is 76.85814357415651
At time: 160.3311846256256 and batch: 250, loss is 4.352770471572876 and perplexity is 77.69341242255409
At time: 161.4902708530426 and batch: 300, loss is 4.357283220291138 and perplexity is 78.04481557057694
At time: 162.64992475509644 and batch: 350, loss is 4.332187738418579 and perplexity is 76.11061467744261
At time: 163.80950903892517 and batch: 400, loss is 4.359884929656983 and perplexity is 78.2481298658713
At time: 164.96914148330688 and batch: 450, loss is 4.276664381027222 and perplexity is 71.99987486491271
At time: 166.12848377227783 and batch: 500, loss is 4.314869203567505 and perplexity is 74.80383872881131
At time: 167.287855386734 and batch: 550, loss is 4.296444206237793 and perplexity is 73.43819779904615
At time: 168.44724369049072 and batch: 600, loss is 4.327014904022217 and perplexity is 75.71792361048236
At time: 169.6063847541809 and batch: 650, loss is 4.341985015869141 and perplexity is 76.85995624610803
At time: 170.77397537231445 and batch: 700, loss is 4.31961163520813 and perplexity is 75.15943334555705
At time: 171.93330001831055 and batch: 750, loss is 4.32056004524231 and perplexity is 75.23074911925735
At time: 173.09285402297974 and batch: 800, loss is 4.359350261688232 and perplexity is 78.2063042796144
At time: 174.25263810157776 and batch: 850, loss is 4.410741138458252 and perplexity is 82.33045916786844
At time: 175.41342186927795 and batch: 900, loss is 4.3478359031677245 and perplexity is 77.31097332629722
At time: 176.57309532165527 and batch: 950, loss is 4.336007251739502 and perplexity is 76.40187606835471
At time: 177.73331356048584 and batch: 1000, loss is 4.341923990249634 and perplexity is 76.85526596277799
At time: 178.89304614067078 and batch: 1050, loss is 4.3318834972381595 and perplexity is 76.08746221633533
At time: 180.0523099899292 and batch: 1100, loss is 4.296247301101684 and perplexity is 73.4237388642789
At time: 181.2124216556549 and batch: 1150, loss is 4.309869222640991 and perplexity is 74.43075444633807
At time: 182.39827847480774 and batch: 1200, loss is 4.308737602233887 and perplexity is 74.34657472441914
At time: 183.5576663017273 and batch: 1250, loss is 4.3648826694488525 and perplexity is 78.640172505675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.651527571852189 and perplexity of 104.74486874266479
Annealing...
Finished 6 epochs...
Completing Train Step...
At time: 186.42498302459717 and batch: 50, loss is 4.322247514724731 and perplexity is 75.35780588459274
At time: 187.61079239845276 and batch: 100, loss is 4.316815042495728 and perplexity is 74.94953665656894
At time: 188.7739839553833 and batch: 150, loss is 4.234441986083985 and perplexity is 69.02315210591836
At time: 189.9432497024536 and batch: 200, loss is 4.281017065048218 and perplexity is 72.31395061012033
At time: 191.10604286193848 and batch: 250, loss is 4.2858810091018675 and perplexity is 72.66653840872439
At time: 192.26970863342285 and batch: 300, loss is 4.270680379867554 and perplexity is 71.57031405822617
At time: 193.43207502365112 and batch: 350, loss is 4.2390173101425175 and perplexity is 69.33967894861354
At time: 194.59511256217957 and batch: 400, loss is 4.256360168457031 and perplexity is 70.55271553292191
At time: 195.75762915611267 and batch: 450, loss is 4.158925256729126 and perplexity is 64.00269915256084
At time: 196.92040991783142 and batch: 500, loss is 4.1746823120117185 and perplexity is 65.01918056694367
At time: 198.08379316329956 and batch: 550, loss is 4.146875858306885 and perplexity is 63.236132737115966
At time: 199.24712562561035 and batch: 600, loss is 4.165522103309631 and perplexity is 64.4263108540705
At time: 200.40959525108337 and batch: 650, loss is 4.172645354270935 and perplexity is 64.88687404093677
At time: 201.5722804069519 and batch: 700, loss is 4.1302232694625856 and perplexity is 62.19180691607598
At time: 202.7449951171875 and batch: 750, loss is 4.12445788860321 and perplexity is 61.83427909523962
At time: 203.9146933555603 and batch: 800, loss is 4.14438720703125 and perplexity is 63.078955715233015
At time: 205.08042120933533 and batch: 850, loss is 4.167909874916076 and perplexity is 64.58032997790349
At time: 206.24428415298462 and batch: 900, loss is 4.093372240066528 and perplexity is 59.941689023786665
At time: 207.40847539901733 and batch: 950, loss is 4.063949556350708 and perplexity is 58.203736653843286
At time: 208.57136416435242 and batch: 1000, loss is 4.043782458305359 and perplexity is 57.041693097776445
At time: 209.7345151901245 and batch: 1050, loss is 4.022681756019592 and perplexity is 55.850683077846945
At time: 210.89799308776855 and batch: 1100, loss is 3.9550770330429077 and perplexity is 52.19971489243383
At time: 212.0925669670105 and batch: 1150, loss is 3.9478059244155883 and perplexity is 51.821541630704864
At time: 213.25615096092224 and batch: 1200, loss is 3.9203396892547606 and perplexity is 50.417568175744194
At time: 214.418470621109 and batch: 1250, loss is 3.986492729187012 and perplexity is 53.86563630243097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.444954308280109 and perplexity of 85.19598499730564
Finished 7 epochs...
Completing Train Step...
At time: 217.34218049049377 and batch: 50, loss is 4.168532195091248 and perplexity is 64.62053212817511
At time: 218.5072102546692 and batch: 100, loss is 4.174144105911255 and perplexity is 64.98419626254291
At time: 219.66645693778992 and batch: 150, loss is 4.103687281608582 and perplexity is 60.56318992936176
At time: 220.82507133483887 and batch: 200, loss is 4.154071865081787 and perplexity is 63.692821573986635
At time: 221.98365807533264 and batch: 250, loss is 4.162033681869507 and perplexity is 64.20195627914947
At time: 223.14128494262695 and batch: 300, loss is 4.1599743461608885 and perplexity is 64.0698789404874
At time: 224.2994740009308 and batch: 350, loss is 4.132999186515808 and perplexity is 62.36468605152204
At time: 225.45727515220642 and batch: 400, loss is 4.154686737060547 and perplexity is 63.73199654778323
At time: 226.61609983444214 and batch: 450, loss is 4.065880236625671 and perplexity is 58.316218007956095
At time: 227.77568173408508 and batch: 500, loss is 4.086144018173218 and perplexity is 59.509979321257944
At time: 228.9358627796173 and batch: 550, loss is 4.059377570152282 and perplexity is 57.9382373651856
At time: 230.0954451560974 and batch: 600, loss is 4.084923596382141 and perplexity is 59.43739634563924
At time: 231.2576665878296 and batch: 650, loss is 4.0995094156265255 and perplexity is 60.31069285512834
At time: 232.41931223869324 and batch: 700, loss is 4.060404543876648 and perplexity is 57.99776897606356
At time: 233.58010339736938 and batch: 750, loss is 4.060568246841431 and perplexity is 58.00726415996941
At time: 234.74060559272766 and batch: 800, loss is 4.087532095909118 and perplexity is 59.592641155871355
At time: 235.9004225730896 and batch: 850, loss is 4.1167220735549925 and perplexity is 61.35778595613953
At time: 237.06110072135925 and batch: 900, loss is 4.046357102394104 and perplexity is 57.188744376844824
At time: 238.22171711921692 and batch: 950, loss is 4.026289463043213 and perplexity is 56.052539880623755
At time: 239.38135766983032 and batch: 1000, loss is 4.013418827056885 and perplexity is 55.33573083299261
At time: 240.5409917831421 and batch: 1050, loss is 4.00242434501648 and perplexity is 54.730675364740485
At time: 241.7272436618805 and batch: 1100, loss is 3.944649567604065 and perplexity is 51.658232221759825
At time: 242.8866090774536 and batch: 1150, loss is 3.9457342052459716 and perplexity is 51.71429308230567
At time: 244.04751682281494 and batch: 1200, loss is 3.932330641746521 and perplexity is 51.025761964596725
At time: 245.2073323726654 and batch: 1250, loss is 4.001805987358093 and perplexity is 54.69684269390362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.434604616930885 and perplexity of 84.31878007868693
Finished 8 epochs...
Completing Train Step...
At time: 248.07896614074707 and batch: 50, loss is 4.123555307388306 and perplexity is 61.77849381565094
At time: 249.2679681777954 and batch: 100, loss is 4.1255904483795165 and perplexity is 61.90434978467204
At time: 250.43052625656128 and batch: 150, loss is 4.053595919609069 and perplexity is 57.60422522481835
At time: 251.59373450279236 and batch: 200, loss is 4.1043053197860715 and perplexity is 60.60063186196876
At time: 252.75663447380066 and batch: 250, loss is 4.112723097801209 and perplexity is 61.11290761540586
At time: 253.92056274414062 and batch: 300, loss is 4.113859434127807 and perplexity is 61.18239190363686
At time: 255.08352327346802 and batch: 350, loss is 4.08717725276947 and perplexity is 59.57149886730353
At time: 256.2466473579407 and batch: 400, loss is 4.110714287757873 and perplexity is 60.99026661528826
At time: 257.40900206565857 and batch: 450, loss is 4.025977702140808 and perplexity is 56.03506761393481
At time: 258.5719413757324 and batch: 500, loss is 4.046846690177918 and perplexity is 57.21675014254434
At time: 259.7350523471832 and batch: 550, loss is 4.020944900512696 and perplexity is 55.753762704086405
At time: 260.8991742134094 and batch: 600, loss is 4.048044462203979 and perplexity is 57.285323824908126
At time: 262.06250405311584 and batch: 650, loss is 4.066112785339356 and perplexity is 58.32978094640163
At time: 263.22752618789673 and batch: 700, loss is 4.026819233894348 and perplexity is 56.08224274954698
At time: 264.391028881073 and batch: 750, loss is 4.0297194719314575 and perplexity is 56.24513069590851
At time: 265.5548310279846 and batch: 800, loss is 4.0605111503601075 and perplexity is 58.003952243844964
At time: 266.71724939346313 and batch: 850, loss is 4.092008819580078 and perplexity is 59.860018984932765
At time: 267.88915753364563 and batch: 900, loss is 4.023182287216186 and perplexity is 55.87864508442302
At time: 269.0532262325287 and batch: 950, loss is 4.007564744949341 and perplexity is 55.01273725908898
At time: 270.2546684741974 and batch: 1000, loss is 3.997843790054321 and perplexity is 54.480551787842046
At time: 271.41844177246094 and batch: 1050, loss is 3.9911622285842894 and perplexity is 54.11775002301712
At time: 272.5812437534332 and batch: 1100, loss is 3.938168225288391 and perplexity is 51.32450021926103
At time: 273.74502778053284 and batch: 1150, loss is 3.942565460205078 and perplexity is 51.550683028730695
At time: 274.9081001281738 and batch: 1200, loss is 3.933519916534424 and perplexity is 51.086481715912726
At time: 276.0763804912567 and batch: 1250, loss is 4.00218318939209 and perplexity is 54.71747834588125
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.430276912494297 and perplexity of 83.95466178652514
Finished 9 epochs...
Completing Train Step...
At time: 278.9759373664856 and batch: 50, loss is 4.091402802467346 and perplexity is 59.823753778837066
At time: 280.1770648956299 and batch: 100, loss is 4.092095680236817 and perplexity is 59.86521869131585
At time: 281.3363690376282 and batch: 150, loss is 4.020569648742676 and perplexity is 55.73284493090759
At time: 282.4962923526764 and batch: 200, loss is 4.070602140426636 and perplexity is 58.59223272405048
At time: 283.65549659729004 and batch: 250, loss is 4.07960207939148 and perplexity is 59.121939330478156
At time: 284.8147156238556 and batch: 300, loss is 4.0829148101806645 and perplexity is 59.31811916529461
At time: 285.97398686408997 and batch: 350, loss is 4.056871476173401 and perplexity is 57.79322048623959
At time: 287.13332509994507 and batch: 400, loss is 4.081315765380859 and perplexity is 59.22334263146575
At time: 288.30128169059753 and batch: 450, loss is 3.9979317665100096 and perplexity is 54.48534500453431
At time: 289.46071696281433 and batch: 500, loss is 4.019839243888855 and perplexity is 55.69215225332653
At time: 290.6356472969055 and batch: 550, loss is 3.9948005723953246 and perplexity is 54.31500763178003
At time: 291.7956078052521 and batch: 600, loss is 4.023174381256103 and perplexity is 55.878203311831804
At time: 292.95421838760376 and batch: 650, loss is 4.043049941062927 and perplexity is 56.99992437406765
At time: 294.11324095726013 and batch: 700, loss is 4.004116339683533 and perplexity is 54.823357762531934
At time: 295.27296686172485 and batch: 750, loss is 4.008509664535523 and perplexity is 55.0647444394466
At time: 296.432240486145 and batch: 800, loss is 4.040660729408264 and perplexity is 56.863902048206405
At time: 297.5924401283264 and batch: 850, loss is 4.073423318862915 and perplexity is 58.75776525615997
At time: 298.7510929107666 and batch: 900, loss is 4.005468521118164 and perplexity is 54.89753903104692
At time: 299.93612146377563 and batch: 950, loss is 3.993648772239685 and perplexity is 54.252483612036265
At time: 301.0957381725311 and batch: 1000, loss is 3.9850974321365356 and perplexity is 53.790530148858494
At time: 302.2558207511902 and batch: 1050, loss is 3.9820245122909546 and perplexity is 53.625489868906214
At time: 303.41677927970886 and batch: 1100, loss is 3.931013321876526 and perplexity is 50.958588968354405
At time: 304.5769340991974 and batch: 1150, loss is 3.935938024520874 and perplexity is 51.21016382342895
At time: 305.7361979484558 and batch: 1200, loss is 3.9309294509887693 and perplexity is 50.954315205483454
At time: 306.898362159729 and batch: 1250, loss is 3.9978242444992067 and perplexity is 54.47948694562091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.428675296532846 and perplexity of 83.82030628185048
Finished 10 epochs...
Completing Train Step...
At time: 309.7963225841522 and batch: 50, loss is 4.0648892211914065 and perplexity is 58.2584543629017
At time: 310.95854210853577 and batch: 100, loss is 4.06518063545227 and perplexity is 58.27543418127937
At time: 312.12090587615967 and batch: 150, loss is 3.9948561286926267 and perplexity is 54.31802525631523
At time: 313.2822308540344 and batch: 200, loss is 4.044517006874084 and perplexity is 57.08360838432129
At time: 314.44427514076233 and batch: 250, loss is 4.054106526374817 and perplexity is 57.63364584249778
At time: 315.61129665374756 and batch: 300, loss is 4.05869607925415 and perplexity is 57.89876643483492
At time: 316.7785005569458 and batch: 350, loss is 4.032941012382508 and perplexity is 56.42661883835426
At time: 317.9409439563751 and batch: 400, loss is 4.0586480617523195 and perplexity is 57.895986347458596
At time: 319.10325384140015 and batch: 450, loss is 3.9747162103652953 and perplexity is 53.23500721859866
At time: 320.2651023864746 and batch: 500, loss is 3.9996256971359254 and perplexity is 54.57771761340883
At time: 321.42676758766174 and batch: 550, loss is 3.9749401950836183 and perplexity is 53.24693238217279
At time: 322.59821248054504 and batch: 600, loss is 4.003453698158264 and perplexity is 54.78704156276368
At time: 323.76192235946655 and batch: 650, loss is 4.02452250957489 and perplexity is 55.953585100864785
At time: 324.9246656894684 and batch: 700, loss is 3.9864188623428345 and perplexity is 53.86165756481791
At time: 326.08696961402893 and batch: 750, loss is 3.9925053882598878 and perplexity is 54.19048764076432
At time: 327.25024151802063 and batch: 800, loss is 4.024733581542969 and perplexity is 55.965396580685386
At time: 328.41201639175415 and batch: 850, loss is 4.057986493110657 and perplexity is 57.85769684537645
At time: 329.5999369621277 and batch: 900, loss is 3.9917245531082153 and perplexity is 54.148190318895075
At time: 330.7626521587372 and batch: 950, loss is 3.981731791496277 and perplexity is 53.60979487013513
At time: 331.9250602722168 and batch: 1000, loss is 3.9735039329528807 and perplexity is 53.17051072351867
At time: 333.0868887901306 and batch: 1050, loss is 3.9725357484817505 and perplexity is 53.11905677319127
At time: 334.2489695549011 and batch: 1100, loss is 3.923237705230713 and perplexity is 50.56389101434047
At time: 335.41377210617065 and batch: 1150, loss is 3.9280661487579347 and perplexity is 50.80862627633873
At time: 336.5784330368042 and batch: 1200, loss is 3.9252034997940064 and perplexity is 50.663386998687386
At time: 337.7406861782074 and batch: 1250, loss is 3.992011003494263 and perplexity is 54.163703310660864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4276448270700275 and perplexity of 83.73397650360224
Finished 11 epochs...
Completing Train Step...
At time: 340.6156921386719 and batch: 50, loss is 4.042635760307312 and perplexity is 56.9763209906914
At time: 341.7979371547699 and batch: 100, loss is 4.043343596458435 and perplexity is 57.01666516729546
At time: 342.9581582546234 and batch: 150, loss is 3.9729478311538697 and perplexity is 53.14095072679645
At time: 344.11807441711426 and batch: 200, loss is 4.0232282161712645 and perplexity is 55.88121159114097
At time: 345.2785029411316 and batch: 250, loss is 4.033796310424805 and perplexity is 56.47490105988158
At time: 346.43850541114807 and batch: 300, loss is 4.039048066139221 and perplexity is 56.772273624793264
At time: 347.59748697280884 and batch: 350, loss is 4.013509168624878 and perplexity is 55.340730175502905
At time: 348.7576355934143 and batch: 400, loss is 4.040028529167175 and perplexity is 56.827964036828966
At time: 349.91718792915344 and batch: 450, loss is 3.956195259094238 and perplexity is 52.25811862169854
At time: 351.0795488357544 and batch: 500, loss is 3.982393183708191 and perplexity is 53.6452636990552
At time: 352.24036622047424 and batch: 550, loss is 3.9579388380050657 and perplexity is 52.34931425553542
At time: 353.4014744758606 and batch: 600, loss is 3.986965842247009 and perplexity is 53.891126867934965
At time: 354.56090784072876 and batch: 650, loss is 4.009232378005981 and perplexity is 55.1045548560297
At time: 355.721209526062 and batch: 700, loss is 3.9712006092071532 and perplexity is 53.04818275813266
At time: 356.88121724128723 and batch: 750, loss is 3.9781472969055174 and perplexity is 53.41797484471804
At time: 358.0826952457428 and batch: 800, loss is 4.010969705581665 and perplexity is 55.20037272814197
At time: 359.24310660362244 and batch: 850, loss is 4.044481143951416 and perplexity is 57.08156123599677
At time: 360.41150879859924 and batch: 900, loss is 3.979143280982971 and perplexity is 53.471204800807286
At time: 361.5725359916687 and batch: 950, loss is 3.9707469606399535 and perplexity is 53.02412298378558
At time: 362.73339200019836 and batch: 1000, loss is 3.9627466440200805 and perplexity is 52.601605601444284
At time: 363.89337396621704 and batch: 1050, loss is 3.9637260818481446 and perplexity is 52.65315084234604
At time: 365.05328464508057 and batch: 1100, loss is 3.9157534885406493 and perplexity is 50.18687250126263
At time: 366.2143669128418 and batch: 1150, loss is 3.920291500091553 and perplexity is 50.41513865386162
At time: 367.37469816207886 and batch: 1200, loss is 3.9194297981262207 and perplexity is 50.371714541809425
At time: 368.5341167449951 and batch: 1250, loss is 3.985051574707031 and perplexity is 53.78806350997148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.427402468493385 and perplexity of 83.71368531521132
Finished 12 epochs...
Completing Train Step...
At time: 371.42457604408264 and batch: 50, loss is 4.023345284461975 and perplexity is 55.887753892005286
At time: 372.5842695236206 and batch: 100, loss is 4.024149599075318 and perplexity is 55.932723311523866
At time: 373.74457335472107 and batch: 150, loss is 3.95451931476593 and perplexity is 52.17061027422848
At time: 374.90457224845886 and batch: 200, loss is 4.004649848937988 and perplexity is 54.852614334890674
At time: 376.0635771751404 and batch: 250, loss is 4.015704274177551 and perplexity is 55.4623423465439
At time: 377.22766757011414 and batch: 300, loss is 4.021769366264343 and perplexity is 55.79974872630671
At time: 378.38992977142334 and batch: 350, loss is 3.996591944694519 and perplexity is 54.41239323277346
At time: 379.54895853996277 and batch: 400, loss is 4.023895783424377 and perplexity is 55.91852851245497
At time: 380.70908784866333 and batch: 450, loss is 3.9389732265472412 and perplexity is 51.36583314084227
At time: 381.8692696094513 and batch: 500, loss is 3.9671398782730103 and perplexity is 52.83320513994548
At time: 383.02864170074463 and batch: 550, loss is 3.9430470180511477 and perplexity is 51.575513642822095
At time: 384.1879298686981 and batch: 600, loss is 3.972680516242981 and perplexity is 53.1267472567726
At time: 385.34681224823 and batch: 650, loss is 3.9956820487976072 and perplexity is 54.36290613689679
At time: 386.505490064621 and batch: 700, loss is 3.957600960731506 and perplexity is 52.33162959975192
At time: 387.693482875824 and batch: 750, loss is 3.9649625205993653 and perplexity is 52.71829350257705
At time: 388.8525116443634 and batch: 800, loss is 3.9989350986480714 and perplexity is 54.54003933593269
At time: 390.01262617111206 and batch: 850, loss is 4.032362051010132 and perplexity is 56.393959460844385
At time: 391.17172265052795 and batch: 900, loss is 3.9679961681365965 and perplexity is 52.878465053004476
At time: 392.3342356681824 and batch: 950, loss is 3.9608235931396485 and perplexity is 52.500547238810775
At time: 393.51252603530884 and batch: 1000, loss is 3.9525393199920655 and perplexity is 52.06741493536573
At time: 394.68584418296814 and batch: 1050, loss is 3.9552166080474853 and perplexity is 52.207001176358624
At time: 395.86438179016113 and batch: 1100, loss is 3.9080661821365354 and perplexity is 49.802549730945366
At time: 397.0359535217285 and batch: 1150, loss is 3.912105875015259 and perplexity is 50.00414365104459
At time: 398.20457100868225 and batch: 1200, loss is 3.9126666498184206 and perplexity is 50.03219257868868
At time: 399.3708770275116 and batch: 1250, loss is 3.9773885297775267 and perplexity is 53.37745841457357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.427470631843065 and perplexity of 83.71939171489792
Annealing...
Finished 13 epochs...
Completing Train Step...
At time: 402.24982810020447 and batch: 50, loss is 4.023671507835388 and perplexity is 55.90598875777043
At time: 403.43948674201965 and batch: 100, loss is 4.037617478370667 and perplexity is 56.69111397139797
At time: 404.60363125801086 and batch: 150, loss is 3.970900659561157 and perplexity is 53.03227336062198
At time: 405.7793483734131 and batch: 200, loss is 4.016914935111999 and perplexity is 55.52952909973824
At time: 406.9444124698639 and batch: 250, loss is 4.030290508270264 and perplexity is 56.277257881438985
At time: 408.10728573799133 and batch: 300, loss is 4.030976371765137 and perplexity is 56.315869637894906
At time: 409.2728753089905 and batch: 350, loss is 4.004316086769104 and perplexity is 54.83430966223485
At time: 410.4358949661255 and batch: 400, loss is 4.027177467346191 and perplexity is 56.10233688393492
At time: 411.5988781452179 and batch: 450, loss is 3.945579514503479 and perplexity is 51.70629397862088
At time: 412.76166248321533 and batch: 500, loss is 3.960911612510681 and perplexity is 52.5051685073352
At time: 413.92535948753357 and batch: 550, loss is 3.932662801742554 and perplexity is 51.042713496643
At time: 415.0880193710327 and batch: 600, loss is 3.956472644805908 and perplexity is 52.2726162877527
At time: 416.25106406211853 and batch: 650, loss is 3.980938057899475 and perplexity is 53.56725985779636
At time: 417.4562771320343 and batch: 700, loss is 3.9409818506240843 and perplexity is 51.46911147897072
At time: 418.6191689968109 and batch: 750, loss is 3.9455043649673462 and perplexity is 51.70240842061401
At time: 419.78178095817566 and batch: 800, loss is 3.9758612155914306 and perplexity is 53.29599648993124
At time: 420.94451332092285 and batch: 850, loss is 3.99804895401001 and perplexity is 54.491730380037836
At time: 422.1075279712677 and batch: 900, loss is 3.925754599571228 and perplexity is 50.69131527490222
At time: 423.27154898643494 and batch: 950, loss is 3.9160843896865845 and perplexity is 50.203482142807346
At time: 424.4351074695587 and batch: 1000, loss is 3.899579405784607 and perplexity is 49.38167509023171
At time: 425.59876108169556 and batch: 1050, loss is 3.898795804977417 and perplexity is 49.34299472673189
At time: 426.76193714141846 and batch: 1100, loss is 3.8438950395584106 and perplexity is 46.707046388610365
At time: 427.92552423477173 and batch: 1150, loss is 3.842414436340332 and perplexity is 46.637942955427185
At time: 429.0892436504364 and batch: 1200, loss is 3.8459924602508546 and perplexity is 46.80511352225527
At time: 430.25275325775146 and batch: 1250, loss is 3.9132443714141845 and perplexity is 50.06110560786131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.407108501796305 and perplexity of 82.0319250847171
Finished 14 epochs...
Completing Train Step...
At time: 433.12626791000366 and batch: 50, loss is 4.00916778087616 and perplexity is 55.100995374913296
At time: 434.31103467941284 and batch: 100, loss is 4.014978437423706 and perplexity is 55.422100346342596
At time: 435.47078251838684 and batch: 150, loss is 3.9482252311706545 and perplexity is 51.84327530938928
At time: 436.6306540966034 and batch: 200, loss is 3.994582805633545 and perplexity is 54.30318091623153
At time: 437.79084873199463 and batch: 250, loss is 4.008046245574951 and perplexity is 55.03923230467438
At time: 438.9520471096039 and batch: 300, loss is 4.011380052566528 and perplexity is 55.22302868273768
At time: 440.11136507987976 and batch: 350, loss is 3.9863522243499756 and perplexity is 53.85806845165271
At time: 441.2711741924286 and batch: 400, loss is 4.0108186149597165 and perplexity is 55.19203309953037
At time: 442.43096828460693 and batch: 450, loss is 3.930214910507202 and perplexity is 50.91791928928489
At time: 443.59030318260193 and batch: 500, loss is 3.9468299102783204 and perplexity is 51.77098774812348
At time: 444.75019812583923 and batch: 550, loss is 3.918990306854248 and perplexity is 50.34958147691468
At time: 445.90940260887146 and batch: 600, loss is 3.9438443994522094 and perplexity is 51.61665539880672
At time: 447.0944993495941 and batch: 650, loss is 3.9693522548675535 and perplexity is 52.950221480789516
At time: 448.25642108917236 and batch: 700, loss is 3.930644087791443 and perplexity is 50.93977679364135
At time: 449.41592955589294 and batch: 750, loss is 3.936374878883362 and perplexity is 51.232540094128815
At time: 450.57731914520264 and batch: 800, loss is 3.9683638429641723 and perplexity is 52.8979107081452
At time: 451.7378137111664 and batch: 850, loss is 3.9920181322097776 and perplexity is 54.16408942966926
At time: 452.89674186706543 and batch: 900, loss is 3.921757140159607 and perplexity is 50.48908327597301
At time: 454.0557544231415 and batch: 950, loss is 3.9134497308731078 and perplexity is 50.071387185095475
At time: 455.21503353118896 and batch: 1000, loss is 3.898666596412659 and perplexity is 49.33661960107166
At time: 456.37493228912354 and batch: 1050, loss is 3.9001896667480467 and perplexity is 49.41181999604263
At time: 457.53453826904297 and batch: 1100, loss is 3.847220735549927 and perplexity is 46.86263840803094
At time: 458.6941246986389 and batch: 1150, loss is 3.8480298376083373 and perplexity is 46.90057040858995
At time: 459.8539524078369 and batch: 1200, loss is 3.85367027759552 and perplexity is 47.165857726609595
At time: 461.01326179504395 and batch: 1250, loss is 3.9204419088363647 and perplexity is 50.42272210188021
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4056360843407845 and perplexity of 81.91122872583118
Finished 15 epochs...
Completing Train Step...
At time: 463.9067711830139 and batch: 50, loss is 4.0029376077651975 and perplexity is 54.7587737919385
At time: 465.07050800323486 and batch: 100, loss is 4.00788230419159 and perplexity is 55.03020983638938
At time: 466.23368644714355 and batch: 150, loss is 3.940464262962341 and perplexity is 51.44247859492898
At time: 467.411337852478 and batch: 200, loss is 3.986329460144043 and perplexity is 53.856842429446104
At time: 468.58566331863403 and batch: 250, loss is 3.9989598751068116 and perplexity is 54.541390661707446
At time: 469.7504153251648 and batch: 300, loss is 4.0035419225692745 and perplexity is 54.791875330461586
At time: 470.91376209259033 and batch: 350, loss is 3.978694701194763 and perplexity is 53.44722407811839
At time: 472.07682394981384 and batch: 400, loss is 4.00375795841217 and perplexity is 54.80371361813362
At time: 473.24063324928284 and batch: 450, loss is 3.92315176486969 and perplexity is 50.55954572201266
At time: 474.40543508529663 and batch: 500, loss is 3.9403361797332765 and perplexity is 51.435890098106434
At time: 475.61453890800476 and batch: 550, loss is 3.912681193351746 and perplexity is 50.03292022884007
At time: 476.7776050567627 and batch: 600, loss is 3.938119874000549 and perplexity is 51.32201867357104
At time: 477.94111156463623 and batch: 650, loss is 3.96416880607605 and perplexity is 52.67646682879471
At time: 479.1051826477051 and batch: 700, loss is 3.925925874710083 and perplexity is 50.699998180526364
At time: 480.2689137458801 and batch: 750, loss is 3.9324510717391967 and perplexity is 51.03190736677433
At time: 481.44104981422424 and batch: 800, loss is 3.9655673694610596 and perplexity is 52.750189747623885
At time: 482.6071138381958 and batch: 850, loss is 3.989529685974121 and perplexity is 54.02947256809784
At time: 483.7713553905487 and batch: 900, loss is 3.9200368690490723 and perplexity is 50.402303028793035
At time: 484.93403792381287 and batch: 950, loss is 3.9124947929382325 and perplexity is 50.02359494096589
At time: 486.09783387184143 and batch: 1000, loss is 3.898579077720642 and perplexity is 49.33230191359759
At time: 487.2618055343628 and batch: 1050, loss is 3.9014936590194704 and perplexity is 49.476294655527255
At time: 488.42530274391174 and batch: 1100, loss is 3.8492136573791504 and perplexity is 46.95612510798981
At time: 489.58821058273315 and batch: 1150, loss is 3.8511931800842287 and perplexity is 47.049167883544534
At time: 490.7515823841095 and batch: 1200, loss is 3.857620620727539 and perplexity is 47.35254755039953
At time: 491.91441655158997 and batch: 1250, loss is 3.9234801816940306 and perplexity is 50.57615305437319
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.405121517877509 and perplexity of 81.8690907968755
Finished 16 epochs...
Completing Train Step...
At time: 494.7866461277008 and batch: 50, loss is 3.9979357814788816 and perplexity is 54.485563761937634
At time: 495.96866631507874 and batch: 100, loss is 4.002584404945374 and perplexity is 54.73943625386263
At time: 497.1274974346161 and batch: 150, loss is 3.934818048477173 and perplexity is 51.152841772406475
At time: 498.2868981361389 and batch: 200, loss is 3.9801948642730713 and perplexity is 53.52746380160552
At time: 499.4465868473053 and batch: 250, loss is 3.992919292449951 and perplexity is 54.21292195316808
At time: 500.60654854774475 and batch: 300, loss is 3.998113741874695 and perplexity is 54.495260897258255
At time: 501.76555013656616 and batch: 350, loss is 3.97329954624176 and perplexity is 53.15964448819816
At time: 502.9257707595825 and batch: 400, loss is 3.9987273359298707 and perplexity is 54.52870912614783
At time: 504.0857763290405 and batch: 450, loss is 3.9180950355529784 and perplexity is 50.30452511343147
At time: 505.2712678909302 and batch: 500, loss is 3.9356741189956663 and perplexity is 51.196650961387064
At time: 506.4312856197357 and batch: 550, loss is 3.9082749271392823 and perplexity is 49.81294684946126
At time: 507.59209871292114 and batch: 600, loss is 3.9342000913619994 and perplexity is 51.121241274753665
At time: 508.75231409072876 and batch: 650, loss is 3.960650849342346 and perplexity is 52.49147887819436
At time: 509.9124381542206 and batch: 700, loss is 3.92278293132782 and perplexity is 50.54090110428022
At time: 511.0721981525421 and batch: 750, loss is 3.9298486042022707 and perplexity is 50.899271150089234
At time: 512.2316932678223 and batch: 800, loss is 3.963383688926697 and perplexity is 52.635125862194606
At time: 513.3916985988617 and batch: 850, loss is 3.987792525291443 and perplexity is 53.935696168566444
At time: 514.5526278018951 and batch: 900, loss is 3.918820505142212 and perplexity is 50.341032757593716
At time: 515.7120161056519 and batch: 950, loss is 3.9117023801803588 and perplexity is 49.98397130734973
At time: 516.8714394569397 and batch: 1000, loss is 3.898352041244507 and perplexity is 49.32110295294588
At time: 518.031257390976 and batch: 1050, loss is 3.902214512825012 and perplexity is 49.5119726885918
At time: 519.191255569458 and batch: 1100, loss is 3.8503159189224245 and perplexity is 47.00791157478963
At time: 520.3513567447662 and batch: 1150, loss is 3.853047823905945 and perplexity is 47.136508299722834
At time: 521.5119104385376 and batch: 1200, loss is 3.8599074363708494 and perplexity is 47.46095800700781
At time: 522.6718266010284 and batch: 1250, loss is 3.924964690208435 and perplexity is 50.651289540786536
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404828816434763 and perplexity of 81.84513110257251
Finished 17 epochs...
Completing Train Step...
At time: 525.5661196708679 and batch: 50, loss is 3.9935943126678466 and perplexity is 54.24952912545835
At time: 526.7291369438171 and batch: 100, loss is 3.9980852222442627 and perplexity is 54.493706734719325
At time: 527.8923389911652 and batch: 150, loss is 3.9301548337936403 and perplexity is 50.914860399917515
At time: 529.0547404289246 and batch: 200, loss is 3.9750844192504884 and perplexity is 53.25461243044496
At time: 530.2165441513062 and batch: 250, loss is 3.988234519958496 and perplexity is 53.959540727832575
At time: 531.3784708976746 and batch: 300, loss is 3.9938109350204467 and perplexity is 54.261282059013055
At time: 532.5403583049774 and batch: 350, loss is 3.9689642000198364 and perplexity is 52.92967787693653
At time: 533.7024855613708 and batch: 400, loss is 3.994704341888428 and perplexity is 54.30978112254237
At time: 534.895872592926 and batch: 450, loss is 3.9135841798782347 and perplexity is 50.07811968586671
At time: 536.0584197044373 and batch: 500, loss is 3.9319907665252685 and perplexity is 51.008422519250246
At time: 537.2209982872009 and batch: 550, loss is 3.904758710861206 and perplexity is 49.6381013323628
At time: 538.383225440979 and batch: 600, loss is 3.93114501953125 and perplexity is 50.965300536948796
At time: 539.5453591346741 and batch: 650, loss is 3.95793336391449 and perplexity is 52.349027691431935
At time: 540.7075171470642 and batch: 700, loss is 3.92024845123291 and perplexity is 50.41296838639836
At time: 541.8701221942902 and batch: 750, loss is 3.9277206325531004 and perplexity is 50.791074105069306
At time: 543.0340468883514 and batch: 800, loss is 3.9616116809844972 and perplexity is 52.5419385898056
At time: 544.1959705352783 and batch: 850, loss is 3.9863087701797486 and perplexity is 53.855728144826536
At time: 545.3582406044006 and batch: 900, loss is 3.9175774335861204 and perplexity is 50.27849412971625
At time: 546.5205886363983 and batch: 950, loss is 3.910852479934692 and perplexity is 49.94150796521472
At time: 547.6830441951752 and batch: 1000, loss is 3.8979467487335206 and perplexity is 49.30111752953025
At time: 548.8454253673553 and batch: 1050, loss is 3.902484450340271 and perplexity is 49.52533963151344
At time: 550.0083663463593 and batch: 1100, loss is 3.850894546508789 and perplexity is 47.03511952007892
At time: 551.1701674461365 and batch: 1150, loss is 3.8541551637649536 and perplexity is 47.18873334427781
At time: 552.3319680690765 and batch: 1200, loss is 3.861276559829712 and perplexity is 47.5259824210633
At time: 553.494303226471 and batch: 1250, loss is 3.9255292415618896 and perplexity is 50.679892868114884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404690707687044 and perplexity of 81.83382835453119
Finished 18 epochs...
Completing Train Step...
At time: 556.3842315673828 and batch: 50, loss is 3.9896909284591673 and perplexity is 54.038185116918086
At time: 557.5704426765442 and batch: 100, loss is 3.9941126346588134 and perplexity is 54.27765513793938
At time: 558.730268239975 and batch: 150, loss is 3.926105761528015 and perplexity is 50.70911926222481
At time: 559.889922618866 and batch: 200, loss is 3.9707501554489135 and perplexity is 53.02429238599939
At time: 561.0493097305298 and batch: 250, loss is 3.9842446517944334 and perplexity is 53.74467819575718
At time: 562.2098960876465 and batch: 300, loss is 3.9901095390319825 and perplexity is 54.0608108078901
At time: 563.3699791431427 and batch: 350, loss is 3.9652690935134887 and perplexity is 52.73445798111278
At time: 564.5562963485718 and batch: 400, loss is 3.9912341690063475 and perplexity is 54.121643416839134
At time: 565.7155268192291 and batch: 450, loss is 3.9100714492797852 and perplexity is 49.9025173449585
At time: 566.8752274513245 and batch: 500, loss is 3.928732919692993 and perplexity is 50.84251528843863
At time: 568.0367133617401 and batch: 550, loss is 3.9017597436904907 and perplexity is 49.48946129075619
At time: 569.197511434555 and batch: 600, loss is 3.928518295288086 and perplexity is 50.831604414762644
At time: 570.3666000366211 and batch: 650, loss is 3.955586647987366 and perplexity is 52.22632342671696
At time: 571.5366792678833 and batch: 700, loss is 3.9179693460464478 and perplexity is 50.298202759828776
At time: 572.6961696147919 and batch: 750, loss is 3.925867533683777 and perplexity is 50.697040376880295
At time: 573.8560717105865 and batch: 800, loss is 3.959955711364746 and perplexity is 52.45500273717002
At time: 575.0152432918549 and batch: 850, loss is 3.984883117675781 and perplexity is 53.77900329562573
At time: 576.1756534576416 and batch: 900, loss is 3.916339602470398 and perplexity is 50.21629634834712
At time: 577.3354651927948 and batch: 950, loss is 3.9099204921722412 and perplexity is 49.894984773842815
At time: 578.4952855110168 and batch: 1000, loss is 3.8973759698867796 and perplexity is 49.272985523864335
At time: 579.654093503952 and batch: 1050, loss is 3.9024804544448854 and perplexity is 49.525141733832726
At time: 580.8139209747314 and batch: 1100, loss is 3.8511213445663453 and perplexity is 47.04578820359561
At time: 581.973081111908 and batch: 1150, loss is 3.8547633028030397 and perplexity is 47.217439382928966
At time: 583.1328914165497 and batch: 1200, loss is 3.8620534706115723 and perplexity is 47.56292021605237
At time: 584.2928771972656 and batch: 1250, loss is 3.9256789302825927 and perplexity is 50.687479644256925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404631900091241 and perplexity of 81.82901604533187
Finished 19 epochs...
Completing Train Step...
At time: 587.1608080863953 and batch: 50, loss is 3.986077308654785 and perplexity is 53.84326405839599
At time: 588.3462946414948 and batch: 100, loss is 3.9905155372619627 and perplexity is 54.0827638575375
At time: 589.5057454109192 and batch: 150, loss is 3.922528429031372 and perplexity is 50.52803996554872
At time: 590.6652920246124 and batch: 200, loss is 3.966950011253357 and perplexity is 52.82317480899149
At time: 591.8252446651459 and batch: 250, loss is 3.980729866027832 and perplexity is 53.556108750532985
At time: 593.0111138820648 and batch: 300, loss is 3.986888761520386 and perplexity is 53.88697306080873
At time: 594.1707246303558 and batch: 350, loss is 3.9620569562911987 and perplexity is 52.56533942714674
At time: 595.3298704624176 and batch: 400, loss is 3.988167304992676 and perplexity is 53.955913961034746
At time: 596.4892370700836 and batch: 450, loss is 3.906965022087097 and perplexity is 49.74773933586853
At time: 597.6482706069946 and batch: 500, loss is 3.9258123731613157 and perplexity is 50.694243978771965
At time: 598.8082776069641 and batch: 550, loss is 3.8990791130065916 and perplexity is 49.35697597372774
At time: 599.9678380489349 and batch: 600, loss is 3.9261735248565675 and perplexity is 50.71255559736142
At time: 601.1272361278534 and batch: 650, loss is 3.9534780168533326 and perplexity is 52.11631340116745
At time: 602.2912812232971 and batch: 700, loss is 3.915895895957947 and perplexity is 50.19401999307362
At time: 603.4564199447632 and batch: 750, loss is 3.9241101932525635 and perplexity is 50.60802665469762
At time: 604.6164124011993 and batch: 800, loss is 3.958327431678772 and perplexity is 52.369660820895234
At time: 605.7765235900879 and batch: 850, loss is 3.9834634923934935 and perplexity is 53.70271142863545
At time: 606.9358999729156 and batch: 900, loss is 3.9151149129867555 and perplexity is 50.15483462174896
At time: 608.0950956344604 and batch: 950, loss is 3.90892041683197 and perplexity is 49.845110972903285
At time: 609.2549800872803 and batch: 1000, loss is 3.896706809997559 and perplexity is 49.24002504747388
At time: 610.414558172226 and batch: 1050, loss is 3.902268843650818 and perplexity is 49.51466278803225
At time: 611.5744359493256 and batch: 1100, loss is 3.8510425233840944 and perplexity is 47.04208014508818
At time: 612.7354378700256 and batch: 1150, loss is 3.8550384187698366 and perplexity is 47.2304314414939
At time: 613.894490480423 and batch: 1200, loss is 3.862463369369507 and perplexity is 47.58242019420806
At time: 615.0555305480957 and batch: 1250, loss is 3.9255628871917723 and perplexity is 50.68159805371868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404586012346031 and perplexity of 81.82526118244456
Finished 20 epochs...
Completing Train Step...
At time: 617.9765477180481 and batch: 50, loss is 3.982667908668518 and perplexity is 53.66000341658811
At time: 619.1408801078796 and batch: 100, loss is 3.987176551818848 and perplexity is 53.902483440630476
At time: 620.3061032295227 and batch: 150, loss is 3.919310202598572 and perplexity is 50.36569067025145
At time: 621.479513168335 and batch: 200, loss is 3.9635082721710204 and perplexity is 52.64168372543135
At time: 622.6830675601959 and batch: 250, loss is 3.9775169897079468 and perplexity is 53.38431571960254
At time: 623.8464169502258 and batch: 300, loss is 3.983859968185425 and perplexity is 53.72400747508246
At time: 625.0092358589172 and batch: 350, loss is 3.9591054487228394 and perplexity is 52.41042116367056
At time: 626.1730477809906 and batch: 400, loss is 3.9853852653503417 and perplexity is 53.80601507845519
At time: 627.3415739536285 and batch: 450, loss is 3.904072632789612 and perplexity is 49.604057399256476
At time: 628.5050420761108 and batch: 500, loss is 3.923099341392517 and perplexity is 50.556895284294804
At time: 629.6682243347168 and batch: 550, loss is 3.8966248846054077 and perplexity is 49.23599120435168
At time: 630.8316249847412 and batch: 600, loss is 3.924026665687561 and perplexity is 50.60379966599907
At time: 631.9948370456696 and batch: 650, loss is 3.9514997005462646 and perplexity is 52.013312766012845
At time: 633.1591107845306 and batch: 700, loss is 3.9139899587631226 and perplexity is 50.09844445283188
At time: 634.3227322101593 and batch: 750, loss is 3.9224649620056153 and perplexity is 50.52483320289772
At time: 635.4858391284943 and batch: 800, loss is 3.9566574239730836 and perplexity is 52.28227607069228
At time: 636.6488811969757 and batch: 850, loss is 3.982062368392944 and perplexity is 53.62751995934534
At time: 637.8122355937958 and batch: 900, loss is 3.9138897037506104 and perplexity is 50.09342208441943
At time: 638.9758768081665 and batch: 950, loss is 3.907943396568298 and perplexity is 49.7964350719806
At time: 640.1393139362335 and batch: 1000, loss is 3.895896944999695 and perplexity is 49.20016341814188
At time: 641.30291223526 and batch: 1050, loss is 3.901877021789551 and perplexity is 49.49526566105599
At time: 642.4664475917816 and batch: 1100, loss is 3.8507810544967653 and perplexity is 47.02978171263379
At time: 643.6293518543243 and batch: 1150, loss is 3.855078058242798 and perplexity is 47.23230366801077
At time: 644.7928285598755 and batch: 1200, loss is 3.8626261711120606 and perplexity is 47.59016732573674
At time: 645.956485748291 and batch: 1250, loss is 3.9251884269714354 and perplexity is 50.66262336419939
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404553935475593 and perplexity of 81.82263652623867
Finished 21 epochs...
Completing Train Step...
At time: 648.8297894001007 and batch: 50, loss is 3.979412627220154 and perplexity is 53.48560900839049
At time: 650.0267164707184 and batch: 100, loss is 3.984041900634766 and perplexity is 53.73378250452137
At time: 651.1860039234161 and batch: 150, loss is 3.9162612104415895 and perplexity is 50.21235994529046
At time: 652.3712723255157 and batch: 200, loss is 3.9603102254867553 and perplexity is 52.4736020730792
At time: 653.5306506156921 and batch: 250, loss is 3.9745099401474 and perplexity is 53.22402755448779
At time: 654.6907341480255 and batch: 300, loss is 3.9810608434677124 and perplexity is 53.573837548051365
At time: 655.8544564247131 and batch: 350, loss is 3.9563770484924317 and perplexity is 52.267619457182924
At time: 657.0207667350769 and batch: 400, loss is 3.9827891397476196 and perplexity is 53.66650907104269
At time: 658.1803019046783 and batch: 450, loss is 3.901220927238464 and perplexity is 49.462802737489305
At time: 659.3419134616852 and batch: 500, loss is 3.9205829238891603 and perplexity is 50.42983296605717
At time: 660.5016965866089 and batch: 550, loss is 3.894349765777588 and perplexity is 49.12410080399815
At time: 661.6617798805237 and batch: 600, loss is 3.922028274536133 and perplexity is 50.5027744580796
At time: 662.822122335434 and batch: 650, loss is 3.949653105735779 and perplexity is 51.91735387844247
At time: 663.9808127880096 and batch: 700, loss is 3.9122143507003786 and perplexity is 50.00956817899534
At time: 665.1399393081665 and batch: 750, loss is 3.9209701585769654 and perplexity is 50.4493649281644
At time: 666.3078298568726 and batch: 800, loss is 3.9552647924423217 and perplexity is 52.209516799722934
At time: 667.4680421352386 and batch: 850, loss is 3.9806481170654298 and perplexity is 53.551730773162326
At time: 668.628075838089 and batch: 900, loss is 3.9126418018341065 and perplexity is 50.03094939499765
At time: 669.7907390594482 and batch: 950, loss is 3.9068966007232664 and perplexity is 49.744335644139625
At time: 670.9498920440674 and batch: 1000, loss is 3.895011625289917 and perplexity is 49.15662481937707
At time: 672.1097209453583 and batch: 1050, loss is 3.901381006240845 and perplexity is 49.47072132738956
At time: 673.2694237232208 and batch: 1100, loss is 3.8503518438339235 and perplexity is 47.00960036018726
At time: 674.4286177158356 and batch: 1150, loss is 3.8549622535705566 and perplexity is 47.22683426326272
At time: 675.5888891220093 and batch: 1200, loss is 3.8625921964645387 and perplexity is 47.58855049404214
At time: 676.748293876648 and batch: 1250, loss is 3.9247066164016724 and perplexity is 50.638219456273085
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404546807282163 and perplexity of 81.82205328073728
Finished 22 epochs...
Completing Train Step...
At time: 679.6698327064514 and batch: 50, loss is 3.976302328109741 and perplexity is 53.31951120709555
At time: 680.8366408348083 and batch: 100, loss is 3.9810713291168214 and perplexity is 53.57439930745851
At time: 682.0320274829865 and batch: 150, loss is 3.913447618484497 and perplexity is 50.071281414979175
At time: 683.2141582965851 and batch: 200, loss is 3.9573394441604615 and perplexity is 52.3179458007699
At time: 684.3769280910492 and batch: 250, loss is 3.9716282272338868 and perplexity is 53.070871968177364
At time: 685.5392782688141 and batch: 300, loss is 3.97842068195343 and perplexity is 53.43258051672556
At time: 686.7022836208344 and batch: 350, loss is 3.9537850046157836 and perplexity is 52.13231492761641
At time: 687.8682782649994 and batch: 400, loss is 3.980320110321045 and perplexity is 53.53416832475348
At time: 689.0324487686157 and batch: 450, loss is 3.89861656665802 and perplexity is 49.334151363841485
At time: 690.1964838504791 and batch: 500, loss is 3.9181779718399046 and perplexity is 50.308697356972765
At time: 691.3589570522308 and batch: 550, loss is 3.8922406911849974 and perplexity is 49.02060359114424
At time: 692.5220391750336 and batch: 600, loss is 3.920133318901062 and perplexity is 50.40716455790321
At time: 693.685040473938 and batch: 650, loss is 3.9478520154953003 and perplexity is 51.82393019655633
At time: 694.84796667099 and batch: 700, loss is 3.9104925775527954 and perplexity is 49.923537131609415
At time: 696.0113191604614 and batch: 750, loss is 3.9194323110580442 and perplexity is 50.371841122652945
At time: 697.1752400398254 and batch: 800, loss is 3.9538649129867554 and perplexity is 52.13648090242317
At time: 698.3386130332947 and batch: 850, loss is 3.979274506568909 and perplexity is 53.478222051399406
At time: 699.5015399456024 and batch: 900, loss is 3.911376852989197 and perplexity is 49.96770281362912
At time: 700.6645655632019 and batch: 950, loss is 3.9058092164993288 and perplexity is 49.690273836635136
At time: 701.8283321857452 and batch: 1000, loss is 3.8940657329559327 and perplexity is 49.110149928382576
At time: 702.993424654007 and batch: 1050, loss is 3.9007780504226686 and perplexity is 49.44090165901053
At time: 704.1576862335205 and batch: 1100, loss is 3.8497670793533327 and perplexity is 46.982118851538026
At time: 705.320734500885 and batch: 1150, loss is 3.854704875946045 and perplexity is 47.21468069694194
At time: 706.483496427536 and batch: 1200, loss is 3.862377042770386 and perplexity is 47.57831274298844
At time: 707.6464576721191 and batch: 1250, loss is 3.924136462211609 and perplexity is 50.60935609233857
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.404562845717382 and perplexity of 81.82336558896198
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 710.5137405395508 and batch: 50, loss is 3.9773570013046267 and perplexity is 53.375775531351984
At time: 711.6992869377136 and batch: 100, loss is 3.986172480583191 and perplexity is 53.84838866952377
At time: 712.8578855991364 and batch: 150, loss is 3.9220716524124146 and perplexity is 50.50496520869663
At time: 714.0168361663818 and batch: 200, loss is 3.9671406841278074 and perplexity is 52.83324771585444
At time: 715.1768529415131 and batch: 250, loss is 3.9801868486404417 and perplexity is 53.52703474683967
At time: 716.3366117477417 and batch: 300, loss is 3.9873725938797 and perplexity is 53.91305163044022
At time: 717.4970681667328 and batch: 350, loss is 3.9618190002441405 and perplexity is 52.55283267485206
At time: 718.6579957008362 and batch: 400, loss is 3.985316104888916 and perplexity is 53.80229395830356
At time: 719.8175585269928 and batch: 450, loss is 3.9046576547622682 and perplexity is 49.6330853529354
At time: 720.9776263237 and batch: 500, loss is 3.923638482093811 and perplexity is 50.58415991334882
At time: 722.1382601261139 and batch: 550, loss is 3.8952144145965577 and perplexity is 49.16659426805562
At time: 723.3067355155945 and batch: 600, loss is 3.922739043235779 and perplexity is 50.53868300923506
At time: 724.4667212963104 and batch: 650, loss is 3.950362539291382 and perplexity is 51.95419885938955
At time: 725.6263673305511 and batch: 700, loss is 3.9118793487548826 and perplexity is 49.99281768224312
At time: 726.7856819629669 and batch: 750, loss is 3.9165251636505127 and perplexity is 50.22561540815965
At time: 727.9454712867737 and batch: 800, loss is 3.9501078510284424 and perplexity is 51.94096841962001
At time: 729.104966878891 and batch: 850, loss is 3.9736235618591307 and perplexity is 53.176871834040035
At time: 730.2791748046875 and batch: 900, loss is 3.9040614652633665 and perplexity is 49.60350344773673
At time: 731.4442458152771 and batch: 950, loss is 3.8984846687316894 and perplexity is 49.32764472069511
At time: 732.6041860580444 and batch: 1000, loss is 3.886874895095825 and perplexity is 48.75827346250721
At time: 733.7640221118927 and batch: 1050, loss is 3.8915672636032106 and perplexity is 48.987602877652634
At time: 734.9235439300537 and batch: 1100, loss is 3.835988440513611 and perplexity is 46.33920858950277
At time: 736.0827884674072 and batch: 1150, loss is 3.8398648071289063 and perplexity is 46.519184952477204
At time: 737.2436010837555 and batch: 1200, loss is 3.849185724258423 and perplexity is 46.95481349519707
At time: 738.4082381725311 and batch: 1250, loss is 3.9127668523788453 and perplexity is 50.03720618367255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.40191650390625 and perplexity of 81.60711925253717
Finished 24 epochs...
Completing Train Step...
At time: 741.3264198303223 and batch: 50, loss is 3.9756253862380984 and perplexity is 53.28342921146865
At time: 742.5154752731323 and batch: 100, loss is 3.982244601249695 and perplexity is 53.637293546015265
At time: 743.685108423233 and batch: 150, loss is 3.917630772590637 and perplexity is 50.2811760060654
At time: 744.8501048088074 and batch: 200, loss is 3.9622180891036987 and perplexity is 52.573810110562846
At time: 746.0138185024261 and batch: 250, loss is 3.975201954841614 and perplexity is 53.26087211065754
At time: 747.1798279285431 and batch: 300, loss is 3.981702060699463 and perplexity is 53.60820103190978
At time: 748.3647935390472 and batch: 350, loss is 3.9569875764846802 and perplexity is 52.299540045164456
At time: 749.5410947799683 and batch: 400, loss is 3.980575966835022 and perplexity is 53.54786714283088
At time: 750.7084593772888 and batch: 450, loss is 3.8998117065429687 and perplexity is 49.393147823315054
At time: 751.8773105144501 and batch: 500, loss is 3.9193040180206298 and perplexity is 50.365379180675106
At time: 753.0415811538696 and batch: 550, loss is 3.8915287351608274 and perplexity is 48.985715497976805
At time: 754.202831029892 and batch: 600, loss is 3.9190933179855345 and perplexity is 50.35476831140867
At time: 755.3641133308411 and batch: 650, loss is 3.946718816757202 and perplexity is 51.76523664626376
At time: 756.5258963108063 and batch: 700, loss is 3.909049892425537 and perplexity is 49.85156511605091
At time: 757.695643901825 and batch: 750, loss is 3.914725513458252 and perplexity is 50.135308154840324
At time: 758.8568155765533 and batch: 800, loss is 3.9493441820144652 and perplexity is 51.90131785336339
At time: 760.0189785957336 and batch: 850, loss is 3.9726709794998167 and perplexity is 53.126240603044785
At time: 761.1797659397125 and batch: 900, loss is 3.903351454734802 and perplexity is 49.56829693801045
At time: 762.3410348892212 and batch: 950, loss is 3.89819128036499 and perplexity is 49.31317468635105
At time: 763.5020620822906 and batch: 1000, loss is 3.8865637397766113 and perplexity is 48.743104426449186
At time: 764.6631376743317 and batch: 1050, loss is 3.8916715145111085 and perplexity is 48.992710145942425
At time: 765.8256366252899 and batch: 1100, loss is 3.8371182250976563 and perplexity is 46.39159149813352
At time: 766.9890472888947 and batch: 1150, loss is 3.842048726081848 and perplexity is 46.620890099646005
At time: 768.1497523784637 and batch: 1200, loss is 3.852093620300293 and perplexity is 47.09155192572356
At time: 769.3360118865967 and batch: 1250, loss is 3.915452470779419 and perplexity is 50.17176763478975
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401531581460994 and perplexity of 81.57571288553977
Finished 25 epochs...
Completing Train Step...
At time: 772.2194888591766 and batch: 50, loss is 3.9750537347793578 and perplexity is 53.2529783658976
At time: 773.3790605068207 and batch: 100, loss is 3.981051344871521 and perplexity is 53.57332867421887
At time: 774.538161277771 and batch: 150, loss is 3.916101641654968 and perplexity is 50.20434825916511
At time: 775.7018682956696 and batch: 200, loss is 3.960507912635803 and perplexity is 52.48397645528053
At time: 776.8773002624512 and batch: 250, loss is 3.9733856868743898 and perplexity is 53.16422389083823
At time: 778.0443561077118 and batch: 300, loss is 3.979771499633789 and perplexity is 53.504806962592106
At time: 779.2097365856171 and batch: 350, loss is 3.955294771194458 and perplexity is 52.21108199934748
At time: 780.3824207782745 and batch: 400, loss is 3.9789169216156006 and perplexity is 53.45910246250653
At time: 781.5519857406616 and batch: 450, loss is 3.8981502866744995 and perplexity is 49.31115319876525
At time: 782.7210502624512 and batch: 500, loss is 3.917711606025696 and perplexity is 50.28524057051491
At time: 783.8931052684784 and batch: 550, loss is 3.890125994682312 and perplexity is 48.917049423579186
At time: 785.053245306015 and batch: 600, loss is 3.917821083068848 and perplexity is 50.29074595131765
At time: 786.2142124176025 and batch: 650, loss is 3.945357575416565 and perplexity is 51.69481960430075
At time: 787.3752002716064 and batch: 700, loss is 3.9078855180740355 and perplexity is 49.79355301270442
At time: 788.5359003543854 and batch: 750, loss is 3.914076566696167 and perplexity is 50.10278356345266
At time: 789.6962215900421 and batch: 800, loss is 3.9490021276474 and perplexity is 51.88356781684701
At time: 790.8570320606232 and batch: 850, loss is 3.972331428527832 and perplexity is 53.10820459865482
At time: 792.0185158252716 and batch: 900, loss is 3.9033666706085204 and perplexity is 49.56905116869522
At time: 793.1789243221283 and batch: 950, loss is 3.898393063545227 and perplexity is 49.32312625956311
At time: 794.3405129909515 and batch: 1000, loss is 3.886818361282349 and perplexity is 48.755517049285864
At time: 795.5020983219147 and batch: 1050, loss is 3.892064619064331 and perplexity is 49.01197318932247
At time: 796.6635944843292 and batch: 1100, loss is 3.8379073619842528 and perplexity is 46.42821526289388
At time: 797.8242027759552 and batch: 1150, loss is 3.8432318353652954 and perplexity is 46.676080349136036
At time: 799.0304825305939 and batch: 1200, loss is 3.8535065031051636 and perplexity is 47.15813379480696
At time: 800.1908507347107 and batch: 1250, loss is 3.916613402366638 and perplexity is 50.23004744751572
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401429113680429 and perplexity of 81.56735443153578
Finished 26 epochs...
Completing Train Step...
At time: 803.0751223564148 and batch: 50, loss is 3.9744298219680787 and perplexity is 53.21976351311989
At time: 804.2653448581696 and batch: 100, loss is 3.9801916694641113 and perplexity is 53.52729279185773
At time: 805.4420614242554 and batch: 150, loss is 3.9150483798980713 and perplexity is 50.15149777669566
At time: 806.6132402420044 and batch: 200, loss is 3.9593313121795655 and perplexity is 52.422260099504214
At time: 807.777411699295 and batch: 250, loss is 3.9722882986068724 and perplexity is 53.10591409538315
At time: 808.9382364749908 and batch: 300, loss is 3.978657393455505 and perplexity is 53.44523012021368
At time: 810.1035974025726 and batch: 350, loss is 3.954244866371155 and perplexity is 52.156294098599844
At time: 811.2671966552734 and batch: 400, loss is 3.9778986978530884 and perplexity is 53.40469683730738
At time: 812.4314517974854 and batch: 450, loss is 3.8971328067779543 and perplexity is 49.261005608119056
At time: 813.5948030948639 and batch: 500, loss is 3.916790781021118 and perplexity is 50.23895797599185
At time: 814.7549328804016 and batch: 550, loss is 3.8893111419677733 and perplexity is 48.877205468751185
At time: 815.9160692691803 and batch: 600, loss is 3.9170532178878785 and perplexity is 50.25214426092034
At time: 817.0765573978424 and batch: 650, loss is 3.944612364768982 and perplexity is 51.65631042481416
At time: 818.236739397049 and batch: 700, loss is 3.90720853805542 and perplexity is 49.75985517992566
At time: 819.3969175815582 and batch: 750, loss is 3.9136673641204833 and perplexity is 50.08228556957155
At time: 820.5570621490479 and batch: 800, loss is 3.9487533807754516 and perplexity is 51.870663546662016
At time: 821.7181262969971 and batch: 850, loss is 3.9721107387542727 and perplexity is 53.09648545420304
At time: 822.8779928684235 and batch: 900, loss is 3.9034071159362793 and perplexity is 49.571056045760116
At time: 824.0384364128113 and batch: 950, loss is 3.8985725831985474 and perplexity is 49.33198152491322
At time: 825.1983368396759 and batch: 1000, loss is 3.887066102027893 and perplexity is 48.767597273749175
At time: 826.3584566116333 and batch: 1050, loss is 3.8923725938796996 and perplexity is 49.02706996731069
At time: 827.5190908908844 and batch: 1100, loss is 3.8384201288223267 and perplexity is 46.452028216756425
At time: 828.7066011428833 and batch: 1150, loss is 3.844004693031311 and perplexity is 46.71216825926612
At time: 829.8673160076141 and batch: 1200, loss is 3.8543480157852175 and perplexity is 47.197834664412724
At time: 831.0282254219055 and batch: 1250, loss is 3.917227644920349 and perplexity is 50.260910357818965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4013836714473085 and perplexity of 81.56364791301756
Finished 27 epochs...
Completing Train Step...
At time: 833.9350860118866 and batch: 50, loss is 3.9738242053985595 and perplexity is 53.18754250028489
At time: 835.0985550880432 and batch: 100, loss is 3.9794543552398682 and perplexity is 53.487840903503574
At time: 836.2640335559845 and batch: 150, loss is 3.914200129508972 and perplexity is 50.10897478681374
At time: 837.4280385971069 and batch: 200, loss is 3.9584414100646974 and perplexity is 52.37563017048901
At time: 838.5928897857666 and batch: 250, loss is 3.97145348072052 and perplexity is 53.061598828587606
At time: 839.7560987472534 and batch: 300, loss is 3.9778251314163207 and perplexity is 53.40076818856451
At time: 840.9205877780914 and batch: 350, loss is 3.9534509992599487 and perplexity is 52.1149053628243
At time: 842.0848293304443 and batch: 400, loss is 3.9771097707748413 and perplexity is 53.36258104119727
At time: 843.2501831054688 and batch: 450, loss is 3.8963895511627196 and perplexity is 49.22440569232008
At time: 844.415041923523 and batch: 500, loss is 3.9161371421813964 and perplexity is 50.206130571593626
At time: 845.5796089172363 and batch: 550, loss is 3.888720679283142 and perplexity is 48.848353821541615
At time: 846.7435092926025 and batch: 600, loss is 3.916496443748474 and perplexity is 50.224172954118956
At time: 847.907662153244 and batch: 650, loss is 3.944098696708679 and perplexity is 51.629783041753825
At time: 849.0721395015717 and batch: 700, loss is 3.906729621887207 and perplexity is 49.7360300863187
At time: 850.2365064620972 and batch: 750, loss is 3.9133349752426145 and perplexity is 50.065641541168176
At time: 851.4023520946503 and batch: 800, loss is 3.9485604524612428 and perplexity is 51.86065719227265
At time: 852.5725820064545 and batch: 850, loss is 3.9719527673721315 and perplexity is 53.08809839148439
At time: 853.7475273609161 and batch: 900, loss is 3.903412733078003 and perplexity is 49.571334494189365
At time: 854.9192502498627 and batch: 950, loss is 3.8986966228485107 and perplexity is 49.33810102615611
At time: 856.0860958099365 and batch: 1000, loss is 3.887267756462097 and perplexity is 48.77743246760679
At time: 857.2506122589111 and batch: 1050, loss is 3.8926018953323362 and perplexity is 49.038313234672216
At time: 858.4578747749329 and batch: 1100, loss is 3.838762378692627 and perplexity is 46.467929138277746
At time: 859.6219334602356 and batch: 1150, loss is 3.8444802808761596 and perplexity is 46.73438928230214
At time: 860.7860391139984 and batch: 1200, loss is 3.8549156188964844 and perplexity is 47.22463190659288
At time: 861.9503138065338 and batch: 1250, loss is 3.917601580619812 and perplexity is 50.27970822086626
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401360504818658 and perplexity of 81.56175838016216
Finished 28 epochs...
Completing Train Step...
At time: 864.8259801864624 and batch: 50, loss is 3.973241057395935 and perplexity is 53.15653533287391
At time: 866.0132284164429 and batch: 100, loss is 3.9787887811660765 and perplexity is 53.45225262796564
At time: 867.1831254959106 and batch: 150, loss is 3.9134558916091917 and perplexity is 50.0716956626475
At time: 868.344557762146 and batch: 200, loss is 3.9576973247528078 and perplexity is 52.33667272900563
At time: 869.5130786895752 and batch: 250, loss is 3.97075852394104 and perplexity is 53.024736121229424
At time: 870.6818044185638 and batch: 300, loss is 3.977144660949707 and perplexity is 53.36444290346124
At time: 871.8417534828186 and batch: 350, loss is 3.9527891731262206 and perplexity is 52.080425767505595
At time: 873.0023603439331 and batch: 400, loss is 3.9764514541625977 and perplexity is 53.32746312824668
At time: 874.1621696949005 and batch: 450, loss is 3.895769896507263 and perplexity is 49.193913008613166
At time: 875.3240694999695 and batch: 500, loss is 3.91560462474823 and perplexity is 50.17940204914614
At time: 876.4841969013214 and batch: 550, loss is 3.8882278108596804 and perplexity is 48.82428394253381
At time: 877.64510846138 and batch: 600, loss is 3.916038017272949 and perplexity is 50.20115414014546
At time: 878.80530834198 and batch: 650, loss is 3.9437013339996336 and perplexity is 51.60927136685417
At time: 879.9652292728424 and batch: 700, loss is 3.9063640356063845 and perplexity is 49.717850599344565
At time: 881.1249086856842 and batch: 750, loss is 3.913038992881775 and perplexity is 50.05082518718575
At time: 882.2849159240723 and batch: 800, loss is 3.948384919166565 and perplexity is 51.85155471916845
At time: 883.4457757472992 and batch: 850, loss is 3.9718038845062256 and perplexity is 53.08019507159942
At time: 884.6064834594727 and batch: 900, loss is 3.9033901691436768 and perplexity is 49.570215982472426
At time: 885.7671179771423 and batch: 950, loss is 3.8987765407562254 and perplexity is 49.342044181523
At time: 886.9552788734436 and batch: 1000, loss is 3.88739755153656 and perplexity is 48.783763948974695
At time: 888.116096496582 and batch: 1050, loss is 3.892767252922058 and perplexity is 49.04642276242023
At time: 889.2764053344727 and batch: 1100, loss is 3.838994245529175 and perplexity is 46.47870475921425
At time: 890.4372661113739 and batch: 1150, loss is 3.8448120546340943 and perplexity is 46.749897098660114
At time: 891.5980131626129 and batch: 1200, loss is 3.855324158668518 and perplexity is 47.24392898849069
At time: 892.762026309967 and batch: 1250, loss is 3.9178458881378173 and perplexity is 50.29199343221136
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401343575359261 and perplexity of 81.56037759537331
Finished 29 epochs...
Completing Train Step...
At time: 895.6804585456848 and batch: 50, loss is 3.972682147026062 and perplexity is 53.126833895043816
At time: 896.8450720310211 and batch: 100, loss is 3.978173322677612 and perplexity is 53.41936510684835
At time: 898.0097935199738 and batch: 150, loss is 3.9127752876281736 and perplexity is 50.03762826176257
At time: 899.1738436222076 and batch: 200, loss is 3.9570394277572634 and perplexity is 52.30225191317761
At time: 900.3467538356781 and batch: 250, loss is 3.9701504707336426 and perplexity is 52.99250406075648
At time: 901.5097832679749 and batch: 300, loss is 3.976549711227417 and perplexity is 53.332703185680025
At time: 902.6744236946106 and batch: 350, loss is 3.952208023071289 and perplexity is 52.05016801820788
At time: 903.8383846282959 and batch: 400, loss is 3.9758739709854125 and perplexity is 53.29667630569978
At time: 905.0027883052826 and batch: 450, loss is 3.8952305936813354 and perplexity is 49.16738974498754
At time: 906.1673169136047 and batch: 500, loss is 3.9151406717300414 and perplexity is 50.156126563897864
At time: 907.3324127197266 and batch: 550, loss is 3.887791256904602 and perplexity is 48.80297416004818
At time: 908.5019409656525 and batch: 600, loss is 3.915637607574463 and perplexity is 50.18105713493895
At time: 909.6682498455048 and batch: 650, loss is 3.943356294631958 and perplexity is 51.591467208240005
At time: 910.8327910900116 and batch: 700, loss is 3.906050190925598 and perplexity is 49.70224936470414
At time: 911.9970841407776 and batch: 750, loss is 3.9127653789520265 and perplexity is 50.03713245756534
At time: 913.1605021953583 and batch: 800, loss is 3.948212242126465 and perplexity is 51.84260191916871
At time: 914.3249404430389 and batch: 850, loss is 3.97165696144104 and perplexity is 53.07239693951561
At time: 915.4891886711121 and batch: 900, loss is 3.903346300125122 and perplexity is 49.56804143344575
At time: 916.6996257305145 and batch: 950, loss is 3.898821244239807 and perplexity is 49.344249992088294
At time: 917.8638122081757 and batch: 1000, loss is 3.8874832820892333 and perplexity is 48.78794638729834
At time: 919.0278222560883 and batch: 1050, loss is 3.892882785797119 and perplexity is 49.05208956399807
At time: 920.1926159858704 and batch: 1100, loss is 3.8391534805297853 and perplexity is 46.48610638507827
At time: 921.3566024303436 and batch: 1150, loss is 3.8450531148910523 and perplexity is 46.761167999296006
At time: 922.5210943222046 and batch: 1200, loss is 3.8556331825256347 and perplexity is 47.258530745681526
At time: 923.6844599246979 and batch: 1250, loss is 3.918012566566467 and perplexity is 50.30037672128763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401331546532846 and perplexity of 81.55939652564945
Finished 30 epochs...
Completing Train Step...
At time: 926.5992794036865 and batch: 50, loss is 3.9721444034576416 and perplexity is 53.09827296172357
At time: 927.769660949707 and batch: 100, loss is 3.9775943088531496 and perplexity is 53.388443508837724
At time: 928.9399075508118 and batch: 150, loss is 3.9121672534942626 and perplexity is 50.00721292351846
At time: 930.1005954742432 and batch: 200, loss is 3.9564380741119383 and perplexity is 52.27080921836802
At time: 931.2612357139587 and batch: 250, loss is 3.9695977449417112 and perplexity is 52.96322183025059
At time: 932.4216873645782 and batch: 300, loss is 3.97601037979126 and perplexity is 53.303946937548105
At time: 933.5828170776367 and batch: 350, loss is 3.9516799640655518 and perplexity is 52.022689713957256
At time: 934.7534699440002 and batch: 400, loss is 3.9753481912612916 and perplexity is 53.26866135942595
At time: 935.9144775867462 and batch: 450, loss is 3.8947446060180666 and perplexity is 49.143500805471476
At time: 937.0741832256317 and batch: 500, loss is 3.9147174835205076 and perplexity is 50.134905573053395
At time: 938.2341091632843 and batch: 550, loss is 3.8873905038833616 and perplexity is 48.783420139136204
At time: 939.3942387104034 and batch: 600, loss is 3.9152731466293336 and perplexity is 50.162771431842685
At time: 940.5561628341675 and batch: 650, loss is 3.9430456018447875 and perplexity is 51.57544060130337
At time: 941.7162902355194 and batch: 700, loss is 3.905763907432556 and perplexity is 49.688022467704016
At time: 942.8760838508606 and batch: 750, loss is 3.9125079679489136 and perplexity is 50.02425400670514
At time: 944.0355713367462 and batch: 800, loss is 3.9480481481552125 and perplexity is 51.834095558680055
At time: 945.1956791877747 and batch: 850, loss is 3.971509656906128 and perplexity is 53.064579710538545
At time: 946.3821680545807 and batch: 900, loss is 3.9032852697372435 and perplexity is 49.565016368962056
At time: 947.5494201183319 and batch: 950, loss is 3.898837013244629 and perplexity is 49.34502810793939
At time: 948.7090337276459 and batch: 1000, loss is 3.8875355911254883 and perplexity is 48.79049850450354
At time: 949.8690996170044 and batch: 1050, loss is 3.892960886955261 and perplexity is 49.05592073860994
At time: 951.0374565124512 and batch: 1100, loss is 3.839264039993286 and perplexity is 46.49124614817993
At time: 952.1981630325317 and batch: 1150, loss is 3.8452333974838258 and perplexity is 46.76959898386107
At time: 953.3572115898132 and batch: 1200, loss is 3.8558751916885377 and perplexity is 47.269969127188055
At time: 954.5187938213348 and batch: 1250, loss is 3.9181287240982057 and perplexity is 50.30621982824697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401324418339416 and perplexity of 81.55881515656702
Finished 31 epochs...
Completing Train Step...
At time: 957.3903160095215 and batch: 50, loss is 3.971624941825867 and perplexity is 53.0706976089954
At time: 958.5791194438934 and batch: 100, loss is 3.9770426988601684 and perplexity is 53.35900203074183
At time: 959.741813659668 and batch: 150, loss is 3.9116146421432494 and perplexity is 49.97958600420205
At time: 960.9055109024048 and batch: 200, loss is 3.9558767080307007 and perplexity is 52.24147439359216
At time: 962.0680720806122 and batch: 250, loss is 3.969083209037781 and perplexity is 52.935977360761036
At time: 963.2312443256378 and batch: 300, loss is 3.9755114269256593 and perplexity is 53.27735741448679
At time: 964.3942885398865 and batch: 350, loss is 3.951188712120056 and perplexity is 51.99713974267573
At time: 965.5612661838531 and batch: 400, loss is 3.97485906124115 and perplexity is 53.24261242919852
At time: 966.7277433872223 and batch: 450, loss is 3.8942930603027346 and perplexity is 49.121315277513354
At time: 967.8902270793915 and batch: 500, loss is 3.914319987297058 and perplexity is 50.11498109763932
At time: 969.0538756847382 and batch: 550, loss is 3.887013840675354 and perplexity is 48.765048679752624
At time: 970.2176792621613 and batch: 600, loss is 3.9149329710006713 and perplexity is 50.1457101816107
At time: 971.3917820453644 and batch: 650, loss is 3.9427586793899536 and perplexity is 51.56064457203493
At time: 972.5643303394318 and batch: 700, loss is 3.9054949855804444 and perplexity is 49.674662069206306
At time: 973.7279217243195 and batch: 750, loss is 3.9122614097595214 and perplexity is 50.011921637597325
At time: 974.890477180481 and batch: 800, loss is 3.9478964948654176 and perplexity is 51.826235343593844
At time: 976.0989634990692 and batch: 850, loss is 3.971360092163086 and perplexity is 53.05664371379695
At time: 977.2612795829773 and batch: 900, loss is 3.903210234642029 and perplexity is 49.56129739276811
At time: 978.4235155582428 and batch: 950, loss is 3.898830041885376 and perplexity is 49.34468410722018
At time: 979.586213350296 and batch: 1000, loss is 3.8875613594055176 and perplexity is 48.79175576793046
At time: 980.7492210865021 and batch: 1050, loss is 3.893010559082031 and perplexity is 49.05835751104302
At time: 981.9126830101013 and batch: 1100, loss is 3.8393391132354737 and perplexity is 46.49473652777704
At time: 983.0763504505157 and batch: 1150, loss is 3.84537082195282 and perplexity is 46.776026712820006
At time: 984.2382872104645 and batch: 1200, loss is 3.8560702276229857 and perplexity is 47.2791893688981
At time: 985.4005455970764 and batch: 1250, loss is 3.9182098579406737 and perplexity is 50.31030153074153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401319963218522 and perplexity of 81.5584518029949
Finished 32 epochs...
Completing Train Step...
At time: 988.3190412521362 and batch: 50, loss is 3.9711226558685304 and perplexity is 53.04404763635416
At time: 989.4801211357117 and batch: 100, loss is 3.9765137100219725 and perplexity is 53.330783178637226
At time: 990.6409437656403 and batch: 150, loss is 3.911097164154053 and perplexity is 49.953729359235034
At time: 991.8006031513214 and batch: 200, loss is 3.9553454399108885 and perplexity is 52.21372753487821
At time: 992.9666030406952 and batch: 250, loss is 3.968596887588501 and perplexity is 52.91023971842294
At time: 994.1270768642426 and batch: 300, loss is 3.9750484895706175 and perplexity is 53.25269904364258
At time: 995.2883167266846 and batch: 350, loss is 3.950724868774414 and perplexity is 51.9730268081581
At time: 996.4492609500885 and batch: 400, loss is 3.974396800994873 and perplexity is 53.2180061737496
At time: 997.6095597743988 and batch: 450, loss is 3.8938654184341432 and perplexity is 49.10031343741355
At time: 998.7701807022095 and batch: 500, loss is 3.9139404678344727 and perplexity is 50.09596509564534
At time: 999.9303045272827 and batch: 550, loss is 3.8866542053222655 and perplexity is 48.74751419745117
At time: 1001.0909793376923 and batch: 600, loss is 3.914609489440918 and perplexity is 50.129491592414915
At time: 1002.2596859931946 and batch: 650, loss is 3.9424872636795043 and perplexity is 51.546652102031175
At time: 1003.4200382232666 and batch: 700, loss is 3.905237765312195 and perplexity is 49.66188638245689
At time: 1004.5810072422028 and batch: 750, loss is 3.9120217847824095 and perplexity is 49.99993896775042
At time: 1005.7677850723267 and batch: 800, loss is 3.947751312255859 and perplexity is 51.818711621673074
At time: 1006.9279754161835 and batch: 850, loss is 3.9712076997756958 and perplexity is 53.048558901242096
At time: 1008.08873295784 and batch: 900, loss is 3.903124237060547 and perplexity is 49.557035424319324
At time: 1009.2492444515228 and batch: 950, loss is 3.898805646896362 and perplexity is 49.34348035887627
At time: 1010.4103229045868 and batch: 1000, loss is 3.8875512409210207 and perplexity is 48.79126207180387
At time: 1011.5708980560303 and batch: 1050, loss is 3.893037586212158 and perplexity is 49.05968343557317
At time: 1012.7312452793121 and batch: 1100, loss is 3.8393885898590088 and perplexity is 46.4970369872616
At time: 1013.8912625312805 and batch: 1150, loss is 3.845477738380432 and perplexity is 46.78102810585481
At time: 1015.0524237155914 and batch: 1200, loss is 3.8562308311462403 and perplexity is 47.286783183067705
At time: 1016.2132637500763 and batch: 1250, loss is 3.9182657623291015 and perplexity is 50.313114175999104
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401316844633896 and perplexity of 81.55819745645756
Finished 33 epochs...
Completing Train Step...
At time: 1019.0852439403534 and batch: 50, loss is 3.9706348514556886 and perplexity is 53.018178825815085
At time: 1020.2730712890625 and batch: 100, loss is 3.9760030412673952 and perplexity is 53.30355576669674
At time: 1021.4332253932953 and batch: 150, loss is 3.9106053638458254 and perplexity is 49.929168139841735
At time: 1022.5987985134125 and batch: 200, loss is 3.954838171005249 and perplexity is 52.187247851180146
At time: 1023.7607853412628 and batch: 250, loss is 3.968132481575012 and perplexity is 52.885673589693035
At time: 1024.922046661377 and batch: 300, loss is 3.9746192789077757 and perplexity is 53.22984732184009
At time: 1026.0818388462067 and batch: 350, loss is 3.950282516479492 and perplexity is 51.95004150465117
At time: 1027.242198228836 and batch: 400, loss is 3.973955135345459 and perplexity is 53.194506798307806
At time: 1028.4029603004456 and batch: 450, loss is 3.8934545135498047 and perplexity is 49.08014202334966
At time: 1029.5630354881287 and batch: 500, loss is 3.913574595451355 and perplexity is 50.07763971809042
At time: 1030.7303533554077 and batch: 550, loss is 3.886307063102722 and perplexity is 48.730594814061504
At time: 1031.8911972045898 and batch: 600, loss is 3.9142983245849607 and perplexity is 50.113895482990756
At time: 1033.0509827136993 and batch: 650, loss is 3.9422263431549074 and perplexity is 51.533204277006575
At time: 1034.2107274532318 and batch: 700, loss is 3.9049880504608154 and perplexity is 49.649486620146504
At time: 1035.4085726737976 and batch: 750, loss is 3.91178635597229 and perplexity is 49.98816892717089
At time: 1036.5693719387054 and batch: 800, loss is 3.9475992345809936 and perplexity is 51.810831751686464
At time: 1037.7302837371826 and batch: 850, loss is 3.971052198410034 and perplexity is 53.04031041922829
At time: 1038.8912374973297 and batch: 900, loss is 3.903029637336731 and perplexity is 49.55234756419369
At time: 1040.0507061481476 and batch: 950, loss is 3.8987679862976075 and perplexity is 49.34162208885332
At time: 1041.2108244895935 and batch: 1000, loss is 3.8875453090667724 and perplexity is 48.79097265000708
At time: 1042.372006893158 and batch: 1050, loss is 3.893047070503235 and perplexity is 49.060148734097524
At time: 1043.5319130420685 and batch: 1100, loss is 3.839418501853943 and perplexity is 46.49842782719772
At time: 1044.692046403885 and batch: 1150, loss is 3.845560517311096 and perplexity is 46.784900749621215
At time: 1045.8525722026825 and batch: 1200, loss is 3.856364436149597 and perplexity is 47.293101355953965
At time: 1047.011819601059 and batch: 1250, loss is 3.918302035331726 and perplexity is 50.31493921682132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.401319072194343 and perplexity of 81.55837913247471
Annealing...
Finished 34 epochs...
Completing Train Step...
At time: 1049.94633436203 and batch: 50, loss is 3.9707603597640992 and perplexity is 53.02483346535207
At time: 1051.1100890636444 and batch: 100, loss is 3.9766093063354493 and perplexity is 53.335881648597514
At time: 1052.274439573288 and batch: 150, loss is 3.912239670753479 and perplexity is 50.01083443994799
At time: 1053.43869638443 and batch: 200, loss is 3.956661648750305 and perplexity is 52.28249695212789
At time: 1054.6025516986847 and batch: 250, loss is 3.969848880767822 and perplexity is 52.97652446303234
At time: 1055.7755739688873 and batch: 300, loss is 3.976250019073486 and perplexity is 53.31672218779681
At time: 1056.9427580833435 and batch: 350, loss is 3.9519836568832396 and perplexity is 52.0384910304319
At time: 1058.127375125885 and batch: 400, loss is 3.9743211841583252 and perplexity is 53.2139821486193
At time: 1059.3027238845825 and batch: 450, loss is 3.8935635805130007 and perplexity is 49.08549533732292
At time: 1060.4787561893463 and batch: 500, loss is 3.9139533281326293 and perplexity is 50.09660934883555
At time: 1061.6552283763885 and batch: 550, loss is 3.885906343460083 and perplexity is 48.71107141948711
At time: 1062.8321828842163 and batch: 600, loss is 3.914048852920532 and perplexity is 50.10139504539094
At time: 1064.0248174667358 and batch: 650, loss is 3.942406511306763 and perplexity is 51.54248975562899
At time: 1065.1967074871063 and batch: 700, loss is 3.905276646614075 and perplexity is 49.663817338792065
At time: 1066.3643355369568 and batch: 750, loss is 3.9107186222076415 and perplexity is 49.934823355876304
At time: 1067.5295844078064 and batch: 800, loss is 3.94545175075531 and perplexity is 51.699688210696074
At time: 1068.6949450969696 and batch: 850, loss is 3.9690479612350464 and perplexity is 52.93411151675712
At time: 1069.873692035675 and batch: 900, loss is 3.9008864307403566 and perplexity is 49.44626037002327
At time: 1071.0401139259338 and batch: 950, loss is 3.8965476894378663 and perplexity is 49.23219057045877
At time: 1072.2083475589752 and batch: 1000, loss is 3.8857043886184695 and perplexity is 48.701234976065884
At time: 1073.374585390091 and batch: 1050, loss is 3.8909441661834716 and perplexity is 48.95708833645359
At time: 1074.5405790805817 and batch: 1100, loss is 3.8367273664474486 and perplexity is 46.373462486470885
At time: 1075.708608865738 and batch: 1150, loss is 3.842290511131287 and perplexity is 46.632163696702314
At time: 1076.8746116161346 and batch: 1200, loss is 3.8530908966064454 and perplexity is 47.13853864015328
At time: 1078.0446193218231 and batch: 1250, loss is 3.9157888650894166 and perplexity is 50.18864797100996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4010945341012775 and perplexity of 81.54006822537582
Finished 35 epochs...
Completing Train Step...
At time: 1080.9787089824677 and batch: 50, loss is 3.97057080745697 and perplexity is 53.014783438366564
At time: 1082.1690473556519 and batch: 100, loss is 3.9762735080718996 and perplexity is 53.317974558908105
At time: 1083.3379864692688 and batch: 150, loss is 3.9117196083068846 and perplexity is 49.98483244494955
At time: 1084.502319574356 and batch: 200, loss is 3.956259160041809 and perplexity is 52.26145807169261
At time: 1085.6641607284546 and batch: 250, loss is 3.9694839334487915 and perplexity is 52.957194349909024
At time: 1086.85218334198 and batch: 300, loss is 3.9758528804779054 and perplexity is 53.29555226360141
At time: 1088.0215995311737 and batch: 350, loss is 3.9515995931625367 and perplexity is 52.01850877142299
At time: 1089.192237854004 and batch: 400, loss is 3.9740274715423585 and perplexity is 53.1983548257997
At time: 1090.3572852611542 and batch: 450, loss is 3.893284296989441 and perplexity is 49.071788481368046
At time: 1091.5177924633026 and batch: 500, loss is 3.913707413673401 and perplexity is 50.0842913828834
At time: 1092.6976058483124 and batch: 550, loss is 3.885728945732117 and perplexity is 48.70243095251277
At time: 1093.9062430858612 and batch: 600, loss is 3.9138561868667603 and perplexity is 50.091743137146466
At time: 1095.0690021514893 and batch: 650, loss is 3.942169427871704 and perplexity is 51.53027133355622
At time: 1096.2312724590302 and batch: 700, loss is 3.9051168251037596 and perplexity is 49.655880626742494
At time: 1097.394696712494 and batch: 750, loss is 3.910637545585632 and perplexity is 49.93077497319479
At time: 1098.557784318924 and batch: 800, loss is 3.945522413253784 and perplexity is 51.703341568911554
At time: 1099.7209837436676 and batch: 850, loss is 3.9690548944473267 and perplexity is 52.93447852146139
At time: 1100.8835849761963 and batch: 900, loss is 3.9009125995635987 and perplexity is 49.447554337401606
At time: 1102.0469789505005 and batch: 950, loss is 3.8966478586196898 and perplexity is 49.23712236571041
At time: 1103.2106335163116 and batch: 1000, loss is 3.8858216762542725 and perplexity is 48.706947363766595
At time: 1104.373990058899 and batch: 1050, loss is 3.891091823577881 and perplexity is 48.96431774627999
At time: 1105.5379376411438 and batch: 1100, loss is 3.8369187211990354 and perplexity is 46.38233711793958
At time: 1106.7014906406403 and batch: 1150, loss is 3.842499179840088 and perplexity is 46.64189538540365
At time: 1107.867604970932 and batch: 1200, loss is 3.853325533866882 and perplexity is 47.149600395420016
At time: 1109.0313305854797 and batch: 1250, loss is 3.9159671449661255 and perplexity is 50.19759639462064
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400994739393248 and perplexity of 81.53193136408915
Finished 36 epochs...
Completing Train Step...
At time: 1111.997358083725 and batch: 50, loss is 3.970428104400635 and perplexity is 53.00721860651411
At time: 1113.1633138656616 and batch: 100, loss is 3.9760505533218384 and perplexity is 53.30608838830489
At time: 1114.330516576767 and batch: 150, loss is 3.9113740348815917 and perplexity is 49.96756199946422
At time: 1115.4982330799103 and batch: 200, loss is 3.9559659433364867 and perplexity is 52.24613638553843
At time: 1116.6655006408691 and batch: 250, loss is 3.9692157745361327 and perplexity is 52.94299531013932
At time: 1117.83167886734 and batch: 300, loss is 3.9755414819717405 and perplexity is 53.27895869198208
At time: 1118.997897386551 and batch: 350, loss is 3.9513063764572145 and perplexity is 52.003258311619305
At time: 1120.1643240451813 and batch: 400, loss is 3.973801155090332 and perplexity is 53.186316525166006
At time: 1121.3315830230713 and batch: 450, loss is 3.8930963230133058 and perplexity is 49.0625651290734
At time: 1122.4993443489075 and batch: 500, loss is 3.9135309648513794 and perplexity is 50.075454848288096
At time: 1123.7143659591675 and batch: 550, loss is 3.8855917644500733 and perplexity is 48.69575034883343
At time: 1124.8847441673279 and batch: 600, loss is 3.9137166595458983 and perplexity is 50.08475445799641
At time: 1126.0629904270172 and batch: 650, loss is 3.9420121765136718 and perplexity is 51.522168765495856
At time: 1127.2303485870361 and batch: 700, loss is 3.905020933151245 and perplexity is 49.651119255687604
At time: 1128.3972415924072 and batch: 750, loss is 3.910591926574707 and perplexity is 49.92849723258031
At time: 1129.5627980232239 and batch: 800, loss is 3.945585150718689 and perplexity is 51.706585407242734
At time: 1130.7303977012634 and batch: 850, loss is 3.9690655946731566 and perplexity is 52.93504493536613
At time: 1131.8968510627747 and batch: 900, loss is 3.9009447479248047 and perplexity is 49.44914402079192
At time: 1133.062326669693 and batch: 950, loss is 3.896750707626343 and perplexity is 49.242186615258234
At time: 1134.2299628257751 and batch: 1000, loss is 3.8859191608428953 and perplexity is 48.711695771937976
At time: 1135.3963799476624 and batch: 1050, loss is 3.89121696472168 and perplexity is 48.97044558042219
At time: 1136.5632622241974 and batch: 1100, loss is 3.8370836210250854 and perplexity is 46.38998618790993
At time: 1137.7298419475555 and batch: 1150, loss is 3.8426797771453858 and perplexity is 46.65031954669176
At time: 1138.8962540626526 and batch: 1200, loss is 3.85352014541626 and perplexity is 47.15877714512729
At time: 1140.062907218933 and batch: 1250, loss is 3.916109228134155 and perplexity is 50.20472913485303
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400937713845803 and perplexity of 81.52728209363372
Finished 37 epochs...
Completing Train Step...
At time: 1143.0353395938873 and batch: 50, loss is 3.9703023052215576 and perplexity is 53.000550761341756
At time: 1144.1999793052673 and batch: 100, loss is 3.975872235298157 and perplexity is 53.29658379941823
At time: 1145.361032962799 and batch: 150, loss is 3.9110973405838014 and perplexity is 49.953738172559724
At time: 1146.521540403366 and batch: 200, loss is 3.9557227754592894 and perplexity is 52.23343334800973
At time: 1147.6839969158173 and batch: 250, loss is 3.968988466262817 and perplexity is 52.93096229694475
At time: 1148.844675064087 and batch: 300, loss is 3.9752926635742187 and perplexity is 53.26570355598795
At time: 1150.0060443878174 and batch: 350, loss is 3.9510655307769778 and perplexity is 51.99073505964282
At time: 1151.1683883666992 and batch: 400, loss is 3.9736142444610594 and perplexity is 53.17637636626521
At time: 1152.3287074565887 and batch: 450, loss is 3.8929512166976927 and perplexity is 49.05544635751484
At time: 1153.5428867340088 and batch: 500, loss is 3.9133763217926028 and perplexity is 50.06771162551398
At time: 1154.7036066055298 and batch: 550, loss is 3.885477566719055 and perplexity is 48.69018972214488
At time: 1155.8645179271698 and batch: 600, loss is 3.9136056327819824 and perplexity is 50.07919401847193
At time: 1157.0247654914856 and batch: 650, loss is 3.941893734931946 and perplexity is 51.516066759706064
At time: 1158.1862726211548 and batch: 700, loss is 3.904951138496399 and perplexity is 49.64765399388621
At time: 1159.3462407588959 and batch: 750, loss is 3.9105564737319947 and perplexity is 49.92672715679836
At time: 1160.5079362392426 and batch: 800, loss is 3.9456396389007566 and perplexity is 51.709402881841335
At time: 1161.6684911251068 and batch: 850, loss is 3.9690754556655885 and perplexity is 52.93556693001731
At time: 1162.8291182518005 and batch: 900, loss is 3.9009747314453125 and perplexity is 49.45062670244365
At time: 1163.9892582893372 and batch: 950, loss is 3.896842565536499 and perplexity is 49.24671010736831
At time: 1165.1501626968384 and batch: 1000, loss is 3.886001935005188 and perplexity is 48.71572800862957
At time: 1166.310598373413 and batch: 1050, loss is 3.891323013305664 and perplexity is 48.975639102211005
At time: 1167.4712190628052 and batch: 1100, loss is 3.83722393989563 and perplexity is 46.39649603509293
At time: 1168.6489751338959 and batch: 1150, loss is 3.8428362607955933 and perplexity is 46.65762013017385
At time: 1169.8144598007202 and batch: 1200, loss is 3.8536831188201903 and perplexity is 47.1664633978742
At time: 1170.981838941574 and batch: 1250, loss is 3.9162232875823975 and perplexity is 50.21045578514038
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400898063269845 and perplexity of 81.52404955402889
Finished 38 epochs...
Completing Train Step...
At time: 1173.8990397453308 and batch: 50, loss is 3.9701868867874146 and perplexity is 52.99443387377175
At time: 1175.0957992076874 and batch: 100, loss is 3.9757195329666137 and perplexity is 53.288445908162075
At time: 1176.2591001987457 and batch: 150, loss is 3.9108641862869264 and perplexity is 49.942092601520045
At time: 1177.4226458072662 and batch: 200, loss is 3.9555130624771118 and perplexity is 52.22248046745371
At time: 1178.5867590904236 and batch: 250, loss is 3.9687944746017454 and perplexity is 52.92069512755146
At time: 1179.7503173351288 and batch: 300, loss is 3.9750762605667114 and perplexity is 53.25417794467489
At time: 1180.9138822555542 and batch: 350, loss is 3.950862350463867 and perplexity is 51.980172638888874
At time: 1182.0762536525726 and batch: 400, loss is 3.97345477104187 and perplexity is 53.16789682385477
At time: 1183.2852828502655 and batch: 450, loss is 3.8928087091445924 and perplexity is 49.0484560839834
At time: 1184.4480047225952 and batch: 500, loss is 3.913234052658081 and perplexity is 50.06058904218744
At time: 1185.6118137836456 and batch: 550, loss is 3.885375633239746 and perplexity is 48.68522681464581
At time: 1186.774908542633 and batch: 600, loss is 3.913513669967651 and perplexity is 50.07458880660792
At time: 1187.9377670288086 and batch: 650, loss is 3.9417994832992553 and perplexity is 51.51121151511513
At time: 1189.1004440784454 and batch: 700, loss is 3.904896740913391 and perplexity is 49.644953354961714
At time: 1190.2642595767975 and batch: 750, loss is 3.910526089668274 and perplexity is 49.92521020298479
At time: 1191.4273009300232 and batch: 800, loss is 3.9456843090057374 and perplexity is 51.711712797888275
At time: 1192.5905339717865 and batch: 850, loss is 3.9690911769866943 and perplexity is 52.93639915360475
At time: 1193.7530162334442 and batch: 900, loss is 3.901000876426697 and perplexity is 49.451919605059615
At time: 1194.9163773059845 and batch: 950, loss is 3.896922245025635 and perplexity is 49.25063421640472
At time: 1196.0788223743439 and batch: 1000, loss is 3.88607506275177 and perplexity is 48.719290610302856
At time: 1197.2434587478638 and batch: 1050, loss is 3.891413617134094 and perplexity is 48.98007668364137
At time: 1198.406542301178 and batch: 1100, loss is 3.837343316078186 and perplexity is 46.40203500227737
At time: 1199.5705230236053 and batch: 1150, loss is 3.84297146320343 and perplexity is 46.663928779222054
At time: 1200.7342298030853 and batch: 1200, loss is 3.853820915222168 and perplexity is 47.172963214639836
At time: 1201.8968262672424 and batch: 1250, loss is 3.916315851211548 and perplexity is 50.21510366225802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4008704415203015 and perplexity of 81.52179774824988
Finished 39 epochs...
Completing Train Step...
At time: 1204.8277461528778 and batch: 50, loss is 3.9700786161422728 and perplexity is 52.98869644283059
At time: 1205.9886348247528 and batch: 100, loss is 3.9755835008621214 and perplexity is 53.281197461741954
At time: 1207.1499211788177 and batch: 150, loss is 3.9106653881073 and perplexity is 49.932165191232365
At time: 1208.3098933696747 and batch: 200, loss is 3.9553276538848876 and perplexity is 52.21279886842134
At time: 1209.4708168506622 and batch: 250, loss is 3.968625297546387 and perplexity is 52.91174291745788
At time: 1210.6304914951324 and batch: 300, loss is 3.974891772270203 and perplexity is 53.24435407832596
At time: 1211.8204672336578 and batch: 350, loss is 3.950687665939331 and perplexity is 51.97109330017921
At time: 1212.9812414646149 and batch: 400, loss is 3.9733151626586913 and perplexity is 53.160474657852525
At time: 1214.3070306777954 and batch: 450, loss is 3.8926804494857787 and perplexity is 49.04216554916028
At time: 1215.466415643692 and batch: 500, loss is 3.913104043006897 and perplexity is 50.05408110552444
At time: 1216.6268634796143 and batch: 550, loss is 3.885291795730591 and perplexity is 48.681145337589825
At time: 1217.7870683670044 and batch: 600, loss is 3.913436632156372 and perplexity is 50.07073131847369
At time: 1218.949333190918 and batch: 650, loss is 3.9417217063903807 and perplexity is 51.50720528810909
At time: 1220.1097435951233 and batch: 700, loss is 3.9048514986038207 and perplexity is 49.64270735342096
At time: 1221.2711985111237 and batch: 750, loss is 3.910498023033142 and perplexity is 49.9238089899899
At time: 1222.4317111968994 and batch: 800, loss is 3.9457197999954223 and perplexity is 51.713548130322465
At time: 1223.5924844741821 and batch: 850, loss is 3.9691099166870116 and perplexity is 52.937391175155824
At time: 1224.752247095108 and batch: 900, loss is 3.9010227489471436 and perplexity is 49.45300125501147
At time: 1225.912609577179 and batch: 950, loss is 3.8969905853271483 and perplexity is 49.25400013460941
At time: 1227.0762910842896 and batch: 1000, loss is 3.8861410236358642 and perplexity is 48.72250428377116
At time: 1228.2472460269928 and batch: 1050, loss is 3.891491651535034 and perplexity is 48.98389896371611
At time: 1229.4116773605347 and batch: 1100, loss is 3.837445607185364 and perplexity is 46.40678176058451
At time: 1230.5819790363312 and batch: 1150, loss is 3.843088302612305 and perplexity is 46.669381283603876
At time: 1231.7429537773132 and batch: 1200, loss is 3.853938617706299 and perplexity is 47.178515916371
At time: 1232.9096159934998 and batch: 1250, loss is 3.9163913440704348 and perplexity is 50.218894687088635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4008463838674725 and perplexity of 81.51983654873268
Finished 40 epochs...
Completing Train Step...
At time: 1235.9404952526093 and batch: 50, loss is 3.9699757289886475 and perplexity is 52.98324486713263
At time: 1237.0998139381409 and batch: 100, loss is 3.9754595279693605 and perplexity is 53.274592446992706
At time: 1238.2588338851929 and batch: 150, loss is 3.9104932880401613 and perplexity is 49.923572601664404
At time: 1239.418425321579 and batch: 200, loss is 3.9551611328125 and perplexity is 52.20410506103257
At time: 1240.5778620243073 and batch: 250, loss is 3.9684749746322634 and perplexity is 52.903789667863975
At time: 1241.766999721527 and batch: 300, loss is 3.974733004570007 and perplexity is 53.23590126571511
At time: 1242.9277529716492 and batch: 350, loss is 3.9505346775054933 and perplexity is 51.96314293218306
At time: 1244.0868999958038 and batch: 400, loss is 3.973190312385559 and perplexity is 53.153837972376266
At time: 1245.2463569641113 and batch: 450, loss is 3.892565007209778 and perplexity is 49.03650433672719
At time: 1246.406343460083 and batch: 500, loss is 3.912989225387573 and perplexity is 50.048334345015455
At time: 1247.5672769546509 and batch: 550, loss is 3.885218744277954 and perplexity is 48.67758923909755
At time: 1248.7272691726685 and batch: 600, loss is 3.9133709907531737 and perplexity is 50.06744471328064
At time: 1249.8873555660248 and batch: 650, loss is 3.9416555976867675 and perplexity is 51.50380032609081
At time: 1251.0468542575836 and batch: 700, loss is 3.9048126935958862 and perplexity is 49.64078100514444
At time: 1252.2077157497406 and batch: 750, loss is 3.9104723262786867 and perplexity is 49.92252612661159
At time: 1253.3678781986237 and batch: 800, loss is 3.9457468605041504 and perplexity is 51.71494754417734
At time: 1254.527682542801 and batch: 850, loss is 3.9691205835342407 and perplexity is 52.937955853231855
At time: 1255.6884970664978 and batch: 900, loss is 3.901040916442871 and perplexity is 49.4538997003617
At time: 1256.8529284000397 and batch: 950, loss is 3.8970493364334104 and perplexity is 49.25689394661166
At time: 1258.0138669013977 and batch: 1000, loss is 3.886199407577515 and perplexity is 48.72534897865978
At time: 1259.1735265254974 and batch: 1050, loss is 3.891559295654297 and perplexity is 48.98721254849057
At time: 1260.3335366249084 and batch: 1100, loss is 3.837533850669861 and perplexity is 46.41087703739944
At time: 1261.4944269657135 and batch: 1150, loss is 3.8431895780563354 and perplexity is 46.67410798526137
At time: 1262.6552555561066 and batch: 1200, loss is 3.8540398597717287 and perplexity is 47.18329260856329
At time: 1263.8235268592834 and batch: 1250, loss is 3.9164533376693726 and perplexity is 50.22200803360775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400831681968522 and perplexity of 81.5186380611433
Finished 41 epochs...
Completing Train Step...
At time: 1266.7524991035461 and batch: 50, loss is 3.969876742362976 and perplexity is 52.9780004940718
At time: 1267.9158432483673 and batch: 100, loss is 3.9753442430496215 and perplexity is 53.26845104389071
At time: 1269.0793616771698 and batch: 150, loss is 3.910340418815613 and perplexity is 49.915941407136366
At time: 1270.2434401512146 and batch: 200, loss is 3.955009727478027 and perplexity is 52.196201679367114
At time: 1271.4109489917755 and batch: 250, loss is 3.968339262008667 and perplexity is 52.896610442936705
At time: 1272.5741937160492 and batch: 300, loss is 3.974592719078064 and perplexity is 53.2284335649343
At time: 1273.7378060817719 and batch: 350, loss is 3.950398678779602 and perplexity is 51.95607649147534
At time: 1274.9015588760376 and batch: 400, loss is 3.973076820373535 and perplexity is 53.14780577866737
At time: 1276.065843820572 and batch: 450, loss is 3.8924599075317383 and perplexity is 49.031350886726926
At time: 1277.2291104793549 and batch: 500, loss is 3.9128885221481324 and perplexity is 50.04329456938343
At time: 1278.393039226532 and batch: 550, loss is 3.8851360845565797 and perplexity is 48.67356572942726
At time: 1279.5564799308777 and batch: 600, loss is 3.913312540054321 and perplexity is 50.06451832167304
At time: 1280.720359325409 and batch: 650, loss is 3.941598262786865 and perplexity is 51.500847445506885
At time: 1281.8841094970703 and batch: 700, loss is 3.904778118133545 and perplexity is 49.63906468186171
At time: 1283.048359632492 and batch: 750, loss is 3.910448966026306 and perplexity is 49.92135993742309
At time: 1284.2133600711823 and batch: 800, loss is 3.9457671070098876 and perplexity is 51.71599460175909
At time: 1285.3915116786957 and batch: 850, loss is 3.9691249656677248 and perplexity is 52.93818783492907
At time: 1286.565841436386 and batch: 900, loss is 3.901055860519409 and perplexity is 49.45463874874611
At time: 1287.7319672107697 and batch: 950, loss is 3.897099666595459 and perplexity is 49.25937311645399
At time: 1288.8957860469818 and batch: 1000, loss is 3.886250548362732 and perplexity is 48.72784089498527
At time: 1290.0629947185516 and batch: 1050, loss is 3.8916181325912476 and perplexity is 48.99009489081993
At time: 1291.2310888767242 and batch: 1100, loss is 3.837610354423523 and perplexity is 46.414427779524374
At time: 1292.3938603401184 and batch: 1150, loss is 3.8432779026031496 and perplexity is 46.678230636760105
At time: 1293.604502916336 and batch: 1200, loss is 3.8541278934478758 and perplexity is 47.18744651010328
At time: 1294.7674827575684 and batch: 1250, loss is 3.916504464149475 and perplexity is 50.224575773741385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400818316605839 and perplexity of 81.51754854226117
Finished 42 epochs...
Completing Train Step...
At time: 1297.689034461975 and batch: 50, loss is 3.969781002998352 and perplexity is 52.97292865675655
At time: 1298.8759407997131 and batch: 100, loss is 3.97523579120636 and perplexity is 53.26267429544249
At time: 1300.0443081855774 and batch: 150, loss is 3.910201668739319 and perplexity is 49.90901604691607
At time: 1301.2068226337433 and batch: 200, loss is 3.9548704528808596 and perplexity is 52.188932580616694
At time: 1302.368673324585 and batch: 250, loss is 3.9682151746749876 and perplexity is 52.890047050811496
At time: 1303.5305235385895 and batch: 300, loss is 3.9744667387008668 and perplexity is 53.22172824917414
At time: 1304.6925978660583 and batch: 350, loss is 3.950275950431824 and perplexity is 51.94970039932214
At time: 1305.8555743694305 and batch: 400, loss is 3.9729720830917357 and perplexity is 53.14223951345933
At time: 1307.017417192459 and batch: 450, loss is 3.8923634099960327 and perplexity is 49.02661971047113
At time: 1308.1795086860657 and batch: 500, loss is 3.9127993059158324 and perplexity is 50.038830094344846
At time: 1309.3462131023407 and batch: 550, loss is 3.8850642681121825 and perplexity is 48.67007029251687
At time: 1310.5088744163513 and batch: 600, loss is 3.913258657455444 and perplexity is 50.061820787990065
At time: 1311.669409275055 and batch: 650, loss is 3.9415476179122924 and perplexity is 51.498239257593845
At time: 1312.829463005066 and batch: 700, loss is 3.904746379852295 and perplexity is 49.63748924826675
At time: 1313.9890723228455 and batch: 750, loss is 3.9104276943206786 and perplexity is 49.92029803624423
At time: 1315.156502008438 and batch: 800, loss is 3.9457810926437378 and perplexity is 51.716717887781584
At time: 1316.316156387329 and batch: 850, loss is 3.9691253757476805 and perplexity is 52.93820954382324
At time: 1317.476957321167 and batch: 900, loss is 3.9010678339004516 and perplexity is 49.45523089152515
At time: 1318.6372570991516 and batch: 950, loss is 3.897142996788025 and perplexity is 49.26150758081985
At time: 1319.7974486351013 and batch: 1000, loss is 3.8862957096099855 and perplexity is 48.73004155474795
At time: 1320.9737768173218 and batch: 1050, loss is 3.8916695880889893 and perplexity is 48.99261576539283
At time: 1322.1340980529785 and batch: 1100, loss is 3.837677073478699 and perplexity is 46.41752460959999
At time: 1323.3219633102417 and batch: 1150, loss is 3.8433553409576415 and perplexity is 46.68184546209247
At time: 1324.481736421585 and batch: 1200, loss is 3.854204888343811 and perplexity is 47.19107984250901
At time: 1325.6417553424835 and batch: 1250, loss is 3.916547179222107 and perplexity is 50.226721165963426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400808960851962 and perplexity of 81.51678588770793
Finished 43 epochs...
Completing Train Step...
At time: 1328.540484905243 and batch: 50, loss is 3.9696881914138795 and perplexity is 52.96801238346086
At time: 1329.7046995162964 and batch: 100, loss is 3.975132718086243 and perplexity is 53.25718462834048
At time: 1330.8684775829315 and batch: 150, loss is 3.910073823928833 and perplexity is 49.902635846064484
At time: 1332.0322625637054 and batch: 200, loss is 3.954741244316101 and perplexity is 52.18218975916619
At time: 1333.1964354515076 and batch: 250, loss is 3.9681004428863527 and perplexity is 52.88397922920504
At time: 1334.3593230247498 and batch: 300, loss is 3.9743518733978274 and perplexity is 53.21561527032183
At time: 1335.5231249332428 and batch: 350, loss is 3.9501639366149903 and perplexity is 51.943881640993794
At time: 1336.6867084503174 and batch: 400, loss is 3.9728743839263916 and perplexity is 53.137047814630805
At time: 1337.8506491184235 and batch: 450, loss is 3.8922735834121704 and perplexity is 49.022216014491676
At time: 1339.0146610736847 and batch: 500, loss is 3.912718639373779 and perplexity is 50.03479379775198
At time: 1340.1787464618683 and batch: 550, loss is 3.8849994373321532 and perplexity is 48.6669150761744
At time: 1341.3425030708313 and batch: 600, loss is 3.9132074737548828 and perplexity is 50.05925850431943
At time: 1342.5072917938232 and batch: 650, loss is 3.9415014934539796 and perplexity is 51.495863983983554
At time: 1343.6703560352325 and batch: 700, loss is 3.9047167110443115 and perplexity is 49.636016584975664
At time: 1344.8341941833496 and batch: 750, loss is 3.9104078817367554 and perplexity is 49.919308995947674
At time: 1345.9981980323792 and batch: 800, loss is 3.9457901096343995 and perplexity is 51.71718421904628
At time: 1347.1632022857666 and batch: 850, loss is 3.9691232109069823 and perplexity is 52.93809494115678
At time: 1348.3285570144653 and batch: 900, loss is 3.9010771322250366 and perplexity is 49.455690744452326
At time: 1349.493011713028 and batch: 950, loss is 3.89718017578125 and perplexity is 49.26333910812341
At time: 1350.6573417186737 and batch: 1000, loss is 3.8863353061676027 and perplexity is 48.73197113484817
At time: 1351.8673079013824 and batch: 1050, loss is 3.8917147588729857 and perplexity is 48.99482885024
At time: 1353.0312719345093 and batch: 1100, loss is 3.8377354955673217 and perplexity is 46.42023649755269
At time: 1354.1950373649597 and batch: 1150, loss is 3.843423671722412 and perplexity is 46.68503537727722
At time: 1355.3580882549286 and batch: 1200, loss is 3.854272999763489 and perplexity is 47.194294203419304
At time: 1356.521992444992 and batch: 1250, loss is 3.9165829372406007 and perplexity is 50.228517206098985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400801832658531 and perplexity of 81.51620482236122
Finished 44 epochs...
Completing Train Step...
At time: 1359.424766778946 and batch: 50, loss is 3.9695979881286623 and perplexity is 52.96323471021659
At time: 1360.587070941925 and batch: 100, loss is 3.975034112930298 and perplexity is 53.2519334542457
At time: 1361.7478137016296 and batch: 150, loss is 3.909955134391785 and perplexity is 49.89671327679889
At time: 1362.907987356186 and batch: 200, loss is 3.9546204662322997 and perplexity is 52.17588767486302
At time: 1364.0676352977753 and batch: 250, loss is 3.967993531227112 and perplexity is 52.87832561746229
At time: 1365.2277896404266 and batch: 300, loss is 3.974246063232422 and perplexity is 53.209984815152865
At time: 1366.3877668380737 and batch: 350, loss is 3.9500606870651245 and perplexity is 51.93851873545964
At time: 1367.547486782074 and batch: 400, loss is 3.9727823877334596 and perplexity is 53.13215963337869
At time: 1368.7074222564697 and batch: 450, loss is 3.89218955039978 and perplexity is 49.01809670308742
At time: 1369.8673746585846 and batch: 500, loss is 3.9126439619064333 and perplexity is 50.03105746558364
At time: 1371.02725481987 and batch: 550, loss is 3.8849383115768434 and perplexity is 48.66394036514843
At time: 1372.1869072914124 and batch: 600, loss is 3.9131582880020144 and perplexity is 50.056796362553506
At time: 1373.346443414688 and batch: 650, loss is 3.9414591693878176 and perplexity is 51.49368451575153
At time: 1374.5075294971466 and batch: 700, loss is 3.904688415527344 and perplexity is 49.63461212809619
At time: 1375.6677372455597 and batch: 750, loss is 3.910389461517334 and perplexity is 49.91838947979148
At time: 1376.8279421329498 and batch: 800, loss is 3.9457949352264405 and perplexity is 51.71743378568099
At time: 1377.9875130653381 and batch: 850, loss is 3.96911904335022 and perplexity is 52.93787431910095
At time: 1379.1479942798615 and batch: 900, loss is 3.9010841989517213 and perplexity is 49.456040235536705
At time: 1380.3071801662445 and batch: 950, loss is 3.8972119808197023 and perplexity is 49.26490595543471
At time: 1381.4947609901428 and batch: 1000, loss is 3.8863704919815065 and perplexity is 48.73368583908214
At time: 1382.6549272537231 and batch: 1050, loss is 3.8917544794082644 and perplexity is 48.99677498971841
At time: 1383.8147959709167 and batch: 1100, loss is 3.83778694152832 and perplexity is 46.42262469266005
At time: 1384.9745576381683 and batch: 1150, loss is 3.843484411239624 and perplexity is 46.68787108990609
At time: 1386.1339366436005 and batch: 1200, loss is 3.854333481788635 and perplexity is 47.197148696229966
At time: 1387.297045469284 and batch: 1250, loss is 3.9166131019592285 and perplexity is 50.23003235803955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400796932025547 and perplexity of 81.515805342338
Finished 45 epochs...
Completing Train Step...
At time: 1390.200710773468 and batch: 50, loss is 3.969509916305542 and perplexity is 52.95857034697982
At time: 1391.41304063797 and batch: 100, loss is 3.974939022064209 and perplexity is 53.246869922524255
At time: 1392.5817964076996 and batch: 150, loss is 3.9098441123962404 and perplexity is 49.89117395161895
At time: 1393.7510161399841 and batch: 200, loss is 3.954506688117981 and perplexity is 52.16995153845808
At time: 1394.9202024936676 and batch: 250, loss is 3.967892804145813 and perplexity is 52.87299960630015
At time: 1396.0896017551422 and batch: 300, loss is 3.9741474914550783 and perplexity is 53.20474007087334
At time: 1397.2583813667297 and batch: 350, loss is 3.949964303970337 and perplexity is 51.93351298152413
At time: 1398.4214618206024 and batch: 400, loss is 3.9726951456069948 and perplexity is 53.127524472982174
At time: 1399.5917479991913 and batch: 450, loss is 3.892109851837158 and perplexity is 49.014190186911655
At time: 1400.7623133659363 and batch: 500, loss is 3.9125737857818605 and perplexity is 50.02754660305324
At time: 1401.963011264801 and batch: 550, loss is 3.8848795318603515 and perplexity is 48.66107999659703
At time: 1403.1387186050415 and batch: 600, loss is 3.913110852241516 and perplexity is 50.054421936666735
At time: 1404.331464767456 and batch: 650, loss is 3.9414194107055662 and perplexity is 51.49163723540977
At time: 1405.4993109703064 and batch: 700, loss is 3.9046612215042114 and perplexity is 49.633262381658405
At time: 1406.6627020835876 and batch: 750, loss is 3.9103717947006227 and perplexity is 49.91750758854414
At time: 1407.8366770744324 and batch: 800, loss is 3.9457962560653685 and perplexity is 51.7175020961259
At time: 1408.9999663829803 and batch: 850, loss is 3.969113144874573 and perplexity is 52.93756206725938
At time: 1410.164371728897 and batch: 900, loss is 3.901089086532593 and perplexity is 49.456281956523654
At time: 1411.3806545734406 and batch: 950, loss is 3.897239508628845 and perplexity is 49.26626212902946
At time: 1412.5445539951324 and batch: 1000, loss is 3.8864015340805054 and perplexity is 48.73519865846296
At time: 1413.7077946662903 and batch: 1050, loss is 3.8917898035049436 and perplexity is 48.998505787104364
At time: 1414.8712904453278 and batch: 1100, loss is 3.8378323793411253 and perplexity is 46.42473408311344
At time: 1416.0342795848846 and batch: 1150, loss is 3.843538522720337 and perplexity is 46.69039750809561
At time: 1417.2048270702362 and batch: 1200, loss is 3.8543875932693483 and perplexity is 47.199702672930464
At time: 1418.3685727119446 and batch: 1250, loss is 3.9166389417648317 and perplexity is 50.231330309080455
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400794704465101 and perplexity of 81.5156237611565
Finished 46 epochs...
Completing Train Step...
At time: 1421.3146562576294 and batch: 50, loss is 3.969423966407776 and perplexity is 52.95401875887973
At time: 1422.4754581451416 and batch: 100, loss is 3.9748471355438233 and perplexity is 53.24197747770395
At time: 1423.6366229057312 and batch: 150, loss is 3.909739446640015 and perplexity is 49.88595232743571
At time: 1424.7978990077972 and batch: 200, loss is 3.954399137496948 and perplexity is 52.1643409294885
At time: 1425.9583597183228 and batch: 250, loss is 3.9677972745895387 and perplexity is 52.867948913357935
At time: 1427.1190285682678 and batch: 300, loss is 3.974054946899414 and perplexity is 53.19981648967309
At time: 1428.279409646988 and batch: 350, loss is 3.9498738861083984 and perplexity is 51.92881747659934
At time: 1429.4403069019318 and batch: 400, loss is 3.9726115608215333 and perplexity is 53.12308400582733
At time: 1430.6014339923859 and batch: 450, loss is 3.8920340299606324 and perplexity is 49.01047397992196
At time: 1431.7622685432434 and batch: 500, loss is 3.9125073909759522 and perplexity is 50.02422514407149
At time: 1432.922259569168 and batch: 550, loss is 3.8848226737976073 and perplexity is 48.65831330051262
At time: 1434.0823092460632 and batch: 600, loss is 3.913064594268799 and perplexity is 50.05210657413482
At time: 1435.2431881427765 and batch: 650, loss is 3.941381640434265 and perplexity is 51.48969241903
At time: 1436.4037945270538 and batch: 700, loss is 3.9046348333358765 and perplexity is 49.631952668056215
At time: 1437.5674831867218 and batch: 750, loss is 3.9103547859191896 and perplexity is 49.916658559788374
At time: 1438.72811961174 and batch: 800, loss is 3.9457946157455446 and perplexity is 51.71741726295154
At time: 1439.8882529735565 and batch: 850, loss is 3.9691054391860963 and perplexity is 52.93715414846903
At time: 1441.095564365387 and batch: 900, loss is 3.9010921239852907 and perplexity is 49.456432177868855
At time: 1442.2566375732422 and batch: 950, loss is 3.8972630739212035 and perplexity is 49.26742311657938
At time: 1443.4206237792969 and batch: 1000, loss is 3.886429009437561 and perplexity is 48.736537693842436
At time: 1444.5811848640442 and batch: 1050, loss is 3.891820878982544 and perplexity is 49.000028462732224
At time: 1445.7429602146149 and batch: 1100, loss is 3.837872681617737 and perplexity is 46.426605143291816
At time: 1446.9036982059479 and batch: 1150, loss is 3.8435873413085937 and perplexity is 46.69267692302555
At time: 1448.0643272399902 and batch: 1200, loss is 3.8544364833831786 and perplexity is 47.2020103281772
At time: 1449.2247676849365 and batch: 1250, loss is 3.916660876274109 and perplexity is 50.23243212074494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400791140368385 and perplexity of 81.51533323210734
Finished 47 epochs...
Completing Train Step...
At time: 1452.1228568553925 and batch: 50, loss is 3.9693397951126097 and perplexity is 52.94956173811577
At time: 1453.3048686981201 and batch: 100, loss is 3.9747577810287478 and perplexity is 53.237220279166515
At time: 1454.4642918109894 and batch: 150, loss is 3.909639983177185 and perplexity is 49.88099074462285
At time: 1455.6245081424713 and batch: 200, loss is 3.954296760559082 and perplexity is 52.15900077735723
At time: 1456.783378124237 and batch: 250, loss is 3.967706322669983 and perplexity is 52.863140690583144
At time: 1457.943999528885 and batch: 300, loss is 3.97396737575531 and perplexity is 53.19515792485786
At time: 1459.118301153183 and batch: 350, loss is 3.949788179397583 and perplexity is 51.92436701917664
At time: 1460.2883117198944 and batch: 400, loss is 3.9725314664840696 and perplexity is 53.11882931800034
At time: 1461.4525501728058 and batch: 450, loss is 3.8919613552093506 and perplexity is 49.006912285339475
At time: 1462.61377120018 and batch: 500, loss is 3.9124434089660642 and perplexity is 50.021024595993524
At time: 1463.7740581035614 and batch: 550, loss is 3.884767141342163 and perplexity is 48.65561125992343
At time: 1464.9346044063568 and batch: 600, loss is 3.913019199371338 and perplexity is 50.0498345154595
At time: 1466.0950133800507 and batch: 650, loss is 3.94134539604187 and perplexity is 51.48782624023313
At time: 1467.2609627246857 and batch: 700, loss is 3.9046090126037596 and perplexity is 49.63067115124686
At time: 1468.4357132911682 and batch: 750, loss is 3.9103380727767942 and perplexity is 49.915824302537516
At time: 1469.640120267868 and batch: 800, loss is 3.94579071521759 and perplexity is 51.717215538113194
At time: 1470.8131620883942 and batch: 850, loss is 3.9690965890884398 and perplexity is 52.93668565155828
At time: 1471.9745705127716 and batch: 900, loss is 3.9010934972763063 and perplexity is 49.45650009598946
At time: 1473.1372380256653 and batch: 950, loss is 3.8972832489013673 and perplexity is 49.26841709589021
At time: 1474.298666715622 and batch: 1000, loss is 3.8864534759521483 and perplexity is 48.73773012164007
At time: 1475.4588010311127 and batch: 1050, loss is 3.891848530769348 and perplexity is 49.00138341980607
At time: 1476.6189908981323 and batch: 1100, loss is 3.8379086542129515 and perplexity is 46.42827525880484
At time: 1477.7794370651245 and batch: 1150, loss is 3.843631157875061 and perplexity is 46.69472288063057
At time: 1478.9397540092468 and batch: 1200, loss is 3.8544807386398316 and perplexity is 47.20409931148272
At time: 1480.1004700660706 and batch: 1250, loss is 3.91667977809906 and perplexity is 50.233381614357306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400788467295849 and perplexity of 81.51511533599998
Finished 48 epochs...
Completing Train Step...
At time: 1483.028770685196 and batch: 50, loss is 3.9692573881149293 and perplexity is 52.94519850348744
At time: 1484.1927528381348 and batch: 100, loss is 3.9746708059310913 and perplexity is 53.232590168088876
At time: 1485.357295036316 and batch: 150, loss is 3.9095454263687133 and perplexity is 49.87627438032033
At time: 1486.5210812091827 and batch: 200, loss is 3.954198980331421 and perplexity is 52.153900907723894
At time: 1487.6869642734528 and batch: 250, loss is 3.9676192140579225 and perplexity is 52.858536056323
At time: 1488.8505599498749 and batch: 300, loss is 3.9738839054107666 and perplexity is 53.19071789200594
At time: 1490.015861749649 and batch: 350, loss is 3.9497065114974976 and perplexity is 51.92012663831279
At time: 1491.1869769096375 and batch: 400, loss is 3.972454113960266 and perplexity is 53.114720601402944
At time: 1492.3509483337402 and batch: 450, loss is 3.8918911695480345 and perplexity is 49.00347282349353
At time: 1493.5156545639038 and batch: 500, loss is 3.9123820447921753 and perplexity is 50.01795519131882
At time: 1494.6799018383026 and batch: 550, loss is 3.884712886810303 and perplexity is 48.65297154412108
At time: 1495.8432357311249 and batch: 600, loss is 3.9129745244979857 and perplexity is 50.04759859538632
At time: 1497.0076324939728 and batch: 650, loss is 3.9413103580474855 and perplexity is 51.48602224167088
At time: 1498.1773002147675 and batch: 700, loss is 3.9045836114883423 and perplexity is 49.62941049285184
At time: 1499.4104940891266 and batch: 750, loss is 3.9103213119506837 and perplexity is 49.914987679097486
At time: 1500.5845656394958 and batch: 800, loss is 3.945784683227539 and perplexity is 51.716903581324466
At time: 1501.749594926834 and batch: 850, loss is 3.9690863609313967 and perplexity is 52.936144209593074
At time: 1502.9146928787231 and batch: 900, loss is 3.9010933303833006 and perplexity is 49.4564918420462
At time: 1504.0789296627045 and batch: 950, loss is 3.897300429344177 and perplexity is 49.269263556383706
At time: 1505.2432849407196 and batch: 1000, loss is 3.8864752721786497 and perplexity is 48.738792431822105
At time: 1506.408364057541 and batch: 1050, loss is 3.8918731689453123 and perplexity is 49.00259073938627
At time: 1507.5729298591614 and batch: 1100, loss is 3.8379405879974366 and perplexity is 46.42975791301423
At time: 1508.7381546497345 and batch: 1150, loss is 3.843671188354492 and perplexity is 46.69659213018762
At time: 1509.9026906490326 and batch: 1200, loss is 3.8545213747024536 and perplexity is 47.2060175391927
At time: 1511.0668301582336 and batch: 1250, loss is 3.9166961812973025 and perplexity is 50.23420560923237
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.400788021783759 and perplexity of 81.51507902003871
Finished 49 epochs...
Completing Train Step...
At time: 1513.9521334171295 and batch: 50, loss is 3.9691765785217283 and perplexity is 52.94092019640089
At time: 1515.1404774188995 and batch: 100, loss is 3.9745856809616087 and perplexity is 53.228058938338464
At time: 1516.302119731903 and batch: 150, loss is 3.909454388618469 and perplexity is 49.87173396318801
At time: 1517.4689626693726 and batch: 200, loss is 3.9541051626205443 and perplexity is 52.14900817764342
At time: 1518.6288793087006 and batch: 250, loss is 3.9675352287292482 and perplexity is 52.85409690121362
At time: 1519.7893300056458 and batch: 300, loss is 3.973804039955139 and perplexity is 53.18646996072009
At time: 1520.9492712020874 and batch: 350, loss is 3.9496281957626342 and perplexity is 51.91606063465902
At time: 1522.1115143299103 and batch: 400, loss is 3.9723790168762205 and perplexity is 53.11073199053428
At time: 1523.272049188614 and batch: 450, loss is 3.891823310852051 and perplexity is 49.00014762455216
At time: 1524.432154417038 and batch: 500, loss is 3.9123224210739136 and perplexity is 50.0149730237553
At time: 1525.5928313732147 and batch: 550, loss is 3.8846595001220705 and perplexity is 48.65037419243029
At time: 1526.7527656555176 and batch: 600, loss is 3.912930760383606 and perplexity is 50.04540835448429
At time: 1527.9159264564514 and batch: 650, loss is 3.941276216506958 and perplexity is 51.48426445956278
At time: 1529.1192698478699 and batch: 700, loss is 3.9045585680007933 and perplexity is 49.62816761489116
At time: 1530.282288312912 and batch: 750, loss is 3.9103046798706056 and perplexity is 49.91415749592916
At time: 1531.4466650485992 and batch: 800, loss is 3.9457772064208982 and perplexity is 51.716516905481875
At time: 1532.6067378520966 and batch: 850, loss is 3.9690750217437745 and perplexity is 52.93554396012507
At time: 1533.766942024231 and batch: 900, loss is 3.901091732978821 and perplexity is 49.45641284008767
At time: 1534.9279494285583 and batch: 950, loss is 3.8973150062561035 and perplexity is 49.269981755333795
At time: 1536.0887937545776 and batch: 1000, loss is 3.8864945697784425 and perplexity is 48.739732982608
At time: 1537.2491166591644 and batch: 1050, loss is 3.891895055770874 and perplexity is 49.00366326227888
At time: 1538.4092063903809 and batch: 1100, loss is 3.8379693460464477 and perplexity is 46.43109316146734
At time: 1539.569708108902 and batch: 1150, loss is 3.843707633018494 and perplexity is 46.69829400280973
At time: 1540.7300992012024 and batch: 1200, loss is 3.8545585346221922 and perplexity is 47.20777174360848
At time: 1541.8909845352173 and batch: 1250, loss is 3.9167103719711305 and perplexity is 50.23491847151716
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.40078713075958 and perplexity of 81.51500638816472
Finished Training.
Improved accuracyfrom -10000000 to -81.51500638816472
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9c69001898>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'anneal': 4.05052866350349, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 29.672411203532512, 'dropout': 0.25293993859191477, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6160762310028076 and batch: 50, loss is 7.694266700744629 and perplexity is 2195.7230977203135
At time: 2.7746167182922363 and batch: 100, loss is 6.4295323467254635 and perplexity is 619.883989440465
At time: 3.9338386058807373 and batch: 150, loss is 6.265949869155884 and perplexity is 526.3413039584942
At time: 5.0924232006073 and batch: 200, loss is 6.362090768814087 and perplexity is 579.4566005720637
At time: 6.249036550521851 and batch: 250, loss is 6.475020456314087 and perplexity is 648.7324981508849
At time: 7.402724266052246 and batch: 300, loss is 6.703150911331177 and perplexity is 814.9696810075716
At time: 8.583304405212402 and batch: 350, loss is 6.887097482681274 and perplexity is 979.5541142628624
At time: 9.738919019699097 and batch: 400, loss is 7.080263223648071 and perplexity is 1188.2812610751425
At time: 10.901428937911987 and batch: 450, loss is 7.039201803207398 and perplexity is 1140.4769182048817
At time: 12.063513994216919 and batch: 500, loss is 6.96000244140625 and perplexity is 1053.636129593011
At time: 13.224640607833862 and batch: 550, loss is 6.923007211685181 and perplexity is 1015.3688370113451
At time: 14.385899066925049 and batch: 600, loss is 6.8741531085968015 and perplexity is 966.9561119057503
At time: 15.547714233398438 and batch: 650, loss is 6.807942228317261 and perplexity is 905.0065926027002
At time: 16.710376977920532 and batch: 700, loss is 6.878520460128784 and perplexity is 971.1883843460946
At time: 17.872215509414673 and batch: 750, loss is 6.6812226104736325 and perplexity is 797.2932955047294
At time: 19.033499240875244 and batch: 800, loss is 6.664291982650757 and perplexity is 783.9082478128107
At time: 20.19588851928711 and batch: 850, loss is 6.775448293685913 and perplexity is 876.0720125683557
At time: 21.367576837539673 and batch: 900, loss is 7.0070194911956785 and perplexity is 1104.358045899561
At time: 22.530097723007202 and batch: 950, loss is 6.810426368713379 and perplexity is 907.2575507285964
At time: 23.69290566444397 and batch: 1000, loss is 6.91608811378479 and perplexity is 1008.3676495051111
At time: 24.8551025390625 and batch: 1050, loss is 7.082886228561401 and perplexity is 1191.4022200172906
At time: 26.01749539375305 and batch: 1100, loss is 7.026813535690308 and perplexity is 1126.4355388033741
At time: 27.180023193359375 and batch: 1150, loss is 7.035059852600098 and perplexity is 1135.7628885193633
At time: 28.343031406402588 and batch: 1200, loss is 6.869247674942017 and perplexity is 962.2243879191483
At time: 29.5048406124115 and batch: 1250, loss is 6.833550567626953 and perplexity is 928.4816036577935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.451902960338732 and perplexity of 633.9074464978515
Finished 1 epochs...
Completing Train Step...
At time: 32.430055379867554 and batch: 50, loss is 6.887732715606689 and perplexity is 980.1765569655865
At time: 33.58618879318237 and batch: 100, loss is 6.843663692474365 and perplexity is 937.9190948624642
At time: 34.740506410598755 and batch: 150, loss is 6.623429555892944 and perplexity is 752.5214911574941
At time: 35.894641160964966 and batch: 200, loss is 6.468496856689453 and perplexity is 644.5142012693877
At time: 37.04827642440796 and batch: 250, loss is 6.543170957565308 and perplexity is 694.4852735412317
At time: 38.20596957206726 and batch: 300, loss is 6.578132705688477 and perplexity is 719.1951256608606
At time: 39.37210011482239 and batch: 350, loss is 6.599451932907105 and perplexity is 734.6924181414787
At time: 40.529144525527954 and batch: 400, loss is 6.55545202255249 and perplexity is 703.0668800926293
At time: 41.68367624282837 and batch: 450, loss is 6.587690715789795 and perplexity is 726.1021560871505
At time: 42.83860468864441 and batch: 500, loss is 6.558283777236938 and perplexity is 705.0606145750063
At time: 43.99324440956116 and batch: 550, loss is 6.415426006317139 and perplexity is 611.2010808866587
At time: 45.17586016654968 and batch: 600, loss is 6.475637845993042 and perplexity is 649.1331425637825
At time: 46.33208513259888 and batch: 650, loss is 6.543254661560058 and perplexity is 694.5434071658968
At time: 47.48868942260742 and batch: 700, loss is 6.7374020767211915 and perplexity is 843.3668848938544
At time: 48.642216205596924 and batch: 750, loss is 6.603922805786133 and perplexity is 737.9844882780369
At time: 49.79660606384277 and batch: 800, loss is 6.558448810577392 and perplexity is 705.176982685497
At time: 50.96805715560913 and batch: 850, loss is 6.667478904724121 and perplexity is 786.4104874147412
At time: 52.12980079650879 and batch: 900, loss is 6.614322233200073 and perplexity is 745.6991488960323
At time: 53.28780245780945 and batch: 950, loss is 6.675549297332764 and perplexity is 792.7828077769195
At time: 54.442476987838745 and batch: 1000, loss is 6.781252536773682 and perplexity is 881.1717331836252
At time: 55.596537828445435 and batch: 1050, loss is 6.999041156768799 and perplexity is 1095.5821630977903
At time: 56.751431703567505 and batch: 1100, loss is 6.87372857093811 and perplexity is 966.5456897479396
At time: 57.91061592102051 and batch: 1150, loss is 6.845764369964599 and perplexity is 939.8914312887682
At time: 59.07436203956604 and batch: 1200, loss is 6.844746198654175 and perplexity is 938.9349478131629
At time: 60.23874354362488 and batch: 1250, loss is 6.8773957633972165 and perplexity is 970.0967059631148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.822943694400092 and perplexity of 918.6853625822062
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 63.31933879852295 and batch: 50, loss is 6.202585201263428 and perplexity is 494.0245445430312
At time: 64.48631930351257 and batch: 100, loss is 6.058352699279785 and perplexity is 427.6703545943304
At time: 65.65339064598083 and batch: 150, loss is 5.935853910446167 and perplexity is 378.36294633908074
At time: 66.82607746124268 and batch: 200, loss is 5.973779535293579 and perplexity is 392.988180083509
At time: 67.98861622810364 and batch: 250, loss is 5.990450582504272 and perplexity is 399.59461971387856
At time: 69.14684343338013 and batch: 300, loss is 5.987949123382569 and perplexity is 398.59629925688387
At time: 70.3047571182251 and batch: 350, loss is 6.024920330047608 and perplexity is 413.6086885258204
At time: 71.46329665184021 and batch: 400, loss is 6.007647552490234 and perplexity is 406.52586377647634
At time: 72.67960119247437 and batch: 450, loss is 6.007689132690429 and perplexity is 406.5427675547054
At time: 73.83856749534607 and batch: 500, loss is 5.972394609451294 and perplexity is 392.44429730278955
At time: 74.99878215789795 and batch: 550, loss is 5.965607566833496 and perplexity is 389.78977948445464
At time: 76.16133904457092 and batch: 600, loss is 5.967149438858033 and perplexity is 390.39124901625473
At time: 77.33589601516724 and batch: 650, loss is 5.956414623260498 and perplexity is 386.22288428933933
At time: 78.49952626228333 and batch: 700, loss is 6.00046103477478 and perplexity is 403.61483107728554
At time: 79.65926218032837 and batch: 750, loss is 5.931077966690063 and perplexity is 376.56021448757383
At time: 80.8205840587616 and batch: 800, loss is 5.9680963802337645 and perplexity is 390.7611017294533
At time: 81.98692774772644 and batch: 850, loss is 5.94071813583374 and perplexity is 380.2078724188342
At time: 83.14686703681946 and batch: 900, loss is 5.895507078170777 and perplexity is 363.4010619321954
At time: 84.30481243133545 and batch: 950, loss is 5.874142150878907 and perplexity is 355.71937619126805
At time: 85.48039889335632 and batch: 1000, loss is 5.891271276473999 and perplexity is 361.86502257315
At time: 86.64869689941406 and batch: 1050, loss is 5.897054891586304 and perplexity is 363.96397450050137
At time: 87.8111617565155 and batch: 1100, loss is 5.863282499313354 and perplexity is 351.87728734251044
At time: 88.97151160240173 and batch: 1150, loss is 5.890810546875 and perplexity is 361.69833904737345
At time: 90.13007998466492 and batch: 1200, loss is 5.893820610046387 and perplexity is 362.78871412254347
At time: 91.28977465629578 and batch: 1250, loss is 5.888653945922852 and perplexity is 360.9191405770747
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.578234150461907 and perplexity of 264.6039422468336
Finished 3 epochs...
Completing Train Step...
At time: 94.1897566318512 and batch: 50, loss is 5.868935613632202 and perplexity is 353.8721230787685
At time: 95.37193727493286 and batch: 100, loss is 5.881542510986328 and perplexity is 358.3615922890983
At time: 96.53074336051941 and batch: 150, loss is 5.774399251937866 and perplexity is 321.95096519263194
At time: 97.69062757492065 and batch: 200, loss is 5.816430835723877 and perplexity is 335.7714887952556
At time: 98.84985756874084 and batch: 250, loss is 5.828742170333863 and perplexity is 339.93083496200074
At time: 100.01005530357361 and batch: 300, loss is 5.824732398986816 and perplexity is 338.57051914032104
At time: 101.16836023330688 and batch: 350, loss is 5.8547784614562985 and perplexity is 348.8975972259333
At time: 102.37267971038818 and batch: 400, loss is 5.828895425796508 and perplexity is 339.9829352116024
At time: 103.53257727622986 and batch: 450, loss is 5.804582004547119 and perplexity is 331.81646657776747
At time: 104.69278502464294 and batch: 500, loss is 5.778705863952637 and perplexity is 323.3404729753649
At time: 105.85268998146057 and batch: 550, loss is 5.763234605789185 and perplexity is 318.37648760734027
At time: 107.01270866394043 and batch: 600, loss is 5.788787965774536 and perplexity is 326.6169134967177
At time: 108.17061519622803 and batch: 650, loss is 5.783075313568116 and perplexity is 324.7563840039011
At time: 109.3292908668518 and batch: 700, loss is 5.810674962997436 and perplexity is 333.84438225101036
At time: 110.48871517181396 and batch: 750, loss is 5.749040689468384 and perplexity is 313.88939840129956
At time: 111.64786624908447 and batch: 800, loss is 5.782383823394776 and perplexity is 324.5318957802784
At time: 112.80599808692932 and batch: 850, loss is 5.799448566436768 and perplexity is 330.1174718557399
At time: 113.96418356895447 and batch: 900, loss is 5.786032238006592 and perplexity is 325.7180852305315
At time: 115.12251019477844 and batch: 950, loss is 5.769783821105957 and perplexity is 320.46844664416926
At time: 116.2909164428711 and batch: 1000, loss is 5.778211269378662 and perplexity is 323.18059007375194
At time: 117.45214295387268 and batch: 1050, loss is 5.773859958648682 and perplexity is 321.77738600690924
At time: 118.60941410064697 and batch: 1100, loss is 5.735510292053223 and perplexity is 309.6709530736867
At time: 119.76836395263672 and batch: 1150, loss is 5.771917896270752 and perplexity is 321.1530806676519
At time: 120.92657613754272 and batch: 1200, loss is 5.767834033966064 and perplexity is 319.84421015006706
At time: 122.08635234832764 and batch: 1250, loss is 5.762743930816651 and perplexity is 318.2203065532288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.491075501824818 and perplexity of 242.51789508620936
Finished 4 epochs...
Completing Train Step...
At time: 125.00113463401794 and batch: 50, loss is 5.732062568664551 and perplexity is 308.6051316704389
At time: 126.16479349136353 and batch: 100, loss is 5.758413963317871 and perplexity is 316.845401763059
At time: 127.32406258583069 and batch: 150, loss is 5.6530075359344485 and perplexity is 285.14776966006394
At time: 128.48520016670227 and batch: 200, loss is 5.688006963729858 and perplexity is 295.30448120558947
At time: 129.64567399024963 and batch: 250, loss is 5.709510288238525 and perplexity is 301.7232746612942
At time: 130.8064887523651 and batch: 300, loss is 5.704828109741211 and perplexity is 300.3138545855899
At time: 131.9945068359375 and batch: 350, loss is 5.738158311843872 and perplexity is 310.49205455270067
At time: 133.16555833816528 and batch: 400, loss is 5.716322326660157 and perplexity is 303.78564168755423
At time: 134.32751941680908 and batch: 450, loss is 5.690636014938354 and perplexity is 296.081873262769
At time: 135.49014353752136 and batch: 500, loss is 5.6786886501312255 and perplexity is 292.56552249955354
At time: 136.65297651290894 and batch: 550, loss is 5.670555181503296 and perplexity is 290.19560090698405
At time: 137.82330536842346 and batch: 600, loss is 5.699697713851929 and perplexity is 298.7770711450528
At time: 138.98285484313965 and batch: 650, loss is 5.6993717765808105 and perplexity is 298.67970443039354
At time: 140.1437954902649 and batch: 700, loss is 5.720799398422241 and perplexity is 305.14876092011036
At time: 141.3092920780182 and batch: 750, loss is 5.662871322631836 and perplexity is 287.974323783447
At time: 142.47133922576904 and batch: 800, loss is 5.691464319229126 and perplexity is 296.3272207459713
At time: 143.63389086723328 and batch: 850, loss is 5.730387954711914 and perplexity is 308.08876968525163
At time: 144.7960443496704 and batch: 900, loss is 5.711642160415649 and perplexity is 302.3671962511173
At time: 145.9592308998108 and batch: 950, loss is 5.695193672180176 and perplexity is 297.4343927755559
At time: 147.1207025051117 and batch: 1000, loss is 5.696781225204468 and perplexity is 297.9069606593697
At time: 148.28110241889954 and batch: 1050, loss is 5.706218938827515 and perplexity is 300.73183042866975
At time: 149.44267868995667 and batch: 1100, loss is 5.663845348358154 and perplexity is 288.2549548320748
At time: 150.60656833648682 and batch: 1150, loss is 5.697702760696411 and perplexity is 298.18161903119585
At time: 151.768048286438 and batch: 1200, loss is 5.693351964950562 and perplexity is 296.8871098262209
At time: 152.9284324645996 and batch: 1250, loss is 5.6928113842010495 and perplexity is 296.7266617413507
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.445891220204151 and perplexity of 231.80377589014554
Finished 5 epochs...
Completing Train Step...
At time: 155.8242220878601 and batch: 50, loss is 5.6702161693573 and perplexity is 290.0972377476568
At time: 157.02217984199524 and batch: 100, loss is 5.68932725906372 and perplexity is 295.69462783190426
At time: 158.18407249450684 and batch: 150, loss is 5.586899671554566 and perplexity is 266.9068367695189
At time: 159.34831380844116 and batch: 200, loss is 5.631442861557007 and perplexity is 279.064478695817
At time: 160.51045155525208 and batch: 250, loss is 5.652545289993286 and perplexity is 285.0159917201625
At time: 161.71105408668518 and batch: 300, loss is 5.649354047775269 and perplexity is 284.10788641724616
At time: 162.87393879890442 and batch: 350, loss is 5.678204193115234 and perplexity is 292.42382140635726
At time: 164.03827619552612 and batch: 400, loss is 5.657460165023804 and perplexity is 286.42025777045353
At time: 165.2024245262146 and batch: 450, loss is 5.627449054718017 and perplexity is 277.952171720372
At time: 166.3662326335907 and batch: 500, loss is 5.622885732650757 and perplexity is 276.6866760696783
At time: 167.52956318855286 and batch: 550, loss is 5.623990564346314 and perplexity is 276.9925372105672
At time: 168.69229078292847 and batch: 600, loss is 5.6592209815979 and perplexity is 286.92503558764514
At time: 169.85474157333374 and batch: 650, loss is 5.653130903244018 and perplexity is 285.1829497432282
At time: 171.02580070495605 and batch: 700, loss is 5.67548436164856 and perplexity is 291.62955851552164
At time: 172.1898729801178 and batch: 750, loss is 5.62000337600708 and perplexity is 275.8903146408657
At time: 173.35341119766235 and batch: 800, loss is 5.643646287918091 and perplexity is 282.4908859431983
At time: 174.51580500602722 and batch: 850, loss is 5.686790714263916 and perplexity is 294.9455356159715
At time: 175.6793053150177 and batch: 900, loss is 5.673068304061889 and perplexity is 290.9258151927019
At time: 176.84880447387695 and batch: 950, loss is 5.65431842803955 and perplexity is 285.5218127315893
At time: 178.0116286277771 and batch: 1000, loss is 5.6613664722442625 and perplexity is 287.5412914168721
At time: 179.17380475997925 and batch: 1050, loss is 5.665484743118286 and perplexity is 288.72790606556975
At time: 180.33670735359192 and batch: 1100, loss is 5.628499593734741 and perplexity is 278.24432475375846
At time: 181.50107336044312 and batch: 1150, loss is 5.6598944568634035 and perplexity is 287.1183375869184
At time: 182.66956233978271 and batch: 1200, loss is 5.660825424194336 and perplexity is 287.3857598406997
At time: 183.83582496643066 and batch: 1250, loss is 5.653914098739624 and perplexity is 285.4063912326532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.430989731837363 and perplexity of 228.37516377672185
Finished 6 epochs...
Completing Train Step...
At time: 186.73575854301453 and batch: 50, loss is 5.631564207077027 and perplexity is 279.09834397476175
At time: 187.89552664756775 and batch: 100, loss is 5.649417543411255 and perplexity is 284.1259266009134
At time: 189.05523324012756 and batch: 150, loss is 5.5559579944610595 and perplexity is 258.77475059050977
At time: 190.2157166004181 and batch: 200, loss is 5.597194604873657 and perplexity is 269.6688176669018
At time: 191.4014720916748 and batch: 250, loss is 5.618076066970826 and perplexity is 275.35910081547985
At time: 192.56167221069336 and batch: 300, loss is 5.618915271759033 and perplexity is 275.59028048133445
At time: 193.72144484519958 and batch: 350, loss is 5.6481728267669675 and perplexity is 283.77249034062964
At time: 194.88242864608765 and batch: 400, loss is 5.629893188476562 and perplexity is 278.6323548972611
At time: 196.043692111969 and batch: 450, loss is 5.596111993789673 and perplexity is 269.37702919110905
At time: 197.20454168319702 and batch: 500, loss is 5.589389514923096 and perplexity is 267.5722209945709
At time: 198.36658477783203 and batch: 550, loss is 5.5902062511444095 and perplexity is 267.79084618667815
At time: 199.52809524536133 and batch: 600, loss is 5.6250736522674565 and perplexity is 277.29270700766386
At time: 200.68761229515076 and batch: 650, loss is 5.617621784210205 and perplexity is 275.2340383320153
At time: 201.85509705543518 and batch: 700, loss is 5.642531280517578 and perplexity is 282.1760820517434
At time: 203.0161690711975 and batch: 750, loss is 5.595927448272705 and perplexity is 269.3273214548086
At time: 204.176593542099 and batch: 800, loss is 5.609933805465698 and perplexity is 273.12615794753196
At time: 205.3370041847229 and batch: 850, loss is 5.656561841964722 and perplexity is 286.16307538204023
At time: 206.49751996994019 and batch: 900, loss is 5.642589931488037 and perplexity is 282.19263243813936
At time: 207.65813612937927 and batch: 950, loss is 5.627686395645141 and perplexity is 278.01814897574604
At time: 208.81929898262024 and batch: 1000, loss is 5.632059240341187 and perplexity is 279.23654114227656
At time: 209.97818088531494 and batch: 1050, loss is 5.627965888977051 and perplexity is 278.0958640544518
At time: 211.1370553970337 and batch: 1100, loss is 5.587739324569702 and perplexity is 267.13104001306533
At time: 212.29822063446045 and batch: 1150, loss is 5.619691619873047 and perplexity is 275.804317548713
At time: 213.45872020721436 and batch: 1200, loss is 5.624255113601684 and perplexity is 277.0658250737269
At time: 214.61994194984436 and batch: 1250, loss is 5.6291481590271 and perplexity is 278.4248428981634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.401937442974453 and perplexity of 221.83579432325826
Finished 7 epochs...
Completing Train Step...
At time: 217.5138463973999 and batch: 50, loss is 5.597408380508423 and perplexity is 269.7264724519497
At time: 218.67651653289795 and batch: 100, loss is 5.620705423355102 and perplexity is 276.08407070961334
At time: 219.88665413856506 and batch: 150, loss is 5.527392711639404 and perplexity is 251.48735542592595
At time: 221.05501866340637 and batch: 200, loss is 5.569580001831055 and perplexity is 262.32390054237476
At time: 222.21664357185364 and batch: 250, loss is 5.589577732086181 and perplexity is 267.622587418691
At time: 223.3795187473297 and batch: 300, loss is 5.592501106262207 and perplexity is 268.40609306175986
At time: 224.54278016090393 and batch: 350, loss is 5.619990310668945 and perplexity is 275.88671006416433
At time: 225.70504236221313 and batch: 400, loss is 5.596872930526733 and perplexity is 269.5820860765049
At time: 226.86706733703613 and batch: 450, loss is 5.563962831497192 and perplexity is 260.8545132732542
At time: 228.0306420326233 and batch: 500, loss is 5.562783613204956 and perplexity is 260.54709015469655
At time: 229.194153547287 and batch: 550, loss is 5.556855974197387 and perplexity is 259.0072294378384
At time: 230.35875582695007 and batch: 600, loss is 5.598888740539551 and perplexity is 270.1260604351708
At time: 231.51978874206543 and batch: 650, loss is 5.596271705627442 and perplexity is 269.42005532729394
At time: 232.6817479133606 and batch: 700, loss is 5.618779449462891 and perplexity is 275.5528517185327
At time: 233.8445074558258 and batch: 750, loss is 5.574006624221802 and perplexity is 263.487683307208
At time: 235.00805759429932 and batch: 800, loss is 5.58136173248291 and perplexity is 265.43280827689006
At time: 236.1713752746582 and batch: 850, loss is 5.629675827026367 and perplexity is 278.5717975461846
At time: 237.3335201740265 and batch: 900, loss is 5.624697103500366 and perplexity is 277.1883124367739
At time: 238.49485683441162 and batch: 950, loss is 5.609591293334961 and perplexity is 273.0326249442217
At time: 239.65760231018066 and batch: 1000, loss is 5.607697172164917 and perplexity is 272.5159575390077
At time: 240.8180124759674 and batch: 1050, loss is 5.607183666229248 and perplexity is 272.3760549007386
At time: 241.9798903465271 and batch: 1100, loss is 5.571550436019898 and perplexity is 262.8413021100175
At time: 243.14182686805725 and batch: 1150, loss is 5.603154544830322 and perplexity is 271.2808265949294
At time: 244.30468082427979 and batch: 1200, loss is 5.6023965549469 and perplexity is 271.0752763851353
At time: 245.4666872024536 and batch: 1250, loss is 5.606768407821655 and perplexity is 272.2629719348654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.389485825587363 and perplexity of 219.09070575295712
Finished 8 epochs...
Completing Train Step...
At time: 248.3332724571228 and batch: 50, loss is 5.577714195251465 and perplexity is 264.4663958104508
At time: 249.51633834838867 and batch: 100, loss is 5.602076826095581 and perplexity is 270.9886196524627
At time: 250.67658257484436 and batch: 150, loss is 5.511181869506836 and perplexity is 247.44340012857333
At time: 251.83781790733337 and batch: 200, loss is 5.551508016586304 and perplexity is 257.6257670475517
At time: 252.99801468849182 and batch: 250, loss is 5.571331634521484 and perplexity is 262.7837983304568
At time: 254.15717840194702 and batch: 300, loss is 5.571915187835693 and perplexity is 262.93719143906856
At time: 255.31667923927307 and batch: 350, loss is 5.595459814071655 and perplexity is 269.2014042319139
At time: 256.47869300842285 and batch: 400, loss is 5.581440334320068 and perplexity is 265.4536726032391
At time: 257.6397247314453 and batch: 450, loss is 5.5421015167236325 and perplexity is 255.2137723041746
At time: 258.8019196987152 and batch: 500, loss is 5.540929698944092 and perplexity is 254.9148834240587
At time: 259.9626989364624 and batch: 550, loss is 5.534929513931274 and perplexity is 253.38992654784045
At time: 261.12501525878906 and batch: 600, loss is 5.574483442306518 and perplexity is 263.6133489571552
At time: 262.28434705734253 and batch: 650, loss is 5.569319438934326 and perplexity is 262.2555575711763
At time: 263.4457058906555 and batch: 700, loss is 5.595262460708618 and perplexity is 269.1482816715847
At time: 264.607004404068 and batch: 750, loss is 5.55236065864563 and perplexity is 257.8455232854946
At time: 265.7682180404663 and batch: 800, loss is 5.565355615615845 and perplexity is 261.21808042312426
At time: 266.9281737804413 and batch: 850, loss is 5.614205780029297 and perplexity is 274.29544174399285
At time: 268.08950543403625 and batch: 900, loss is 5.60482515335083 and perplexity is 271.7344094294701
At time: 269.2501151561737 and batch: 950, loss is 5.592727737426758 and perplexity is 268.46692914061816
At time: 270.4101734161377 and batch: 1000, loss is 5.590480546951294 and perplexity is 267.86431016787975
At time: 271.5693464279175 and batch: 1050, loss is 5.593274641036987 and perplexity is 268.61379483041964
At time: 272.728374004364 and batch: 1100, loss is 5.545315103530884 and perplexity is 256.0352431428514
At time: 273.88920736312866 and batch: 1150, loss is 5.583529968261718 and perplexity is 266.00895357237056
At time: 275.04980754852295 and batch: 1200, loss is 5.581952028274536 and perplexity is 265.5895384005905
At time: 276.2103657722473 and batch: 1250, loss is 5.583962421417237 and perplexity is 266.12401486125464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.389415880189325 and perplexity of 219.07538190225927
Finished 9 epochs...
Completing Train Step...
At time: 279.1002631187439 and batch: 50, loss is 5.55609022140503 and perplexity is 258.808969847261
At time: 280.2628130912781 and batch: 100, loss is 5.575558366775513 and perplexity is 263.8968657485717
At time: 281.4249300956726 and batch: 150, loss is 5.488083190917969 and perplexity is 241.79329080495484
At time: 282.58804631233215 and batch: 200, loss is 5.530754880905151 and perplexity is 252.33432150687156
At time: 283.7507498264313 and batch: 250, loss is 5.554402694702149 and perplexity is 258.3725911036479
At time: 284.91201734542847 and batch: 300, loss is 5.551827688217163 and perplexity is 257.7081358614401
At time: 286.0740466117859 and batch: 350, loss is 5.573933181762695 and perplexity is 263.4683328343833
At time: 287.2367932796478 and batch: 400, loss is 5.559348764419556 and perplexity is 259.6536855318105
At time: 288.39931869506836 and batch: 450, loss is 5.518765935897827 and perplexity is 249.32716155985707
At time: 289.5618658065796 and batch: 500, loss is 5.523810119628906 and perplexity is 250.5879908257938
At time: 290.7251088619232 and batch: 550, loss is 5.5155017280578615 and perplexity is 248.51463273731477
At time: 291.8862211704254 and batch: 600, loss is 5.557437553405761 and perplexity is 259.15790646836507
At time: 293.0464448928833 and batch: 650, loss is 5.5474285125732425 and perplexity is 256.57692253431657
At time: 294.209041595459 and batch: 700, loss is 5.569673404693604 and perplexity is 262.3484034899054
At time: 295.37146282196045 and batch: 750, loss is 5.526498394012451 and perplexity is 251.26254639133754
At time: 296.5341408252716 and batch: 800, loss is 5.544907350540161 and perplexity is 255.93086528836673
At time: 297.6955087184906 and batch: 850, loss is 5.586153879165649 and perplexity is 266.7078538912874
At time: 298.8581533432007 and batch: 900, loss is 5.580241441726685 and perplexity is 265.1356128591011
At time: 300.0194294452667 and batch: 950, loss is 5.568798532485962 and perplexity is 262.118982534615
At time: 301.1822373867035 and batch: 1000, loss is 5.570622491836548 and perplexity is 262.59751318133306
At time: 302.34342217445374 and batch: 1050, loss is 5.569791641235351 and perplexity is 262.3794244917389
At time: 303.5042359828949 and batch: 1100, loss is 5.519493141174316 and perplexity is 249.50853952883045
At time: 304.66566252708435 and batch: 1150, loss is 5.561832246780395 and perplexity is 260.29933227405803
At time: 305.8273718357086 and batch: 1200, loss is 5.559069986343384 and perplexity is 259.58130986573013
At time: 306.98780941963196 and batch: 1250, loss is 5.563265209197998 and perplexity is 260.67259880912053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.376941987197765 and perplexity of 216.35963223040295
Finished 10 epochs...
Completing Train Step...
At time: 309.84852480888367 and batch: 50, loss is 5.52891396522522 and perplexity is 251.8702226123588
At time: 311.031809091568 and batch: 100, loss is 5.5528112983703615 and perplexity is 257.96174490620405
At time: 312.1944842338562 and batch: 150, loss is 5.468344125747681 and perplexity is 237.06731391077932
At time: 313.3561005592346 and batch: 200, loss is 5.5129483127593994 and perplexity is 247.8808811320484
At time: 314.51727294921875 and batch: 250, loss is 5.531983890533447 and perplexity is 252.6446334666866
At time: 315.6794171333313 and batch: 300, loss is 5.530077476501464 and perplexity is 252.1634470083845
At time: 316.8450837135315 and batch: 350, loss is 5.552479591369629 and perplexity is 257.8761913796337
At time: 318.0067937374115 and batch: 400, loss is 5.53854133605957 and perplexity is 254.3067806518874
At time: 319.1726508140564 and batch: 450, loss is 5.498629636764527 and perplexity is 244.35684508424285
At time: 320.3470001220703 and batch: 500, loss is 5.500661478042603 and perplexity is 244.8538441492899
At time: 321.52113485336304 and batch: 550, loss is 5.4944994926452635 and perplexity is 243.34969736180565
At time: 322.6844837665558 and batch: 600, loss is 5.5312079811096195 and perplexity is 252.44868014554544
At time: 323.84383845329285 and batch: 650, loss is 5.526485185623169 and perplexity is 251.2592276397306
At time: 325.0105278491974 and batch: 700, loss is 5.557398233413696 and perplexity is 259.1477165818736
At time: 326.1745171546936 and batch: 750, loss is 5.512063951492309 and perplexity is 247.66176178653507
At time: 327.3365652561188 and batch: 800, loss is 5.523855924606323 and perplexity is 250.59946926593744
At time: 328.49909567832947 and batch: 850, loss is 5.568031921386718 and perplexity is 261.91811621629336
At time: 329.66199588775635 and batch: 900, loss is 5.560108814239502 and perplexity is 259.8511102855676
At time: 330.82322359085083 and batch: 950, loss is 5.548905153274536 and perplexity is 256.9560743277343
At time: 331.9851713180542 and batch: 1000, loss is 5.54949909210205 and perplexity is 257.10873584855784
At time: 333.14659357070923 and batch: 1050, loss is 5.545416221618653 and perplexity is 256.06113424604683
At time: 334.30780029296875 and batch: 1100, loss is 5.498654346466065 and perplexity is 244.36288314355272
At time: 335.47008657455444 and batch: 1150, loss is 5.535938882827759 and perplexity is 253.6458195818892
At time: 336.63303780555725 and batch: 1200, loss is 5.543324747085571 and perplexity is 255.52614855434828
At time: 337.8201434612274 and batch: 1250, loss is 5.5388916492462155 and perplexity is 254.39588327660414
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.365727556882984 and perplexity of 213.94683656300012
Finished 11 epochs...
Completing Train Step...
At time: 340.71453762054443 and batch: 50, loss is 5.505411014556885 and perplexity is 246.01955251862583
At time: 341.8785185813904 and batch: 100, loss is 5.525937032699585 and perplexity is 251.1215369007606
At time: 343.04260873794556 and batch: 150, loss is 5.444124746322632 and perplexity is 231.39466202526063
At time: 344.2064597606659 and batch: 200, loss is 5.4879795265197755 and perplexity is 241.76822674812394
At time: 345.3693959712982 and batch: 250, loss is 5.507659215927124 and perplexity is 246.57327622166878
At time: 346.53011560440063 and batch: 300, loss is 5.502229061126709 and perplexity is 245.23797389243967
At time: 347.69335675239563 and batch: 350, loss is 5.5240441417694095 and perplexity is 250.64664082622298
At time: 348.85610842704773 and batch: 400, loss is 5.5078026294708256 and perplexity is 246.60864070480625
At time: 350.01913499832153 and batch: 450, loss is 5.467919940948486 and perplexity is 236.9667748849033
At time: 351.1829423904419 and batch: 500, loss is 5.470228567123413 and perplexity is 237.51447455763196
At time: 352.3453447818756 and batch: 550, loss is 5.466206197738647 and perplexity is 236.56102246052296
At time: 353.50821137428284 and batch: 600, loss is 5.508032503128052 and perplexity is 246.6653360510824
At time: 354.66973066329956 and batch: 650, loss is 5.500694904327393 and perplexity is 244.86202884040745
At time: 355.8337495326996 and batch: 700, loss is 5.53411301612854 and perplexity is 253.18311867015728
At time: 356.99726128578186 and batch: 750, loss is 5.4951972484588625 and perplexity is 243.51955528066114
At time: 358.159494638443 and batch: 800, loss is 5.502489070892334 and perplexity is 245.30174645093865
At time: 359.321989774704 and batch: 850, loss is 5.543752145767212 and perplexity is 255.63538343512434
At time: 360.4852936267853 and batch: 900, loss is 5.531725358963013 and perplexity is 252.57932529531044
At time: 361.64733934402466 and batch: 950, loss is 5.519393692016601 and perplexity is 249.48372734852725
At time: 362.80931425094604 and batch: 1000, loss is 5.526780452728271 and perplexity is 251.33342717830834
At time: 363.97060990333557 and batch: 1050, loss is 5.527758312225342 and perplexity is 251.5793161598504
At time: 365.1399202346802 and batch: 1100, loss is 5.476414489746094 and perplexity is 238.98827442532883
At time: 366.3164839744568 and batch: 1150, loss is 5.519145927429199 and perplexity is 249.42192177268987
At time: 367.5042860507965 and batch: 1200, loss is 5.514446592330932 and perplexity is 248.25255435809407
At time: 368.6652715206146 and batch: 1250, loss is 5.518395729064942 and perplexity is 249.2348760244456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.352665587933394 and perplexity of 211.17044169318822
Finished 12 epochs...
Completing Train Step...
At time: 371.53946685791016 and batch: 50, loss is 5.479149198532104 and perplexity is 239.64273222703275
At time: 372.7290093898773 and batch: 100, loss is 5.503529672622681 and perplexity is 245.55714073157142
At time: 373.8930752277374 and batch: 150, loss is 5.420220537185669 and perplexity is 225.92894274141702
At time: 375.0571300983429 and batch: 200, loss is 5.467202138900757 and perplexity is 236.7967406813975
At time: 376.22153425216675 and batch: 250, loss is 5.492516803741455 and perplexity is 242.86768861170165
At time: 377.38515067100525 and batch: 300, loss is 5.481950416564941 and perplexity is 240.31496486550589
At time: 378.55037474632263 and batch: 350, loss is 5.506404886245727 and perplexity is 246.26418593375143
At time: 379.71437788009644 and batch: 400, loss is 5.488966903686523 and perplexity is 242.00706106523228
At time: 380.88395643234253 and batch: 450, loss is 5.446505298614502 and perplexity is 231.9461652990139
At time: 382.04909801483154 and batch: 500, loss is 5.451593751907349 and perplexity is 233.12942044375728
At time: 383.21377968788147 and batch: 550, loss is 5.440184268951416 and perplexity is 230.48465071272827
At time: 384.3804759979248 and batch: 600, loss is 5.480252771377564 and perplexity is 239.90734141983515
At time: 385.54339599609375 and batch: 650, loss is 5.471309938430786 and perplexity is 237.77145481600147
At time: 386.7078363895416 and batch: 700, loss is 5.497520809173584 and perplexity is 244.08604563507376
At time: 387.87293767929077 and batch: 750, loss is 5.455385780334472 and perplexity is 234.01513209381736
At time: 389.0387020111084 and batch: 800, loss is 5.477893981933594 and perplexity is 239.34211739965951
At time: 390.2025899887085 and batch: 850, loss is 5.516651926040649 and perplexity is 248.800638216514
At time: 391.36829924583435 and batch: 900, loss is 5.504719829559326 and perplexity is 245.8495662475663
At time: 392.5329577922821 and batch: 950, loss is 5.492397394180298 and perplexity is 242.83868961899822
At time: 393.69647216796875 and batch: 1000, loss is 5.5021693134307865 and perplexity is 245.22332192626192
At time: 394.85963320732117 and batch: 1050, loss is 5.504388294219971 and perplexity is 245.7680719380583
At time: 396.05064272880554 and batch: 1100, loss is 5.453030815124512 and perplexity is 233.4646829977605
At time: 397.2151162624359 and batch: 1150, loss is 5.493806781768799 and perplexity is 243.18118475165616
At time: 398.3804440498352 and batch: 1200, loss is 5.489854135513306 and perplexity is 242.22187271188895
At time: 399.5450839996338 and batch: 1250, loss is 5.489362258911132 and perplexity is 242.1027587372575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.342098932196624 and perplexity of 208.9508239547236
Finished 13 epochs...
Completing Train Step...
At time: 402.41077947616577 and batch: 50, loss is 5.4595224571228025 and perplexity is 234.9851820671735
At time: 403.59685254096985 and batch: 100, loss is 5.478188695907593 and perplexity is 239.4126652614335
At time: 404.7578673362732 and batch: 150, loss is 5.397752122879028 and perplexity is 220.90928074581026
At time: 405.9187910556793 and batch: 200, loss is 5.444146461486817 and perplexity is 231.39968685289534
At time: 407.0798189640045 and batch: 250, loss is 5.468389596939087 and perplexity is 237.0780938890736
At time: 408.2413341999054 and batch: 300, loss is 5.45790207862854 and perplexity is 234.60472545678095
At time: 409.4016659259796 and batch: 350, loss is 5.482270660400391 and perplexity is 240.39193657577047
At time: 410.56300687789917 and batch: 400, loss is 5.457670516967774 and perplexity is 234.55040628629197
At time: 411.72296118736267 and batch: 450, loss is 5.420874328613281 and perplexity is 226.07670144385412
At time: 412.8846595287323 and batch: 500, loss is 5.4282896900177 and perplexity is 227.7593729884969
At time: 414.04528999328613 and batch: 550, loss is 5.427527580261231 and perplexity is 227.5858614740221
At time: 415.2065544128418 and batch: 600, loss is 5.469260730743408 and perplexity is 237.28471061326462
At time: 416.3750901222229 and batch: 650, loss is 5.4571947002410885 and perplexity is 234.4388298268081
At time: 417.5350196361542 and batch: 700, loss is 5.483418836593628 and perplexity is 240.66810739043027
At time: 418.6968078613281 and batch: 750, loss is 5.444488382339477 and perplexity is 231.47882075912483
At time: 419.8582453727722 and batch: 800, loss is 5.4575150775909425 and perplexity is 234.51395075068956
At time: 421.0182189941406 and batch: 850, loss is 5.502338914871216 and perplexity is 245.26491568196826
At time: 422.1785559654236 and batch: 900, loss is 5.495016145706177 and perplexity is 243.47545721212828
At time: 423.33918166160583 and batch: 950, loss is 5.483019018173218 and perplexity is 240.57190308129952
At time: 424.49947333335876 and batch: 1000, loss is 5.487386178970337 and perplexity is 241.6248167134546
At time: 425.6840343475342 and batch: 1050, loss is 5.494743871688843 and perplexity is 243.40917419525204
At time: 426.84453535079956 and batch: 1100, loss is 5.4428933906555175 and perplexity is 231.1099082494032
At time: 428.0045280456543 and batch: 1150, loss is 5.480374088287354 and perplexity is 239.9364480026568
At time: 429.1658363342285 and batch: 1200, loss is 5.478551273345947 and perplexity is 239.49948663109726
At time: 430.32513999938965 and batch: 1250, loss is 5.482031154632568 and perplexity is 240.33436821467492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.343605208570939 and perplexity of 209.2657988042745
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 433.2108736038208 and batch: 50, loss is 5.425712947845459 and perplexity is 227.17325127356577
At time: 434.3744306564331 and batch: 100, loss is 5.421941404342651 and perplexity is 226.31807116192195
At time: 435.5374917984009 and batch: 150, loss is 5.322658615112305 and perplexity is 204.92798302747642
At time: 436.7015061378479 and batch: 200, loss is 5.357418394088745 and perplexity is 212.1764827335185
At time: 437.8650896549225 and batch: 250, loss is 5.372257890701294 and perplexity is 215.34855268083655
At time: 439.0282335281372 and batch: 300, loss is 5.361972789764405 and perplexity is 213.14502227093524
At time: 440.1944601535797 and batch: 350, loss is 5.376276340484619 and perplexity is 216.21566107463062
At time: 441.3689434528351 and batch: 400, loss is 5.347707843780517 and perplexity is 210.1261035902694
At time: 442.5321102142334 and batch: 450, loss is 5.306063060760498 and perplexity is 201.55515394071642
At time: 443.69568824768066 and batch: 500, loss is 5.301235399246216 and perplexity is 200.5844588599498
At time: 444.8587415218353 and batch: 550, loss is 5.299731693267822 and perplexity is 200.28306547030104
At time: 446.02098536491394 and batch: 600, loss is 5.335124683380127 and perplexity is 207.4986188242919
At time: 447.1838958263397 and batch: 650, loss is 5.320429048538208 and perplexity is 204.47159141322416
At time: 448.34717416763306 and batch: 700, loss is 5.342125158309937 and perplexity is 208.95630399456925
At time: 449.5104911327362 and batch: 750, loss is 5.304658737182617 and perplexity is 201.27230393874206
At time: 450.6738443374634 and batch: 800, loss is 5.302211790084839 and perplexity is 200.78040333157756
At time: 451.8368237018585 and batch: 850, loss is 5.345676097869873 and perplexity is 209.69961414438617
At time: 452.9998686313629 and batch: 900, loss is 5.3253398036956785 and perplexity is 205.4781708450332
At time: 454.1631700992584 and batch: 950, loss is 5.305645523071289 and perplexity is 201.47101463437994
At time: 455.35314297676086 and batch: 1000, loss is 5.309775152206421 and perplexity is 202.30473550044366
At time: 456.51609230041504 and batch: 1050, loss is 5.289267807006836 and perplexity is 198.1982528966903
At time: 457.67912459373474 and batch: 1100, loss is 5.241146001815796 and perplexity is 188.8864426250744
At time: 458.8423547744751 and batch: 1150, loss is 5.279097805023193 and perplexity is 196.1927913308284
At time: 460.00587606430054 and batch: 1200, loss is 5.294175901412964 and perplexity is 199.17341978100455
At time: 461.16897797584534 and batch: 1250, loss is 5.3110746479034425 and perplexity is 202.56780052262616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.187838589187956 and perplexity of 179.08106657056067
Finished 15 epochs...
Completing Train Step...
At time: 464.0336444377899 and batch: 50, loss is 5.31133397102356 and perplexity is 202.62033784847017
At time: 465.21542525291443 and batch: 100, loss is 5.332302808761597 and perplexity is 206.91390911519755
At time: 466.37731194496155 and batch: 150, loss is 5.2471374416351315 and perplexity is 190.02154142122936
At time: 467.5387442111969 and batch: 200, loss is 5.283720970153809 and perplexity is 197.1019229160209
At time: 468.70061564445496 and batch: 250, loss is 5.311307830810547 and perplexity is 202.61504137890378
At time: 469.8619258403778 and batch: 300, loss is 5.3073100757598874 and perplexity is 201.8066530198557
At time: 471.02886605262756 and batch: 350, loss is 5.325395393371582 and perplexity is 205.48959362744714
At time: 472.19046330451965 and batch: 400, loss is 5.2990247249603275 and perplexity is 200.1415217298503
At time: 473.3539288043976 and batch: 450, loss is 5.257978572845459 and perplexity is 192.09279697497556
At time: 474.515638589859 and batch: 500, loss is 5.262921838760376 and perplexity is 193.04471360085932
At time: 475.6783573627472 and batch: 550, loss is 5.263541488647461 and perplexity is 193.16437080480222
At time: 476.84317660331726 and batch: 600, loss is 5.303507013320923 and perplexity is 201.04062726298477
At time: 478.0024471282959 and batch: 650, loss is 5.294301948547363 and perplexity is 199.19852660210466
At time: 479.1634478569031 and batch: 700, loss is 5.312552661895752 and perplexity is 202.86741993248611
At time: 480.3248655796051 and batch: 750, loss is 5.275101442337036 and perplexity is 195.41029838449433
At time: 481.4860680103302 and batch: 800, loss is 5.282632970809937 and perplexity is 196.88759276988424
At time: 482.6479375362396 and batch: 850, loss is 5.327596607208252 and perplexity is 205.942418363428
At time: 483.80954241752625 and batch: 900, loss is 5.309218778610229 and perplexity is 202.19220979329552
At time: 484.99547815322876 and batch: 950, loss is 5.289715471267701 and perplexity is 198.28699903383276
At time: 486.15677857398987 and batch: 1000, loss is 5.29437891960144 and perplexity is 199.2138597127631
At time: 487.31745767593384 and batch: 1050, loss is 5.276270170211792 and perplexity is 195.63881335712864
At time: 488.479620218277 and batch: 1100, loss is 5.241283159255982 and perplexity is 188.9123515827932
At time: 489.64093136787415 and batch: 1150, loss is 5.285016860961914 and perplexity is 197.35751105758118
At time: 490.8024892807007 and batch: 1200, loss is 5.301435489654541 and perplexity is 200.62459790181143
At time: 491.9635190963745 and batch: 1250, loss is 5.30792085647583 and perplexity is 201.92995028183108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.183404407361998 and perplexity of 178.28874650363468
Finished 16 epochs...
Completing Train Step...
At time: 494.8486342430115 and batch: 50, loss is 5.2929090404510495 and perplexity is 198.92125451372533
At time: 496.0111389160156 and batch: 100, loss is 5.31195484161377 and perplexity is 202.74617791837557
At time: 497.1741018295288 and batch: 150, loss is 5.22907974243164 and perplexity is 186.62098509974012
At time: 498.33652782440186 and batch: 200, loss is 5.265447216033936 and perplexity is 193.53284042614305
At time: 499.50126123428345 and batch: 250, loss is 5.293544044494629 and perplexity is 199.04761042870805
At time: 500.66934180259705 and batch: 300, loss is 5.290487766265869 and perplexity is 198.44019423971974
At time: 501.8412518501282 and batch: 350, loss is 5.310175695419312 and perplexity is 202.38578351971742
At time: 503.00338411331177 and batch: 400, loss is 5.284736576080323 and perplexity is 197.30220248240406
At time: 504.1677403450012 and batch: 450, loss is 5.242353057861328 and perplexity is 189.11457680523569
At time: 505.3301010131836 and batch: 500, loss is 5.251805915832519 and perplexity is 190.91072603706493
At time: 506.4943015575409 and batch: 550, loss is 5.254287672042847 and perplexity is 191.3851083241379
At time: 507.655145406723 and batch: 600, loss is 5.294008407592774 and perplexity is 199.14006225771257
At time: 508.81647849082947 and batch: 650, loss is 5.286705722808838 and perplexity is 197.69110224363297
At time: 509.9786891937256 and batch: 700, loss is 5.303522233963013 and perplexity is 201.04368725370531
At time: 511.14176893234253 and batch: 750, loss is 5.268407468795776 and perplexity is 194.10659536234957
At time: 512.304639339447 and batch: 800, loss is 5.277440376281739 and perplexity is 195.86788508846104
At time: 513.4921126365662 and batch: 850, loss is 5.321429672241211 and perplexity is 204.676292931699
At time: 514.6542391777039 and batch: 900, loss is 5.303981590270996 and perplexity is 201.13605915380876
At time: 515.8178248405457 and batch: 950, loss is 5.285703067779541 and perplexity is 197.49298560364133
At time: 516.9810230731964 and batch: 1000, loss is 5.289891958236694 and perplexity is 198.32199719355182
At time: 518.1440920829773 and batch: 1050, loss is 5.273566732406616 and perplexity is 195.1106302696914
At time: 519.306535243988 and batch: 1100, loss is 5.241725797653198 and perplexity is 188.99598995272362
At time: 520.468578338623 and batch: 1150, loss is 5.286308469772339 and perplexity is 197.61258444972648
At time: 521.6316180229187 and batch: 1200, loss is 5.300007753372192 and perplexity is 200.33836326666483
At time: 522.7942249774933 and batch: 1250, loss is 5.302905340194702 and perplexity is 200.91970290239902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1815871635492705 and perplexity of 177.9650465920843
Finished 17 epochs...
Completing Train Step...
At time: 525.6574020385742 and batch: 50, loss is 5.282355165481567 and perplexity is 196.83290394429815
At time: 526.8440172672272 and batch: 100, loss is 5.301225738525391 and perplexity is 200.5825210788511
At time: 528.0051295757294 and batch: 150, loss is 5.219496717453003 and perplexity is 184.8411333408658
At time: 529.1671602725983 and batch: 200, loss is 5.255150089263916 and perplexity is 191.55023333050283
At time: 530.3292462825775 and batch: 250, loss is 5.284486169815064 and perplexity is 197.25280295998613
At time: 531.5055675506592 and batch: 300, loss is 5.281415414810181 and perplexity is 196.64801697809753
At time: 532.6821401119232 and batch: 350, loss is 5.299842691421508 and perplexity is 200.30529775463128
At time: 533.8520727157593 and batch: 400, loss is 5.274959707260132 and perplexity is 195.38260385351467
At time: 535.022230386734 and batch: 450, loss is 5.233747348785401 and perplexity is 187.49409447580288
At time: 536.1846404075623 and batch: 500, loss is 5.244583806991577 and perplexity is 189.53691487296206
At time: 537.3535842895508 and batch: 550, loss is 5.248067493438721 and perplexity is 190.19835350800395
At time: 538.5215048789978 and batch: 600, loss is 5.287544546127319 and perplexity is 197.8569997196593
At time: 539.6905319690704 and batch: 650, loss is 5.28015814781189 and perplexity is 196.4009332737128
At time: 540.8587965965271 and batch: 700, loss is 5.29804370880127 and perplexity is 199.9452759388198
At time: 542.0292465686798 and batch: 750, loss is 5.263346366882324 and perplexity is 193.12668390869558
At time: 543.2344741821289 and batch: 800, loss is 5.272363948822021 and perplexity is 194.87609548199433
At time: 544.3978297710419 and batch: 850, loss is 5.31748836517334 and perplexity is 203.87118843596213
At time: 545.5660724639893 and batch: 900, loss is 5.298849906921387 and perplexity is 200.10653643962985
At time: 546.7433121204376 and batch: 950, loss is 5.282008371353149 and perplexity is 196.76465528373225
At time: 547.9666368961334 and batch: 1000, loss is 5.2854580783844 and perplexity is 197.44460784281478
At time: 549.1442725658417 and batch: 1050, loss is 5.272164793014526 and perplexity is 194.83728864026932
At time: 550.3083562850952 and batch: 1100, loss is 5.241471405029297 and perplexity is 188.9479168819093
At time: 551.5257949829102 and batch: 1150, loss is 5.28413803100586 and perplexity is 197.1841435562465
At time: 552.6940407752991 and batch: 1200, loss is 5.297586669921875 and perplexity is 199.85391405352163
At time: 553.858410358429 and batch: 1250, loss is 5.298515338897705 and perplexity is 200.03959838949737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.178704700330748 and perplexity of 177.4528075009351
Finished 18 epochs...
Completing Train Step...
At time: 556.737487077713 and batch: 50, loss is 5.272690839767456 and perplexity is 194.93980912622845
At time: 557.9258549213409 and batch: 100, loss is 5.290366230010986 and perplexity is 198.41607802722032
At time: 559.0881345272064 and batch: 150, loss is 5.210585479736328 and perplexity is 183.20128744184555
At time: 560.2501678466797 and batch: 200, loss is 5.2471424579620365 and perplexity is 190.02249463379093
At time: 561.4141781330109 and batch: 250, loss is 5.277915697097779 and perplexity is 195.96100730114716
At time: 562.577024936676 and batch: 300, loss is 5.27456485748291 and perplexity is 195.30547230459965
At time: 563.7392132282257 and batch: 350, loss is 5.291497535705567 and perplexity is 198.64067428574785
At time: 564.901674747467 and batch: 400, loss is 5.268979654312134 and perplexity is 194.21769212579386
At time: 566.0641331672668 and batch: 450, loss is 5.227992038726807 and perplexity is 186.41810711841555
At time: 567.2270269393921 and batch: 500, loss is 5.238072061538697 and perplexity is 188.30670847187045
At time: 568.3897132873535 and batch: 550, loss is 5.243076057434082 and perplexity is 189.25135600316005
At time: 569.5511269569397 and batch: 600, loss is 5.282331686019898 and perplexity is 196.82828246792985
At time: 570.7220923900604 and batch: 650, loss is 5.276224870681762 and perplexity is 195.6299512115551
At time: 571.8840668201447 and batch: 700, loss is 5.292403736114502 and perplexity is 198.820764132439
At time: 573.0730707645416 and batch: 750, loss is 5.256639280319214 and perplexity is 191.83570072961103
At time: 574.2352786064148 and batch: 800, loss is 5.266803836822509 and perplexity is 193.79556927214972
At time: 575.3971161842346 and batch: 850, loss is 5.312121810913086 and perplexity is 202.78003313195367
At time: 576.559788942337 and batch: 900, loss is 5.294048528671265 and perplexity is 199.1480521320613
At time: 577.7214651107788 and batch: 950, loss is 5.276613206863403 and perplexity is 195.7059361527192
At time: 578.8837456703186 and batch: 1000, loss is 5.280970344543457 and perplexity is 196.56051426660235
At time: 580.0456964969635 and batch: 1050, loss is 5.2683291721344 and perplexity is 194.09139805893835
At time: 581.2082488536835 and batch: 1100, loss is 5.23911301612854 and perplexity is 188.50282926306193
At time: 582.3706891536713 and batch: 1150, loss is 5.280769233703613 and perplexity is 196.52098779123264
At time: 583.5326278209686 and batch: 1200, loss is 5.294295129776001 and perplexity is 199.19716831752692
At time: 584.6950833797455 and batch: 1250, loss is 5.295608224868775 and perplexity is 199.45890494667324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.177411824247263 and perplexity of 177.22353125499413
Finished 19 epochs...
Completing Train Step...
At time: 587.5852029323578 and batch: 50, loss is 5.265148906707764 and perplexity is 193.47511638516139
At time: 588.7492105960846 and batch: 100, loss is 5.283587665557861 and perplexity is 197.07565007501023
At time: 589.9137613773346 and batch: 150, loss is 5.203738946914672 and perplexity is 181.95127781322776
At time: 591.0771310329437 and batch: 200, loss is 5.240985345840454 and perplexity is 188.856099326887
At time: 592.2405297756195 and batch: 250, loss is 5.271780834197998 and perplexity is 194.7624935055521
At time: 593.4043302536011 and batch: 300, loss is 5.2693662929534915 and perplexity is 194.29279870902295
At time: 594.5684716701508 and batch: 350, loss is 5.286777801513672 and perplexity is 197.70535207578845
At time: 595.7333528995514 and batch: 400, loss is 5.263039684295654 and perplexity is 193.06746439897984
At time: 596.8966066837311 and batch: 450, loss is 5.221539640426636 and perplexity is 185.21913552184293
At time: 598.0596423149109 and batch: 500, loss is 5.23378851890564 and perplexity is 187.50181378911802
At time: 599.2312767505646 and batch: 550, loss is 5.240824604034424 and perplexity is 188.82574469609582
At time: 600.3932459354401 and batch: 600, loss is 5.279526920318603 and perplexity is 196.27699872448582
At time: 601.5573170185089 and batch: 650, loss is 5.275652236938477 and perplexity is 195.517958968623
At time: 602.7477536201477 and batch: 700, loss is 5.294548358917236 and perplexity is 199.24761723269467
At time: 603.9103050231934 and batch: 750, loss is 5.254017601013183 and perplexity is 191.3334277299003
At time: 605.0730156898499 and batch: 800, loss is 5.265695419311523 and perplexity is 193.58088187323335
At time: 606.2371962070465 and batch: 850, loss is 5.30946117401123 and perplexity is 202.24122619550286
At time: 607.4012703895569 and batch: 900, loss is 5.289467735290527 and perplexity is 198.23788229460925
At time: 608.5647792816162 and batch: 950, loss is 5.272058830261231 and perplexity is 194.81664423850853
At time: 609.7285158634186 and batch: 1000, loss is 5.276725587844848 and perplexity is 195.72793101377758
At time: 610.8910427093506 and batch: 1050, loss is 5.263614778518677 and perplexity is 193.17852831545665
At time: 612.0542771816254 and batch: 1100, loss is 5.235750284194946 and perplexity is 187.87000937767004
At time: 613.217404127121 and batch: 1150, loss is 5.277367715835571 and perplexity is 195.85365375757434
At time: 614.381664276123 and batch: 1200, loss is 5.290455455780029 and perplexity is 198.43378264421517
At time: 615.5444052219391 and batch: 1250, loss is 5.290821828842163 and perplexity is 198.50649675622589
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.174463870751596 and perplexity of 176.70185384468934
Finished 20 epochs...
Completing Train Step...
At time: 618.4081315994263 and batch: 50, loss is 5.257492914199829 and perplexity is 191.999528097613
At time: 619.5940017700195 and batch: 100, loss is 5.275925636291504 and perplexity is 195.57142075998675
At time: 620.7563118934631 and batch: 150, loss is 5.195138282775879 and perplexity is 180.39308632906656
At time: 621.9196321964264 and batch: 200, loss is 5.2338319015502925 and perplexity is 187.50994829012419
At time: 623.0815143585205 and batch: 250, loss is 5.26315616607666 and perplexity is 193.08995455090846
At time: 624.2434775829315 and batch: 300, loss is 5.262386884689331 and perplexity is 192.94147116286817
At time: 625.405152797699 and batch: 350, loss is 5.2812126350402835 and perplexity is 196.60814478123802
At time: 626.5664629936218 and batch: 400, loss is 5.258028898239136 and perplexity is 192.10246436386143
At time: 627.7428703308105 and batch: 450, loss is 5.215350942611694 and perplexity is 184.0764099015234
At time: 628.9110999107361 and batch: 500, loss is 5.227347974777222 and perplexity is 186.29808059259884
At time: 630.0731537342072 and batch: 550, loss is 5.234738626480103 and perplexity is 187.6800453387897
At time: 631.2594149112701 and batch: 600, loss is 5.274643144607544 and perplexity is 195.3207628069685
At time: 632.4194095134735 and batch: 650, loss is 5.2695490169525145 and perplexity is 194.32830390991182
At time: 633.5818967819214 and batch: 700, loss is 5.288139295578003 and perplexity is 197.97471006216762
At time: 634.7439668178558 and batch: 750, loss is 5.24779541015625 and perplexity is 190.14661075514923
At time: 635.9143731594086 and batch: 800, loss is 5.261326198577881 and perplexity is 192.73692932060533
At time: 637.0775923728943 and batch: 850, loss is 5.304053564071655 and perplexity is 201.15053620141344
At time: 638.2473208904266 and batch: 900, loss is 5.284776697158813 and perplexity is 197.3101186183571
At time: 639.418023109436 and batch: 950, loss is 5.26835976600647 and perplexity is 194.09733615717477
At time: 640.5788023471832 and batch: 1000, loss is 5.270939455032349 and perplexity is 194.59869331996146
At time: 641.739825963974 and batch: 1050, loss is 5.260297441482544 and perplexity is 192.53875179278063
At time: 642.9014575481415 and batch: 1100, loss is 5.232059555053711 and perplexity is 187.17791002054676
At time: 644.0706832408905 and batch: 1150, loss is 5.27313401222229 and perplexity is 195.026220226079
At time: 645.2324829101562 and batch: 1200, loss is 5.285101919174195 and perplexity is 197.374298648603
At time: 646.3934714794159 and batch: 1250, loss is 5.286066493988037 and perplexity is 197.56477277448337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.173048033331432 and perplexity of 176.4518497721273
Finished 21 epochs...
Completing Train Step...
At time: 649.2835311889648 and batch: 50, loss is 5.251610813140869 and perplexity is 190.873482473828
At time: 650.4475629329681 and batch: 100, loss is 5.270570936203003 and perplexity is 194.52699324953144
At time: 651.6120448112488 and batch: 150, loss is 5.189632329940796 and perplexity is 179.40257984725787
At time: 652.77521276474 and batch: 200, loss is 5.2293627738952635 and perplexity is 186.6738121858068
At time: 653.938553571701 and batch: 250, loss is 5.258579912185669 and perplexity is 192.2083446689728
At time: 655.1012272834778 and batch: 300, loss is 5.256366033554077 and perplexity is 191.7832894058873
At time: 656.2661421298981 and batch: 350, loss is 5.276793661117554 and perplexity is 195.74125530811065
At time: 657.4297289848328 and batch: 400, loss is 5.252927522659302 and perplexity is 191.12497293862765
At time: 658.5972034931183 and batch: 450, loss is 5.211664381027222 and perplexity is 183.39905021143028
At time: 659.7646381855011 and batch: 500, loss is 5.223309164047241 and perplexity is 185.54717530861558
At time: 660.9585893154144 and batch: 550, loss is 5.2290182876586915 and perplexity is 186.60951670187086
At time: 662.1374406814575 and batch: 600, loss is 5.269866552352905 and perplexity is 194.3900198236762
At time: 663.3013875484467 and batch: 650, loss is 5.265339794158936 and perplexity is 193.51205188214263
At time: 664.4654159545898 and batch: 700, loss is 5.283640365600586 and perplexity is 197.08603624386265
At time: 665.6288890838623 and batch: 750, loss is 5.244138164520264 and perplexity is 189.4524679917689
At time: 666.7925522327423 and batch: 800, loss is 5.258533535003662 and perplexity is 192.19943079429052
At time: 667.9573919773102 and batch: 850, loss is 5.301573076248169 and perplexity is 200.6522030558408
At time: 669.1207847595215 and batch: 900, loss is 5.281445446014405 and perplexity is 196.6539226435323
At time: 670.284187078476 and batch: 950, loss is 5.2639467144012455 and perplexity is 193.24266184426727
At time: 671.4477815628052 and batch: 1000, loss is 5.267076663970947 and perplexity is 193.84844917790303
At time: 672.6116743087769 and batch: 1050, loss is 5.2566464138031 and perplexity is 191.83706919137197
At time: 673.775315284729 and batch: 1100, loss is 5.228721599578858 and perplexity is 186.55416009491066
At time: 674.9378023147583 and batch: 1150, loss is 5.268707218170166 and perplexity is 194.16478741395426
At time: 676.1019968986511 and batch: 1200, loss is 5.280903749465942 and perplexity is 196.54742473977228
At time: 677.2658295631409 and batch: 1250, loss is 5.281664714813233 and perplexity is 196.69704744072362
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.170938979100137 and perplexity of 176.08009541459614
Finished 22 epochs...
Completing Train Step...
At time: 680.130176782608 and batch: 50, loss is 5.246149272918701 and perplexity is 189.83386082386863
At time: 681.3196334838867 and batch: 100, loss is 5.265196866989136 and perplexity is 193.4843957286999
At time: 682.480295419693 and batch: 150, loss is 5.184800043106079 and perplexity is 178.53774636716062
At time: 683.6422927379608 and batch: 200, loss is 5.223936357498169 and perplexity is 185.66358578393513
At time: 684.804036617279 and batch: 250, loss is 5.25468415260315 and perplexity is 191.46100384367475
At time: 685.965788602829 and batch: 300, loss is 5.253161401748657 and perplexity is 191.16967830087242
At time: 687.1269989013672 and batch: 350, loss is 5.272546138763428 and perplexity is 194.91160318088623
At time: 688.2880899906158 and batch: 400, loss is 5.248289737701416 and perplexity is 190.24062869838045
At time: 689.4494915008545 and batch: 450, loss is 5.205461769104004 and perplexity is 182.26501769343395
At time: 690.6359348297119 and batch: 500, loss is 5.2190343856811525 and perplexity is 184.75569516409115
At time: 691.798094034195 and batch: 550, loss is 5.227184171676636 and perplexity is 186.2675668885525
At time: 692.9584426879883 and batch: 600, loss is 5.267390718460083 and perplexity is 193.90933771423806
At time: 694.1193518638611 and batch: 650, loss is 5.26146819114685 and perplexity is 192.76429847539714
At time: 695.2797672748566 and batch: 700, loss is 5.278474283218384 and perplexity is 196.0704989774238
At time: 696.441113948822 and batch: 750, loss is 5.240064544677734 and perplexity is 188.68228044963357
At time: 697.6020255088806 and batch: 800, loss is 5.2527382278442385 and perplexity is 191.0887973962505
At time: 698.7643692493439 and batch: 850, loss is 5.295902357101441 and perplexity is 199.51758086852723
At time: 699.9277160167694 and batch: 900, loss is 5.276884994506836 and perplexity is 195.7591338368211
At time: 701.0907843112946 and batch: 950, loss is 5.259613246917724 and perplexity is 192.4070628808429
At time: 702.2525653839111 and batch: 1000, loss is 5.262405042648315 and perplexity is 192.94497461799563
At time: 703.4132208824158 and batch: 1050, loss is 5.2532182312011715 and perplexity is 191.180542677733
At time: 704.5747969150543 and batch: 1100, loss is 5.224625625610352 and perplexity is 185.79160188685
At time: 705.7362759113312 and batch: 1150, loss is 5.264505882263183 and perplexity is 193.3507471464233
At time: 706.8975629806519 and batch: 1200, loss is 5.2765326499938965 and perplexity is 195.69017133014978
At time: 708.0604848861694 and batch: 1250, loss is 5.277789211273193 and perplexity is 195.93622257904306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.168235166229471 and perplexity of 175.60465083258512
Finished 23 epochs...
Completing Train Step...
At time: 710.9223265647888 and batch: 50, loss is 5.240154075622558 and perplexity is 188.69917410871514
At time: 712.109783411026 and batch: 100, loss is 5.260051279067993 and perplexity is 192.49136182179885
At time: 713.2726316452026 and batch: 150, loss is 5.17913405418396 and perplexity is 177.52901390618513
At time: 714.4362342357635 and batch: 200, loss is 5.218101816177368 and perplexity is 184.58347795187194
At time: 715.5995674133301 and batch: 250, loss is 5.249032745361328 and perplexity is 190.38203146789496
At time: 716.7626576423645 and batch: 300, loss is 5.247767734527588 and perplexity is 190.14134840097833
At time: 717.9252955913544 and batch: 350, loss is 5.266814956665039 and perplexity is 193.79772426034447
At time: 719.1130003929138 and batch: 400, loss is 5.243519353866577 and perplexity is 189.33526905192454
At time: 720.2759647369385 and batch: 450, loss is 5.200188140869141 and perplexity is 181.3063498007355
At time: 721.4387426376343 and batch: 500, loss is 5.21325800895691 and perplexity is 183.69155306858212
At time: 722.6021959781647 and batch: 550, loss is 5.220881128311158 and perplexity is 185.09720662731655
At time: 723.7647528648376 and batch: 600, loss is 5.260899667739868 and perplexity is 192.65473860630942
At time: 724.9267952442169 and batch: 650, loss is 5.256451902389526 and perplexity is 191.79975832068035
At time: 726.0889227390289 and batch: 700, loss is 5.274413661956787 and perplexity is 195.27594522319737
At time: 727.2517349720001 and batch: 750, loss is 5.236153049468994 and perplexity is 187.9456921336523
At time: 728.4144697189331 and batch: 800, loss is 5.2509581565856935 and perplexity is 190.74894828772935
At time: 729.5777549743652 and batch: 850, loss is 5.2925439453125 and perplexity is 198.84864258668546
At time: 730.7408802509308 and batch: 900, loss is 5.273014211654663 and perplexity is 195.00285737366337
At time: 731.9050240516663 and batch: 950, loss is 5.255918169021607 and perplexity is 191.69741570397005
At time: 733.068708896637 and batch: 1000, loss is 5.257980451583863 and perplexity is 192.09315786742937
At time: 734.2319250106812 and batch: 1050, loss is 5.250648059844971 and perplexity is 190.68980683082947
At time: 735.3944101333618 and batch: 1100, loss is 5.220706377029419 and perplexity is 185.0648634792978
At time: 736.5579943656921 and batch: 1150, loss is 5.261344347000122 and perplexity is 192.74042722352078
At time: 737.7208375930786 and batch: 1200, loss is 5.273696308135986 and perplexity is 195.13591350992826
At time: 738.885436296463 and batch: 1250, loss is 5.2735077667236325 and perplexity is 195.09912577730867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1667845788663325 and perplexity of 175.3501056098553
Finished 24 epochs...
Completing Train Step...
At time: 741.799439907074 and batch: 50, loss is 5.236087284088135 and perplexity is 187.93333222005992
At time: 742.9622597694397 and batch: 100, loss is 5.255592746734619 and perplexity is 191.63504324178712
At time: 744.126443862915 and batch: 150, loss is 5.172981195449829 and perplexity is 176.44005649840662
At time: 745.288290977478 and batch: 200, loss is 5.213341188430786 and perplexity is 183.70683307080444
At time: 746.4500758647919 and batch: 250, loss is 5.244923086166382 and perplexity is 189.60123171110004
At time: 747.6118896007538 and batch: 300, loss is 5.242978096008301 and perplexity is 189.23281757853508
At time: 748.799574136734 and batch: 350, loss is 5.2626718044281 and perplexity is 192.99645182859618
At time: 749.9629848003387 and batch: 400, loss is 5.238884801864624 and perplexity is 188.45981513704086
At time: 751.1325242519379 and batch: 450, loss is 5.195204849243164 and perplexity is 180.4050948592244
At time: 752.2948310375214 and batch: 500, loss is 5.209111623764038 and perplexity is 182.9314740121274
At time: 753.4570560455322 and batch: 550, loss is 5.2167047595977785 and perplexity is 184.32578443763865
At time: 754.6187648773193 and batch: 600, loss is 5.256881189346314 and perplexity is 191.88211313090062
At time: 755.7800769805908 and batch: 650, loss is 5.25203872680664 and perplexity is 190.95517732333667
At time: 756.941623210907 and batch: 700, loss is 5.269693374633789 and perplexity is 194.35635871818508
At time: 758.1037082672119 and batch: 750, loss is 5.232650995254517 and perplexity is 187.28864730524865
At time: 759.2657063007355 and batch: 800, loss is 5.2464978218078615 and perplexity is 189.90003873763303
At time: 760.4281685352325 and batch: 850, loss is 5.288571844100952 and perplexity is 198.0603622536152
At time: 761.5906095504761 and batch: 900, loss is 5.268664093017578 and perplexity is 194.15641420841888
At time: 762.7527484893799 and batch: 950, loss is 5.251267414093018 and perplexity is 190.8079479545763
At time: 763.916051864624 and batch: 1000, loss is 5.25191575050354 and perplexity is 190.93169580543622
At time: 765.0795965194702 and batch: 1050, loss is 5.245697002410889 and perplexity is 189.7480239794337
At time: 766.2412304878235 and batch: 1100, loss is 5.2168603515625 and perplexity is 184.3544662798623
At time: 767.4036297798157 and batch: 1150, loss is 5.258541440963745 and perplexity is 192.20095032132502
At time: 768.5740768909454 and batch: 1200, loss is 5.273161249160767 and perplexity is 195.03153221558156
At time: 769.7408533096313 and batch: 1250, loss is 5.270040264129639 and perplexity is 194.42379059250428
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.164663050296533 and perplexity of 174.97848968736827
Finished 25 epochs...
Completing Train Step...
At time: 772.6080543994904 and batch: 50, loss is 5.231144933700562 and perplexity is 187.00679137351725
At time: 773.7945213317871 and batch: 100, loss is 5.250751209259033 and perplexity is 190.70947738715753
At time: 774.956216096878 and batch: 150, loss is 5.1680105113983155 and perplexity is 175.56520483043556
At time: 776.119127035141 and batch: 200, loss is 5.207379035949707 and perplexity is 182.61480357828052
At time: 777.2816934585571 and batch: 250, loss is 5.240439281463623 and perplexity is 188.7529998907239
At time: 778.4691517353058 and batch: 300, loss is 5.239258089065552 and perplexity is 188.53017790586418
At time: 779.6317591667175 and batch: 350, loss is 5.25826024055481 and perplexity is 192.146910933802
At time: 780.7941265106201 and batch: 400, loss is 5.234294786453247 and perplexity is 187.59676390561015
At time: 781.9562406539917 and batch: 450, loss is 5.18929202079773 and perplexity is 179.34153789620228
At time: 783.1180186271667 and batch: 500, loss is 5.203951749801636 and perplexity is 181.99000169056342
At time: 784.2798736095428 and batch: 550, loss is 5.211430711746216 and perplexity is 183.35620049375646
At time: 785.4423561096191 and batch: 600, loss is 5.253284978866577 and perplexity is 191.19330395851583
At time: 786.603312253952 and batch: 650, loss is 5.247894153594971 and perplexity is 190.16538741237687
At time: 787.7639758586884 and batch: 700, loss is 5.266769104003906 and perplexity is 193.78883832268914
At time: 788.9254491329193 and batch: 750, loss is 5.227424402236938 and perplexity is 186.3123194257606
At time: 790.0877294540405 and batch: 800, loss is 5.241442642211914 and perplexity is 188.94248228563865
At time: 791.2500092983246 and batch: 850, loss is 5.2834326171875 and perplexity is 197.0450961853548
At time: 792.4126780033112 and batch: 900, loss is 5.263719940185547 and perplexity is 193.19884435971372
At time: 793.575088262558 and batch: 950, loss is 5.245762023925781 and perplexity is 189.76036208451745
At time: 794.7371461391449 and batch: 1000, loss is 5.24722770690918 and perplexity is 190.03869454189473
At time: 795.8995680809021 and batch: 1050, loss is 5.240506019592285 and perplexity is 188.7655973330761
At time: 797.0634393692017 and batch: 1100, loss is 5.213970947265625 and perplexity is 183.82256050831433
At time: 798.2256329059601 and batch: 1150, loss is 5.253834867477417 and perplexity is 191.29846789040582
At time: 799.3881137371063 and batch: 1200, loss is 5.266968584060669 and perplexity is 193.82749918706523
At time: 800.5516340732574 and batch: 1250, loss is 5.265895357131958 and perplexity is 193.61958988230148
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.163768016508896 and perplexity of 174.82194809244686
Finished 26 epochs...
Completing Train Step...
At time: 803.4646265506744 and batch: 50, loss is 5.226774339675903 and perplexity is 186.19124411976776
At time: 804.6277873516083 and batch: 100, loss is 5.2463703632354735 and perplexity is 189.87583589226193
At time: 805.7920718193054 and batch: 150, loss is 5.163644971847535 and perplexity is 174.8004385083922
At time: 806.9562904834747 and batch: 200, loss is 5.202361640930175 and perplexity is 181.7008477284242
At time: 808.1448493003845 and batch: 250, loss is 5.2363064670562744 and perplexity is 187.97452852022704
At time: 809.3087725639343 and batch: 300, loss is 5.233721208572388 and perplexity is 187.48919340429245
At time: 810.472697019577 and batch: 350, loss is 5.252912340164184 and perplexity is 191.1220712066869
At time: 811.6390881538391 and batch: 400, loss is 5.230806474685669 and perplexity is 186.94350794915752
At time: 812.8082840442657 and batch: 450, loss is 5.186980686187744 and perplexity is 178.92749826904068
At time: 813.9722337722778 and batch: 500, loss is 5.199016475677491 and perplexity is 181.09404386167594
At time: 815.136266708374 and batch: 550, loss is 5.206335029602051 and perplexity is 182.424252049979
At time: 816.2996153831482 and batch: 600, loss is 5.2482007884979245 and perplexity is 190.22370769855186
At time: 817.462849855423 and batch: 650, loss is 5.241391410827637 and perplexity is 188.93280274867257
At time: 818.6273925304413 and batch: 700, loss is 5.261482744216919 and perplexity is 192.7671038081527
At time: 819.7916784286499 and batch: 750, loss is 5.224966812133789 and perplexity is 185.8550022926486
At time: 820.9561336040497 and batch: 800, loss is 5.240423936843872 and perplexity is 188.75010356993513
At time: 822.1208362579346 and batch: 850, loss is 5.28092885017395 and perplexity is 196.55235828120786
At time: 823.2847628593445 and batch: 900, loss is 5.259327535629272 and perplexity is 192.35209786343682
At time: 824.4492824077606 and batch: 950, loss is 5.2422302722930905 and perplexity is 189.09135768997598
At time: 825.6129784584045 and batch: 1000, loss is 5.24326922416687 and perplexity is 189.28791660030672
At time: 826.776547908783 and batch: 1050, loss is 5.237331857681275 and perplexity is 188.16737469394286
At time: 827.9415822029114 and batch: 1100, loss is 5.210716304779052 and perplexity is 183.22525632593326
At time: 829.1052930355072 and batch: 1150, loss is 5.2508024311065675 and perplexity is 190.71924612911593
At time: 830.270920753479 and batch: 1200, loss is 5.26152943611145 and perplexity is 192.77610467956507
At time: 831.4373931884766 and batch: 1250, loss is 5.262216815948486 and perplexity is 192.908660639912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.163391558793339 and perplexity of 174.75614740760375
Finished 27 epochs...
Completing Train Step...
At time: 834.3103773593903 and batch: 50, loss is 5.221336803436279 and perplexity is 185.18157003980065
At time: 835.4965682029724 and batch: 100, loss is 5.2409068775177 and perplexity is 188.84128068693545
At time: 836.6604473590851 and batch: 150, loss is 5.158839893341065 and perplexity is 173.96252341450042
At time: 837.8459224700928 and batch: 200, loss is 5.198927612304687 and perplexity is 181.0779519491452
At time: 839.0129582881927 and batch: 250, loss is 5.232449235916138 and perplexity is 187.2508638833898
At time: 840.1769726276398 and batch: 300, loss is 5.22972954750061 and perplexity is 186.7422917704104
At time: 841.3381404876709 and batch: 350, loss is 5.250000648498535 and perplexity is 190.56639204061597
At time: 842.5061521530151 and batch: 400, loss is 5.228712005615234 and perplexity is 186.55237030967047
At time: 843.6711523532867 and batch: 450, loss is 5.182970123291016 and perplexity is 178.2113353514246
At time: 844.8322124481201 and batch: 500, loss is 5.198095407485962 and perplexity is 180.92732069169082
At time: 845.9941129684448 and batch: 550, loss is 5.208446817398071 and perplexity is 182.8099004195953
At time: 847.155378818512 and batch: 600, loss is 5.254306049346924 and perplexity is 191.38862549878732
At time: 848.318523645401 and batch: 650, loss is 5.238633270263672 and perplexity is 188.41241749927605
At time: 849.4883854389191 and batch: 700, loss is 5.260777816772461 and perplexity is 192.6312648702125
At time: 850.6495504379272 and batch: 750, loss is 5.223343286514282 and perplexity is 185.55350674401112
At time: 851.8163313865662 and batch: 800, loss is 5.237757186889649 and perplexity is 188.24742479708
At time: 852.9812161922455 and batch: 850, loss is 5.278863391876221 and perplexity is 196.14680655112826
At time: 854.1442635059357 and batch: 900, loss is 5.2559703540802 and perplexity is 191.70741970586823
At time: 855.3063781261444 and batch: 950, loss is 5.23827428817749 and perplexity is 188.3447929553058
At time: 856.4680533409119 and batch: 1000, loss is 5.241840810775757 and perplexity is 189.01772822174823
At time: 857.6290802955627 and batch: 1050, loss is 5.234871892929077 and perplexity is 187.70505845864324
At time: 858.789844751358 and batch: 1100, loss is 5.207153825759888 and perplexity is 182.5736814944329
At time: 859.9508225917816 and batch: 1150, loss is 5.247097063064575 and perplexity is 190.01386877791805
At time: 861.1177241802216 and batch: 1200, loss is 5.257677316665649 and perplexity is 192.03493654863308
At time: 862.2816262245178 and batch: 1250, loss is 5.259006958007813 and perplexity is 192.29044396837855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.162176647325502 and perplexity of 174.54396307872926
Finished 28 epochs...
Completing Train Step...
At time: 865.1468703746796 and batch: 50, loss is 5.217171926498413 and perplexity is 184.41191546027633
At time: 866.3352360725403 and batch: 100, loss is 5.2370395088195805 and perplexity is 188.11237221649068
At time: 867.4982798099518 and batch: 150, loss is 5.155796957015991 and perplexity is 173.43397111613763
At time: 868.6767342090607 and batch: 200, loss is 5.195176267623902 and perplexity is 180.3999386631767
At time: 869.8511824607849 and batch: 250, loss is 5.228995990753174 and perplexity is 186.6053559334946
At time: 871.0197906494141 and batch: 300, loss is 5.226601829528809 and perplexity is 186.15912701120018
At time: 872.1835539340973 and batch: 350, loss is 5.247252197265625 and perplexity is 190.04334871425334
At time: 873.3464524745941 and batch: 400, loss is 5.2256020736694335 and perplexity is 185.97310633631852
At time: 874.5096249580383 and batch: 450, loss is 5.180646123886109 and perplexity is 177.79765319884893
At time: 875.6728599071503 and batch: 500, loss is 5.193948755264282 and perplexity is 180.1786313653033
At time: 876.8363327980042 and batch: 550, loss is 5.202580652236938 and perplexity is 181.74064662657042
At time: 877.9998075962067 and batch: 600, loss is 5.247050123214722 and perplexity is 190.0049497647778
At time: 879.1616065502167 and batch: 650, loss is 5.234912376403809 and perplexity is 187.71265756545245
At time: 880.324310541153 and batch: 700, loss is 5.256816768646241 and perplexity is 191.86975234899063
At time: 881.4879338741302 and batch: 750, loss is 5.218311882019043 and perplexity is 184.62225670843057
At time: 882.6517436504364 and batch: 800, loss is 5.23422685623169 and perplexity is 187.58402084869888
At time: 883.8173978328705 and batch: 850, loss is 5.2746749687194825 and perplexity is 195.32697881569695
At time: 884.9813165664673 and batch: 900, loss is 5.252230005264282 and perplexity is 190.99170642863788
At time: 886.1446900367737 and batch: 950, loss is 5.234325475692749 and perplexity is 187.60252119597055
At time: 887.3071486949921 and batch: 1000, loss is 5.236118612289428 and perplexity is 187.9392199255466
At time: 888.4700272083282 and batch: 1050, loss is 5.230559635162353 and perplexity is 186.89736859751048
At time: 889.6334252357483 and batch: 1100, loss is 5.2032285404205325 and perplexity is 181.85843239588246
At time: 890.7983298301697 and batch: 1150, loss is 5.2438336372375485 and perplexity is 189.39478333021012
At time: 891.9617593288422 and batch: 1200, loss is 5.25404574394226 and perplexity is 191.33881248875807
At time: 893.124792098999 and batch: 1250, loss is 5.255914115905762 and perplexity is 191.69663873371164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.160906937870666 and perplexity of 174.32248359557562
Finished 29 epochs...
Completing Train Step...
At time: 896.007830619812 and batch: 50, loss is 5.213090581893921 and perplexity is 183.66080070581674
At time: 897.1695342063904 and batch: 100, loss is 5.231537008285523 and perplexity is 187.0801263590825
At time: 898.331796169281 and batch: 150, loss is 5.1511768627166745 and perplexity is 172.63453796515714
At time: 899.4929378032684 and batch: 200, loss is 5.1911664390563965 and perplexity is 179.6780141995197
At time: 900.6544201374054 and batch: 250, loss is 5.224968519210815 and perplexity is 185.85531956172406
At time: 901.8155579566956 and batch: 300, loss is 5.2216698169708256 and perplexity is 185.2432482782465
At time: 902.9775114059448 and batch: 350, loss is 5.2427888679504395 and perplexity is 189.19701280772153
At time: 904.138845205307 and batch: 400, loss is 5.220626382827759 and perplexity is 185.0500599553944
At time: 905.3010506629944 and batch: 450, loss is 5.174763212203979 and perplexity is 176.7547559517597
At time: 906.461859703064 and batch: 500, loss is 5.189972467422486 and perplexity is 179.46361176800923
At time: 907.6230380535126 and batch: 550, loss is 5.198301544189453 and perplexity is 180.96462029742656
At time: 908.7840704917908 and batch: 600, loss is 5.241678380966187 and perplexity is 188.98702860148208
At time: 909.9443559646606 and batch: 650, loss is 5.230916204452515 and perplexity is 186.96402234219738
At time: 911.1055924892426 and batch: 700, loss is 5.253067350387573 and perplexity is 191.15169937791455
At time: 912.267769575119 and batch: 750, loss is 5.215282831192017 and perplexity is 184.0638726228866
At time: 913.4289450645447 and batch: 800, loss is 5.231604919433594 and perplexity is 187.09283161665428
At time: 914.5908205509186 and batch: 850, loss is 5.271455574035644 and perplexity is 194.69915532654562
At time: 915.7545211315155 and batch: 900, loss is 5.249235172271728 and perplexity is 190.42057381519325
At time: 916.9168231487274 and batch: 950, loss is 5.231320457458496 and perplexity is 187.03961838917468
At time: 918.0779161453247 and batch: 1000, loss is 5.232437582015991 and perplexity is 187.24868169323517
At time: 919.2389671802521 and batch: 1050, loss is 5.227127056121827 and perplexity is 186.2569284169407
At time: 920.4005315303802 and batch: 1100, loss is 5.20040657043457 and perplexity is 181.34595679344358
At time: 921.5633249282837 and batch: 1150, loss is 5.239836530685425 and perplexity is 188.63926315404922
At time: 922.7249739170074 and batch: 1200, loss is 5.251075973510742 and perplexity is 190.77142306620158
At time: 923.8870809078217 and batch: 1250, loss is 5.2519794273376466 and perplexity is 190.9438541184531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.159641683536725 and perplexity of 174.10206079256196
Finished 30 epochs...
Completing Train Step...
At time: 926.7478594779968 and batch: 50, loss is 5.209725580215454 and perplexity is 183.04382045514697
At time: 927.9347558021545 and batch: 100, loss is 5.227925662994385 and perplexity is 186.40573389066432
At time: 929.0968470573425 and batch: 150, loss is 5.1487153625488284 and perplexity is 172.21012058699844
At time: 930.2687890529633 and batch: 200, loss is 5.187245006561279 and perplexity is 178.97479870317773
At time: 931.4315707683563 and batch: 250, loss is 5.2212651920318605 and perplexity is 185.16830940230972
At time: 932.5954074859619 and batch: 300, loss is 5.218235425949096 and perplexity is 184.60814175585182
At time: 933.7581603527069 and batch: 350, loss is 5.238176794052124 and perplexity is 188.32643133953863
At time: 934.9295210838318 and batch: 400, loss is 5.216854085922241 and perplexity is 184.35331118471532
At time: 936.0928580760956 and batch: 450, loss is 5.171081886291504 and perplexity is 176.10526032456806
At time: 937.2552366256714 and batch: 500, loss is 5.186159076690674 and perplexity is 178.78055011244277
At time: 938.4175112247467 and batch: 550, loss is 5.1938189601898195 and perplexity is 180.15524658407622
At time: 939.5870380401611 and batch: 600, loss is 5.236959190368652 and perplexity is 188.09726392889905
At time: 940.7493531703949 and batch: 650, loss is 5.2272714138031 and perplexity is 186.2838179760586
At time: 941.9113855361938 and batch: 700, loss is 5.249050664901733 and perplexity is 190.38544305696726
At time: 943.0734813213348 and batch: 750, loss is 5.210933637619019 and perplexity is 183.2650815187485
At time: 944.2420470714569 and batch: 800, loss is 5.2282846069335935 and perplexity is 186.47265510884247
At time: 945.4070644378662 and batch: 850, loss is 5.26821967124939 and perplexity is 194.07014604265657
At time: 946.5784785747528 and batch: 900, loss is 5.245368518829346 and perplexity is 189.6857051048501
At time: 947.7415552139282 and batch: 950, loss is 5.227626438140869 and perplexity is 186.3499650063807
At time: 948.9062476158142 and batch: 1000, loss is 5.230358448028564 and perplexity is 186.8597710338095
At time: 950.0685520172119 and batch: 1050, loss is 5.223579301834106 and perplexity is 185.59730538262127
At time: 951.2312014102936 and batch: 1100, loss is 5.196182556152344 and perplexity is 180.58156442061681
At time: 952.3935074806213 and batch: 1150, loss is 5.236333494186401 and perplexity is 187.979609000925
At time: 953.5562386512756 and batch: 1200, loss is 5.2471991062164305 and perplexity is 190.03325938130672
At time: 954.7437958717346 and batch: 1250, loss is 5.248389883041382 and perplexity is 190.25968136481868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.158305147268476 and perplexity of 173.86952250647823
Finished 31 epochs...
Completing Train Step...
At time: 957.6268639564514 and batch: 50, loss is 5.20497875213623 and perplexity is 182.1770018555392
At time: 958.7883968353271 and batch: 100, loss is 5.221881971359253 and perplexity is 185.28255261544174
At time: 959.9501090049744 and batch: 150, loss is 5.144708032608032 and perplexity is 171.5213987042371
At time: 961.1120686531067 and batch: 200, loss is 5.18313081741333 and perplexity is 178.23997516660873
At time: 962.273955821991 and batch: 250, loss is 5.215661325454712 and perplexity is 184.13355292860993
At time: 963.4350287914276 and batch: 300, loss is 5.214014949798584 and perplexity is 183.83064934455513
At time: 964.5969684123993 and batch: 350, loss is 5.2337119674682615 and perplexity is 187.4874608051392
At time: 965.7582042217255 and batch: 400, loss is 5.212839241027832 and perplexity is 183.61464504174606
At time: 966.9201662540436 and batch: 450, loss is 5.167720603942871 and perplexity is 175.51431454573145
At time: 968.0825002193451 and batch: 500, loss is 5.182250356674194 and perplexity is 178.0831109328631
At time: 969.244330406189 and batch: 550, loss is 5.189418354034424 and perplexity is 179.36419612437194
At time: 970.4050612449646 and batch: 600, loss is 5.23312481880188 and perplexity is 187.37741010378704
At time: 971.5658805370331 and batch: 650, loss is 5.223504981994629 and perplexity is 185.5835123332328
At time: 972.7270715236664 and batch: 700, loss is 5.245882997512817 and perplexity is 189.78331946478636
At time: 973.88898396492 and batch: 750, loss is 5.2069958972930905 and perplexity is 182.54485018953872
At time: 975.051326751709 and batch: 800, loss is 5.223378381729126 and perplexity is 185.56001889846735
At time: 976.2136926651001 and batch: 850, loss is 5.263090419769287 and perplexity is 193.07726001671978
At time: 977.3757297992706 and batch: 900, loss is 5.239593734741211 and perplexity is 188.59346786571606
At time: 978.5371778011322 and batch: 950, loss is 5.223078966140747 and perplexity is 185.5044676530987
At time: 979.6990973949432 and batch: 1000, loss is 5.2250699710845945 and perplexity is 185.8741758886341
At time: 980.8622894287109 and batch: 1050, loss is 5.218730583190918 and perplexity is 184.69957444905336
At time: 982.0268285274506 and batch: 1100, loss is 5.1919543647766115 and perplexity is 179.8196429173922
At time: 983.1882767677307 and batch: 1150, loss is 5.232155847549438 and perplexity is 187.1959347164552
At time: 984.376547574997 and batch: 1200, loss is 5.243270149230957 and perplexity is 189.2880917038416
At time: 985.5382022857666 and batch: 1250, loss is 5.24383318901062 and perplexity is 189.39469843838714
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.155645885606752 and perplexity of 173.40777218071605
Finished 32 epochs...
Completing Train Step...
At time: 988.4160804748535 and batch: 50, loss is 5.1996720600128175 and perplexity is 181.21280520483722
At time: 989.6112353801727 and batch: 100, loss is 5.216977434158325 and perplexity is 184.37605224297204
At time: 990.7804179191589 and batch: 150, loss is 5.139606599807739 and perplexity is 170.6486219138141
At time: 991.9429214000702 and batch: 200, loss is 5.1787897491455075 and perplexity is 177.46790029369183
At time: 993.1055512428284 and batch: 250, loss is 5.209900779724121 and perplexity is 183.07589245197224
At time: 994.2678444385529 and batch: 300, loss is 5.209898996353149 and perplexity is 183.07556596003116
At time: 995.4296808242798 and batch: 350, loss is 5.228545665740967 and perplexity is 186.5213417925602
At time: 996.5926983356476 and batch: 400, loss is 5.2072991943359375 and perplexity is 182.60022389970493
At time: 997.7541034221649 and batch: 450, loss is 5.161165571212768 and perplexity is 174.3675750329347
At time: 998.9155797958374 and batch: 500, loss is 5.1765835666656494 and perplexity is 177.0768052934249
At time: 1000.083753824234 and batch: 550, loss is 5.184363050460815 and perplexity is 178.45974372962704
At time: 1001.2543227672577 and batch: 600, loss is 5.226600599288941 and perplexity is 186.15889799096118
At time: 1002.4220175743103 and batch: 650, loss is 5.217653503417969 and perplexity is 184.50074536995353
At time: 1003.5871720314026 and batch: 700, loss is 5.239833650588989 and perplexity is 188.6387198555621
At time: 1004.7485172748566 and batch: 750, loss is 5.203331851959229 and perplexity is 181.87722144090407
At time: 1005.9102921485901 and batch: 800, loss is 5.219821910858155 and perplexity is 184.9012522330328
At time: 1007.0725009441376 and batch: 850, loss is 5.258741674423217 and perplexity is 192.23943923577744
At time: 1008.2352414131165 and batch: 900, loss is 5.234754781723023 and perplexity is 187.68307738000505
At time: 1009.3975150585175 and batch: 950, loss is 5.218689250946045 and perplexity is 184.6919405587783
At time: 1010.5599818229675 and batch: 1000, loss is 5.21982774734497 and perplexity is 184.90233140990296
At time: 1011.7229464054108 and batch: 1050, loss is 5.214195299148559 and perplexity is 183.86380607247722
At time: 1012.9119029045105 and batch: 1100, loss is 5.186547517776489 and perplexity is 178.8500093129813
At time: 1014.0743772983551 and batch: 1150, loss is 5.228885841369629 and perplexity is 186.58480260056157
At time: 1015.2367179393768 and batch: 1200, loss is 5.23885214805603 and perplexity is 188.45366130678326
At time: 1016.5790722370148 and batch: 1250, loss is 5.239120817184448 and perplexity is 188.50429978990763
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.153757359859717 and perplexity of 173.08059617561167
Finished 33 epochs...
Completing Train Step...
At time: 1019.4730956554413 and batch: 50, loss is 5.195614414215088 and perplexity is 180.4789975997892
At time: 1020.6399431228638 and batch: 100, loss is 5.21079041481018 and perplexity is 183.23883565855914
At time: 1021.8033316135406 and batch: 150, loss is 5.131955900192261 and perplexity is 169.3480221607878
At time: 1022.9670219421387 and batch: 200, loss is 5.172893409729004 and perplexity is 176.4245682606975
At time: 1024.1306765079498 and batch: 250, loss is 5.204179458618164 and perplexity is 182.031447137036
At time: 1025.29416847229 and batch: 300, loss is 5.204082698822021 and perplexity is 182.0138346634229
At time: 1026.4829919338226 and batch: 350, loss is 5.223671274185181 and perplexity is 185.61437598815024
At time: 1027.64670586586 and batch: 400, loss is 5.20204327583313 and perplexity is 181.64300972769286
At time: 1028.8106045722961 and batch: 450, loss is 5.155968475341797 and perplexity is 173.46372077173388
At time: 1029.9742274284363 and batch: 500, loss is 5.170828599929809 and perplexity is 176.06066091235496
At time: 1031.1379911899567 and batch: 550, loss is 5.177720069885254 and perplexity is 177.27816805579803
At time: 1032.301551580429 and batch: 600, loss is 5.219809312820434 and perplexity is 184.89892285475543
At time: 1033.4641561508179 and batch: 650, loss is 5.211884727478028 and perplexity is 183.43946599379902
At time: 1034.6273164749146 and batch: 700, loss is 5.232992496490478 and perplexity is 187.35261753211907
At time: 1035.7918195724487 and batch: 750, loss is 5.198819131851196 and perplexity is 181.05830959622543
At time: 1036.9609169960022 and batch: 800, loss is 5.215012741088867 and perplexity is 184.0141655055462
At time: 1038.1277024745941 and batch: 850, loss is 5.25026538848877 and perplexity is 190.6168492641116
At time: 1039.2912726402283 and batch: 900, loss is 5.225586414337158 and perplexity is 185.9701941444537
At time: 1040.4574019908905 and batch: 950, loss is 5.21050989151001 and perplexity is 183.18744010482305
At time: 1041.6266276836395 and batch: 1000, loss is 5.211915283203125 and perplexity is 183.4450712053292
At time: 1042.7956285476685 and batch: 1050, loss is 5.205522775650024 and perplexity is 182.27613739180748
At time: 1043.9618437290192 and batch: 1100, loss is 5.1772864627838135 and perplexity is 177.2013156462781
At time: 1045.1268103122711 and batch: 1150, loss is 5.220373411178588 and perplexity is 185.0032534571565
At time: 1046.2913343906403 and batch: 1200, loss is 5.230302715301514 and perplexity is 186.8493571193944
At time: 1047.4550640583038 and batch: 1250, loss is 5.2317970180511475 and perplexity is 187.12877534322155
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.149187296846487 and perplexity of 172.29141162981423
Finished 34 epochs...
Completing Train Step...
At time: 1050.3405163288116 and batch: 50, loss is 5.188773717880249 and perplexity is 179.24860873869227
At time: 1051.5020279884338 and batch: 100, loss is 5.204223852157593 and perplexity is 182.03952833663706
At time: 1052.6634273529053 and batch: 150, loss is 5.126293783187866 and perplexity is 168.391863340909
At time: 1053.824016571045 and batch: 200, loss is 5.165942831039429 and perplexity is 175.20256714331404
At time: 1054.9849190711975 and batch: 250, loss is 5.197323522567749 and perplexity is 180.78771950651225
At time: 1056.146282672882 and batch: 300, loss is 5.197107257843018 and perplexity is 180.74862572757345
At time: 1057.30748462677 and batch: 350, loss is 5.214985265731811 and perplexity is 184.0091097201006
At time: 1058.4690515995026 and batch: 400, loss is 5.194815826416016 and perplexity is 180.33492680856196
At time: 1059.6301221847534 and batch: 450, loss is 5.148383302688599 and perplexity is 172.15294601164226
At time: 1060.790457725525 and batch: 500, loss is 5.163217315673828 and perplexity is 174.72570000402794
At time: 1061.9687151908875 and batch: 550, loss is 5.169945497512817 and perplexity is 175.90524994916464
At time: 1063.1292119026184 and batch: 600, loss is 5.2108098411560055 and perplexity is 183.2423953541251
At time: 1064.2896349430084 and batch: 650, loss is 5.202495737075806 and perplexity is 181.72521474548736
At time: 1065.4820239543915 and batch: 700, loss is 5.2241999053955075 and perplexity is 185.71252348001101
At time: 1066.6440255641937 and batch: 750, loss is 5.18962965965271 and perplexity is 179.40210079132595
At time: 1067.805181980133 and batch: 800, loss is 5.206606140136719 and perplexity is 182.47371589127
At time: 1068.966501712799 and batch: 850, loss is 5.242892475128174 and perplexity is 189.216615991752
At time: 1070.1307203769684 and batch: 900, loss is 5.219193677902222 and perplexity is 184.78512765325013
At time: 1071.3033182621002 and batch: 950, loss is 5.202426309585571 and perplexity is 181.71259845787895
At time: 1072.4644920825958 and batch: 1000, loss is 5.203333444595337 and perplexity is 181.87751110536487
At time: 1073.626224040985 and batch: 1050, loss is 5.1953822612762455 and perplexity is 180.43710373317734
At time: 1074.7868037223816 and batch: 1100, loss is 5.168059844970703 and perplexity is 175.57386630282565
At time: 1075.9551010131836 and batch: 1150, loss is 5.2109929466247555 and perplexity is 183.2759511108488
At time: 1077.1189272403717 and batch: 1200, loss is 5.222041301727295 and perplexity is 185.31207610467334
At time: 1078.281759262085 and batch: 1250, loss is 5.224368772506714 and perplexity is 185.74388686541351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.141554338218522 and perplexity of 170.98132469327345
Finished 35 epochs...
Completing Train Step...
At time: 1081.1514222621918 and batch: 50, loss is 5.18061074256897 and perplexity is 177.7913625949801
At time: 1082.3348453044891 and batch: 100, loss is 5.192885761260986 and perplexity is 179.98720432156506
At time: 1083.4978499412537 and batch: 150, loss is 5.1146901512145995 and perplexity is 166.44919891029673
At time: 1084.661504983902 and batch: 200, loss is 5.160150184631347 and perplexity is 174.19061439394451
At time: 1085.824996471405 and batch: 250, loss is 5.185997467041016 and perplexity is 178.75165978491412
At time: 1086.9885556697845 and batch: 300, loss is 5.186671485900879 and perplexity is 178.87218238753178
At time: 1088.1521997451782 and batch: 350, loss is 5.205513257980346 and perplexity is 182.2744025559974
At time: 1089.3156521320343 and batch: 400, loss is 5.1836489486694335 and perplexity is 178.33235079811405
At time: 1090.4785332679749 and batch: 450, loss is 5.139434900283813 and perplexity is 170.6193241419516
At time: 1091.6426174640656 and batch: 500, loss is 5.152730808258057 and perplexity is 172.90301117806823
At time: 1092.806655406952 and batch: 550, loss is 5.160390062332153 and perplexity is 174.23240385000622
At time: 1093.9960598945618 and batch: 600, loss is 5.202092819213867 and perplexity is 181.65200915941122
At time: 1095.159453868866 and batch: 650, loss is 5.194746389389038 and perplexity is 180.32240532211648
At time: 1096.322928905487 and batch: 700, loss is 5.213360900878906 and perplexity is 183.71045441791333
At time: 1097.48526096344 and batch: 750, loss is 5.178847942352295 and perplexity is 177.47822802041063
At time: 1098.6485829353333 and batch: 800, loss is 5.197130174636841 and perplexity is 180.75276795402607
At time: 1099.811811208725 and batch: 850, loss is 5.23490683555603 and perplexity is 187.71161748107218
At time: 1100.982063293457 and batch: 900, loss is 5.2105818939208985 and perplexity is 183.20063051702016
At time: 1102.1457195281982 and batch: 950, loss is 5.193449954986573 and perplexity is 180.08878062459112
At time: 1103.3086194992065 and batch: 1000, loss is 5.1956707382202145 and perplexity is 180.48916318605583
At time: 1104.4721348285675 and batch: 1050, loss is 5.186757078170777 and perplexity is 178.88749311887463
At time: 1105.6348242759705 and batch: 1100, loss is 5.15776177406311 and perplexity is 173.7750721299443
At time: 1106.798553943634 and batch: 1150, loss is 5.200916023254394 and perplexity is 181.43836753995475
At time: 1107.9622268676758 and batch: 1200, loss is 5.212421035766601 and perplexity is 183.53787248561295
At time: 1109.1266918182373 and batch: 1250, loss is 5.2162998676300045 and perplexity is 184.25116751498192
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.137185646669708 and perplexity of 170.23598928076214
Finished 36 epochs...
Completing Train Step...
At time: 1112.0182914733887 and batch: 50, loss is 5.172623014450073 and perplexity is 176.37687033928938
At time: 1113.1800167560577 and batch: 100, loss is 5.185091228485107 and perplexity is 178.58974151825393
At time: 1114.3415386676788 and batch: 150, loss is 5.108721151351928 and perplexity is 165.45862297975353
At time: 1115.5032978057861 and batch: 200, loss is 5.150730924606323 and perplexity is 172.55757080808536
At time: 1116.672490119934 and batch: 250, loss is 5.179117212295532 and perplexity is 177.52602400751803
At time: 1117.848643541336 and batch: 300, loss is 5.18172929763794 and perplexity is 177.9903432895156
At time: 1119.0141506195068 and batch: 350, loss is 5.19845874786377 and perplexity is 180.99307083686466
At time: 1120.1756782531738 and batch: 400, loss is 5.1771415328979495 and perplexity is 177.17563574076436
At time: 1121.337263584137 and batch: 450, loss is 5.132479400634765 and perplexity is 169.43669913451856
At time: 1122.4994494915009 and batch: 500, loss is 5.144682874679566 and perplexity is 171.5170836354373
At time: 1123.6900720596313 and batch: 550, loss is 5.153462524414063 and perplexity is 173.02957340293472
At time: 1124.8511662483215 and batch: 600, loss is 5.193872995376587 and perplexity is 180.1649815694861
At time: 1126.0122435092926 and batch: 650, loss is 5.188586826324463 and perplexity is 179.215111817576
At time: 1127.172886133194 and batch: 700, loss is 5.207151002883911 and perplexity is 182.57316611230087
At time: 1128.3345279693604 and batch: 750, loss is 5.172230768203735 and perplexity is 176.30770074060257
At time: 1129.4960210323334 and batch: 800, loss is 5.191301908493042 and perplexity is 179.70235672767794
At time: 1130.6576564311981 and batch: 850, loss is 5.22937123298645 and perplexity is 186.6753912832851
At time: 1131.8191103935242 and batch: 900, loss is 5.203239068984986 and perplexity is 181.8603471141889
At time: 1132.980506181717 and batch: 950, loss is 5.186730613708496 and perplexity is 178.88275902020354
At time: 1134.141834974289 and batch: 1000, loss is 5.190363750457764 and perplexity is 179.5338465746919
At time: 1135.305695772171 and batch: 1050, loss is 5.1835071849823 and perplexity is 178.30707153841263
At time: 1136.4740788936615 and batch: 1100, loss is 5.152367858886719 and perplexity is 172.84026752593203
At time: 1137.6347556114197 and batch: 1150, loss is 5.197053365707397 and perplexity is 180.73888506059748
At time: 1138.8043122291565 and batch: 1200, loss is 5.206673431396484 and perplexity is 182.48599519062648
At time: 1139.965222120285 and batch: 1250, loss is 5.211148462295532 and perplexity is 183.30445560971373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.136129783017792 and perplexity of 170.05633814767378
Finished 37 epochs...
Completing Train Step...
At time: 1142.8394117355347 and batch: 50, loss is 5.168364143371582 and perplexity is 175.62730127925437
At time: 1144.0237879753113 and batch: 100, loss is 5.177392063140869 and perplexity is 177.22002915654028
At time: 1145.1860525608063 and batch: 150, loss is 5.100766668319702 and perplexity is 164.14770591671092
At time: 1146.348370552063 and batch: 200, loss is 5.144531707763672 and perplexity is 171.49115788648825
At time: 1147.5111553668976 and batch: 250, loss is 5.1732719612121585 and perplexity is 176.49136668520114
At time: 1148.6739354133606 and batch: 300, loss is 5.174402828216553 and perplexity is 176.69106784478947
At time: 1149.8456251621246 and batch: 350, loss is 5.191623764038086 and perplexity is 179.76020423641958
At time: 1151.0082771778107 and batch: 400, loss is 5.16821517944336 and perplexity is 175.60114109506307
At time: 1152.1708703041077 and batch: 450, loss is 5.124653129577637 and perplexity is 168.11581713241029
At time: 1153.3579399585724 and batch: 500, loss is 5.137709274291992 and perplexity is 170.32515288928332
At time: 1154.5203688144684 and batch: 550, loss is 5.144693374633789 and perplexity is 171.51888456641885
At time: 1155.6834948062897 and batch: 600, loss is 5.185165987014771 and perplexity is 178.60309312380983
At time: 1156.8460621833801 and batch: 650, loss is 5.182688360214233 and perplexity is 178.16112905073152
At time: 1158.0086653232574 and batch: 700, loss is 5.201035261154175 and perplexity is 181.4600031597071
At time: 1159.1707186698914 and batch: 750, loss is 5.166723585128784 and perplexity is 175.33941067769985
At time: 1160.3334755897522 and batch: 800, loss is 5.184465427398681 and perplexity is 178.47801482697594
At time: 1161.4979074001312 and batch: 850, loss is 5.222179307937622 and perplexity is 185.337652086806
At time: 1162.6614031791687 and batch: 900, loss is 5.195555801391602 and perplexity is 180.4684195261683
At time: 1163.8259012699127 and batch: 950, loss is 5.179226064682007 and perplexity is 177.54534919066978
At time: 1164.9890439510345 and batch: 1000, loss is 5.181270933151245 and perplexity is 177.90877753202835
At time: 1166.1510980129242 and batch: 1050, loss is 5.17682315826416 and perplexity is 177.11923649114053
At time: 1167.3140025138855 and batch: 1100, loss is 5.146207275390625 and perplexity is 171.7787437864798
At time: 1168.4761927127838 and batch: 1150, loss is 5.191072683334351 and perplexity is 179.66116914723509
At time: 1169.6385934352875 and batch: 1200, loss is 5.201505317687988 and perplexity is 181.54531967003874
At time: 1170.8018622398376 and batch: 1250, loss is 5.205800342559814 and perplexity is 182.3267382382272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.130545288976962 and perplexity of 169.10930634862362
Finished 38 epochs...
Completing Train Step...
At time: 1173.6901686191559 and batch: 50, loss is 5.161675624847412 and perplexity is 174.45653453347393
At time: 1174.8548884391785 and batch: 100, loss is 5.17183406829834 and perplexity is 176.2377733634073
At time: 1176.0168159008026 and batch: 150, loss is 5.095991134643555 and perplexity is 163.3656817967218
At time: 1177.179486989975 and batch: 200, loss is 5.137917690277099 and perplexity is 170.3606550732941
At time: 1178.341460466385 and batch: 250, loss is 5.166631660461426 and perplexity is 175.32329340149752
At time: 1179.5034983158112 and batch: 300, loss is 5.1698930549621585 and perplexity is 175.89602527106803
At time: 1180.6654584407806 and batch: 350, loss is 5.185300436019897 and perplexity is 178.6271077463277
At time: 1181.8271009922028 and batch: 400, loss is 5.161529016494751 and perplexity is 174.43095962312862
At time: 1183.0143222808838 and batch: 450, loss is 5.1194215965271 and perplexity is 167.2386102478525
At time: 1184.1762189865112 and batch: 500, loss is 5.13289740562439 and perplexity is 169.50753932492734
At time: 1185.338874578476 and batch: 550, loss is 5.1398208045959475 and perplexity is 170.6851795810031
At time: 1186.5008721351624 and batch: 600, loss is 5.178869066238403 and perplexity is 177.48197708988326
At time: 1187.6624603271484 and batch: 650, loss is 5.176481800079346 and perplexity is 177.058785708348
At time: 1188.8246591091156 and batch: 700, loss is 5.194246644973755 and perplexity is 180.23231272061864
At time: 1189.9866740703583 and batch: 750, loss is 5.159371356964112 and perplexity is 174.05500273999064
At time: 1191.1483745574951 and batch: 800, loss is 5.178630075454712 and perplexity is 177.43956560126682
At time: 1192.311033964157 and batch: 850, loss is 5.217304830551147 and perplexity is 184.43642617996449
At time: 1193.473254442215 and batch: 900, loss is 5.189018325805664 and perplexity is 179.29245973194102
At time: 1194.6355230808258 and batch: 950, loss is 5.17408655166626 and perplexity is 176.6351934397318
At time: 1195.8019938468933 and batch: 1000, loss is 5.175984840393067 and perplexity is 176.97081649013748
At time: 1196.968183040619 and batch: 1050, loss is 5.171068315505981 and perplexity is 176.102870454067
At time: 1198.1301777362823 and batch: 1100, loss is 5.139439382553101 and perplexity is 170.62008890542197
At time: 1199.2924544811249 and batch: 1150, loss is 5.18418776512146 and perplexity is 178.42846509430933
At time: 1200.4541606903076 and batch: 1200, loss is 5.19523063659668 and perplexity is 180.40974708916565
At time: 1201.6156678199768 and batch: 1250, loss is 5.200679922103882 and perplexity is 181.39553478926135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.128628250456204 and perplexity of 168.78542783711472
Finished 39 epochs...
Completing Train Step...
At time: 1204.4990665912628 and batch: 50, loss is 5.154025707244873 and perplexity is 173.12704813336865
At time: 1205.6622605323792 and batch: 100, loss is 5.164230642318725 and perplexity is 174.90284394857179
At time: 1206.827159166336 and batch: 150, loss is 5.088709735870362 and perplexity is 162.18047135316866
At time: 1207.9884300231934 and batch: 200, loss is 5.130954742431641 and perplexity is 169.17856291602916
At time: 1209.15109872818 and batch: 250, loss is 5.158969593048096 and perplexity is 173.98508776608497
At time: 1210.3139035701752 and batch: 300, loss is 5.162846965789795 and perplexity is 174.6610023424191
At time: 1211.475970029831 and batch: 350, loss is 5.177779712677002 and perplexity is 177.28874173597563
At time: 1212.6630191802979 and batch: 400, loss is 5.155990390777588 and perplexity is 173.46752234642503
At time: 1213.8257811069489 and batch: 450, loss is 5.109942417144776 and perplexity is 165.66081537632843
At time: 1214.9881167411804 and batch: 500, loss is 5.1235037040710445 and perplexity is 167.92269153714201
At time: 1216.150162935257 and batch: 550, loss is 5.131368398666382 and perplexity is 169.24855915955712
At time: 1217.3128073215485 and batch: 600, loss is 5.170568075180054 and perplexity is 176.01479872710553
At time: 1218.4747898578644 and batch: 650, loss is 5.171896333694458 and perplexity is 176.248747219819
At time: 1219.6375885009766 and batch: 700, loss is 5.187202081680298 and perplexity is 178.96711639612698
At time: 1220.8010137081146 and batch: 750, loss is 5.153288116455078 and perplexity is 172.99939829965916
At time: 1221.9642248153687 and batch: 800, loss is 5.174311380386353 and perplexity is 176.67491056880456
At time: 1223.1263275146484 and batch: 850, loss is 5.2091342544555665 and perplexity is 182.93561392473106
At time: 1224.2899770736694 and batch: 900, loss is 5.181196556091309 and perplexity is 177.8955456922975
At time: 1225.4517402648926 and batch: 950, loss is 5.166054201126099 and perplexity is 175.22208055498663
At time: 1226.6134452819824 and batch: 1000, loss is 5.166363735198974 and perplexity is 175.27632615423605
At time: 1227.7904546260834 and batch: 1050, loss is 5.162992601394653 and perplexity is 174.68644105548663
At time: 1228.9586162567139 and batch: 1100, loss is 5.133535861968994 and perplexity is 169.61579704410403
At time: 1230.1322300434113 and batch: 1150, loss is 5.179131307601929 and perplexity is 177.52852630885513
At time: 1231.295305967331 and batch: 1200, loss is 5.1893524742126464 and perplexity is 179.3523800323233
At time: 1232.4575164318085 and batch: 1250, loss is 5.194815731048584 and perplexity is 180.33490961048386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.125753360943203 and perplexity of 168.3008852176543
Finished 40 epochs...
Completing Train Step...
At time: 1235.321689605713 and batch: 50, loss is 5.148225784301758 and perplexity is 172.12583089291758
At time: 1236.5061860084534 and batch: 100, loss is 5.157727136611938 and perplexity is 173.76905310861085
At time: 1237.66987991333 and batch: 150, loss is 5.0830068778991695 and perplexity is 161.25821141757518
At time: 1238.8357818126678 and batch: 200, loss is 5.123830709457398 and perplexity is 167.97761214094282
At time: 1240.0006325244904 and batch: 250, loss is 5.152390117645264 and perplexity is 172.8441147785311
At time: 1241.200566291809 and batch: 300, loss is 5.156257390975952 and perplexity is 173.51384439302385
At time: 1242.3660922050476 and batch: 350, loss is 5.172088298797608 and perplexity is 176.28258407640294
At time: 1243.5381240844727 and batch: 400, loss is 5.149803991317749 and perplexity is 172.3976955597998
At time: 1244.7021670341492 and batch: 450, loss is 5.105044679641724 and perplexity is 164.851435870192
At time: 1245.8734767436981 and batch: 500, loss is 5.119090805053711 and perplexity is 167.18329829042742
At time: 1247.0365536212921 and batch: 550, loss is 5.125673723220825 and perplexity is 168.28748265216342
At time: 1248.1996326446533 and batch: 600, loss is 5.165803394317627 and perplexity is 175.17813917481783
At time: 1249.3643653392792 and batch: 650, loss is 5.167812337875366 and perplexity is 175.5304159025231
At time: 1250.5367703437805 and batch: 700, loss is 5.18355242729187 and perplexity is 178.3151387446298
At time: 1251.7003736495972 and batch: 750, loss is 5.149760789871216 and perplexity is 172.39024789084888
At time: 1252.8647980690002 and batch: 800, loss is 5.168166847229004 and perplexity is 175.59265410816957
At time: 1254.028106212616 and batch: 850, loss is 5.202780475616455 and perplexity is 181.77696628541256
At time: 1255.1915946006775 and batch: 900, loss is 5.176796588897705 and perplexity is 177.1145306077565
At time: 1256.3558638095856 and batch: 950, loss is 5.161147232055664 and perplexity is 174.3643773079042
At time: 1257.518966436386 and batch: 1000, loss is 5.160422105789184 and perplexity is 174.2379869480028
At time: 1258.682143688202 and batch: 1050, loss is 5.15806022644043 and perplexity is 173.82694345351524
At time: 1259.8456807136536 and batch: 1100, loss is 5.127949028015137 and perplexity is 168.6708239119875
At time: 1261.0096111297607 and batch: 1150, loss is 5.17517237663269 and perplexity is 176.82709250826437
At time: 1262.1732227802277 and batch: 1200, loss is 5.184353256225586 and perplexity is 178.45799586147757
At time: 1263.3379220962524 and batch: 1250, loss is 5.191556177139282 and perplexity is 179.74805521224897
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.122716305029654 and perplexity of 167.79052141294122
Finished 41 epochs...
Completing Train Step...
At time: 1266.225338459015 and batch: 50, loss is 5.141994724273681 and perplexity is 171.05663906684381
At time: 1267.3866589069366 and batch: 100, loss is 5.1515165710449216 and perplexity is 172.69319331773877
At time: 1268.5484409332275 and batch: 150, loss is 5.076486120223999 and perplexity is 160.21010663026436
At time: 1269.7108676433563 and batch: 200, loss is 5.117749462127685 and perplexity is 166.95919848685912
At time: 1270.8986897468567 and batch: 250, loss is 5.147155208587646 and perplexity is 171.9416557628938
At time: 1272.060825586319 and batch: 300, loss is 5.15194619178772 and perplexity is 172.76740183534594
At time: 1273.2228627204895 and batch: 350, loss is 5.167049951553345 and perplexity is 175.39664491340582
At time: 1274.3845555782318 and batch: 400, loss is 5.143580808639526 and perplexity is 171.32816460202255
At time: 1275.5459394454956 and batch: 450, loss is 5.0982862567901615 and perplexity is 163.74105659173506
At time: 1276.7081561088562 and batch: 500, loss is 5.112586851119995 and perplexity is 166.09947421140612
At time: 1277.8696746826172 and batch: 550, loss is 5.120323152542114 and perplexity is 167.38945320933658
At time: 1279.0305778980255 and batch: 600, loss is 5.159321727752686 and perplexity is 174.04636474181024
At time: 1280.2061433792114 and batch: 650, loss is 5.164184455871582 and perplexity is 174.894765994162
At time: 1281.3904118537903 and batch: 700, loss is 5.175821161270141 and perplexity is 176.94185243258138
At time: 1282.564574956894 and batch: 750, loss is 5.141603174209595 and perplexity is 170.98967493961447
At time: 1283.7376592159271 and batch: 800, loss is 5.1608256816864015 and perplexity is 174.30831939119952
At time: 1284.9117829799652 and batch: 850, loss is 5.196513757705689 and perplexity is 180.6413832207513
At time: 1286.088287115097 and batch: 900, loss is 5.169673070907593 and perplexity is 175.8573352060017
At time: 1287.2496147155762 and batch: 950, loss is 5.155254173278808 and perplexity is 173.33985952057307
At time: 1288.412071466446 and batch: 1000, loss is 5.15529125213623 and perplexity is 173.34628688366863
At time: 1289.5764424800873 and batch: 1050, loss is 5.151658372879028 and perplexity is 172.71768326560772
At time: 1290.7402143478394 and batch: 1100, loss is 5.119783906936646 and perplexity is 167.29921351515767
At time: 1291.9038774967194 and batch: 1150, loss is 5.167050342559815 and perplexity is 175.39671349464217
At time: 1293.0677366256714 and batch: 1200, loss is 5.177251405715943 and perplexity is 177.19510359661743
At time: 1294.2317860126495 and batch: 1250, loss is 5.184795379638672 and perplexity is 178.53691376414096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1198084476220345 and perplexity of 167.30331920290033
Finished 42 epochs...
Completing Train Step...
At time: 1297.1355152130127 and batch: 50, loss is 5.137003870010376 and perplexity is 170.20504716369516
At time: 1298.3260445594788 and batch: 100, loss is 5.1443921661376955 and perplexity is 171.46722940102484
At time: 1299.493445634842 and batch: 150, loss is 5.071183242797852 and perplexity is 159.36278069077477
At time: 1300.6850171089172 and batch: 200, loss is 5.110762062072754 and perplexity is 165.79665408556605
At time: 1301.8511281013489 and batch: 250, loss is 5.141125373840332 and perplexity is 170.90799552456934
At time: 1303.0186655521393 and batch: 300, loss is 5.145945959091186 and perplexity is 171.7338610653799
At time: 1304.188349723816 and batch: 350, loss is 5.160855731964111 and perplexity is 174.313557483307
At time: 1305.3547530174255 and batch: 400, loss is 5.138846549987793 and perplexity is 170.51896973675278
At time: 1306.5227916240692 and batch: 450, loss is 5.0922501850128175 and perplexity is 162.7556807127926
At time: 1307.6893796920776 and batch: 500, loss is 5.106185731887817 and perplexity is 165.0396473304972
At time: 1308.8549423217773 and batch: 550, loss is 5.114082593917846 and perplexity is 166.34810219909212
At time: 1310.0222640037537 and batch: 600, loss is 5.154581346511841 and perplexity is 173.22327104962548
At time: 1311.1992869377136 and batch: 650, loss is 5.158234672546387 and perplexity is 173.85726953196735
At time: 1312.3660788536072 and batch: 700, loss is 5.17096432685852 and perplexity is 176.08455870687789
At time: 1313.5297470092773 and batch: 750, loss is 5.134408912658691 and perplexity is 169.76394489357793
At time: 1314.7054092884064 and batch: 800, loss is 5.154939594268799 and perplexity is 173.28533901512287
At time: 1315.882427930832 and batch: 850, loss is 5.190725173950195 and perplexity is 179.59874605192192
At time: 1317.0492861270905 and batch: 900, loss is 5.164559946060181 and perplexity is 174.96044959383198
At time: 1318.2150638103485 and batch: 950, loss is 5.150263147354126 and perplexity is 172.47687117796406
At time: 1319.3803329467773 and batch: 1000, loss is 5.150860042572021 and perplexity is 172.57985252904817
At time: 1320.5456886291504 and batch: 1050, loss is 5.146481599807739 and perplexity is 171.8258733543416
At time: 1321.7111189365387 and batch: 1100, loss is 5.115747060775757 and perplexity is 166.62521365952279
At time: 1322.8826909065247 and batch: 1150, loss is 5.162681226730347 and perplexity is 174.63205659095564
At time: 1324.0485458374023 and batch: 1200, loss is 5.173242149353027 and perplexity is 176.48610522786691
At time: 1325.214260339737 and batch: 1250, loss is 5.180796661376953 and perplexity is 177.82442042612456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.119022564296305 and perplexity of 167.1718899647873
Finished 43 epochs...
Completing Train Step...
At time: 1328.123018503189 and batch: 50, loss is 5.132620859146118 and perplexity is 169.46066909308396
At time: 1329.2925293445587 and batch: 100, loss is 5.1407019233703615 and perplexity is 170.83563977416222
At time: 1330.4822573661804 and batch: 150, loss is 5.066292552947998 and perplexity is 158.58528954056806
At time: 1331.645478963852 and batch: 200, loss is 5.1067971515655515 and perplexity is 165.14058667348715
At time: 1332.8089697360992 and batch: 250, loss is 5.137397804260254 and perplexity is 170.27210996957587
At time: 1333.971960067749 and batch: 300, loss is 5.14094446182251 and perplexity is 170.87707901090593
At time: 1335.1360030174255 and batch: 350, loss is 5.15654486656189 and perplexity is 173.56373255758018
At time: 1336.3010838031769 and batch: 400, loss is 5.133825807571411 and perplexity is 169.66498352891534
At time: 1337.463794708252 and batch: 450, loss is 5.087962684631347 and perplexity is 162.05935947512216
At time: 1338.62677359581 and batch: 500, loss is 5.10138466835022 and perplexity is 164.2491805564125
At time: 1339.7902402877808 and batch: 550, loss is 5.110472030639649 and perplexity is 165.74857481696412
At time: 1340.9527566432953 and batch: 600, loss is 5.149440650939941 and perplexity is 172.33506789423075
At time: 1342.1171536445618 and batch: 650, loss is 5.1534016323089595 and perplexity is 173.01903758874232
At time: 1343.2797656059265 and batch: 700, loss is 5.167027263641358 and perplexity is 175.39266557490475
At time: 1344.443025112152 and batch: 750, loss is 5.129089202880859 and perplexity is 168.86324782360052
At time: 1345.6053547859192 and batch: 800, loss is 5.1502190113067625 and perplexity is 172.46925889859784
At time: 1346.7693161964417 and batch: 850, loss is 5.184991321563721 and perplexity is 178.57190005824515
At time: 1347.9323296546936 and batch: 900, loss is 5.161343278884888 and perplexity is 174.39856424221554
At time: 1349.095284461975 and batch: 950, loss is 5.1452142524719235 and perplexity is 171.60824822396495
At time: 1350.2594695091248 and batch: 1000, loss is 5.146459922790528 and perplexity is 171.82214872229713
At time: 1351.4216692447662 and batch: 1050, loss is 5.143042631149292 and perplexity is 171.23598444725772
At time: 1352.585513830185 and batch: 1100, loss is 5.112830438613892 and perplexity is 166.13993889421204
At time: 1353.7482106685638 and batch: 1150, loss is 5.158383884429932 and perplexity is 173.88321303811375
At time: 1354.9113397598267 and batch: 1200, loss is 5.168860940933228 and perplexity is 175.71457417097332
At time: 1356.0742404460907 and batch: 1250, loss is 5.176802940368653 and perplexity is 177.11565554912454
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.117408919508439 and perplexity of 166.902351444069
Finished 44 epochs...
Completing Train Step...
At time: 1358.962387084961 and batch: 50, loss is 5.125120010375976 and perplexity is 168.19432550492627
At time: 1360.152811050415 and batch: 100, loss is 5.13513180732727 and perplexity is 169.88671071229012
At time: 1361.3175172805786 and batch: 150, loss is 5.0613132667541505 and perplexity is 157.79761066451633
At time: 1362.4825649261475 and batch: 200, loss is 5.1020137977600095 and perplexity is 164.3525470584782
At time: 1363.6467134952545 and batch: 250, loss is 5.133331832885742 and perplexity is 169.58119401865983
At time: 1364.8117060661316 and batch: 300, loss is 5.137640008926391 and perplexity is 170.31335566387176
At time: 1365.9760282039642 and batch: 350, loss is 5.151611118316651 and perplexity is 172.7095217599058
At time: 1367.141119480133 and batch: 400, loss is 5.131322050094605 and perplexity is 169.24071491235097
At time: 1368.3080592155457 and batch: 450, loss is 5.08309492111206 and perplexity is 161.27240973363692
At time: 1369.4704911708832 and batch: 500, loss is 5.096286573410034 and perplexity is 163.41395348254946
At time: 1370.6337313652039 and batch: 550, loss is 5.106638507843018 and perplexity is 165.11439023408113
At time: 1371.7961735725403 and batch: 600, loss is 5.1439718246459964 and perplexity is 171.39516975593128
At time: 1372.9593307971954 and batch: 650, loss is 5.150986223220825 and perplexity is 172.6016301407385
At time: 1374.122522354126 and batch: 700, loss is 5.1621322917938235 and perplexity is 174.5362212601431
At time: 1375.2853467464447 and batch: 750, loss is 5.125578145980835 and perplexity is 168.271398967676
At time: 1376.449287891388 and batch: 800, loss is 5.145248651504517 and perplexity is 171.61415148322146
At time: 1377.6182930469513 and batch: 850, loss is 5.181489286422729 and perplexity is 177.94762873711775
At time: 1378.781034708023 and batch: 900, loss is 5.155722064971924 and perplexity is 173.42098277789765
At time: 1379.9447286128998 and batch: 950, loss is 5.140308609008789 and perplexity is 170.76846087564948
At time: 1381.1091260910034 and batch: 1000, loss is 5.142710952758789 and perplexity is 171.17919858938163
At time: 1382.274286031723 and batch: 1050, loss is 5.139255933761596 and perplexity is 170.58879172710846
At time: 1383.4385912418365 and batch: 1100, loss is 5.108614044189453 and perplexity is 165.44090212517128
At time: 1384.6027076244354 and batch: 1150, loss is 5.154463424682617 and perplexity is 173.2028454489749
At time: 1385.7661485671997 and batch: 1200, loss is 5.164463777542114 and perplexity is 174.94362471569895
At time: 1386.9382588863373 and batch: 1250, loss is 5.172251501083374 and perplexity is 176.31135614483483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.115809085595346 and perplexity of 166.63554887885584
Finished 45 epochs...
Completing Train Step...
At time: 1389.8199915885925 and batch: 50, loss is 5.122078876495362 and perplexity is 167.68360102748147
At time: 1391.0088431835175 and batch: 100, loss is 5.129832038879394 and perplexity is 168.98873212427108
At time: 1392.1727397441864 and batch: 150, loss is 5.0566379165649415 and perplexity is 157.06157353205882
At time: 1393.336472272873 and batch: 200, loss is 5.097287435531616 and perplexity is 163.57759019396119
At time: 1394.5004711151123 and batch: 250, loss is 5.128602085113525 and perplexity is 168.781011566333
At time: 1395.6642518043518 and batch: 300, loss is 5.132944459915161 and perplexity is 169.51551556962747
At time: 1396.8274331092834 and batch: 350, loss is 5.148374042510986 and perplexity is 172.15135185216684
At time: 1397.9905331134796 and batch: 400, loss is 5.126964569091797 and perplexity is 168.50485612192028
At time: 1399.1542093753815 and batch: 450, loss is 5.079123315811157 and perplexity is 160.63316962122101
At time: 1400.3203542232513 and batch: 500, loss is 5.0917220878601075 and perplexity is 162.66975259241568
At time: 1401.497773885727 and batch: 550, loss is 5.102092370986939 and perplexity is 164.36546127580382
At time: 1402.669838666916 and batch: 600, loss is 5.139446525573731 and perplexity is 170.6213076525897
At time: 1403.8355598449707 and batch: 650, loss is 5.146425876617432 and perplexity is 171.816298935262
At time: 1404.9987208843231 and batch: 700, loss is 5.1570953941345214 and perplexity is 173.65931048469005
At time: 1406.1620404720306 and batch: 750, loss is 5.121092672348023 and perplexity is 167.51831228232703
At time: 1407.3307824134827 and batch: 800, loss is 5.1402138900756835 and perplexity is 170.75228663524274
At time: 1408.4964292049408 and batch: 850, loss is 5.178251028060913 and perplexity is 177.3723203417442
At time: 1409.6642248630524 and batch: 900, loss is 5.150919494628906 and perplexity is 172.59011306125973
At time: 1410.8289959430695 and batch: 950, loss is 5.13724793434143 and perplexity is 170.24659321442158
At time: 1411.9908497333527 and batch: 1000, loss is 5.139317855834961 and perplexity is 170.59935526583976
At time: 1413.152767419815 and batch: 1050, loss is 5.135942544937134 and perplexity is 170.0245001060551
At time: 1414.3147246837616 and batch: 1100, loss is 5.10353138923645 and perplexity is 164.60215643765267
At time: 1415.4796164035797 and batch: 1150, loss is 5.149451103210449 and perplexity is 172.3368691963922
At time: 1416.6444885730743 and batch: 1200, loss is 5.160118980407715 and perplexity is 174.1851789958624
At time: 1417.8353807926178 and batch: 1250, loss is 5.168315439224243 and perplexity is 175.61874770959508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.116007783987226 and perplexity of 166.66866238413922
Annealing...
Finished 46 epochs...
Completing Train Step...
At time: 1420.74653673172 and batch: 50, loss is 5.121679315567016 and perplexity is 167.61661459565826
At time: 1421.9105989933014 and batch: 100, loss is 5.134643478393555 and perplexity is 169.8037703687285
At time: 1423.0752556324005 and batch: 150, loss is 5.054883680343628 and perplexity is 156.7862919558431
At time: 1424.2418422698975 and batch: 200, loss is 5.099220333099365 and perplexity is 163.89407468770483
At time: 1425.406828403473 and batch: 250, loss is 5.127272453308105 and perplexity is 168.5567440948256
At time: 1426.5707168579102 and batch: 300, loss is 5.1260482025146485 and perplexity is 168.35051463117517
At time: 1427.7353098392487 and batch: 350, loss is 5.138311920166015 and perplexity is 170.4278295756476
At time: 1428.8997695446014 and batch: 400, loss is 5.107149839401245 and perplexity is 165.198840021606
At time: 1430.0639402866364 and batch: 450, loss is 5.060104875564575 and perplexity is 157.6070445844283
At time: 1431.2298421859741 and batch: 500, loss is 5.065300798416137 and perplexity is 158.4280898256294
At time: 1432.3949415683746 and batch: 550, loss is 5.077502098083496 and perplexity is 160.37295926479572
At time: 1433.558761358261 and batch: 600, loss is 5.1104234790802 and perplexity is 165.740527660533
At time: 1434.720304965973 and batch: 650, loss is 5.12148886680603 and perplexity is 167.58469525868335
At time: 1435.8835861682892 and batch: 700, loss is 5.1264133644104 and perplexity is 168.41200104981397
At time: 1437.0473153591156 and batch: 750, loss is 5.086480312347412 and perplexity is 161.81930514118804
At time: 1438.210753440857 and batch: 800, loss is 5.095545902252197 and perplexity is 163.29296229329012
At time: 1439.373550415039 and batch: 850, loss is 5.128907747268677 and perplexity is 168.83260941943087
At time: 1440.537526845932 and batch: 900, loss is 5.102477264404297 and perplexity is 164.42873663624366
At time: 1441.7010028362274 and batch: 950, loss is 5.0844788646698 and perplexity is 161.4957561599048
At time: 1442.8653812408447 and batch: 1000, loss is 5.084206819534302 and perplexity is 161.45182800052993
At time: 1444.029489994049 and batch: 1050, loss is 5.071902751922607 and perplexity is 159.47748492605174
At time: 1445.1934831142426 and batch: 1100, loss is 5.033005247116089 and perplexity is 153.39330549134323
At time: 1446.3570640087128 and batch: 1150, loss is 5.0742760848999025 and perplexity is 159.85642760086333
At time: 1447.5476422309875 and batch: 1200, loss is 5.0965868091583255 and perplexity is 163.46302355908148
At time: 1448.7108845710754 and batch: 1250, loss is 5.119967670440674 and perplexity is 167.32995982979307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.079082405480155 and perplexity of 160.62659819950235
Finished 47 epochs...
Completing Train Step...
At time: 1451.6007807254791 and batch: 50, loss is 5.091351671218872 and perplexity is 162.60950816748738
At time: 1452.8119883537292 and batch: 100, loss is 5.107716665267945 and perplexity is 165.29250554079073
At time: 1453.9853434562683 and batch: 150, loss is 5.030588798522949 and perplexity is 153.02308594249516
At time: 1455.1514151096344 and batch: 200, loss is 5.072455778121948 and perplexity is 159.56570454505268
At time: 1456.316398382187 and batch: 250, loss is 5.105547389984131 and perplexity is 164.9343292258925
At time: 1457.4824051856995 and batch: 300, loss is 5.105590143203735 and perplexity is 164.94138085022897
At time: 1458.6476509571075 and batch: 350, loss is 5.118124790191651 and perplexity is 167.02187472092743
At time: 1459.813604593277 and batch: 400, loss is 5.090746202468872 and perplexity is 162.5110829915301
At time: 1460.9799900054932 and batch: 450, loss is 5.04486741065979 and perplexity is 155.2237168236899
At time: 1462.145899772644 and batch: 500, loss is 5.0512766456604 and perplexity is 156.22177708731968
At time: 1463.311266899109 and batch: 550, loss is 5.06256064414978 and perplexity is 157.99456665082843
At time: 1464.4857482910156 and batch: 600, loss is 5.09738639831543 and perplexity is 163.59377908869217
At time: 1465.6572260856628 and batch: 650, loss is 5.109659366607666 and perplexity is 165.613931629106
At time: 1466.823468208313 and batch: 700, loss is 5.115458288192749 and perplexity is 166.57710381291676
At time: 1467.9888834953308 and batch: 750, loss is 5.077384595870972 and perplexity is 160.35411619432594
At time: 1469.152872800827 and batch: 800, loss is 5.089523801803589 and perplexity is 162.31255070328754
At time: 1470.3170342445374 and batch: 850, loss is 5.125282325744629 and perplexity is 168.22162824464306
At time: 1471.4820981025696 and batch: 900, loss is 5.099514455795288 and perplexity is 163.9422867445803
At time: 1472.6487505435944 and batch: 950, loss is 5.083002090454102 and perplexity is 161.25743940459424
At time: 1473.813990354538 and batch: 1000, loss is 5.084762029647827 and perplexity is 161.54149257731953
At time: 1474.9783611297607 and batch: 1050, loss is 5.076116762161255 and perplexity is 160.15094266266425
At time: 1476.1432349681854 and batch: 1100, loss is 5.039877967834473 and perplexity is 154.45116586557353
At time: 1477.3353457450867 and batch: 1150, loss is 5.082297382354736 and perplexity is 161.14384001286984
At time: 1478.5001258850098 and batch: 1200, loss is 5.103343658447265 and perplexity is 164.57125844526607
At time: 1479.683293581009 and batch: 1250, loss is 5.121540861129761 and perplexity is 167.59340893810995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.077826061388002 and perplexity of 160.42492263529073
Finished 48 epochs...
Completing Train Step...
At time: 1482.6154584884644 and batch: 50, loss is 5.083846445083618 and perplexity is 161.39365536929697
At time: 1483.778446674347 and batch: 100, loss is 5.100155696868897 and perplexity is 164.04744698547177
At time: 1484.9424183368683 and batch: 150, loss is 5.0235233402252195 and perplexity is 151.94571824037277
At time: 1486.1056311130524 and batch: 200, loss is 5.064507846832275 and perplexity is 158.30251381530195
At time: 1487.2693870067596 and batch: 250, loss is 5.099076242446899 and perplexity is 163.87046078486
At time: 1488.4322052001953 and batch: 300, loss is 5.098987903594971 and perplexity is 163.8559852958727
At time: 1489.598292350769 and batch: 350, loss is 5.111630773544311 and perplexity is 165.94074611905543
At time: 1490.7630932331085 and batch: 400, loss is 5.085487327575684 and perplexity is 161.65870078744652
At time: 1491.9264390468597 and batch: 450, loss is 5.039669923782348 and perplexity is 154.4190365614427
At time: 1493.0971992015839 and batch: 500, loss is 5.046376390457153 and perplexity is 155.458123089129
At time: 1494.2603437900543 and batch: 550, loss is 5.0576161289215085 and perplexity is 157.21528827460253
At time: 1495.4226191043854 and batch: 600, loss is 5.09302827835083 and perplexity is 162.88236910498415
At time: 1496.5965149402618 and batch: 650, loss is 5.105921630859375 and perplexity is 164.99606594510115
At time: 1497.7612676620483 and batch: 700, loss is 5.111913690567016 and perplexity is 165.98770022263733
At time: 1498.92374253273 and batch: 750, loss is 5.074236850738526 and perplexity is 159.85015589101917
At time: 1500.0858085155487 and batch: 800, loss is 5.087265710830689 and perplexity is 161.94644770026173
At time: 1501.2483830451965 and batch: 850, loss is 5.12421558380127 and perplexity is 168.0422748569128
At time: 1502.4157388210297 and batch: 900, loss is 5.098606157302856 and perplexity is 163.7934458189115
At time: 1503.5795516967773 and batch: 950, loss is 5.08295599937439 and perplexity is 161.25000704638452
At time: 1504.7421333789825 and batch: 1000, loss is 5.085686483383179 and perplexity is 161.69089926269072
At time: 1505.905124425888 and batch: 1050, loss is 5.078305263519287 and perplexity is 160.50181702263438
At time: 1507.0939457416534 and batch: 1100, loss is 5.042517166137696 and perplexity is 154.85933149856115
At time: 1508.256741285324 and batch: 1150, loss is 5.084863901138306 and perplexity is 161.55794988819372
At time: 1509.4200510978699 and batch: 1200, loss is 5.105647563934326 and perplexity is 164.9508521767447
At time: 1510.5829713344574 and batch: 1250, loss is 5.121229839324951 and perplexity is 167.54129183878513
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.077246895671761 and perplexity of 160.33203692077686
Finished 49 epochs...
Completing Train Step...
At time: 1513.4655663967133 and batch: 50, loss is 5.079234466552735 and perplexity is 160.65102510945343
At time: 1514.656739473343 and batch: 100, loss is 5.095408067703247 and perplexity is 163.270456432564
At time: 1515.821706533432 and batch: 150, loss is 5.019460706710816 and perplexity is 151.3296707080762
At time: 1516.986862897873 and batch: 200, loss is 5.060700006484986 and perplexity is 157.70086932616817
At time: 1518.1516082286835 and batch: 250, loss is 5.095365791320801 and perplexity is 163.26355409420964
At time: 1519.3164858818054 and batch: 300, loss is 5.095328960418701 and perplexity is 163.25754106096576
At time: 1520.4816403388977 and batch: 350, loss is 5.108215885162354 and perplexity is 165.375043448522
At time: 1521.6551067829132 and batch: 400, loss is 5.08188307762146 and perplexity is 161.07709118534657
At time: 1522.820826292038 and batch: 450, loss is 5.03665756225586 and perplexity is 153.95457051779383
At time: 1523.9867663383484 and batch: 500, loss is 5.0435568046569825 and perplexity is 155.02041294338673
At time: 1525.1519830226898 and batch: 550, loss is 5.054535160064697 and perplexity is 156.73165827466403
At time: 1526.3167510032654 and batch: 600, loss is 5.090361156463623 and perplexity is 162.44852079365063
At time: 1527.4806454181671 and batch: 650, loss is 5.1037570667266845 and perplexity is 164.639307631141
At time: 1528.6482989788055 and batch: 700, loss is 5.109859886169434 and perplexity is 165.64714379183187
At time: 1529.8175151348114 and batch: 750, loss is 5.072407684326172 and perplexity is 159.55803060918126
At time: 1530.988008737564 and batch: 800, loss is 5.086048030853272 and perplexity is 161.74936876737007
At time: 1532.158599615097 and batch: 850, loss is 5.123260231018066 and perplexity is 167.88181186352054
At time: 1533.328898191452 and batch: 900, loss is 5.098047962188721 and perplexity is 163.70204263050786
At time: 1534.4932067394257 and batch: 950, loss is 5.082883491516113 and perplexity is 161.2383155775933
At time: 1535.684231519699 and batch: 1000, loss is 5.085718097686768 and perplexity is 161.69601108867062
At time: 1536.848197221756 and batch: 1050, loss is 5.0790359401702885 and perplexity is 160.61913480823998
At time: 1538.0111756324768 and batch: 1100, loss is 5.043534164428711 and perplexity is 155.0169032855808
At time: 1539.1745421886444 and batch: 1150, loss is 5.0857790660858155 and perplexity is 161.705869736129
At time: 1540.3372340202332 and batch: 1200, loss is 5.106140260696411 and perplexity is 165.03214295172168
At time: 1541.5006620883942 and batch: 1250, loss is 5.119937391281128 and perplexity is 167.32489329594816
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.076494871264827 and perplexity of 160.21150864158307
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9c69001898>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'anneal': 2.7403205097693326, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 22.362301406722917, 'dropout': 0.46601156043889025, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7691535949707031 and batch: 50, loss is 7.660231523513794 and perplexity is 2122.248722184569
At time: 2.924643039703369 and batch: 100, loss is 6.56069655418396 and perplexity is 706.7638224762184
At time: 4.080772876739502 and batch: 150, loss is 6.290067205429077 and perplexity is 539.1895643330514
At time: 5.236695289611816 and batch: 200, loss is 6.322249021530151 and perplexity is 556.8238941982116
At time: 6.393102407455444 and batch: 250, loss is 6.371563701629639 and perplexity is 584.9718355007066
At time: 7.548954486846924 and batch: 300, loss is 6.419302082061767 and perplexity is 613.5747398405097
At time: 8.703237771987915 and batch: 350, loss is 6.4958161354064945 and perplexity is 662.3645839658133
At time: 9.859776973724365 and batch: 400, loss is 6.543865823745728 and perplexity is 694.9680155717468
At time: 11.018582105636597 and batch: 450, loss is 6.571764831542969 and perplexity is 714.6299323293525
At time: 12.182633876800537 and batch: 500, loss is 6.616292200088501 and perplexity is 747.1695994219699
At time: 13.37076735496521 and batch: 550, loss is 6.6731452465057375 and perplexity is 790.8792067059111
At time: 14.531858921051025 and batch: 600, loss is 6.72608699798584 and perplexity is 833.877907690638
At time: 15.699495792388916 and batch: 650, loss is 6.724515943527222 and perplexity is 832.568868640884
At time: 16.861552000045776 and batch: 700, loss is 6.7712047576904295 and perplexity is 872.3622462787754
At time: 18.02475094795227 and batch: 750, loss is 6.737862014770508 and perplexity is 843.7548706316128
At time: 19.185795783996582 and batch: 800, loss is 6.787062549591065 and perplexity is 886.3062536190035
At time: 20.347601413726807 and batch: 850, loss is 6.839616184234619 and perplexity is 934.1305318898378
At time: 21.50892734527588 and batch: 900, loss is 6.870625486373902 and perplexity is 963.5510654267035
At time: 22.6699857711792 and batch: 950, loss is 6.825546169281006 and perplexity is 921.0793319331698
At time: 23.832043886184692 and batch: 1000, loss is 6.830601110458374 and perplexity is 925.7471215372763
At time: 24.99335741996765 and batch: 1050, loss is 6.868237028121948 and perplexity is 961.2524101471596
At time: 26.154282093048096 and batch: 1100, loss is 6.901258087158203 and perplexity is 993.5238692894808
At time: 27.31601858139038 and batch: 1150, loss is 6.960277833938599 and perplexity is 1053.9263330730157
At time: 28.484379529953003 and batch: 1200, loss is 6.96069595336914 and perplexity is 1054.3670922898082
At time: 29.659780740737915 and batch: 1250, loss is 6.956883487701416 and perplexity is 1050.3550067780116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.553730094519845 and perplexity of 701.8572912343277
Finished 1 epochs...
Completing Train Step...
At time: 32.62578463554382 and batch: 50, loss is 6.9808463287353515 and perplexity is 1075.8284863736842
At time: 33.78088712692261 and batch: 100, loss is 6.999190425872802 and perplexity is 1095.7457118717239
At time: 34.93570327758789 and batch: 150, loss is 6.997969589233398 and perplexity is 1094.4088015997718
At time: 36.09205412864685 and batch: 200, loss is 7.0291457366943355 and perplexity is 1129.0656787132668
At time: 37.2492618560791 and batch: 250, loss is 7.050015687942505 and perplexity is 1152.8768289069274
At time: 38.40377330780029 and batch: 300, loss is 7.026771621704102 and perplexity is 1126.3883263891755
At time: 39.55926990509033 and batch: 350, loss is 7.050786533355713 and perplexity is 1153.7658613317456
At time: 40.715784311294556 and batch: 400, loss is 6.984753894805908 and perplexity is 1080.0405814250664
At time: 41.871140241622925 and batch: 450, loss is 7.042772102355957 and perplexity is 1144.5560394834558
At time: 43.02476644515991 and batch: 500, loss is 7.006658382415772 and perplexity is 1103.9593245082563
At time: 44.17910718917847 and batch: 550, loss is 6.897941513061523 and perplexity is 990.2342319385834
At time: 45.334312915802 and batch: 600, loss is 6.994552011489868 and perplexity is 1090.6749584209965
At time: 46.4968204498291 and batch: 650, loss is 6.901696586608887 and perplexity is 993.9596244926322
At time: 47.65325140953064 and batch: 700, loss is 7.005044794082641 and perplexity is 1102.1794250206915
At time: 48.81081199645996 and batch: 750, loss is 7.011335716247559 and perplexity is 1109.1350055638584
At time: 49.97293758392334 and batch: 800, loss is 6.997249631881714 and perplexity is 1093.621157506412
At time: 51.129788637161255 and batch: 850, loss is 6.9713453102111815 and perplexity is 1065.6554237502344
At time: 52.28685665130615 and batch: 900, loss is 6.955764074325561 and perplexity is 1049.1798831812755
At time: 53.47040319442749 and batch: 950, loss is 7.006765251159668 and perplexity is 1104.0773095589236
At time: 54.62622141838074 and batch: 1000, loss is 7.075701932907105 and perplexity is 1182.8735073057937
At time: 55.78248739242554 and batch: 1050, loss is 6.961519727706909 and perplexity is 1055.2360106902265
At time: 56.93885850906372 and batch: 1100, loss is 7.022509679794312 and perplexity is 1121.5979401991783
At time: 58.09376120567322 and batch: 1150, loss is 6.942780723571778 and perplexity is 1035.6460600533842
At time: 59.24652051925659 and batch: 1200, loss is 6.661536979675293 and perplexity is 781.7515504755048
At time: 60.404197454452515 and batch: 1250, loss is 6.880215215682983 and perplexity is 972.8357067647622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.466729157162408 and perplexity of 643.3759002041678
Finished 2 epochs...
Completing Train Step...
At time: 63.34534692764282 and batch: 50, loss is 6.949080381393433 and perplexity is 1042.1908692406694
At time: 64.500253200531 and batch: 100, loss is 6.946592741012573 and perplexity is 1039.6014952008245
At time: 65.65505337715149 and batch: 150, loss is 6.991851472854615 and perplexity is 1087.7335220761586
At time: 66.81093716621399 and batch: 200, loss is 6.916372566223145 and perplexity is 1008.6545229407609
At time: 67.97379422187805 and batch: 250, loss is 6.982334270477295 and perplexity is 1077.4304480029616
At time: 69.12861633300781 and batch: 300, loss is 6.968433094024658 and perplexity is 1062.5565193059256
At time: 70.28226947784424 and batch: 350, loss is 6.796777067184448 and perplexity is 894.9582482479752
At time: 71.43771362304688 and batch: 400, loss is 6.942988777160645 and perplexity is 1035.86155234917
At time: 72.59234762191772 and batch: 450, loss is 6.888959226608276 and perplexity is 981.3794918518375
At time: 73.75925397872925 and batch: 500, loss is 6.779644165039063 and perplexity is 879.7556205976291
At time: 74.9191505908966 and batch: 550, loss is 6.848543291091919 and perplexity is 942.5069479186014
At time: 76.07530355453491 and batch: 600, loss is 6.93814642906189 and perplexity is 1030.857675165873
At time: 77.23062515258789 and batch: 650, loss is 7.094125280380249 and perplexity is 1204.867980751382
At time: 78.38462162017822 and batch: 700, loss is 6.885480670928955 and perplexity is 977.9716392856866
At time: 79.53826880455017 and batch: 750, loss is 6.816973657608032 and perplexity is 913.2171162245277
At time: 80.69110202789307 and batch: 800, loss is 6.789723825454712 and perplexity is 888.6681004285538
At time: 81.8443956375122 and batch: 850, loss is 6.867411880493164 and perplexity is 960.4595621535705
At time: 83.03568458557129 and batch: 900, loss is 6.710744972229004 and perplexity is 821.1821695471423
At time: 84.19108152389526 and batch: 950, loss is 6.79382363319397 and perplexity is 892.3189475562294
At time: 85.34595394134521 and batch: 1000, loss is 6.852809238433838 and perplexity is 946.5362211510387
At time: 86.50143122673035 and batch: 1050, loss is 6.8021220779418945 and perplexity is 899.7546166135771
At time: 87.6561770439148 and batch: 1100, loss is 6.667089357376098 and perplexity is 786.104202954934
At time: 88.82303595542908 and batch: 1150, loss is 6.754676771163941 and perplexity is 858.0623545636673
At time: 89.98117852210999 and batch: 1200, loss is 6.8094642162323 and perplexity is 906.3850504316569
At time: 91.13807272911072 and batch: 1250, loss is 6.793986597061157 and perplexity is 892.4643751520875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.533245448648494 and perplexity of 687.6262497271669
Annealing...
Finished 3 epochs...
Completing Train Step...
At time: 94.03409719467163 and batch: 50, loss is 6.554091835021973 and perplexity is 702.1112273700045
At time: 95.21651363372803 and batch: 100, loss is 6.444423561096191 and perplexity is 629.1838863390105
At time: 96.37529635429382 and batch: 150, loss is 6.33941541671753 and perplexity is 566.4650685623368
At time: 97.53395676612854 and batch: 200, loss is 6.36132306098938 and perplexity is 579.0119179207422
At time: 98.69928693771362 and batch: 250, loss is 6.403621368408203 and perplexity is 604.028491637903
At time: 99.85762929916382 and batch: 300, loss is 6.407807998657226 and perplexity is 606.5626366548447
At time: 101.01517081260681 and batch: 350, loss is 6.419467782974243 and perplexity is 613.6764181586356
At time: 102.1736741065979 and batch: 400, loss is 6.381466541290283 and perplexity is 590.7934957018152
At time: 103.33244681358337 and batch: 450, loss is 6.346603441238403 and perplexity is 570.5515024642223
At time: 104.49309992790222 and batch: 500, loss is 6.335567951202393 and perplexity is 564.2898010622313
At time: 105.6513409614563 and batch: 550, loss is 6.338361291885376 and perplexity is 565.8682582785515
At time: 106.810791015625 and batch: 600, loss is 6.36817346572876 and perplexity is 582.9920009324063
At time: 107.96954250335693 and batch: 650, loss is 6.3441362476348875 and perplexity is 569.1455765057932
At time: 109.12879991531372 and batch: 700, loss is 6.368526391983032 and perplexity is 583.1977904276877
At time: 110.289057970047 and batch: 750, loss is 6.31013072013855 and perplexity is 550.1168554636778
At time: 111.4487292766571 and batch: 800, loss is 6.320965719223023 and perplexity is 556.1097791208902
At time: 112.65503406524658 and batch: 850, loss is 6.360549974441528 and perplexity is 578.5644645783171
At time: 113.81319642066956 and batch: 900, loss is 6.335222053527832 and perplexity is 564.0946482856571
At time: 114.9723744392395 and batch: 950, loss is 6.301446886062622 and perplexity is 545.3604139328102
At time: 116.1317789554596 and batch: 1000, loss is 6.299667654037475 and perplexity is 544.3909539219918
At time: 117.29188656806946 and batch: 1050, loss is 6.317679748535157 and perplexity is 554.2854177280543
At time: 118.45084047317505 and batch: 1100, loss is 6.325647716522217 and perplexity is 558.7195883972926
At time: 119.61308240890503 and batch: 1150, loss is 6.356361045837402 and perplexity is 576.1459683338327
At time: 120.77431082725525 and batch: 1200, loss is 6.312273893356323 and perplexity is 551.2971154740122
At time: 121.93401098251343 and batch: 1250, loss is 6.291183710098267 and perplexity is 539.791908196503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.905722513686132 and perplexity of 367.1323881592775
Finished 4 epochs...
Completing Train Step...
At time: 124.86443614959717 and batch: 50, loss is 6.284646987915039 and perplexity is 536.2749456822487
At time: 126.02693891525269 and batch: 100, loss is 6.331470937728882 and perplexity is 561.9826276318872
At time: 127.1889717578888 and batch: 150, loss is 6.250221786499023 and perplexity is 518.1277256604477
At time: 128.35054397583008 and batch: 200, loss is 6.272014179229736 and perplexity is 529.5428987503719
At time: 129.51184010505676 and batch: 250, loss is 6.2950780391693115 and perplexity is 541.8981340232586
At time: 130.67521834373474 and batch: 300, loss is 6.312047395706177 and perplexity is 551.172262112855
At time: 131.83783555030823 and batch: 350, loss is 6.3478625202178955 and perplexity is 571.270324299463
At time: 133.00115442276 and batch: 400, loss is 6.3198269081115725 and perplexity is 555.4768355952061
At time: 134.16237258911133 and batch: 450, loss is 6.31860689163208 and perplexity is 554.7995579307668
At time: 135.32852506637573 and batch: 500, loss is 6.281090145111084 and perplexity is 534.3708882246633
At time: 136.49937677383423 and batch: 550, loss is 6.26754789352417 and perplexity is 527.1830826005101
At time: 137.6682207584381 and batch: 600, loss is 6.303360805511475 and perplexity is 546.4051893247165
At time: 138.830584526062 and batch: 650, loss is 6.282603664398193 and perplexity is 535.1802812319157
At time: 139.99247431755066 and batch: 700, loss is 6.322626676559448 and perplexity is 557.0342212553239
At time: 141.18048548698425 and batch: 750, loss is 6.260178346633911 and perplexity is 523.312262766411
At time: 142.34989547729492 and batch: 800, loss is 6.2700615501403805 and perplexity is 528.5099067355513
At time: 143.5111210346222 and batch: 850, loss is 6.312288007736206 and perplexity is 551.3048967458419
At time: 144.67366647720337 and batch: 900, loss is 6.281712198257447 and perplexity is 534.7033987258878
At time: 145.8355233669281 and batch: 950, loss is 6.257623929977417 and perplexity is 521.9772110709362
At time: 147.00667262077332 and batch: 1000, loss is 6.261531095504761 and perplexity is 524.0206518672234
At time: 148.16866731643677 and batch: 1050, loss is 6.263747644424439 and perplexity is 525.183457508722
At time: 149.3308298587799 and batch: 1100, loss is 6.258661394119263 and perplexity is 522.5190247178004
At time: 150.4944338798523 and batch: 1150, loss is 6.286357192993164 and perplexity is 537.1928705137509
At time: 151.6578266620636 and batch: 1200, loss is 6.26731580734253 and perplexity is 527.0607448888395
At time: 152.81848573684692 and batch: 1250, loss is 6.253514575958252 and perplexity is 519.836623149802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.864468539718294 and perplexity of 352.29487561220793
Finished 5 epochs...
Completing Train Step...
At time: 155.7171754837036 and batch: 50, loss is 6.238788528442383 and perplexity is 512.2375736436222
At time: 156.90115904808044 and batch: 100, loss is 6.271215353012085 and perplexity is 529.120054911337
At time: 158.06311511993408 and batch: 150, loss is 6.187175970077515 and perplexity is 486.4703577047177
At time: 159.22378015518188 and batch: 200, loss is 6.221012811660767 and perplexity is 503.2126336359377
At time: 160.38519525527954 and batch: 250, loss is 6.247134876251221 and perplexity is 516.5307779596587
At time: 161.54540419578552 and batch: 300, loss is 6.259008855819702 and perplexity is 522.7006116120594
At time: 162.70504069328308 and batch: 350, loss is 6.284512872695923 and perplexity is 536.2030278731462
At time: 163.86672568321228 and batch: 400, loss is 6.260208225250244 and perplexity is 523.3278988463236
At time: 165.02798891067505 and batch: 450, loss is 6.2519834518432615 and perplexity is 519.0412977865399
At time: 166.18746066093445 and batch: 500, loss is 6.228881406784057 and perplexity is 507.187829202908
At time: 167.34884929656982 and batch: 550, loss is 6.221579732894898 and perplexity is 503.49799644470374
At time: 168.5114290714264 and batch: 600, loss is 6.256409654617309 and perplexity is 521.3437716677569
At time: 169.6720747947693 and batch: 650, loss is 6.24112078666687 and perplexity is 513.433638161139
At time: 170.8798098564148 and batch: 700, loss is 6.2798779010772705 and perplexity is 533.7234927834809
At time: 172.03947353363037 and batch: 750, loss is 6.224542350769043 and perplexity is 504.9918804195854
At time: 173.19962573051453 and batch: 800, loss is 6.229101400375367 and perplexity is 507.29941954905314
At time: 174.35882949829102 and batch: 850, loss is 6.273111019134522 and perplexity is 530.124041184861
At time: 175.5203378200531 and batch: 900, loss is 6.24530408859253 and perplexity is 515.585984907661
At time: 176.6811547279358 and batch: 950, loss is 6.218223552703858 and perplexity is 501.8109989600981
At time: 177.84179306030273 and batch: 1000, loss is 6.2217942905426025 and perplexity is 503.606037380535
At time: 179.00256443023682 and batch: 1050, loss is 6.239980344772339 and perplexity is 512.8484306911035
At time: 180.1639940738678 and batch: 1100, loss is 6.224151811599731 and perplexity is 504.7946998159781
At time: 181.3253710269928 and batch: 1150, loss is 6.257576093673706 and perplexity is 521.9522422077514
At time: 182.48669338226318 and batch: 1200, loss is 6.239959354400635 and perplexity is 512.8376659248942
At time: 183.64612531661987 and batch: 1250, loss is 6.23621787071228 and perplexity is 510.9224772208891
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.846127698021213 and perplexity of 345.8923840846551
Finished 6 epochs...
Completing Train Step...
At time: 186.56478786468506 and batch: 50, loss is 6.219517068862915 and perplexity is 502.4605195881958
At time: 187.7280285358429 and batch: 100, loss is 6.256741228103638 and perplexity is 521.516664101394
At time: 188.89106035232544 and batch: 150, loss is 6.170525283813476 and perplexity is 478.4373554934643
At time: 190.05455780029297 and batch: 200, loss is 6.208336343765259 and perplexity is 496.87393587210624
At time: 191.21746397018433 and batch: 250, loss is 6.231685600280762 and perplexity is 508.61207801649834
At time: 192.37910723686218 and batch: 300, loss is 6.2407283687591555 and perplexity is 513.2321971342197
At time: 193.54740262031555 and batch: 350, loss is 6.2726027870178225 and perplexity is 529.8546835751885
At time: 194.7132544517517 and batch: 400, loss is 6.2464631652832034 and perplexity is 516.1839350729114
At time: 195.87563109397888 and batch: 450, loss is 6.2468951797485355 and perplexity is 516.4069821759533
At time: 197.037921667099 and batch: 500, loss is 6.213261985778809 and perplexity is 499.327396479924
At time: 198.2008137702942 and batch: 550, loss is 6.203393583297729 and perplexity is 494.42406657080176
At time: 199.3650450706482 and batch: 600, loss is 6.241127815246582 and perplexity is 513.4372468830737
At time: 200.5533049106598 and batch: 650, loss is 6.217313833236695 and perplexity is 501.3546993093142
At time: 201.71474146842957 and batch: 700, loss is 6.262253856658935 and perplexity is 524.3995305412785
At time: 202.8759241104126 and batch: 750, loss is 6.205525131225586 and perplexity is 495.47907917089
At time: 204.03664660453796 and batch: 800, loss is 6.203436632156372 and perplexity is 494.44535142069384
At time: 205.19839239120483 and batch: 850, loss is 6.248025436401367 and perplexity is 516.990984577255
At time: 206.36303091049194 and batch: 900, loss is 6.216526908874512 and perplexity is 500.96032627355885
At time: 207.52445936203003 and batch: 950, loss is 6.197768745422363 and perplexity is 491.6508182027414
At time: 208.68754744529724 and batch: 1000, loss is 6.197707529067993 and perplexity is 491.62072205322585
At time: 209.85035729408264 and batch: 1050, loss is 6.208266439437867 and perplexity is 496.8392034478078
At time: 211.01304626464844 and batch: 1100, loss is 6.189531488418579 and perplexity is 487.61759819723386
At time: 212.17551255226135 and batch: 1150, loss is 6.223898258209228 and perplexity is 504.66672363341496
At time: 213.33839750289917 and batch: 1200, loss is 6.2102470874786375 and perplexity is 497.8242422282373
At time: 214.5002155303955 and batch: 1250, loss is 6.1927885246276855 and perplexity is 489.2083755742338
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.828371813697537 and perplexity of 339.80496263164594
Finished 7 epochs...
Completing Train Step...
At time: 217.44001173973083 and batch: 50, loss is 6.192249069213867 and perplexity is 488.9445406375357
At time: 218.59976434707642 and batch: 100, loss is 6.223494176864624 and perplexity is 504.4628384210316
At time: 219.7596628665924 and batch: 150, loss is 6.1407839298248295 and perplexity is 464.41749892218525
At time: 220.91911697387695 and batch: 200, loss is 6.177995481491089 and perplexity is 482.02475974223233
At time: 222.08147549629211 and batch: 250, loss is 6.200956659317017 and perplexity is 493.2206596075234
At time: 223.2412109375 and batch: 300, loss is 6.2059879493713375 and perplexity is 495.70844895372517
At time: 224.40218877792358 and batch: 350, loss is 6.241169662475586 and perplexity is 513.4587332586926
At time: 225.56278204917908 and batch: 400, loss is 6.217683401107788 and perplexity is 501.5400181400342
At time: 226.72335052490234 and batch: 450, loss is 6.201408205032348 and perplexity is 493.4434215729027
At time: 227.88365769386292 and batch: 500, loss is 6.183214082717895 and perplexity is 484.5468298600865
At time: 229.04802250862122 and batch: 550, loss is 6.173936262130737 and perplexity is 480.07208136149353
At time: 230.25377225875854 and batch: 600, loss is 6.213088474273682 and perplexity is 499.24076494781053
At time: 231.42139387130737 and batch: 650, loss is 6.196289196014404 and perplexity is 490.9239343886052
At time: 232.5810182094574 and batch: 700, loss is 6.234960575103759 and perplexity is 510.2805002959384
At time: 233.73978996276855 and batch: 750, loss is 6.177657918930054 and perplexity is 481.8620736897565
At time: 234.89941549301147 and batch: 800, loss is 6.182001161575317 and perplexity is 483.95946904876257
At time: 236.05851793289185 and batch: 850, loss is 6.2245083427429195 and perplexity is 504.9747069345439
At time: 237.2199604511261 and batch: 900, loss is 6.196765699386597 and perplexity is 491.1579170411657
At time: 238.37931489944458 and batch: 950, loss is 6.17473895072937 and perplexity is 480.4575844464916
At time: 239.54075479507446 and batch: 1000, loss is 6.177660264968872 and perplexity is 481.8632041582125
At time: 240.70138835906982 and batch: 1050, loss is 6.186983585357666 and perplexity is 486.3767772432498
At time: 241.86260175704956 and batch: 1100, loss is 6.1686114406585695 and perplexity is 477.52257708601854
At time: 243.02432012557983 and batch: 1150, loss is 6.207465906143188 and perplexity is 496.4416262814581
At time: 244.18666648864746 and batch: 1200, loss is 6.1920372676849365 and perplexity is 488.84099240249077
At time: 245.34756779670715 and batch: 1250, loss is 6.1869027709960935 and perplexity is 486.33747260272435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.826210189039689 and perplexity of 339.0712251637704
Finished 8 epochs...
Completing Train Step...
At time: 248.24375939369202 and batch: 50, loss is 6.183415813446045 and perplexity is 484.6445877049459
At time: 249.42854142189026 and batch: 100, loss is 6.213824815750122 and perplexity is 499.6085120068618
At time: 250.59011387825012 and batch: 150, loss is 6.124148263931274 and perplexity is 456.7555123997285
At time: 251.7569715976715 and batch: 200, loss is 6.167521991729736 and perplexity is 477.0026239085623
At time: 252.92292428016663 and batch: 250, loss is 6.186772022247315 and perplexity is 486.27388874354284
At time: 254.08636665344238 and batch: 300, loss is 6.187290515899658 and perplexity is 486.52608404333785
At time: 255.250009059906 and batch: 350, loss is 6.228079576492309 and perplexity is 506.78131363787423
At time: 256.41346645355225 and batch: 400, loss is 6.198024616241455 and perplexity is 491.77663339583427
At time: 257.5756449699402 and batch: 450, loss is 6.1768224334716795 and perplexity is 481.45965306594496
At time: 258.7378668785095 and batch: 500, loss is 6.12258394241333 and perplexity is 456.0415584955628
At time: 259.92754459381104 and batch: 550, loss is 6.098455076217651 and perplexity is 445.16948562038795
At time: 261.0910131931305 and batch: 600, loss is 6.135334959030152 and perplexity is 461.8937836039116
At time: 262.25171422958374 and batch: 650, loss is 6.119213790893554 and perplexity is 454.5072162792388
At time: 263.41203236579895 and batch: 700, loss is 6.158854427337647 and perplexity is 472.88603910464855
At time: 264.5732614994049 and batch: 750, loss is 6.101464958190918 and perplexity is 446.5114117367328
At time: 265.73350167274475 and batch: 800, loss is 6.09904203414917 and perplexity is 445.4308580806581
At time: 266.89437675476074 and batch: 850, loss is 6.130896949768067 and perplexity is 459.84843670550003
At time: 268.0548942089081 and batch: 900, loss is 6.09819787979126 and perplexity is 445.0550043422669
At time: 269.21511602401733 and batch: 950, loss is 6.078545732498169 and perplexity is 436.39409927442335
At time: 270.3789384365082 and batch: 1000, loss is 6.084478178024292 and perplexity is 438.99067791164185
At time: 271.5439236164093 and batch: 1050, loss is 6.097841129302979 and perplexity is 444.89625907006604
At time: 272.70558881759644 and batch: 1100, loss is 6.075634088516235 and perplexity is 435.125323030233
At time: 273.86770009994507 and batch: 1150, loss is 6.108518028259278 and perplexity is 449.67182019979253
At time: 275.0283553600311 and batch: 1200, loss is 6.100036382675171 and perplexity is 445.8739918760326
At time: 276.18836998939514 and batch: 1250, loss is 6.101234302520752 and perplexity is 446.40843322456755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.733270130018248 and perplexity of 308.9780163962769
Finished 9 epochs...
Completing Train Step...
At time: 279.1087441444397 and batch: 50, loss is 6.081468591690063 and perplexity is 437.67148367674463
At time: 280.2678413391113 and batch: 100, loss is 6.109421358108521 and perplexity is 450.0782056997843
At time: 281.4270911216736 and batch: 150, loss is 6.0191007995605466 and perplexity is 411.2086704159453
At time: 282.58614134788513 and batch: 200, loss is 6.05399245262146 and perplexity is 425.80966583872186
At time: 283.75150966644287 and batch: 250, loss is 6.076457786560058 and perplexity is 435.4838825597496
At time: 284.91632652282715 and batch: 300, loss is 6.0730257320404055 and perplexity is 433.99183998267307
At time: 286.08028268814087 and batch: 350, loss is 6.118326482772827 and perplexity is 454.1041072027313
At time: 287.2483232021332 and batch: 400, loss is 6.086571102142334 and perplexity is 439.9104142224631
At time: 288.4513256549835 and batch: 450, loss is 6.066261253356934 and perplexity is 431.06601841779184
At time: 289.61274886131287 and batch: 500, loss is 6.05509280204773 and perplexity is 426.2784631333441
At time: 290.7913887500763 and batch: 550, loss is 6.047847805023193 and perplexity is 423.2012376413663
At time: 291.9632976055145 and batch: 600, loss is 6.083770065307617 and perplexity is 438.6799330642846
At time: 293.1247282028198 and batch: 650, loss is 6.071738157272339 and perplexity is 433.4334026320361
At time: 294.28607177734375 and batch: 700, loss is 6.1097647571563725 and perplexity is 450.23278866738326
At time: 295.4466395378113 and batch: 750, loss is 6.0409981155395505 and perplexity is 420.31234587534
At time: 296.6164343357086 and batch: 800, loss is 6.055450534820556 and perplexity is 426.43098418922057
At time: 297.7789771556854 and batch: 850, loss is 6.092675943374633 and perplexity is 442.6042116943897
At time: 298.93942284584045 and batch: 900, loss is 6.066928653717041 and perplexity is 431.3538080584866
At time: 300.0997881889343 and batch: 950, loss is 6.042023935317993 and perplexity is 420.743731817149
At time: 301.2635278701782 and batch: 1000, loss is 6.048563737869262 and perplexity is 423.50432979175775
At time: 302.42786979675293 and batch: 1050, loss is 6.052183933258057 and perplexity is 425.0402767501666
At time: 303.59079670906067 and batch: 1100, loss is 6.041459321975708 and perplexity is 420.5062413439362
At time: 304.7528817653656 and batch: 1150, loss is 6.080104675292969 and perplexity is 437.07494327171423
At time: 305.91377425193787 and batch: 1200, loss is 6.074444675445557 and perplexity is 434.608086948294
At time: 307.074599981308 and batch: 1250, loss is 6.065818109512329 and perplexity is 430.8750364844713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.72726418154083 and perplexity of 307.1278718470068
Finished 10 epochs...
Completing Train Step...
At time: 309.95916533470154 and batch: 50, loss is 6.056306114196778 and perplexity is 426.7959858663627
At time: 311.1459391117096 and batch: 100, loss is 6.0842913722991945 and perplexity is 438.90867959885907
At time: 312.3124084472656 and batch: 150, loss is 5.993738126754761 and perplexity is 400.91046647553964
At time: 313.47359442710876 and batch: 200, loss is 6.029083967208862 and perplexity is 415.33439514664013
At time: 314.6398034095764 and batch: 250, loss is 6.052149753570557 and perplexity is 425.02574925460647
At time: 315.81628251075745 and batch: 300, loss is 6.054785451889038 and perplexity is 426.1474665120006
At time: 316.9803640842438 and batch: 350, loss is 6.097502918243408 and perplexity is 444.74581567713057
At time: 318.1791045665741 and batch: 400, loss is 6.065764751434326 and perplexity is 430.8520464340229
At time: 319.34212589263916 and batch: 450, loss is 6.057494430541992 and perplexity is 427.30345597023296
At time: 320.5022118091583 and batch: 500, loss is 6.040355615615844 and perplexity is 420.04238196036664
At time: 321.6751067638397 and batch: 550, loss is 6.037551984786988 and perplexity is 418.8663874863672
At time: 322.842449426651 and batch: 600, loss is 6.067940092086792 and perplexity is 431.7903165644081
At time: 324.0035800933838 and batch: 650, loss is 6.054014692306518 and perplexity is 425.819135816889
At time: 325.1638972759247 and batch: 700, loss is 6.08768404006958 and perplexity is 440.40027975144153
At time: 326.325147151947 and batch: 750, loss is 6.023127202987671 and perplexity is 412.8677001359082
At time: 327.4878284931183 and batch: 800, loss is 6.034233617782593 and perplexity is 417.4787387241633
At time: 328.6489019393921 and batch: 850, loss is 6.075696315765381 and perplexity is 435.152400524589
At time: 329.808908700943 and batch: 900, loss is 6.0472539520263675 and perplexity is 422.94999292672975
At time: 330.9703166484833 and batch: 950, loss is 6.02390344619751 and perplexity is 413.18831030433495
At time: 332.1327133178711 and batch: 1000, loss is 6.027441959381104 and perplexity is 414.6529724224994
At time: 333.29533529281616 and batch: 1050, loss is 6.036642141342163 and perplexity is 418.48545796884605
At time: 334.4581561088562 and batch: 1100, loss is 6.016635131835938 and perplexity is 410.1960154176318
At time: 335.6215331554413 and batch: 1150, loss is 6.058654384613037 and perplexity is 427.7993959317388
At time: 336.7840027809143 and batch: 1200, loss is 6.050097446441651 and perplexity is 424.1543603641134
At time: 337.9453299045563 and batch: 1250, loss is 6.041127319335938 and perplexity is 420.36665533451406
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.726931829522126 and perplexity of 307.0258142392636
Finished 11 epochs...
Completing Train Step...
At time: 340.87606024742126 and batch: 50, loss is 6.039557619094849 and perplexity is 419.70732330649196
At time: 342.04064297676086 and batch: 100, loss is 6.055447092056275 and perplexity is 426.4295160903868
At time: 343.20799684524536 and batch: 150, loss is 5.965979986190796 and perplexity is 389.9349717781444
At time: 344.3703908920288 and batch: 200, loss is 6.005556230545044 and perplexity is 405.67657569321534
At time: 345.531631231308 and batch: 250, loss is 6.029423933029175 and perplexity is 415.47561864921215
At time: 346.6941375732422 and batch: 300, loss is 6.024361867904663 and perplexity is 413.3777682174123
At time: 347.89148139953613 and batch: 350, loss is 6.070285339355468 and perplexity is 432.8041600170721
At time: 349.05605602264404 and batch: 400, loss is 6.039274158477784 and perplexity is 419.58836966977293
At time: 350.22070717811584 and batch: 450, loss is 6.027049255371094 and perplexity is 414.4901685064341
At time: 351.38367319107056 and batch: 500, loss is 6.015708255767822 and perplexity is 409.8159906928228
At time: 352.54616713523865 and batch: 550, loss is 6.012609367370605 and perplexity is 408.54798239710357
At time: 353.70881748199463 and batch: 600, loss is 6.043010244369507 and perplexity is 421.1589198864251
At time: 354.8709878921509 and batch: 650, loss is 6.0300056266784665 and perplexity is 415.71736848337065
At time: 356.033269405365 and batch: 700, loss is 6.0705557060241695 and perplexity is 432.9211916560302
At time: 357.1941006183624 and batch: 750, loss is 6.012978801727295 and perplexity is 408.69894194126124
At time: 358.3558750152588 and batch: 800, loss is 6.025640277862549 and perplexity is 413.90657241492613
At time: 359.5186126232147 and batch: 850, loss is 6.054752511978149 and perplexity is 426.1334294836187
At time: 360.6801908016205 and batch: 900, loss is 6.0307234764099125 and perplexity is 416.01589822162055
At time: 361.84337735176086 and batch: 950, loss is 6.008427152633667 and perplexity is 406.8429149687049
At time: 363.0062847137451 and batch: 1000, loss is 6.019227113723755 and perplexity is 411.2606151756632
At time: 364.1690957546234 and batch: 1050, loss is 6.020467081069946 and perplexity is 411.7708812004813
At time: 365.34257316589355 and batch: 1100, loss is 6.0055871677398684 and perplexity is 405.6891263826139
At time: 366.51630425453186 and batch: 1150, loss is 6.038655395507813 and perplexity is 419.32882423082503
At time: 367.6793291568756 and batch: 1200, loss is 6.036910991668702 and perplexity is 418.5979830463966
At time: 368.84207248687744 and batch: 1250, loss is 6.02472183227539 and perplexity is 413.5265962704127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.7153244575444795 and perplexity of 303.4826545738111
Finished 12 epochs...
Completing Train Step...
At time: 371.74581265449524 and batch: 50, loss is 6.017704725265503 and perplexity is 410.63499310252985
At time: 372.9393742084503 and batch: 100, loss is 6.043951053619384 and perplexity is 421.5553365409107
At time: 374.09891176223755 and batch: 150, loss is 5.952367496490479 and perplexity is 384.6629500701171
At time: 375.2597198486328 and batch: 200, loss is 5.993712921142578 and perplexity is 400.9003614091542
At time: 376.4208495616913 and batch: 250, loss is 6.013218879699707 and perplexity is 408.7970733336911
At time: 377.60917711257935 and batch: 300, loss is 6.006808023452759 and perplexity is 406.1847167308137
At time: 378.7708089351654 and batch: 350, loss is 6.044035091400146 and perplexity is 421.590764604489
At time: 379.9317944049835 and batch: 400, loss is 6.018701248168945 and perplexity is 411.04440423801725
At time: 381.0931396484375 and batch: 450, loss is 6.0045093822479245 and perplexity is 405.252116071949
At time: 382.2540807723999 and batch: 500, loss is 5.989273996353149 and perplexity is 399.12473870021336
At time: 383.419650554657 and batch: 550, loss is 5.990095729827881 and perplexity is 399.4528476491874
At time: 384.58546566963196 and batch: 600, loss is 6.023377361297608 and perplexity is 412.9709953415401
At time: 385.74622774124146 and batch: 650, loss is 6.007415828704834 and perplexity is 406.43167297800437
At time: 386.9063079357147 and batch: 700, loss is 6.046224641799927 and perplexity is 422.51487015031586
At time: 388.06477189064026 and batch: 750, loss is 5.980965948104858 and perplexity is 395.8225275728958
At time: 389.2251467704773 and batch: 800, loss is 5.9988028430938725 and perplexity is 402.9461149051242
At time: 390.3863101005554 and batch: 850, loss is 6.026987752914429 and perplexity is 414.4646771267066
At time: 391.5468182563782 and batch: 900, loss is 6.006521682739258 and perplexity is 406.068426159369
At time: 392.70790457725525 and batch: 950, loss is 5.9842635536193844 and perplexity is 397.12994861725366
At time: 393.8751847743988 and batch: 1000, loss is 5.9912623596191406 and perplexity is 399.91913317986854
At time: 395.0349690914154 and batch: 1050, loss is 5.995138807296753 and perplexity is 401.47240742301534
At time: 396.1967968940735 and batch: 1100, loss is 5.9780481338500975 and perplexity is 394.6692742672541
At time: 397.3580346107483 and batch: 1150, loss is 6.011175222396851 and perplexity is 407.9624853058193
At time: 398.5180666446686 and batch: 1200, loss is 6.013928670883178 and perplexity is 409.0873368932436
At time: 399.6796381473541 and batch: 1250, loss is 5.996962966918946 and perplexity is 402.2054255458286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.708783894559763 and perplexity of 301.5041843643816
Finished 13 epochs...
Completing Train Step...
At time: 402.5896165370941 and batch: 50, loss is 5.992886772155762 and perplexity is 400.5692947557821
At time: 403.7752597332001 and batch: 100, loss is 6.0217708969116215 and perplexity is 412.3081047427097
At time: 404.9381353855133 and batch: 150, loss is 5.92351957321167 and perplexity is 373.72475348213624
At time: 406.12803649902344 and batch: 200, loss is 5.9631103515625 and perplexity is 388.81760486451503
At time: 407.2894413471222 and batch: 250, loss is 5.983757238388062 and perplexity is 396.928926570011
At time: 408.4521074295044 and batch: 300, loss is 5.97980562210083 and perplexity is 395.36351075724673
At time: 409.6162507534027 and batch: 350, loss is 6.025033807754516 and perplexity is 413.6556265545113
At time: 410.7795603275299 and batch: 400, loss is 5.999205875396728 and perplexity is 403.1085479364218
At time: 411.94286823272705 and batch: 450, loss is 5.983449249267578 and perplexity is 396.8066956028925
At time: 413.1045837402344 and batch: 500, loss is 5.967686462402344 and perplexity is 390.60095461185074
At time: 414.26789259910583 and batch: 550, loss is 5.973462047576905 and perplexity is 392.8634309677382
At time: 415.43239307403564 and batch: 600, loss is 6.0076380443573 and perplexity is 406.52199849289815
At time: 416.5951991081238 and batch: 650, loss is 5.988152103424072 and perplexity is 398.67721456206846
At time: 417.75611543655396 and batch: 700, loss is 6.026001071929931 and perplexity is 414.0559343935416
At time: 418.9248502254486 and batch: 750, loss is 5.969482011795044 and perplexity is 391.30292794399077
At time: 420.0880742073059 and batch: 800, loss is 5.982141056060791 and perplexity is 396.287935172715
At time: 421.25061321258545 and batch: 850, loss is 6.006584148406983 and perplexity is 406.09379228699873
At time: 422.41270685195923 and batch: 900, loss is 5.9886064624786375 and perplexity is 398.8583983224776
At time: 423.5743799209595 and batch: 950, loss is 5.966522846221924 and perplexity is 390.1467093557961
At time: 424.7377164363861 and batch: 1000, loss is 5.972360639572144 and perplexity is 392.4309662438654
At time: 425.89957070350647 and batch: 1050, loss is 5.979498767852784 and perplexity is 395.2422103961656
At time: 427.06331849098206 and batch: 1100, loss is 5.966350831985474 and perplexity is 390.0796043391572
At time: 428.2257447242737 and batch: 1150, loss is 6.000497274398803 and perplexity is 403.629458192053
At time: 429.38711404800415 and batch: 1200, loss is 5.99868782043457 and perplexity is 402.89976963686144
At time: 430.5513172149658 and batch: 1250, loss is 5.985680007934571 and perplexity is 397.6928636242409
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.701445419422901 and perplexity of 299.2997020662278
Finished 14 epochs...
Completing Train Step...
At time: 433.48581528663635 and batch: 50, loss is 5.975495529174805 and perplexity is 393.6631243304034
At time: 434.6461763381958 and batch: 100, loss is 6.003937473297119 and perplexity is 405.0204150217024
At time: 435.8470597267151 and batch: 150, loss is 5.911386222839355 and perplexity is 369.2176187221562
At time: 437.0117383003235 and batch: 200, loss is 5.950634384155274 and perplexity is 383.99686333474887
At time: 438.17216324806213 and batch: 250, loss is 5.971358137130737 and perplexity is 392.0377503749889
At time: 439.3333156108856 and batch: 300, loss is 5.968187913894654 and perplexity is 390.7968711606561
At time: 440.4957196712494 and batch: 350, loss is 6.014928579330444 and perplexity is 409.49659135146936
At time: 441.6580879688263 and batch: 400, loss is 5.984878063201904 and perplexity is 397.3740637740542
At time: 442.8201072216034 and batch: 450, loss is 5.979720869064331 and perplexity is 395.3300039191123
At time: 443.98098373413086 and batch: 500, loss is 5.960924558639526 and perplexity is 387.968658244232
At time: 445.1403737068176 and batch: 550, loss is 5.965071802139282 and perplexity is 389.5809998158232
At time: 446.31501936912537 and batch: 600, loss is 5.992828254699707 and perplexity is 400.5458551454991
At time: 447.48378443717957 and batch: 650, loss is 5.980353927612304 and perplexity is 395.5803501909265
At time: 448.6515533924103 and batch: 700, loss is 6.017154731750488 and perplexity is 410.40920861497887
At time: 449.8104326725006 and batch: 750, loss is 5.956536817550659 and perplexity is 386.2700814040793
At time: 450.9714002609253 and batch: 800, loss is 5.974174356460571 and perplexity is 393.1433707696516
At time: 452.13189601898193 and batch: 850, loss is 6.000598754882812 and perplexity is 403.67042078324727
At time: 453.29250597953796 and batch: 900, loss is 5.980559921264648 and perplexity is 395.66184562554065
At time: 454.45349645614624 and batch: 950, loss is 5.959939260482788 and perplexity is 387.58658170095896
At time: 455.6159906387329 and batch: 1000, loss is 5.965595560073853 and perplexity is 389.7850994003573
At time: 456.7755162715912 and batch: 1050, loss is 5.9719061756134035 and perplexity is 392.2526610331288
At time: 457.93621158599854 and batch: 1100, loss is 5.953690433502198 and perplexity is 385.1721716835933
At time: 459.0969579219818 and batch: 1150, loss is 5.992658119201661 and perplexity is 400.4777138737325
At time: 460.2593050003052 and batch: 1200, loss is 5.988018522262573 and perplexity is 398.62396235350906
At time: 461.4266858100891 and batch: 1250, loss is 5.975764484405517 and perplexity is 393.7690163262954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.6913777372262775 and perplexity of 296.30156525236873
Finished 15 epochs...
Completing Train Step...
At time: 464.32668900489807 and batch: 50, loss is 5.970130739212036 and perplexity is 391.5568592388881
At time: 465.51114654541016 and batch: 100, loss is 5.992231931686401 and perplexity is 400.3070716373182
At time: 466.67319560050964 and batch: 150, loss is 5.900891542434692 and perplexity is 365.36305937594113
At time: 467.8342001438141 and batch: 200, loss is 5.942150611877441 and perplexity is 380.7529013649555
At time: 469.00128984451294 and batch: 250, loss is 5.963608512878418 and perplexity is 389.01134700762
At time: 470.16571521759033 and batch: 300, loss is 5.955517148971557 and perplexity is 385.87641467792884
At time: 471.32991886138916 and batch: 350, loss is 6.008186836242675 and perplexity is 406.74515569472766
At time: 472.4930057525635 and batch: 400, loss is 5.980062017440796 and perplexity is 395.464893115424
At time: 473.6558463573456 and batch: 450, loss is 5.970215091705322 and perplexity is 391.5898894292981
At time: 474.81621980667114 and batch: 500, loss is 5.951478309631348 and perplexity is 384.3210648521679
At time: 475.97639203071594 and batch: 550, loss is 5.951779623031616 and perplexity is 384.4368833869751
At time: 477.13750076293945 and batch: 600, loss is 5.983268117904663 and perplexity is 396.7348279742411
At time: 478.29816937446594 and batch: 650, loss is 5.972225742340088 and perplexity is 392.3780319631704
At time: 479.45821261405945 and batch: 700, loss is 6.006797285079956 and perplexity is 406.1803549913174
At time: 480.6180968284607 and batch: 750, loss is 5.949631881713867 and perplexity is 383.612098437862
At time: 481.77899169921875 and batch: 800, loss is 5.961629819869995 and perplexity is 388.24237400674116
At time: 482.9403295516968 and batch: 850, loss is 5.988613557815552 and perplexity is 398.8612283672349
At time: 484.1006438732147 and batch: 900, loss is 5.970934782028198 and perplexity is 391.87181432039586
At time: 485.26114082336426 and batch: 950, loss is 5.945370063781739 and perplexity is 381.9806923648671
At time: 486.42345428466797 and batch: 1000, loss is 5.954547023773193 and perplexity is 385.5022477683144
At time: 487.58294200897217 and batch: 1050, loss is 5.959412136077881 and perplexity is 387.3823291927004
At time: 488.74467372894287 and batch: 1100, loss is 5.944281234741211 and perplexity is 381.56500704029366
At time: 489.90589427948 and batch: 1150, loss is 5.980303497314453 and perplexity is 395.5604014590566
At time: 491.066787481308 and batch: 1200, loss is 5.980163593292236 and perplexity is 395.50506483886085
At time: 492.2283673286438 and batch: 1250, loss is 5.966881170272827 and perplexity is 390.2865333547936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.684596597713275 and perplexity of 294.29910017142913
Finished 16 epochs...
Completing Train Step...
At time: 495.1583516597748 and batch: 50, loss is 5.951744575500488 and perplexity is 384.4234100594427
At time: 496.3178298473358 and batch: 100, loss is 5.979896450042725 and perplexity is 395.39942244209664
At time: 497.48760628700256 and batch: 150, loss is 5.887279920578003 and perplexity is 360.42356907235217
At time: 498.65420269966125 and batch: 200, loss is 5.928682823181152 and perplexity is 375.65937798113845
At time: 499.81257009506226 and batch: 250, loss is 5.952146825790405 and perplexity is 384.57807559263233
At time: 500.97245955467224 and batch: 300, loss is 5.944265480041504 and perplexity is 381.55899564554323
At time: 502.13297033309937 and batch: 350, loss is 5.995714998245239 and perplexity is 401.70379884665954
At time: 503.29341888427734 and batch: 400, loss is 5.9606428146362305 and perplexity is 387.85936579827364
At time: 504.4544641971588 and batch: 450, loss is 5.949739208221436 and perplexity is 383.6532723941373
At time: 505.6134684085846 and batch: 500, loss is 5.93349066734314 and perplexity is 377.46983844689817
At time: 506.7729516029358 and batch: 550, loss is 5.9343374156951905 and perplexity is 377.7895957684394
At time: 507.93190145492554 and batch: 600, loss is 5.964534397125244 and perplexity is 389.37169327937676
At time: 509.09166169166565 and batch: 650, loss is 5.949759607315063 and perplexity is 383.6610986529854
At time: 510.2513864040375 and batch: 700, loss is 5.98014461517334 and perplexity is 395.4975589679403
At time: 511.41118454933167 and batch: 750, loss is 5.925529594421387 and perplexity is 374.4767036279343
At time: 512.5752375125885 and batch: 800, loss is 5.937495136260987 and perplexity is 378.9844352362898
At time: 513.7362811565399 and batch: 850, loss is 5.964843988418579 and perplexity is 389.4922580274222
At time: 514.8972642421722 and batch: 900, loss is 5.939437561035156 and perplexity is 379.7212994122494
At time: 516.059317111969 and batch: 950, loss is 5.917706861495971 and perplexity is 371.5587006482765
At time: 517.2222890853882 and batch: 1000, loss is 5.926034708023071 and perplexity is 374.6659046844414
At time: 518.3839013576508 and batch: 1050, loss is 5.932052297592163 and perplexity is 376.92728753731905
At time: 519.5461006164551 and batch: 1100, loss is 5.909939107894897 and perplexity is 368.68370479892906
At time: 520.7068490982056 and batch: 1150, loss is 5.951438856124878 and perplexity is 384.30590233765844
At time: 521.8675901889801 and batch: 1200, loss is 5.944365921020508 and perplexity is 381.59732172933616
At time: 523.0428717136383 and batch: 1250, loss is 5.9348450756073 and perplexity is 377.9814330913624
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.655579504305429 and perplexity of 285.88210464305683
Finished 17 epochs...
Completing Train Step...
At time: 526.0519125461578 and batch: 50, loss is 5.921548099517822 and perplexity is 372.988690764454
At time: 527.238835811615 and batch: 100, loss is 5.943416414260864 and perplexity is 381.23516445549075
At time: 528.4026861190796 and batch: 150, loss is 5.852948627471924 and perplexity is 348.2597562949418
At time: 529.5667214393616 and batch: 200, loss is 5.894199304580688 and perplexity is 362.92612624265524
At time: 530.7296171188354 and batch: 250, loss is 5.917412509918213 and perplexity is 371.44934785338427
At time: 531.895094871521 and batch: 300, loss is 5.904682912826538 and perplexity is 366.7509153370564
At time: 533.0618042945862 and batch: 350, loss is 5.95217734336853 and perplexity is 384.5898121631843
At time: 534.229026556015 and batch: 400, loss is 5.923257484436035 and perplexity is 373.62681725362535
At time: 535.3953309059143 and batch: 450, loss is 5.909225387573242 and perplexity is 368.4206616273457
At time: 536.5598750114441 and batch: 500, loss is 5.897165384292602 and perplexity is 364.00419208687305
At time: 537.7238576412201 and batch: 550, loss is 5.895177936553955 and perplexity is 363.281471201339
At time: 538.8879170417786 and batch: 600, loss is 5.920433130264282 and perplexity is 372.5730515978379
At time: 540.0519115924835 and batch: 650, loss is 5.904922103881836 and perplexity is 366.83864936770664
At time: 541.2142112255096 and batch: 700, loss is 5.943665685653687 and perplexity is 381.3302073211693
At time: 542.3756680488586 and batch: 750, loss is 5.8932029819488525 and perplexity is 362.56471480049134
At time: 543.5388457775116 and batch: 800, loss is 5.898452577590942 and perplexity is 364.4730375261139
At time: 544.7029011249542 and batch: 850, loss is 5.931212139129639 and perplexity is 376.61074187981467
At time: 545.8678271770477 and batch: 900, loss is 5.907064180374146 and perplexity is 367.6252880342955
At time: 547.0307269096375 and batch: 950, loss is 5.883697862625122 and perplexity is 359.13482052435285
At time: 548.194415807724 and batch: 1000, loss is 5.891626787185669 and perplexity is 361.99369233523765
At time: 549.3581116199493 and batch: 1050, loss is 5.891192569732666 and perplexity is 361.83654247722325
At time: 550.5268115997314 and batch: 1100, loss is 5.877089529037476 and perplexity is 356.7693623041964
At time: 551.6902115345001 and batch: 1150, loss is 5.9173578357696535 and perplexity is 371.429039731727
At time: 552.8558733463287 and batch: 1200, loss is 5.911455020904541 and perplexity is 369.2430210537625
At time: 554.0778024196625 and batch: 1250, loss is 5.902035264968872 and perplexity is 365.78117239717056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.62651028598312 and perplexity of 277.69136135122847
Finished 18 epochs...
Completing Train Step...
At time: 557.032084941864 and batch: 50, loss is 5.8915486240386965 and perplexity is 361.9653988748278
At time: 558.2279160022736 and batch: 100, loss is 5.923470077514648 and perplexity is 373.7062561727407
At time: 559.3917920589447 and batch: 150, loss is 5.823200359344482 and perplexity is 338.0522128175675
At time: 560.5558428764343 and batch: 200, loss is 5.866210346221924 and perplexity is 352.9090398400925
At time: 561.7173116207123 and batch: 250, loss is 5.8876309299469 and perplexity is 360.5501033279233
At time: 562.8858940601349 and batch: 300, loss is 5.872294692993164 and perplexity is 355.062806303965
At time: 564.0614941120148 and batch: 350, loss is 5.917836170196534 and perplexity is 371.60674952754204
At time: 565.2267558574677 and batch: 400, loss is 5.884107418060303 and perplexity is 359.2819362660307
At time: 566.3922274112701 and batch: 450, loss is 5.876136569976807 and perplexity is 356.42953765302684
At time: 567.5685906410217 and batch: 500, loss is 5.86329345703125 and perplexity is 351.8811431356844
At time: 568.7403898239136 and batch: 550, loss is 5.8599011421203615 and perplexity is 350.68947388843367
At time: 569.9041030406952 and batch: 600, loss is 5.884876365661621 and perplexity is 359.558311494536
At time: 571.0685029029846 and batch: 650, loss is 5.8727967643737795 and perplexity is 355.24111793616913
At time: 572.2301034927368 and batch: 700, loss is 5.907141504287719 and perplexity is 367.65371535933673
At time: 573.3933217525482 and batch: 750, loss is 5.854932632446289 and perplexity is 348.95139126053607
At time: 574.5586540699005 and batch: 800, loss is 5.866189050674438 and perplexity is 352.90152452889816
At time: 575.7300217151642 and batch: 850, loss is 5.896258964538574 and perplexity is 363.6744009837812
At time: 576.8930058479309 and batch: 900, loss is 5.872068891525268 and perplexity is 354.9826416520949
At time: 578.0564692020416 and batch: 950, loss is 5.857834577560425 and perplexity is 349.96549977742876
At time: 579.2212119102478 and batch: 1000, loss is 5.861752958297729 and perplexity is 351.3394879969801
At time: 580.3855931758881 and batch: 1050, loss is 5.863542919158935 and perplexity is 351.9689351042738
At time: 581.5510835647583 and batch: 1100, loss is 5.842866764068604 and perplexity is 344.76628892429426
At time: 582.7441036701202 and batch: 1150, loss is 5.885936336517334 and perplexity is 359.93963488578754
At time: 583.9083399772644 and batch: 1200, loss is 5.8749873828887935 and perplexity is 356.0201686963811
At time: 585.0730957984924 and batch: 1250, loss is 5.868746538162231 and perplexity is 353.8052208657706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.606332152429288 and perplexity of 272.1442216498263
Finished 19 epochs...
Completing Train Step...
At time: 588.0253252983093 and batch: 50, loss is 5.864798097610474 and perplexity is 352.4109963020345
At time: 589.1861526966095 and batch: 100, loss is 5.89094880104065 and perplexity is 361.7483488064046
At time: 590.3486294746399 and batch: 150, loss is 5.793851203918457 and perplexity is 328.2748464242551
At time: 591.5082862377167 and batch: 200, loss is 5.838127241134644 and perplexity is 343.1361273367325
At time: 592.6703877449036 and batch: 250, loss is 5.862184772491455 and perplexity is 351.4912341354238
At time: 593.8324098587036 and batch: 300, loss is 5.84763918876648 and perplexity is 346.41559253481194
At time: 594.9956166744232 and batch: 350, loss is 5.893741054534912 and perplexity is 362.75985342886315
At time: 596.1648120880127 and batch: 400, loss is 5.863534336090088 and perplexity is 351.96591414363616
At time: 597.3292090892792 and batch: 450, loss is 5.852741241455078 and perplexity is 348.1875395798834
At time: 598.4914553165436 and batch: 500, loss is 5.842444686889649 and perplexity is 344.62080164725586
At time: 599.6539704799652 and batch: 550, loss is 5.839026670455933 and perplexity is 343.44489286642295
At time: 600.8171181678772 and batch: 600, loss is 5.864271421432495 and perplexity is 352.22543869409947
At time: 601.9799184799194 and batch: 650, loss is 5.853813858032226 and perplexity is 348.5612116744087
At time: 603.1416506767273 and batch: 700, loss is 5.8884929752349855 and perplexity is 360.8610478504904
At time: 604.3034505844116 and batch: 750, loss is 5.839203004837036 and perplexity is 343.5054593488692
At time: 605.4662039279938 and batch: 800, loss is 5.8524386501312256 and perplexity is 348.0821969900136
At time: 606.6288895606995 and batch: 850, loss is 5.881704416275024 and perplexity is 358.4196176233329
At time: 607.79132771492 and batch: 900, loss is 5.857788572311401 and perplexity is 349.9493998978041
At time: 608.9538052082062 and batch: 950, loss is 5.837816066741944 and perplexity is 343.02936877182225
At time: 610.1181507110596 and batch: 1000, loss is 5.8472128677368165 and perplexity is 346.2679397586892
At time: 611.2819056510925 and batch: 1050, loss is 5.844202880859375 and perplexity is 345.2272448287045
At time: 612.4731955528259 and batch: 1100, loss is 5.834878225326538 and perplexity is 342.02308176513026
At time: 613.6364915370941 and batch: 1150, loss is 5.873848381042481 and perplexity is 355.6148919162265
At time: 614.8079996109009 and batch: 1200, loss is 5.869696025848389 and perplexity is 354.1413140992244
At time: 615.9876952171326 and batch: 1250, loss is 5.859568252563476 and perplexity is 350.57275245361785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.598157896612682 and perplexity of 269.92871256844194
Finished 20 epochs...
Completing Train Step...
At time: 618.909126996994 and batch: 50, loss is 5.85267373085022 and perplexity is 348.16403402192833
At time: 620.0983109474182 and batch: 100, loss is 5.876816902160645 and perplexity is 356.6721106445128
At time: 621.2615435123444 and batch: 150, loss is 5.784114465713501 and perplexity is 325.094030699827
At time: 622.4243426322937 and batch: 200, loss is 5.8261797237396244 and perplexity is 339.0608954145027
At time: 623.5873582363129 and batch: 250, loss is 5.845141019821167 and perplexity is 345.55126792333067
At time: 624.7520644664764 and batch: 300, loss is 5.833743038177491 and perplexity is 341.6350418486669
At time: 625.9167966842651 and batch: 350, loss is 5.8845083522796635 and perplexity is 359.4260135695084
At time: 627.0822658538818 and batch: 400, loss is 5.853112201690674 and perplexity is 348.31672727186447
At time: 628.2466011047363 and batch: 450, loss is 5.847519941329956 and perplexity is 346.3742858263389
At time: 629.4099338054657 and batch: 500, loss is 5.83248420715332 and perplexity is 341.20525163242894
At time: 630.5756046772003 and batch: 550, loss is 5.825915384292602 and perplexity is 338.97128008985646
At time: 631.7433006763458 and batch: 600, loss is 5.851971645355224 and perplexity is 347.91967889289566
At time: 632.9077489376068 and batch: 650, loss is 5.838706951141358 and perplexity is 343.3351044523548
At time: 634.0708174705505 and batch: 700, loss is 5.880940055847168 and perplexity is 358.1457605271511
At time: 635.234855890274 and batch: 750, loss is 5.831169958114624 and perplexity is 340.7571175029007
At time: 636.3995060920715 and batch: 800, loss is 5.842508821487427 and perplexity is 344.64290447252586
At time: 637.5636034011841 and batch: 850, loss is 5.871392278671265 and perplexity is 354.74253707188774
At time: 638.7286794185638 and batch: 900, loss is 5.847461719512939 and perplexity is 346.35411987310493
At time: 639.8923876285553 and batch: 950, loss is 5.827568950653077 and perplexity is 339.5322552728037
At time: 641.0571181774139 and batch: 1000, loss is 5.834211177825928 and perplexity is 341.795012198362
At time: 642.2692222595215 and batch: 1050, loss is 5.831950330734253 and perplexity is 341.02313881171636
At time: 643.4354286193848 and batch: 1100, loss is 5.820118684768676 and perplexity is 337.01204945370733
At time: 644.6024296283722 and batch: 1150, loss is 5.863620691299438 and perplexity is 351.996309546218
At time: 645.7666039466858 and batch: 1200, loss is 5.854045190811157 and perplexity is 348.6418546354534
At time: 646.9311983585358 and batch: 1250, loss is 5.8465977478027344 and perplexity is 346.05500894211895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.594104627623175 and perplexity of 268.8368332236275
Finished 21 epochs...
Completing Train Step...
At time: 649.8748724460602 and batch: 50, loss is 5.838541412353516 and perplexity is 343.27827387930205
At time: 651.0389926433563 and batch: 100, loss is 5.863650712966919 and perplexity is 352.0068772210065
At time: 652.2008531093597 and batch: 150, loss is 5.770519971847534 and perplexity is 320.7044465839261
At time: 653.3621366024017 and batch: 200, loss is 5.814565515518188 and perplexity is 335.14575123460276
At time: 654.5238974094391 and batch: 250, loss is 5.829847192764282 and perplexity is 340.30667377634103
At time: 655.6856579780579 and batch: 300, loss is 5.818741817474365 and perplexity is 336.54834788604273
At time: 656.8478217124939 and batch: 350, loss is 5.8694157218933105 and perplexity is 354.042060799422
At time: 658.0114705562592 and batch: 400, loss is 5.837323751449585 and perplexity is 342.8605317317862
At time: 659.1753256320953 and batch: 450, loss is 5.8352391815185545 and perplexity is 342.14655939797353
At time: 660.338020324707 and batch: 500, loss is 5.823436307907104 and perplexity is 338.131985161988
At time: 661.4998741149902 and batch: 550, loss is 5.819783010482788 and perplexity is 336.89894215933845
At time: 662.6614189147949 and batch: 600, loss is 5.840272722244262 and perplexity is 343.8731097241566
At time: 663.8242604732513 and batch: 650, loss is 5.833032703399658 and perplexity is 341.3924527670465
At time: 664.9853446483612 and batch: 700, loss is 5.870691223144531 and perplexity is 354.4939300095656
At time: 666.1471364498138 and batch: 750, loss is 5.819571533203125 and perplexity is 336.82770322050317
At time: 667.3124713897705 and batch: 800, loss is 5.830368604660034 and perplexity is 340.4841599919428
At time: 668.480147600174 and batch: 850, loss is 5.861349086761475 and perplexity is 351.19762062824043
At time: 669.646388053894 and batch: 900, loss is 5.841074781417847 and perplexity is 344.1490269425365
At time: 670.8567426204681 and batch: 950, loss is 5.816925048828125 and perplexity is 335.93747247732796
At time: 672.0202641487122 and batch: 1000, loss is 5.824046220779419 and perplexity is 338.3382791165537
At time: 673.1841156482697 and batch: 1050, loss is 5.822161712646484 and perplexity is 337.7012782830201
At time: 674.5610632896423 and batch: 1100, loss is 5.814374103546142 and perplexity is 335.0816064646705
At time: 675.7379207611084 and batch: 1150, loss is 5.8554950714111325 and perplexity is 349.14771032338734
At time: 676.9096183776855 and batch: 1200, loss is 5.849653367996216 and perplexity is 347.1140387879921
At time: 678.0741219520569 and batch: 1250, loss is 5.8417526054382325 and perplexity is 344.38237849630036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.583988384608805 and perplexity of 266.13092437972955
Finished 22 epochs...
Completing Train Step...
At time: 681.2254934310913 and batch: 50, loss is 5.834969892501831 and perplexity is 342.0544354919526
At time: 682.3895487785339 and batch: 100, loss is 5.858989658355713 and perplexity is 350.36997175917344
At time: 683.5543184280396 and batch: 150, loss is 5.763757696151734 and perplexity is 318.54307084491444
At time: 684.7642545700073 and batch: 200, loss is 5.805897245407104 and perplexity is 332.25317227626243
At time: 685.9269053936005 and batch: 250, loss is 5.824920387268066 and perplexity is 338.6341724131434
At time: 687.0911040306091 and batch: 300, loss is 5.812825651168823 and perplexity is 334.56315006066205
At time: 688.2556798458099 and batch: 350, loss is 5.861172275543213 and perplexity is 351.13553043836924
At time: 689.4195704460144 and batch: 400, loss is 5.831665449142456 and perplexity is 340.92600143407424
At time: 690.5833866596222 and batch: 450, loss is 5.828460464477539 and perplexity is 339.83508794192824
At time: 691.746001958847 and batch: 500, loss is 5.817704534530639 and perplexity is 336.199433018047
At time: 692.9079229831696 and batch: 550, loss is 5.813124933242798 and perplexity is 334.66329379895313
At time: 694.0711627006531 and batch: 600, loss is 5.832119998931884 and perplexity is 341.0810045018739
At time: 695.2349524497986 and batch: 650, loss is 5.823551177978516 and perplexity is 338.1708286382037
At time: 696.3978486061096 and batch: 700, loss is 5.863499603271484 and perplexity is 351.95368958768347
At time: 697.5611937046051 and batch: 750, loss is 5.814887285232544 and perplexity is 335.2536083388893
At time: 698.724178314209 and batch: 800, loss is 5.824818983078003 and perplexity is 338.59983523015853
At time: 699.8875241279602 and batch: 850, loss is 5.857706756591797 and perplexity is 349.9207697070421
At time: 701.0536575317383 and batch: 900, loss is 5.834254608154297 and perplexity is 341.8098567903271
At time: 702.2207946777344 and batch: 950, loss is 5.811443481445313 and perplexity is 334.101046430436
At time: 703.384458065033 and batch: 1000, loss is 5.81501238822937 and perplexity is 335.2955521935855
At time: 704.5482771396637 and batch: 1050, loss is 5.8168284034729005 and perplexity is 335.90500724979916
At time: 705.715001821518 and batch: 1100, loss is 5.803190536499024 and perplexity is 331.35507564650663
At time: 706.8792886734009 and batch: 1150, loss is 5.84367561340332 and perplexity is 345.04526571761045
At time: 708.0428442955017 and batch: 1200, loss is 5.841024742126465 and perplexity is 344.13180639995414
At time: 709.2074139118195 and batch: 1250, loss is 5.829953451156616 and perplexity is 340.3428361376401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.5861740669194795 and perplexity of 266.7132381781346
Annealing...
Finished 23 epochs...
Completing Train Step...
At time: 712.1683013439178 and batch: 50, loss is 5.795223016738891 and perplexity is 328.7254870938592
At time: 713.3321340084076 and batch: 100, loss is 5.785139894485473 and perplexity is 325.42756244980353
At time: 714.5025012493134 and batch: 150, loss is 5.681017007827759 and perplexity is 293.24751333680956
At time: 715.6703443527222 and batch: 200, loss is 5.710981464385986 and perplexity is 302.167489425072
At time: 716.83456158638 and batch: 250, loss is 5.7180818176269534 and perplexity is 304.3206202869366
At time: 717.9987082481384 and batch: 300, loss is 5.709545698165893 and perplexity is 301.7339588496973
At time: 719.1623959541321 and batch: 350, loss is 5.73775273323059 and perplexity is 310.36615114936995
At time: 720.3264865875244 and batch: 400, loss is 5.713461866378784 and perplexity is 302.9179165637424
At time: 721.490932226181 and batch: 450, loss is 5.7003891563415525 and perplexity is 298.9837297450026
At time: 722.6538016796112 and batch: 500, loss is 5.6824980640411376 and perplexity is 293.6821511705779
At time: 723.8756597042084 and batch: 550, loss is 5.679880113601684 and perplexity is 292.9143113756686
At time: 725.0427839756012 and batch: 600, loss is 5.70385329246521 and perplexity is 300.0212460949947
At time: 726.2058417797089 and batch: 650, loss is 5.683628988265991 and perplexity is 294.01447130877176
At time: 727.3727207183838 and batch: 700, loss is 5.714793252944946 and perplexity is 303.321486002356
At time: 728.5395851135254 and batch: 750, loss is 5.684884223937988 and perplexity is 294.38376048522747
At time: 729.7047731876373 and batch: 800, loss is 5.687598705291748 and perplexity is 295.18394526590697
At time: 730.8698220252991 and batch: 850, loss is 5.703022050857544 and perplexity is 299.7719595750745
At time: 732.0325458049774 and batch: 900, loss is 5.687141284942627 and perplexity is 295.04895299906724
At time: 733.1946978569031 and batch: 950, loss is 5.657126502990723 and perplexity is 286.3247061467912
At time: 734.3575818538666 and batch: 1000, loss is 5.656590328216553 and perplexity is 286.17122721157733
At time: 735.5263154506683 and batch: 1050, loss is 5.657700023651123 and perplexity is 286.4889663801657
At time: 736.6890313625336 and batch: 1100, loss is 5.636296329498291 and perplexity is 280.42220136321225
At time: 737.8534762859344 and batch: 1150, loss is 5.673437328338623 and perplexity is 291.0331936926546
At time: 739.0155439376831 and batch: 1200, loss is 5.671116609573364 and perplexity is 290.3585706067447
At time: 740.1770436763763 and batch: 1250, loss is 5.673875904083252 and perplexity is 291.16086178630826
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.434617982293568 and perplexity of 229.20527107683418
Finished 24 epochs...
Completing Train Step...
At time: 743.1177456378937 and batch: 50, loss is 5.6863783264160155 and perplexity is 294.82392873761387
At time: 744.28080701828 and batch: 100, loss is 5.708242635726929 and perplexity is 301.34103671799346
At time: 745.4436483383179 and batch: 150, loss is 5.617187852859497 and perplexity is 275.11463156304507
At time: 746.6061482429504 and batch: 200, loss is 5.648266801834106 and perplexity is 283.7991591325426
At time: 747.7697434425354 and batch: 250, loss is 5.6700303268432615 and perplexity is 290.04333035697584
At time: 748.9338917732239 and batch: 300, loss is 5.6679713153839115 and perplexity is 289.4467422177414
At time: 750.0975000858307 and batch: 350, loss is 5.697652864456177 and perplexity is 298.1667412606746
At time: 751.2609775066376 and batch: 400, loss is 5.668446960449219 and perplexity is 289.58444887949753
At time: 752.4511723518372 and batch: 450, loss is 5.653338804244995 and perplexity is 285.242245727575
At time: 753.6125004291534 and batch: 500, loss is 5.650548992156982 and perplexity is 284.44758245843803
At time: 754.7752583026886 and batch: 550, loss is 5.650043058395386 and perplexity is 284.30370722180663
At time: 755.9375152587891 and batch: 600, loss is 5.673452520370484 and perplexity is 291.03761511179084
At time: 757.0994546413422 and batch: 650, loss is 5.658569860458374 and perplexity is 286.738273440504
At time: 758.2614104747772 and batch: 700, loss is 5.684290752410889 and perplexity is 294.20910393730946
At time: 759.4231770038605 and batch: 750, loss is 5.649732627868652 and perplexity is 284.21546436956777
At time: 760.586615562439 and batch: 800, loss is 5.663939161300659 and perplexity is 288.28199814606586
At time: 761.7497973442078 and batch: 850, loss is 5.679625062942505 and perplexity is 292.8396129138201
At time: 762.9131419658661 and batch: 900, loss is 5.669776563644409 and perplexity is 289.9697373716493
At time: 764.0762574672699 and batch: 950, loss is 5.642289152145386 and perplexity is 282.10776748710555
At time: 765.2402458190918 and batch: 1000, loss is 5.639644031524658 and perplexity is 281.36254445097404
At time: 766.4030575752258 and batch: 1050, loss is 5.640717315673828 and perplexity is 281.6646885243214
At time: 767.5665874481201 and batch: 1100, loss is 5.629459571838379 and perplexity is 278.51156146315554
At time: 768.7301840782166 and batch: 1150, loss is 5.673500299453735 and perplexity is 291.0515209544342
At time: 769.8964748382568 and batch: 1200, loss is 5.671928491592407 and perplexity is 290.59440323047465
At time: 771.0597088336945 and batch: 1250, loss is 5.668597564697266 and perplexity is 289.62806481195696
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.426857161695939 and perplexity of 227.43333482135483
Finished 25 epochs...
Completing Train Step...
At time: 773.9603435993195 and batch: 50, loss is 5.673976593017578 and perplexity is 291.19017993918106
At time: 775.1470818519592 and batch: 100, loss is 5.69546724319458 and perplexity is 297.5157733352797
At time: 776.3099496364594 and batch: 150, loss is 5.605860919952392 and perplexity is 272.0160086656191
At time: 777.4738922119141 and batch: 200, loss is 5.638682432174683 and perplexity is 281.09211645365394
At time: 778.6392524242401 and batch: 250, loss is 5.6586461353302 and perplexity is 286.76014519968066
At time: 779.8039262294769 and batch: 300, loss is 5.657571430206299 and perplexity is 286.4521281457033
At time: 780.9667453765869 and batch: 350, loss is 5.6873766136169435 and perplexity is 295.1183946485201
At time: 782.1776735782623 and batch: 400, loss is 5.658450908660889 and perplexity is 286.704167435996
At time: 783.3408210277557 and batch: 450, loss is 5.646331014633179 and perplexity is 283.2503157460502
At time: 784.5043482780457 and batch: 500, loss is 5.642988376617431 and perplexity is 282.30509312130323
At time: 785.6691315174103 and batch: 550, loss is 5.643548641204834 and perplexity is 282.4633029833756
At time: 786.8333628177643 and batch: 600, loss is 5.666332406997681 and perplexity is 288.9727540422466
At time: 787.9962584972382 and batch: 650, loss is 5.652814283370971 and perplexity is 285.09266944690756
At time: 789.159265756607 and batch: 700, loss is 5.679588785171509 and perplexity is 292.82898953810155
At time: 790.3223123550415 and batch: 750, loss is 5.646967601776123 and perplexity is 283.43068666010026
At time: 791.4864265918732 and batch: 800, loss is 5.6592996883392335 and perplexity is 286.94761941094106
At time: 792.650547504425 and batch: 850, loss is 5.673864965438843 and perplexity is 291.1576768985946
At time: 793.8154752254486 and batch: 900, loss is 5.663399181365967 and perplexity is 288.1263736723619
At time: 794.9836902618408 and batch: 950, loss is 5.637175788879395 and perplexity is 280.66892977677725
At time: 796.1501636505127 and batch: 1000, loss is 5.635574645996094 and perplexity is 280.2198982950125
At time: 797.3152189254761 and batch: 1050, loss is 5.6383837223052975 and perplexity is 281.00816400359975
At time: 798.4792325496674 and batch: 1100, loss is 5.626881237030029 and perplexity is 277.7943903606173
At time: 799.6427795886993 and batch: 1150, loss is 5.671727542877197 and perplexity is 290.53601452526243
At time: 800.8222227096558 and batch: 1200, loss is 5.6701274013519285 and perplexity is 290.0714875374125
At time: 802.0026807785034 and batch: 1250, loss is 5.6641389274597165 and perplexity is 288.33959288610936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.424089640596487 and perplexity of 226.80477844122524
Finished 26 epochs...
Completing Train Step...
At time: 804.9324004650116 and batch: 50, loss is 5.667534999847412 and perplexity is 289.3204796542847
At time: 806.0933482646942 and batch: 100, loss is 5.68700138092041 and perplexity is 295.0076773511737
At time: 807.2553923130035 and batch: 150, loss is 5.5950900840759275 and perplexity is 269.1018907955621
At time: 808.4161250591278 and batch: 200, loss is 5.628095598220825 and perplexity is 278.13193799818646
At time: 809.5782272815704 and batch: 250, loss is 5.647415657043457 and perplexity is 283.5577077262861
At time: 810.7406983375549 and batch: 300, loss is 5.648653345108032 and perplexity is 283.9088809934087
At time: 811.9297075271606 and batch: 350, loss is 5.679275541305542 and perplexity is 292.7372770182936
At time: 813.0934388637543 and batch: 400, loss is 5.651229648590088 and perplexity is 284.6412594415847
At time: 814.2567255496979 and batch: 450, loss is 5.640101022720337 and perplexity is 281.4911540410689
At time: 815.4193096160889 and batch: 500, loss is 5.636331968307495 and perplexity is 280.4321954546309
At time: 816.583359003067 and batch: 550, loss is 5.63818829536438 and perplexity is 280.95325280347424
At time: 817.7461135387421 and batch: 600, loss is 5.660709257125855 and perplexity is 287.35237701848564
At time: 818.9095375537872 and batch: 650, loss is 5.647713394165039 and perplexity is 283.6421459515603
At time: 820.0717487335205 and batch: 700, loss is 5.675946340560913 and perplexity is 291.76431634701623
At time: 821.2347693443298 and batch: 750, loss is 5.643068046569824 and perplexity is 282.3275852505939
At time: 822.3976275920868 and batch: 800, loss is 5.655816974639893 and perplexity is 285.95000122345647
At time: 823.5607097148895 and batch: 850, loss is 5.669606161117554 and perplexity is 289.9203300053792
At time: 824.7323710918427 and batch: 900, loss is 5.659421272277832 and perplexity is 286.9825097536903
At time: 825.8963761329651 and batch: 950, loss is 5.6353560829162594 and perplexity is 280.1586592635497
At time: 827.060436964035 and batch: 1000, loss is 5.633123788833618 and perplexity is 279.5339602616311
At time: 828.2250070571899 and batch: 1050, loss is 5.63410120010376 and perplexity is 279.8073134722705
At time: 829.3881683349609 and batch: 1100, loss is 5.623863143920898 and perplexity is 276.95724495216405
At time: 830.5514414310455 and batch: 1150, loss is 5.667482957839966 and perplexity is 289.3054232275151
At time: 831.7231504917145 and batch: 1200, loss is 5.668954277038575 and perplexity is 289.73139714592844
At time: 832.8930184841156 and batch: 1250, loss is 5.65899775505066 and perplexity is 286.8609934508525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.422735729356752 and perplexity of 226.49791268384269
Finished 27 epochs...
Completing Train Step...
At time: 835.853120803833 and batch: 50, loss is 5.660726118087768 and perplexity is 287.3572220968166
At time: 837.0198147296906 and batch: 100, loss is 5.680836877822876 and perplexity is 293.1946954179892
At time: 838.1860105991364 and batch: 150, loss is 5.590894060134888 and perplexity is 267.9750984961868
At time: 839.3501989841461 and batch: 200, loss is 5.62218864440918 and perplexity is 276.4938682510283
At time: 840.5626118183136 and batch: 250, loss is 5.642174224853516 and perplexity is 282.075347468381
At time: 841.727147102356 and batch: 300, loss is 5.63940092086792 and perplexity is 281.2941505319931
At time: 842.8918764591217 and batch: 350, loss is 5.671727705001831 and perplexity is 290.5360616283112
At time: 844.0560591220856 and batch: 400, loss is 5.647400131225586 and perplexity is 283.55330529513594
At time: 845.2191610336304 and batch: 450, loss is 5.633831624984741 and perplexity is 279.73189454842833
At time: 846.382331609726 and batch: 500, loss is 5.633354825973511 and perplexity is 279.5985504493915
At time: 847.5472161769867 and batch: 550, loss is 5.631922969818115 and perplexity is 279.19849202529974
At time: 848.7110877037048 and batch: 600, loss is 5.655840845108032 and perplexity is 285.9568270653177
At time: 849.875251531601 and batch: 650, loss is 5.639917669296264 and perplexity is 281.4395464054922
At time: 851.0390722751617 and batch: 700, loss is 5.66854681968689 and perplexity is 289.61336800570086
At time: 852.2021489143372 and batch: 750, loss is 5.635055952072143 and perplexity is 280.0745876255354
At time: 853.3678288459778 and batch: 800, loss is 5.646931343078613 and perplexity is 283.4204100188775
At time: 854.5340483188629 and batch: 850, loss is 5.665470485687256 and perplexity is 288.72378957670776
At time: 855.703047990799 and batch: 900, loss is 5.654519281387329 and perplexity is 285.579166503196
At time: 856.8676755428314 and batch: 950, loss is 5.630014696121216 and perplexity is 278.66621291539127
At time: 858.0398547649384 and batch: 1000, loss is 5.628830175399781 and perplexity is 278.3363224314583
At time: 859.2059996128082 and batch: 1050, loss is 5.631163244247436 and perplexity is 278.9864583455159
At time: 860.3713779449463 and batch: 1100, loss is 5.619196910858154 and perplexity is 275.66790841067245
At time: 861.5362930297852 and batch: 1150, loss is 5.662935104370117 and perplexity is 287.99269187216635
At time: 862.7008445262909 and batch: 1200, loss is 5.664296131134034 and perplexity is 288.38492449261634
At time: 863.8664236068726 and batch: 1250, loss is 5.657071952819824 and perplexity is 286.30908751114185
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.421607692746351 and perplexity of 226.24255879749947
Finished 28 epochs...
Completing Train Step...
At time: 866.7954947948456 and batch: 50, loss is 5.656982250213623 and perplexity is 286.28340599168
At time: 867.9823994636536 and batch: 100, loss is 5.6769623374938964 and perplexity is 292.06089863541416
At time: 869.1456398963928 and batch: 150, loss is 5.584623737335205 and perplexity is 266.30006511443725
At time: 870.3363244533539 and batch: 200, loss is 5.615614452362061 and perplexity is 274.68210642175745
At time: 871.4999794960022 and batch: 250, loss is 5.634833135604858 and perplexity is 280.01218934728456
At time: 872.6687695980072 and batch: 300, loss is 5.6329081344604495 and perplexity is 279.4736840403004
At time: 873.8336448669434 and batch: 350, loss is 5.667962923049926 and perplexity is 289.44431309420287
At time: 874.9981484413147 and batch: 400, loss is 5.639305238723755 and perplexity is 281.26723699212226
At time: 876.1618549823761 and batch: 450, loss is 5.624853410720825 and perplexity is 277.23164235773623
At time: 877.3255534172058 and batch: 500, loss is 5.6255827808380126 and perplexity is 277.43392059209316
At time: 878.4892990589142 and batch: 550, loss is 5.626611442565918 and perplexity is 277.7194530812228
At time: 879.6519448757172 and batch: 600, loss is 5.649560737609863 and perplexity is 284.1666146983569
At time: 880.8158612251282 and batch: 650, loss is 5.632819652557373 and perplexity is 279.4489567708508
At time: 881.9791240692139 and batch: 700, loss is 5.662650766372681 and perplexity is 287.9108162476104
At time: 883.1409602165222 and batch: 750, loss is 5.6335952186584475 and perplexity is 279.6657719750963
At time: 884.3049631118774 and batch: 800, loss is 5.644380168914795 and perplexity is 282.6982767269036
At time: 885.4686706066132 and batch: 850, loss is 5.658881092071534 and perplexity is 286.82752934481056
At time: 886.6312770843506 and batch: 900, loss is 5.647734441757202 and perplexity is 283.6481159985959
At time: 887.7953617572784 and batch: 950, loss is 5.620159540176392 and perplexity is 275.93340218695056
At time: 888.959433555603 and batch: 1000, loss is 5.619478187561035 and perplexity is 275.74545827701445
At time: 890.12779545784 and batch: 1050, loss is 5.621699094772339 and perplexity is 276.35854390493216
At time: 891.2928056716919 and batch: 1100, loss is 5.609548940658569 and perplexity is 273.0210615266856
At time: 892.4556992053986 and batch: 1150, loss is 5.651741933822632 and perplexity is 284.78711431182757
At time: 893.617728471756 and batch: 1200, loss is 5.655313758850098 and perplexity is 285.80614286668293
At time: 894.780962228775 and batch: 1250, loss is 5.645846099853515 and perplexity is 283.1129967782898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.412851152628877 and perplexity of 224.27010529269947
Finished 29 epochs...
Completing Train Step...
At time: 897.7359707355499 and batch: 50, loss is 5.639451398849487 and perplexity is 281.3083500513172
At time: 898.9006743431091 and batch: 100, loss is 5.661219997406006 and perplexity is 287.4991769371508
At time: 900.091979265213 and batch: 150, loss is 5.56987753868103 and perplexity is 262.4019631820956
At time: 901.2537219524384 and batch: 200, loss is 5.601704034805298 and perplexity is 270.8876162830485
At time: 902.4173450469971 and batch: 250, loss is 5.6210720825195315 and perplexity is 276.18531802478566
At time: 903.5810770988464 and batch: 300, loss is 5.618644418716431 and perplexity is 275.5156461232822
At time: 904.744535446167 and batch: 350, loss is 5.651870260238647 and perplexity is 284.8236623665248
At time: 905.9078741073608 and batch: 400, loss is 5.627322521209717 and perplexity is 277.91700368195905
At time: 907.070104598999 and batch: 450, loss is 5.611790962219239 and perplexity is 273.63386733836927
At time: 908.2335870265961 and batch: 500, loss is 5.613834838867188 and perplexity is 274.19371314290026
At time: 909.3972067832947 and batch: 550, loss is 5.610419597625732 and perplexity is 273.2588727270528
At time: 910.5603814125061 and batch: 600, loss is 5.634521265029907 and perplexity is 279.9248754008324
At time: 911.7235372066498 and batch: 650, loss is 5.612093734741211 and perplexity is 273.7167286979187
At time: 912.8886358737946 and batch: 700, loss is 5.642578496932983 and perplexity is 282.1894057093961
At time: 914.0508332252502 and batch: 750, loss is 5.620469837188721 and perplexity is 276.01903678264347
At time: 915.2257905006409 and batch: 800, loss is 5.628021402359009 and perplexity is 278.1113025248909
At time: 916.3901083469391 and batch: 850, loss is 5.643498249053955 and perplexity is 282.44906940862717
At time: 917.5585043430328 and batch: 900, loss is 5.632489528656006 and perplexity is 279.35671921675686
At time: 918.7217311859131 and batch: 950, loss is 5.609013032913208 and perplexity is 272.8747866235944
At time: 919.8871514797211 and batch: 1000, loss is 5.6029869556427006 and perplexity is 271.2353666709853
At time: 921.0526740550995 and batch: 1050, loss is 5.605799818038941 and perplexity is 271.99938847476807
At time: 922.223132610321 and batch: 1100, loss is 5.5928268718719485 and perplexity is 268.49354477993205
At time: 923.3897018432617 and batch: 1150, loss is 5.631178255081177 and perplexity is 278.9906461962895
At time: 924.555341720581 and batch: 1200, loss is 5.638790054321289 and perplexity is 281.12236981855756
At time: 925.7203118801117 and batch: 1250, loss is 5.628486185073853 and perplexity is 278.2405938949767
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.401080723226506 and perplexity of 221.64582460448406
Finished 30 epochs...
Completing Train Step...
At time: 928.6538500785828 and batch: 50, loss is 5.625230140686035 and perplexity is 277.33610350030267
At time: 929.841810464859 and batch: 100, loss is 5.636947231292725 and perplexity is 280.60478809384756
At time: 931.0119667053223 and batch: 150, loss is 5.5371118259429934 and perplexity is 253.9435062503102
At time: 932.1767551898956 and batch: 200, loss is 5.569960775375367 and perplexity is 262.42380556312935
At time: 933.3413372039795 and batch: 250, loss is 5.591015214920044 and perplexity is 268.0075669284856
At time: 934.5061497688293 and batch: 300, loss is 5.588745994567871 and perplexity is 267.4000882152821
At time: 935.6707005500793 and batch: 350, loss is 5.620261754989624 and perplexity is 275.9616081096266
At time: 936.8355605602264 and batch: 400, loss is 5.600847520828247 and perplexity is 270.6556965889002
At time: 938.0006790161133 and batch: 450, loss is 5.573132953643799 and perplexity is 263.2575824014682
At time: 939.1663024425507 and batch: 500, loss is 5.577018051147461 and perplexity is 264.28235315583714
At time: 940.3327434062958 and batch: 550, loss is 5.576666669845581 and perplexity is 264.1895055919256
At time: 941.4966735839844 and batch: 600, loss is 5.59674075126648 and perplexity is 269.54645527066833
At time: 942.6653954982758 and batch: 650, loss is 5.580950222015381 and perplexity is 265.32360236908664
At time: 943.8290758132935 and batch: 700, loss is 5.606978120803833 and perplexity is 272.3200750020645
At time: 944.9933824539185 and batch: 750, loss is 5.588862781524658 and perplexity is 267.4313188814611
At time: 946.1767086982727 and batch: 800, loss is 5.600495233535766 and perplexity is 270.5603648194258
At time: 947.3413271903992 and batch: 850, loss is 5.614469347000122 and perplexity is 274.36774649083725
At time: 948.505309343338 and batch: 900, loss is 5.594448518753052 and perplexity is 268.9292997242506
At time: 949.6696696281433 and batch: 950, loss is 5.57448257446289 and perplexity is 263.6131201820893
At time: 950.8340678215027 and batch: 1000, loss is 5.573264961242676 and perplexity is 263.29233669667224
At time: 951.9986383914948 and batch: 1050, loss is 5.562578868865967 and perplexity is 260.4937500736735
At time: 953.1647498607635 and batch: 1100, loss is 5.550613317489624 and perplexity is 257.3953725887189
At time: 954.330774307251 and batch: 1150, loss is 5.588300371170044 and perplexity is 267.2809550256323
At time: 955.4949338436127 and batch: 1200, loss is 5.590842180252075 and perplexity is 267.96119634010427
At time: 956.6596353054047 and batch: 1250, loss is 5.588058757781982 and perplexity is 267.2163841694299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.357734178974681 and perplexity of 212.24349544015996
Finished 31 epochs...
Completing Train Step...
At time: 959.6143350601196 and batch: 50, loss is 5.589534912109375 and perplexity is 267.6111280710512
At time: 960.7790803909302 and batch: 100, loss is 5.590948905944824 and perplexity is 267.9897962105569
At time: 961.9443669319153 and batch: 150, loss is 5.500697927474976 and perplexity is 244.86276909557705
At time: 963.1066596508026 and batch: 200, loss is 5.536834468841553 and perplexity is 253.8730829821352
At time: 964.2707762718201 and batch: 250, loss is 5.555955381393432 and perplexity is 258.77407439546977
At time: 965.4345026016235 and batch: 300, loss is 5.559393320083618 and perplexity is 259.66525483193254
At time: 966.5985975265503 and batch: 350, loss is 5.590768013000488 and perplexity is 267.94132313161964
At time: 967.7636530399323 and batch: 400, loss is 5.566729354858398 and perplexity is 261.5771725440631
At time: 968.9279608726501 and batch: 450, loss is 5.538839540481567 and perplexity is 254.38262736677257
At time: 970.0916588306427 and batch: 500, loss is 5.54658613204956 and perplexity is 256.36087814051757
At time: 971.2572042942047 and batch: 550, loss is 5.53384635925293 and perplexity is 253.11561465138092
At time: 972.4303228855133 and batch: 600, loss is 5.554626512527466 and perplexity is 258.4304259671054
At time: 973.59477186203 and batch: 650, loss is 5.543186445236206 and perplexity is 255.49081125910487
At time: 974.7622992992401 and batch: 700, loss is 5.57069169998169 and perplexity is 262.6156876970442
At time: 975.928656578064 and batch: 750, loss is 5.557780838012695 and perplexity is 259.2468866603121
At time: 977.0991597175598 and batch: 800, loss is 5.5678894996643065 and perplexity is 261.8808160432914
At time: 978.2641558647156 and batch: 850, loss is 5.580277452468872 and perplexity is 265.14516076121294
At time: 979.4291615486145 and batch: 900, loss is 5.559666566848755 and perplexity is 259.7362172175133
At time: 980.5938229560852 and batch: 950, loss is 5.540144166946411 and perplexity is 254.71471825480944
At time: 981.7598431110382 and batch: 1000, loss is 5.542981929779053 and perplexity is 255.43856478183014
At time: 982.9276683330536 and batch: 1050, loss is 5.527605209350586 and perplexity is 251.54080159173802
At time: 984.0957458019257 and batch: 1100, loss is 5.515151824951172 and perplexity is 248.4276919065825
At time: 985.2597424983978 and batch: 1150, loss is 5.554035816192627 and perplexity is 258.27781713885105
At time: 986.4241199493408 and batch: 1200, loss is 5.558343849182129 and perplexity is 259.3928866490984
At time: 987.5889539718628 and batch: 1250, loss is 5.5577754783630375 and perplexity is 259.24549719154834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.331562125769845 and perplexity of 206.7607082456023
Finished 32 epochs...
Completing Train Step...
At time: 990.5275864601135 and batch: 50, loss is 5.560060987472534 and perplexity is 259.83868274425646
At time: 991.7004618644714 and batch: 100, loss is 5.5608636665344235 and perplexity is 260.04733354297525
At time: 992.864217042923 and batch: 150, loss is 5.470008039474488 and perplexity is 237.46210182400225
At time: 994.0279262065887 and batch: 200, loss is 5.507995090484619 and perplexity is 246.65610782144472
At time: 995.1936795711517 and batch: 250, loss is 5.530557374954224 and perplexity is 252.28448889803113
At time: 996.3588171005249 and batch: 300, loss is 5.5354299736022945 and perplexity is 253.51676972442016
At time: 997.523434638977 and batch: 350, loss is 5.563111486434937 and perplexity is 260.6325305767559
At time: 998.6881308555603 and batch: 400, loss is 5.542585344314575 and perplexity is 255.33728164500795
At time: 999.8523316383362 and batch: 450, loss is 5.514595222473145 and perplexity is 248.28945491275235
At time: 1001.0165343284607 and batch: 500, loss is 5.524133520126343 and perplexity is 250.6690442123249
At time: 1002.1896691322327 and batch: 550, loss is 5.504458656311035 and perplexity is 245.78536530190829
At time: 1003.3537418842316 and batch: 600, loss is 5.5324785518646244 and perplexity is 252.76963791227578
At time: 1004.5178070068359 and batch: 650, loss is 5.5167875003814695 and perplexity is 248.83437148566708
At time: 1005.6821982860565 and batch: 700, loss is 5.543953943252563 and perplexity is 255.6869752180393
At time: 1006.8465769290924 and batch: 750, loss is 5.538312120437622 and perplexity is 254.24849624509818
At time: 1008.0103452205658 and batch: 800, loss is 5.548909435272217 and perplexity is 256.95717461540437
At time: 1009.1755030155182 and batch: 850, loss is 5.566376895904541 and perplexity is 261.4849935730813
At time: 1010.3413631916046 and batch: 900, loss is 5.547534637451172 and perplexity is 256.6041531737985
At time: 1011.5056345462799 and batch: 950, loss is 5.523518476486206 and perplexity is 250.51491921256192
At time: 1012.6704437732697 and batch: 1000, loss is 5.523679361343384 and perplexity is 250.55522651189048
At time: 1013.8347265720367 and batch: 1050, loss is 5.518513603210449 and perplexity is 249.26425610402953
At time: 1014.9990718364716 and batch: 1100, loss is 5.502460346221924 and perplexity is 245.29470034031988
At time: 1016.164053440094 and batch: 1150, loss is 5.5382865524291995 and perplexity is 254.24199570050814
At time: 1017.3551268577576 and batch: 1200, loss is 5.547076263427734 and perplexity is 256.4865594486909
At time: 1018.521889925003 and batch: 1250, loss is 5.547702140808106 and perplexity is 256.6471388308788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.320319182681342 and perplexity of 204.44912820062137
Finished 33 epochs...
Completing Train Step...
At time: 1021.4085447788239 and batch: 50, loss is 5.542822732925415 and perplexity is 255.39790300271852
At time: 1022.5923964977264 and batch: 100, loss is 5.5474398326873775 and perplexity is 256.57982703080364
At time: 1023.7532041072845 and batch: 150, loss is 5.456104278564453 and perplexity is 234.18333197043637
At time: 1024.926862001419 and batch: 200, loss is 5.497151966094971 and perplexity is 243.99603278788356
At time: 1026.093489408493 and batch: 250, loss is 5.512134532928467 and perplexity is 247.679242726271
At time: 1027.2545657157898 and batch: 300, loss is 5.5135881710052494 and perplexity is 248.03954051217247
At time: 1028.4157876968384 and batch: 350, loss is 5.547920227050781 and perplexity is 256.703116144799
At time: 1029.5797500610352 and batch: 400, loss is 5.52834379196167 and perplexity is 251.72665387895697
At time: 1030.7499780654907 and batch: 450, loss is 5.496160211563111 and perplexity is 243.75416857140104
At time: 1031.9121344089508 and batch: 500, loss is 5.510719614028931 and perplexity is 247.3290444941879
At time: 1033.0749139785767 and batch: 550, loss is 5.488822116851806 and perplexity is 241.97202416538357
At time: 1034.2383618354797 and batch: 600, loss is 5.517479963302613 and perplexity is 249.00673973384957
At time: 1035.403627872467 and batch: 650, loss is 5.500931091308594 and perplexity is 244.91986889407505
At time: 1036.567735671997 and batch: 700, loss is 5.529070262908935 and perplexity is 251.90959242137484
At time: 1037.7326486110687 and batch: 750, loss is 5.526086845397949 and perplexity is 251.15916091402815
At time: 1038.8969571590424 and batch: 800, loss is 5.542900514602661 and perplexity is 255.41776905257666
At time: 1040.0606262683868 and batch: 850, loss is 5.559309778213501 and perplexity is 259.64356281704767
At time: 1041.2255191802979 and batch: 900, loss is 5.537120590209961 and perplexity is 253.94573188874662
At time: 1042.389014005661 and batch: 950, loss is 5.515432577133179 and perplexity is 248.49744831483008
At time: 1043.5548815727234 and batch: 1000, loss is 5.5159592628479 and perplexity is 248.62836284338192
At time: 1044.7201364040375 and batch: 1050, loss is 5.508192920684815 and perplexity is 246.70490867561676
At time: 1045.884689092636 and batch: 1100, loss is 5.4951870822906494 and perplexity is 243.51707963248293
At time: 1047.101561307907 and batch: 1150, loss is 5.5263713836669925 and perplexity is 251.2306354750707
At time: 1048.2674806118011 and batch: 1200, loss is 5.5329043674469 and perplexity is 252.87729408203558
At time: 1049.4328334331512 and batch: 1250, loss is 5.537863779067993 and perplexity is 254.13453167549017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.314586333114735 and perplexity of 203.28040536162675
Finished 34 epochs...
Completing Train Step...
At time: 1052.3773024082184 and batch: 50, loss is 5.533213510513305 and perplexity is 252.95548142906782
At time: 1053.5433621406555 and batch: 100, loss is 5.533995018005371 and perplexity is 253.15324529987174
At time: 1054.7089204788208 and batch: 150, loss is 5.447191228866577 and perplexity is 232.1053187684949
At time: 1055.8723406791687 and batch: 200, loss is 5.488733291625977 and perplexity is 241.95053190023438
At time: 1057.0416140556335 and batch: 250, loss is 5.506594524383545 and perplexity is 246.31089144381616
At time: 1058.2089321613312 and batch: 300, loss is 5.5041208171844485 and perplexity is 245.7023434135776
At time: 1059.374673128128 and batch: 350, loss is 5.53695894241333 and perplexity is 253.90468543834652
At time: 1060.5412845611572 and batch: 400, loss is 5.5165814018249515 and perplexity is 248.7830923653497
At time: 1061.7146229743958 and batch: 450, loss is 5.489321165084839 and perplexity is 242.09281001296165
At time: 1062.8782970905304 and batch: 500, loss is 5.503008069992066 and perplexity is 245.42909087947265
At time: 1064.0440909862518 and batch: 550, loss is 5.481400785446167 and perplexity is 240.18291657475777
At time: 1065.208387851715 and batch: 600, loss is 5.511863803863525 and perplexity is 247.61219783239235
At time: 1066.3726444244385 and batch: 650, loss is 5.492477531433106 and perplexity is 242.8581508242331
At time: 1067.537371635437 and batch: 700, loss is 5.522834758758545 and perplexity is 250.34369626200626
At time: 1068.702110528946 and batch: 750, loss is 5.52002480506897 and perplexity is 249.6412294808084
At time: 1069.867182970047 and batch: 800, loss is 5.531151752471924 and perplexity is 252.43448569924382
At time: 1071.032955646515 and batch: 850, loss is 5.548464403152466 and perplexity is 256.84284586117104
At time: 1072.1982021331787 and batch: 900, loss is 5.529297704696655 and perplexity is 251.9668937055252
At time: 1073.363436460495 and batch: 950, loss is 5.507953834533692 and perplexity is 246.64593199907253
At time: 1074.5279774665833 and batch: 1000, loss is 5.504384613037109 and perplexity is 245.76716722250916
At time: 1075.692358493805 and batch: 1050, loss is 5.502665023803711 and perplexity is 245.344911804816
At time: 1076.9052081108093 and batch: 1100, loss is 5.485983543395996 and perplexity is 241.28614272350706
At time: 1078.070394039154 and batch: 1150, loss is 5.51882851600647 and perplexity is 249.34276496894148
At time: 1079.234941959381 and batch: 1200, loss is 5.529035358428955 and perplexity is 251.90079980150117
At time: 1080.400182723999 and batch: 1250, loss is 5.535036277770996 and perplexity is 253.41698087353168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.307302655964873 and perplexity of 201.80515566141273
Finished 35 epochs...
Completing Train Step...
At time: 1083.2927913665771 and batch: 50, loss is 5.5243165397644045 and perplexity is 250.71492576855474
At time: 1084.501696586609 and batch: 100, loss is 5.528563919067383 and perplexity is 251.78207183797988
At time: 1085.6641159057617 and batch: 150, loss is 5.439930276870728 and perplexity is 230.42611687060784
At time: 1086.826462984085 and batch: 200, loss is 5.478562669754028 and perplexity is 239.50221608053508
At time: 1087.9898176193237 and batch: 250, loss is 5.498443078994751 and perplexity is 244.31126266820368
At time: 1089.1522014141083 and batch: 300, loss is 5.497817325592041 and perplexity is 244.15843188644644
At time: 1090.3230006694794 and batch: 350, loss is 5.533370027542114 and perplexity is 252.99507636800226
At time: 1091.4866342544556 and batch: 400, loss is 5.509285326004028 and perplexity is 246.97455768632145
At time: 1092.648151397705 and batch: 450, loss is 5.482698421478272 and perplexity is 240.49478888621326
At time: 1093.8105313777924 and batch: 500, loss is 5.498838214874268 and perplexity is 244.40781788881364
At time: 1094.9761445522308 and batch: 550, loss is 5.471200170516968 and perplexity is 237.74535657184197
At time: 1096.137351512909 and batch: 600, loss is 5.504158210754395 and perplexity is 245.7115312731244
At time: 1097.3085322380066 and batch: 650, loss is 5.491005058288574 and perplexity is 242.50081186978537
At time: 1098.4787666797638 and batch: 700, loss is 5.519645404815674 and perplexity is 249.54653350008573
At time: 1099.6413311958313 and batch: 750, loss is 5.5101722240447994 and perplexity is 247.1936961000043
At time: 1100.8047318458557 and batch: 800, loss is 5.522768163681031 and perplexity is 250.3270251592612
At time: 1101.9679255485535 and batch: 850, loss is 5.541906290054321 and perplexity is 255.16395263266773
At time: 1103.1310966014862 and batch: 900, loss is 5.5215901756286625 and perplexity is 250.03231653014026
At time: 1104.2941167354584 and batch: 950, loss is 5.497362194061279 and perplexity is 244.04733296982133
At time: 1105.4570395946503 and batch: 1000, loss is 5.501356639862061 and perplexity is 245.02411636960616
At time: 1106.6659653186798 and batch: 1050, loss is 5.497547149658203 and perplexity is 244.09247506448136
At time: 1107.8291215896606 and batch: 1100, loss is 5.478378839492798 and perplexity is 239.45819237215665
At time: 1108.9928963184357 and batch: 1150, loss is 5.512962465286255 and perplexity is 247.88438929770018
At time: 1110.155812740326 and batch: 1200, loss is 5.515976266860962 and perplexity is 248.63259055925516
At time: 1111.318835735321 and batch: 1250, loss is 5.519978818893432 and perplexity is 249.62974969936568
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3033812585538325 and perplexity of 201.0153470353155
Finished 36 epochs...
Completing Train Step...
At time: 1114.2435505390167 and batch: 50, loss is 5.51766282081604 and perplexity is 249.05227665036065
At time: 1115.4093143939972 and batch: 100, loss is 5.520507946014404 and perplexity is 249.76187052140554
At time: 1116.5729825496674 and batch: 150, loss is 5.42979658126831 and perplexity is 228.10284031385825
At time: 1117.736787557602 and batch: 200, loss is 5.472066898345947 and perplexity is 237.95150641359035
At time: 1118.900844335556 and batch: 250, loss is 5.4905908203125 and perplexity is 242.40037962714058
At time: 1120.0646574497223 and batch: 300, loss is 5.4891939449310305 and perplexity is 242.06201288748503
At time: 1121.229202747345 and batch: 350, loss is 5.523783168792725 and perplexity is 250.5812373609102
At time: 1122.3946878910065 and batch: 400, loss is 5.502033128738403 and perplexity is 245.1899285374859
At time: 1123.558474779129 and batch: 450, loss is 5.475456733703613 and perplexity is 238.7594915380008
At time: 1124.7307767868042 and batch: 500, loss is 5.4915227127075195 and perplexity is 242.62637598325273
At time: 1125.895810842514 and batch: 550, loss is 5.463799362182617 and perplexity is 235.9923436132518
At time: 1127.060064315796 and batch: 600, loss is 5.493442077636718 and perplexity is 243.09251173942275
At time: 1128.2251381874084 and batch: 650, loss is 5.4790386295318605 and perplexity is 239.61623663453733
At time: 1129.3894345760345 and batch: 700, loss is 5.511536951065064 and perplexity is 247.53127831770297
At time: 1130.5618422031403 and batch: 750, loss is 5.506062688827515 and perplexity is 246.17992938213615
At time: 1131.7249419689178 and batch: 800, loss is 5.521102180480957 and perplexity is 249.91033173931785
At time: 1132.888528585434 and batch: 850, loss is 5.5369734191894535 and perplexity is 253.90836118624077
At time: 1134.0606777668 and batch: 900, loss is 5.517051467895508 and perplexity is 248.90006434612448
At time: 1135.2708671092987 and batch: 950, loss is 5.494811725616455 and perplexity is 243.42569102409766
At time: 1136.4353206157684 and batch: 1000, loss is 5.498464221954346 and perplexity is 244.31642818596578
At time: 1137.603768825531 and batch: 1050, loss is 5.496671571731567 and perplexity is 243.87884661908052
At time: 1138.7672591209412 and batch: 1100, loss is 5.476563844680786 and perplexity is 239.023971169124
At time: 1139.931146621704 and batch: 1150, loss is 5.507251129150391 and perplexity is 246.47267345687226
At time: 1141.0953795909882 and batch: 1200, loss is 5.512817735671997 and perplexity is 247.84851568170657
At time: 1142.2594921588898 and batch: 1250, loss is 5.512009935379028 and perplexity is 247.64838442205493
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.296590317774863 and perplexity of 199.65488834347715
Finished 37 epochs...
Completing Train Step...
At time: 1145.157806634903 and batch: 50, loss is 5.507787857055664 and perplexity is 246.60499772649123
At time: 1146.3482601642609 and batch: 100, loss is 5.511535987854004 and perplexity is 247.53103989295298
At time: 1147.5120704174042 and batch: 150, loss is 5.424769811630249 and perplexity is 226.95909695738254
At time: 1148.6741044521332 and batch: 200, loss is 5.467106847763062 and perplexity is 236.77417712564596
At time: 1149.8382103443146 and batch: 250, loss is 5.481400651931763 and perplexity is 240.18288450688095
At time: 1151.0115957260132 and batch: 300, loss is 5.482180166244507 and perplexity is 240.37018349466683
At time: 1152.186104774475 and batch: 350, loss is 5.517895402908326 and perplexity is 249.11020848664526
At time: 1153.3495452404022 and batch: 400, loss is 5.496484489440918 and perplexity is 243.8332254734014
At time: 1154.511636018753 and batch: 450, loss is 5.467429237365723 and perplexity is 236.85052296442095
At time: 1155.6783969402313 and batch: 500, loss is 5.482954864501953 and perplexity is 240.55647000556314
At time: 1156.8470442295074 and batch: 550, loss is 5.463295392990112 and perplexity is 235.8734407066212
At time: 1158.0106978416443 and batch: 600, loss is 5.492865400314331 and perplexity is 242.95236621391754
At time: 1159.174247264862 and batch: 650, loss is 5.474175338745117 and perplexity is 238.4537422638762
At time: 1160.3413670063019 and batch: 700, loss is 5.5092740154266355 and perplexity is 246.97176427727024
At time: 1161.5030989646912 and batch: 750, loss is 5.502400569915771 and perplexity is 245.28003796745023
At time: 1162.6685461997986 and batch: 800, loss is 5.522241373062133 and perplexity is 250.1951899585731
At time: 1163.837346315384 and batch: 850, loss is 5.531128158569336 and perplexity is 252.42852985483944
At time: 1165.0478246212006 and batch: 900, loss is 5.515273885726929 and perplexity is 248.45801703409282
At time: 1166.2141389846802 and batch: 950, loss is 5.488668766021728 and perplexity is 241.93492039964153
At time: 1167.3880145549774 and batch: 1000, loss is 5.496054182052612 and perplexity is 243.72832480635049
At time: 1168.5536236763 and batch: 1050, loss is 5.495376996994018 and perplexity is 243.56333149824226
At time: 1169.7163166999817 and batch: 1100, loss is 5.476822719573975 and perplexity is 239.08585648406148
At time: 1170.8797061443329 and batch: 1150, loss is 5.511435012817383 and perplexity is 247.50604669900054
At time: 1172.0448668003082 and batch: 1200, loss is 5.514756269454956 and perplexity is 248.32944440008836
At time: 1173.2082359790802 and batch: 1250, loss is 5.509412250518799 and perplexity is 247.00590680165996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.294121289775319 and perplexity of 199.16254289138004
Finished 38 epochs...
Completing Train Step...
At time: 1176.1443972587585 and batch: 50, loss is 5.495574607849121 and perplexity is 243.61146701234463
At time: 1177.3357074260712 and batch: 100, loss is 5.503986721038818 and perplexity is 245.6693978853371
At time: 1178.499207019806 and batch: 150, loss is 5.418401470184326 and perplexity is 225.51833643044816
At time: 1179.6625936031342 and batch: 200, loss is 5.457696485519409 and perplexity is 234.55649729971574
At time: 1180.8262557983398 and batch: 250, loss is 5.4742798995971675 and perplexity is 238.47867649389138
At time: 1181.9897599220276 and batch: 300, loss is 5.473253507614135 and perplexity is 238.23402946556575
At time: 1183.1546342372894 and batch: 350, loss is 5.508067283630371 and perplexity is 246.67391534457016
At time: 1184.3189811706543 and batch: 400, loss is 5.4868162155151365 and perplexity is 241.4871386375215
At time: 1185.4822199344635 and batch: 450, loss is 5.4567317771911625 and perplexity is 234.33032780466473
At time: 1186.6451303958893 and batch: 500, loss is 5.472954597473144 and perplexity is 238.1628295399494
At time: 1187.8091232776642 and batch: 550, loss is 5.448346147537231 and perplexity is 232.3735363896547
At time: 1188.9724855422974 and batch: 600, loss is 5.478561925888061 and perplexity is 239.50203792305376
At time: 1190.1363816261292 and batch: 650, loss is 5.461414756774903 and perplexity is 235.43026542817972
At time: 1191.3088719844818 and batch: 700, loss is 5.493213262557983 and perplexity is 243.03689487044096
At time: 1192.4768750667572 and batch: 750, loss is 5.486097183227539 and perplexity is 241.31356399816485
At time: 1193.6398782730103 and batch: 800, loss is 5.508683824539185 and perplexity is 246.82604679733035
At time: 1194.8503618240356 and batch: 850, loss is 5.524276819229126 and perplexity is 250.70496743527733
At time: 1196.0135443210602 and batch: 900, loss is 5.504549751281738 and perplexity is 245.80775613239018
At time: 1197.176210641861 and batch: 950, loss is 5.479527454376221 and perplexity is 239.73339563689913
At time: 1198.3398036956787 and batch: 1000, loss is 5.488566102981568 and perplexity is 241.91008390010938
At time: 1199.5033252239227 and batch: 1050, loss is 5.485034093856812 and perplexity is 241.05716242630686
At time: 1200.6667020320892 and batch: 1100, loss is 5.460076112747192 and perplexity is 235.1153189570594
At time: 1201.8300914764404 and batch: 1150, loss is 5.4936115932464595 and perplexity is 243.13372320766868
At time: 1202.9934105873108 and batch: 1200, loss is 5.494098329544068 and perplexity is 243.2520940212753
At time: 1204.1574850082397 and batch: 1250, loss is 5.48845965385437 and perplexity is 241.88433415336132
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.277223016223768 and perplexity of 195.82531586019627
Finished 39 epochs...
Completing Train Step...
At time: 1207.0979270935059 and batch: 50, loss is 5.478907871246338 and perplexity is 239.5849068746091
At time: 1208.2620136737823 and batch: 100, loss is 5.491883325576782 and perplexity is 242.71388595451742
At time: 1209.4265365600586 and batch: 150, loss is 5.405335350036621 and perplexity is 222.59085381995038
At time: 1210.5992257595062 and batch: 200, loss is 5.44302698135376 and perplexity is 231.14078444575733
At time: 1211.766235589981 and batch: 250, loss is 5.466187801361084 and perplexity is 236.556670634666
At time: 1212.9313340187073 and batch: 300, loss is 5.459718074798584 and perplexity is 235.03115381862952
At time: 1214.0965602397919 and batch: 350, loss is 5.496921691894531 and perplexity is 243.93985326511853
At time: 1215.2620470523834 and batch: 400, loss is 5.473102293014526 and perplexity is 238.198007725762
At time: 1216.4267582893372 and batch: 450, loss is 5.440176124572754 and perplexity is 230.48277356610117
At time: 1217.5904676914215 and batch: 500, loss is 5.454335775375366 and perplexity is 233.76954400153397
At time: 1218.7552936077118 and batch: 550, loss is 5.432549619674683 and perplexity is 228.73168140851044
At time: 1219.9205932617188 and batch: 600, loss is 5.460450315475464 and perplexity is 235.20331621425174
At time: 1221.0952689647675 and batch: 650, loss is 5.449872121810913 and perplexity is 232.72840311795704
At time: 1222.271314382553 and batch: 700, loss is 5.4843848514556885 and perplexity is 240.90070868906088
At time: 1223.4358959197998 and batch: 750, loss is 5.475075883865356 and perplexity is 238.66857733769447
At time: 1224.6471374034882 and batch: 800, loss is 5.495339603424072 and perplexity is 243.55422396605215
At time: 1225.811858177185 and batch: 850, loss is 5.5188585376739505 and perplexity is 249.35025076688765
At time: 1226.9773752689362 and batch: 900, loss is 5.495492792129516 and perplexity is 243.59153658018954
At time: 1228.1421597003937 and batch: 950, loss is 5.47272557258606 and perplexity is 238.10829057043537
At time: 1229.3069508075714 and batch: 1000, loss is 5.48180326461792 and perplexity is 240.2796046522535
At time: 1230.4710717201233 and batch: 1050, loss is 5.478747730255127 and perplexity is 239.54654258207293
At time: 1231.6365332603455 and batch: 1100, loss is 5.451784467697143 and perplexity is 233.17388614532246
At time: 1232.8009634017944 and batch: 1150, loss is 5.483639020919799 and perplexity is 240.72110456984709
At time: 1233.965721130371 and batch: 1200, loss is 5.485273284912109 and perplexity is 241.11482803964913
At time: 1235.130738735199 and batch: 1250, loss is 5.476243619918823 and perplexity is 238.94744202876973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.277582544479927 and perplexity of 195.8957332522817
Annealing...
Finished 40 epochs...
Completing Train Step...
At time: 1238.0286285877228 and batch: 50, loss is 5.4672113800048825 and perplexity is 236.79892895484574
At time: 1239.220191001892 and batch: 100, loss is 5.478277139663696 and perplexity is 239.433840753214
At time: 1240.3840115070343 and batch: 150, loss is 5.380234622955323 and perplexity is 217.07319980635933
At time: 1241.547199010849 and batch: 200, loss is 5.417063512802124 and perplexity is 225.21680427097024
At time: 1242.714448928833 and batch: 250, loss is 5.434863605499268 and perplexity is 229.2615761249095
At time: 1243.8774535655975 and batch: 300, loss is 5.427447271347046 and perplexity is 227.5675850344937
At time: 1245.0423634052277 and batch: 350, loss is 5.460135354995727 and perplexity is 235.12924812981316
At time: 1246.2054734230042 and batch: 400, loss is 5.430626983642578 and perplexity is 228.29233612202742
At time: 1247.3686788082123 and batch: 450, loss is 5.400620079040527 and perplexity is 221.54374825627673
At time: 1248.5406231880188 and batch: 500, loss is 5.401350021362305 and perplexity is 221.70552144962147
At time: 1249.707614183426 and batch: 550, loss is 5.383584470748901 and perplexity is 217.8015812881992
At time: 1250.8773136138916 and batch: 600, loss is 5.4088610172271725 and perplexity is 223.37702015625064
At time: 1252.0404603481293 and batch: 650, loss is 5.4015187168121335 and perplexity is 221.74292531713436
At time: 1253.2303836345673 and batch: 700, loss is 5.429987478256225 and perplexity is 228.14638861549656
At time: 1254.3931050300598 and batch: 750, loss is 5.41848970413208 and perplexity is 225.53823568144404
At time: 1255.5566930770874 and batch: 800, loss is 5.431414146423339 and perplexity is 228.47211009856463
At time: 1256.720004081726 and batch: 850, loss is 5.4570309448242185 and perplexity is 234.4004423416644
At time: 1257.8890223503113 and batch: 900, loss is 5.427832412719726 and perplexity is 227.65524760672952
At time: 1259.0536749362946 and batch: 950, loss is 5.40533935546875 and perplexity is 222.5917453942935
At time: 1260.21768784523 and batch: 1000, loss is 5.413423147201538 and perplexity is 224.39842327083187
At time: 1261.380802154541 and batch: 1050, loss is 5.3956947517395015 and perplexity is 220.45525557643913
At time: 1262.5436499118805 and batch: 1100, loss is 5.369321632385254 and perplexity is 214.7171611201749
At time: 1263.705573797226 and batch: 1150, loss is 5.401858854293823 and perplexity is 221.81836122589695
At time: 1264.8716125488281 and batch: 1200, loss is 5.417962112426758 and perplexity is 225.41927496316907
At time: 1266.0528211593628 and batch: 1250, loss is 5.418175945281982 and perplexity is 225.4674821643151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.216127715841697 and perplexity of 184.21945107709558
Finished 41 epochs...
Completing Train Step...
At time: 1269.265315771103 and batch: 50, loss is 5.42234808921814 and perplexity is 226.41013001671638
At time: 1270.4419965744019 and batch: 100, loss is 5.440294189453125 and perplexity is 230.50998709363913
At time: 1271.6423053741455 and batch: 150, loss is 5.34604507446289 and perplexity is 209.77700266996942
At time: 1272.8170399665833 and batch: 200, loss is 5.379886436462402 and perplexity is 216.9976310069928
At time: 1273.9830868244171 and batch: 250, loss is 5.403614301681518 and perplexity is 222.2080936659675
At time: 1275.1497259140015 and batch: 300, loss is 5.401107292175293 and perplexity is 221.65171357927858
At time: 1276.3156201839447 and batch: 350, loss is 5.43591760635376 and perplexity is 229.503345412183
At time: 1277.4830651283264 and batch: 400, loss is 5.40741735458374 and perplexity is 223.05477176180239
At time: 1278.6485624313354 and batch: 450, loss is 5.377289743423462 and perplexity is 216.43488572367755
At time: 1279.8127148151398 and batch: 500, loss is 5.380424680709839 and perplexity is 217.11446017208232
At time: 1280.9775884151459 and batch: 550, loss is 5.363402690887451 and perplexity is 213.45001658153944
At time: 1282.1434864997864 and batch: 600, loss is 5.391857709884643 and perplexity is 219.61098032924653
At time: 1283.3651785850525 and batch: 650, loss is 5.38831784248352 and perplexity is 218.83496089234018
At time: 1284.5299971103668 and batch: 700, loss is 5.417577257156372 and perplexity is 225.33253785883946
At time: 1285.6944935321808 and batch: 750, loss is 5.407893342971802 and perplexity is 223.1609685152685
At time: 1286.859102010727 and batch: 800, loss is 5.424299983978272 and perplexity is 226.85249034309288
At time: 1288.031807899475 and batch: 850, loss is 5.450115461349487 and perplexity is 232.78504203114872
At time: 1289.1984419822693 and batch: 900, loss is 5.421297311782837 and perplexity is 226.17234831067498
At time: 1290.3635108470917 and batch: 950, loss is 5.40252875328064 and perplexity is 221.9670069045754
At time: 1291.5285840034485 and batch: 1000, loss is 5.411505155563354 and perplexity is 223.96844145395568
At time: 1292.6922895908356 and batch: 1050, loss is 5.396514549255371 and perplexity is 220.63605834802655
At time: 1293.8577935695648 and batch: 1100, loss is 5.372776947021484 and perplexity is 215.46035972271494
At time: 1295.022331237793 and batch: 1150, loss is 5.4056996154785155 and perplexity is 222.6719507451857
At time: 1296.187956571579 and batch: 1200, loss is 5.419475288391113 and perplexity is 225.76063219359006
At time: 1297.3539118766785 and batch: 1250, loss is 5.4160973072052006 and perplexity is 224.99930362625324
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.213283121150775 and perplexity of 183.69616602439476
Finished 42 epochs...
Completing Train Step...
At time: 1300.3100123405457 and batch: 50, loss is 5.410317850112915 and perplexity is 223.7026803037423
At time: 1301.5016551017761 and batch: 100, loss is 5.428058605194092 and perplexity is 227.70674733469392
At time: 1302.6646387577057 and batch: 150, loss is 5.335752649307251 and perplexity is 207.62896180803182
At time: 1303.8288340568542 and batch: 200, loss is 5.368689737319946 and perplexity is 214.58152526395352
At time: 1304.9936349391937 and batch: 250, loss is 5.393559761047364 and perplexity is 219.98508773832225
At time: 1306.1567523479462 and batch: 300, loss is 5.392116489410401 and perplexity is 219.6678185085486
At time: 1307.3196516036987 and batch: 350, loss is 5.426981611251831 and perplexity is 227.46164056015007
At time: 1308.4832077026367 and batch: 400, loss is 5.4003236293792725 and perplexity is 221.4780814210895
At time: 1309.6474673748016 and batch: 450, loss is 5.370187549591065 and perplexity is 214.90316892642906
At time: 1310.8111062049866 and batch: 500, loss is 5.373281717300415 and perplexity is 215.56914516210915
At time: 1311.9740777015686 and batch: 550, loss is 5.35751049041748 and perplexity is 212.19602430846214
At time: 1313.1644246578217 and batch: 600, loss is 5.386292905807495 and perplexity is 218.3922823033501
At time: 1314.3271567821503 and batch: 650, loss is 5.384388828277588 and perplexity is 217.97684210659835
At time: 1315.490609884262 and batch: 700, loss is 5.4139006805419925 and perplexity is 224.50560658925633
At time: 1316.6541311740875 and batch: 750, loss is 5.40448073387146 and perplexity is 222.40070534161376
At time: 1317.8178009986877 and batch: 800, loss is 5.421481256484985 and perplexity is 226.2139553424984
At time: 1318.9822125434875 and batch: 850, loss is 5.445480518341064 and perplexity is 231.7085931947099
At time: 1320.145250082016 and batch: 900, loss is 5.417369270324707 and perplexity is 225.2856765316576
At time: 1321.3083629608154 and batch: 950, loss is 5.402098255157471 and perplexity is 221.87147109016738
At time: 1322.4717681407928 and batch: 1000, loss is 5.40964373588562 and perplexity is 223.55192996144953
At time: 1323.6353571414948 and batch: 1050, loss is 5.395187540054321 and perplexity is 220.34346644752418
At time: 1324.8076360225677 and batch: 1100, loss is 5.373683195114136 and perplexity is 215.65570876673522
At time: 1325.9712557792664 and batch: 1150, loss is 5.405539178848267 and perplexity is 222.6362288733823
At time: 1327.1337387561798 and batch: 1200, loss is 5.416643228530884 and perplexity is 225.12216907875055
At time: 1328.296802520752 and batch: 1250, loss is 5.411876640319824 and perplexity is 224.05165777172593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2091799548072535 and perplexity of 182.94397433765891
Finished 43 epochs...
Completing Train Step...
At time: 1331.2362835407257 and batch: 50, loss is 5.40265344619751 and perplexity is 221.9946863437944
At time: 1332.443393945694 and batch: 100, loss is 5.421132774353027 and perplexity is 226.13513755515487
At time: 1333.6082129478455 and batch: 150, loss is 5.328196535110473 and perplexity is 206.06600603459452
At time: 1334.7777500152588 and batch: 200, loss is 5.362407140731811 and perplexity is 213.2376221265191
At time: 1335.9428946971893 and batch: 250, loss is 5.386503324508667 and perplexity is 218.4382409588492
At time: 1337.1075699329376 and batch: 300, loss is 5.386618394851684 and perplexity is 218.46337816841026
At time: 1338.2724056243896 and batch: 350, loss is 5.420066080093384 and perplexity is 225.894049108712
At time: 1339.443761587143 and batch: 400, loss is 5.3945720005035405 and perplexity is 220.2078780634572
At time: 1340.6182262897491 and batch: 450, loss is 5.365215873718261 and perplexity is 213.8373915715529
At time: 1341.7883579730988 and batch: 500, loss is 5.369648084640503 and perplexity is 214.7872674642278
At time: 1343.0002171993256 and batch: 550, loss is 5.353568696975708 and perplexity is 211.3612377704142
At time: 1344.1706008911133 and batch: 600, loss is 5.382018270492554 and perplexity is 217.4607273881614
At time: 1345.3406872749329 and batch: 650, loss is 5.38170539855957 and perplexity is 217.39270067241537
At time: 1346.5109233856201 and batch: 700, loss is 5.410420894622803 and perplexity is 223.72573282449218
At time: 1347.6825437545776 and batch: 750, loss is 5.402938537597656 and perplexity is 222.0579841421494
At time: 1348.8524618148804 and batch: 800, loss is 5.417257223129273 and perplexity is 225.26043531756065
At time: 1350.0224823951721 and batch: 850, loss is 5.44000922203064 and perplexity is 230.44430861531782
At time: 1351.1931307315826 and batch: 900, loss is 5.414263772964477 and perplexity is 224.58713767457817
At time: 1352.3641369342804 and batch: 950, loss is 5.397839841842651 and perplexity is 220.92865952890284
At time: 1353.535278558731 and batch: 1000, loss is 5.408217353820801 and perplexity is 223.2332868054632
At time: 1354.705785036087 and batch: 1050, loss is 5.395400791168213 and perplexity is 220.39045994771297
At time: 1355.8752901554108 and batch: 1100, loss is 5.372851705551147 and perplexity is 215.4764678245099
At time: 1357.0510461330414 and batch: 1150, loss is 5.4023781013488765 and perplexity is 221.93356966495313
At time: 1358.2253453731537 and batch: 1200, loss is 5.415344152450562 and perplexity is 224.82990812945596
At time: 1359.3896987438202 and batch: 1250, loss is 5.410379686355591 and perplexity is 223.71651366466588
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.208216312157846 and perplexity of 182.7677666357991
Finished 44 epochs...
Completing Train Step...
At time: 1362.3432703018188 and batch: 50, loss is 5.398063173294068 and perplexity is 220.97800535712759
At time: 1363.5136282444 and batch: 100, loss is 5.414274654388428 and perplexity is 224.58958151573322
At time: 1364.6763050556183 and batch: 150, loss is 5.322902765274048 and perplexity is 204.9780223359823
At time: 1365.8397257328033 and batch: 200, loss is 5.355847978591919 and perplexity is 211.84353899540045
At time: 1367.003003835678 and batch: 250, loss is 5.381727132797241 and perplexity is 217.39742558838574
At time: 1368.1668453216553 and batch: 300, loss is 5.381786289215088 and perplexity is 217.41028642172935
At time: 1369.3303952217102 and batch: 350, loss is 5.4169917678833 and perplexity is 225.20064668924923
At time: 1370.501452922821 and batch: 400, loss is 5.391414184570312 and perplexity is 219.51359889732686
At time: 1371.6915714740753 and batch: 450, loss is 5.361472816467285 and perplexity is 213.0384820872286
At time: 1372.8542461395264 and batch: 500, loss is 5.364939861297607 and perplexity is 213.77837794010114
At time: 1374.0354890823364 and batch: 550, loss is 5.348708190917969 and perplexity is 210.33640780757315
At time: 1375.200266122818 and batch: 600, loss is 5.378718481063843 and perplexity is 216.74433540016318
At time: 1376.364343881607 and batch: 650, loss is 5.3775600337982175 and perplexity is 216.49339389679184
At time: 1377.527361869812 and batch: 700, loss is 5.407208862304688 and perplexity is 223.0082714117348
At time: 1378.6906807422638 and batch: 750, loss is 5.39940016746521 and perplexity is 221.27364925528502
At time: 1379.85378408432 and batch: 800, loss is 5.4167131900787355 and perplexity is 225.13791952511144
At time: 1381.0215084552765 and batch: 850, loss is 5.436849479675293 and perplexity is 229.71731313689838
At time: 1382.1884696483612 and batch: 900, loss is 5.412460222244262 and perplexity is 224.18244842916178
At time: 1383.35187458992 and batch: 950, loss is 5.396754150390625 and perplexity is 220.68892933180564
At time: 1384.5221862792969 and batch: 1000, loss is 5.403876447677613 and perplexity is 222.2663522638173
At time: 1385.7003359794617 and batch: 1050, loss is 5.392846231460571 and perplexity is 219.82817785614677
At time: 1386.8703229427338 and batch: 1100, loss is 5.370664186477661 and perplexity is 215.0056241188082
At time: 1388.0403625965118 and batch: 1150, loss is 5.400302133560181 and perplexity is 221.47332061948734
At time: 1389.2103369235992 and batch: 1200, loss is 5.41430697441101 and perplexity is 224.59684037338224
At time: 1390.379907131195 and batch: 1250, loss is 5.4071680927276615 and perplexity is 222.99917964417097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.206557225136861 and perplexity of 182.46479040777993
Finished 45 epochs...
Completing Train Step...
At time: 1393.40714097023 and batch: 50, loss is 5.3941215133666995 and perplexity is 220.10869958794956
At time: 1394.6201112270355 and batch: 100, loss is 5.410433979034424 and perplexity is 223.72866016322186
At time: 1395.7911701202393 and batch: 150, loss is 5.320613069534302 and perplexity is 204.50922194144636
At time: 1396.9616243839264 and batch: 200, loss is 5.353037939071656 and perplexity is 211.2490858882389
At time: 1398.1440660953522 and batch: 250, loss is 5.377450714111328 and perplexity is 216.4697282003442
At time: 1399.3080415725708 and batch: 300, loss is 5.377054052352905 and perplexity is 216.38387996479267
At time: 1400.4717326164246 and batch: 350, loss is 5.411769618988037 and perplexity is 224.02768074797154
At time: 1401.6679294109344 and batch: 400, loss is 5.38558837890625 and perplexity is 218.2384732531119
At time: 1402.831949710846 and batch: 450, loss is 5.3535168170928955 and perplexity is 211.3502726586043
At time: 1403.9966988563538 and batch: 500, loss is 5.359808034896851 and perplexity is 212.68411460252943
At time: 1405.1601903438568 and batch: 550, loss is 5.345724172592163 and perplexity is 209.70969563743105
At time: 1406.3260827064514 and batch: 600, loss is 5.375228109359742 and perplexity is 215.98913583515147
At time: 1407.4909992218018 and batch: 650, loss is 5.373054857254028 and perplexity is 215.52024668260353
At time: 1408.6544137001038 and batch: 700, loss is 5.4026218509674075 and perplexity is 221.98767248140075
At time: 1409.8190457820892 and batch: 750, loss is 5.39605408668518 and perplexity is 220.53448708819923
At time: 1410.9827184677124 and batch: 800, loss is 5.414359874725342 and perplexity is 224.6087219311022
At time: 1412.1462922096252 and batch: 850, loss is 5.434022541046143 and perplexity is 229.0688334286407
At time: 1413.309777021408 and batch: 900, loss is 5.409753150939942 and perplexity is 223.57639124620198
At time: 1414.4743957519531 and batch: 950, loss is 5.396778879165649 and perplexity is 220.69438676616696
At time: 1415.6378440856934 and batch: 1000, loss is 5.401894931793213 and perplexity is 221.82636402204827
At time: 1416.8019576072693 and batch: 1050, loss is 5.390572996139526 and perplexity is 219.32902423945137
At time: 1417.9680109024048 and batch: 1100, loss is 5.369656343460083 and perplexity is 214.78904136084284
At time: 1419.1324136257172 and batch: 1150, loss is 5.398685331344605 and perplexity is 221.11553137917687
At time: 1420.2983860969543 and batch: 1200, loss is 5.410854253768921 and perplexity is 223.82270742793173
At time: 1421.4690787792206 and batch: 1250, loss is 5.404653377532959 and perplexity is 222.43910472831394
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.204378671019617 and perplexity of 182.0677136710119
Finished 46 epochs...
Completing Train Step...
At time: 1424.5050027370453 and batch: 50, loss is 5.3896604347229005 and perplexity is 219.1289643317443
At time: 1425.6756994724274 and batch: 100, loss is 5.406370220184326 and perplexity is 222.82132568337767
At time: 1426.8454372882843 and batch: 150, loss is 5.316636381149292 and perplexity is 203.69756741212257
At time: 1428.0158591270447 and batch: 200, loss is 5.349799680709839 and perplexity is 210.5661131872873
At time: 1429.1867473125458 and batch: 250, loss is 5.375021648406983 and perplexity is 215.94454711545453
At time: 1430.3612456321716 and batch: 300, loss is 5.3732100677490235 and perplexity is 215.55370028288038
At time: 1431.5640687942505 and batch: 350, loss is 5.4084744548797605 and perplexity is 223.290687698495
At time: 1432.7282264232635 and batch: 400, loss is 5.38091588973999 and perplexity is 217.2211349531581
At time: 1433.892804145813 and batch: 450, loss is 5.349938154220581 and perplexity is 210.59527303511035
At time: 1435.0583593845367 and batch: 500, loss is 5.356638736724854 and perplexity is 212.01112224694776
At time: 1436.2243008613586 and batch: 550, loss is 5.342184562683105 and perplexity is 208.9687172815257
At time: 1437.3914868831635 and batch: 600, loss is 5.372969055175782 and perplexity is 215.50175539084088
At time: 1438.5581657886505 and batch: 650, loss is 5.369749336242676 and perplexity is 214.8090161202094
At time: 1439.723356962204 and batch: 700, loss is 5.397491941452026 and perplexity is 220.85181173041718
At time: 1440.887849330902 and batch: 750, loss is 5.392390365600586 and perplexity is 219.72798853298045
At time: 1442.052323102951 and batch: 800, loss is 5.413404483795166 and perplexity is 224.39423527095033
At time: 1443.2174038887024 and batch: 850, loss is 5.4311580562591555 and perplexity is 228.4136081295872
At time: 1444.382838010788 and batch: 900, loss is 5.407000160217285 and perplexity is 222.96173397638188
At time: 1445.5481345653534 and batch: 950, loss is 5.393038635253906 and perplexity is 219.87047770064297
At time: 1446.7123363018036 and batch: 1000, loss is 5.399364671707153 and perplexity is 221.2657951187619
At time: 1447.8767549991608 and batch: 1050, loss is 5.387789459228515 and perplexity is 218.71936270615748
At time: 1449.043378829956 and batch: 1100, loss is 5.367748641967774 and perplexity is 214.3796785812503
At time: 1450.2238142490387 and batch: 1150, loss is 5.396002216339111 and perplexity is 220.5230481847063
At time: 1451.388736963272 and batch: 1200, loss is 5.408371315002442 and perplexity is 223.26765871198316
At time: 1452.5534582138062 and batch: 1250, loss is 5.402040300369262 and perplexity is 221.8586129486499
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.204618802035812 and perplexity of 182.11143902580883
Annealing...
Finished 47 epochs...
Completing Train Step...
At time: 1455.491218328476 and batch: 50, loss is 5.388744421005249 and perplexity is 218.92833109990698
At time: 1456.6811277866364 and batch: 100, loss is 5.4092160511016845 and perplexity is 223.4563406450907
At time: 1457.853857755661 and batch: 150, loss is 5.313700885772705 and perplexity is 203.10049093114424
At time: 1459.0293321609497 and batch: 200, loss is 5.348723726272583 and perplexity is 210.33967548363893
At time: 1460.2011954784393 and batch: 250, loss is 5.372439050674439 and perplexity is 215.3875687528255
At time: 1461.39106631279 and batch: 300, loss is 5.370196666717529 and perplexity is 214.9051282347294
At time: 1462.553908109665 and batch: 350, loss is 5.401808471679687 and perplexity is 221.80718571852307
At time: 1463.7173147201538 and batch: 400, loss is 5.368223829269409 and perplexity is 214.48157328985926
At time: 1464.8800370693207 and batch: 450, loss is 5.339931430816651 and perplexity is 208.49841323300816
At time: 1466.0425791740417 and batch: 500, loss is 5.342425584793091 and perplexity is 209.01908943284315
At time: 1467.205992937088 and batch: 550, loss is 5.327432584762573 and perplexity is 205.90864195441932
At time: 1468.3693809509277 and batch: 600, loss is 5.356684646606445 and perplexity is 212.02085587589937
At time: 1469.5329444408417 and batch: 650, loss is 5.356085271835327 and perplexity is 211.8938140005872
At time: 1470.6973156929016 and batch: 700, loss is 5.381213846206665 and perplexity is 217.28586703820872
At time: 1471.8601813316345 and batch: 750, loss is 5.369017181396484 and perplexity is 214.6518002182665
At time: 1473.0240304470062 and batch: 800, loss is 5.386571054458618 and perplexity is 218.4530362710139
At time: 1474.187982559204 and batch: 850, loss is 5.402627229690552 and perplexity is 221.98886649484356
At time: 1475.3510580062866 and batch: 900, loss is 5.375932569503784 and perplexity is 216.14134517930714
At time: 1476.5199801921844 and batch: 950, loss is 5.366181325912476 and perplexity is 214.0439410412264
At time: 1477.6897003650665 and batch: 1000, loss is 5.371656532287598 and perplexity is 215.21908994745803
At time: 1478.8740797042847 and batch: 1050, loss is 5.355447492599487 and perplexity is 211.75871561186776
At time: 1480.0377247333527 and batch: 1100, loss is 5.333899822235107 and perplexity is 207.24461741843135
At time: 1481.2008395195007 and batch: 1150, loss is 5.35518141746521 and perplexity is 211.70237937834474
At time: 1482.3701133728027 and batch: 1200, loss is 5.378200426101684 and perplexity is 216.63207900169206
At time: 1483.5333878993988 and batch: 1250, loss is 5.378777523040771 and perplexity is 216.75713279200107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.18855274506729 and perplexity of 179.20900404534174
Finished 48 epochs...
Completing Train Step...
At time: 1486.4329109191895 and batch: 50, loss is 5.373919067382812 and perplexity is 215.70658196756773
At time: 1487.625828742981 and batch: 100, loss is 5.393653163909912 and perplexity is 220.0056359348502
At time: 1488.7905616760254 and batch: 150, loss is 5.300001258850098 and perplexity is 200.33706216896326
At time: 1490.010904788971 and batch: 200, loss is 5.334096441268921 and perplexity is 207.2853696610732
At time: 1491.1764557361603 and batch: 250, loss is 5.360250949859619 and perplexity is 212.77833644382136
At time: 1492.3430843353271 and batch: 300, loss is 5.360484437942505 and perplexity is 212.82802345011345
At time: 1493.5110569000244 and batch: 350, loss is 5.3923248863220214 and perplexity is 219.71360137384642
At time: 1494.6869401931763 and batch: 400, loss is 5.361175899505615 and perplexity is 212.9752367381816
At time: 1495.8555400371552 and batch: 450, loss is 5.332840623855591 and perplexity is 207.02522046845706
At time: 1497.028490781784 and batch: 500, loss is 5.335638828277588 and perplexity is 207.60533061070012
At time: 1498.1961495876312 and batch: 550, loss is 5.319340667724609 and perplexity is 204.24916951802192
At time: 1499.3631074428558 and batch: 600, loss is 5.350031366348267 and perplexity is 210.61490398349756
At time: 1500.5307848453522 and batch: 650, loss is 5.350999965667724 and perplexity is 210.8190042659039
At time: 1501.6955676078796 and batch: 700, loss is 5.376903924942017 and perplexity is 216.35139725146936
At time: 1502.8606498241425 and batch: 750, loss is 5.366144218444824 and perplexity is 214.0359985599718
At time: 1504.0285034179688 and batch: 800, loss is 5.384412708282471 and perplexity is 217.98204745680385
At time: 1505.1961569786072 and batch: 850, loss is 5.402324018478393 and perplexity is 221.92156718501622
At time: 1506.3609864711761 and batch: 900, loss is 5.3763948249816895 and perplexity is 216.24128079623176
At time: 1507.526044368744 and batch: 950, loss is 5.367120981216431 and perplexity is 214.2451630905824
At time: 1508.6919360160828 and batch: 1000, loss is 5.37337474822998 and perplexity is 215.58920069294706
At time: 1509.8562772274017 and batch: 1050, loss is 5.3586201477050786 and perplexity is 212.43161986422322
At time: 1511.0201480388641 and batch: 1100, loss is 5.337318954467773 and perplexity is 207.9544269447082
At time: 1512.1851451396942 and batch: 1150, loss is 5.3592116737365725 and perplexity is 212.5573158698499
At time: 1513.3500142097473 and batch: 1200, loss is 5.382022981643677 and perplexity is 217.4617518809246
At time: 1514.5228006839752 and batch: 1250, loss is 5.3798559951782225 and perplexity is 216.9910254209828
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1876995894160585 and perplexity of 179.05617607308292
Finished 49 epochs...
Completing Train Step...
At time: 1517.4430956840515 and batch: 50, loss is 5.37051212310791 and perplexity is 214.97293212477965
At time: 1518.6052887439728 and batch: 100, loss is 5.389106893539429 and perplexity is 219.00770099071823
At time: 1519.7952308654785 and batch: 150, loss is 5.295630521774292 and perplexity is 199.46335231261253
At time: 1520.965728521347 and batch: 200, loss is 5.329232788085937 and perplexity is 206.27965322363923
At time: 1522.1331040859222 and batch: 250, loss is 5.35629695892334 and perplexity is 211.93867393300775
At time: 1523.2960968017578 and batch: 300, loss is 5.3571107006073 and perplexity is 212.1112074557692
At time: 1524.458899974823 and batch: 350, loss is 5.388939332962036 and perplexity is 218.97100700820508
At time: 1525.621672153473 and batch: 400, loss is 5.357696256637573 and perplexity is 212.23544682338905
At time: 1526.7850742340088 and batch: 450, loss is 5.329402933120727 and perplexity is 206.314753668412
At time: 1527.9481856822968 and batch: 500, loss is 5.332994384765625 and perplexity is 207.05705530217
At time: 1529.1113955974579 and batch: 550, loss is 5.316923694610596 and perplexity is 203.75610087359803
At time: 1530.2745432853699 and batch: 600, loss is 5.347405433654785 and perplexity is 210.06256893610865
At time: 1531.4370324611664 and batch: 650, loss is 5.34951322555542 and perplexity is 210.50580407715893
At time: 1532.5994324684143 and batch: 700, loss is 5.375128002166748 and perplexity is 215.96751485127191
At time: 1533.7618939876556 and batch: 750, loss is 5.365082998275756 and perplexity is 213.80897972118413
At time: 1534.926897764206 and batch: 800, loss is 5.38364541053772 and perplexity is 217.81485447499577
At time: 1536.0905923843384 and batch: 850, loss is 5.401783380508423 and perplexity is 221.8016203862592
At time: 1537.2549192905426 and batch: 900, loss is 5.376275300979614 and perplexity is 216.21543631748563
At time: 1538.4175477027893 and batch: 950, loss is 5.367767477035523 and perplexity is 214.38371647504727
At time: 1539.580563545227 and batch: 1000, loss is 5.374566097259521 and perplexity is 215.84619573297147
At time: 1540.743591785431 and batch: 1050, loss is 5.359972867965698 and perplexity is 212.7191748673006
At time: 1541.9087328910828 and batch: 1100, loss is 5.338379831314087 and perplexity is 208.175158044897
At time: 1543.07181930542 and batch: 1150, loss is 5.360304727554321 and perplexity is 212.78977947992507
At time: 1544.2349638938904 and batch: 1200, loss is 5.383204832077026 and perplexity is 217.71891107854387
At time: 1545.397675037384 and batch: 1250, loss is 5.379853715896607 and perplexity is 216.99053083789138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.187435846259124 and perplexity of 179.00895745900365
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9c69001898>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'anneal': 2.0663804460052724, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 2.827637190594716, 'dropout': 0.21859992004272255, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.6303982734680176 and batch: 50, loss is 7.433309192657471 and perplexity is 1691.3954760342829
At time: 2.809124708175659 and batch: 100, loss is 6.6640675830841065 and perplexity is 783.7323588771466
At time: 3.9818150997161865 and batch: 150, loss is 6.41105471611023 and perplexity is 608.535174557491
At time: 5.15269136428833 and batch: 200, loss is 6.254769239425659 and perplexity is 520.4892524993769
At time: 6.309991121292114 and batch: 250, loss is 6.1924142742156985 and perplexity is 489.02532339394116
At time: 7.468649625778198 and batch: 300, loss is 6.102839918136596 and perplexity is 447.1257693048958
At time: 8.634855031967163 and batch: 350, loss is 6.058021545410156 and perplexity is 427.528753348675
At time: 9.804154634475708 and batch: 400, loss is 5.9425803565979 and perplexity is 380.9165630779763
At time: 10.968766927719116 and batch: 450, loss is 5.867519311904907 and perplexity is 353.371288130031
At time: 12.13992953300476 and batch: 500, loss is 5.808430032730103 and perplexity is 333.0957655033146
At time: 13.3059401512146 and batch: 550, loss is 5.770327291488647 and perplexity is 320.64265908887126
At time: 14.471928834915161 and batch: 600, loss is 5.752309169769287 and perplexity is 314.917018179302
At time: 15.638113498687744 and batch: 650, loss is 5.681688985824585 and perplexity is 293.44463543683526
At time: 16.811971187591553 and batch: 700, loss is 5.662019128799439 and perplexity is 287.72901837955186
At time: 17.97749972343445 and batch: 750, loss is 5.59789436340332 and perplexity is 269.8575867609375
At time: 19.143235445022583 and batch: 800, loss is 5.580663366317749 and perplexity is 265.2475036972215
At time: 20.30957293510437 and batch: 850, loss is 5.615456113815307 and perplexity is 274.63861709931643
At time: 21.475727796554565 and batch: 900, loss is 5.576435842514038 and perplexity is 264.1285304709611
At time: 22.641616344451904 and batch: 950, loss is 5.526799688339233 and perplexity is 251.33826177683332
At time: 23.806602954864502 and batch: 1000, loss is 5.498462944030762 and perplexity is 244.3161159684398
At time: 24.972182035446167 and batch: 1050, loss is 5.4627736377716065 and perplexity is 235.75040460818005
At time: 26.138611793518066 and batch: 1100, loss is 5.432855644226074 and perplexity is 228.80168963027342
At time: 27.3045711517334 and batch: 1150, loss is 5.455321884155273 and perplexity is 234.0001798987009
At time: 28.47181749343872 and batch: 1200, loss is 5.436212520599366 and perplexity is 229.57103919959968
At time: 29.637760400772095 and batch: 1250, loss is 5.416275196075439 and perplexity is 225.03933205838052
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.007875317204608 and perplexity of 149.58657425144725
Finished 1 epochs...
Completing Train Step...
At time: 32.58214259147644 and batch: 50, loss is 5.2511828517913814 and perplexity is 190.79181347752058
At time: 33.74109101295471 and batch: 100, loss is 5.260254430770874 and perplexity is 192.5304707421301
At time: 34.89940404891968 and batch: 150, loss is 5.132798347473145 and perplexity is 169.49074905308007
At time: 36.08563542366028 and batch: 200, loss is 5.152628631591797 and perplexity is 172.88534542732882
At time: 37.245630979537964 and batch: 250, loss is 5.1645578956604 and perplexity is 174.9600908553323
At time: 38.40979480743408 and batch: 300, loss is 5.15529951095581 and perplexity is 173.34771852528874
At time: 39.569645166397095 and batch: 350, loss is 5.144721145629883 and perplexity is 171.52364788283288
At time: 40.72859597206116 and batch: 400, loss is 5.120341720581055 and perplexity is 167.3925613320779
At time: 41.89500331878662 and batch: 450, loss is 5.052704601287842 and perplexity is 156.44501420145437
At time: 43.06941843032837 and batch: 500, loss is 5.0509381771087645 and perplexity is 156.16890987614386
At time: 44.22886347770691 and batch: 550, loss is 5.039866828918457 and perplexity is 154.44944545659024
At time: 45.38782835006714 and batch: 600, loss is 5.044680824279785 and perplexity is 155.19475689412516
At time: 46.54664397239685 and batch: 650, loss is 5.022547521591187 and perplexity is 151.79751909665515
At time: 47.720216512680054 and batch: 700, loss is 5.012489681243896 and perplexity is 150.27841613801644
At time: 48.88897681236267 and batch: 750, loss is 4.996406135559082 and perplexity is 147.880739622287
At time: 50.05194115638733 and batch: 800, loss is 5.000251693725586 and perplexity is 148.45051846487843
At time: 51.21090292930603 and batch: 850, loss is 5.035876865386963 and perplexity is 153.83442557113236
At time: 52.36979269981384 and batch: 900, loss is 5.009845304489136 and perplexity is 149.88154835287736
At time: 53.52904009819031 and batch: 950, loss is 4.979577827453613 and perplexity is 145.4129793255738
At time: 54.6883909702301 and batch: 1000, loss is 4.9589750194549564 and perplexity is 142.44771490822572
At time: 55.847999811172485 and batch: 1050, loss is 4.925736246109008 and perplexity is 137.79075221188268
At time: 57.00818943977356 and batch: 1100, loss is 4.902735862731934 and perplexity is 134.65768112289487
At time: 58.167510747909546 and batch: 1150, loss is 4.915213413238526 and perplexity is 136.34840525248978
At time: 59.3294837474823 and batch: 1200, loss is 4.9158531761169435 and perplexity is 136.4356638101186
At time: 60.490033864974976 and batch: 1250, loss is 4.909030361175537 and perplexity is 135.50795690957438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.717162194913321 and perplexity of 111.85039225608568
Finished 2 epochs...
Completing Train Step...
At time: 63.409395933151245 and batch: 50, loss is 4.860561246871948 and perplexity is 129.096636862205
At time: 64.56909132003784 and batch: 100, loss is 4.891426916122437 and perplexity is 133.14342306673015
At time: 65.72630739212036 and batch: 150, loss is 4.78117748260498 and perplexity is 119.24467594460906
At time: 66.8926887512207 and batch: 200, loss is 4.824408836364746 and perplexity is 124.51283915924664
At time: 68.05005979537964 and batch: 250, loss is 4.841730794906616 and perplexity is 126.68843377828776
At time: 69.2074327468872 and batch: 300, loss is 4.84426323890686 and perplexity is 127.00967172918841
At time: 70.36674880981445 and batch: 350, loss is 4.838487434387207 and perplexity is 126.27820313645377
At time: 71.53570914268494 and batch: 400, loss is 4.83182222366333 and perplexity is 125.4393310379448
At time: 72.69488787651062 and batch: 450, loss is 4.764970855712891 and perplexity is 117.32769780586902
At time: 73.85379791259766 and batch: 500, loss is 4.776919288635254 and perplexity is 118.73798853682545
At time: 75.01418495178223 and batch: 550, loss is 4.776300373077393 and perplexity is 118.66452248539714
At time: 76.17427897453308 and batch: 600, loss is 4.784229726791382 and perplexity is 119.60919583237961
At time: 77.33396458625793 and batch: 650, loss is 4.776778831481933 and perplexity is 118.72131210815382
At time: 78.49291920661926 and batch: 700, loss is 4.767085771560669 and perplexity is 117.57609859413522
At time: 79.65204048156738 and batch: 750, loss is 4.760004758834839 and perplexity is 116.74648147489133
At time: 80.81172919273376 and batch: 800, loss is 4.770673933029175 and perplexity is 117.99873841723831
At time: 81.97034549713135 and batch: 850, loss is 4.810332736968994 and perplexity is 122.77246165847866
At time: 83.13065814971924 and batch: 900, loss is 4.782935352325439 and perplexity is 119.45447689712329
At time: 84.28993821144104 and batch: 950, loss is 4.758709602355957 and perplexity is 116.59537438780649
At time: 85.44958734512329 and batch: 1000, loss is 4.745812435150146 and perplexity is 115.10127985262338
At time: 86.61192607879639 and batch: 1050, loss is 4.715365362167359 and perplexity is 111.64959626106504
At time: 87.77293872833252 and batch: 1100, loss is 4.698879528045654 and perplexity is 109.82404872015694
At time: 88.93194127082825 and batch: 1150, loss is 4.705344724655151 and perplexity is 110.53638299693645
At time: 90.13795495033264 and batch: 1200, loss is 4.714099006652832 and perplexity is 111.5082976651561
At time: 91.29695105552673 and batch: 1250, loss is 4.714751081466675 and perplexity is 111.58103312950296
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.605257577269617 and perplexity of 100.00873951002548
Finished 3 epochs...
Completing Train Step...
At time: 94.19302177429199 and batch: 50, loss is 4.66794207572937 and perplexity is 106.47839233903146
At time: 95.37866830825806 and batch: 100, loss is 4.699255046844482 and perplexity is 109.86529745936853
At time: 96.54190921783447 and batch: 150, loss is 4.598139514923096 and perplexity is 99.29939862830751
At time: 97.70576810836792 and batch: 200, loss is 4.647931022644043 and perplexity is 104.36882530264779
At time: 98.86936235427856 and batch: 250, loss is 4.666548357009888 and perplexity is 106.3300947769574
At time: 100.03212189674377 and batch: 300, loss is 4.672010660171509 and perplexity is 106.91249115455284
At time: 101.21171593666077 and batch: 350, loss is 4.663525123596191 and perplexity is 106.0091195175852
At time: 102.38340353965759 and batch: 400, loss is 4.663649463653565 and perplexity is 106.02230151709647
At time: 103.55115747451782 and batch: 450, loss is 4.5975109958648686 and perplexity is 99.2370066731195
At time: 104.71401882171631 and batch: 500, loss is 4.616478395462036 and perplexity is 101.13723889649306
At time: 105.87679839134216 and batch: 550, loss is 4.61623031616211 and perplexity is 101.11215195297545
At time: 107.03919553756714 and batch: 600, loss is 4.625425386428833 and perplexity is 102.04617290826634
At time: 108.20151019096375 and batch: 650, loss is 4.627209939956665 and perplexity is 102.22844235254972
At time: 109.36452913284302 and batch: 700, loss is 4.615421323776245 and perplexity is 101.0303860703748
At time: 110.52751421928406 and batch: 750, loss is 4.611179027557373 and perplexity is 100.60269308715515
At time: 111.69276261329651 and batch: 800, loss is 4.625065078735352 and perplexity is 102.00941151018218
At time: 112.85779094696045 and batch: 850, loss is 4.668812551498413 and perplexity is 106.57111955203007
At time: 114.02275967597961 and batch: 900, loss is 4.638686494827271 and perplexity is 103.40843083034392
At time: 115.18696737289429 and batch: 950, loss is 4.618652877807617 and perplexity is 101.35739931767871
At time: 116.3519868850708 and batch: 1000, loss is 4.60674186706543 and perplexity is 100.15729168153533
At time: 117.51765155792236 and batch: 1050, loss is 4.57993330001831 and perplexity is 97.50789021565637
At time: 118.68285846710205 and batch: 1100, loss is 4.564924755096436 and perplexity is 96.05536606784227
At time: 119.89373755455017 and batch: 1150, loss is 4.566945199966431 and perplexity is 96.24963683004434
At time: 121.0588595867157 and batch: 1200, loss is 4.5781848430633545 and perplexity is 97.33755082578661
At time: 122.22475361824036 and batch: 1250, loss is 4.584237022399902 and perplexity is 97.92844142386147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.5425089815237225 and perplexity of 93.92616374394149
Finished 4 epochs...
Completing Train Step...
At time: 125.14892196655273 and batch: 50, loss is 4.537096490859986 and perplexity is 93.4191625668838
At time: 126.31047511100769 and batch: 100, loss is 4.56818470954895 and perplexity is 96.36901314596895
At time: 127.47266435623169 and batch: 150, loss is 4.473863372802734 and perplexity is 87.69486736953624
At time: 128.6322591304779 and batch: 200, loss is 4.5247868633270265 and perplexity is 92.27625627832344
At time: 129.79248023033142 and batch: 250, loss is 4.542763042449951 and perplexity is 93.95002974367999
At time: 130.95296478271484 and batch: 300, loss is 4.5518526840209965 and perplexity is 94.80789477596316
At time: 132.114009141922 and batch: 350, loss is 4.540167989730835 and perplexity is 93.70654053384476
At time: 133.27575492858887 and batch: 400, loss is 4.543821821212768 and perplexity is 94.04955471810591
At time: 134.43692016601562 and batch: 450, loss is 4.47769006729126 and perplexity is 88.03109173884746
At time: 135.5975992679596 and batch: 500, loss is 4.5027640914916995 and perplexity is 90.26629107883254
At time: 136.75765299797058 and batch: 550, loss is 4.500064258575439 and perplexity is 90.02291585899607
At time: 137.91796231269836 and batch: 600, loss is 4.512310037612915 and perplexity is 91.13209410988794
At time: 139.07941317558289 and batch: 650, loss is 4.5206548118591305 and perplexity is 91.89575270966958
At time: 140.2409074306488 and batch: 700, loss is 4.505469923019409 and perplexity is 90.51086719678601
At time: 141.40254306793213 and batch: 750, loss is 4.50167932510376 and perplexity is 90.16842633009787
At time: 142.56269192695618 and batch: 800, loss is 4.520066337585449 and perplexity is 91.84169033205873
At time: 143.72285556793213 and batch: 850, loss is 4.564391393661499 and perplexity is 96.00414750018149
At time: 144.88386392593384 and batch: 900, loss is 4.532515230178833 and perplexity is 92.992163873531
At time: 146.04495286941528 and batch: 950, loss is 4.515525245666504 and perplexity is 91.42557429992337
At time: 147.2069206237793 and batch: 1000, loss is 4.50441089630127 and perplexity is 90.41506450790011
At time: 148.3955361843109 and batch: 1050, loss is 4.479415416717529 and perplexity is 88.18310723468687
At time: 149.55673360824585 and batch: 1100, loss is 4.463499689102173 and perplexity is 86.7907187496762
At time: 150.71761536598206 and batch: 1150, loss is 4.4644145584106445 and perplexity is 86.87015724690121
At time: 151.87895250320435 and batch: 1200, loss is 4.477648019790649 and perplexity is 88.02739032928191
At time: 153.03971552848816 and batch: 1250, loss is 4.487115402221679 and perplexity is 88.86473678312396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.502604463674726 and perplexity of 90.25188321781962
Finished 5 epochs...
Completing Train Step...
At time: 155.95835208892822 and batch: 50, loss is 4.439808025360107 and perplexity is 84.75866859698723
At time: 157.12322068214417 and batch: 100, loss is 4.4682160663604735 and perplexity is 87.2010233368557
At time: 158.28763341903687 and batch: 150, loss is 4.379708442687988 and perplexity is 79.81475943568148
At time: 159.4525306224823 and batch: 200, loss is 4.43000807762146 and perplexity is 83.93209487922144
At time: 160.61663055419922 and batch: 250, loss is 4.44861611366272 and perplexity is 85.50852800033502
At time: 161.7872109413147 and batch: 300, loss is 4.457946310043335 and perplexity is 86.31007282942447
At time: 162.95410537719727 and batch: 350, loss is 4.445466728210449 and perplexity is 85.2396523050471
At time: 164.11865615844727 and batch: 400, loss is 4.450737218856812 and perplexity is 85.69009307445353
At time: 165.28265404701233 and batch: 450, loss is 4.384476251602173 and perplexity is 80.19620957529692
At time: 166.44679021835327 and batch: 500, loss is 4.413847246170044 and perplexity is 82.58658401175923
At time: 167.61146020889282 and batch: 550, loss is 4.410759038925171 and perplexity is 82.33193293471975
At time: 168.77688217163086 and batch: 600, loss is 4.425688123703003 and perplexity is 83.57029414098234
At time: 169.94179439544678 and batch: 650, loss is 4.43691535949707 and perplexity is 84.51384435781404
At time: 171.1071581840515 and batch: 700, loss is 4.4189856910705565 and perplexity is 83.0120427853208
At time: 172.27118945121765 and batch: 750, loss is 4.416518859863281 and perplexity is 82.80751845481409
At time: 173.43676376342773 and batch: 800, loss is 4.438358507156372 and perplexity is 84.63589836427123
At time: 174.60249948501587 and batch: 850, loss is 4.4821929836273195 and perplexity is 88.42838219145744
At time: 175.76669669151306 and batch: 900, loss is 4.448898916244507 and perplexity is 85.5327134525113
At time: 176.93919372558594 and batch: 950, loss is 4.435359716415405 and perplexity is 84.38247319035125
At time: 178.15123462677002 and batch: 1000, loss is 4.423712968826294 and perplexity is 83.40539277344187
At time: 179.3152153491974 and batch: 1050, loss is 4.400452909469604 and perplexity is 81.48776688984096
At time: 180.47980093955994 and batch: 1100, loss is 4.382959594726563 and perplexity is 80.0746716316229
At time: 181.64608001708984 and batch: 1150, loss is 4.382938585281372 and perplexity is 80.07298932487032
At time: 182.82443261146545 and batch: 1200, loss is 4.3972984790802006 and perplexity is 81.2311243948503
At time: 183.99445700645447 and batch: 1250, loss is 4.4091055297851565 and perplexity is 82.19590882064693
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.47941555942062 and perplexity of 88.18311981868976
Finished 6 epochs...
Completing Train Step...
At time: 186.9067108631134 and batch: 50, loss is 4.362853908538819 and perplexity is 78.48079212474624
At time: 188.09037613868713 and batch: 100, loss is 4.387410879135132 and perplexity is 80.4319012444375
At time: 189.25378894805908 and batch: 150, loss is 4.30398663520813 and perplexity is 73.99419433604639
At time: 190.41470289230347 and batch: 200, loss is 4.352649097442627 and perplexity is 77.68398302444926
At time: 191.57528948783875 and batch: 250, loss is 4.372211580276489 and perplexity is 79.21863648364736
At time: 192.73646926879883 and batch: 300, loss is 4.381124811172485 and perplexity is 79.92788664153642
At time: 193.9028136730194 and batch: 350, loss is 4.368233642578125 and perplexity is 78.90413563028781
At time: 195.0639944076538 and batch: 400, loss is 4.377224626541138 and perplexity is 79.61676024602406
At time: 196.22487711906433 and batch: 450, loss is 4.309522762298584 and perplexity is 74.40497160829031
At time: 197.39235019683838 and batch: 500, loss is 4.341691179275513 and perplexity is 76.83737529609543
At time: 198.55365467071533 and batch: 550, loss is 4.338569097518921 and perplexity is 76.59785682114631
At time: 199.71423625946045 and batch: 600, loss is 4.355323657989502 and perplexity is 77.8920316358555
At time: 200.87560486793518 and batch: 650, loss is 4.368309507369995 and perplexity is 78.91012190318588
At time: 202.0359296798706 and batch: 700, loss is 4.3477629280090335 and perplexity is 77.30533175160008
At time: 203.19751620292664 and batch: 750, loss is 4.346911687850952 and perplexity is 77.23955434893452
At time: 204.35881662368774 and batch: 800, loss is 4.370838174819946 and perplexity is 79.10991185463028
At time: 205.51992893218994 and batch: 850, loss is 4.414376630783081 and perplexity is 82.630315652986
At time: 206.6807897090912 and batch: 900, loss is 4.381074409484864 and perplexity is 79.92385824268155
At time: 207.8691816329956 and batch: 950, loss is 4.369268388748169 and perplexity is 78.98582363831997
At time: 209.02969527244568 and batch: 1000, loss is 4.357213735580444 and perplexity is 78.03939283754659
At time: 210.19122099876404 and batch: 1050, loss is 4.335400094985962 and perplexity is 76.35550223283504
At time: 211.3523886203766 and batch: 1100, loss is 4.317428607940673 and perplexity is 74.99553721310893
At time: 212.513653755188 and batch: 1150, loss is 4.316030759811401 and perplexity is 74.8907780774508
At time: 213.68272852897644 and batch: 1200, loss is 4.330871849060059 and perplexity is 76.01052739585482
At time: 214.84381294250488 and batch: 1250, loss is 4.344881629943847 and perplexity is 77.08291263056186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.462977945369525 and perplexity of 86.7454480469962
Finished 7 epochs...
Completing Train Step...
At time: 217.76239848136902 and batch: 50, loss is 4.29799747467041 and perplexity is 73.55235566939201
At time: 218.92444133758545 and batch: 100, loss is 4.320048103332519 and perplexity is 75.19224520260352
At time: 220.08574390411377 and batch: 150, loss is 4.240432181358337 and perplexity is 69.43785510140054
At time: 221.24845147132874 and batch: 200, loss is 4.287855110168457 and perplexity is 72.81013118639756
At time: 222.41198825836182 and batch: 250, loss is 4.30801815032959 and perplexity is 74.29310517634622
At time: 223.5734338760376 and batch: 300, loss is 4.317442054748535 and perplexity is 74.99654567046859
At time: 224.7357006072998 and batch: 350, loss is 4.303106889724732 and perplexity is 73.92912690336819
At time: 225.89792037010193 and batch: 400, loss is 4.3165911674499515 and perplexity is 74.93275920371562
At time: 227.05956363677979 and batch: 450, loss is 4.246192102432251 and perplexity is 69.83896574017739
At time: 228.2339804172516 and batch: 500, loss is 4.281251726150512 and perplexity is 72.33092187265149
At time: 229.40889739990234 and batch: 550, loss is 4.278540282249451 and perplexity is 72.13506628139893
At time: 230.57068991661072 and batch: 600, loss is 4.296446647644043 and perplexity is 73.43837709174008
At time: 231.73259091377258 and batch: 650, loss is 4.309537868499756 and perplexity is 74.40609559324922
At time: 232.89414811134338 and batch: 700, loss is 4.2889394855499265 and perplexity is 72.88912752327833
At time: 234.05496621131897 and batch: 750, loss is 4.287558879852295 and perplexity is 72.78856581253285
At time: 235.2165241241455 and batch: 800, loss is 4.3130162239074705 and perplexity is 74.66535707861833
At time: 236.3783302307129 and batch: 850, loss is 4.357201461791992 and perplexity is 78.03843500442608
At time: 237.6001091003418 and batch: 900, loss is 4.323219046592713 and perplexity is 75.43105397017408
At time: 238.76505732536316 and batch: 950, loss is 4.3121909904479985 and perplexity is 74.60376614463873
At time: 239.93166422843933 and batch: 1000, loss is 4.300579328536987 and perplexity is 73.7425024632643
At time: 241.09305262565613 and batch: 1050, loss is 4.279117641448974 and perplexity is 72.17672615072186
At time: 242.25489330291748 and batch: 1100, loss is 4.261545324325562 and perplexity is 70.91949243585512
At time: 243.41589760780334 and batch: 1150, loss is 4.258195009231567 and perplexity is 70.6822873676269
At time: 244.58216214179993 and batch: 1200, loss is 4.274698104858398 and perplexity is 71.85844232009646
At time: 245.74426007270813 and batch: 1250, loss is 4.2904148864746094 and perplexity is 72.9967475812697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.451483287950502 and perplexity of 85.7540476588347
Finished 8 epochs...
Completing Train Step...
At time: 248.6421480178833 and batch: 50, loss is 4.242772483825684 and perplexity is 69.60055098955853
At time: 249.82653856277466 and batch: 100, loss is 4.263346071243286 and perplexity is 71.0473155472616
At time: 250.98795247077942 and batch: 150, loss is 4.18640516281128 and perplexity is 65.78587586689159
At time: 252.14966559410095 and batch: 200, loss is 4.233812704086303 and perplexity is 68.97973074244895
At time: 253.31262969970703 and batch: 250, loss is 4.252876181602478 and perplexity is 70.30733849284535
At time: 254.47483277320862 and batch: 300, loss is 4.264617948532105 and perplexity is 71.13773650432927
At time: 255.63606691360474 and batch: 350, loss is 4.248257246017456 and perplexity is 69.98334226008342
At time: 256.7979817390442 and batch: 400, loss is 4.26238392829895 and perplexity is 70.97899074827625
At time: 257.9599733352661 and batch: 450, loss is 4.19160071849823 and perplexity is 66.12855949344448
At time: 259.1232867240906 and batch: 500, loss is 4.230181245803833 and perplexity is 68.72968801257916
At time: 260.29121828079224 and batch: 550, loss is 4.226559624671936 and perplexity is 68.48122531253917
At time: 261.4527220726013 and batch: 600, loss is 4.2457677936553955 and perplexity is 69.80933873998994
At time: 262.61404728889465 and batch: 650, loss is 4.258206634521485 and perplexity is 70.68310907448586
At time: 263.77618956565857 and batch: 700, loss is 4.237741651535035 and perplexity is 69.25128158473915
At time: 264.93814730644226 and batch: 750, loss is 4.236304249763489 and perplexity is 69.15181117650444
At time: 266.12725138664246 and batch: 800, loss is 4.262982382774353 and perplexity is 71.02148115596825
At time: 267.289617061615 and batch: 850, loss is 4.3085645580291745 and perplexity is 74.33371059358664
At time: 268.4550838470459 and batch: 900, loss is 4.27347279548645 and perplexity is 71.77044741876618
At time: 269.620254278183 and batch: 950, loss is 4.261874494552612 and perplexity is 70.94284086387535
At time: 270.7858815193176 and batch: 1000, loss is 4.2509722328186035 and perplexity is 70.17360427317603
At time: 271.94745326042175 and batch: 1050, loss is 4.230542922019959 and perplexity is 68.75455040187147
At time: 273.1093988418579 and batch: 1100, loss is 4.213020343780517 and perplexity is 67.5602872549831
At time: 274.2717227935791 and batch: 1150, loss is 4.208599777221679 and perplexity is 67.26229164841578
At time: 275.43339467048645 and batch: 1200, loss is 4.225750188827515 and perplexity is 68.42581658203284
At time: 276.5948600769043 and batch: 1250, loss is 4.242217035293579 and perplexity is 69.56190220037794
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.444949407647126 and perplexity of 85.19556748407459
Finished 9 epochs...
Completing Train Step...
At time: 279.555047750473 and batch: 50, loss is 4.195035743713379 and perplexity is 66.35610334844566
At time: 280.7231984138489 and batch: 100, loss is 4.2147421169281 and perplexity is 67.67671094224943
At time: 281.8913412094116 and batch: 150, loss is 4.138917894363403 and perplexity is 62.73489891842184
At time: 283.0586061477661 and batch: 200, loss is 4.187249450683594 and perplexity is 65.84144153746577
At time: 284.22968554496765 and batch: 250, loss is 4.206008925437927 and perplexity is 67.08825057478663
At time: 285.39763617515564 and batch: 300, loss is 4.216914710998535 and perplexity is 67.82390480150372
At time: 286.5647716522217 and batch: 350, loss is 4.201834263801575 and perplexity is 66.80876361650685
At time: 287.7335150241852 and batch: 400, loss is 4.215718297958374 and perplexity is 67.74280791972157
At time: 288.9017217159271 and batch: 450, loss is 4.144595050811768 and perplexity is 63.09206764642947
At time: 290.06845474243164 and batch: 500, loss is 4.1847965526580815 and perplexity is 65.68013710805663
At time: 291.2366099357605 and batch: 550, loss is 4.1815922069549565 and perplexity is 65.4700120793442
At time: 292.4105224609375 and batch: 600, loss is 4.202027974128723 and perplexity is 66.82170641749997
At time: 293.58167457580566 and batch: 650, loss is 4.2152445602416995 and perplexity is 67.71072319704822
At time: 294.7490475177765 and batch: 700, loss is 4.192341904640198 and perplexity is 66.17759123390795
At time: 295.95894503593445 and batch: 750, loss is 4.191826806068421 and perplexity is 66.1435120290081
At time: 297.12586784362793 and batch: 800, loss is 4.219192843437195 and perplexity is 67.97859277208939
At time: 298.30257058143616 and batch: 850, loss is 4.265484495162964 and perplexity is 71.19940738671264
At time: 299.47158789634705 and batch: 900, loss is 4.229426102638245 and perplexity is 68.6778068497216
At time: 300.633549451828 and batch: 950, loss is 4.2183268356323245 and perplexity is 67.91974826376399
At time: 301.79597210884094 and batch: 1000, loss is 4.207534351348877 and perplexity is 67.19066682487495
At time: 302.9571580886841 and batch: 1050, loss is 4.187878165245056 and perplexity is 65.88285002620395
At time: 304.1214463710785 and batch: 1100, loss is 4.170981121063233 and perplexity is 64.7789769582711
At time: 305.2850077152252 and batch: 1150, loss is 4.165981802940369 and perplexity is 64.45593441382796
At time: 306.4507658481598 and batch: 1200, loss is 4.183485813140869 and perplexity is 65.5941039526988
At time: 307.6171588897705 and batch: 1250, loss is 4.200534782409668 and perplexity is 66.72200325531735
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.441647272040374 and perplexity of 84.91470414677477
Finished 10 epochs...
Completing Train Step...
At time: 310.522198677063 and batch: 50, loss is 4.153385334014892 and perplexity is 63.64910947981014
At time: 311.7139296531677 and batch: 100, loss is 4.171833348274231 and perplexity is 64.83420689602075
At time: 312.879266500473 and batch: 150, loss is 4.097533240318298 and perplexity is 60.191626040284696
At time: 314.0450916290283 and batch: 200, loss is 4.146276354789734 and perplexity is 63.19823381453283
At time: 315.21018075942993 and batch: 250, loss is 4.1635936784744265 and perplexity is 64.30218927421299
At time: 316.37603974342346 and batch: 300, loss is 4.175494842529297 and perplexity is 65.07203210421797
At time: 317.54203820228577 and batch: 350, loss is 4.160101704597473 and perplexity is 64.07803929973568
At time: 318.70664620399475 and batch: 400, loss is 4.175643510818482 and perplexity is 65.08170697105983
At time: 319.87285113334656 and batch: 450, loss is 4.10298704624176 and perplexity is 60.52079628634203
At time: 321.03768253326416 and batch: 500, loss is 4.143843288421631 and perplexity is 63.04465522652687
At time: 322.20233488082886 and batch: 550, loss is 4.141276016235351 and perplexity is 62.883010019074874
At time: 323.3674886226654 and batch: 600, loss is 4.162609686851502 and perplexity is 64.23894757838364
At time: 324.531653881073 and batch: 650, loss is 4.176818904876709 and perplexity is 65.15824859720699
At time: 325.723726272583 and batch: 700, loss is 4.152503447532654 and perplexity is 63.593002933991826
At time: 326.8899154663086 and batch: 750, loss is 4.152922296524048 and perplexity is 63.61964437811198
At time: 328.0558798313141 and batch: 800, loss is 4.18163957118988 and perplexity is 65.47311308981469
At time: 329.2210371494293 and batch: 850, loss is 4.227286033630371 and perplexity is 68.53098876020962
At time: 330.38718724250793 and batch: 900, loss is 4.19097553730011 and perplexity is 66.08723008192207
At time: 331.5517690181732 and batch: 950, loss is 4.179992599487305 and perplexity is 65.36536947497332
At time: 332.717324256897 and batch: 1000, loss is 4.169615116119385 and perplexity is 64.69054896577649
At time: 333.8832757472992 and batch: 1050, loss is 4.149778542518615 and perplexity is 63.41995391957836
At time: 335.0520532131195 and batch: 1100, loss is 4.132496013641357 and perplexity is 62.33331372669061
At time: 336.2226412296295 and batch: 1150, loss is 4.128644623756409 and perplexity is 62.09370554115906
At time: 337.38810205459595 and batch: 1200, loss is 4.1451812267303465 and perplexity is 63.12906153854222
At time: 338.5527081489563 and batch: 1250, loss is 4.163126716613769 and perplexity is 64.2721696138289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.439312788691834 and perplexity of 84.71670338855668
Finished 11 epochs...
Completing Train Step...
At time: 341.4675381183624 and batch: 50, loss is 4.116072268486023 and perplexity is 61.317928307058146
At time: 342.6548638343811 and batch: 100, loss is 4.133853845596313 and perplexity is 62.418009380146536
At time: 343.82649874687195 and batch: 150, loss is 4.060502796173096 and perplexity is 58.003467670004625
At time: 344.98777890205383 and batch: 200, loss is 4.108917288780212 and perplexity is 60.880765584620164
At time: 346.149307012558 and batch: 250, loss is 4.12609534740448 and perplexity is 61.935613122269494
At time: 347.31896209716797 and batch: 300, loss is 4.138906922340393 and perplexity is 62.734210593443535
At time: 348.48008394241333 and batch: 350, loss is 4.123480281829834 and perplexity is 61.773859023517
At time: 349.6497983932495 and batch: 400, loss is 4.139886856079102 and perplexity is 62.795716093710894
At time: 350.8124830722809 and batch: 450, loss is 4.066264762878418 and perplexity is 58.338646436624515
At time: 351.97326278686523 and batch: 500, loss is 4.107796392440796 and perplexity is 60.812562788608574
At time: 353.1354229450226 and batch: 550, loss is 4.105239143371582 and perplexity is 60.65724859220306
At time: 354.2969446182251 and batch: 600, loss is 4.128126792907715 and perplexity is 62.061559828659234
At time: 355.48670721054077 and batch: 650, loss is 4.142362728118896 and perplexity is 62.951382877405806
At time: 356.64956974983215 and batch: 700, loss is 4.116473984718323 and perplexity is 61.342565662471586
At time: 357.810839176178 and batch: 750, loss is 4.117498650550842 and perplexity is 61.405453507595695
At time: 358.97264432907104 and batch: 800, loss is 4.147596397399902 and perplexity is 63.28171326214736
At time: 360.1344726085663 and batch: 850, loss is 4.193667011260986 and perplexity is 66.26534172473787
At time: 361.29515957832336 and batch: 900, loss is 4.155887207984924 and perplexity is 63.80855089803345
At time: 362.4561688899994 and batch: 950, loss is 4.145165014266968 and perplexity is 63.128038069240375
At time: 363.61791491508484 and batch: 1000, loss is 4.135530662536621 and perplexity is 62.52276075555162
At time: 364.7867338657379 and batch: 1050, loss is 4.116426796913147 and perplexity is 61.33967110972843
At time: 365.9475247859955 and batch: 1100, loss is 4.098273491859436 and perplexity is 60.236199479967894
At time: 367.1098871231079 and batch: 1150, loss is 4.094625267982483 and perplexity is 60.01684470971586
At time: 368.2715539932251 and batch: 1200, loss is 4.111171970367431 and perplexity is 61.018187188563424
At time: 369.4326138496399 and batch: 1250, loss is 4.1292688226699825 and perplexity is 62.132476463822684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.438554972627737 and perplexity of 84.6525280294618
Finished 12 epochs...
Completing Train Step...
At time: 372.3556730747223 and batch: 50, loss is 4.081900877952576 and perplexity is 59.25800509351074
At time: 373.5202968120575 and batch: 100, loss is 4.099853157997131 and perplexity is 60.33142775919161
At time: 374.68494153022766 and batch: 150, loss is 4.02678729057312 and perplexity is 56.08045132506381
At time: 375.8494288921356 and batch: 200, loss is 4.075657205581665 and perplexity is 58.88917016478396
At time: 377.01477456092834 and batch: 250, loss is 4.093015260696411 and perplexity is 59.92029489625351
At time: 378.1792209148407 and batch: 300, loss is 4.106145615577698 and perplexity is 60.712257630466766
At time: 379.3451211452484 and batch: 350, loss is 4.088888592720032 and perplexity is 59.67353323607362
At time: 380.5108530521393 and batch: 400, loss is 4.1071726417541505 and perplexity is 60.77464273837065
At time: 381.67531657218933 and batch: 450, loss is 4.0335390090942385 and perplexity is 56.460371861966415
At time: 382.85299706459045 and batch: 500, loss is 4.075271158218384 and perplexity is 58.866440543553495
At time: 384.066965341568 and batch: 550, loss is 4.072645053863526 and perplexity is 58.71205393408707
At time: 385.23303866386414 and batch: 600, loss is 4.096770167350769 and perplexity is 60.14571295732583
At time: 386.397953748703 and batch: 650, loss is 4.11225697517395 and perplexity is 61.084428144327326
At time: 387.56212186813354 and batch: 700, loss is 4.083699088096619 and perplexity is 59.364659303987864
At time: 388.72623205184937 and batch: 750, loss is 4.087185730934143 and perplexity is 59.57200392642175
At time: 389.89139890670776 and batch: 800, loss is 4.115864877700806 and perplexity is 61.3052128523383
At time: 391.0553252696991 and batch: 850, loss is 4.164092965126038 and perplexity is 64.33430251517615
At time: 392.22039127349854 and batch: 900, loss is 4.1254481601715085 and perplexity is 61.89554215229926
At time: 393.38552951812744 and batch: 950, loss is 4.113524174690246 and perplexity is 61.161883367370876
At time: 394.54998803138733 and batch: 1000, loss is 4.104716935157776 and perplexity is 60.62558114798071
At time: 395.7228105068207 and batch: 1050, loss is 4.087756719589233 and perplexity is 59.60602857774525
At time: 396.88738894462585 and batch: 1100, loss is 4.067823982238769 and perplexity is 58.42968013591523
At time: 398.05131936073303 and batch: 1150, loss is 4.064095449447632 and perplexity is 58.21222879669373
At time: 399.2239463329315 and batch: 1200, loss is 4.079806342124939 and perplexity is 59.13401697287733
At time: 400.38896560668945 and batch: 1250, loss is 4.09821964263916 and perplexity is 60.23295589492657
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.437674195226962 and perplexity of 84.57800082161316
Finished 13 epochs...
Completing Train Step...
At time: 403.28745889663696 and batch: 50, loss is 4.050353398323059 and perplexity is 57.41774479513114
At time: 404.4713237285614 and batch: 100, loss is 4.068940916061401 and perplexity is 58.49497868219351
At time: 405.632203578949 and batch: 150, loss is 3.996418309211731 and perplexity is 54.40294613080463
At time: 406.794203042984 and batch: 200, loss is 4.046343841552734 and perplexity is 57.1879860110058
At time: 407.9556670188904 and batch: 250, loss is 4.06343035697937 and perplexity is 58.17352515393765
At time: 409.1166572570801 and batch: 300, loss is 4.075775046348571 and perplexity is 58.896110118655365
At time: 410.27680492401123 and batch: 350, loss is 4.058208065032959 and perplexity is 57.87051790682762
At time: 411.43765902519226 and batch: 400, loss is 4.077633872032165 and perplexity is 59.00568953377143
At time: 412.59827303886414 and batch: 450, loss is 4.003855381011963 and perplexity is 54.80905299847653
At time: 413.78500056266785 and batch: 500, loss is 4.046079654693603 and perplexity is 57.172879692134835
At time: 414.94772815704346 and batch: 550, loss is 4.043806171417236 and perplexity is 57.04304574986425
At time: 416.1088910102844 and batch: 600, loss is 4.067221078872681 and perplexity is 58.39446330232233
At time: 417.2696678638458 and batch: 650, loss is 4.085096545219422 and perplexity is 59.44767686320437
At time: 418.43090057373047 and batch: 700, loss is 4.05354480266571 and perplexity is 57.60128074815726
At time: 419.5922210216522 and batch: 750, loss is 4.05952778339386 and perplexity is 57.94694110932397
At time: 420.7535388469696 and batch: 800, loss is 4.087424850463867 and perplexity is 59.58625045922956
At time: 421.921174287796 and batch: 850, loss is 4.136244659423828 and perplexity is 62.56741775269199
At time: 423.0920686721802 and batch: 900, loss is 4.096170558929443 and perplexity is 60.10965989127115
At time: 424.26393151283264 and batch: 950, loss is 4.085982813835144 and perplexity is 59.500386827629235
At time: 425.42567253112793 and batch: 1000, loss is 4.076237783432007 and perplexity is 58.92336983943281
At time: 426.58750200271606 and batch: 1050, loss is 4.05968936920166 and perplexity is 57.956305269149944
At time: 427.7486300468445 and batch: 1100, loss is 4.039279766082764 and perplexity is 56.7854292814105
At time: 428.9105713367462 and batch: 1150, loss is 4.036284499168396 and perplexity is 56.61559623848608
At time: 430.0711245536804 and batch: 1200, loss is 4.051851344108582 and perplexity is 57.503817914299724
At time: 431.2404103279114 and batch: 1250, loss is 4.0696902227401734 and perplexity is 58.538825785806964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.439382734089873 and perplexity of 84.72262913933281
Annealing...
Finished 14 epochs...
Completing Train Step...
At time: 434.1833481788635 and batch: 50, loss is 4.031139240264893 and perplexity is 56.3250424660574
At time: 435.3455533981323 and batch: 100, loss is 4.047888355255127 and perplexity is 57.2763818857594
At time: 436.50792837142944 and batch: 150, loss is 3.973045339584351 and perplexity is 53.14613267013356
At time: 437.6709237098694 and batch: 200, loss is 4.0215314722061155 and perplexity is 55.786475876462774
At time: 438.83332562446594 and batch: 250, loss is 4.035901250839234 and perplexity is 56.59390256312112
At time: 439.9948925971985 and batch: 300, loss is 4.038890118598938 and perplexity is 56.763307291941686
At time: 441.1634120941162 and batch: 350, loss is 4.019357299804687 and perplexity is 55.665318216785444
At time: 442.3249430656433 and batch: 400, loss is 4.036313519477845 and perplexity is 56.617239264449
At time: 443.51377868652344 and batch: 450, loss is 3.9611839151382444 and perplexity is 52.519467749452645
At time: 444.6753215789795 and batch: 500, loss is 4.00092839717865 and perplexity is 54.648862338532645
At time: 445.8483271598816 and batch: 550, loss is 3.9944693517684935 and perplexity is 54.29702035994806
At time: 447.014511346817 and batch: 600, loss is 4.010044617652893 and perplexity is 55.149331142284964
At time: 448.1745355129242 and batch: 650, loss is 4.025781192779541 and perplexity is 56.02405728044168
At time: 449.33464193344116 and batch: 700, loss is 3.992826404571533 and perplexity is 54.2078864637366
At time: 450.4939224720001 and batch: 750, loss is 3.9969667339324952 and perplexity is 54.43279023421919
At time: 451.65731620788574 and batch: 800, loss is 4.0214089679718015 and perplexity is 55.7796422155352
At time: 452.8239607810974 and batch: 850, loss is 4.068519110679627 and perplexity is 58.47031038835386
At time: 453.99172472953796 and batch: 900, loss is 4.023399415016175 and perplexity is 55.89077920897675
At time: 455.15980863571167 and batch: 950, loss is 4.010120544433594 and perplexity is 55.15351861242496
At time: 456.3308515548706 and batch: 1000, loss is 3.9935239696502687 and perplexity is 54.24571318409148
At time: 457.5352292060852 and batch: 1050, loss is 3.976725125312805 and perplexity is 53.34205931360494
At time: 458.7200119495392 and batch: 1100, loss is 3.951300439834595 and perplexity is 52.0029495888161
At time: 459.892290353775 and batch: 1150, loss is 3.944581775665283 and perplexity is 51.654730328744854
At time: 461.066606760025 and batch: 1200, loss is 3.9620302629470827 and perplexity is 52.563936301179986
At time: 462.2368302345276 and batch: 1250, loss is 3.980946869850159 and perplexity is 53.567731891928254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.392340667056342 and perplexity of 80.82939242827834
Finished 15 epochs...
Completing Train Step...
At time: 465.3876464366913 and batch: 50, loss is 3.988504996299744 and perplexity is 53.97413748093325
At time: 466.63211941719055 and batch: 100, loss is 4.003720149993897 and perplexity is 54.801641615576024
At time: 467.8011953830719 and batch: 150, loss is 3.9312776136398315 and perplexity is 50.97205868357741
At time: 468.9832456111908 and batch: 200, loss is 3.9829830884933473 and perplexity is 53.67691863259267
At time: 470.1808259487152 and batch: 250, loss is 3.9990733671188354 and perplexity is 54.54758102514398
At time: 471.3422122001648 and batch: 300, loss is 4.005238437652588 and perplexity is 54.8849094679976
At time: 472.5039608478546 and batch: 350, loss is 3.9865609121322634 and perplexity is 53.86930914537312
At time: 473.83494567871094 and batch: 400, loss is 4.005451602935791 and perplexity is 54.89661027232623
At time: 474.995728969574 and batch: 450, loss is 3.9316401529312133 and perplexity is 50.990541407767246
At time: 476.15715622901917 and batch: 500, loss is 3.973840870857239 and perplexity is 53.18842890246281
At time: 477.31949377059937 and batch: 550, loss is 3.9685454511642457 and perplexity is 52.907518274876594
At time: 478.48071002960205 and batch: 600, loss is 3.9866944408416747 and perplexity is 53.87650272496419
At time: 479.6426463127136 and batch: 650, loss is 4.004372425079346 and perplexity is 54.837399021608334
At time: 480.80419278144836 and batch: 700, loss is 3.9712447214126585 and perplexity is 53.0505228820858
At time: 481.9659435749054 and batch: 750, loss is 3.9772268867492677 and perplexity is 53.36883101785272
At time: 483.126656293869 and batch: 800, loss is 4.003282632827759 and perplexity is 54.77767020097174
At time: 484.2973930835724 and batch: 850, loss is 4.052034978866577 and perplexity is 57.51437858360952
At time: 485.4586269855499 and batch: 900, loss is 4.007517490386963 and perplexity is 55.01013771768518
At time: 486.6202974319458 and batch: 950, loss is 3.995908498764038 and perplexity is 54.37521800912549
At time: 487.78195357322693 and batch: 1000, loss is 3.9811097764968872 and perplexity is 53.576459142347865
At time: 488.9436752796173 and batch: 1050, loss is 3.966654815673828 and perplexity is 52.80758394258183
At time: 490.1047112941742 and batch: 1100, loss is 3.941424298286438 and perplexity is 51.491888905566
At time: 491.265825510025 and batch: 1150, loss is 3.9359660816192625 and perplexity is 51.21160065219037
At time: 492.42735600471497 and batch: 1200, loss is 3.955848135948181 and perplexity is 52.23998176719867
At time: 493.5885102748871 and batch: 1250, loss is 3.97435866355896 and perplexity is 53.21597661415106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.392639605668339 and perplexity of 80.85355906665018
Annealing...
Finished 16 epochs...
Completing Train Step...
At time: 496.5427145957947 and batch: 50, loss is 3.9796939086914063 and perplexity is 53.500655635252656
At time: 497.7322931289673 and batch: 100, loss is 3.996438503265381 and perplexity is 54.40404475791033
At time: 498.8939940929413 and batch: 150, loss is 3.9265089511871336 and perplexity is 50.729568776973764
At time: 500.0557641983032 and batch: 200, loss is 3.977884135246277 and perplexity is 53.40391913136817
At time: 501.21659207344055 and batch: 250, loss is 3.99231388092041 and perplexity is 54.180110758307535
At time: 502.4244649410248 and batch: 300, loss is 3.9936849641799927 and perplexity is 54.25444715021661
At time: 503.5856738090515 and batch: 350, loss is 3.9738566732406615 and perplexity is 53.189269413051
At time: 504.74724555015564 and batch: 400, loss is 3.991103286743164 and perplexity is 54.1145603171977
At time: 505.9089801311493 and batch: 450, loss is 3.917932696342468 and perplexity is 50.2963593793668
At time: 507.07025051116943 and batch: 500, loss is 3.9564773082733153 and perplexity is 52.27286005996346
At time: 508.2315490245819 and batch: 550, loss is 3.949505524635315 and perplexity is 51.90969242358079
At time: 509.3920421600342 and batch: 600, loss is 3.964809226989746 and perplexity is 52.71021274445318
At time: 510.553249835968 and batch: 650, loss is 3.980421805381775 and perplexity is 53.539612762083884
At time: 511.7154438495636 and batch: 700, loss is 3.9449041175842283 and perplexity is 51.671383497504614
At time: 512.8775038719177 and batch: 750, loss is 3.9517371702194213 and perplexity is 52.02566581707463
At time: 514.0394639968872 and batch: 800, loss is 3.9753655862808226 and perplexity is 53.26958797688993
At time: 515.2011799812317 and batch: 850, loss is 4.023803906440735 and perplexity is 55.913391122733074
At time: 516.362056016922 and batch: 900, loss is 3.9777586174011232 and perplexity is 53.39721640718059
At time: 517.5234642028809 and batch: 950, loss is 3.9637256193161012 and perplexity is 52.653126488582224
At time: 518.6855111122131 and batch: 1000, loss is 3.9452008962631226 and perplexity is 51.68672073820609
At time: 519.8477017879486 and batch: 1050, loss is 3.9309326887130736 and perplexity is 50.954480181775274
At time: 521.0094323158264 and batch: 1100, loss is 3.9031859827041626 and perplexity is 49.560095449837945
At time: 522.1707465648651 and batch: 1150, loss is 3.8945074605941774 and perplexity is 49.13184803089726
At time: 523.3316757678986 and batch: 1200, loss is 3.9184589338302613 and perplexity is 50.32283417457564
At time: 524.4933052062988 and batch: 1250, loss is 3.935691223144531 and perplexity is 51.19752664401537
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.371585149834626 and perplexity of 79.16902705824056
Finished 17 epochs...
Completing Train Step...
At time: 527.4580631256104 and batch: 50, loss is 3.961324396133423 and perplexity is 52.526846254806074
At time: 528.6222043037415 and batch: 100, loss is 3.9721776962280275 and perplexity is 53.100040779760796
At time: 529.7952706813812 and batch: 150, loss is 3.9018025016784668 and perplexity is 49.49157740578709
At time: 530.9591445922852 and batch: 200, loss is 3.9528423357009888 and perplexity is 52.083194570632116
At time: 532.1512053012848 and batch: 250, loss is 3.968673601150513 and perplexity is 52.91429880707022
At time: 533.3154776096344 and batch: 300, loss is 3.972841639518738 and perplexity is 53.135307901962
At time: 534.6684737205505 and batch: 350, loss is 3.9532898092269897 and perplexity is 52.10650563650548
At time: 535.8323647975922 and batch: 400, loss is 3.9726291131973266 and perplexity is 53.124016450344385
At time: 536.9973902702332 and batch: 450, loss is 3.9003718185424803 and perplexity is 49.42082126749513
At time: 538.1616003513336 and batch: 500, loss is 3.9395814990997313 and perplexity is 51.39708707176815
At time: 539.3339173793793 and batch: 550, loss is 3.933754334449768 and perplexity is 51.09845870621453
At time: 540.4982130527496 and batch: 600, loss is 3.951040687561035 and perplexity is 51.98944345862855
At time: 541.6629538536072 and batch: 650, loss is 3.968067417144775 and perplexity is 52.88223272541338
At time: 542.828971862793 and batch: 700, loss is 3.933215270042419 and perplexity is 51.070920768883425
At time: 544.0065531730652 and batch: 750, loss is 3.9410106563568115 and perplexity is 51.47059410579366
At time: 545.1836221218109 and batch: 800, loss is 3.965827169418335 and perplexity is 52.76389602503463
At time: 546.3489058017731 and batch: 850, loss is 4.015235042572021 and perplexity is 55.43632376744538
At time: 547.5146970748901 and batch: 900, loss is 3.970298228263855 and perplexity is 53.000334680778565
At time: 548.6797833442688 and batch: 950, loss is 3.9566773128509523 and perplexity is 52.28331591683639
At time: 549.8438727855682 and batch: 1000, loss is 3.9397094154357912 and perplexity is 51.40366201934316
At time: 551.0088574886322 and batch: 1050, loss is 3.927175612449646 and perplexity is 50.7633994909016
At time: 552.173394203186 and batch: 1100, loss is 3.9002758121490477 and perplexity is 49.41607678043895
At time: 553.3385815620422 and batch: 1150, loss is 3.892367124557495 and perplexity is 49.02680182320158
At time: 554.5033676624298 and batch: 1200, loss is 3.9172498559951783 and perplexity is 50.26202671905765
At time: 555.6761002540588 and batch: 1250, loss is 3.9345757150650025 and perplexity is 51.14044723158389
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.370951631643476 and perplexity of 79.11888792312752
Finished 18 epochs...
Completing Train Step...
At time: 558.6529219150543 and batch: 50, loss is 3.9482367610931397 and perplexity is 51.84387306178098
At time: 559.8138146400452 and batch: 100, loss is 3.9587750482559203 and perplexity is 52.393107596409514
At time: 560.9838736057281 and batch: 150, loss is 3.888650484085083 and perplexity is 48.844925022014294
At time: 562.1717803478241 and batch: 200, loss is 3.939932413101196 and perplexity is 51.41512619416134
At time: 563.3376431465149 and batch: 250, loss is 3.955930247306824 and perplexity is 52.24427143919003
At time: 564.500214099884 and batch: 300, loss is 3.9610707759857178 and perplexity is 52.51352607750452
At time: 565.6615240573883 and batch: 350, loss is 3.9413109016418457 and perplexity is 51.48605022918981
At time: 566.8225438594818 and batch: 400, loss is 3.961317639350891 and perplexity is 52.52649134352789
At time: 567.9841101169586 and batch: 450, loss is 3.88974817276001 and perplexity is 48.8985709809328
At time: 569.1454312801361 and batch: 500, loss is 3.929160141944885 and perplexity is 50.86424098283162
At time: 570.305682182312 and batch: 550, loss is 3.9238179874420167 and perplexity is 50.59324085560222
At time: 571.4661982059479 and batch: 600, loss is 3.941861181259155 and perplexity is 51.514389749821866
At time: 572.6270067691803 and batch: 650, loss is 3.9592244911193846 and perplexity is 52.41666059718102
At time: 573.7878651618958 and batch: 700, loss is 3.9245566606521605 and perplexity is 50.630626533436015
At time: 574.9493200778961 and batch: 750, loss is 3.9328078269958495 and perplexity is 51.05011651589615
At time: 576.1099467277527 and batch: 800, loss is 3.9582080173492433 and perplexity is 52.363407506335776
At time: 577.2712438106537 and batch: 850, loss is 4.00814428806305 and perplexity is 55.04462875248883
At time: 578.4323630332947 and batch: 900, loss is 3.963733129501343 and perplexity is 52.65352192480061
At time: 579.5947527885437 and batch: 950, loss is 3.950082621574402 and perplexity is 51.93965799387518
At time: 580.756245136261 and batch: 1000, loss is 3.9338084983825685 and perplexity is 51.10122647465405
At time: 581.9175071716309 and batch: 1050, loss is 3.9223070192337035 and perplexity is 50.516853800852296
At time: 583.0786633491516 and batch: 1100, loss is 3.8956925296783447 and perplexity is 49.190107178785986
At time: 584.238997220993 and batch: 1150, loss is 3.8881773567199707 and perplexity is 48.821820617433545
At time: 585.3999171257019 and batch: 1200, loss is 3.913537611961365 and perplexity is 50.07578770645032
At time: 586.5610673427582 and batch: 1250, loss is 3.9309183835983275 and perplexity is 50.95375127730299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.371013112311815 and perplexity of 79.12375235476803
Annealing...
Finished 19 epochs...
Completing Train Step...
At time: 589.5087089538574 and batch: 50, loss is 3.9486814832687376 and perplexity is 51.86693430934445
At time: 590.6730587482452 and batch: 100, loss is 3.9620141077041624 and perplexity is 52.56308712487953
At time: 591.8370013237 and batch: 150, loss is 3.8947842597961424 and perplexity is 49.14544956958396
At time: 593.000821352005 and batch: 200, loss is 3.9463481807708742 and perplexity is 51.7460541418047
At time: 594.1647465229034 and batch: 250, loss is 3.9609794139862062 and perplexity is 52.508728555919596
At time: 595.3299283981323 and batch: 300, loss is 3.9639368152618406 and perplexity is 52.66424778977264
At time: 596.4946060180664 and batch: 350, loss is 3.942102494239807 and perplexity is 51.52682234077132
At time: 597.6597592830658 and batch: 400, loss is 3.9620433759689333 and perplexity is 52.56462557774449
At time: 598.8244478702545 and batch: 450, loss is 3.891382155418396 and perplexity is 48.97853571063484
At time: 599.9889304637909 and batch: 500, loss is 3.925360541343689 and perplexity is 50.67134388025798
At time: 601.1622822284698 and batch: 550, loss is 3.919044909477234 and perplexity is 50.352330771188214
At time: 602.3257791996002 and batch: 600, loss is 3.93617299079895 and perplexity is 51.22219789876779
At time: 603.4906504154205 and batch: 650, loss is 3.95273775100708 and perplexity is 52.077747750502134
At time: 604.6564769744873 and batch: 700, loss is 3.915789465904236 and perplexity is 50.18867812510247
At time: 605.8374054431915 and batch: 750, loss is 3.9237561082839965 and perplexity is 50.590110285316086
At time: 607.0147964954376 and batch: 800, loss is 3.9471377611160277 and perplexity is 51.78692794354603
At time: 608.1795663833618 and batch: 850, loss is 3.9979601812362673 and perplexity is 54.486893212693516
At time: 609.3456065654755 and batch: 900, loss is 3.95375937461853 and perplexity is 52.13097879365063
At time: 610.5101127624512 and batch: 950, loss is 3.937702651023865 and perplexity is 51.30061041448994
At time: 611.674233675003 and batch: 1000, loss is 3.918867926597595 and perplexity is 50.34342005923679
At time: 612.8412833213806 and batch: 1050, loss is 3.9076898956298827 and perplexity is 49.783813228852246
At time: 614.0632932186127 and batch: 1100, loss is 3.878623013496399 and perplexity is 48.35758146805928
At time: 615.227520942688 and batch: 1150, loss is 3.8687419939041137 and perplexity is 47.882112187290005
At time: 616.3916299343109 and batch: 1200, loss is 3.8957502317428587 and perplexity is 49.19294563141537
At time: 617.5567922592163 and batch: 1250, loss is 3.913447208404541 and perplexity is 50.0712608817545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.361248378335994 and perplexity of 78.35488993957395
Finished 20 epochs...
Completing Train Step...
At time: 620.4571855068207 and batch: 50, loss is 3.9413578510284424 and perplexity is 51.48846752441118
At time: 621.6410994529724 and batch: 100, loss is 3.9498348522186277 and perplexity is 51.926790532422054
At time: 622.802205324173 and batch: 150, loss is 3.881262402534485 and perplexity is 48.48538452529492
At time: 623.963005065918 and batch: 200, loss is 3.932281560897827 and perplexity is 51.023257638351964
At time: 625.1243255138397 and batch: 250, loss is 3.9468182706832886 and perplexity is 51.77038515829865
At time: 626.2847650051117 and batch: 300, loss is 3.951476426124573 and perplexity is 52.01210220032561
At time: 627.4455575942993 and batch: 350, loss is 3.930230860710144 and perplexity is 50.918731446907955
At time: 628.6134858131409 and batch: 400, loss is 3.9519563627243044 and perplexity is 52.03707070297038
At time: 629.7768468856812 and batch: 450, loss is 3.881569867134094 and perplexity is 48.5002943566401
At time: 630.9383172988892 and batch: 500, loss is 3.915795335769653 and perplexity is 50.18897272675317
At time: 632.0998408794403 and batch: 550, loss is 3.9104581308364867 and perplexity is 49.92181745930742
At time: 633.2605113983154 and batch: 600, loss is 3.928537616729736 and perplexity is 50.832586564129564
At time: 634.4213523864746 and batch: 650, loss is 3.9460625982284547 and perplexity is 51.73127848203837
At time: 635.582236289978 and batch: 700, loss is 3.9094555139541627 and perplexity is 49.87179008566191
At time: 636.7436826229095 and batch: 750, loss is 3.9185214042663574 and perplexity is 50.32597796216799
At time: 637.9055445194244 and batch: 800, loss is 3.9427059507369995 and perplexity is 51.55792592037725
At time: 639.0673830509186 and batch: 850, loss is 3.9941367292404175 and perplexity is 54.27896295108592
At time: 640.228401184082 and batch: 900, loss is 3.9505357456207277 and perplexity is 51.9631984348373
At time: 641.3907630443573 and batch: 950, loss is 3.93498037815094 and perplexity is 51.161146070522406
At time: 642.5991730690002 and batch: 1000, loss is 3.917068295478821 and perplexity is 50.25290194790752
At time: 643.7598657608032 and batch: 1050, loss is 3.9066718435287475 and perplexity is 49.73315650316029
At time: 644.936594247818 and batch: 1100, loss is 3.878427758216858 and perplexity is 48.34814031671913
At time: 646.0997231006622 and batch: 1150, loss is 3.8687381172180175 and perplexity is 47.881926563731234
At time: 647.2601697444916 and batch: 1200, loss is 3.8959031867980958 and perplexity is 49.20047051660164
At time: 648.4212861061096 and batch: 1250, loss is 3.9136979007720947 and perplexity is 50.08381493822867
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.360829151459854 and perplexity of 78.32204834835413
Finished 21 epochs...
Completing Train Step...
At time: 651.3228204250336 and batch: 50, loss is 3.9342385005950926 and perplexity is 51.12320484013508
At time: 652.511109828949 and batch: 100, loss is 3.9424177408218384 and perplexity is 51.54306855604455
At time: 653.6728417873383 and batch: 150, loss is 3.873971257209778 and perplexity is 48.13315617503501
At time: 654.8334720134735 and batch: 200, loss is 3.9251759815216065 and perplexity is 50.66199284898564
At time: 655.9944624900818 and batch: 250, loss is 3.939547004699707 and perplexity is 51.39531419066402
At time: 657.1564373970032 and batch: 300, loss is 3.94485324382782 and perplexity is 51.668754846992506
At time: 658.3182775974274 and batch: 350, loss is 3.923594841957092 and perplexity is 50.581952461861476
At time: 659.481582403183 and batch: 400, loss is 3.945918502807617 and perplexity is 51.72382477873019
At time: 660.6441721916199 and batch: 450, loss is 3.8755767488479616 and perplexity is 48.21049562209866
At time: 661.8052616119385 and batch: 500, loss is 3.9099523496627806 and perplexity is 49.89657432816768
At time: 662.9668953418732 and batch: 550, loss is 3.9051839017868044 and perplexity is 49.659211490219
At time: 664.1326475143433 and batch: 600, loss is 3.9235223722457886 and perplexity is 50.57828693519083
At time: 665.2969992160797 and batch: 650, loss is 3.9414546251297 and perplexity is 51.49345051568933
At time: 666.4591748714447 and batch: 700, loss is 3.9049678230285645 and perplexity is 49.64848234867655
At time: 667.62096118927 and batch: 750, loss is 3.914432530403137 and perplexity is 50.120621510661984
At time: 668.7821559906006 and batch: 800, loss is 3.938935647010803 and perplexity is 51.363902872913584
At time: 669.9440219402313 and batch: 850, loss is 3.9906990766525268 and perplexity is 54.0926910860468
At time: 671.1050305366516 and batch: 900, loss is 3.9473817205429076 and perplexity is 51.79956339401298
At time: 672.2929198741913 and batch: 950, loss is 3.9320507574081422 and perplexity is 51.01148265134027
At time: 673.456549167633 and batch: 1000, loss is 3.914581227302551 and perplexity is 50.128074845807504
At time: 674.6167669296265 and batch: 1050, loss is 3.904610900878906 and perplexity is 49.630764867698176
At time: 675.7785797119141 and batch: 1100, loss is 3.8766410112380982 and perplexity is 48.26183155200335
At time: 676.9401125907898 and batch: 1150, loss is 3.867274761199951 and perplexity is 47.81190950077742
At time: 678.1019651889801 and batch: 1200, loss is 3.894359393119812 and perplexity is 49.124573740804585
At time: 679.2630224227905 and batch: 1250, loss is 3.9123298740386963 and perplexity is 50.015345784976944
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.360838061701642 and perplexity of 78.32274621985134
Annealing...
Finished 22 epochs...
Completing Train Step...
At time: 682.1817667484283 and batch: 50, loss is 3.936359906196594 and perplexity is 51.23177301109633
At time: 683.3428318500519 and batch: 100, loss is 3.9494328022003176 and perplexity is 51.90591756160802
At time: 684.5042924880981 and batch: 150, loss is 3.882769045829773 and perplexity is 48.55848976273843
At time: 685.6658511161804 and batch: 200, loss is 3.9378679609298706 and perplexity is 51.309091614569475
At time: 686.8273885250092 and batch: 250, loss is 3.9501277875900267 and perplexity is 51.942003954258126
At time: 687.9894368648529 and batch: 300, loss is 3.954276623725891 and perplexity is 52.157950470834116
At time: 689.150242805481 and batch: 350, loss is 3.9303784704208375 and perplexity is 50.92624810087763
At time: 690.3109245300293 and batch: 400, loss is 3.95107120513916 and perplexity is 51.99103007474069
At time: 691.4717373847961 and batch: 450, loss is 3.880900206565857 and perplexity is 48.46782649439844
At time: 692.6328873634338 and batch: 500, loss is 3.9117790174484255 and perplexity is 49.9878020891463
At time: 693.7947332859039 and batch: 550, loss is 3.9069000339508055 and perplexity is 49.74450642805584
At time: 694.9560401439667 and batch: 600, loss is 3.9254506063461303 and perplexity is 50.675907800489924
At time: 696.1174774169922 and batch: 650, loss is 3.9437826919555663 and perplexity is 51.613470362488286
At time: 697.2789738178253 and batch: 700, loss is 3.906451230049133 and perplexity is 49.72218590862693
At time: 698.4396326541901 and batch: 750, loss is 3.9135045289993284 and perplexity is 50.07413107846991
At time: 699.601763010025 and batch: 800, loss is 3.935440878868103 and perplexity is 51.184711240451165
At time: 700.7628319263458 and batch: 850, loss is 3.9874891901016234 and perplexity is 53.919338055052265
At time: 701.9717075824738 and batch: 900, loss is 3.943434977531433 and perplexity is 51.59552673417366
At time: 703.1329171657562 and batch: 950, loss is 3.9288566493988037 and perplexity is 50.84880640708906
At time: 704.2941429615021 and batch: 1000, loss is 3.9108948135375976 and perplexity is 49.94362221393298
At time: 705.4556031227112 and batch: 1050, loss is 3.9013312244415284 and perplexity is 49.468258647167225
At time: 706.6177299022675 and batch: 1100, loss is 3.869897084236145 and perplexity is 47.9374523074206
At time: 707.7804634571075 and batch: 1150, loss is 3.8598225498199463 and perplexity is 47.45692938097054
At time: 708.942962884903 and batch: 1200, loss is 3.884970440864563 and perplexity is 48.66550392800794
At time: 710.1041369438171 and batch: 1250, loss is 3.904383635520935 and perplexity is 49.61948679576022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.354630296247719 and perplexity of 77.83804300011961
Finished 23 epochs...
Completing Train Step...
At time: 713.0068635940552 and batch: 50, loss is 3.932063965797424 and perplexity is 51.012156435310764
At time: 714.1921808719635 and batch: 100, loss is 3.9421877098083495 and perplexity is 51.53121341532358
At time: 715.3538401126862 and batch: 150, loss is 3.874185004234314 and perplexity is 48.143445593576075
At time: 716.5152044296265 and batch: 200, loss is 3.9287748908996583 and perplexity is 50.844649254937444
At time: 717.6759288311005 and batch: 250, loss is 3.9404387044906617 and perplexity is 51.44116382059857
At time: 718.8540065288544 and batch: 300, loss is 3.9459115266799927 and perplexity is 51.723463947985906
At time: 720.0254507064819 and batch: 350, loss is 3.9226187086105346 and perplexity is 50.53260182165085
At time: 721.1880095005035 and batch: 400, loss is 3.9447219371795654 and perplexity is 51.66197084137634
At time: 722.349680185318 and batch: 450, loss is 3.8748042488098147 and perplexity is 48.17326739364681
At time: 723.5128095149994 and batch: 500, loss is 3.906256327629089 and perplexity is 49.712495878599285
At time: 724.6782131195068 and batch: 550, loss is 3.901953725814819 and perplexity is 49.49906229277052
At time: 725.8414652347565 and batch: 600, loss is 3.920928468704224 and perplexity is 50.447261744401686
At time: 727.0056853294373 and batch: 650, loss is 3.939822850227356 and perplexity is 51.40949331375957
At time: 728.168860912323 and batch: 700, loss is 3.902866554260254 and perplexity is 49.54426707381979
At time: 729.3321797847748 and batch: 750, loss is 3.91078950881958 and perplexity is 49.93836319178376
At time: 730.540988445282 and batch: 800, loss is 3.9332932949066164 and perplexity is 51.07490572600169
At time: 731.7027146816254 and batch: 850, loss is 3.985717453956604 and perplexity is 53.82389179266863
At time: 732.8633253574371 and batch: 900, loss is 3.9420965242385866 and perplexity is 51.52651472649729
At time: 734.0246939659119 and batch: 950, loss is 3.9281504201889037 and perplexity is 50.81290817239883
At time: 735.1861071586609 and batch: 1000, loss is 3.910648808479309 and perplexity is 49.93133734137145
At time: 736.3486361503601 and batch: 1050, loss is 3.9015111923217773 and perplexity is 49.47716214596344
At time: 737.510457277298 and batch: 1100, loss is 3.870368366241455 and perplexity is 47.960049690525246
At time: 738.6729118824005 and batch: 1150, loss is 3.8606106758117678 and perplexity is 47.494346163140236
At time: 739.8339245319366 and batch: 1200, loss is 3.8858198976516722 and perplexity is 48.7068607335404
At time: 740.9961180686951 and batch: 1250, loss is 3.905024881362915 and perplexity is 49.651315289203076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.354399966497491 and perplexity of 77.82011664768253
Finished 24 epochs...
Completing Train Step...
At time: 743.9224133491516 and batch: 50, loss is 3.928157024383545 and perplexity is 50.813243751842805
At time: 745.0872235298157 and batch: 100, loss is 3.9380344104766847 and perplexity is 51.31763270042638
At time: 746.2516121864319 and batch: 150, loss is 3.8698512172698973 and perplexity is 47.93525361233773
At time: 747.416618347168 and batch: 200, loss is 3.9245307207107545 and perplexity is 50.62931319498442
At time: 748.5814745426178 and batch: 250, loss is 3.9361570262908936 and perplexity is 51.22138016810412
At time: 749.7458939552307 and batch: 300, loss is 3.942066626548767 and perplexity is 51.52497422577134
At time: 750.9115746021271 and batch: 350, loss is 3.918819155693054 and perplexity is 50.340964824975295
At time: 752.076806306839 and batch: 400, loss is 3.9412875843048094 and perplexity is 51.48484972560028
At time: 753.2423214912415 and batch: 450, loss is 3.8712663650512695 and perplexity is 48.00313710139234
At time: 754.4068360328674 and batch: 500, loss is 3.902975311279297 and perplexity is 49.549655653635035
At time: 755.5721302032471 and batch: 550, loss is 3.899030785560608 and perplexity is 49.35459073477417
At time: 756.7371809482574 and batch: 600, loss is 3.9181121397018432 and perplexity is 50.30538553687597
At time: 757.9026117324829 and batch: 650, loss is 3.9372297382354735 and perplexity is 51.27635543546957
At time: 759.0730395317078 and batch: 700, loss is 3.900489344596863 and perplexity is 49.426629842945836
At time: 760.266170501709 and batch: 750, loss is 3.908678593635559 and perplexity is 49.83305872615751
At time: 761.4306111335754 and batch: 800, loss is 3.9314632654190063 and perplexity is 50.98152261543092
At time: 762.5955708026886 and batch: 850, loss is 3.98403461933136 and perplexity is 53.733391253972215
At time: 763.7600657939911 and batch: 900, loss is 3.9405036067962644 and perplexity is 51.44450257907881
At time: 764.9252018928528 and batch: 950, loss is 3.926910262107849 and perplexity is 50.74993119248373
At time: 766.090291261673 and batch: 1000, loss is 3.9096727180480957 and perplexity is 49.882623619141604
At time: 767.2552316188812 and batch: 1050, loss is 3.90078369140625 and perplexity is 49.441180555111664
At time: 768.4207580089569 and batch: 1100, loss is 3.8697195196151735 and perplexity is 47.92894106754119
At time: 769.5846922397614 and batch: 1150, loss is 3.8601886796951295 and perplexity is 47.47430796181606
At time: 770.7517659664154 and batch: 1200, loss is 3.8854037952423095 and perplexity is 48.686597907434425
At time: 771.9174447059631 and batch: 1250, loss is 3.904643568992615 and perplexity is 49.63238623765174
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.354312200615876 and perplexity of 77.81328699624716
Finished 25 epochs...
Completing Train Step...
At time: 774.8391406536102 and batch: 50, loss is 3.924731659889221 and perplexity is 50.639487629771196
At time: 776.0274319648743 and batch: 100, loss is 3.934530339241028 and perplexity is 51.13812674429974
At time: 777.1884417533875 and batch: 150, loss is 3.866342639923096 and perplexity is 47.76736376688313
At time: 778.3496549129486 and batch: 200, loss is 3.921139287948608 and perplexity is 50.45789811914075
At time: 779.5112776756287 and batch: 250, loss is 3.932766361236572 and perplexity is 51.04799972794103
At time: 780.6725087165833 and batch: 300, loss is 3.9390020895004274 and perplexity is 51.36731573187546
At time: 781.8331687450409 and batch: 350, loss is 3.9157828998565676 and perplexity is 50.18834858493138
At time: 782.9941928386688 and batch: 400, loss is 3.938471574783325 and perplexity is 51.34007184218285
At time: 784.157723903656 and batch: 450, loss is 3.8683530950546263 and perplexity is 47.86349450938075
At time: 785.3193922042847 and batch: 500, loss is 3.900216221809387 and perplexity is 49.41313214737563
At time: 786.4818170070648 and batch: 550, loss is 3.8965690994262694 and perplexity is 49.23324464237173
At time: 787.6481721401215 and batch: 600, loss is 3.9156922817230226 and perplexity is 50.18380081651517
At time: 788.8087463378906 and batch: 650, loss is 3.934949560165405 and perplexity is 51.15956941135772
At time: 789.9962499141693 and batch: 700, loss is 3.8983768606185913 and perplexity is 49.32232708704138
At time: 791.1566581726074 and batch: 750, loss is 3.9066976070404054 and perplexity is 49.73443782042318
At time: 792.3224296569824 and batch: 800, loss is 3.9296771049499513 and perplexity is 50.89054271162512
At time: 793.4991600513458 and batch: 850, loss is 3.982334794998169 and perplexity is 53.64213151275207
At time: 794.6645221710205 and batch: 900, loss is 3.9388423061370847 and perplexity is 51.35910874508937
At time: 795.82532954216 and batch: 950, loss is 3.9255075693130492 and perplexity is 50.678794532767185
At time: 796.9848885536194 and batch: 1000, loss is 3.908473539352417 and perplexity is 49.82284129162379
At time: 798.1444916725159 and batch: 1050, loss is 3.899776272773743 and perplexity is 49.39139766892121
At time: 799.3072671890259 and batch: 1100, loss is 3.8687201166152954 and perplexity is 47.88106466795094
At time: 800.4745063781738 and batch: 1150, loss is 3.859394769668579 and perplexity is 47.43663259012818
At time: 801.6719419956207 and batch: 1200, loss is 3.884608578681946 and perplexity is 48.647896908387786
At time: 802.8466122150421 and batch: 1250, loss is 3.9039477157592772 and perplexity is 49.597861394714904
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.354287697450958 and perplexity of 77.8113803478026
Finished 26 epochs...
Completing Train Step...
At time: 806.0256803035736 and batch: 50, loss is 3.921607265472412 and perplexity is 50.48151680743547
At time: 807.2646450996399 and batch: 100, loss is 3.931377339363098 and perplexity is 50.977142162468596
At time: 808.4463114738464 and batch: 150, loss is 3.863230061531067 and perplexity is 47.6189152512484
At time: 809.6240448951721 and batch: 200, loss is 3.918144016265869 and perplexity is 50.30698912527718
At time: 810.7935719490051 and batch: 250, loss is 3.929776725769043 and perplexity is 50.89561272170916
At time: 811.9734382629395 and batch: 300, loss is 3.9362856578826904 and perplexity is 51.22796927954403
At time: 813.1461727619171 and batch: 350, loss is 3.9130866050720217 and perplexity is 50.053208273328096
At time: 814.3179666996002 and batch: 400, loss is 3.935939145088196 and perplexity is 51.21022120789724
At time: 815.490335226059 and batch: 450, loss is 3.8657294702529907 and perplexity is 47.738083246081445
At time: 816.6606204509735 and batch: 500, loss is 3.8977039527893065 and perplexity is 49.289148871178504
At time: 817.8306849002838 and batch: 550, loss is 3.8943066453933715 and perplexity is 49.12198259956641
At time: 819.0007848739624 and batch: 600, loss is 3.913464231491089 and perplexity is 50.072113256417055
At time: 820.2276630401611 and batch: 650, loss is 3.932802128791809 and perplexity is 51.04982562274473
At time: 821.3992965221405 and batch: 700, loss is 3.8963822889328004 and perplexity is 49.224048214666354
At time: 822.5650210380554 and batch: 750, loss is 3.904785113334656 and perplexity is 49.63941191831661
At time: 823.7292943000793 and batch: 800, loss is 3.9279218769073485 and perplexity is 50.80129655054937
At time: 824.8936116695404 and batch: 850, loss is 3.9806510543823244 and perplexity is 53.55188807179688
At time: 826.0569937229156 and batch: 900, loss is 3.937167410850525 and perplexity is 51.273159613920214
At time: 827.2217264175415 and batch: 950, loss is 3.9240540218353273 and perplexity is 50.60518400995534
At time: 828.3961222171783 and batch: 1000, loss is 3.907180919647217 and perplexity is 49.758480910910826
At time: 829.5720586776733 and batch: 1050, loss is 3.8986592864990235 and perplexity is 49.33625895596147
At time: 830.7508833408356 and batch: 1100, loss is 3.867590250968933 and perplexity is 47.82699604876103
At time: 831.9290158748627 and batch: 1150, loss is 3.8584328413009645 and perplexity is 47.39102388724502
At time: 833.0985491275787 and batch: 1200, loss is 3.8836430358886718 and perplexity is 48.60094795138093
At time: 834.2680141925812 and batch: 1250, loss is 3.9031013298034667 and perplexity is 49.55590022157097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.354315319200502 and perplexity of 77.81352966394608
Annealing...
Finished 27 epochs...
Completing Train Step...
At time: 837.2795927524567 and batch: 50, loss is 3.9235161352157593 and perplexity is 50.57797147788015
At time: 838.4458215236664 and batch: 100, loss is 3.9385710525512696 and perplexity is 51.34517929197046
At time: 839.6129055023193 and batch: 150, loss is 3.8715204334259035 and perplexity is 48.01533472986323
At time: 840.7799763679504 and batch: 200, loss is 3.929958477020264 and perplexity is 50.904863903684465
At time: 841.9474105834961 and batch: 250, loss is 3.941243257522583 and perplexity is 51.48256761845813
At time: 843.122816324234 and batch: 300, loss is 3.9475160694122313 and perplexity is 51.80652307428854
At time: 844.3021440505981 and batch: 350, loss is 3.922984218597412 and perplexity is 50.55107536820678
At time: 845.4738144874573 and batch: 400, loss is 3.9439322757720947 and perplexity is 51.621191479832085
At time: 846.6422019004822 and batch: 450, loss is 3.87147114276886 and perplexity is 48.012968080793556
At time: 847.8096742630005 and batch: 500, loss is 3.9016661167144777 and perplexity is 49.48482795905683
At time: 849.0246403217316 and batch: 550, loss is 3.898044605255127 and perplexity is 49.30594220146195
At time: 850.1916997432709 and batch: 600, loss is 3.9187909507751466 and perplexity is 50.33954498221839
At time: 851.3586974143982 and batch: 650, loss is 3.9382156991958617 and perplexity is 51.32693685167318
At time: 852.5240669250488 and batch: 700, loss is 3.9034813117980955 and perplexity is 49.57473414943292
At time: 853.6903507709503 and batch: 750, loss is 3.9081059455871583 and perplexity is 49.80453009154521
At time: 854.8569769859314 and batch: 800, loss is 3.9291995191574096 and perplexity is 50.866243914293364
At time: 856.0231697559357 and batch: 850, loss is 3.980246624946594 and perplexity is 53.53023449088962
At time: 857.1899926662445 and batch: 900, loss is 3.934809675216675 and perplexity is 51.152413458130304
At time: 858.3548529148102 and batch: 950, loss is 3.9227544546127318 and perplexity is 50.53946188593138
At time: 859.5206952095032 and batch: 1000, loss is 3.906331672668457 and perplexity is 49.7162416096677
At time: 860.6873590946198 and batch: 1050, loss is 3.9002014636993407 and perplexity is 49.41240290831477
At time: 861.8536365032196 and batch: 1100, loss is 3.8654154348373413 and perplexity is 47.723094150943496
At time: 863.020295381546 and batch: 1150, loss is 3.856206431388855 and perplexity is 47.28562941110645
At time: 864.1896336078644 and batch: 1200, loss is 3.8800231122970583 and perplexity is 48.4253342791224
At time: 865.3553194999695 and batch: 1250, loss is 3.899859232902527 and perplexity is 49.3954953556026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3494997790260035 and perplexity of 77.43971626482669
Finished 28 epochs...
Completing Train Step...
At time: 868.3262343406677 and batch: 50, loss is 3.921822166442871 and perplexity is 50.49236650015064
At time: 869.5382618904114 and batch: 100, loss is 3.934302315711975 and perplexity is 51.12646737752585
At time: 870.7063887119293 and batch: 150, loss is 3.8661758279800416 and perplexity is 47.759396264674336
At time: 871.8730993270874 and batch: 200, loss is 3.923641171455383 and perplexity is 50.58429595262758
At time: 873.0395770072937 and batch: 250, loss is 3.9343586921691895 and perplexity is 51.12934978787577
At time: 874.2063109874725 and batch: 300, loss is 3.9415099143981935 and perplexity is 51.49629762960726
At time: 875.3736448287964 and batch: 350, loss is 3.9178199911117555 and perplexity is 50.29069103601092
At time: 876.5407018661499 and batch: 400, loss is 3.939718689918518 and perplexity is 51.404138763929424
At time: 877.7079799175262 and batch: 450, loss is 3.8679018020629883 and perplexity is 47.84189892308825
At time: 878.9029643535614 and batch: 500, loss is 3.898463478088379 and perplexity is 49.32659944724555
At time: 880.0687642097473 and batch: 550, loss is 3.894955611228943 and perplexity is 49.15387143431202
At time: 881.2355859279633 and batch: 600, loss is 3.915863757133484 and perplexity is 50.19240684219849
At time: 882.403341293335 and batch: 650, loss is 3.9357141733169554 and perplexity is 51.198701649562786
At time: 883.5729949474335 and batch: 700, loss is 3.901180601119995 and perplexity is 49.460808134863875
At time: 884.7407851219177 and batch: 750, loss is 3.906739468574524 and perplexity is 49.736519823866495
At time: 885.9061608314514 and batch: 800, loss is 3.928263711929321 and perplexity is 50.81866518130597
At time: 887.0725955963135 and batch: 850, loss is 3.9797472286224367 and perplexity is 53.503508362574145
At time: 888.2387602329254 and batch: 900, loss is 3.9346435117721557 and perplexity is 51.143914503042005
At time: 889.4105184078217 and batch: 950, loss is 3.9230203104019163 and perplexity is 50.55289988066122
At time: 890.5837597846985 and batch: 1000, loss is 3.9068000745773315 and perplexity is 49.73953424687172
At time: 891.7501640319824 and batch: 1050, loss is 3.9009196281433107 and perplexity is 49.44790188470021
At time: 892.9111738204956 and batch: 1100, loss is 3.8662062644958497 and perplexity is 47.76084991641567
At time: 894.0729513168335 and batch: 1150, loss is 3.857125663757324 and perplexity is 47.32911587624561
At time: 895.2347693443298 and batch: 1200, loss is 3.8807613182067873 and perplexity is 48.46109534495893
At time: 896.3965735435486 and batch: 1250, loss is 3.900299711227417 and perplexity is 49.41725779324313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.349390183052007 and perplexity of 77.4312296487546
Finished 29 epochs...
Completing Train Step...
At time: 899.3911242485046 and batch: 50, loss is 3.9198441743850707 and perplexity is 50.39259170963574
At time: 900.5523359775543 and batch: 100, loss is 3.9321147346496583 and perplexity is 51.0147463296854
At time: 901.7149524688721 and batch: 150, loss is 3.8637249851226807 and perplexity is 47.64248880888701
At time: 902.8762793540955 and batch: 200, loss is 3.921165280342102 and perplexity is 50.459209657728486
At time: 904.0434651374817 and batch: 250, loss is 3.9319036531448366 and perplexity is 51.003979196673335
At time: 905.2081961631775 and batch: 300, loss is 3.939310507774353 and perplexity is 51.38316079405744
At time: 906.3695533275604 and batch: 350, loss is 3.915718536376953 and perplexity is 50.1851183921347
At time: 907.5310072898865 and batch: 400, loss is 3.9378053045272825 and perplexity is 51.30587687218199
At time: 908.7404224872589 and batch: 450, loss is 3.86598970413208 and perplexity is 47.75050792925649
At time: 909.9068765640259 and batch: 500, loss is 3.896713304519653 and perplexity is 49.240344838942924
At time: 911.0738382339478 and batch: 550, loss is 3.89335000038147 and perplexity is 49.07501277024639
At time: 912.2412011623383 and batch: 600, loss is 3.9143431568145752 and perplexity is 50.11614225102336
At time: 913.40913438797 and batch: 650, loss is 3.934279203414917 and perplexity is 51.125285741079516
At time: 914.576691865921 and batch: 700, loss is 3.8998968505859377 and perplexity is 49.39735353465879
At time: 915.7437498569489 and batch: 750, loss is 3.9057333421707154 and perplexity is 49.686503763496866
At time: 916.9109144210815 and batch: 800, loss is 3.927433285713196 and perplexity is 50.77648154709298
At time: 918.079375743866 and batch: 850, loss is 3.9790203619003295 and perplexity is 53.46463257329979
At time: 919.2472486495972 and batch: 900, loss is 3.9340311527252196 and perplexity is 51.112605651407975
At time: 920.4139869213104 and batch: 950, loss is 3.9226288270950316 and perplexity is 50.53311313758584
At time: 921.5819199085236 and batch: 1000, loss is 3.906524577140808 and perplexity is 49.725833020105924
At time: 922.7483654022217 and batch: 1050, loss is 3.9008095169067385 and perplexity is 49.442457414831935
At time: 923.9154784679413 and batch: 1100, loss is 3.8660792207717893 and perplexity is 47.75478258559431
At time: 925.0815689563751 and batch: 1150, loss is 3.8570647764205934 and perplexity is 47.32623422015916
At time: 926.2485282421112 and batch: 1200, loss is 3.880666079521179 and perplexity is 48.4564801937091
At time: 927.4150266647339 and batch: 1250, loss is 3.900145468711853 and perplexity is 49.409636138895515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3493897375399175 and perplexity of 77.43119515221338
Finished 30 epochs...
Completing Train Step...
At time: 930.40198802948 and batch: 50, loss is 3.918112964630127 and perplexity is 50.30542703522845
At time: 931.5962011814117 and batch: 100, loss is 3.9303269529342653 and perplexity is 50.9236245761542
At time: 932.7626013755798 and batch: 150, loss is 3.861822876930237 and perplexity is 47.55195377162968
At time: 933.9304583072662 and batch: 200, loss is 3.919271068572998 and perplexity is 50.36371969659103
At time: 935.0972533226013 and batch: 250, loss is 3.930035991668701 and perplexity is 50.908809929249124
At time: 936.2641541957855 and batch: 300, loss is 3.9376617908477782 and perplexity is 51.29851430533904
At time: 937.4310083389282 and batch: 350, loss is 3.91411901473999 and perplexity is 50.10491037374417
At time: 938.6498775482178 and batch: 400, loss is 3.9363079833984376 and perplexity is 51.22911298314572
At time: 939.8174011707306 and batch: 450, loss is 3.8644327878952027 and perplexity is 47.67622223145247
At time: 940.9847264289856 and batch: 500, loss is 3.895262351036072 and perplexity is 49.16895119601879
At time: 942.1513085365295 and batch: 550, loss is 3.8920482110977175 and perplexity is 49.01116900910022
At time: 943.3179979324341 and batch: 600, loss is 3.913094277381897 and perplexity is 50.05359229852541
At time: 944.4841830730438 and batch: 650, loss is 3.933040199279785 and perplexity is 51.06198052644636
At time: 945.6502358913422 and batch: 700, loss is 3.8987798404693605 and perplexity is 49.34220699638293
At time: 946.8169357776642 and batch: 750, loss is 3.904758906364441 and perplexity is 49.63811103677314
At time: 947.9837126731873 and batch: 800, loss is 3.9265766382217406 and perplexity is 50.73300262726343
At time: 949.1492474079132 and batch: 850, loss is 3.9782263088226317 and perplexity is 53.42219566806436
At time: 950.3162002563477 and batch: 900, loss is 3.9332920837402345 and perplexity is 51.074843865830374
At time: 951.4829261302948 and batch: 950, loss is 3.922053952217102 and perplexity is 50.50407126885966
At time: 952.6498153209686 and batch: 1000, loss is 3.906029725074768 and perplexity is 49.70123217629157
At time: 953.8173494338989 and batch: 1050, loss is 3.9004556560516357 and perplexity is 49.42496475973822
At time: 954.9865438938141 and batch: 1100, loss is 3.8657033967971803 and perplexity is 47.73683856550409
At time: 956.1520190238953 and batch: 1150, loss is 3.856735920906067 and perplexity is 47.31067328584386
At time: 957.3183860778809 and batch: 1200, loss is 3.880324306488037 and perplexity is 48.439921905247324
At time: 958.4854309558868 and batch: 1250, loss is 3.899800410270691 and perplexity is 49.39258986802001
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.349411567632299 and perplexity of 77.43288550080695
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 961.4594578742981 and batch: 50, loss is 3.9193371772766112 and perplexity is 50.367049286865544
At time: 962.6564254760742 and batch: 100, loss is 3.935760974884033 and perplexity is 51.20109788510568
At time: 963.8334712982178 and batch: 150, loss is 3.8687250566482545 and perplexity is 47.88130120257276
At time: 965.0006966590881 and batch: 200, loss is 3.9276338911056516 and perplexity is 50.786668604856445
At time: 966.1708040237427 and batch: 250, loss is 3.9385979843139647 and perplexity is 51.346562126775694
At time: 967.3842098712921 and batch: 300, loss is 3.9461039972305296 and perplexity is 51.73342014967473
At time: 968.5467474460602 and batch: 350, loss is 3.9220914220809937 and perplexity is 50.50596368499014
At time: 969.709775686264 and batch: 400, loss is 3.944370651245117 and perplexity is 51.64382590489096
At time: 970.8724677562714 and batch: 450, loss is 3.8702632188796997 and perplexity is 47.95500708294372
At time: 972.0350391864777 and batch: 500, loss is 3.900574789047241 and perplexity is 49.430853254597785
At time: 973.1978912353516 and batch: 550, loss is 3.8962721109390257 and perplexity is 49.21862510654767
At time: 974.3609526157379 and batch: 600, loss is 3.9196607494354248 and perplexity is 50.383349298709156
At time: 975.5244660377502 and batch: 650, loss is 3.9390365409851076 and perplexity is 51.369085442650864
At time: 976.6872501373291 and batch: 700, loss is 3.906275596618652 and perplexity is 49.71345379739255
At time: 977.8540658950806 and batch: 750, loss is 3.909524984359741 and perplexity is 49.875254819492916
At time: 979.0172672271729 and batch: 800, loss is 3.9300153255462646 and perplexity is 50.90775785242125
At time: 980.1834795475006 and batch: 850, loss is 3.9793909406661987 and perplexity is 53.484449102422126
At time: 981.3478076457977 and batch: 900, loss is 3.9326543951034547 and perplexity is 51.04228440077563
At time: 982.5150220394135 and batch: 950, loss is 3.921154570579529 and perplexity is 50.45866925446722
At time: 983.6792135238647 and batch: 1000, loss is 3.904940266609192 and perplexity is 49.64711423312601
At time: 984.8427360057831 and batch: 1050, loss is 3.901595935821533 and perplexity is 49.48135519150584
At time: 986.0158948898315 and batch: 1100, loss is 3.8651843738555907 and perplexity is 47.71206847980706
At time: 987.1841208934784 and batch: 1150, loss is 3.8554983711242676 and perplexity is 47.25216018634682
At time: 988.3499705791473 and batch: 1200, loss is 3.8788026332855225 and perplexity is 48.36626822677864
At time: 989.5154564380646 and batch: 1250, loss is 3.8987494230270388 and perplexity is 49.34070615547357
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346619097855839 and perplexity of 77.2169581339795
Finished 32 epochs...
Completing Train Step...
At time: 992.5221683979034 and batch: 50, loss is 3.9196396923065184 and perplexity is 50.38228838119822
At time: 993.6825833320618 and batch: 100, loss is 3.93366669178009 and perplexity is 51.09398049712107
At time: 994.8447108268738 and batch: 150, loss is 3.8651561832427976 and perplexity is 47.710723466317454
At time: 996.0059475898743 and batch: 200, loss is 3.922641634941101 and perplexity is 50.53376036206509
At time: 997.2068815231323 and batch: 250, loss is 3.933524098396301 and perplexity is 51.086695352969755
At time: 998.3765285015106 and batch: 300, loss is 3.9416829109191895 and perplexity is 51.505207080571154
At time: 999.5376963615417 and batch: 350, loss is 3.9184521293640135 and perplexity is 50.322491755714
At time: 1000.69868516922 and batch: 400, loss is 3.941276903152466 and perplexity is 51.48429981101384
At time: 1001.8682899475098 and batch: 450, loss is 3.867824144363403 and perplexity is 47.83818377553088
At time: 1003.0306537151337 and batch: 500, loss is 3.898264775276184 and perplexity is 49.31679908693118
At time: 1004.1922109127045 and batch: 550, loss is 3.8938098192214965 and perplexity is 49.097583574535534
At time: 1005.3522336483002 and batch: 600, loss is 3.9171739625930786 and perplexity is 50.25821230759968
At time: 1006.5342619419098 and batch: 650, loss is 3.9371107006073 and perplexity is 51.2702519830146
At time: 1007.6973412036896 and batch: 700, loss is 3.904261245727539 and perplexity is 49.613414248639344
At time: 1008.8582127094269 and batch: 750, loss is 3.9083620834350588 and perplexity is 49.817288550590916
At time: 1010.0197026729584 and batch: 800, loss is 3.9293063497543335 and perplexity is 50.871678275766875
At time: 1011.1889536380768 and batch: 850, loss is 3.979280867576599 and perplexity is 53.47856222786307
At time: 1012.3506021499634 and batch: 900, loss is 3.9328992795944213 and perplexity is 51.05478539519624
At time: 1013.5106947422028 and batch: 950, loss is 3.92172182559967 and perplexity is 50.487300307698064
At time: 1014.6789944171906 and batch: 1000, loss is 3.905698709487915 and perplexity is 49.684783016369785
At time: 1015.8407447338104 and batch: 1050, loss is 3.902694697380066 and perplexity is 49.535753282247065
At time: 1017.0035350322723 and batch: 1100, loss is 3.8661828899383544 and perplexity is 47.759733540730714
At time: 1018.1655249595642 and batch: 1150, loss is 3.856604290008545 and perplexity is 47.30444614930766
At time: 1019.3266186714172 and batch: 1200, loss is 3.8796611261367797 and perplexity is 48.40780815060585
At time: 1020.4947185516357 and batch: 1250, loss is 3.8992535209655763 and perplexity is 49.36558497388623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.34645113979813 and perplexity of 77.20399001274977
Finished 33 epochs...
Completing Train Step...
At time: 1023.5483901500702 and batch: 50, loss is 3.9185740089416505 and perplexity is 50.32862541353104
At time: 1024.745478630066 and batch: 100, loss is 3.9323412656784056 and perplexity is 51.02630406169578
At time: 1025.91019821167 and batch: 150, loss is 3.8635572671890257 and perplexity is 47.634498979147395
At time: 1027.1274831295013 and batch: 200, loss is 3.9208686113357545 and perplexity is 50.44424219443924
At time: 1028.2963321208954 and batch: 250, loss is 3.9318074989318847 and perplexity is 50.999075184970735
At time: 1029.4664347171783 and batch: 300, loss is 3.940097241401672 and perplexity is 51.42360156050161
At time: 1030.636415719986 and batch: 350, loss is 3.917063536643982 and perplexity is 50.25266280321599
At time: 1031.808085680008 and batch: 400, loss is 3.94002863407135 and perplexity is 51.420073645504786
At time: 1032.97917842865 and batch: 450, loss is 3.8666788816452025 and perplexity is 47.78342784809242
At time: 1034.1485257148743 and batch: 500, loss is 3.8972195911407472 and perplexity is 49.265280878611925
At time: 1035.3193228244781 and batch: 550, loss is 3.8927725982666015 and perplexity is 49.04668493314921
At time: 1036.4903481006622 and batch: 600, loss is 3.9161846828460694 and perplexity is 50.20851746114837
At time: 1037.6602346897125 and batch: 650, loss is 3.936196722984314 and perplexity is 51.223413527887786
At time: 1038.8315925598145 and batch: 700, loss is 3.903417553901672 and perplexity is 49.57157346942803
At time: 1040.003066778183 and batch: 750, loss is 3.9078097677230836 and perplexity is 49.789781276445545
At time: 1041.172688484192 and batch: 800, loss is 3.9288821840286254 and perplexity is 50.85010482911483
At time: 1042.3428962230682 and batch: 850, loss is 3.9790000438690187 and perplexity is 53.46354628825677
At time: 1043.5126514434814 and batch: 900, loss is 3.932720966339111 and perplexity is 51.04568246182422
At time: 1044.6778843402863 and batch: 950, loss is 3.9216970300674436 and perplexity is 50.48604846373638
At time: 1045.84219956398 and batch: 1000, loss is 3.9057539749145507 and perplexity is 49.68752894297718
At time: 1047.0056412220001 and batch: 1050, loss is 3.902904953956604 and perplexity is 49.546169595159185
At time: 1048.1698679924011 and batch: 1100, loss is 3.8663666009902955 and perplexity is 47.768508337608814
At time: 1049.3358161449432 and batch: 1150, loss is 3.8568189430236814 and perplexity is 47.3146012811788
At time: 1050.5005195140839 and batch: 1200, loss is 3.879771475791931 and perplexity is 48.41315023028481
At time: 1051.6647481918335 and batch: 1250, loss is 3.899235348701477 and perplexity is 49.36468789758965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346406143077099 and perplexity of 77.20051616450516
Finished 34 epochs...
Completing Train Step...
At time: 1054.618690252304 and batch: 50, loss is 3.9175901651382445 and perplexity is 50.27913425705989
At time: 1055.779450893402 and batch: 100, loss is 3.93128671169281 and perplexity is 50.972522432177335
At time: 1056.9678971767426 and batch: 150, loss is 3.8623696756362915 and perplexity is 47.57796222846963
At time: 1058.1292178630829 and batch: 200, loss is 3.9196320247650145 and perplexity is 50.38190207439201
At time: 1059.2904305458069 and batch: 250, loss is 3.9306217670440673 and perplexity is 50.93863979244156
At time: 1060.454760313034 and batch: 300, loss is 3.939016809463501 and perplexity is 51.36807186243132
At time: 1061.6257328987122 and batch: 350, loss is 3.9160749244689943 and perplexity is 50.20300695817395
At time: 1062.80166554451 and batch: 400, loss is 3.939107966423035 and perplexity is 51.372754633109736
At time: 1063.9694769382477 and batch: 450, loss is 3.865779409408569 and perplexity is 47.74046730517614
At time: 1065.1375501155853 and batch: 500, loss is 3.89639524936676 and perplexity is 49.22468618382665
At time: 1066.3055412769318 and batch: 550, loss is 3.891981153488159 and perplexity is 49.007882547457164
At time: 1067.4720373153687 and batch: 600, loss is 3.915440421104431 and perplexity is 50.1711630849385
At time: 1068.6385962963104 and batch: 650, loss is 3.9354674768447877 and perplexity is 51.18607266831288
At time: 1069.805279493332 and batch: 700, loss is 3.9027572774887087 and perplexity is 49.53885333206888
At time: 1070.9725728034973 and batch: 750, loss is 3.907297010421753 and perplexity is 49.7642577468117
At time: 1072.1390309333801 and batch: 800, loss is 3.92844708442688 and perplexity is 50.82798478131592
At time: 1073.3054158687592 and batch: 850, loss is 3.978624014854431 and perplexity is 53.443446222970344
At time: 1074.4716131687164 and batch: 900, loss is 3.9324094343185423 and perplexity is 51.02978257401625
At time: 1075.6378073692322 and batch: 950, loss is 3.9214889430999755 and perplexity is 50.47554406796395
At time: 1076.803743839264 and batch: 1000, loss is 3.905597848892212 and perplexity is 49.67977203226703
At time: 1077.970713376999 and batch: 1050, loss is 3.902864627838135 and perplexity is 49.544171630739754
At time: 1079.1425487995148 and batch: 1100, loss is 3.8663108587265014 and perplexity is 47.765845687027785
At time: 1080.3084073066711 and batch: 1150, loss is 3.856786427497864 and perplexity is 47.31306284705093
At time: 1081.4768886566162 and batch: 1200, loss is 3.879683423042297 and perplexity is 48.40888750696361
At time: 1082.6428635120392 and batch: 1250, loss is 3.899087052345276 and perplexity is 49.357367837031994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346391441178148 and perplexity of 77.19938117866081
Finished 35 epochs...
Completing Train Step...
At time: 1085.693425655365 and batch: 50, loss is 3.9166867923736572 and perplexity is 50.23373396632564
At time: 1086.9022769927979 and batch: 100, loss is 3.9303631448745726 and perplexity is 50.92546763428681
At time: 1088.0702443122864 and batch: 150, loss is 3.861357989311218 and perplexity is 47.52985259474089
At time: 1089.2425651550293 and batch: 200, loss is 3.9185968828201294 and perplexity is 50.32977663755919
At time: 1090.4117076396942 and batch: 250, loss is 3.9296276140213013 and perplexity is 50.88802415373022
At time: 1091.5790655612946 and batch: 300, loss is 3.9381184577941895 and perplexity is 51.32194599105326
At time: 1092.7500388622284 and batch: 350, loss is 3.9152347660064697 and perplexity is 50.16084619037677
At time: 1093.9185326099396 and batch: 400, loss is 3.9383122301101685 and perplexity is 51.33189172696152
At time: 1095.0862362384796 and batch: 450, loss is 3.8649823331832884 and perplexity is 47.70242967516254
At time: 1096.2553098201752 and batch: 500, loss is 3.8956594705581664 and perplexity is 49.18848102400096
At time: 1097.4319829940796 and batch: 550, loss is 3.891283326148987 and perplexity is 48.97369543691813
At time: 1098.599621772766 and batch: 600, loss is 3.914785599708557 and perplexity is 50.138320688020244
At time: 1099.7660505771637 and batch: 650, loss is 3.934809012413025 and perplexity is 51.15237955413518
At time: 1100.9426126480103 and batch: 700, loss is 3.902161865234375 and perplexity is 49.50936607113871
At time: 1102.1102254390717 and batch: 750, loss is 3.9067906093597413 and perplexity is 49.739063453585324
At time: 1103.2889187335968 and batch: 800, loss is 3.9280012226104737 and perplexity is 50.80532757506396
At time: 1104.455124616623 and batch: 850, loss is 3.9782114601135254 and perplexity is 53.42140242331041
At time: 1105.6282682418823 and batch: 900, loss is 3.932047390937805 and perplexity is 51.01131092298613
At time: 1106.794885635376 and batch: 950, loss is 3.9212061405181884 and perplexity is 50.46127147204303
At time: 1107.961020708084 and batch: 1000, loss is 3.9053542852401733 and perplexity is 49.66767331902161
At time: 1109.1350529193878 and batch: 1050, loss is 3.9027191543579103 and perplexity is 49.53696479188246
At time: 1110.3017060756683 and batch: 1100, loss is 3.8661540222167967 and perplexity is 47.75835484594118
At time: 1111.468014240265 and batch: 1150, loss is 3.8566502428054807 and perplexity is 47.30661997086192
At time: 1112.6364786624908 and batch: 1200, loss is 3.8795110893249514 and perplexity is 48.400545742231266
At time: 1113.8074703216553 and batch: 1250, loss is 3.898883857727051 and perplexity is 49.34733970438355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.346388322593522 and perplexity of 77.19914042623293
Finished 36 epochs...
Completing Train Step...
At time: 1116.7689945697784 and batch: 50, loss is 3.915836048126221 and perplexity is 50.19101607970117
At time: 1117.9576530456543 and batch: 100, loss is 3.9295124626159668 and perplexity is 50.88216466360496
At time: 1119.1196010112762 and batch: 150, loss is 3.8604419136047365 and perplexity is 47.48633158875789
At time: 1120.2804243564606 and batch: 200, loss is 3.917665734291077 and perplexity is 50.28293395220889
At time: 1121.4421985149384 and batch: 250, loss is 3.928731098175049 and perplexity is 50.84242267796906
At time: 1122.6050186157227 and batch: 300, loss is 3.937310481071472 and perplexity is 51.28049580097728
At time: 1123.7664177417755 and batch: 350, loss is 3.9144712829589845 and perplexity is 50.12256385048126
At time: 1124.9279918670654 and batch: 400, loss is 3.937584013938904 and perplexity is 51.29452462062121
At time: 1126.0884563922882 and batch: 450, loss is 3.864243369102478 and perplexity is 47.66719231424072
At time: 1127.2500467300415 and batch: 500, loss is 3.8949725008010865 and perplexity is 49.15470162918054
At time: 1128.4114997386932 and batch: 550, loss is 3.8906359910964965 and perplexity is 48.94200330603212
At time: 1129.5744578838348 and batch: 600, loss is 3.9141784143447875 and perplexity is 50.10788667401344
At time: 1130.7361342906952 and batch: 650, loss is 3.934189114570618 and perplexity is 51.12068013063281
At time: 1131.9025280475616 and batch: 700, loss is 3.9015999698638915 and perplexity is 49.48155480179125
At time: 1133.0637378692627 and batch: 750, loss is 3.906288070678711 and perplexity is 49.714073929868725
At time: 1134.2345249652863 and batch: 800, loss is 3.9275496912002565 and perplexity is 50.782392552188725
At time: 1135.4051213264465 and batch: 850, loss is 3.9777841091156008 and perplexity is 53.39857761112478
At time: 1136.5664579868317 and batch: 900, loss is 3.931662573814392 and perplexity is 50.991684673555845
At time: 1137.7404112815857 and batch: 950, loss is 3.9208866643905638 and perplexity is 50.44515287532865
At time: 1138.9014730453491 and batch: 1000, loss is 3.905066957473755 and perplexity is 49.65340446740047
At time: 1140.070186138153 and batch: 1050, loss is 3.9025195264816284 and perplexity is 49.527076819793876
At time: 1141.2310938835144 and batch: 1100, loss is 3.8659454727172853 and perplexity is 47.748395903442905
At time: 1142.3982203006744 and batch: 1150, loss is 3.8564606046676637 and perplexity is 47.29764968212553
At time: 1143.563557624817 and batch: 1200, loss is 3.8792933559417726 and perplexity is 48.39000847485818
At time: 1144.8097982406616 and batch: 1250, loss is 3.8986504316329955 and perplexity is 49.33582209193228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.34638965912979 and perplexity of 77.19924360575295
Annealing...
Finished 37 epochs...
Completing Train Step...
At time: 1147.7871551513672 and batch: 50, loss is 3.9166345167160035 and perplexity is 50.231108033482926
At time: 1148.9489629268646 and batch: 100, loss is 3.9330478572845458 and perplexity is 51.06237156083358
At time: 1150.1112480163574 and batch: 150, loss is 3.8652183771133424 and perplexity is 47.713690873152615
At time: 1151.2733931541443 and batch: 200, loss is 3.923052682876587 and perplexity is 50.55453642962157
At time: 1152.4346868991852 and batch: 250, loss is 3.934163293838501 and perplexity is 51.11936017428672
At time: 1153.596223115921 and batch: 300, loss is 3.9424717617034912 and perplexity is 51.54585303326033
At time: 1154.7584252357483 and batch: 350, loss is 3.919104256629944 and perplexity is 50.355319127326126
At time: 1155.9202942848206 and batch: 400, loss is 3.9432969903945922 and perplexity is 51.58840770634425
At time: 1157.0824854373932 and batch: 450, loss is 3.868833179473877 and perplexity is 47.88647854404262
At time: 1158.2444870471954 and batch: 500, loss is 3.8997346210479735 and perplexity is 49.38934047481328
At time: 1159.4057009220123 and batch: 550, loss is 3.8940546798706053 and perplexity is 49.10960711270488
At time: 1160.5667815208435 and batch: 600, loss is 3.919611110687256 and perplexity is 50.380848394392814
At time: 1161.7281568050385 and batch: 650, loss is 3.9391531467437746 and perplexity is 51.375075723074744
At time: 1162.8890476226807 and batch: 700, loss is 3.9067393064498903 and perplexity is 49.736511760352094
At time: 1164.0504446029663 and batch: 750, loss is 3.9097453927993775 and perplexity is 49.886248958140115
At time: 1165.2124705314636 and batch: 800, loss is 3.929738779067993 and perplexity is 50.893681437751624
At time: 1166.3740136623383 and batch: 850, loss is 3.9778602647781374 and perplexity is 53.40264437003266
At time: 1167.53568816185 and batch: 900, loss is 3.931068329811096 and perplexity is 50.96139217218068
At time: 1168.697641134262 and batch: 950, loss is 3.919647288322449 and perplexity is 50.38267108731689
At time: 1169.8595056533813 and batch: 1000, loss is 3.903150177001953 and perplexity is 49.55832094758764
At time: 1171.020673274994 and batch: 1050, loss is 3.9014486503601074 and perplexity is 49.474067843947836
At time: 1172.1832671165466 and batch: 1100, loss is 3.864905552864075 and perplexity is 47.69876720798931
At time: 1173.3444645404816 and batch: 1150, loss is 3.8544901466369628 and perplexity is 47.20454340960266
At time: 1174.5525529384613 and batch: 1200, loss is 3.8777851009368898 and perplexity is 48.317079014326765
At time: 1175.7141919136047 and batch: 1250, loss is 3.8981724405288696 and perplexity is 49.31224564297292
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3452593949589415 and perplexity of 77.11203735902065
Finished 38 epochs...
Completing Train Step...
At time: 1178.6130452156067 and batch: 50, loss is 3.9174529027938845 and perplexity is 50.27223329885105
At time: 1179.8060562610626 and batch: 100, loss is 3.9319659423828126 and perplexity is 51.00715629461971
At time: 1180.9707291126251 and batch: 150, loss is 3.8628593492507934 and perplexity is 47.60126560626365
At time: 1182.1351878643036 and batch: 200, loss is 3.9197418880462647 and perplexity is 50.387437499533945
At time: 1183.3001782894135 and batch: 250, loss is 3.9310438108444212 and perplexity is 50.96014266682266
At time: 1184.4652378559113 and batch: 300, loss is 3.9398499155044555 and perplexity is 51.4108847447713
At time: 1185.630279302597 and batch: 350, loss is 3.9170300436019896 and perplexity is 50.2509797168565
At time: 1186.7951016426086 and batch: 400, loss is 3.9410048007965086 and perplexity is 51.47029271750845
At time: 1187.9590091705322 and batch: 450, loss is 3.8671450996398926 and perplexity is 47.805710535894576
At time: 1189.1244144439697 and batch: 500, loss is 3.898001012802124 and perplexity is 49.30379288134118
At time: 1190.28972697258 and batch: 550, loss is 3.892300214767456 and perplexity is 49.02352155992738
At time: 1191.4544961452484 and batch: 600, loss is 3.917638521194458 and perplexity is 50.281565616487384
At time: 1192.6189539432526 and batch: 650, loss is 3.9376911735534668 and perplexity is 51.30002161663147
At time: 1193.7838459014893 and batch: 700, loss is 3.9053091764450074 and perplexity is 49.665432920650716
At time: 1194.9496388435364 and batch: 750, loss is 3.908855619430542 and perplexity is 49.841881243878504
At time: 1196.1219511032104 and batch: 800, loss is 3.9293168640136718 and perplexity is 50.87221315659717
At time: 1197.286635875702 and batch: 850, loss is 3.977807378768921 and perplexity is 53.39982019197074
At time: 1198.4515826702118 and batch: 900, loss is 3.9312781858444215 and perplexity is 50.972087850031684
At time: 1199.6170551776886 and batch: 950, loss is 3.9201489305496215 and perplexity is 50.40795150298391
At time: 1200.781861782074 and batch: 1000, loss is 3.903888854980469 and perplexity is 49.59494211188246
At time: 1201.9469575881958 and batch: 1050, loss is 3.9026387119293213 and perplexity is 49.532980078401835
At time: 1203.111881494522 and batch: 1100, loss is 3.8658711242675783 and perplexity is 47.74484601619743
At time: 1204.3360958099365 and batch: 1150, loss is 3.8555102586746215 and perplexity is 47.25272190211907
At time: 1205.5105936527252 and batch: 1200, loss is 3.8786016416549685 and perplexity is 48.35654798853991
At time: 1206.676146030426 and batch: 1250, loss is 3.898760828971863 and perplexity is 49.34126893605508
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.345112375969435 and perplexity of 77.10070125854074
Finished 39 epochs...
Completing Train Step...
At time: 1209.5982398986816 and batch: 50, loss is 3.9170426607131956 and perplexity is 50.25161374305558
At time: 1210.7578094005585 and batch: 100, loss is 3.9312784910202025 and perplexity is 50.97210340548079
At time: 1211.9207768440247 and batch: 150, loss is 3.861900978088379 and perplexity is 47.55566777932343
At time: 1213.0828919410706 and batch: 200, loss is 3.9184716320037842 and perplexity is 50.32347318671329
At time: 1214.2443771362305 and batch: 250, loss is 3.9298667335510253 and perplexity is 50.90019392909194
At time: 1215.4054479599 and batch: 300, loss is 3.938783874511719 and perplexity is 51.35610783656289
At time: 1216.5664587020874 and batch: 350, loss is 3.916132173538208 and perplexity is 50.20588111586467
At time: 1217.727430343628 and batch: 400, loss is 3.9401547288894654 and perplexity is 51.426557859142875
At time: 1218.8881921768188 and batch: 450, loss is 3.8664557456970217 and perplexity is 47.77276683708384
At time: 1220.0493159294128 and batch: 500, loss is 3.8973487424850464 and perplexity is 49.27164396675652
At time: 1221.2106544971466 and batch: 550, loss is 3.8916319608688354 and perplexity is 48.99077234413514
At time: 1222.370658159256 and batch: 600, loss is 3.9169564390182496 and perplexity is 50.247281150529304
At time: 1223.531448841095 and batch: 650, loss is 3.937121524810791 and perplexity is 51.27080694565863
At time: 1224.6919152736664 and batch: 700, loss is 3.9047514486312864 and perplexity is 49.63774085036711
At time: 1225.8529920578003 and batch: 750, loss is 3.908499364852905 and perplexity is 49.82412800805088
At time: 1227.0141015052795 and batch: 800, loss is 3.929104418754578 and perplexity is 50.86140674401864
At time: 1228.1756122112274 and batch: 850, loss is 3.977719841003418 and perplexity is 53.39514589562459
At time: 1229.3362505435944 and batch: 900, loss is 3.931271891593933 and perplexity is 50.97176701995254
At time: 1230.4993524551392 and batch: 950, loss is 3.9202451038360597 and perplexity is 50.4127996344692
At time: 1231.6666812896729 and batch: 1000, loss is 3.904066185951233 and perplexity is 49.60373761094629
At time: 1232.8280050754547 and batch: 1050, loss is 3.9029704475402833 and perplexity is 49.549414657627786
At time: 1234.0165889263153 and batch: 1100, loss is 3.8661542081832887 and perplexity is 47.75836372739572
At time: 1235.1869103908539 and batch: 1150, loss is 3.8558108377456666 and perplexity is 47.26692721617586
At time: 1236.347553730011 and batch: 1200, loss is 3.878806734085083 and perplexity is 48.36646656755679
At time: 1237.5082778930664 and batch: 1250, loss is 3.8988514518737794 and perplexity is 49.345740587644265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.345058914518704 and perplexity of 77.09657945337895
Finished 40 epochs...
Completing Train Step...
At time: 1240.4250283241272 and batch: 50, loss is 3.9165406465530395 and perplexity is 50.22639305248744
At time: 1241.6179549694061 and batch: 100, loss is 3.9306859159469605 and perplexity is 50.941907555109694
At time: 1242.7818615436554 and batch: 150, loss is 3.861201510429382 and perplexity is 47.52241575842213
At time: 1243.9559245109558 and batch: 200, loss is 3.9176623678207396 and perplexity is 50.282764676488206
At time: 1245.119683265686 and batch: 250, loss is 3.9291090393066406 and perplexity is 50.861641752339416
At time: 1246.2851490974426 and batch: 300, loss is 3.9380827140808106 and perplexity is 51.320111586910116
At time: 1247.4488699436188 and batch: 350, loss is 3.91552405834198 and perplexity is 50.175359437906785
At time: 1248.6140413284302 and batch: 400, loss is 3.9395928478240965 and perplexity is 51.39767036645232
At time: 1249.7777044773102 and batch: 450, loss is 3.8659551525115967 and perplexity is 47.74885810033093
At time: 1250.942628622055 and batch: 500, loss is 3.8968843841552734 and perplexity is 49.24876957982604
At time: 1252.1064352989197 and batch: 550, loss is 3.891161756515503 and perplexity is 48.96774208459427
At time: 1253.2718634605408 and batch: 600, loss is 3.9165117645263674 and perplexity is 50.22494243341216
At time: 1254.435611963272 and batch: 650, loss is 3.9367135334014893 and perplexity is 51.24989316348866
At time: 1255.6079456806183 and batch: 700, loss is 3.9043632316589356 and perplexity is 49.618474376927814
At time: 1256.7724890708923 and batch: 750, loss is 3.908222222328186 and perplexity is 49.810321536691326
At time: 1257.9362740516663 and batch: 800, loss is 3.9289048099517823 and perplexity is 50.85125537269521
At time: 1259.1003901958466 and batch: 850, loss is 3.977569432258606 and perplexity is 53.387115402694704
At time: 1260.2646036148071 and batch: 900, loss is 3.9311628437042234 and perplexity is 50.966208959377134
At time: 1261.4286270141602 and batch: 950, loss is 3.9201991510391236 and perplexity is 50.41048307855114
At time: 1262.620225429535 and batch: 1000, loss is 3.904058995246887 and perplexity is 49.60338092641709
At time: 1263.7890124320984 and batch: 1050, loss is 3.9030567836761474 and perplexity is 49.55369274729786
At time: 1264.9570705890656 and batch: 1100, loss is 3.8662287425994872 and perplexity is 47.76192350181594
At time: 1266.121419429779 and batch: 1150, loss is 3.855897583961487 and perplexity is 47.27102762109005
At time: 1267.2999913692474 and batch: 1200, loss is 3.878837671279907 and perplexity is 48.367962913502204
At time: 1268.4641284942627 and batch: 1250, loss is 3.898817415237427 and perplexity is 49.344061053199354
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.345033965841697 and perplexity of 77.09465601971348
Finished 41 epochs...
Completing Train Step...
At time: 1271.3808970451355 and batch: 50, loss is 3.916055498123169 and perplexity is 50.20203170667212
At time: 1272.541484594345 and batch: 100, loss is 3.93016263961792 and perplexity is 50.915257833922176
At time: 1273.7019312381744 and batch: 150, loss is 3.8606152534484863 and perplexity is 47.49456357550078
At time: 1274.8631331920624 and batch: 200, loss is 3.9170256805419923 and perplexity is 50.250760469295365
At time: 1276.0316197872162 and batch: 250, loss is 3.928510642051697 and perplexity is 50.83121538996667
At time: 1277.2107560634613 and batch: 300, loss is 3.9375287771224974 and perplexity is 51.29169135263315
At time: 1278.3719284534454 and batch: 350, loss is 3.9150308847427366 and perplexity is 50.150620376126945
At time: 1279.5328035354614 and batch: 400, loss is 3.9391328573226927 and perplexity is 51.37403336310476
At time: 1280.6938133239746 and batch: 450, loss is 3.8655226325988767 and perplexity is 47.728210234021454
At time: 1281.8548591136932 and batch: 500, loss is 3.8964854145050047 and perplexity is 49.22912473455972
At time: 1283.015971660614 and batch: 550, loss is 3.89076078414917 and perplexity is 48.94811130913888
At time: 1284.1772153377533 and batch: 600, loss is 3.916141223907471 and perplexity is 50.20633549968411
At time: 1285.3376083374023 and batch: 650, loss is 3.936357169151306 and perplexity is 51.23163278760531
At time: 1286.4987378120422 and batch: 700, loss is 3.904030876159668 and perplexity is 49.601986144232555
At time: 1287.6586594581604 and batch: 750, loss is 3.907960891723633 and perplexity is 49.797306275968204
At time: 1288.8189363479614 and batch: 800, loss is 3.9286983251571654 and perplexity is 50.840756445645276
At time: 1289.9819784164429 and batch: 850, loss is 3.977382574081421 and perplexity is 53.37714051559902
At time: 1291.1431856155396 and batch: 900, loss is 3.9310063028335573 and perplexity is 50.958231289084104
At time: 1292.3499298095703 and batch: 950, loss is 3.920090117454529 and perplexity is 50.404986942517084
At time: 1293.510407924652 and batch: 1000, loss is 3.903973388671875 and perplexity is 49.5991347326206
At time: 1294.6719000339508 and batch: 1050, loss is 3.9030426645278933 and perplexity is 49.55299309630267
At time: 1295.8330662250519 and batch: 1100, loss is 3.866212110519409 and perplexity is 47.76112912828564
At time: 1297.0033612251282 and batch: 1150, loss is 3.8558944606781007 and perplexity is 47.27087998050539
At time: 1298.1653525829315 and batch: 1200, loss is 3.878797016143799 and perplexity is 48.365996547358385
At time: 1299.3271527290344 and batch: 1250, loss is 3.8987355518341062 and perplexity is 49.34002174576586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.345017036382299 and perplexity of 77.09335085991246
Finished 42 epochs...
Completing Train Step...
At time: 1302.259378194809 and batch: 50, loss is 3.915594706535339 and perplexity is 50.17890436162197
At time: 1303.420726776123 and batch: 100, loss is 3.929686541557312 and perplexity is 50.89102294796097
At time: 1304.5829660892487 and batch: 150, loss is 3.8600924253463744 and perplexity is 47.46973857313589
At time: 1305.7449223995209 and batch: 200, loss is 3.9164741373062135 and perplexity is 50.22305264399999
At time: 1306.9127798080444 and batch: 250, loss is 3.9279904890060426 and perplexity is 50.80478225370142
At time: 1308.074592590332 and batch: 300, loss is 3.937047924995422 and perplexity is 51.267033562595465
At time: 1309.2377009391785 and batch: 350, loss is 3.9145939207077025 and perplexity is 50.128711145809376
At time: 1310.3998041152954 and batch: 400, loss is 3.9387205839157104 and perplexity is 51.352857580745635
At time: 1311.5611882209778 and batch: 450, loss is 3.8651249599456787 and perplexity is 47.70923380347912
At time: 1312.7238039970398 and batch: 500, loss is 3.8961182689666747 and perplexity is 49.211053798592275
At time: 1313.884808063507 and batch: 550, loss is 3.890394082069397 and perplexity is 48.930165225555065
At time: 1315.05255484581 and batch: 600, loss is 3.915805015563965 and perplexity is 50.189458548037216
At time: 1316.2136883735657 and batch: 650, loss is 3.936026201248169 and perplexity is 51.21467956716796
At time: 1317.3741478919983 and batch: 700, loss is 3.9037245988845823 and perplexity is 49.586796509316194
At time: 1318.5336775779724 and batch: 750, loss is 3.9077036809921264 and perplexity is 49.78449952148186
At time: 1319.7020590305328 and batch: 800, loss is 3.9284860610961916 and perplexity is 50.82996592547947
At time: 1320.861281633377 and batch: 850, loss is 3.977176284790039 and perplexity is 53.36613051876733
At time: 1322.068757534027 and batch: 900, loss is 3.9308258438110353 and perplexity is 50.949036246165406
At time: 1323.235594511032 and batch: 950, loss is 3.9199483156204225 and perplexity is 50.39783992966227
At time: 1324.4076709747314 and batch: 1000, loss is 3.9038477754592895 and perplexity is 49.59290481725341
At time: 1325.5690417289734 and batch: 1050, loss is 3.9029778909683226 and perplexity is 49.54978347650282
At time: 1326.7312853336334 and batch: 1100, loss is 3.8661473560333253 and perplexity is 47.758036481046624
At time: 1327.89315700531 and batch: 1150, loss is 3.855844645500183 and perplexity is 47.26852523186043
At time: 1329.0543630123138 and batch: 1200, loss is 3.878719711303711 and perplexity is 48.3622577662444
At time: 1330.2239301204681 and batch: 1250, loss is 3.8986307048797606 and perplexity is 49.334848865943556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.345007680628422 and perplexity of 77.09262959687021
Finished 43 epochs...
Completing Train Step...
At time: 1333.122727394104 and batch: 50, loss is 3.9151539278030394 and perplexity is 50.15679144157971
At time: 1334.3153018951416 and batch: 100, loss is 3.929241771697998 and perplexity is 50.86839318773475
At time: 1335.4796071052551 and batch: 150, loss is 3.8596093034744263 and perplexity is 47.44681044316205
At time: 1336.6408927440643 and batch: 200, loss is 3.9159721660614015 and perplexity is 50.19784844216754
At time: 1337.8024837970734 and batch: 250, loss is 3.9275158786773683 and perplexity is 50.78067550040733
At time: 1338.9651873111725 and batch: 300, loss is 3.936609411239624 and perplexity is 51.24455719161839
At time: 1340.1264109611511 and batch: 350, loss is 3.9141898918151856 and perplexity is 50.10846178909988
At time: 1341.2895982265472 and batch: 400, loss is 3.9383363342285156 and perplexity is 51.33312905186694
At time: 1342.4545323848724 and batch: 450, loss is 3.8647492599487303 and perplexity is 47.69131281115392
At time: 1343.6155230998993 and batch: 500, loss is 3.8957704734802245 and perplexity is 49.19394139217902
At time: 1344.7769091129303 and batch: 550, loss is 3.890048384666443 and perplexity is 48.913253117914735
At time: 1345.937669992447 and batch: 600, loss is 3.915488591194153 and perplexity is 50.17357989257421
At time: 1347.0998091697693 and batch: 650, loss is 3.9357102870941163 and perplexity is 51.19850268038572
At time: 1348.2610116004944 and batch: 700, loss is 3.9034336519241335 and perplexity is 49.57237148015437
At time: 1349.4215822219849 and batch: 750, loss is 3.9074479866027834 and perplexity is 49.77177153158512
At time: 1350.581978559494 and batch: 800, loss is 3.9282699155807497 and perplexity is 50.818980443568705
At time: 1351.7898952960968 and batch: 850, loss is 3.9769592809677126 and perplexity is 53.354551120894214
At time: 1352.9518160820007 and batch: 900, loss is 3.9306325817108156 and perplexity is 50.93919067983436
At time: 1354.1145586967468 and batch: 950, loss is 3.919787735939026 and perplexity is 50.38974771032369
At time: 1355.2755813598633 and batch: 1000, loss is 3.9036993741989137 and perplexity is 49.58554571373646
At time: 1356.4369716644287 and batch: 1050, loss is 3.902883791923523 and perplexity is 49.54512110857327
At time: 1357.6009657382965 and batch: 1100, loss is 3.86605441570282 and perplexity is 47.75359803961008
At time: 1358.7673676013947 and batch: 1150, loss is 3.8557669019699095 and perplexity is 47.2648505526812
At time: 1359.9415245056152 and batch: 1200, loss is 3.878620924949646 and perplexity is 48.357480471094995
At time: 1361.1149542331696 and batch: 1250, loss is 3.8985128021240234 and perplexity is 49.32903249419825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344998324874544 and perplexity of 77.09190834057583
Finished 44 epochs...
Completing Train Step...
At time: 1364.036942243576 and batch: 50, loss is 3.914728493690491 and perplexity is 50.13545756992463
At time: 1365.1976914405823 and batch: 100, loss is 3.928818998336792 and perplexity is 50.84689193156704
At time: 1366.3602240085602 and batch: 150, loss is 3.8591534233093263 and perplexity is 47.425185312992234
At time: 1367.5222823619843 and batch: 200, loss is 3.915503044128418 and perplexity is 50.17430505326657
At time: 1368.6837344169617 and batch: 250, loss is 3.927070965766907 and perplexity is 50.75808754748379
At time: 1369.8522686958313 and batch: 300, loss is 3.9361984968185424 and perplexity is 51.22350438981258
At time: 1371.0168430805206 and batch: 350, loss is 3.91380753993988 and perplexity is 50.08930638705146
At time: 1372.1837894916534 and batch: 400, loss is 3.937970676422119 and perplexity is 51.31436212384834
At time: 1373.347005367279 and batch: 450, loss is 3.864388818740845 and perplexity is 47.674125994363195
At time: 1374.5091996192932 and batch: 500, loss is 3.8954360055923463 and perplexity is 49.17749034983123
At time: 1375.6708886623383 and batch: 550, loss is 3.8897170305252073 and perplexity is 48.89704819386542
At time: 1376.8319787979126 and batch: 600, loss is 3.9151853036880495 and perplexity is 50.15836517998905
At time: 1377.993881225586 and batch: 650, loss is 3.9354049968719482 and perplexity is 51.18287466378946
At time: 1379.1552684307098 and batch: 700, loss is 3.9031528425216675 and perplexity is 49.55845304644519
At time: 1380.3159098625183 and batch: 750, loss is 3.907193217277527 and perplexity is 49.759092826076404
At time: 1381.5247287750244 and batch: 800, loss is 3.9280508756637573 and perplexity is 50.80785027733056
At time: 1382.68599152565 and batch: 850, loss is 3.976736168861389 and perplexity is 53.34264840248136
At time: 1383.84672665596 and batch: 900, loss is 3.9304319047927856 and perplexity is 50.92896938566499
At time: 1385.0169939994812 and batch: 950, loss is 3.919615616798401 and perplexity is 50.381075416606755
At time: 1386.1788582801819 and batch: 1000, loss is 3.9035369348526 and perplexity is 49.57749172426422
At time: 1387.3484122753143 and batch: 1050, loss is 3.9027709913253785 and perplexity is 49.53953270447067
At time: 1388.5098509788513 and batch: 1100, loss is 3.8659434127807617 and perplexity is 47.74829754487954
At time: 1389.6714639663696 and batch: 1150, loss is 3.855671830177307 and perplexity is 47.26035721221042
At time: 1390.8321633338928 and batch: 1200, loss is 3.8785081624984743 and perplexity is 48.35202787049467
At time: 1391.9930398464203 and batch: 1250, loss is 3.8983863973617554 and perplexity is 49.32279746364999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344996097314096 and perplexity of 77.09173661388128
Finished 45 epochs...
Completing Train Step...
At time: 1394.8983471393585 and batch: 50, loss is 3.914315309524536 and perplexity is 50.114746671706094
At time: 1396.0915794372559 and batch: 100, loss is 3.9284125852584837 and perplexity is 50.82623128835691
At time: 1397.2558715343475 and batch: 150, loss is 3.858717694282532 and perplexity is 47.40452528456417
At time: 1398.4199657440186 and batch: 200, loss is 3.9150568723678587 and perplexity is 50.15192368858384
At time: 1399.5846147537231 and batch: 250, loss is 3.926647090911865 and perplexity is 50.73657702968827
At time: 1400.7499265670776 and batch: 300, loss is 3.935807204246521 and perplexity is 51.20346493393275
At time: 1401.9141340255737 and batch: 350, loss is 3.9134408044815063 and perplexity is 50.070940230280286
At time: 1403.078871011734 and batch: 400, loss is 3.937618923187256 and perplexity is 51.29631530517584
At time: 1404.244181394577 and batch: 450, loss is 3.864040107727051 and perplexity is 47.657504399790945
At time: 1405.4096231460571 and batch: 500, loss is 3.895111165046692 and perplexity is 49.161518101389476
At time: 1406.573689699173 and batch: 550, loss is 3.8893966627120973 and perplexity is 48.881385662487446
At time: 1407.7402443885803 and batch: 600, loss is 3.9148920583724975 and perplexity is 50.14365863078299
At time: 1408.9055223464966 and batch: 650, loss is 3.935107707977295 and perplexity is 51.16766082512018
At time: 1410.070648908615 and batch: 700, loss is 3.902879571914673 and perplexity is 49.54491202816488
At time: 1411.2624571323395 and batch: 750, loss is 3.906939535140991 and perplexity is 49.74647143407472
At time: 1412.4285147190094 and batch: 800, loss is 3.9278306674957277 and perplexity is 50.796663205486965
At time: 1413.5969574451447 and batch: 850, loss is 3.9765098237991334 and perplexity is 53.33057592373233
At time: 1414.7617485523224 and batch: 900, loss is 3.930227060317993 and perplexity is 50.918537936128345
At time: 1415.9262917041779 and batch: 950, loss is 3.9194362354278565 and perplexity is 50.37203880077351
At time: 1417.1060276031494 and batch: 1000, loss is 3.903365201950073 and perplexity is 49.568978368742954
At time: 1418.2715322971344 and batch: 1050, loss is 3.9026460027694703 and perplexity is 49.53334121675819
At time: 1419.4369983673096 and batch: 1100, loss is 3.86582004070282 and perplexity is 47.74240710155884
At time: 1420.6013276576996 and batch: 1150, loss is 3.855564522743225 and perplexity is 47.25528609663325
At time: 1421.7647378444672 and batch: 1200, loss is 3.8783857011795044 and perplexity is 48.34610697993421
At time: 1422.928545475006 and batch: 1250, loss is 3.89825400352478 and perplexity is 49.3162678614925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344992087705292 and perplexity of 77.09142750679513
Finished 46 epochs...
Completing Train Step...
At time: 1425.8658690452576 and batch: 50, loss is 3.9139125108718873 and perplexity is 50.0945645842006
At time: 1427.0274066925049 and batch: 100, loss is 3.928018684387207 and perplexity is 50.80621473409661
At time: 1428.18807888031 and batch: 150, loss is 3.858297333717346 and perplexity is 47.38460247919742
At time: 1429.3470995426178 and batch: 200, loss is 3.914628167152405 and perplexity is 50.13042790533992
At time: 1430.50688290596 and batch: 250, loss is 3.926239113807678 and perplexity is 50.71588188977402
At time: 1431.6663918495178 and batch: 300, loss is 3.935430407524109 and perplexity is 51.18417527053869
At time: 1432.8255441188812 and batch: 350, loss is 3.9130858850479124 and perplexity is 50.05317223382437
At time: 1433.9926421642303 and batch: 400, loss is 3.9372777938842773 and perplexity is 51.27881961320667
At time: 1435.1601903438568 and batch: 450, loss is 3.863700633049011 and perplexity is 47.641328629616176
At time: 1436.3298976421356 and batch: 500, loss is 3.894794239997864 and perplexity is 49.145940053531916
At time: 1437.4963114261627 and batch: 550, loss is 3.8890847826004027 and perplexity is 48.86614290754685
At time: 1438.6635122299194 and batch: 600, loss is 3.914606556892395 and perplexity is 50.12934458546393
At time: 1439.871672153473 and batch: 650, loss is 3.9348169469833376 and perplexity is 51.15278542789763
At time: 1441.0382182598114 and batch: 700, loss is 3.902612090110779 and perplexity is 49.5316614379467
At time: 1442.2061610221863 and batch: 750, loss is 3.9066869115829466 and perplexity is 49.73390589070385
At time: 1443.3733406066895 and batch: 800, loss is 3.9276094150543215 and perplexity is 50.78542556296123
At time: 1444.5447618961334 and batch: 850, loss is 3.976281752586365 and perplexity is 53.31841414153269
At time: 1445.7164552211761 and batch: 900, loss is 3.930019664764404 and perplexity is 50.90797875276682
At time: 1446.8775732517242 and batch: 950, loss is 3.9192518377304078 and perplexity is 50.362751169138114
At time: 1448.0384240150452 and batch: 1000, loss is 3.9031873321533204 and perplexity is 49.56016232871213
At time: 1449.2074522972107 and batch: 1050, loss is 3.9025121116638184 and perplexity is 49.52670958690408
At time: 1450.368378162384 and batch: 1100, loss is 3.8656881046295166 and perplexity is 47.73610857134662
At time: 1451.5302476882935 and batch: 1150, loss is 3.8554485702514647 and perplexity is 47.24980704612242
At time: 1452.6919691562653 and batch: 1200, loss is 3.8782558727264402 and perplexity is 48.339830687082845
At time: 1453.8575501441956 and batch: 1250, loss is 3.898117394447327 and perplexity is 49.30953127178663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344991196681113 and perplexity of 77.09135881649983
Finished 47 epochs...
Completing Train Step...
At time: 1456.7904829978943 and batch: 50, loss is 3.9135177659988405 and perplexity is 50.07479391410553
At time: 1457.9840791225433 and batch: 100, loss is 3.9276351022720335 and perplexity is 50.786730115999354
At time: 1459.1490664482117 and batch: 150, loss is 3.8578893184661864 and perplexity is 47.36527278238948
At time: 1460.3184649944305 and batch: 200, loss is 3.914213290214539 and perplexity is 50.109634260616716
At time: 1461.485827922821 and batch: 250, loss is 3.9258437061309817 and perplexity is 50.69583240486571
At time: 1462.6500594615936 and batch: 300, loss is 3.935064787864685 and perplexity is 51.16546475048381
At time: 1463.826810836792 and batch: 350, loss is 3.912740468978882 and perplexity is 50.03588604946365
At time: 1464.9907402992249 and batch: 400, loss is 3.9369456481933596 and perplexity is 51.26179040248284
At time: 1466.1574828624725 and batch: 450, loss is 3.863368983268738 and perplexity is 47.62553101322704
At time: 1467.321168422699 and batch: 500, loss is 3.8944836759567263 and perplexity is 49.130679461601595
At time: 1468.485759973526 and batch: 550, loss is 3.8887801456451414 and perplexity is 48.851258741804706
At time: 1469.6963481903076 and batch: 600, loss is 3.9143272495269774 and perplexity is 50.11534504547598
At time: 1470.8749797344208 and batch: 650, loss is 3.9345315074920655 and perplexity is 51.13818648650427
At time: 1472.0470459461212 and batch: 700, loss is 3.902349338531494 and perplexity is 49.51864862532268
At time: 1473.2421159744263 and batch: 750, loss is 3.9064351511001587 and perplexity is 49.721386434564174
At time: 1474.427980184555 and batch: 800, loss is 3.927387442588806 and perplexity is 50.774153847888236
At time: 1475.593956232071 and batch: 850, loss is 3.9760527658462523 and perplexity is 53.30620632945733
At time: 1476.7936253547668 and batch: 900, loss is 3.929811110496521 and perplexity is 50.89736278356996
At time: 1477.9628789424896 and batch: 950, loss is 3.919064221382141 and perplexity is 50.353303180001426
At time: 1479.1279056072235 and batch: 1000, loss is 3.903005061149597 and perplexity is 49.551129771391444
At time: 1480.2953567504883 and batch: 1050, loss is 3.902371869087219 and perplexity is 49.51976432056353
At time: 1481.4658453464508 and batch: 1100, loss is 3.8655499029159546 and perplexity is 47.72951181519527
At time: 1482.629691362381 and batch: 1150, loss is 3.855326313972473 and perplexity is 47.24403081362743
At time: 1483.79359126091 and batch: 1200, loss is 3.878121032714844 and perplexity is 48.33331298318594
At time: 1484.9577004909515 and batch: 1250, loss is 3.8979772901535035 and perplexity is 49.302623278660086
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344991642193203 and perplexity of 77.09139316163983
Annealing...
Finished 48 epochs...
Completing Train Step...
At time: 1487.9006052017212 and batch: 50, loss is 3.914009141921997 and perplexity is 50.09940550846918
At time: 1489.0614414215088 and batch: 100, loss is 3.929621057510376 and perplexity is 50.88769050693767
At time: 1490.2223827838898 and batch: 150, loss is 3.8606360292434694 and perplexity is 47.49555032306663
At time: 1491.3827195167542 and batch: 200, loss is 3.9173728942871096 and perplexity is 50.26821125343364
At time: 1492.543910741806 and batch: 250, loss is 3.928938808441162 and perplexity is 50.852984267950696
At time: 1493.7046399116516 and batch: 300, loss is 3.9379774284362794 and perplexity is 51.31470860031773
At time: 1494.8661844730377 and batch: 350, loss is 3.915193643569946 and perplexity is 50.158783496575126
At time: 1496.0266182422638 and batch: 400, loss is 3.9400807332992556 and perplexity is 51.42275266142729
At time: 1497.187079668045 and batch: 450, loss is 3.865989022254944 and perplexity is 47.750475369288004
At time: 1498.347898721695 and batch: 500, loss is 3.897281746864319 and perplexity is 49.268343092957984
At time: 1499.5560698509216 and batch: 550, loss is 3.8904578351974486 and perplexity is 48.93328477608376
At time: 1500.7179210186005 and batch: 600, loss is 3.9171453857421876 and perplexity is 50.25677610668166
At time: 1501.8790125846863 and batch: 650, loss is 3.937351279258728 and perplexity is 51.282587994926125
At time: 1503.0405707359314 and batch: 700, loss is 3.9049826526641844 and perplexity is 49.649218623038195
At time: 1504.2109389305115 and batch: 750, loss is 3.908207559585571 and perplexity is 49.80959118612156
At time: 1505.3714735507965 and batch: 800, loss is 3.9280579710006713 and perplexity is 50.80821077742508
At time: 1506.5329990386963 and batch: 850, loss is 3.9751392650604247 and perplexity is 53.25753330289463
At time: 1507.6935992240906 and batch: 900, loss is 3.9286957120895387 and perplexity is 50.84062359548406
At time: 1508.8552803993225 and batch: 950, loss is 3.9174564123153686 and perplexity is 50.27240973064346
At time: 1510.0163660049438 and batch: 1000, loss is 3.901089935302734 and perplexity is 49.456323933556895
At time: 1511.176917552948 and batch: 1050, loss is 3.900329785346985 and perplexity is 49.418743996110734
At time: 1512.3376541137695 and batch: 1100, loss is 3.8638772487640383 and perplexity is 47.64974358002131
At time: 1513.498506784439 and batch: 1150, loss is 3.8532019090652465 and perplexity is 47.143771895704916
At time: 1514.6601803302765 and batch: 1200, loss is 3.876536974906921 and perplexity is 48.25681082928611
At time: 1515.8213510513306 and batch: 1250, loss is 3.8973387908935546 and perplexity is 49.27115363792342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344610729356752 and perplexity of 77.06203365246664
Finished 49 epochs...
Completing Train Step...
At time: 1518.7669956684113 and batch: 50, loss is 3.9144126653671263 and perplexity is 50.11962587260002
At time: 1519.928685426712 and batch: 100, loss is 3.929023084640503 and perplexity is 50.85727014478611
At time: 1521.0901732444763 and batch: 150, loss is 3.859435963630676 and perplexity is 47.43858673322228
At time: 1522.2523324489594 and batch: 200, loss is 3.915881242752075 and perplexity is 50.19328449515384
At time: 1523.4190328121185 and batch: 250, loss is 3.9275351428985594 and perplexity is 50.78165375999507
At time: 1524.5802562236786 and batch: 300, loss is 3.936805567741394 and perplexity is 51.2546101306343
At time: 1525.741554737091 and batch: 350, loss is 3.914267659187317 and perplexity is 50.11235874402077
At time: 1526.902461528778 and batch: 400, loss is 3.9387743616104127 and perplexity is 51.35561929330131
At time: 1528.0639679431915 and batch: 450, loss is 3.8650252866744994 and perplexity is 47.70447870506251
At time: 1529.2529034614563 and batch: 500, loss is 3.8962591981887815 and perplexity is 49.21798956283762
At time: 1530.4140238761902 and batch: 550, loss is 3.889464454650879 and perplexity is 48.88469953871762
At time: 1531.5752491950989 and batch: 600, loss is 3.916019024848938 and perplexity is 50.2002007075942
At time: 1532.7358167171478 and batch: 650, loss is 3.936519446372986 and perplexity is 51.23994718923695
At time: 1533.8975756168365 and batch: 700, loss is 3.904269051551819 and perplexity is 49.61380152374439
At time: 1535.0616521835327 and batch: 750, loss is 3.9077410554885863 and perplexity is 49.786360226854235
At time: 1536.2243797779083 and batch: 800, loss is 3.92796067237854 and perplexity is 50.80326744901691
At time: 1537.38591837883 and batch: 850, loss is 3.9751944589614867 and perplexity is 53.26047287504104
At time: 1538.5586757659912 and batch: 900, loss is 3.9288369274139403 and perplexity is 50.8478035775877
At time: 1539.7260339260101 and batch: 950, loss is 3.917803626060486 and perplexity is 50.28986803300799
At time: 1540.8871822357178 and batch: 1000, loss is 3.901559343338013 and perplexity is 49.47954457895904
At time: 1542.0481729507446 and batch: 1050, loss is 3.901150732040405 and perplexity is 49.459330808112405
At time: 1543.2097473144531 and batch: 1100, loss is 3.8645579862594603 and perplexity is 47.68219159015901
At time: 1544.3708882331848 and batch: 1150, loss is 3.853839135169983 and perplexity is 47.173822711397854
At time: 1545.5324020385742 and batch: 1200, loss is 3.8770849609375 and perplexity is 48.283262134311755
At time: 1546.6929910182953 and batch: 1250, loss is 3.897825574874878 and perplexity is 49.29514388481591
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.344532319229015 and perplexity of 77.05599144545246
Finished Training.
Improved accuracyfrom -81.51500638816472 to -77.05599144545246
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9c6396a6a0>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'anneal': 7.00904806048869, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 17.082850391630302, 'dropout': 0.9255275613624224, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.7772505283355713 and batch: 50, loss is 7.743127927780152 and perplexity is 2305.6730945230865
At time: 2.9312119483947754 and batch: 100, loss is 7.134543323516846 and perplexity is 1254.5639280764583
At time: 4.089202642440796 and batch: 150, loss is 6.9507978153228756 and perplexity is 1043.9823010930506
At time: 5.2488555908203125 and batch: 200, loss is 6.862740631103516 and perplexity is 955.9834786005828
At time: 6.403213262557983 and batch: 250, loss is 6.8579857063293455 and perplexity is 951.4486390291295
At time: 7.584136962890625 and batch: 300, loss is 6.813521680831909 and perplexity is 910.0701467055875
At time: 8.742696762084961 and batch: 350, loss is 6.798449516296387 and perplexity is 896.4562727111462
At time: 9.906611919403076 and batch: 400, loss is 6.7386644744873045 and perplexity is 844.4322216632561
At time: 11.070393085479736 and batch: 450, loss is 6.716879043579102 and perplexity is 826.2348404454822
At time: 12.23386549949646 and batch: 500, loss is 6.713714818954468 and perplexity is 823.624579721141
At time: 13.397333860397339 and batch: 550, loss is 6.706122522354126 and perplexity is 817.3950557454092
At time: 14.560888528823853 and batch: 600, loss is 6.729096412658691 and perplexity is 836.3911719317534
At time: 15.724779605865479 and batch: 650, loss is 6.6992853736877445 and perplexity is 811.8254659833192
At time: 16.889127254486084 and batch: 700, loss is 6.703512668609619 and perplexity is 815.264555554677
At time: 18.053128004074097 and batch: 750, loss is 6.644152736663818 and perplexity is 768.2788373067995
At time: 19.217293739318848 and batch: 800, loss is 6.659704208374023 and perplexity is 780.3200908388363
At time: 20.3823664188385 and batch: 850, loss is 6.689714117050171 and perplexity is 804.0923429910448
At time: 21.54724907875061 and batch: 900, loss is 6.694194002151489 and perplexity is 807.7026651747029
At time: 22.711800813674927 and batch: 950, loss is 6.67845871925354 and perplexity is 795.0927060612422
At time: 23.87649393081665 and batch: 1000, loss is 6.666043138504028 and perplexity is 785.2821959769742
At time: 25.041818380355835 and batch: 1050, loss is 6.647600231170654 and perplexity is 770.93204521014
At time: 26.20898962020874 and batch: 1100, loss is 6.649381713867188 and perplexity is 772.3066713817026
At time: 27.373605489730835 and batch: 1150, loss is 6.696203031539917 and perplexity is 809.3269946826663
At time: 28.549036741256714 and batch: 1200, loss is 6.690978488922119 and perplexity is 805.1096577285149
At time: 29.714701175689697 and batch: 1250, loss is 6.6754578018188475 and perplexity is 792.7102750247592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.046330584226734 and perplexity of 422.5596347722162
Finished 1 epochs...
Completing Train Step...
At time: 32.62818479537964 and batch: 50, loss is 6.493814573287964 and perplexity is 661.0401460204703
At time: 33.78185486793518 and batch: 100, loss is 6.634260129928589 and perplexity is 760.7160265396236
At time: 34.93817377090454 and batch: 150, loss is 6.665159215927124 and perplexity is 784.5883740023382
At time: 36.09555172920227 and batch: 200, loss is 6.602559185028076 and perplexity is 736.9788431258291
At time: 37.25069999694824 and batch: 250, loss is 6.576140518188477 and perplexity is 717.7637803489723
At time: 38.405301332473755 and batch: 300, loss is 6.670203695297241 and perplexity is 788.5562132967037
At time: 39.56070804595947 and batch: 350, loss is 6.7563065242767335 and perplexity is 859.461924523955
At time: 40.71659564971924 and batch: 400, loss is 6.725596199035644 and perplexity is 833.4687417062831
At time: 41.87313222885132 and batch: 450, loss is 6.688414907455444 and perplexity is 803.0483368422393
At time: 43.05680513381958 and batch: 500, loss is 6.672094421386719 and perplexity is 790.0485674741147
At time: 44.2211275100708 and batch: 550, loss is 6.639795904159546 and perplexity is 764.938856248385
At time: 45.37766790390015 and batch: 600, loss is 6.667221622467041 and perplexity is 786.2081839752062
At time: 46.53418517112732 and batch: 650, loss is 6.691633110046387 and perplexity is 805.6368720618083
At time: 47.690314054489136 and batch: 700, loss is 6.725243349075317 and perplexity is 833.1747041724548
At time: 48.846991777420044 and batch: 750, loss is 6.641672668457031 and perplexity is 766.3758141774438
At time: 50.00546956062317 and batch: 800, loss is 6.661011753082275 and perplexity is 781.3410615813403
At time: 51.16198468208313 and batch: 850, loss is 6.736287593841553 and perplexity is 842.4274905061001
At time: 52.320056200027466 and batch: 900, loss is 6.690419416427613 and perplexity is 804.6596688637368
At time: 53.47794818878174 and batch: 950, loss is 6.626037549972534 and perplexity is 754.4866241649793
At time: 54.63833260536194 and batch: 1000, loss is 6.615313711166382 and perplexity is 746.4388598146004
At time: 55.79592418670654 and batch: 1050, loss is 6.649646892547607 and perplexity is 772.5114978022984
At time: 56.95436072349548 and batch: 1100, loss is 6.677395868301391 and perplexity is 794.2480899515867
At time: 58.11274552345276 and batch: 1150, loss is 6.688033561706543 and perplexity is 802.7421561568848
At time: 59.27192735671997 and batch: 1200, loss is 6.677187423706055 and perplexity is 794.0825504833832
At time: 60.430097818374634 and batch: 1250, loss is 6.683943128585815 and perplexity is 799.4652995043592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 6.332233066976506 and perplexity of 562.4110942818056
Annealing...
Finished 2 epochs...
Completing Train Step...
At time: 63.363723278045654 and batch: 50, loss is 6.602727565765381 and perplexity is 737.1029466148383
At time: 64.52687954902649 and batch: 100, loss is 6.548044757843018 and perplexity is 697.8783178539305
At time: 65.69819688796997 and batch: 150, loss is 6.4247768592834475 and perplexity is 616.9431370486793
At time: 66.86081290245056 and batch: 200, loss is 6.4167725944519045 and perplexity is 612.0246714042947
At time: 68.02314186096191 and batch: 250, loss is 6.437610330581665 and perplexity is 624.9116817689658
At time: 69.1865565776825 and batch: 300, loss is 6.43637508392334 and perplexity is 624.1402382619107
At time: 70.37632060050964 and batch: 350, loss is 6.461236534118652 and perplexity is 639.8517661388323
At time: 71.53923797607422 and batch: 400, loss is 6.428689908981323 and perplexity is 619.3619956752689
At time: 72.70128774642944 and batch: 450, loss is 6.425113029479981 and perplexity is 617.1505698087132
At time: 73.86334037780762 and batch: 500, loss is 6.368936367034912 and perplexity is 583.4369359905825
At time: 75.02656745910645 and batch: 550, loss is 6.3727975273132325 and perplexity is 585.6940342175704
At time: 76.1892728805542 and batch: 600, loss is 6.398699369430542 and perplexity is 601.0627686496575
At time: 77.35100507736206 and batch: 650, loss is 6.348128747940064 and perplexity is 571.4224325434607
At time: 78.51381087303162 and batch: 700, loss is 6.371904439926148 and perplexity is 585.1711917696696
At time: 79.67681288719177 and batch: 750, loss is 6.302961483001709 and perplexity is 546.1870409918348
At time: 80.84059548377991 and batch: 800, loss is 6.288410806655884 and perplexity is 538.2971906675248
At time: 82.00289630889893 and batch: 850, loss is 6.334021682739258 and perplexity is 563.4179317844803
At time: 83.1704306602478 and batch: 900, loss is 6.311385078430176 and perplexity is 550.8073320646829
At time: 84.350426197052 and batch: 950, loss is 6.2682504463195805 and perplexity is 527.5535866829844
At time: 85.5170567035675 and batch: 1000, loss is 6.252267990112305 and perplexity is 519.1890059122846
At time: 86.67861342430115 and batch: 1050, loss is 6.229864568710327 and perplexity is 507.68672217214777
At time: 87.84110832214355 and batch: 1100, loss is 6.160573062896728 and perplexity is 473.69945665078995
At time: 89.00404953956604 and batch: 1150, loss is 6.175668077468872 and perplexity is 480.90419788351124
At time: 90.1701910495758 and batch: 1200, loss is 6.147890100479126 and perplexity is 467.7294827482291
At time: 91.34683275222778 and batch: 1250, loss is 6.147498140335083 and perplexity is 467.54618735738853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.773905733205976 and perplexity of 321.79211556141775
Finished 3 epochs...
Completing Train Step...
At time: 94.26868033409119 and batch: 50, loss is 6.1756735610961915 and perplexity is 480.9068349901392
At time: 95.4515027999878 and batch: 100, loss is 6.1946430683135985 and perplexity is 490.116475673632
At time: 96.61924862861633 and batch: 150, loss is 6.1188039875030515 and perplexity is 454.3209958404892
At time: 97.779958486557 and batch: 200, loss is 6.129326734542847 and perplexity is 459.1269422878928
At time: 98.93979334831238 and batch: 250, loss is 6.165989894866943 and perplexity is 476.27236923821766
At time: 100.12730956077576 and batch: 300, loss is 6.167840566635132 and perplexity is 477.15460918240905
At time: 101.28569769859314 and batch: 350, loss is 6.204214744567871 and perplexity is 494.83023520744877
At time: 102.44569039344788 and batch: 400, loss is 6.178916177749634 and perplexity is 482.4687624995273
At time: 103.61168241500854 and batch: 450, loss is 6.160286378860474 and perplexity is 473.56367404286885
At time: 104.77248334884644 and batch: 500, loss is 6.119245367050171 and perplexity is 454.52156809686977
At time: 105.92964625358582 and batch: 550, loss is 6.114535493850708 and perplexity is 452.3858625438968
At time: 107.09199094772339 and batch: 600, loss is 6.1412607860565185 and perplexity is 464.63901211142655
At time: 108.25142931938171 and batch: 650, loss is 6.115289516448975 and perplexity is 452.7271003417293
At time: 109.41219305992126 and batch: 700, loss is 6.141276626586914 and perplexity is 464.64637229811547
At time: 110.57247519493103 and batch: 750, loss is 6.079892511367798 and perplexity is 436.98222157260363
At time: 111.73855805397034 and batch: 800, loss is 6.085620040893555 and perplexity is 439.4922313648383
At time: 112.90544867515564 and batch: 850, loss is 6.1192317295074465 and perplexity is 454.51536958183203
At time: 114.06961011886597 and batch: 900, loss is 6.106161527633667 and perplexity is 448.61341582923774
At time: 115.23149132728577 and batch: 950, loss is 6.074271640777588 and perplexity is 434.5328911881965
At time: 116.39358258247375 and batch: 1000, loss is 6.063095941543579 and perplexity is 429.7037172487803
At time: 117.556232213974 and batch: 1050, loss is 6.058738622665405 and perplexity is 427.83543443754183
At time: 118.71788835525513 and batch: 1100, loss is 6.001726980209351 and perplexity is 404.1261089866985
At time: 119.88129663467407 and batch: 1150, loss is 6.016832494735718 and perplexity is 410.2769808822397
At time: 121.04451584815979 and batch: 1200, loss is 5.994075040817261 and perplexity is 401.04556160594547
At time: 122.20526695251465 and batch: 1250, loss is 5.975634670257568 and perplexity is 393.7179028546498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.610269588275547 and perplexity of 273.2178844155145
Finished 4 epochs...
Completing Train Step...
At time: 125.1410117149353 and batch: 50, loss is 5.979586601257324 and perplexity is 395.2769273897564
At time: 126.30570030212402 and batch: 100, loss is 5.992008256912231 and perplexity is 400.2175430564869
At time: 127.47882986068726 and batch: 150, loss is 5.91424180984497 and perplexity is 370.27345856020787
At time: 128.64365410804749 and batch: 200, loss is 5.930229835510254 and perplexity is 376.24097742518063
At time: 129.83526134490967 and batch: 250, loss is 5.949355754852295 and perplexity is 383.50618745616566
At time: 130.9998791217804 and batch: 300, loss is 5.946567878723145 and perplexity is 382.4385086803839
At time: 132.16464400291443 and batch: 350, loss is 5.98226185798645 and perplexity is 396.3358104100515
At time: 133.3287229537964 and batch: 400, loss is 5.943108205795288 and perplexity is 381.1176826558029
At time: 134.49388480186462 and batch: 450, loss is 5.92129451751709 and perplexity is 372.8941195372869
At time: 135.6589879989624 and batch: 500, loss is 5.895351276397705 and perplexity is 363.3444478128149
At time: 136.82300186157227 and batch: 550, loss is 5.89518856048584 and perplexity is 363.28533069944575
At time: 137.9873378276825 and batch: 600, loss is 5.919460821151733 and perplexity is 372.2109714801036
At time: 139.15303707122803 and batch: 650, loss is 5.9022039031982425 and perplexity is 365.8428622879113
At time: 140.31741452217102 and batch: 700, loss is 5.924477996826172 and perplexity is 374.08311181330953
At time: 141.48215126991272 and batch: 750, loss is 5.871766376495361 and perplexity is 354.87527030918034
At time: 142.6465015411377 and batch: 800, loss is 5.882182674407959 and perplexity is 358.59107571781055
At time: 143.810231924057 and batch: 850, loss is 5.915340309143066 and perplexity is 370.68042718098786
At time: 144.97409772872925 and batch: 900, loss is 5.902527837753296 and perplexity is 365.96139062940244
At time: 146.13967490196228 and batch: 950, loss is 5.87568925857544 and perplexity is 356.2701383102801
At time: 147.3048758506775 and batch: 1000, loss is 5.865634860992432 and perplexity is 352.7060043278831
At time: 148.46836924552917 and batch: 1050, loss is 5.876209897994995 and perplexity is 356.45567488293057
At time: 149.63352346420288 and batch: 1100, loss is 5.832631311416626 and perplexity is 341.2554480715709
At time: 150.7967085838318 and batch: 1150, loss is 5.867120952606201 and perplexity is 353.23054742605393
At time: 151.96144199371338 and batch: 1200, loss is 5.862862911224365 and perplexity is 351.72967479435493
At time: 153.1263861656189 and batch: 1250, loss is 5.849551773071289 and perplexity is 347.07877555459345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.516561466411953 and perplexity of 248.77813282109176
Finished 5 epochs...
Completing Train Step...
At time: 156.01240515708923 and batch: 50, loss is 5.852131433486939 and perplexity is 347.97527677019485
At time: 157.19758772850037 and batch: 100, loss is 5.876678133010865 and perplexity is 356.62261899300506
At time: 158.35920333862305 and batch: 150, loss is 5.789800243377686 and perplexity is 326.9477078826871
At time: 159.54886102676392 and batch: 200, loss is 5.8178004550933835 and perplexity is 336.23168300355
At time: 160.71111512184143 and batch: 250, loss is 5.847310972213745 and perplexity is 346.3019118601768
At time: 161.8728485107422 and batch: 300, loss is 5.847183399200439 and perplexity is 346.25773589965723
At time: 163.03490376472473 and batch: 350, loss is 5.888509197235107 and perplexity is 360.8669017859338
At time: 164.19668889045715 and batch: 400, loss is 5.847704725265503 and perplexity is 346.43829614390313
At time: 165.35877799987793 and batch: 450, loss is 5.830438060760498 and perplexity is 340.5078095152579
At time: 166.52077555656433 and batch: 500, loss is 5.813657903671265 and perplexity is 334.8417069782924
At time: 167.68219542503357 and batch: 550, loss is 5.821331872940063 and perplexity is 337.4211565975154
At time: 168.84340381622314 and batch: 600, loss is 5.8441525650024415 and perplexity is 345.20987486104025
At time: 170.00524187088013 and batch: 650, loss is 5.830080184936524 and perplexity is 340.38597180504377
At time: 171.16687154769897 and batch: 700, loss is 5.856083211898803 and perplexity is 349.3531186265549
At time: 172.3371922969818 and batch: 750, loss is 5.805059213638305 and perplexity is 331.9748502002702
At time: 173.4996144771576 and batch: 800, loss is 5.824221897125244 and perplexity is 338.3977223703156
At time: 174.66239261627197 and batch: 850, loss is 5.852585649490356 and perplexity is 348.13336861090147
At time: 175.82319331169128 and batch: 900, loss is 5.844798316955567 and perplexity is 345.432866802962
At time: 176.99988985061646 and batch: 950, loss is 5.817259120941162 and perplexity is 336.049718566707
At time: 178.16973686218262 and batch: 1000, loss is 5.807266407012939 and perplexity is 332.70839212681767
At time: 179.33568906784058 and batch: 1050, loss is 5.810623693466186 and perplexity is 333.8272666447803
At time: 180.49652433395386 and batch: 1100, loss is 5.782678918838501 and perplexity is 324.6276777957898
At time: 181.65895891189575 and batch: 1150, loss is 5.820445222854614 and perplexity is 337.12211469254214
At time: 182.82113647460938 and batch: 1200, loss is 5.822514686584473 and perplexity is 337.8204990727574
At time: 183.98240733146667 and batch: 1250, loss is 5.809279317855835 and perplexity is 333.3787789449431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.485568081375456 and perplexity of 241.18591831628197
Finished 6 epochs...
Completing Train Step...
At time: 186.8911488056183 and batch: 50, loss is 5.809122581481933 and perplexity is 333.32653045872155
At time: 188.08248257637024 and batch: 100, loss is 5.830719327926635 and perplexity is 340.60359665214474
At time: 189.2464461326599 and batch: 150, loss is 5.741496047973633 and perplexity is 311.53012654265956
At time: 190.40912055969238 and batch: 200, loss is 5.770428609848023 and perplexity is 320.67514772285557
At time: 191.5722050666809 and batch: 250, loss is 5.799457292556763 and perplexity is 330.1203525129804
At time: 192.7357213497162 and batch: 300, loss is 5.7990124320983885 and perplexity is 329.97352768234475
At time: 193.8989405632019 and batch: 350, loss is 5.841038112640381 and perplexity is 344.1364076498209
At time: 195.06195735931396 and batch: 400, loss is 5.797279434204102 and perplexity is 329.4021794693192
At time: 196.23790955543518 and batch: 450, loss is 5.773803567886352 and perplexity is 321.75924124641506
At time: 197.40388703346252 and batch: 500, loss is 5.765804576873779 and perplexity is 319.1957582745452
At time: 198.5661301612854 and batch: 550, loss is 5.7753236198425295 and perplexity is 322.24870392059313
At time: 199.729483127594 and batch: 600, loss is 5.796518154144287 and perplexity is 329.15150758628926
At time: 200.89363884925842 and batch: 650, loss is 5.782781238555908 and perplexity is 324.6608953074186
At time: 202.05708956718445 and batch: 700, loss is 5.803606977462769 and perplexity is 331.4930942098478
At time: 203.22043919563293 and batch: 750, loss is 5.757886457443237 and perplexity is 316.6783080276165
At time: 204.38342761993408 and batch: 800, loss is 5.775379028320312 and perplexity is 322.2665597254218
At time: 205.54647874832153 and batch: 850, loss is 5.804768743515015 and perplexity is 331.87843542808656
At time: 206.71034479141235 and batch: 900, loss is 5.8002181720733645 and perplexity is 330.3716299110105
At time: 207.87305212020874 and batch: 950, loss is 5.775405645370483 and perplexity is 322.2751376247692
At time: 209.03649020195007 and batch: 1000, loss is 5.762917051315307 and perplexity is 318.27540178030097
At time: 210.19932317733765 and batch: 1050, loss is 5.7674340152740475 and perplexity is 319.71629207399013
At time: 211.3611249923706 and batch: 1100, loss is 5.740478296279907 and perplexity is 311.21322751833765
At time: 212.52395486831665 and batch: 1150, loss is 5.783973779678345 and perplexity is 325.0482977266655
At time: 213.68785095214844 and batch: 1200, loss is 5.784633893966674 and perplexity is 325.2629375880177
At time: 214.85180473327637 and batch: 1250, loss is 5.7734032821655275 and perplexity is 321.6304713906874
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.4605066898095345 and perplexity of 235.21657601832723
Finished 7 epochs...
Completing Train Step...
At time: 217.7640929222107 and batch: 50, loss is 5.7718619441986085 and perplexity is 321.13511199001033
At time: 218.9242548942566 and batch: 100, loss is 5.7907399559021 and perplexity is 327.2550891410025
At time: 220.088148355484 and batch: 150, loss is 5.699266176223755 and perplexity is 298.64816541225565
At time: 221.25150346755981 and batch: 200, loss is 5.731738758087158 and perplexity is 308.5052182419518
At time: 222.41419386863708 and batch: 250, loss is 5.759533214569092 and perplexity is 317.2002299093074
At time: 223.57754182815552 and batch: 300, loss is 5.761745729446411 and perplexity is 317.90281709275894
At time: 224.7403872013092 and batch: 350, loss is 5.798947887420654 and perplexity is 329.9522303346623
At time: 225.90206694602966 and batch: 400, loss is 5.756166906356811 and perplexity is 316.1342314168267
At time: 227.06492972373962 and batch: 450, loss is 5.734172716140747 and perplexity is 309.25702156018673
At time: 228.22710275650024 and batch: 500, loss is 5.721723766326904 and perplexity is 305.4309610491295
At time: 229.39034485816956 and batch: 550, loss is 5.731399459838867 and perplexity is 308.40056071787495
At time: 230.55313181877136 and batch: 600, loss is 5.752362785339355 and perplexity is 314.9339030873988
At time: 231.7195942401886 and batch: 650, loss is 5.734633588790894 and perplexity is 309.3995825119882
At time: 232.88900351524353 and batch: 700, loss is 5.75606143951416 and perplexity is 316.10089149574424
At time: 234.0515787601471 and batch: 750, loss is 5.712971849441528 and perplexity is 302.76951801589814
At time: 235.21412658691406 and batch: 800, loss is 5.727220020294189 and perplexity is 307.1143089967863
At time: 236.37615370750427 and batch: 850, loss is 5.757262935638428 and perplexity is 316.4809137436665
At time: 237.53914499282837 and batch: 900, loss is 5.754097652435303 and perplexity is 315.4807457657926
At time: 238.7015097141266 and batch: 950, loss is 5.730644912719726 and perplexity is 308.16794573376336
At time: 239.86324954032898 and batch: 1000, loss is 5.7175795841217045 and perplexity is 304.1678186493533
At time: 241.02539348602295 and batch: 1050, loss is 5.719137563705444 and perplexity is 304.6420752459568
At time: 242.19427919387817 and batch: 1100, loss is 5.697305669784546 and perplexity is 298.06323732589857
At time: 243.35783219337463 and batch: 1150, loss is 5.740947828292847 and perplexity is 311.3593864019602
At time: 244.52084970474243 and batch: 1200, loss is 5.741728086471557 and perplexity is 311.6024219126109
At time: 245.6829354763031 and batch: 1250, loss is 5.729570055007935 and perplexity is 307.836886993158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.425883717780565 and perplexity of 227.21204894758756
Finished 8 epochs...
Completing Train Step...
At time: 248.58987021446228 and batch: 50, loss is 5.7227639484405515 and perplexity is 305.7488301639902
At time: 249.77662634849548 and batch: 100, loss is 5.741209468841553 and perplexity is 311.44086130076687
At time: 250.9394829273224 and batch: 150, loss is 5.652584896087647 and perplexity is 285.0272803139719
At time: 252.10362219810486 and batch: 200, loss is 5.688494977951049 and perplexity is 295.44862916226487
At time: 253.26621413230896 and batch: 250, loss is 5.71547700881958 and perplexity is 303.5289547712244
At time: 254.42935609817505 and batch: 300, loss is 5.715932550430298 and perplexity is 303.6672563388484
At time: 255.59183931350708 and batch: 350, loss is 5.758415031433105 and perplexity is 316.8457401906403
At time: 256.7595431804657 and batch: 400, loss is 5.71281907081604 and perplexity is 302.7232648384343
At time: 257.92625761032104 and batch: 450, loss is 5.688065595626831 and perplexity is 295.32179597510117
At time: 259.0989582538605 and batch: 500, loss is 5.676209144592285 and perplexity is 291.8410032619364
At time: 260.25948572158813 and batch: 550, loss is 5.684667310714722 and perplexity is 294.3199116799387
At time: 261.43325567245483 and batch: 600, loss is 5.704041919708252 and perplexity is 300.0778436132488
At time: 262.6026556491852 and batch: 650, loss is 5.6890199089050295 and perplexity is 295.6037600059518
At time: 263.7663629055023 and batch: 700, loss is 5.709693050384521 and perplexity is 301.7784232938559
At time: 264.93607449531555 and batch: 750, loss is 5.668157234191894 and perplexity is 289.5005608138186
At time: 266.10149788856506 and batch: 800, loss is 5.6738146209716795 and perplexity is 291.14301908946356
At time: 267.2753870487213 and batch: 850, loss is 5.698401288986206 and perplexity is 298.38998009216147
At time: 268.4411520957947 and batch: 900, loss is 5.692555360794067 and perplexity is 296.6507024945567
At time: 269.6038498878479 and batch: 950, loss is 5.673236570358276 and perplexity is 290.97477232094957
At time: 270.76852440834045 and batch: 1000, loss is 5.656376571655273 and perplexity is 286.1100627714944
At time: 271.9311294555664 and batch: 1050, loss is 5.659694995880127 and perplexity is 287.06107439206295
At time: 273.09261560440063 and batch: 1100, loss is 5.634112453460693 and perplexity is 279.81046226155877
At time: 274.25515937805176 and batch: 1150, loss is 5.672009611129761 and perplexity is 290.61797707015893
At time: 275.4180371761322 and batch: 1200, loss is 5.675832204818725 and perplexity is 291.73101751055583
At time: 276.62763833999634 and batch: 1250, loss is 5.666182193756104 and perplexity is 288.92934976816446
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.368480821595575 and perplexity of 214.53670049162915
Finished 9 epochs...
Completing Train Step...
At time: 279.5357484817505 and batch: 50, loss is 5.654632511138916 and perplexity is 285.6115043920487
At time: 280.70038986206055 and batch: 100, loss is 5.672742347717286 and perplexity is 290.8310015308276
At time: 281.86536288261414 and batch: 150, loss is 5.584747524261474 and perplexity is 266.3330316213323
At time: 283.03088879585266 and batch: 200, loss is 5.612664766311646 and perplexity is 273.87307422622933
At time: 284.1982958316803 and batch: 250, loss is 5.643171873092651 and perplexity is 282.35689986385955
At time: 285.3634979724884 and batch: 300, loss is 5.640817012786865 and perplexity is 281.6927710804634
At time: 286.52865624427795 and batch: 350, loss is 5.682942867279053 and perplexity is 293.81281099913963
At time: 287.692999124527 and batch: 400, loss is 5.632671756744385 and perplexity is 279.40763049626725
At time: 288.85791659355164 and batch: 450, loss is 5.609377059936524 and perplexity is 272.97413850219397
At time: 290.0220079421997 and batch: 500, loss is 5.603902816772461 and perplexity is 271.4838943914028
At time: 291.18888211250305 and batch: 550, loss is 5.6090107345581055 and perplexity is 272.87415946115686
At time: 292.3536717891693 and batch: 600, loss is 5.628591823577881 and perplexity is 278.2699883676423
At time: 293.52562165260315 and batch: 650, loss is 5.613339424133301 and perplexity is 274.0579071803851
At time: 294.6911244392395 and batch: 700, loss is 5.63383807182312 and perplexity is 279.733697940555
At time: 295.8571090698242 and batch: 750, loss is 5.59763072013855 and perplexity is 269.7864500035165
At time: 297.02183628082275 and batch: 800, loss is 5.61158712387085 and perplexity is 273.57809594715496
At time: 298.1865322589874 and batch: 850, loss is 5.637844982147217 and perplexity is 280.85681439361963
At time: 299.3516936302185 and batch: 900, loss is 5.634829416275024 and perplexity is 280.0111478915315
At time: 300.5167191028595 and batch: 950, loss is 5.611997547149659 and perplexity is 273.69040181119794
At time: 301.6819567680359 and batch: 1000, loss is 5.598760900497436 and perplexity is 270.0915297154798
At time: 302.8460371494293 and batch: 1050, loss is 5.600982151031494 and perplexity is 270.6921374733087
At time: 304.01112604141235 and batch: 1100, loss is 5.57650857925415 and perplexity is 264.14774301795865
At time: 305.17635846138 and batch: 1150, loss is 5.615605087280273 and perplexity is 274.6795340134107
At time: 306.3873064517975 and batch: 1200, loss is 5.623419418334961 and perplexity is 276.8343791977233
At time: 307.5519127845764 and batch: 1250, loss is 5.6133156013488765 and perplexity is 274.0513784357093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.3221399905907845 and perplexity of 204.82172990545155
Finished 10 epochs...
Completing Train Step...
At time: 310.4338126182556 and batch: 50, loss is 5.605073289871216 and perplexity is 271.80184502656624
At time: 311.62286710739136 and batch: 100, loss is 5.619659490585327 and perplexity is 275.7954562947939
At time: 312.78631258010864 and batch: 150, loss is 5.532447061538696 and perplexity is 252.76167823932016
At time: 313.9490616321564 and batch: 200, loss is 5.563369665145874 and perplexity is 260.6998290346693
At time: 315.1272211074829 and batch: 250, loss is 5.596205377578736 and perplexity is 269.40218581337336
At time: 316.3004992008209 and batch: 300, loss is 5.593193273544312 and perplexity is 268.59193928861373
At time: 317.4657053947449 and batch: 350, loss is 5.631857671737671 and perplexity is 279.1802614949234
At time: 318.627858877182 and batch: 400, loss is 5.586416597366333 and perplexity is 266.7779321037779
At time: 319.78992223739624 and batch: 450, loss is 5.559567651748657 and perplexity is 259.7105266542016
At time: 320.9528999328613 and batch: 500, loss is 5.556654472351074 and perplexity is 258.95504426077946
At time: 322.11642241477966 and batch: 550, loss is 5.563967504501343 and perplexity is 260.8557322503257
At time: 323.2784616947174 and batch: 600, loss is 5.58567400932312 and perplexity is 266.57989953861227
At time: 324.443941116333 and batch: 650, loss is 5.56588059425354 and perplexity is 261.3552503376131
At time: 325.61048698425293 and batch: 700, loss is 5.589313983917236 and perplexity is 267.5520117588007
At time: 326.78072595596313 and batch: 750, loss is 5.551863327026367 and perplexity is 257.7173204361871
At time: 327.94438791275024 and batch: 800, loss is 5.5648603439331055 and perplexity is 261.0887385372018
At time: 329.10684609413147 and batch: 850, loss is 5.5924342823028566 and perplexity is 268.38815770317007
At time: 330.2688879966736 and batch: 900, loss is 5.583961267471313 and perplexity is 266.12370776870966
At time: 331.43052315711975 and batch: 950, loss is 5.561839780807495 and perplexity is 260.30129338366896
At time: 332.5933392047882 and batch: 1000, loss is 5.548521900177002 and perplexity is 256.857613985139
At time: 333.75683546066284 and batch: 1050, loss is 5.548825159072876 and perplexity is 256.9355201538251
At time: 334.96553444862366 and batch: 1100, loss is 5.526294279098511 and perplexity is 251.2112651921112
At time: 336.12807297706604 and batch: 1150, loss is 5.561326265335083 and perplexity is 260.1676589566371
At time: 337.28959918022156 and batch: 1200, loss is 5.571717138290405 and perplexity is 262.88512200419893
At time: 338.45186281204224 and batch: 1250, loss is 5.561987590789795 and perplexity is 260.33977135688093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.282667173956432 and perplexity of 196.8943270602292
Finished 11 epochs...
Completing Train Step...
At time: 341.3407258987427 and batch: 50, loss is 5.554458389282226 and perplexity is 258.38698145734156
At time: 342.53283524513245 and batch: 100, loss is 5.568039894104004 and perplexity is 261.92020442371023
At time: 343.6987223625183 and batch: 150, loss is 5.478981504440307 and perplexity is 239.60254892604152
At time: 344.8639304637909 and batch: 200, loss is 5.509631090164184 and perplexity is 247.05996740179808
At time: 346.02939653396606 and batch: 250, loss is 5.544366788864136 and perplexity is 255.7925562565289
At time: 347.1955609321594 and batch: 300, loss is 5.542329683303833 and perplexity is 255.27201020153953
At time: 348.3601415157318 and batch: 350, loss is 5.5814026927948 and perplexity is 265.4436807101705
At time: 349.5264856815338 and batch: 400, loss is 5.538197689056396 and perplexity is 254.21940390306847
At time: 350.6949303150177 and batch: 450, loss is 5.509958419799805 and perplexity is 247.14085068793025
At time: 351.8746259212494 and batch: 500, loss is 5.506795711517334 and perplexity is 246.36045101129326
At time: 353.0460503101349 and batch: 550, loss is 5.514358177185058 and perplexity is 248.23060604258296
At time: 354.22205233573914 and batch: 600, loss is 5.533539915084839 and perplexity is 253.0380607309985
At time: 355.3904433250427 and batch: 650, loss is 5.518801002502442 and perplexity is 249.33590477014766
At time: 356.55856132507324 and batch: 700, loss is 5.5406438255310055 and perplexity is 254.8420204515772
At time: 357.73703694343567 and batch: 750, loss is 5.5023332118988035 and perplexity is 245.26351694690882
At time: 358.91550517082214 and batch: 800, loss is 5.519629192352295 and perplexity is 249.54248776884575
At time: 360.08088207244873 and batch: 850, loss is 5.54845404624939 and perplexity is 256.8401857784857
At time: 361.2455117702484 and batch: 900, loss is 5.54066421508789 and perplexity is 254.84721662042344
At time: 362.41014528274536 and batch: 950, loss is 5.517881870269775 and perplexity is 249.10683739104462
At time: 363.5754680633545 and batch: 1000, loss is 5.505445041656494 and perplexity is 246.02792399287284
At time: 364.7742636203766 and batch: 1050, loss is 5.505354652404785 and perplexity is 246.00568671794403
At time: 365.93910574913025 and batch: 1100, loss is 5.478358879089355 and perplexity is 239.45341273773104
At time: 367.320015668869 and batch: 1150, loss is 5.50902985572815 and perplexity is 246.9114710866529
At time: 368.48475456237793 and batch: 1200, loss is 5.521861095428466 and perplexity is 250.10006441198604
At time: 369.6499195098877 and batch: 1250, loss is 5.515471801757813 and perplexity is 248.50719572513066
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.2458210966012775 and perplexity of 189.7715720679075
Finished 12 epochs...
Completing Train Step...
At time: 372.6511158943176 and batch: 50, loss is 5.508238925933838 and perplexity is 246.71625865745287
At time: 373.8142476081848 and batch: 100, loss is 5.518897123336792 and perplexity is 249.35987229721863
At time: 374.98288083076477 and batch: 150, loss is 5.429055061340332 and perplexity is 227.93376020801838
At time: 376.14943408966064 and batch: 200, loss is 5.457026853561401 and perplexity is 234.399483349812
At time: 377.3232605457306 and batch: 250, loss is 5.490267658233643 and perplexity is 242.32205767256903
At time: 378.53960633277893 and batch: 300, loss is 5.48878475189209 and perplexity is 241.96298305935946
At time: 379.7121784687042 and batch: 350, loss is 5.520183029174805 and perplexity is 249.68073186615496
At time: 380.87583446502686 and batch: 400, loss is 5.48350248336792 and perplexity is 240.68823934326315
At time: 382.03612208366394 and batch: 450, loss is 5.448156051635742 and perplexity is 232.32936733108397
At time: 383.1967513561249 and batch: 500, loss is 5.447129030227661 and perplexity is 232.09088258254238
At time: 384.35757851600647 and batch: 550, loss is 5.451753063201904 and perplexity is 233.16656355210694
At time: 385.51880836486816 and batch: 600, loss is 5.474038095474243 and perplexity is 238.42101833795638
At time: 386.6944913864136 and batch: 650, loss is 5.459397974014283 and perplexity is 234.95593220184853
At time: 387.86556673049927 and batch: 700, loss is 5.480719070434571 and perplexity is 240.01923607306642
At time: 389.04043841362 and batch: 750, loss is 5.4527771949768065 and perplexity is 233.4054791583352
At time: 390.2035422325134 and batch: 800, loss is 5.472161655426025 and perplexity is 237.97405507184402
At time: 391.3740653991699 and batch: 850, loss is 5.495330419540405 and perplexity is 243.55198720266378
At time: 392.53704261779785 and batch: 900, loss is 5.476591053009034 and perplexity is 239.03047470026524
At time: 393.6996910572052 and batch: 950, loss is 5.454969120025635 and perplexity is 233.91764758697923
At time: 394.86209535598755 and batch: 1000, loss is 5.4468246173858645 and perplexity is 232.02024188993295
At time: 396.02639603614807 and batch: 1050, loss is 5.4457935619354245 and perplexity is 231.781139440044
At time: 397.1951334476471 and batch: 1100, loss is 5.421231727600098 and perplexity is 226.15751546845814
At time: 398.358952999115 and batch: 1150, loss is 5.448454666137695 and perplexity is 232.39875460890676
At time: 399.52290058135986 and batch: 1200, loss is 5.468810005187988 and perplexity is 237.17778442927283
At time: 400.6865155696869 and batch: 1250, loss is 5.463347597122192 and perplexity is 235.88575459628925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.195737073021213 and perplexity of 180.50113629589077
Finished 13 epochs...
Completing Train Step...
At time: 403.6276729106903 and batch: 50, loss is 5.446879539489746 and perplexity is 232.03298527970438
At time: 404.7947955131531 and batch: 100, loss is 5.453027057647705 and perplexity is 233.46380576127697
At time: 405.9591681957245 and batch: 150, loss is 5.373144445419311 and perplexity is 215.53955561099792
At time: 407.1311595439911 and batch: 200, loss is 5.405111017227173 and perplexity is 222.54092498890245
At time: 408.29461145401 and batch: 250, loss is 5.436099100112915 and perplexity is 229.54500261722748
At time: 409.45831179618835 and batch: 300, loss is 5.440008306503296 and perplexity is 230.44409763734865
At time: 410.62258791923523 and batch: 350, loss is 5.467817802429199 and perplexity is 236.94257268540562
At time: 411.7925226688385 and batch: 400, loss is 5.437201023101807 and perplexity is 229.79808294450373
At time: 412.9556636810303 and batch: 450, loss is 5.393867111206054 and perplexity is 220.05271058136168
At time: 414.1223192214966 and batch: 500, loss is 5.399921989440918 and perplexity is 221.38914483955773
At time: 415.28646636009216 and batch: 550, loss is 5.407508859634399 and perplexity is 223.075183333859
At time: 416.4932372570038 and batch: 600, loss is 5.429975271224976 and perplexity is 228.14360364239937
At time: 417.65539741516113 and batch: 650, loss is 5.415994615554809 and perplexity is 224.97619926275988
At time: 418.81754422187805 and batch: 700, loss is 5.435149345397949 and perplexity is 229.32709466462774
At time: 419.9794192314148 and batch: 750, loss is 5.412184085845947 and perplexity is 224.12055204160313
At time: 421.14140152931213 and batch: 800, loss is 5.432155027389526 and perplexity is 228.6414434564264
At time: 422.3042995929718 and batch: 850, loss is 5.457045221328736 and perplexity is 234.403788784526
At time: 423.468355178833 and batch: 900, loss is 5.4371231746673585 and perplexity is 229.78019421982094
At time: 424.63236379623413 and batch: 950, loss is 5.4141495990753175 and perplexity is 224.56149715138156
At time: 425.7953824996948 and batch: 1000, loss is 5.410798282623291 and perplexity is 223.8101801651648
At time: 426.96127009391785 and batch: 1050, loss is 5.410322885513306 and perplexity is 223.70380673914218
At time: 428.1235432624817 and batch: 1100, loss is 5.387186431884766 and perplexity is 218.58750870961651
At time: 429.2942097187042 and batch: 1150, loss is 5.41268295288086 and perplexity is 224.23238628975366
At time: 430.4561893939972 and batch: 1200, loss is 5.434209327697754 and perplexity is 229.1116244252945
At time: 431.61769104003906 and batch: 1250, loss is 5.42946231842041 and perplexity is 228.02660675057456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.173596904225593 and perplexity of 176.5487256404282
Finished 14 epochs...
Completing Train Step...
At time: 434.50296211242676 and batch: 50, loss is 5.410449857711792 and perplexity is 223.73221270663944
At time: 435.6932330131531 and batch: 100, loss is 5.418296518325806 and perplexity is 225.49466910389606
At time: 436.85396218299866 and batch: 150, loss is 5.341959476470947 and perplexity is 208.92168659767114
At time: 438.0151114463806 and batch: 200, loss is 5.37374870300293 and perplexity is 215.66983637965276
At time: 439.17587065696716 and batch: 250, loss is 5.402465200424194 and perplexity is 221.9529007154989
At time: 440.33664774894714 and batch: 300, loss is 5.406450729370118 and perplexity is 222.83926556903862
At time: 441.4998745918274 and batch: 350, loss is 5.432064695358276 and perplexity is 228.6207907432259
At time: 442.66240191459656 and batch: 400, loss is 5.403017492294311 and perplexity is 222.07551735508287
At time: 443.82497096061707 and batch: 450, loss is 5.359444065093994 and perplexity is 212.60671809311734
At time: 444.9885001182556 and batch: 500, loss is 5.368113689422607 and perplexity is 214.4579516231025
At time: 446.1809346675873 and batch: 550, loss is 5.374308242797851 and perplexity is 215.79054600344114
At time: 447.3526825904846 and batch: 600, loss is 5.398589611053467 and perplexity is 221.09436714907747
At time: 448.5305824279785 and batch: 650, loss is 5.385425291061401 and perplexity is 218.202884113003
At time: 449.6948449611664 and batch: 700, loss is 5.407202587127686 and perplexity is 223.00687199974956
At time: 450.85743618011475 and batch: 750, loss is 5.387680644989014 and perplexity is 218.6955642198709
At time: 452.01974964141846 and batch: 800, loss is 5.409809179306031 and perplexity is 223.58891821702912
At time: 453.1821219921112 and batch: 850, loss is 5.432343034744263 and perplexity is 228.68443377051486
At time: 454.3448119163513 and batch: 900, loss is 5.407914333343506 and perplexity is 223.16565279610967
At time: 455.50895500183105 and batch: 950, loss is 5.387910585403443 and perplexity is 218.7458569504855
At time: 456.6713833808899 and batch: 1000, loss is 5.38315263748169 and perplexity is 217.70754762464105
At time: 457.84318447113037 and batch: 1050, loss is 5.384525108337402 and perplexity is 218.00655002793135
At time: 459.0058376789093 and batch: 1100, loss is 5.3620427799224855 and perplexity is 213.15994084680878
At time: 460.16933703422546 and batch: 1150, loss is 5.388760051727295 and perplexity is 218.93175313454267
At time: 461.33234548568726 and batch: 1200, loss is 5.408489131927491 and perplexity is 223.2939649706263
At time: 462.4974365234375 and batch: 1250, loss is 5.406228408813477 and perplexity is 222.78972932614246
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.1539992729242705 and perplexity of 173.12247169796106
Finished 15 epochs...
Completing Train Step...
At time: 465.4235887527466 and batch: 50, loss is 5.384296531677246 and perplexity is 217.95672451352564
At time: 466.58700919151306 and batch: 100, loss is 5.392626218795776 and perplexity is 219.77981819300385
At time: 467.75057578086853 and batch: 150, loss is 5.31470703125 and perplexity is 203.3049424082479
At time: 468.9140627384186 and batch: 200, loss is 5.349289665222168 and perplexity is 210.45874858951422
At time: 470.077707529068 and batch: 250, loss is 5.376261148452759 and perplexity is 216.21237634436986
At time: 471.2482967376709 and batch: 300, loss is 5.381790237426758 and perplexity is 217.41114480525383
At time: 472.42283272743225 and batch: 350, loss is 5.407920608520508 and perplexity is 223.16705320447565
At time: 473.5879023075104 and batch: 400, loss is 5.379393634796142 and perplexity is 216.89072055784547
At time: 474.7818601131439 and batch: 450, loss is 5.337291746139527 and perplexity is 207.9487689293725
At time: 475.95302629470825 and batch: 500, loss is 5.344517469406128 and perplexity is 209.45679090072719
At time: 477.1189024448395 and batch: 550, loss is 5.348976411819458 and perplexity is 210.39283199522626
At time: 478.2861363887787 and batch: 600, loss is 5.373888645172119 and perplexity is 215.70001979630166
At time: 479.4532587528229 and batch: 650, loss is 5.363223962783813 and perplexity is 213.41187047384665
At time: 480.6195116043091 and batch: 700, loss is 5.384288778305054 and perplexity is 217.95503462046986
At time: 481.7854778766632 and batch: 750, loss is 5.367903985977173 and perplexity is 214.41298376686836
At time: 482.95255494117737 and batch: 800, loss is 5.384944086074829 and perplexity is 218.09790905636936
At time: 484.1183226108551 and batch: 850, loss is 5.409915714263916 and perplexity is 223.61273952189342
At time: 485.2848081588745 and batch: 900, loss is 5.383238153457642 and perplexity is 217.7261658941167
At time: 486.45038294792175 and batch: 950, loss is 5.361729755401611 and perplexity is 213.0932270005352
At time: 487.6139934062958 and batch: 1000, loss is 5.361350746154785 and perplexity is 213.01247800034147
At time: 488.777489900589 and batch: 1050, loss is 5.358506031036377 and perplexity is 212.40737925859239
At time: 489.9409227371216 and batch: 1100, loss is 5.340868425369263 and perplexity is 208.69386666553655
At time: 491.10268449783325 and batch: 1150, loss is 5.3692057418823245 and perplexity is 214.6922788822205
At time: 492.26506638526917 and batch: 1200, loss is 5.385656356811523 and perplexity is 218.2533091516247
At time: 493.4288601875305 and batch: 1250, loss is 5.383984756469727 and perplexity is 217.88878160251812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.146001885407163 and perplexity of 171.74346577613326
Finished 16 epochs...
Completing Train Step...
At time: 496.3433949947357 and batch: 50, loss is 5.359996395111084 and perplexity is 212.72417960112736
At time: 497.53218483924866 and batch: 100, loss is 5.367613582611084 and perplexity is 214.35072655493883
At time: 498.69648146629333 and batch: 150, loss is 5.294704866409302 and perplexity is 199.27880341794
At time: 499.86047887802124 and batch: 200, loss is 5.329383907318115 and perplexity is 206.31082840197345
At time: 501.02546095848083 and batch: 250, loss is 5.35394868850708 and perplexity is 211.44156851236133
At time: 502.189834356308 and batch: 300, loss is 5.36054310798645 and perplexity is 212.84051044590467
At time: 503.3561897277832 and batch: 350, loss is 5.386907148361206 and perplexity is 218.52646934401918
At time: 504.5539433956146 and batch: 400, loss is 5.358279657363892 and perplexity is 212.35930126208825
At time: 505.7212882041931 and batch: 450, loss is 5.315503187179566 and perplexity is 203.46686929462828
At time: 506.8926753997803 and batch: 500, loss is 5.324193277359009 and perplexity is 205.24271971180343
At time: 508.07096123695374 and batch: 550, loss is 5.327968626022339 and perplexity is 206.0190470704543
At time: 509.2351927757263 and batch: 600, loss is 5.354105281829834 and perplexity is 211.47468144270724
At time: 510.3994879722595 and batch: 650, loss is 5.343091373443603 and perplexity is 209.15829830702995
At time: 511.5644428730011 and batch: 700, loss is 5.363026428222656 and perplexity is 213.36971841704795
At time: 512.7279298305511 and batch: 750, loss is 5.346119527816772 and perplexity is 209.79262185282857
At time: 513.8921437263489 and batch: 800, loss is 5.368353977203369 and perplexity is 214.50948944007058
At time: 515.0568459033966 and batch: 850, loss is 5.392182102203369 and perplexity is 219.68223200049582
At time: 516.2209057807922 and batch: 900, loss is 5.364287490844727 and perplexity is 213.63896072364912
At time: 517.3858952522278 and batch: 950, loss is 5.34411211013794 and perplexity is 209.37190285549093
At time: 518.550714969635 and batch: 1000, loss is 5.343556432723999 and perplexity is 209.25559193662858
At time: 519.7148697376251 and batch: 1050, loss is 5.344448623657226 and perplexity is 209.44237118746875
At time: 520.8790175914764 and batch: 1100, loss is 5.3244513988494875 and perplexity is 205.29570410643663
At time: 522.0440495014191 and batch: 1150, loss is 5.354226264953613 and perplexity is 211.5002678579995
At time: 523.208176612854 and batch: 1200, loss is 5.36978235244751 and perplexity is 214.81610841576554
At time: 524.3731849193573 and batch: 1250, loss is 5.367161359786987 and perplexity is 214.25381417867473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.138153744440009 and perplexity of 170.4008741618751
Finished 17 epochs...
Completing Train Step...
At time: 527.2863790988922 and batch: 50, loss is 5.344330549240112 and perplexity is 209.4176428614919
At time: 528.4494225978851 and batch: 100, loss is 5.350603170394898 and perplexity is 210.73536887575025
At time: 529.61297082901 and batch: 150, loss is 5.276960849761963 and perplexity is 195.77398375907694
At time: 530.776025056839 and batch: 200, loss is 5.3156367206573485 and perplexity is 203.4940407474077
At time: 531.9430458545685 and batch: 250, loss is 5.338356838226319 and perplexity is 208.17037151024576
At time: 533.1057236194611 and batch: 300, loss is 5.345363321304322 and perplexity is 209.63403527557793
At time: 534.2949655056 and batch: 350, loss is 5.372687520980835 and perplexity is 215.4410928173225
At time: 535.4576730728149 and batch: 400, loss is 5.341665735244751 and perplexity is 208.86032669767832
At time: 536.6197171211243 and batch: 450, loss is 5.300798044204712 and perplexity is 200.49675141666444
At time: 537.781822681427 and batch: 500, loss is 5.311368741989136 and perplexity is 202.62738327574996
At time: 538.9448549747467 and batch: 550, loss is 5.310677146911621 and perplexity is 202.48729562245612
At time: 540.1090519428253 and batch: 600, loss is 5.340490398406982 and perplexity is 208.61498966682976
At time: 541.2738716602325 and batch: 650, loss is 5.329695835113525 and perplexity is 206.37519252180326
At time: 542.4422602653503 and batch: 700, loss is 5.350269069671631 and perplexity is 210.66497379676923
At time: 543.6132509708405 and batch: 750, loss is 5.33028790473938 and perplexity is 206.49741718400986
At time: 544.7818632125854 and batch: 800, loss is 5.351654863357544 and perplexity is 210.9571143638235
At time: 545.9468278884888 and batch: 850, loss is 5.378884658813477 and perplexity is 216.78035647893387
At time: 547.1086392402649 and batch: 900, loss is 5.351842060089111 and perplexity is 210.99660854260856
At time: 548.2697184085846 and batch: 950, loss is 5.332908363342285 and perplexity is 207.03924472561687
At time: 549.4324305057526 and batch: 1000, loss is 5.3305277633667 and perplexity is 206.54695331163705
At time: 550.5941643714905 and batch: 1050, loss is 5.328653726577759 and perplexity is 206.1602391939079
At time: 551.7590539455414 and batch: 1100, loss is 5.310082454681396 and perplexity is 202.36691379964543
At time: 552.9265756607056 and batch: 1150, loss is 5.337959928512573 and perplexity is 208.08776306281143
At time: 554.089524269104 and batch: 1200, loss is 5.351089086532593 and perplexity is 210.83779347513763
At time: 555.2513213157654 and batch: 1250, loss is 5.3517453002929685 and perplexity is 210.97619354147082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.130976099167427 and perplexity of 169.18217605648033
Finished 18 epochs...
Completing Train Step...
At time: 558.1305849552155 and batch: 50, loss is 5.329423818588257 and perplexity is 206.31906269349847
At time: 559.3181982040405 and batch: 100, loss is 5.338768348693848 and perplexity is 208.25605342544506
At time: 560.4836277961731 and batch: 150, loss is 5.262234735488891 and perplexity is 192.91211750542348
At time: 561.6486926078796 and batch: 200, loss is 5.304445791244507 and perplexity is 201.22944838228446
At time: 562.8131875991821 and batch: 250, loss is 5.327046871185303 and perplexity is 205.82923551059636
At time: 564.0057799816132 and batch: 300, loss is 5.332831058502197 and perplexity is 207.02324020853283
At time: 565.1704843044281 and batch: 350, loss is 5.356743116378784 and perplexity is 212.03325304949988
At time: 566.3364734649658 and batch: 400, loss is 5.3270862102508545 and perplexity is 205.83733279965344
At time: 567.5036067962646 and batch: 450, loss is 5.287851362228394 and perplexity is 197.9177147465813
At time: 568.6786036491394 and batch: 500, loss is 5.295250387191772 and perplexity is 199.38754380408386
At time: 569.8443479537964 and batch: 550, loss is 5.294835243225098 and perplexity is 199.30478644754075
At time: 571.0104579925537 and batch: 600, loss is 5.324387445449829 and perplexity is 205.28257516804828
At time: 572.1779453754425 and batch: 650, loss is 5.312978181838989 and perplexity is 202.953762434426
At time: 573.3445372581482 and batch: 700, loss is 5.334859209060669 and perplexity is 207.4435405809241
At time: 574.5168838500977 and batch: 750, loss is 5.317608995437622 and perplexity is 203.89578295469454
At time: 575.6833500862122 and batch: 800, loss is 5.339517488479614 and perplexity is 208.41212477302153
At time: 576.8472604751587 and batch: 850, loss is 5.365499620437622 and perplexity is 213.8980758389617
At time: 578.0121281147003 and batch: 900, loss is 5.33707914352417 and perplexity is 207.90456317653369
At time: 579.1767027378082 and batch: 950, loss is 5.3170715427398685 and perplexity is 203.78622805900923
At time: 580.3423366546631 and batch: 1000, loss is 5.313345069885254 and perplexity is 203.02823740496044
At time: 581.50599360466 and batch: 1050, loss is 5.316609468460083 and perplexity is 203.69208543656592
At time: 582.6692321300507 and batch: 1100, loss is 5.298711585998535 and perplexity is 200.07885943303955
At time: 583.8331625461578 and batch: 1150, loss is 5.327281980514527 and perplexity is 205.877633573287
At time: 584.9962553977966 and batch: 1200, loss is 5.341203317642212 and perplexity is 208.7637683330056
At time: 586.1601371765137 and batch: 1250, loss is 5.339968547821045 and perplexity is 208.50615221315138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.125548425382071 and perplexity of 168.26639791526128
Finished 19 epochs...
Completing Train Step...
At time: 589.0309665203094 and batch: 50, loss is 5.319097452163696 and perplexity is 204.19949898225997
At time: 590.2183735370636 and batch: 100, loss is 5.3228609752655025 and perplexity is 204.96945648166212
At time: 591.378885269165 and batch: 150, loss is 5.252382869720459 and perplexity is 191.02090450359245
At time: 592.5673308372498 and batch: 200, loss is 5.290205364227295 and perplexity is 198.3841622364797
At time: 593.7282366752625 and batch: 250, loss is 5.31364727973938 and perplexity is 203.08960381126928
At time: 594.8964884281158 and batch: 300, loss is 5.321613836288452 and perplexity is 204.71399041733375
At time: 596.0576660633087 and batch: 350, loss is 5.348538398742676 and perplexity is 210.300697363111
At time: 597.2183799743652 and batch: 400, loss is 5.3138963413238525 and perplexity is 203.14019192930138
At time: 598.3803796768188 and batch: 450, loss is 5.272765188217163 and perplexity is 194.95430313762014
At time: 599.5410263538361 and batch: 500, loss is 5.280390510559082 and perplexity is 196.44657483661345
At time: 600.7023975849152 and batch: 550, loss is 5.2827277088165285 and perplexity is 196.9062463915354
At time: 601.8634650707245 and batch: 600, loss is 5.309558439254761 and perplexity is 202.26089819430604
At time: 603.0251939296722 and batch: 650, loss is 5.298459587097168 and perplexity is 200.02844613259055
At time: 604.1860179901123 and batch: 700, loss is 5.324723529815674 and perplexity is 205.35157902705222
At time: 605.3468585014343 and batch: 750, loss is 5.306551475524902 and perplexity is 201.65362049804554
At time: 606.5074546337128 and batch: 800, loss is 5.326741943359375 and perplexity is 205.76648201742907
At time: 607.6686210632324 and batch: 850, loss is 5.352952470779419 and perplexity is 211.23103156117878
At time: 608.8289394378662 and batch: 900, loss is 5.324439115524292 and perplexity is 205.29318240802934
At time: 609.9920701980591 and batch: 950, loss is 5.304086656570434 and perplexity is 201.1571928854297
At time: 611.1558890342712 and batch: 1000, loss is 5.300823583602905 and perplexity is 200.50187204842388
At time: 612.3169593811035 and batch: 1050, loss is 5.303236055374145 and perplexity is 200.98616108675762
At time: 613.479320526123 and batch: 1100, loss is 5.283953428268433 and perplexity is 197.14774618319953
At time: 614.6405584812164 and batch: 1150, loss is 5.314023399353028 and perplexity is 203.16600416152517
At time: 615.8033747673035 and batch: 1200, loss is 5.328470287322998 and perplexity is 206.1224247816986
At time: 616.9680366516113 and batch: 1250, loss is 5.327190008163452 and perplexity is 205.85869939401744
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.119854335367244 and perplexity of 167.31099655113144
Finished 20 epochs...
Completing Train Step...
At time: 619.896960735321 and batch: 50, loss is 5.303342685699463 and perplexity is 201.00759344914826
At time: 621.0627698898315 and batch: 100, loss is 5.310996770858765 and perplexity is 202.55202575522867
At time: 622.254319190979 and batch: 150, loss is 5.237887926101685 and perplexity is 188.27203772596818
At time: 623.4170451164246 and batch: 200, loss is 5.277791519165039 and perplexity is 195.93667477917523
At time: 624.588864326477 and batch: 250, loss is 5.302823143005371 and perplexity is 200.90318854626545
At time: 625.7511217594147 and batch: 300, loss is 5.309370431900025 and perplexity is 202.22287523228024
At time: 626.9148132801056 and batch: 350, loss is 5.3319219303131105 and perplexity is 206.83511507295236
At time: 628.077045917511 and batch: 400, loss is 5.301541347503662 and perplexity is 200.64583671435375
At time: 629.2469382286072 and batch: 450, loss is 5.261621036529541 and perplexity is 192.7937638601334
At time: 630.4093372821808 and batch: 500, loss is 5.264407024383545 and perplexity is 193.33163384629972
At time: 631.5715577602386 and batch: 550, loss is 5.268057851791382 and perplexity is 194.03874425758696
At time: 632.7336497306824 and batch: 600, loss is 5.296164932250977 and perplexity is 199.56997610570147
At time: 633.8960766792297 and batch: 650, loss is 5.287079877853394 and perplexity is 197.7650832061133
At time: 635.0583710670471 and batch: 700, loss is 5.31181432723999 and perplexity is 202.71769116759492
At time: 636.2204177379608 and batch: 750, loss is 5.293677768707275 and perplexity is 199.0742296934722
At time: 637.3836205005646 and batch: 800, loss is 5.314268140792847 and perplexity is 203.21573338705886
At time: 638.5469398498535 and batch: 850, loss is 5.343361606597901 and perplexity is 209.21482745140938
At time: 639.7086336612701 and batch: 900, loss is 5.312382078170776 and perplexity is 202.83281700374997
At time: 640.8708639144897 and batch: 950, loss is 5.291892776489258 and perplexity is 198.71920069892371
At time: 642.0336263179779 and batch: 1000, loss is 5.291015691757202 and perplexity is 198.5449835348003
At time: 643.1956994533539 and batch: 1050, loss is 5.292011308670044 and perplexity is 198.74275671519206
At time: 644.3599681854248 and batch: 1100, loss is 5.272242498397827 and perplexity is 194.85242913470583
At time: 645.5244374275208 and batch: 1150, loss is 5.305088863372803 and perplexity is 201.35889504924242
At time: 646.6977679729462 and batch: 1200, loss is 5.31604531288147 and perplexity is 203.5772038188473
At time: 647.8666326999664 and batch: 1250, loss is 5.313105945587158 and perplexity is 202.97969422435565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.113234916742701 and perplexity of 166.20715245810834
Finished 21 epochs...
Completing Train Step...
At time: 650.7744610309601 and batch: 50, loss is 5.292206573486328 and perplexity is 198.78156797218304
At time: 651.9613299369812 and batch: 100, loss is 5.2959227371215825 and perplexity is 199.52164708227858
At time: 653.1332325935364 and batch: 150, loss is 5.227778835296631 and perplexity is 186.37836637511285
At time: 654.2967791557312 and batch: 200, loss is 5.268541116714477 and perplexity is 194.1325390384464
At time: 655.4647722244263 and batch: 250, loss is 5.2924512672424315 and perplexity is 198.83021453220633
At time: 656.6279091835022 and batch: 300, loss is 5.29568416595459 and perplexity is 199.4740526476496
At time: 657.79083776474 and batch: 350, loss is 5.317836446762085 and perplexity is 203.9421645951628
At time: 658.9536361694336 and batch: 400, loss is 5.290058069229126 and perplexity is 198.35494339361384
At time: 660.114827632904 and batch: 450, loss is 5.245381469726563 and perplexity is 189.68816172082805
At time: 661.2774477005005 and batch: 500, loss is 5.2529347705841065 and perplexity is 191.1263582030799
At time: 662.4401590824127 and batch: 550, loss is 5.253794221878052 and perplexity is 191.29069260753732
At time: 663.6029455661774 and batch: 600, loss is 5.28011001586914 and perplexity is 196.39148034273228
At time: 664.7650759220123 and batch: 650, loss is 5.274779281616211 and perplexity is 195.34735500139743
At time: 665.9283242225647 and batch: 700, loss is 5.297923412322998 and perplexity is 199.92122467294755
At time: 667.0904819965363 and batch: 750, loss is 5.280203161239624 and perplexity is 196.4097741519072
At time: 668.2599375247955 and batch: 800, loss is 5.29956618309021 and perplexity is 200.24991932765
At time: 669.4216566085815 and batch: 850, loss is 5.329591522216797 and perplexity is 206.35366605042225
At time: 670.5835916996002 and batch: 900, loss is 5.298809576034546 and perplexity is 200.0984661282951
At time: 671.7460191249847 and batch: 950, loss is 5.280966129302978 and perplexity is 196.5596857185124
At time: 672.9085354804993 and batch: 1000, loss is 5.282300701141358 and perplexity is 196.82218386198707
At time: 674.0788326263428 and batch: 1050, loss is 5.281264476776123 and perplexity is 196.61833755294683
At time: 675.2414996623993 and batch: 1100, loss is 5.257651166915894 and perplexity is 192.02991494875505
At time: 676.4043023586273 and batch: 1150, loss is 5.288682270050049 and perplexity is 198.08223446470313
At time: 677.5666747093201 and batch: 1200, loss is 5.30124496459961 and perplexity is 200.58637753036047
At time: 678.7301530838013 and batch: 1250, loss is 5.30045449256897 and perplexity is 200.4278822604872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.107106452440694 and perplexity of 165.1916727015361
Finished 22 epochs...
Completing Train Step...
At time: 681.6320130825043 and batch: 50, loss is 5.277745761871338 and perplexity is 195.92770945231672
At time: 682.7943680286407 and batch: 100, loss is 5.285340585708618 and perplexity is 197.42141091028225
At time: 683.9571146965027 and batch: 150, loss is 5.21688136100769 and perplexity is 184.35833950560428
At time: 685.120038986206 and batch: 200, loss is 5.258742008209229 and perplexity is 192.23950340262377
At time: 686.2834270000458 and batch: 250, loss is 5.2795265102386475 and perplexity is 196.27691823523938
At time: 687.4459023475647 and batch: 300, loss is 5.2839696502685545 and perplexity is 197.1509443399023
At time: 688.6085929870605 and batch: 350, loss is 5.304970045089721 and perplexity is 201.33497135236385
At time: 689.7704322338104 and batch: 400, loss is 5.277757043838501 and perplexity is 195.9299199147703
At time: 690.9322488307953 and batch: 450, loss is 5.231970520019531 and perplexity is 187.16124537081018
At time: 692.0947933197021 and batch: 500, loss is 5.238026809692383 and perplexity is 188.29818743843657
At time: 693.2574155330658 and batch: 550, loss is 5.23907506942749 and perplexity is 188.49567633826865
At time: 694.420398235321 and batch: 600, loss is 5.266424732208252 and perplexity is 193.72211440203586
At time: 695.5830748081207 and batch: 650, loss is 5.265186309814453 and perplexity is 193.48235309091802
At time: 696.745302438736 and batch: 700, loss is 5.285490026473999 and perplexity is 197.4509159216021
At time: 697.9078662395477 and batch: 750, loss is 5.26854022026062 and perplexity is 194.13236500766104
At time: 699.0705945491791 and batch: 800, loss is 5.28686092376709 and perplexity is 197.72178647318842
At time: 700.2336874008179 and batch: 850, loss is 5.319131002426148 and perplexity is 204.20635004397016
At time: 701.3961877822876 and batch: 900, loss is 5.287488927841187 and perplexity is 197.84599555845477
At time: 702.5588698387146 and batch: 950, loss is 5.2692483615875245 and perplexity is 194.26988684491394
At time: 703.7210047245026 and batch: 1000, loss is 5.271209936141968 and perplexity is 194.6513357095261
At time: 704.8830842971802 and batch: 1050, loss is 5.26110806465149 and perplexity is 192.69489144256244
At time: 706.0456535816193 and batch: 1100, loss is 5.243572607040405 and perplexity is 189.3453520243919
At time: 707.2093632221222 and batch: 1150, loss is 5.274108877182007 and perplexity is 195.21643715724946
At time: 708.3730635643005 and batch: 1200, loss is 5.287589445114135 and perplexity is 197.86588349791612
At time: 709.5349373817444 and batch: 1250, loss is 5.290309734344483 and perplexity is 198.40486869528954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.096662312528513 and perplexity of 163.47536603420454
Finished 23 epochs...
Completing Train Step...
At time: 712.4348409175873 and batch: 50, loss is 5.2669040489196775 and perplexity is 193.81499090569343
At time: 713.6271996498108 and batch: 100, loss is 5.276014585494995 and perplexity is 195.58881745578856
At time: 714.7943351268768 and batch: 150, loss is 5.202321968078613 and perplexity is 181.69363928065437
At time: 715.9599664211273 and batch: 200, loss is 5.242780885696411 and perplexity is 189.19550259513133
At time: 717.1244080066681 and batch: 250, loss is 5.2652701568603515 and perplexity is 193.49857669479928
At time: 718.2894711494446 and batch: 300, loss is 5.269143943786621 and perplexity is 194.24960266958072
At time: 719.4538419246674 and batch: 350, loss is 5.290514268875122 and perplexity is 198.44545349233942
At time: 720.618241071701 and batch: 400, loss is 5.263208122253418 and perplexity is 193.09998702733947
At time: 721.7840988636017 and batch: 450, loss is 5.219163970947266 and perplexity is 184.77963833132225
At time: 722.951336145401 and batch: 500, loss is 5.225053443908691 and perplexity is 185.8711039388187
At time: 724.1155390739441 and batch: 550, loss is 5.225652627944946 and perplexity is 185.98250830962715
At time: 725.2802386283875 and batch: 600, loss is 5.250574235916138 and perplexity is 190.67572987971508
At time: 726.4448537826538 and batch: 650, loss is 5.2488358783721925 and perplexity is 190.3445552196135
At time: 727.6096923351288 and batch: 700, loss is 5.2726524066925045 and perplexity is 194.9323171339043
At time: 728.7884356975555 and batch: 750, loss is 5.254193010330201 and perplexity is 191.36699233946754
At time: 729.9592587947845 and batch: 800, loss is 5.2718973636627195 and perplexity is 194.7851903970707
At time: 731.1300961971283 and batch: 850, loss is 5.3072284317016605 and perplexity is 201.79017737830424
At time: 732.2947628498077 and batch: 900, loss is 5.2750037670135494 and perplexity is 195.39121255250967
At time: 733.4589321613312 and batch: 950, loss is 5.25762243270874 and perplexity is 192.02439720067375
At time: 734.6231987476349 and batch: 1000, loss is 5.256307601928711 and perplexity is 191.77208352396144
At time: 735.7866744995117 and batch: 1050, loss is 5.251059341430664 and perplexity is 190.76825016700246
At time: 736.9506592750549 and batch: 1100, loss is 5.230536680221558 and perplexity is 186.89307842872
At time: 738.11523604393 and batch: 1150, loss is 5.259243688583374 and perplexity is 192.33597038438882
At time: 739.2806689739227 and batch: 1200, loss is 5.27322024345398 and perplexity is 195.0430383023723
At time: 740.4722056388855 and batch: 1250, loss is 5.272559108734131 and perplexity is 194.9141311950634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.090650126881843 and perplexity of 162.49547039383967
Finished 24 epochs...
Completing Train Step...
At time: 743.3483378887177 and batch: 50, loss is 5.250059747695923 and perplexity is 190.57765469423836
At time: 744.5360171794891 and batch: 100, loss is 5.256978750228882 and perplexity is 191.90083423241546
At time: 745.6967399120331 and batch: 150, loss is 5.185604009628296 and perplexity is 178.6813424536805
At time: 746.8595790863037 and batch: 200, loss is 5.229024477005005 and perplexity is 186.61067169636948
At time: 748.0228474140167 and batch: 250, loss is 5.2516741371154785 and perplexity is 190.88556972408819
At time: 749.1879851818085 and batch: 300, loss is 5.25575288772583 and perplexity is 191.66573432494724
At time: 750.3505985736847 and batch: 350, loss is 5.276661891937255 and perplexity is 195.7154643426126
At time: 751.5132255554199 and batch: 400, loss is 5.246321115493775 and perplexity is 189.866485166394
At time: 752.6751275062561 and batch: 450, loss is 5.203991966247559 and perplexity is 181.9973208287988
At time: 753.8363482952118 and batch: 500, loss is 5.208689012527466 and perplexity is 182.8541814491904
At time: 754.9978725910187 and batch: 550, loss is 5.21651439666748 and perplexity is 184.29069898077475
At time: 756.1598908901215 and batch: 600, loss is 5.242859935760498 and perplexity is 189.21045910288504
At time: 757.3217613697052 and batch: 650, loss is 5.23827561378479 and perplexity is 188.34504262670364
At time: 758.4833717346191 and batch: 700, loss is 5.258402137756348 and perplexity is 192.1741779772605
At time: 759.6451945304871 and batch: 750, loss is 5.241452970504761 and perplexity is 188.9444337490045
At time: 760.8068444728851 and batch: 800, loss is 5.260840301513672 and perplexity is 192.64330176100404
At time: 761.9739902019501 and batch: 850, loss is 5.2959419536590575 and perplexity is 199.52548123432626
At time: 763.1390182971954 and batch: 900, loss is 5.261644048690796 and perplexity is 192.79820051236456
At time: 764.3013579845428 and batch: 950, loss is 5.245339403152466 and perplexity is 189.6801823575511
At time: 765.4632022380829 and batch: 1000, loss is 5.2474853134155275 and perplexity is 190.0876560521976
At time: 766.6331260204315 and batch: 1050, loss is 5.2365225982666015 and perplexity is 188.01516007330198
At time: 767.7952032089233 and batch: 1100, loss is 5.216679096221924 and perplexity is 184.3210540764517
At time: 768.9832136631012 and batch: 1150, loss is 5.250205812454223 and perplexity is 190.60549340638656
At time: 770.1494402885437 and batch: 1200, loss is 5.261787624359131 and perplexity is 192.82588363012036
At time: 771.3149926662445 and batch: 1250, loss is 5.261475887298584 and perplexity is 192.76578202439586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.087201417797673 and perplexity of 161.93603600665625
Finished 25 epochs...
Completing Train Step...
At time: 774.2173688411713 and batch: 50, loss is 5.239285078048706 and perplexity is 188.5352662123235
At time: 775.3821430206299 and batch: 100, loss is 5.244081516265869 and perplexity is 189.44173614413947
At time: 776.5467340946198 and batch: 150, loss is 5.178290948867798 and perplexity is 177.37940132922975
At time: 777.7121062278748 and batch: 200, loss is 5.221111288070679 and perplexity is 185.13981345888294
At time: 778.8766295909882 and batch: 250, loss is 5.239637308120727 and perplexity is 188.60168569950372
At time: 780.0411083698273 and batch: 300, loss is 5.241840543746949 and perplexity is 189.01767774857632
At time: 781.2059528827667 and batch: 350, loss is 5.260251779556274 and perplexity is 192.5299603032118
At time: 782.3710219860077 and batch: 400, loss is 5.236888494491577 and perplexity is 188.08396669788542
At time: 783.5368528366089 and batch: 450, loss is 5.193400096893311 and perplexity is 180.0798019652026
At time: 784.7031931877136 and batch: 500, loss is 5.200563945770264 and perplexity is 181.37449842008579
At time: 785.8671338558197 and batch: 550, loss is 5.201370506286621 and perplexity is 181.520846940721
At time: 787.0310537815094 and batch: 600, loss is 5.225606784820557 and perplexity is 185.97398248579123
At time: 788.1974503993988 and batch: 650, loss is 5.225426244735718 and perplexity is 185.94040975791808
At time: 789.3675582408905 and batch: 700, loss is 5.23715256690979 and perplexity is 188.1336410443222
At time: 790.5326342582703 and batch: 750, loss is 5.2224408149719235 and perplexity is 185.38612552434387
At time: 791.6975395679474 and batch: 800, loss is 5.240833539962768 and perplexity is 188.8274320369589
At time: 792.8616888523102 and batch: 850, loss is 5.277066793441772 and perplexity is 195.79472587405593
At time: 794.0263781547546 and batch: 900, loss is 5.244678983688354 and perplexity is 189.55495522893412
At time: 795.1909284591675 and batch: 950, loss is 5.226249256134033 and perplexity is 186.09350382499593
At time: 796.3551218509674 and batch: 1000, loss is 5.2283047103881835 and perplexity is 186.47640389107835
At time: 797.519971370697 and batch: 1050, loss is 5.222605438232422 and perplexity is 185.41664690497447
At time: 798.7122478485107 and batch: 1100, loss is 5.201370706558228 and perplexity is 181.52088329419624
At time: 799.8775844573975 and batch: 1150, loss is 5.229475765228272 and perplexity is 186.69490590036443
At time: 801.041909456253 and batch: 1200, loss is 5.24089747428894 and perplexity is 188.8395049775225
At time: 802.2053191661835 and batch: 1250, loss is 5.242422180175781 and perplexity is 189.1276492942765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.080563287665374 and perplexity of 160.8646434822546
Finished 26 epochs...
Completing Train Step...
At time: 805.0799536705017 and batch: 50, loss is 5.219884252548217 and perplexity is 184.9127796489073
At time: 806.2651627063751 and batch: 100, loss is 5.2266831207275395 and perplexity is 186.1742607249001
At time: 807.4264466762543 and batch: 150, loss is 5.159202394485473 and perplexity is 174.02559645965724
At time: 808.5884912014008 and batch: 200, loss is 5.1996591186523435 and perplexity is 181.21046007977716
At time: 809.750424861908 and batch: 250, loss is 5.219782085418701 and perplexity is 184.8938886060379
At time: 810.9125576019287 and batch: 300, loss is 5.222719116210937 and perplexity is 185.43772589266345
At time: 812.0748436450958 and batch: 350, loss is 5.238044519424438 and perplexity is 188.30152217841115
At time: 813.2368376255035 and batch: 400, loss is 5.218656120300293 and perplexity is 184.68582169688383
At time: 814.3984127044678 and batch: 450, loss is 5.173892831802368 and perplexity is 176.60097900821458
At time: 815.5604214668274 and batch: 500, loss is 5.179644460678101 and perplexity is 177.61964899618286
At time: 816.7234451770782 and batch: 550, loss is 5.17797119140625 and perplexity is 177.32269200922713
At time: 817.8865752220154 and batch: 600, loss is 5.201259937286377 and perplexity is 181.50077747170224
At time: 819.0500500202179 and batch: 650, loss is 5.197668094635009 and perplexity is 180.85002463844546
At time: 820.2129650115967 and batch: 700, loss is 5.215331563949585 and perplexity is 184.07284278153665
At time: 821.3746888637543 and batch: 750, loss is 5.202558841705322 and perplexity is 181.73668280967786
At time: 822.5353598594666 and batch: 800, loss is 5.22030366897583 and perplexity is 184.990351372658
At time: 823.697199344635 and batch: 850, loss is 5.261229391098023 and perplexity is 192.71827184730824
At time: 824.858806848526 and batch: 900, loss is 5.232433052062988 and perplexity is 187.24783346742845
At time: 826.0212454795837 and batch: 950, loss is 5.217658538818359 and perplexity is 184.50167440741777
At time: 827.1989047527313 and batch: 1000, loss is 5.216596603393555 and perplexity is 184.30584953851425
At time: 828.3971440792084 and batch: 1050, loss is 5.2049117755889895 and perplexity is 182.1648006775692
At time: 829.5613744258881 and batch: 1100, loss is 5.172593097686768 and perplexity is 176.37159379313596
At time: 830.7236375808716 and batch: 1150, loss is 5.194160871505737 and perplexity is 180.21685423308122
At time: 831.8875703811646 and batch: 1200, loss is 5.2066320705413816 and perplexity is 182.47844756991023
At time: 833.0493702888489 and batch: 1250, loss is 5.216309061050415 and perplexity is 184.25286142121243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.059380524349908 and perplexity of 157.4929230672631
Finished 27 epochs...
Completing Train Step...
At time: 835.9461994171143 and batch: 50, loss is 5.190545530319214 and perplexity is 179.56648517887837
At time: 837.1104235649109 and batch: 100, loss is 5.202025709152221 and perplexity is 181.6398188909279
At time: 838.278644323349 and batch: 150, loss is 5.130381946563721 and perplexity is 169.08168588228125
At time: 839.4588167667389 and batch: 200, loss is 5.174804925918579 and perplexity is 176.76212920298542
At time: 840.6345887184143 and batch: 250, loss is 5.194261693954468 and perplexity is 180.23502505362552
At time: 841.7980268001556 and batch: 300, loss is 5.196182403564453 and perplexity is 180.58153686605888
At time: 842.9610066413879 and batch: 350, loss is 5.211099472045898 and perplexity is 183.29547569864025
At time: 844.1237452030182 and batch: 400, loss is 5.193026752471924 and perplexity is 180.01258272447942
At time: 845.2953951358795 and batch: 450, loss is 5.144503545761109 and perplexity is 171.48632842006424
At time: 846.4586200714111 and batch: 500, loss is 5.152771825790405 and perplexity is 172.91010337837366
At time: 847.6221153736115 and batch: 550, loss is 5.149077062606811 and perplexity is 172.27242026381916
At time: 848.7852928638458 and batch: 600, loss is 5.171031055450439 and perplexity is 176.09630897357405
At time: 849.9485430717468 and batch: 650, loss is 5.167545413970947 and perplexity is 175.48356889113774
At time: 851.1112878322601 and batch: 700, loss is 5.184964666366577 and perplexity is 178.56714025248183
At time: 852.2785096168518 and batch: 750, loss is 5.184386310577392 and perplexity is 178.46389477234715
At time: 853.4492664337158 and batch: 800, loss is 5.201871690750122 and perplexity is 181.6118451705538
At time: 854.6150226593018 and batch: 850, loss is 5.241363382339477 and perplexity is 188.92750732205948
At time: 855.7785384654999 and batch: 900, loss is 5.21411129951477 and perplexity is 183.84836222874773
At time: 856.9675109386444 and batch: 950, loss is 5.2018101978302 and perplexity is 181.60067767126657
At time: 858.1307430267334 and batch: 1000, loss is 5.199442472457886 and perplexity is 181.17120577550625
At time: 859.2947227954865 and batch: 1050, loss is 5.180727014541626 and perplexity is 177.81203594927277
At time: 860.4586913585663 and batch: 1100, loss is 5.144775819778443 and perplexity is 171.533026048608
At time: 861.6225116252899 and batch: 1150, loss is 5.169581384658813 and perplexity is 175.8412122457544
At time: 862.7860813140869 and batch: 1200, loss is 5.18177020072937 and perplexity is 177.99762379369744
At time: 863.949431180954 and batch: 1250, loss is 5.1957995891571045 and perplexity is 180.51242088218663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.043410252480611 and perplexity of 154.99769602913517
Finished 28 epochs...
Completing Train Step...
At time: 866.8201365470886 and batch: 50, loss is 5.1691782093048095 and perplexity is 175.77033169236503
At time: 868.0095183849335 and batch: 100, loss is 5.176312122344971 and perplexity is 177.02874532340215
At time: 869.1720373630524 and batch: 150, loss is 5.1093518161773686 and perplexity is 165.56300482485196
At time: 870.3345191478729 and batch: 200, loss is 5.157404689788819 and perplexity is 173.7130308620626
At time: 871.4963624477386 and batch: 250, loss is 5.174880838394165 and perplexity is 176.77554816312977
At time: 872.6590237617493 and batch: 300, loss is 5.176420879364014 and perplexity is 177.04799948902232
At time: 873.8256752490997 and batch: 350, loss is 5.186715135574341 and perplexity is 178.87999027028889
At time: 874.9920635223389 and batch: 400, loss is 5.173659801483154 and perplexity is 176.55983042032335
At time: 876.1553542613983 and batch: 450, loss is 5.129297733306885 and perplexity is 168.89846462036843
At time: 877.3179457187653 and batch: 500, loss is 5.134092216491699 and perplexity is 169.71018981539953
At time: 878.480729341507 and batch: 550, loss is 5.130142822265625 and perplexity is 169.04125917652024
At time: 879.6429116725922 and batch: 600, loss is 5.15142186164856 and perplexity is 172.67683842413712
At time: 880.8056743144989 and batch: 650, loss is 5.151873712539673 and perplexity is 172.75488023775736
At time: 881.9689409732819 and batch: 700, loss is 5.173110427856446 and perplexity is 176.462859744959
At time: 883.131932258606 and batch: 750, loss is 5.166443471908569 and perplexity is 175.29030266896672
At time: 884.2945187091827 and batch: 800, loss is 5.1875903511047365 and perplexity is 179.03661734707748
At time: 885.4565563201904 and batch: 850, loss is 5.226395959854126 and perplexity is 186.1208064369406
At time: 886.6475529670715 and batch: 900, loss is 5.20355528831482 and perplexity is 181.91786396476743
At time: 887.8122079372406 and batch: 950, loss is 5.1891468334198 and perplexity is 179.3155016586735
At time: 888.9744305610657 and batch: 1000, loss is 5.185284843444824 and perplexity is 178.6243225114547
At time: 890.145795583725 and batch: 1050, loss is 5.165125169754028 and perplexity is 175.05936933868685
At time: 891.3084237575531 and batch: 1100, loss is 5.1313472175598145 and perplexity is 169.2449743257547
At time: 892.473147392273 and batch: 1150, loss is 5.154420280456543 and perplexity is 173.19537290745393
At time: 893.6428532600403 and batch: 1200, loss is 5.169252605438232 and perplexity is 175.78340881185088
At time: 894.8146991729736 and batch: 1250, loss is 5.185009155273438 and perplexity is 178.57508468607116
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.038138062414462 and perplexity of 154.18266909218704
Finished 29 epochs...
Completing Train Step...
At time: 897.6977627277374 and batch: 50, loss is 5.158913297653198 and perplexity is 173.97529348255196
At time: 898.885656118393 and batch: 100, loss is 5.166732702255249 and perplexity is 175.34100927656866
At time: 900.0484330654144 and batch: 150, loss is 5.0985113048553465 and perplexity is 163.77791034648993
At time: 901.2112007141113 and batch: 200, loss is 5.146023302078247 and perplexity is 171.7471439888381
At time: 902.3742022514343 and batch: 250, loss is 5.162885541915894 and perplexity is 174.66774021722972
At time: 903.5375986099243 and batch: 300, loss is 5.1607738208770755 and perplexity is 174.29927985508448
At time: 904.7002012729645 and batch: 350, loss is 5.173836278915405 and perplexity is 176.5909919954111
At time: 905.8627800941467 and batch: 400, loss is 5.1614694499969485 and perplexity is 174.42056969120452
At time: 907.0250761508942 and batch: 450, loss is 5.11601167678833 and perplexity is 166.66931119335496
At time: 908.187667131424 and batch: 500, loss is 5.124319295883179 and perplexity is 168.05970377486602
At time: 909.3516187667847 and batch: 550, loss is 5.119026603698731 and perplexity is 167.172565240689
At time: 910.5148818492889 and batch: 600, loss is 5.137509212493897 and perplexity is 170.2910807413167
At time: 911.6780724525452 and batch: 650, loss is 5.135490522384644 and perplexity is 169.94766256499037
At time: 912.8412635326385 and batch: 700, loss is 5.157270879745483 and perplexity is 173.68978786898265
At time: 914.0041298866272 and batch: 750, loss is 5.155938587188721 and perplexity is 173.45853633897124
At time: 915.1669042110443 and batch: 800, loss is 5.169815645217896 and perplexity is 175.8824097317295
At time: 916.3569107055664 and batch: 850, loss is 5.21400502204895 and perplexity is 183.82882432895258
At time: 917.5194222927094 and batch: 900, loss is 5.1887216472625735 and perplexity is 179.23927539591637
At time: 918.6828362941742 and batch: 950, loss is 5.1772237968444825 and perplexity is 177.1902115073117
At time: 919.8523769378662 and batch: 1000, loss is 5.170378942489624 and perplexity is 175.98151172259637
At time: 921.0171165466309 and batch: 1050, loss is 5.152110328674317 and perplexity is 172.79576166617213
At time: 922.1881539821625 and batch: 1100, loss is 5.11322190284729 and perplexity is 166.20498946968416
At time: 923.3518512248993 and batch: 1150, loss is 5.139835233688355 and perplexity is 170.68764243100009
At time: 924.5142486095428 and batch: 1200, loss is 5.154702672958374 and perplexity is 173.2442888885428
At time: 925.6764252185822 and batch: 1250, loss is 5.170054922103882 and perplexity is 175.92449936236684
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.034430956318431 and perplexity of 153.61215570987477
Finished 30 epochs...
Completing Train Step...
At time: 928.5698153972626 and batch: 50, loss is 5.143164472579956 and perplexity is 171.25684935566326
At time: 929.7338559627533 and batch: 100, loss is 5.153063268661499 and perplexity is 172.9605041394578
At time: 930.906402349472 and batch: 150, loss is 5.084534120559693 and perplexity is 161.50467999817042
At time: 932.0695450305939 and batch: 200, loss is 5.135092067718506 and perplexity is 169.879959615016
At time: 933.233451128006 and batch: 250, loss is 5.154092073440552 and perplexity is 173.13853829819737
At time: 934.3970291614532 and batch: 300, loss is 5.151271915435791 and perplexity is 172.650948127307
At time: 935.560587644577 and batch: 350, loss is 5.163329591751099 and perplexity is 174.7453186215529
At time: 936.7238109111786 and batch: 400, loss is 5.152383003234863 and perplexity is 172.8428850989375
At time: 937.8878614902496 and batch: 450, loss is 5.105444965362548 and perplexity is 164.91743675475286
At time: 939.0515422821045 and batch: 500, loss is 5.112100591659546 and perplexity is 166.01872640447877
At time: 940.2171304225922 and batch: 550, loss is 5.106269388198853 and perplexity is 165.05345451608858
At time: 941.3812222480774 and batch: 600, loss is 5.124781751632691 and perplexity is 168.13744192497973
At time: 942.544534444809 and batch: 650, loss is 5.125492820739746 and perplexity is 168.25704178251445
At time: 943.7084741592407 and batch: 700, loss is 5.143921070098877 and perplexity is 171.3864708924671
At time: 944.873194694519 and batch: 750, loss is 5.139476871490478 and perplexity is 170.62648539114826
At time: 946.0648682117462 and batch: 800, loss is 5.16058087348938 and perplexity is 174.26565250861785
At time: 947.2284953594208 and batch: 850, loss is 5.206064519882202 and perplexity is 182.37491119055522
At time: 948.3921885490417 and batch: 900, loss is 5.179156484603882 and perplexity is 177.5329960011753
At time: 949.5554859638214 and batch: 950, loss is 5.1666531658172605 and perplexity is 175.32706383185032
At time: 950.720237493515 and batch: 1000, loss is 5.160558357238769 and perplexity is 174.26172874368754
At time: 951.8870916366577 and batch: 1050, loss is 5.140496406555176 and perplexity is 170.80053378511474
At time: 953.0606405735016 and batch: 1100, loss is 5.1049059295654295 and perplexity is 164.82856430764002
At time: 954.2249138355255 and batch: 1150, loss is 5.130565710067749 and perplexity is 169.11275978038282
At time: 955.3888623714447 and batch: 1200, loss is 5.141961097717285 and perplexity is 171.05088711783293
At time: 956.5543620586395 and batch: 1250, loss is 5.160286846160889 and perplexity is 174.21442117643798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.02827487026688 and perplexity of 152.66941084730578
Finished 31 epochs...
Completing Train Step...
At time: 959.45228266716 and batch: 50, loss is 5.129944810867309 and perplexity is 169.00779039412728
At time: 960.6435296535492 and batch: 100, loss is 5.141607465744019 and perplexity is 170.99040874926519
At time: 961.805501461029 and batch: 150, loss is 5.076241159439087 and perplexity is 160.17086624316767
At time: 962.9663505554199 and batch: 200, loss is 5.122362699508667 and perplexity is 167.73120024696436
At time: 964.1274788379669 and batch: 250, loss is 5.140817270278931 and perplexity is 170.85534627360371
At time: 965.2891008853912 and batch: 300, loss is 5.138748073577881 and perplexity is 170.50217846757675
At time: 966.4505481719971 and batch: 350, loss is 5.153029365539551 and perplexity is 172.95464033779493
At time: 967.6258187294006 and batch: 400, loss is 5.140016794204712 and perplexity is 170.71863538095855
At time: 968.7987711429596 and batch: 450, loss is 5.092440214157104 and perplexity is 162.78661197435378
At time: 969.9585645198822 and batch: 500, loss is 5.100896434783936 and perplexity is 164.16900816624872
At time: 971.1191728115082 and batch: 550, loss is 5.096353273391724 and perplexity is 163.42485355376783
At time: 972.2804546356201 and batch: 600, loss is 5.115495948791504 and perplexity is 166.58337732451662
At time: 973.4412829875946 and batch: 650, loss is 5.115293779373169 and perplexity is 166.54970266412656
At time: 974.6277885437012 and batch: 700, loss is 5.137458343505859 and perplexity is 170.28241842669098
At time: 975.7880871295929 and batch: 750, loss is 5.133253059387207 and perplexity is 169.5678360408555
At time: 976.9490876197815 and batch: 800, loss is 5.15131914138794 and perplexity is 172.65910192525564
At time: 978.1103718280792 and batch: 850, loss is 5.197712297439575 and perplexity is 180.85801889342326
At time: 979.2717604637146 and batch: 900, loss is 5.169526147842407 and perplexity is 175.83149960524707
At time: 980.4347116947174 and batch: 950, loss is 5.15845061302185 and perplexity is 173.89481640721203
At time: 981.5962018966675 and batch: 1000, loss is 5.152732486724854 and perplexity is 172.9033013902751
At time: 982.7576241493225 and batch: 1050, loss is 5.13118579864502 and perplexity is 169.21765719047505
At time: 983.9188742637634 and batch: 1100, loss is 5.093136568069458 and perplexity is 162.90000854597176
At time: 985.0800178050995 and batch: 1150, loss is 5.120476760864258 and perplexity is 167.41516759731007
At time: 986.2411775588989 and batch: 1200, loss is 5.134035873413086 and perplexity is 169.70062809020433
At time: 987.4023234844208 and batch: 1250, loss is 5.149989204406738 and perplexity is 172.42962882666728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.025514031848768 and perplexity of 152.24849657806948
Finished 32 epochs...
Completing Train Step...
At time: 990.293425321579 and batch: 50, loss is 5.124821758270263 and perplexity is 168.14416867323757
At time: 991.4586577415466 and batch: 100, loss is 5.133033266067505 and perplexity is 169.5305702587924
At time: 992.6232581138611 and batch: 150, loss is 5.065261936187744 and perplexity is 158.4219330766519
At time: 993.7878682613373 and batch: 200, loss is 5.114226627349853 and perplexity is 166.37206361274062
At time: 994.9520227909088 and batch: 250, loss is 5.133156232833862 and perplexity is 169.5514181665894
At time: 996.1161363124847 and batch: 300, loss is 5.12856767654419 and perplexity is 168.77520415310704
At time: 997.281082868576 and batch: 350, loss is 5.142840232849121 and perplexity is 171.20133008218804
At time: 998.4459805488586 and batch: 400, loss is 5.135060920715332 and perplexity is 169.87466844577713
At time: 999.6110315322876 and batch: 450, loss is 5.0827766799926755 and perplexity is 161.22109438719713
At time: 1000.7850134372711 and batch: 500, loss is 5.091610898971558 and perplexity is 162.65166652892756
At time: 1001.9505989551544 and batch: 550, loss is 5.088101282119751 and perplexity is 162.08182205191008
At time: 1003.1151401996613 and batch: 600, loss is 5.105169057846069 and perplexity is 164.8719410709429
At time: 1004.3069486618042 and batch: 650, loss is 5.110632734298706 and perplexity is 165.77521335981908
At time: 1005.4717197418213 and batch: 700, loss is 5.13245852470398 and perplexity is 169.43316202263523
At time: 1006.6371002197266 and batch: 750, loss is 5.123054676055908 and perplexity is 167.84730647053476
At time: 1007.8018815517426 and batch: 800, loss is 5.145368251800537 and perplexity is 171.63467781399333
At time: 1008.9668760299683 and batch: 850, loss is 5.191352262496948 and perplexity is 179.71140568867432
At time: 1010.13236951828 and batch: 900, loss is 5.164089841842651 and perplexity is 174.87821927850047
At time: 1011.3021147251129 and batch: 950, loss is 5.15185227394104 and perplexity is 172.7511766549181
At time: 1012.466614484787 and batch: 1000, loss is 5.14406452178955 and perplexity is 171.41105833498816
At time: 1013.6309320926666 and batch: 1050, loss is 5.123220472335816 and perplexity is 167.8751372365951
At time: 1014.8035509586334 and batch: 1100, loss is 5.086143894195557 and perplexity is 161.7648753457169
At time: 1015.9695801734924 and batch: 1150, loss is 5.111531057357788 and perplexity is 165.92419996563703
At time: 1017.1341054439545 and batch: 1200, loss is 5.127435417175293 and perplexity is 168.58421499200483
At time: 1018.298332452774 and batch: 1250, loss is 5.142086372375489 and perplexity is 171.07231680152213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0202511975364965 and perplexity of 151.44934271800554
Finished 33 epochs...
Completing Train Step...
At time: 1021.1763203144073 and batch: 50, loss is 5.113039083480835 and perplexity is 166.17460675616658
At time: 1022.3644821643829 and batch: 100, loss is 5.1240855312347415 and perplexity is 168.02042194882657
At time: 1023.5266809463501 and batch: 150, loss is 5.058530044555664 and perplexity is 157.35903546088016
At time: 1024.689383506775 and batch: 200, loss is 5.108269100189209 and perplexity is 165.3838441200622
At time: 1025.853589296341 and batch: 250, loss is 5.123354158401489 and perplexity is 167.89758130341275
At time: 1027.0165657997131 and batch: 300, loss is 5.122117166519165 and perplexity is 167.69002175948043
At time: 1028.1775662899017 and batch: 350, loss is 5.1323936080932615 and perplexity is 169.42216335301552
At time: 1029.3388607501984 and batch: 400, loss is 5.124551019668579 and perplexity is 168.09865171800607
At time: 1030.5000395774841 and batch: 450, loss is 5.072653665542602 and perplexity is 159.5972837152072
At time: 1031.6614830493927 and batch: 500, loss is 5.084126195907593 and perplexity is 161.4388116933357
At time: 1032.822140455246 and batch: 550, loss is 5.0832336807250975 and perplexity is 161.294789383465
At time: 1034.0083723068237 and batch: 600, loss is 5.100730714797973 and perplexity is 164.1418043346915
At time: 1035.1693835258484 and batch: 650, loss is 5.100768947601319 and perplexity is 164.14808005598584
At time: 1036.330331325531 and batch: 700, loss is 5.1198526096344 and perplexity is 167.31070781729855
At time: 1037.491415977478 and batch: 750, loss is 5.113468236923218 and perplexity is 166.24593646529297
At time: 1038.6523094177246 and batch: 800, loss is 5.135278768539429 and perplexity is 169.91167930388838
At time: 1039.8128085136414 and batch: 850, loss is 5.183059110641479 and perplexity is 178.2271946116064
At time: 1040.981908082962 and batch: 900, loss is 5.151793479919434 and perplexity is 172.7410202170771
At time: 1042.144577741623 and batch: 950, loss is 5.141295671463013 and perplexity is 170.93710322832038
At time: 1043.3056716918945 and batch: 1000, loss is 5.133028383255005 and perplexity is 169.52974247482575
At time: 1044.4663789272308 and batch: 1050, loss is 5.113044033050537 and perplexity is 166.17542925100088
At time: 1045.627163887024 and batch: 1100, loss is 5.0787748527526855 and perplexity is 160.5772046470722
At time: 1046.7886228561401 and batch: 1150, loss is 5.103747816085815 and perplexity is 164.6377846190776
At time: 1047.9570689201355 and batch: 1200, loss is 5.117212867736816 and perplexity is 166.86963314972488
At time: 1049.1183047294617 and batch: 1250, loss is 5.135031461715698 and perplexity is 169.86966418169231
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.0174845674612225 and perplexity of 151.03091749236214
Finished 34 epochs...
Completing Train Step...
At time: 1051.9828143119812 and batch: 50, loss is 5.1049544143676755 and perplexity is 164.83655618172563
At time: 1053.1768963336945 and batch: 100, loss is 5.116482753753662 and perplexity is 166.7478437626758
At time: 1054.3434569835663 and batch: 150, loss is 5.049137372970581 and perplexity is 155.8879333247829
At time: 1055.5092906951904 and batch: 200, loss is 5.096667747497559 and perplexity is 163.47625452017323
At time: 1056.6715593338013 and batch: 250, loss is 5.113998250961304 and perplexity is 166.33407249999888
At time: 1057.8342974185944 and batch: 300, loss is 5.115257730484009 and perplexity is 166.54369884057184
At time: 1058.9978029727936 and batch: 350, loss is 5.120712337493896 and perplexity is 167.45461134407302
At time: 1060.1619126796722 and batch: 400, loss is 5.115025787353516 and perplexity is 166.50507465317946
At time: 1061.3253617286682 and batch: 450, loss is 5.063960771560669 and perplexity is 158.21593410952795
At time: 1062.4879307746887 and batch: 500, loss is 5.076255645751953 and perplexity is 160.1731865452543
At time: 1063.6759305000305 and batch: 550, loss is 5.069737796783447 and perplexity is 159.13259679385627
At time: 1064.8383393287659 and batch: 600, loss is 5.08953447341919 and perplexity is 162.31428284967825
At time: 1066.017229795456 and batch: 650, loss is 5.089631052017212 and perplexity is 162.32995969256638
At time: 1067.1893229484558 and batch: 700, loss is 5.108945703506469 and perplexity is 165.4957812418588
At time: 1068.3517282009125 and batch: 750, loss is 5.104253301620483 and perplexity is 164.72102767497012
At time: 1069.5294606685638 and batch: 800, loss is 5.128723087310791 and perplexity is 168.80143567524547
At time: 1070.70534157753 and batch: 850, loss is 5.171999483108521 and perplexity is 176.26692811248208
At time: 1071.8704597949982 and batch: 900, loss is 5.145471916198731 and perplexity is 171.65247114182947
At time: 1073.03356051445 and batch: 950, loss is 5.132797269821167 and perplexity is 169.49056640113758
At time: 1074.1967661380768 and batch: 1000, loss is 5.126515035629272 and perplexity is 168.42912457370346
At time: 1075.359492778778 and batch: 1050, loss is 5.105818223953247 and perplexity is 164.97900509451864
At time: 1076.5218257904053 and batch: 1100, loss is 5.071662836074829 and perplexity is 159.43922833941832
At time: 1077.684958934784 and batch: 1150, loss is 5.097154760360718 and perplexity is 163.55588894886722
At time: 1078.847121477127 and batch: 1200, loss is 5.110659627914429 and perplexity is 165.77967171465392
At time: 1080.0097630023956 and batch: 1250, loss is 5.1273791790008545 and perplexity is 168.57473439010295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.018789917883212 and perplexity of 151.2281944940406
Annealing...
Finished 35 epochs...
Completing Train Step...
At time: 1082.8957602977753 and batch: 50, loss is 5.108335876464844 and perplexity is 165.39488820596014
At time: 1084.0570290088654 and batch: 100, loss is 5.125111770629883 and perplexity is 168.19293963209938
At time: 1085.2184765338898 and batch: 150, loss is 5.053146409988403 and perplexity is 156.51414824077347
At time: 1086.3801546096802 and batch: 200, loss is 5.102645072937012 and perplexity is 164.45633149653648
At time: 1087.5420372486115 and batch: 250, loss is 5.117354135513306 and perplexity is 166.89320811691604
At time: 1088.7032511234283 and batch: 300, loss is 5.10451488494873 and perplexity is 164.7641215857002
At time: 1089.8647158145905 and batch: 350, loss is 5.115542078018189 and perplexity is 166.5910618641307
At time: 1091.0263059139252 and batch: 400, loss is 5.095238246917725 and perplexity is 163.24273206955314
At time: 1092.213291168213 and batch: 450, loss is 5.045696773529053 and perplexity is 155.35250701036955
At time: 1093.375671863556 and batch: 500, loss is 5.048915939331055 and perplexity is 155.8534183138819
At time: 1094.5382206439972 and batch: 550, loss is 5.043229093551636 and perplexity is 154.96961935577573
At time: 1095.7030653953552 and batch: 600, loss is 5.062269525527954 and perplexity is 157.94857818470328
At time: 1096.8671135902405 and batch: 650, loss is 5.061851806640625 and perplexity is 157.88261385860042
At time: 1098.0298235416412 and batch: 700, loss is 5.071422300338745 and perplexity is 159.4008821192721
At time: 1099.1908113956451 and batch: 750, loss is 5.053415269851684 and perplexity is 156.55623427063964
At time: 1100.352816581726 and batch: 800, loss is 5.068293733596802 and perplexity is 158.90296511026594
At time: 1101.515839099884 and batch: 850, loss is 5.107521409988403 and perplexity is 165.26023445706755
At time: 1102.677860736847 and batch: 900, loss is 5.072686738967896 and perplexity is 159.60256223133604
At time: 1103.8395383358002 and batch: 950, loss is 5.054623184204101 and perplexity is 156.74545505121682
At time: 1105.001187801361 and batch: 1000, loss is 5.04749041557312 and perplexity is 155.63140384436716
At time: 1106.162523508072 and batch: 1050, loss is 5.015678396224976 and perplexity is 150.758375995801
At time: 1107.3243713378906 and batch: 1100, loss is 4.968312997817993 and perplexity is 143.78411853101466
At time: 1108.485327720642 and batch: 1150, loss is 4.984645910263062 and perplexity is 146.15181500512367
At time: 1109.6466212272644 and batch: 1200, loss is 4.999154567718506 and perplexity is 148.28773885150767
At time: 1110.808964252472 and batch: 1250, loss is 5.039288206100464 and perplexity is 154.36010333341082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.957513294080748 and perplexity of 142.23964757454402
Finished 36 epochs...
Completing Train Step...
At time: 1113.6741354465485 and batch: 50, loss is 5.06371675491333 and perplexity is 158.17733149776245
At time: 1114.8618967533112 and batch: 100, loss is 5.083578777313233 and perplexity is 161.35046127050182
At time: 1116.022709608078 and batch: 150, loss is 5.01225079536438 and perplexity is 150.24252103399238
At time: 1117.1840596199036 and batch: 200, loss is 5.061845140457153 and perplexity is 157.8815613876374
At time: 1118.3459331989288 and batch: 250, loss is 5.082322130203247 and perplexity is 161.1478280255581
At time: 1119.5072748661041 and batch: 300, loss is 5.073277730941772 and perplexity is 159.69691394243063
At time: 1120.668824672699 and batch: 350, loss is 5.0847767734527585 and perplexity is 161.54387433113249
At time: 1121.8563706874847 and batch: 400, loss is 5.067898397445679 and perplexity is 158.84015743952665
At time: 1123.0173053741455 and batch: 450, loss is 5.020974254608154 and perplexity is 151.55888883553047
At time: 1124.1788878440857 and batch: 500, loss is 5.024826965332031 and perplexity is 152.14392766086152
At time: 1125.3408226966858 and batch: 550, loss is 5.017866725921631 and perplexity is 151.08864626533202
At time: 1126.503356218338 and batch: 600, loss is 5.03902195930481 and perplexity is 154.31901092112577
At time: 1127.6665093898773 and batch: 650, loss is 5.041689987182617 and perplexity is 154.73128808325814
At time: 1128.8296995162964 and batch: 700, loss is 5.053218536376953 and perplexity is 156.525437448163
At time: 1129.992243051529 and batch: 750, loss is 5.038465709686279 and perplexity is 154.23319489994608
At time: 1131.1561169624329 and batch: 800, loss is 5.0563319873809816 and perplexity is 157.0135311621929
At time: 1132.324021577835 and batch: 850, loss is 5.098659400939941 and perplexity is 163.80216700986946
At time: 1133.4896154403687 and batch: 900, loss is 5.0640614795684815 and perplexity is 158.23186852340433
At time: 1134.661477804184 and batch: 950, loss is 5.048334398269653 and perplexity is 155.76280950049895
At time: 1135.8238968849182 and batch: 1000, loss is 5.0444005012512205 and perplexity is 155.15125832696512
At time: 1136.9854545593262 and batch: 1050, loss is 5.016246938705445 and perplexity is 150.84411290706976
At time: 1138.1476497650146 and batch: 1100, loss is 4.972279376983643 and perplexity is 144.35555337758373
At time: 1139.309690952301 and batch: 1150, loss is 4.992624568939209 and perplexity is 147.32257478752075
At time: 1140.471229314804 and batch: 1200, loss is 5.011298036575317 and perplexity is 150.09944432120727
At time: 1141.632709980011 and batch: 1250, loss is 5.046331281661987 and perplexity is 155.4511107186587
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.953435076414234 and perplexity of 141.66074457949014
Finished 37 epochs...
Completing Train Step...
At time: 1144.5180249214172 and batch: 50, loss is 5.05526273727417 and perplexity is 156.8457341516993
At time: 1145.6819274425507 and batch: 100, loss is 5.073180160522461 and perplexity is 159.6813330077061
At time: 1146.8464975357056 and batch: 150, loss is 4.99935097694397 and perplexity is 148.31686679184577
At time: 1148.018375635147 and batch: 200, loss is 5.048275718688965 and perplexity is 155.75366967231383
At time: 1149.182536125183 and batch: 250, loss is 5.069716014862061 and perplexity is 159.129130617893
At time: 1150.3468253612518 and batch: 300, loss is 5.061931600570679 and perplexity is 157.89521243548563
At time: 1151.5365996360779 and batch: 350, loss is 5.07310040473938 and perplexity is 159.66859800580042
At time: 1152.6998569965363 and batch: 400, loss is 5.05668758392334 and perplexity is 157.06937455924867
At time: 1153.8638212680817 and batch: 450, loss is 5.010252780914307 and perplexity is 149.9426339950337
At time: 1155.0280570983887 and batch: 500, loss is 5.014981489181519 and perplexity is 150.65334802332714
At time: 1156.1935584545135 and batch: 550, loss is 5.00802622795105 and perplexity is 149.6091501764573
At time: 1157.3574917316437 and batch: 600, loss is 5.03046667098999 and perplexity is 153.00439875165677
At time: 1158.521157503128 and batch: 650, loss is 5.034297933578491 and perplexity is 153.59172315906127
At time: 1159.684202671051 and batch: 700, loss is 5.046013927459716 and perplexity is 155.40178548262386
At time: 1160.8478903770447 and batch: 750, loss is 5.032919597625733 and perplexity is 153.3801679955218
At time: 1162.0120639801025 and batch: 800, loss is 5.051936531066895 and perplexity is 156.32489957896783
At time: 1163.1775968074799 and batch: 850, loss is 5.09548677444458 and perplexity is 163.28330742386885
At time: 1164.3441925048828 and batch: 900, loss is 5.06129153251648 and perplexity is 157.79418109101195
At time: 1165.5084490776062 and batch: 950, loss is 5.047118453979492 and perplexity is 155.57352570426468
At time: 1166.685308456421 and batch: 1000, loss is 5.044329938888549 and perplexity is 155.14031087384978
At time: 1167.8597905635834 and batch: 1050, loss is 5.0174268627166745 and perplexity is 151.02220254329876
At time: 1169.0235674381256 and batch: 1100, loss is 4.974656801223755 and perplexity is 144.6991560522012
At time: 1170.1869678497314 and batch: 1150, loss is 4.996434507369995 and perplexity is 147.88493532618884
At time: 1171.3511519432068 and batch: 1200, loss is 5.016525945663452 and perplexity is 150.88620533591248
At time: 1172.5151522159576 and batch: 1250, loss is 5.048444995880127 and perplexity is 155.78003744769765
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.951667284443431 and perplexity of 141.41053907344642
Finished 38 epochs...
Completing Train Step...
At time: 1175.3784289360046 and batch: 50, loss is 5.049300327301025 and perplexity is 155.9133380084276
At time: 1176.5657169818878 and batch: 100, loss is 5.065840730667114 and perplexity is 158.51365335796353
At time: 1177.7276766300201 and batch: 150, loss is 4.991448421478271 and perplexity is 147.14940357265056
At time: 1178.8893518447876 and batch: 200, loss is 5.039889097213745 and perplexity is 154.45288482074292
At time: 1180.0503346920013 and batch: 250, loss is 5.061638126373291 and perplexity is 157.8488810636083
At time: 1181.2393825054169 and batch: 300, loss is 5.0548432922363284 and perplexity is 156.7799597821336
At time: 1182.4006333351135 and batch: 350, loss is 5.0658687973022465 and perplexity is 158.51810236526987
At time: 1183.5624692440033 and batch: 400, loss is 5.049444742202759 and perplexity is 155.93585584373145
At time: 1184.7353630065918 and batch: 450, loss is 5.003237829208374 and perplexity is 148.89447435182177
At time: 1185.9072127342224 and batch: 500, loss is 5.008821706771851 and perplexity is 149.7282084347039
At time: 1187.0783817768097 and batch: 550, loss is 5.002392902374267 and perplexity is 148.76872254800958
At time: 1188.2407371997833 and batch: 600, loss is 5.025335273742676 and perplexity is 152.22128335753007
At time: 1189.4023509025574 and batch: 650, loss is 5.029716987609863 and perplexity is 152.88973688224542
At time: 1190.5651624202728 and batch: 700, loss is 5.041682767868042 and perplexity is 154.73017103344696
At time: 1191.7301404476166 and batch: 750, loss is 5.029073162078857 and perplexity is 152.79133424667552
At time: 1192.901099205017 and batch: 800, loss is 5.049174041748047 and perplexity is 155.8936496495243
At time: 1194.0637831687927 and batch: 850, loss is 5.093444442749023 and perplexity is 162.9501690550832
At time: 1195.227040052414 and batch: 900, loss is 5.059630374908448 and perplexity is 157.53227767823847
At time: 1196.3885769844055 and batch: 950, loss is 5.046185035705566 and perplexity is 155.42837828459867
At time: 1197.5516357421875 and batch: 1000, loss is 5.04357006072998 and perplexity is 155.02246791891724
At time: 1198.7177102565765 and batch: 1050, loss is 5.017859325408936 and perplexity is 151.08752813602462
At time: 1199.880063533783 and batch: 1100, loss is 4.975565795898437 and perplexity is 144.83074661298235
At time: 1201.0416495800018 and batch: 1150, loss is 4.998025074005127 and perplexity is 148.12034333659085
At time: 1202.2038691043854 and batch: 1200, loss is 5.018873348236084 and perplexity is 151.2408120420073
At time: 1203.3660924434662 and batch: 1250, loss is 5.0483792209625244 and perplexity is 155.76979136554175
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.950479994725137 and perplexity of 141.24274342508127
Finished 39 epochs...
Completing Train Step...
At time: 1206.2422637939453 and batch: 50, loss is 5.044359970092773 and perplexity is 155.14496999416815
At time: 1207.4341011047363 and batch: 100, loss is 5.060266904830932 and perplexity is 157.6325836072137
At time: 1208.5991563796997 and batch: 150, loss is 4.985541095733643 and perplexity is 146.28270656378515
At time: 1209.7890510559082 and batch: 200, loss is 5.03384654045105 and perplexity is 153.52240855604
At time: 1210.9542343616486 and batch: 250, loss is 5.0561481285095216 and perplexity is 156.98466548523618
At time: 1212.1184220314026 and batch: 300, loss is 5.049592094421387 and perplexity is 155.95883503102917
At time: 1213.2826476097107 and batch: 350, loss is 5.06044828414917 and perplexity is 157.66117749085157
At time: 1214.4475660324097 and batch: 400, loss is 5.044266204833985 and perplexity is 155.13042346789697
At time: 1215.6125633716583 and batch: 450, loss is 4.998251419067383 and perplexity is 148.1538734394683
At time: 1216.7765910625458 and batch: 500, loss is 5.004505605697632 and perplexity is 149.08335897222693
At time: 1217.9411535263062 and batch: 550, loss is 4.998456544876099 and perplexity is 148.18426673969051
At time: 1219.1061699390411 and batch: 600, loss is 5.021547021865845 and perplexity is 151.64572166979582
At time: 1220.279595375061 and batch: 650, loss is 5.026353731155395 and perplexity is 152.37639322492896
At time: 1221.4445178508759 and batch: 700, loss is 5.038366746902466 and perplexity is 154.21793230884913
At time: 1222.6093983650208 and batch: 750, loss is 5.0260520362854 and perplexity is 152.33042898272137
At time: 1223.7738423347473 and batch: 800, loss is 5.047184362411499 and perplexity is 155.58377964931253
At time: 1224.938289642334 and batch: 850, loss is 5.09171953201294 and perplexity is 162.66933683392054
At time: 1226.1042246818542 and batch: 900, loss is 5.0581190872192385 and perplexity is 157.29438089684828
At time: 1227.2724795341492 and batch: 950, loss is 5.044959888458252 and perplexity is 155.23807223505912
At time: 1228.4363124370575 and batch: 1000, loss is 5.042439432144165 and perplexity is 154.84729413215058
At time: 1229.600708246231 and batch: 1050, loss is 5.017712345123291 and perplexity is 151.06532287988924
At time: 1230.765388250351 and batch: 1100, loss is 4.975570478439331 and perplexity is 144.83142479046384
At time: 1231.9314410686493 and batch: 1150, loss is 4.998431005477905 and perplexity is 148.18048225102322
At time: 1233.1001353263855 and batch: 1200, loss is 5.0196967124938965 and perplexity is 151.3653896002818
At time: 1234.2797858715057 and batch: 1250, loss is 5.04749137878418 and perplexity is 155.63155375032886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.949285576813413 and perplexity of 141.07414127319288
Finished 40 epochs...
Completing Train Step...
At time: 1237.1988492012024 and batch: 50, loss is 5.039963874816895 and perplexity is 154.46443486910644
At time: 1238.361065864563 and batch: 100, loss is 5.055503091812134 and perplexity is 156.88343726653883
At time: 1239.5498802661896 and batch: 150, loss is 4.980633859634399 and perplexity is 145.56662122236074
At time: 1240.7126519680023 and batch: 200, loss is 5.028905916213989 and perplexity is 152.76578266459285
At time: 1241.8743891716003 and batch: 250, loss is 5.051289224624634 and perplexity is 156.22374220782575
At time: 1243.0367126464844 and batch: 300, loss is 5.045033292770386 and perplexity is 155.24946779720506
At time: 1244.2020092010498 and batch: 350, loss is 5.056223697662354 and perplexity is 156.99652913167176
At time: 1245.3632836341858 and batch: 400, loss is 5.0401653289794925 and perplexity is 154.49555550706995
At time: 1246.5258312225342 and batch: 450, loss is 4.994177417755127 and perplexity is 147.55152218768137
At time: 1247.68816280365 and batch: 500, loss is 5.001029100418091 and perplexity is 148.56596976192978
At time: 1248.8503546714783 and batch: 550, loss is 4.995098428726196 and perplexity is 147.68748135874753
At time: 1250.012045621872 and batch: 600, loss is 5.018266315460205 and perplexity is 151.1490317716818
At time: 1251.1746771335602 and batch: 650, loss is 5.023458213806152 and perplexity is 151.93582288207926
At time: 1252.3385071754456 and batch: 700, loss is 5.035526762008667 and perplexity is 153.78057704586698
At time: 1253.5006158351898 and batch: 750, loss is 5.023193645477295 and perplexity is 151.89563079233724
At time: 1254.664029598236 and batch: 800, loss is 5.045285739898682 and perplexity is 155.28866502692577
At time: 1255.8265011310577 and batch: 850, loss is 5.090026044845581 and perplexity is 162.3940915274481
At time: 1256.9884722232819 and batch: 900, loss is 5.056429738998413 and perplexity is 157.02888023900897
At time: 1258.1508619785309 and batch: 950, loss is 5.043341436386108 and perplexity is 154.9870300600369
At time: 1259.313985824585 and batch: 1000, loss is 5.040986032485962 and perplexity is 154.62240259601396
At time: 1260.4765872955322 and batch: 1050, loss is 5.017120246887207 and perplexity is 150.97590384371773
At time: 1261.6388351917267 and batch: 1100, loss is 4.974904718399048 and perplexity is 144.7350339054212
At time: 1262.8005456924438 and batch: 1150, loss is 4.998161334991455 and perplexity is 148.140527735811
At time: 1263.9626398086548 and batch: 1200, loss is 5.019694261550903 and perplexity is 151.36501861279532
At time: 1265.124856710434 and batch: 1250, loss is 5.046395311355591 and perplexity is 155.46106452431445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.948162440835994 and perplexity of 140.91578477423255
Finished 41 epochs...
Completing Train Step...
At time: 1268.0054693222046 and batch: 50, loss is 5.036167068481445 and perplexity is 153.87907527589888
At time: 1269.196019411087 and batch: 100, loss is 5.051296138763428 and perplexity is 156.22482236419646
At time: 1270.3597946166992 and batch: 150, loss is 4.976727132797241 and perplexity is 144.999041607784
At time: 1271.5226428508759 and batch: 200, loss is 5.024696130752563 and perplexity is 152.12402327618668
At time: 1272.6858518123627 and batch: 250, loss is 5.047164583206177 and perplexity is 155.58070235622347
At time: 1273.8495638370514 and batch: 300, loss is 5.041155166625977 and perplexity is 154.64855673482066
At time: 1275.0130352973938 and batch: 350, loss is 5.052673683166504 and perplexity is 156.44017729029707
At time: 1276.1759278774261 and batch: 400, loss is 5.036363039016724 and perplexity is 153.90923399566256
At time: 1277.337954044342 and batch: 450, loss is 4.9906959724426265 and perplexity is 147.03872279188212
At time: 1278.5018832683563 and batch: 500, loss is 4.998065128326416 and perplexity is 148.12627631523225
At time: 1279.664983034134 and batch: 550, loss is 4.992103519439698 and perplexity is 147.2458324286811
At time: 1280.828097820282 and batch: 600, loss is 5.0152872276306155 and perplexity is 150.69941558625675
At time: 1281.9922511577606 and batch: 650, loss is 5.020783519744873 and perplexity is 151.5299840282542
At time: 1283.1563663482666 and batch: 700, loss is 5.032821350097656 and perplexity is 153.36509951339295
At time: 1284.322435617447 and batch: 750, loss is 5.020524940490723 and perplexity is 151.49080658345835
At time: 1285.485160112381 and batch: 800, loss is 5.043428344726562 and perplexity is 155.0005003109414
At time: 1286.649048089981 and batch: 850, loss is 5.088410110473633 and perplexity is 162.1318852442773
At time: 1287.8133356571198 and batch: 900, loss is 5.054516067504883 and perplexity is 156.72866589466986
At time: 1288.9771647453308 and batch: 950, loss is 5.041562356948853 and perplexity is 154.71154095298158
At time: 1290.1410491466522 and batch: 1000, loss is 5.03936800956726 and perplexity is 154.37242229633333
At time: 1291.305844783783 and batch: 1050, loss is 5.016238822937011 and perplexity is 150.84288869614758
At time: 1292.469441652298 and batch: 1100, loss is 4.973927907943725 and perplexity is 144.59372423866682
At time: 1293.633427619934 and batch: 1150, loss is 4.997403974533081 and perplexity is 148.02837443342045
At time: 1294.796852350235 and batch: 1200, loss is 5.019133005142212 and perplexity is 151.28008786224078
At time: 1295.9617290496826 and batch: 1250, loss is 5.0453079795837406 and perplexity is 155.29211863633265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.947057570854243 and perplexity of 140.76017713261632
Finished 42 epochs...
Completing Train Step...
At time: 1298.905689239502 and batch: 50, loss is 5.032711715698242 and perplexity is 153.3482863444844
At time: 1300.0711979866028 and batch: 100, loss is 5.047636919021606 and perplexity is 155.65420605198338
At time: 1301.237905740738 and batch: 150, loss is 4.973173246383667 and perplexity is 144.48464607688294
At time: 1302.4048461914062 and batch: 200, loss is 5.020998849868774 and perplexity is 151.56261651174208
At time: 1303.568534374237 and batch: 250, loss is 5.043624305725098 and perplexity is 155.0308773400151
At time: 1304.7309720516205 and batch: 300, loss is 5.037771244049072 and perplexity is 154.12612242935043
At time: 1305.893869638443 and batch: 350, loss is 5.049363851547241 and perplexity is 155.92324260028715
At time: 1307.0557217597961 and batch: 400, loss is 5.03303524017334 and perplexity is 153.3979062945325
At time: 1308.2178754806519 and batch: 450, loss is 4.987618169784546 and perplexity is 146.58686234537535
At time: 1309.3794531822205 and batch: 500, loss is 4.995149421691894 and perplexity is 147.69501257343634
At time: 1310.5423440933228 and batch: 550, loss is 4.989270219802856 and perplexity is 146.8292313217088
At time: 1311.7171869277954 and batch: 600, loss is 5.012477054595947 and perplexity is 150.27651863734104
At time: 1312.8882625102997 and batch: 650, loss is 5.018308582305909 and perplexity is 151.15542049950065
At time: 1314.058129787445 and batch: 700, loss is 5.030318937301636 and perplexity is 152.98179651709154
At time: 1315.2237601280212 and batch: 750, loss is 5.017959289550781 and perplexity is 151.10263222603933
At time: 1316.3865232467651 and batch: 800, loss is 5.0414964389801025 and perplexity is 154.70134301857703
At time: 1317.549551486969 and batch: 850, loss is 5.086817827224731 and perplexity is 161.87393078207538
At time: 1318.7121803760529 and batch: 900, loss is 5.052486972808838 and perplexity is 156.41097101548368
At time: 1319.8741159439087 and batch: 950, loss is 5.039690656661987 and perplexity is 154.4222381459306
At time: 1321.0368564128876 and batch: 1000, loss is 5.037659215927124 and perplexity is 154.10885693643996
At time: 1322.1991069316864 and batch: 1050, loss is 5.014990816116333 and perplexity is 150.65475316383646
At time: 1323.3623785972595 and batch: 1100, loss is 4.972724227905274 and perplexity is 144.4197843640938
At time: 1324.5328576564789 and batch: 1150, loss is 4.996473999023437 and perplexity is 147.8907756621256
At time: 1325.7042067050934 and batch: 1200, loss is 5.018525924682617 and perplexity is 151.1882765482209
At time: 1326.8665354251862 and batch: 1250, loss is 5.043695240020752 and perplexity is 155.04187473614556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.945619012317518 and perplexity of 140.5578309564763
Finished 43 epochs...
Completing Train Step...
At time: 1329.7450003623962 and batch: 50, loss is 5.0294794082641605 and perplexity is 152.85341775310062
At time: 1330.9336149692535 and batch: 100, loss is 5.044619188308716 and perplexity is 155.18519160936498
At time: 1332.0975272655487 and batch: 150, loss is 4.97012677192688 and perplexity is 144.04514709433985
At time: 1333.2600717544556 and batch: 200, loss is 5.017848501205444 and perplexity is 151.08589274272592
At time: 1334.4299576282501 and batch: 250, loss is 5.040341796875 and perplexity is 154.52282141832382
At time: 1335.5999674797058 and batch: 300, loss is 5.034549264907837 and perplexity is 153.63033042242353
At time: 1336.774426460266 and batch: 350, loss is 5.046474151611328 and perplexity is 155.47332159756778
At time: 1337.9377732276917 and batch: 400, loss is 5.030109586715699 and perplexity is 152.94977304053674
At time: 1339.1081562042236 and batch: 450, loss is 4.985246238708496 and perplexity is 146.23958043843024
At time: 1340.2712061405182 and batch: 500, loss is 4.992565813064576 and perplexity is 147.31391897507828
At time: 1341.43270778656 and batch: 550, loss is 4.9866862392425535 and perplexity is 146.45031720647592
At time: 1342.5944170951843 and batch: 600, loss is 5.009694843292237 and perplexity is 149.85899869218613
At time: 1343.7571742534637 and batch: 650, loss is 5.015932531356811 and perplexity is 150.79669386431206
At time: 1344.9201641082764 and batch: 700, loss is 5.027964887619018 and perplexity is 152.62209331327006
At time: 1346.0834467411041 and batch: 750, loss is 5.015549945831299 and perplexity is 150.73901226672015
At time: 1347.2475843429565 and batch: 800, loss is 5.039630947113037 and perplexity is 154.41301793901292
At time: 1348.4281690120697 and batch: 850, loss is 5.085186862945557 and perplexity is 161.61013536216933
At time: 1349.5919411182404 and batch: 900, loss is 5.050419807434082 and perplexity is 156.08797762735983
At time: 1350.7541871070862 and batch: 950, loss is 5.037661752700806 and perplexity is 154.10924787622824
At time: 1351.9172656536102 and batch: 1000, loss is 5.035831317901612 and perplexity is 153.8274189594551
At time: 1353.080003976822 and batch: 1050, loss is 5.013584671020507 and perplexity is 150.44305959233185
At time: 1354.2432551383972 and batch: 1100, loss is 4.971272401809692 and perplexity is 144.21026408274398
At time: 1355.4056684970856 and batch: 1150, loss is 4.995166301727295 and perplexity is 147.69750569151898
At time: 1356.5679461956024 and batch: 1200, loss is 5.017308616638184 and perplexity is 151.0043458158478
At time: 1357.7576551437378 and batch: 1250, loss is 5.0420873165130615 and perplexity is 154.79277957772922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.944417911724453 and perplexity of 140.3891082091343
Finished 44 epochs...
Completing Train Step...
At time: 1360.6320142745972 and batch: 50, loss is 5.026286544799805 and perplexity is 152.36615595429652
At time: 1361.822027683258 and batch: 100, loss is 5.041673641204834 and perplexity is 154.7287588697321
At time: 1362.987449169159 and batch: 150, loss is 4.967071552276611 and perplexity is 143.60572913141613
At time: 1364.1512615680695 and batch: 200, loss is 5.014998302459717 and perplexity is 150.65588102127288
At time: 1365.3168907165527 and batch: 250, loss is 5.037261867523194 and perplexity is 154.04763419229013
At time: 1366.4818501472473 and batch: 300, loss is 5.031670408248901 and perplexity is 153.18868674211916
At time: 1367.646472454071 and batch: 350, loss is 5.044147424697876 and perplexity is 155.11199814938536
At time: 1368.811627149582 and batch: 400, loss is 5.027805013656616 and perplexity is 152.59769496484424
At time: 1369.9774506092072 and batch: 450, loss is 4.982201337814331 and perplexity is 145.79497264603546
At time: 1371.1508173942566 and batch: 500, loss is 4.989999513626099 and perplexity is 146.9363520296707
At time: 1372.3161461353302 and batch: 550, loss is 4.984329242706298 and perplexity is 146.10554079410682
At time: 1373.486840248108 and batch: 600, loss is 5.0072160339355465 and perplexity is 149.4879868278575
At time: 1374.6524031162262 and batch: 650, loss is 5.013892908096313 and perplexity is 150.4894388686349
At time: 1375.8165996074677 and batch: 700, loss is 5.025964841842652 and perplexity is 152.31714719491
At time: 1376.9813241958618 and batch: 750, loss is 5.013511371612549 and perplexity is 150.43203260927288
At time: 1378.1457245349884 and batch: 800, loss is 5.038026037216187 and perplexity is 154.16539771554417
At time: 1379.3109810352325 and batch: 850, loss is 5.083515586853028 and perplexity is 161.34026578273205
At time: 1380.4762110710144 and batch: 900, loss is 5.048390827178955 and perplexity is 155.77159927394516
At time: 1381.641546010971 and batch: 950, loss is 5.035867490768433 and perplexity is 153.83298343883553
At time: 1382.806034564972 and batch: 1000, loss is 5.0339438915252686 and perplexity is 153.53735485493692
At time: 1383.9706497192383 and batch: 1050, loss is 5.012171974182129 and perplexity is 150.2306792075597
At time: 1385.1351323127747 and batch: 1100, loss is 4.969780693054199 and perplexity is 143.99530473740907
At time: 1386.3255355358124 and batch: 1150, loss is 4.993644304275513 and perplexity is 147.4728814463477
At time: 1387.4906992912292 and batch: 1200, loss is 5.016051921844483 and perplexity is 150.8146986299092
At time: 1388.6561193466187 and batch: 1250, loss is 5.040310201644897 and perplexity is 154.51793931135094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.942971779482208 and perplexity of 140.18623372031865
Finished 45 epochs...
Completing Train Step...
At time: 1391.5559177398682 and batch: 50, loss is 5.023270797729492 and perplexity is 151.90735033444057
At time: 1392.718058347702 and batch: 100, loss is 5.038878870010376 and perplexity is 154.29693110246586
At time: 1393.8806099891663 and batch: 150, loss is 4.964183950424195 and perplexity is 143.19165109593345
At time: 1395.043547153473 and batch: 200, loss is 5.012275238037109 and perplexity is 150.24619340764517
At time: 1396.2054677009583 and batch: 250, loss is 5.034291124343872 and perplexity is 153.59067732054334
At time: 1397.366707086563 and batch: 300, loss is 5.028656129837036 and perplexity is 152.7276286185975
At time: 1398.5286078453064 and batch: 350, loss is 5.041400175094605 and perplexity is 154.68645158297159
At time: 1399.6906797885895 and batch: 400, loss is 5.024891996383667 and perplexity is 152.15382206219556
At time: 1400.8528594970703 and batch: 450, loss is 4.979478330612182 and perplexity is 145.39851191317072
At time: 1402.0262439250946 and batch: 500, loss is 4.987662496566773 and perplexity is 146.5933602133136
At time: 1403.1976771354675 and batch: 550, loss is 4.981867227554321 and perplexity is 145.74626918643241
At time: 1404.3612368106842 and batch: 600, loss is 5.004680061340332 and perplexity is 149.10936967422333
At time: 1405.530243396759 and batch: 650, loss is 5.011717119216919 and perplexity is 150.16236157568017
At time: 1406.7010927200317 and batch: 700, loss is 5.023794384002685 and perplexity is 151.9869077636442
At time: 1407.867259979248 and batch: 750, loss is 5.011360921859741 and perplexity is 150.10888366425013
At time: 1409.0396084785461 and batch: 800, loss is 5.036094999313354 and perplexity is 153.86798573856876
At time: 1410.2060282230377 and batch: 850, loss is 5.081638298034668 and perplexity is 161.03766762676264
At time: 1411.3676767349243 and batch: 900, loss is 5.046316938400269 and perplexity is 155.4488810586835
At time: 1412.5301365852356 and batch: 950, loss is 5.03392954826355 and perplexity is 153.5351526442661
At time: 1413.69229221344 and batch: 1000, loss is 5.031843347549438 and perplexity is 153.21518137737033
At time: 1414.85409283638 and batch: 1050, loss is 5.010471830368042 and perplexity is 149.975482444688
At time: 1416.0438389778137 and batch: 1100, loss is 4.968169012069702 and perplexity is 143.7634171575028
At time: 1417.205298423767 and batch: 1150, loss is 4.991818008422851 and perplexity is 147.2037981222523
At time: 1418.367341518402 and batch: 1200, loss is 5.014513826370239 and perplexity is 150.58290952707816
At time: 1419.5297601222992 and batch: 1250, loss is 5.0382900810241695 and perplexity is 154.20610950882806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.941452583257299 and perplexity of 139.97342501328825
Finished 46 epochs...
Completing Train Step...
At time: 1422.4073646068573 and batch: 50, loss is 5.020279312133789 and perplexity is 151.453600715142
At time: 1423.597635269165 and batch: 100, loss is 5.036009130477905 and perplexity is 153.85477384107372
At time: 1424.7630534172058 and batch: 150, loss is 4.961285552978516 and perplexity is 142.7772256550962
At time: 1425.9276571273804 and batch: 200, loss is 5.009613332748413 and perplexity is 149.84678410152156
At time: 1427.0924973487854 and batch: 250, loss is 5.0313412380218505 and perplexity is 153.1382698856419
At time: 1428.2573006153107 and batch: 300, loss is 5.025827016830444 and perplexity is 152.29615552886
At time: 1429.4221918582916 and batch: 350, loss is 5.038520965576172 and perplexity is 154.2417174278391
At time: 1430.58646774292 and batch: 400, loss is 5.021803398132324 and perplexity is 151.6846050179158
At time: 1431.7524292469025 and batch: 450, loss is 4.976386947631836 and perplexity is 144.9497234739554
At time: 1432.9175171852112 and batch: 500, loss is 4.984653263092041 and perplexity is 146.15288963837523
At time: 1434.0818955898285 and batch: 550, loss is 4.979089641571045 and perplexity is 145.3420080869128
At time: 1435.2465000152588 and batch: 600, loss is 5.001878986358642 and perplexity is 148.69228756113233
At time: 1436.412014245987 and batch: 650, loss is 5.009392986297607 and perplexity is 149.81376953193538
At time: 1437.5773973464966 and batch: 700, loss is 5.021057996749878 and perplexity is 151.57158123291572
At time: 1438.7411651611328 and batch: 750, loss is 5.008874969482422 and perplexity is 149.73618357732104
At time: 1439.9069674015045 and batch: 800, loss is 5.03355185508728 and perplexity is 153.47717441447497
At time: 1441.0728373527527 and batch: 850, loss is 5.07926905632019 and perplexity is 160.6565820871602
At time: 1442.238447189331 and batch: 900, loss is 5.043491840362549 and perplexity is 155.0103424787509
At time: 1443.403603553772 and batch: 950, loss is 5.031487264633179 and perplexity is 153.16063378108015
At time: 1444.5688898563385 and batch: 1000, loss is 5.02923152923584 and perplexity is 152.81553329200824
At time: 1445.7598688602448 and batch: 1050, loss is 5.007908849716187 and perplexity is 149.59159034908112
At time: 1446.924222946167 and batch: 1100, loss is 4.965778951644897 and perplexity is 143.42022419295222
At time: 1448.0888741016388 and batch: 1150, loss is 4.9889352416992185 and perplexity is 146.78005498120055
At time: 1449.2535219192505 and batch: 1200, loss is 5.01192813873291 and perplexity is 150.1940521080827
At time: 1450.4185066223145 and batch: 1250, loss is 5.0357053852081295 and perplexity is 153.80804827798076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.938628482122491 and perplexity of 139.57868356211185
Finished 47 epochs...
Completing Train Step...
At time: 1453.3153810501099 and batch: 50, loss is 5.016631422042846 and perplexity is 150.902121105906
At time: 1454.4771838188171 and batch: 100, loss is 5.031831493377686 and perplexity is 153.2133651490601
At time: 1455.640478849411 and batch: 150, loss is 4.956782808303833 and perplexity is 142.13578147604994
At time: 1456.8069500923157 and batch: 200, loss is 5.005703420639038 and perplexity is 149.2620402393009
At time: 1457.9744443893433 and batch: 250, loss is 5.027051458358764 and perplexity is 152.48274747844658
At time: 1459.1366183757782 and batch: 300, loss is 5.021811733245849 and perplexity is 151.6858693315878
At time: 1460.3185522556305 and batch: 350, loss is 5.034301042556763 and perplexity is 153.59220067313353
At time: 1461.4832589626312 and batch: 400, loss is 5.0179464149475095 and perplexity is 151.10068685211914
At time: 1462.6450486183167 and batch: 450, loss is 4.97213716506958 and perplexity is 144.335025757703
At time: 1463.807517528534 and batch: 500, loss is 4.980174083709716 and perplexity is 145.49970857807546
At time: 1464.9699351787567 and batch: 550, loss is 4.974917793273926 and perplexity is 144.73692631025142
At time: 1466.1318247318268 and batch: 600, loss is 4.997766904830932 and perplexity is 148.0821081656539
At time: 1467.2939534187317 and batch: 650, loss is 5.005485115051269 and perplexity is 149.22945905833123
At time: 1468.4556090831757 and batch: 700, loss is 5.017100667953491 and perplexity is 150.97294792544062
At time: 1469.6180157661438 and batch: 750, loss is 5.0048105430603025 and perplexity is 149.12882699062655
At time: 1470.779348373413 and batch: 800, loss is 5.0296251010894775 and perplexity is 152.87568902173504
At time: 1471.940839290619 and batch: 850, loss is 5.074524507522583 and perplexity is 159.89614448694257
At time: 1473.103298664093 and batch: 900, loss is 5.038115148544311 and perplexity is 154.17913621100413
At time: 1474.2936825752258 and batch: 950, loss is 5.027170181274414 and perplexity is 152.50085174948813
At time: 1475.4582467079163 and batch: 1000, loss is 5.024574222564698 and perplexity is 152.10547924253603
At time: 1476.6204307079315 and batch: 1050, loss is 5.0035667228698735 and perplexity is 148.94345285458638
At time: 1477.782392501831 and batch: 1100, loss is 4.960880746841431 and perplexity is 142.71944025464657
At time: 1478.9448337554932 and batch: 1150, loss is 4.983940048217773 and perplexity is 146.04868838692616
At time: 1480.107394695282 and batch: 1200, loss is 5.007584943771362 and perplexity is 149.54314459002947
At time: 1481.2690980434418 and batch: 1250, loss is 5.0310777568817135 and perplexity is 153.09792615483332
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.933832098967837 and perplexity of 138.9108136758398
Finished 48 epochs...
Completing Train Step...
At time: 1484.1461057662964 and batch: 50, loss is 5.010683040618897 and perplexity is 150.00716214937867
At time: 1485.3360831737518 and batch: 100, loss is 5.025615453720093 and perplexity is 152.2639386885694
At time: 1486.5003116130829 and batch: 150, loss is 4.950159759521484 and perplexity is 141.1975197678661
At time: 1487.6632163524628 and batch: 200, loss is 4.998382072448731 and perplexity is 148.173231508564
At time: 1488.8255367279053 and batch: 250, loss is 5.020194244384766 and perplexity is 151.44071744623074
At time: 1489.9942984580994 and batch: 300, loss is 5.015141344070434 and perplexity is 150.6774326225093
At time: 1491.1581342220306 and batch: 350, loss is 5.027468681335449 and perplexity is 152.54638005780663
At time: 1492.3246841430664 and batch: 400, loss is 5.0105929279327395 and perplexity is 149.99364521008755
At time: 1493.48916721344 and batch: 450, loss is 4.96519024848938 and perplexity is 143.33581710220292
At time: 1494.6524317264557 and batch: 500, loss is 4.972238645553589 and perplexity is 144.34967368920334
At time: 1495.815581560135 and batch: 550, loss is 4.966875190734863 and perplexity is 143.57753325742334
At time: 1496.979697227478 and batch: 600, loss is 4.989521436691284 and perplexity is 146.86612193791055
At time: 1498.1428337097168 and batch: 650, loss is 4.99687876701355 and perplexity is 147.95064923078115
At time: 1499.3065185546875 and batch: 700, loss is 5.0098834896087645 and perplexity is 149.88727170700417
At time: 1500.470272064209 and batch: 750, loss is 4.996590213775635 and perplexity is 147.90796375070687
At time: 1501.633850812912 and batch: 800, loss is 5.020522336959839 and perplexity is 151.4904121729783
At time: 1502.8002507686615 and batch: 850, loss is 5.064587154388428 and perplexity is 158.31506889865534
At time: 1503.991471529007 and batch: 900, loss is 5.029511861801147 and perplexity is 152.85837846764315
At time: 1505.1588079929352 and batch: 950, loss is 5.017411804199218 and perplexity is 151.01992838994823
At time: 1506.3251791000366 and batch: 1000, loss is 5.013705492019653 and perplexity is 150.46123737121295
At time: 1507.4896454811096 and batch: 1050, loss is 4.992724046707154 and perplexity is 147.33723083739196
At time: 1508.6531126499176 and batch: 1100, loss is 4.950740709304809 and perplexity is 141.27957226826473
At time: 1509.8174085617065 and batch: 1150, loss is 4.974019632339478 and perplexity is 144.60698761893067
At time: 1510.981234550476 and batch: 1200, loss is 4.9955112266540525 and perplexity is 147.74845902988497
At time: 1512.1445727348328 and batch: 1250, loss is 5.019683017730713 and perplexity is 151.36331670131094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.924335563269845 and perplexity of 137.5978861734714
Finished 49 epochs...
Completing Train Step...
At time: 1515.0173783302307 and batch: 50, loss is 5.00123854637146 and perplexity is 148.59708956194913
At time: 1516.2053310871124 and batch: 100, loss is 5.014993047714233 and perplexity is 150.65508936504241
At time: 1517.3674955368042 and batch: 150, loss is 4.936483335494995 and perplexity is 139.27958773669354
At time: 1518.530526638031 and batch: 200, loss is 4.984543752670288 and perplexity is 146.13688525013015
At time: 1519.6933193206787 and batch: 250, loss is 5.009033365249634 and perplexity is 149.75990303347928
At time: 1520.8604567050934 and batch: 300, loss is 5.002266435623169 and perplexity is 148.7499094406453
At time: 1522.0265271663666 and batch: 350, loss is 5.015316562652588 and perplexity is 150.70383642176654
At time: 1523.1867344379425 and batch: 400, loss is 4.999459180831909 and perplexity is 148.33291612176393
At time: 1524.3490810394287 and batch: 450, loss is 4.953729934692383 and perplexity is 141.7025205814155
At time: 1525.5106415748596 and batch: 500, loss is 4.961229286193848 and perplexity is 142.7691922656934
At time: 1526.6863613128662 and batch: 550, loss is 4.9555565547943115 and perplexity is 141.96159379614036
At time: 1527.8487842082977 and batch: 600, loss is 4.97885461807251 and perplexity is 145.30785331343245
At time: 1529.0113790035248 and batch: 650, loss is 4.986869163513184 and perplexity is 146.47710897429207
At time: 1530.173883676529 and batch: 700, loss is 4.999445495605468 and perplexity is 148.3308861661084
At time: 1531.3372220993042 and batch: 750, loss is 4.986214570999145 and perplexity is 146.38125753052088
At time: 1532.505957365036 and batch: 800, loss is 5.010086975097656 and perplexity is 149.9177746951186
At time: 1533.7021605968475 and batch: 850, loss is 5.055796022415161 and perplexity is 156.92939995803314
At time: 1534.864759683609 and batch: 900, loss is 5.018490257263184 and perplexity is 151.18288414871495
At time: 1536.0357267856598 and batch: 950, loss is 5.00791482925415 and perplexity is 149.59248484034893
At time: 1537.1974234580994 and batch: 1000, loss is 5.004246263504029 and perplexity is 149.0447003800001
At time: 1538.35964179039 and batch: 1050, loss is 4.983712501525879 and perplexity is 146.01545927175923
At time: 1539.521879196167 and batch: 1100, loss is 4.942865419387817 and perplexity is 140.17132429216605
At time: 1540.6842546463013 and batch: 1150, loss is 4.966098041534424 and perplexity is 143.46599543862828
At time: 1541.8541293144226 and batch: 1200, loss is 4.9875422286987305 and perplexity is 146.57573080255864
At time: 1543.0177083015442 and batch: 1250, loss is 5.013048505783081 and perplexity is 150.36241887387314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.917295135720803 and perplexity of 136.63254043581617
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9c6396a6a0>
SETTINGS FOR THIS RUN
{'seq_len': 35, 'anneal': 8.0, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 1.1269305926304014, 'dropout': 0.6554049880811403, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.train.tokens...
Got Train Dataset with 2199934 words
Retrieving Valid Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.valid.tokens...
Retrieving Test Data from file: /home-nfs/siddsach/Interpreting-Attention/interpreting_language/data/wikitext-2/wikitext-2/wiki.test.tokens...
Loading Vectors From Memory...
Using these vectors: ['GloVe']
Building Vocab...
Found 20471 tokens
Getting Batches...
Created Iterator with 1258 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 200 hidden size...
here
200
200
Using Cross Entropy Loss ...
Begin Training...
Finished 0 epochs...
Completing Train Step...
At time: 1.622795820236206 and batch: 50, loss is 8.08940996170044 and perplexity is 3259.7636172978514
At time: 2.7773189544677734 and batch: 100, loss is 6.890490760803223 and perplexity is 982.8836596500229
At time: 3.932062864303589 and batch: 150, loss is 6.616653146743775 and perplexity is 747.4393364672172
At time: 5.087019920349121 and batch: 200, loss is 6.5216890144348145 and perplexity is 679.7254824865536
At time: 6.241713523864746 and batch: 250, loss is 6.512004528045654 and perplexity is 673.1744631331294
At time: 7.39737606048584 and batch: 300, loss is 6.470947151184082 and perplexity is 646.0953872630236
At time: 8.579712390899658 and batch: 350, loss is 6.453171253204346 and perplexity is 634.7119368465496
At time: 9.737095594406128 and batch: 400, loss is 6.3862134456634525 and perplexity is 593.6046026796274
At time: 10.898399114608765 and batch: 450, loss is 6.34456618309021 and perplexity is 569.3903249776258
At time: 12.06225848197937 and batch: 500, loss is 6.323572225570679 and perplexity is 557.5611735027812
At time: 13.231628179550171 and batch: 550, loss is 6.313589057922363 and perplexity is 552.0226388924139
At time: 14.39546799659729 and batch: 600, loss is 6.323745365142822 and perplexity is 557.6577177633801
At time: 15.55914855003357 and batch: 650, loss is 6.28323914527893 and perplexity is 535.5204861537989
At time: 16.72365427017212 and batch: 700, loss is 6.2745443534851075 and perplexity is 530.8844309994014
At time: 17.896151781082153 and batch: 750, loss is 6.203364791870118 and perplexity is 494.4098316010031
At time: 19.0722553730011 and batch: 800, loss is 6.192759857177735 and perplexity is 489.19435141863084
At time: 20.251421689987183 and batch: 850, loss is 6.237134294509888 and perplexity is 511.39091334797035
At time: 21.420417308807373 and batch: 900, loss is 6.2118901443481445 and perplexity is 498.64286810951046
At time: 22.58494782447815 and batch: 950, loss is 6.17596941947937 and perplexity is 481.0491363582871
At time: 23.748994827270508 and batch: 1000, loss is 6.153908901214599 and perplexity is 470.55314230094956
At time: 24.915156841278076 and batch: 1050, loss is 6.135946359634399 and perplexity is 462.1762720903211
At time: 26.07906985282898 and batch: 1100, loss is 6.115047969818115 and perplexity is 452.61775884200995
At time: 27.242900848388672 and batch: 1150, loss is 6.145318565368652 and perplexity is 466.5282451354481
At time: 28.406710147857666 and batch: 1200, loss is 6.133008966445923 and perplexity is 460.82067059881007
At time: 29.571566820144653 and batch: 1250, loss is 6.107923612594605 and perplexity is 449.4046076514339
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.487229841468978 and perplexity of 241.58704464601558
Finished 1 epochs...
Completing Train Step...
At time: 32.497607946395874 and batch: 50, loss is 5.789489192962646 and perplexity is 326.8460264772956
At time: 33.65759205818176 and batch: 100, loss is 5.762083473205567 and perplexity is 318.01020491901323
At time: 34.81944441795349 and batch: 150, loss is 5.631257791519165 and perplexity is 279.0128370009969
At time: 35.98085975646973 and batch: 200, loss is 5.6142152976989745 and perplexity is 274.29805240982506
At time: 37.14192605018616 and batch: 250, loss is 5.625367650985718 and perplexity is 277.3742426932135
At time: 38.30420970916748 and batch: 300, loss is 5.602808246612549 and perplexity is 271.1868987926174
At time: 39.46541357040405 and batch: 350, loss is 5.613609838485718 and perplexity is 274.1320263928777
At time: 40.62691259384155 and batch: 400, loss is 5.568962421417236 and perplexity is 262.16194445492465
At time: 41.78765940666199 and batch: 450, loss is 5.52764892578125 and perplexity is 251.55179829811655
At time: 42.94899916648865 and batch: 500, loss is 5.509283275604248 and perplexity is 246.9740512902617
At time: 44.11958289146423 and batch: 550, loss is 5.500126962661743 and perplexity is 244.7230009754891
At time: 45.32891321182251 and batch: 600, loss is 5.51328667640686 and perplexity is 247.96476920266045
At time: 46.49186038970947 and batch: 650, loss is 5.478101291656494 and perplexity is 239.39174049119296
At time: 47.653303384780884 and batch: 700, loss is 5.474434766769409 and perplexity is 238.5156118721249
At time: 48.81444239616394 and batch: 750, loss is 5.441155252456665 and perplexity is 230.70855619345588
At time: 49.97567582130432 and batch: 800, loss is 5.436463289260864 and perplexity is 229.62861564070036
At time: 51.137510538101196 and batch: 850, loss is 5.474457521438598 and perplexity is 238.52103927771861
At time: 52.29856634140015 and batch: 900, loss is 5.445299110412598 and perplexity is 231.66656323121012
At time: 53.46017265319824 and batch: 950, loss is 5.409801816940307 and perplexity is 223.58727207970128
At time: 54.62088680267334 and batch: 1000, loss is 5.385836534500122 and perplexity is 218.292637071297
At time: 55.78273963928223 and batch: 1050, loss is 5.361169233322143 and perplexity is 212.9738170109106
At time: 56.94435405731201 and batch: 1100, loss is 5.33678822517395 and perplexity is 207.84408872100218
At time: 58.106367349624634 and batch: 1150, loss is 5.365456285476685 and perplexity is 213.88880677503943
At time: 59.26757621765137 and batch: 1200, loss is 5.360309038162232 and perplexity is 212.79069673520877
At time: 60.42939281463623 and batch: 1250, loss is 5.342088661193848 and perplexity is 208.9486778312521
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 5.023130096658303 and perplexity of 151.88597831109547
Finished 2 epochs...
Completing Train Step...
At time: 63.34748435020447 and batch: 50, loss is 5.293459415435791 and perplexity is 199.03076592955063
At time: 64.50495386123657 and batch: 100, loss is 5.317574424743652 and perplexity is 203.88873425782018
At time: 65.66256737709045 and batch: 150, loss is 5.215880727767944 and perplexity is 184.17395668824318
At time: 66.82118368148804 and batch: 200, loss is 5.233783702850342 and perplexity is 187.5009107721889
At time: 67.97975707054138 and batch: 250, loss is 5.261772623062134 and perplexity is 192.82299101346797
At time: 69.13776063919067 and batch: 300, loss is 5.259989757537841 and perplexity is 192.4795198229512
At time: 70.29573655128479 and batch: 350, loss is 5.269749717712402 and perplexity is 194.36730966228538
At time: 71.45377326011658 and batch: 400, loss is 5.25189130783081 and perplexity is 190.92702898151703
At time: 72.63886785507202 and batch: 450, loss is 5.207140483856201 and perplexity is 182.5712456302082
At time: 73.79647374153137 and batch: 500, loss is 5.204509696960449 and perplexity is 182.09157082740987
At time: 74.9548966884613 and batch: 550, loss is 5.200379037857056 and perplexity is 181.3409639405647
At time: 76.11262011528015 and batch: 600, loss is 5.2196923351287845 and perplexity is 184.87729507058017
At time: 77.27085089683533 and batch: 650, loss is 5.200563859939575 and perplexity is 181.37448285258841
At time: 78.43039536476135 and batch: 700, loss is 5.202529191970825 and perplexity is 181.73129444516638
At time: 79.59023427963257 and batch: 750, loss is 5.186653804779053 and perplexity is 178.8690197546432
At time: 80.74975299835205 and batch: 800, loss is 5.188793802261353 and perplexity is 179.25220887221565
At time: 81.90776419639587 and batch: 850, loss is 5.228871021270752 and perplexity is 186.58203741582832
At time: 83.06602191925049 and batch: 900, loss is 5.205598487854004 and perplexity is 182.2899384423496
At time: 84.22494769096375 and batch: 950, loss is 5.177691164016724 and perplexity is 177.2730437504406
At time: 85.38376641273499 and batch: 1000, loss is 5.16042839050293 and perplexity is 174.23908198731542
At time: 86.54269862174988 and batch: 1050, loss is 5.135913610458374 and perplexity is 170.01958060694005
At time: 87.70214676856995 and batch: 1100, loss is 5.117427883148193 and perplexity is 166.9055165501467
At time: 88.86033654212952 and batch: 1150, loss is 5.144758806228638 and perplexity is 171.5301076877521
At time: 90.0188615322113 and batch: 1200, loss is 5.147138586044312 and perplexity is 171.93879767902422
At time: 91.17737436294556 and batch: 1250, loss is 5.135465440750122 and perplexity is 169.94340005328561
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.861761162750913 and perplexity of 129.25163494042732
Finished 3 epochs...
Completing Train Step...
At time: 94.07376027107239 and batch: 50, loss is 5.098316860198975 and perplexity is 163.74606770290762
At time: 95.2585871219635 and batch: 100, loss is 5.125694923400879 and perplexity is 168.29105041491482
At time: 96.41922354698181 and batch: 150, loss is 5.025048751831054 and perplexity is 152.17767487212464
At time: 97.58020853996277 and batch: 200, loss is 5.0547160720825195 and perplexity is 156.76001548022145
At time: 98.74263763427734 and batch: 250, loss is 5.084660472869873 and perplexity is 161.52508777685105
At time: 99.90414142608643 and batch: 300, loss is 5.089501314163208 and perplexity is 162.30890071805794
At time: 101.06545042991638 and batch: 350, loss is 5.0960901260375975 and perplexity is 163.38185439376235
At time: 102.254234790802 and batch: 400, loss is 5.084400777816772 and perplexity is 161.48314595688097
At time: 103.41771149635315 and batch: 450, loss is 5.035701818466187 and perplexity is 153.8074996853422
At time: 104.58437967300415 and batch: 500, loss is 5.038418550491333 and perplexity is 154.22592155814442
At time: 105.75765228271484 and batch: 550, loss is 5.035924768447876 and perplexity is 153.84179488749592
At time: 106.93001079559326 and batch: 600, loss is 5.054745454788208 and perplexity is 156.76462158128948
At time: 108.09196829795837 and batch: 650, loss is 5.044441556930542 and perplexity is 155.1576282980341
At time: 109.25316739082336 and batch: 700, loss is 5.047605361938476 and perplexity is 155.64929413676677
At time: 110.42592525482178 and batch: 750, loss is 5.035276050567627 and perplexity is 153.74202732842141
At time: 111.5907154083252 and batch: 800, loss is 5.042554006576538 and perplexity is 154.86503668938266
At time: 112.75310897827148 and batch: 850, loss is 5.082845554351807 and perplexity is 161.23219876915073
At time: 113.91494846343994 and batch: 900, loss is 5.060248537063599 and perplexity is 157.62968827518424
At time: 115.07862877845764 and batch: 950, loss is 5.035438070297241 and perplexity is 153.76693858812274
At time: 116.24076175689697 and batch: 1000, loss is 5.021644773483277 and perplexity is 151.66054600890538
At time: 117.40512537956238 and batch: 1050, loss is 4.996944580078125 and perplexity is 147.96038663683365
At time: 118.56850385665894 and batch: 1100, loss is 4.9797597503662105 and perplexity is 145.43943568473816
At time: 119.73230147361755 and batch: 1150, loss is 5.0034921264648435 and perplexity is 148.9323426228475
At time: 120.89631748199463 and batch: 1200, loss is 5.011252241134644 and perplexity is 150.09257060840332
At time: 122.06016397476196 and batch: 1250, loss is 5.002776050567627 and perplexity is 148.82573393649014
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.770469498460311 and perplexity of 117.97461786164511
Finished 4 epochs...
Completing Train Step...
At time: 124.97054076194763 and batch: 50, loss is 4.967559270858764 and perplexity is 143.67578539649966
At time: 126.13023447990417 and batch: 100, loss is 4.995361709594727 and perplexity is 147.72636976617298
At time: 127.29036617279053 and batch: 150, loss is 4.896431713104248 and perplexity is 133.81144914169235
At time: 128.45035338401794 and batch: 200, loss is 4.933460960388183 and perplexity is 138.8592680796249
At time: 129.6083788871765 and batch: 250, loss is 4.962445192337036 and perplexity is 142.9428917833824
At time: 130.76692271232605 and batch: 300, loss is 4.971110134124756 and perplexity is 144.1868653155305
At time: 131.9636037349701 and batch: 350, loss is 4.976477766036988 and perplexity is 144.96288817445802
At time: 133.12377309799194 and batch: 400, loss is 4.966323194503784 and perplexity is 143.49830087019896
At time: 134.28332328796387 and batch: 450, loss is 4.915153579711914 and perplexity is 136.34024729061792
At time: 135.44396090507507 and batch: 500, loss is 4.922227621078491 and perplexity is 137.3081432716963
At time: 136.60408854484558 and batch: 550, loss is 4.9201881885528564 and perplexity is 137.02839793610337
At time: 137.76344799995422 and batch: 600, loss is 4.9393931579589845 and perplexity is 139.6854568269864
At time: 138.92457699775696 and batch: 650, loss is 4.9328809833526615 and perplexity is 138.77875624273443
At time: 140.08460807800293 and batch: 700, loss is 4.936500205993652 and perplexity is 139.2819374726119
At time: 141.24441289901733 and batch: 750, loss is 4.926379728317261 and perplexity is 137.8794466429764
At time: 142.41009616851807 and batch: 800, loss is 4.936552076339722 and perplexity is 139.28916226228446
At time: 143.57162618637085 and batch: 850, loss is 4.976833896636963 and perplexity is 145.01452308863847
At time: 144.7325475215912 and batch: 900, loss is 4.954590368270874 and perplexity is 141.82449865777141
At time: 145.8946189880371 and batch: 950, loss is 4.932194375991822 and perplexity is 138.68350243189593
At time: 147.05498361587524 and batch: 1000, loss is 4.918999319076538 and perplexity is 136.86558585674078
At time: 148.21639466285706 and batch: 1050, loss is 4.89545241355896 and perplexity is 133.6804717938897
At time: 149.37697291374207 and batch: 1100, loss is 4.879058361053467 and perplexity is 131.50677366524
At time: 150.539870262146 and batch: 1150, loss is 4.896657152175903 and perplexity is 133.8416188711538
At time: 151.708491563797 and batch: 1200, loss is 4.908406906127929 and perplexity is 135.4235001200896
At time: 152.86763215065002 and batch: 1250, loss is 4.903052177429199 and perplexity is 134.70028206383037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.7062253186302465 and perplexity of 110.63376353989932
Finished 5 epochs...
Completing Train Step...
At time: 155.76575899124146 and batch: 50, loss is 4.869839754104614 and perplexity is 130.3000351775928
At time: 156.95808005332947 and batch: 100, loss is 4.89647216796875 and perplexity is 133.8168625752353
At time: 158.1178925037384 and batch: 150, loss is 4.799457817077637 and perplexity is 121.44455450198191
At time: 159.27842783927917 and batch: 200, loss is 4.840349130630493 and perplexity is 126.51351376333301
At time: 160.44240832328796 and batch: 250, loss is 4.869634370803833 and perplexity is 130.27327647426566
At time: 161.64617347717285 and batch: 300, loss is 4.879801578521729 and perplexity is 131.6045481259657
At time: 162.80557298660278 and batch: 350, loss is 4.883708610534668 and perplexity is 132.11973708305746
At time: 163.96695923805237 and batch: 400, loss is 4.874197645187378 and perplexity is 130.86910761621544
At time: 165.12691354751587 and batch: 450, loss is 4.821498527526855 and perplexity is 124.15099513733645
At time: 166.28912687301636 and batch: 500, loss is 4.830560970306396 and perplexity is 125.28121999058597
At time: 167.44849967956543 and batch: 550, loss is 4.829590272903443 and perplexity is 125.1596688399397
At time: 168.61062240600586 and batch: 600, loss is 4.849488353729248 and perplexity is 127.67504866914986
At time: 169.770925283432 and batch: 650, loss is 4.848520011901855 and perplexity is 127.55147541943845
At time: 170.93107223510742 and batch: 700, loss is 4.84796028137207 and perplexity is 127.48010094162765
At time: 172.10658431053162 and batch: 750, loss is 4.840197305679322 and perplexity is 126.49430731332946
At time: 173.2745532989502 and batch: 800, loss is 4.853013906478882 and perplexity is 128.12596819126617
At time: 174.43425750732422 and batch: 850, loss is 4.893204870223999 and perplexity is 133.38035652786266
At time: 175.5947675704956 and batch: 900, loss is 4.870639743804932 and perplexity is 130.40431556974428
At time: 176.7548794746399 and batch: 950, loss is 4.848973560333252 and perplexity is 127.6093393120842
At time: 177.91570496559143 and batch: 1000, loss is 4.837074356079102 and perplexity is 126.09988816299712
At time: 179.07569313049316 and batch: 1050, loss is 4.814856462478637 and perplexity is 123.32910868506916
At time: 180.23552632331848 and batch: 1100, loss is 4.79898250579834 and perplexity is 121.38684425167084
At time: 181.39560651779175 and batch: 1150, loss is 4.811971426010132 and perplexity is 122.97381247658414
At time: 182.55525875091553 and batch: 1200, loss is 4.826034097671509 and perplexity is 124.71536959678754
At time: 183.72471165657043 and batch: 1250, loss is 4.824029006958008 and perplexity is 124.46555450203759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.658413406706204 and perplexity of 105.46861354454575
Finished 6 epochs...
Completing Train Step...
At time: 186.66089391708374 and batch: 50, loss is 4.790367517471314 and perplexity is 120.34558964351582
At time: 187.82009983062744 and batch: 100, loss is 4.816527194976807 and perplexity is 123.53533085799286
At time: 188.97904992103577 and batch: 150, loss is 4.721046266555786 and perplexity is 112.2856719744319
At time: 190.13881659507751 and batch: 200, loss is 4.7649554252624515 and perplexity is 117.32588740061061
At time: 191.34557175636292 and batch: 250, loss is 4.793326454162598 and perplexity is 120.70221197560315
At time: 192.50648546218872 and batch: 300, loss is 4.804298686981201 and perplexity is 122.03387705688272
At time: 193.66619396209717 and batch: 350, loss is 4.806593580245972 and perplexity is 122.31425337322342
At time: 194.8255262374878 and batch: 400, loss is 4.800532188415527 and perplexity is 121.57510116572136
At time: 195.98636555671692 and batch: 450, loss is 4.74414400100708 and perplexity is 114.90940106048139
At time: 197.14640021324158 and batch: 500, loss is 4.755497131347656 and perplexity is 116.22141611454244
At time: 198.30683946609497 and batch: 550, loss is 4.754549961090088 and perplexity is 116.1113867623971
At time: 199.46692180633545 and batch: 600, loss is 4.775092658996582 and perplexity is 118.52129617632272
At time: 200.6268355846405 and batch: 650, loss is 4.776268758773804 and perplexity is 118.66077104845787
At time: 201.78763365745544 and batch: 700, loss is 4.774360551834106 and perplexity is 118.43455764131171
At time: 202.9486689567566 and batch: 750, loss is 4.768129940032959 and perplexity is 117.69893196758969
At time: 204.10993552207947 and batch: 800, loss is 4.784522180557251 and perplexity is 119.64418110767178
At time: 205.27152013778687 and batch: 850, loss is 4.822398529052735 and perplexity is 124.26278151881087
At time: 206.43348026275635 and batch: 900, loss is 4.799644546508789 and perplexity is 121.46723389194949
At time: 207.59447932243347 and batch: 950, loss is 4.779266681671142 and perplexity is 119.01704065857852
At time: 208.7564070224762 and batch: 1000, loss is 4.76732741355896 and perplexity is 117.60451335051175
At time: 209.916757106781 and batch: 1050, loss is 4.747273712158203 and perplexity is 115.26959765598686
At time: 211.0785529613495 and batch: 1100, loss is 4.732257213592529 and perplexity is 113.55158347044195
At time: 212.23991632461548 and batch: 1150, loss is 4.741064739227295 and perplexity is 114.5561091519485
At time: 213.40051436424255 and batch: 1200, loss is 4.756776876449585 and perplexity is 116.37024511383366
At time: 214.56039595603943 and batch: 1250, loss is 4.757861804962158 and perplexity is 116.49656702350276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.620261087904882 and perplexity of 101.52053445331987
Finished 7 epochs...
Completing Train Step...
At time: 217.4767210483551 and batch: 50, loss is 4.723802461624145 and perplexity is 112.59558007717989
At time: 218.63774514198303 and batch: 100, loss is 4.7484558582305905 and perplexity is 115.4059437327328
At time: 219.82555675506592 and batch: 150, loss is 4.6555210399627684 and perplexity is 105.16400037311723
At time: 220.98541045188904 and batch: 200, loss is 4.700502262115479 and perplexity is 110.00240862190677
At time: 222.33046650886536 and batch: 250, loss is 4.72927230834961 and perplexity is 113.21314810362824
At time: 223.4924259185791 and batch: 300, loss is 4.740366411209107 and perplexity is 114.47613933710404
At time: 224.65261006355286 and batch: 350, loss is 4.740304546356201 and perplexity is 114.46905750664324
At time: 225.8128378391266 and batch: 400, loss is 4.735162858963013 and perplexity is 113.88200391310659
At time: 226.972962141037 and batch: 450, loss is 4.677034130096436 and perplexity is 107.45091408188866
At time: 228.13373136520386 and batch: 500, loss is 4.691428356170654 and perplexity is 109.0087720125791
At time: 229.29381585121155 and batch: 550, loss is 4.690057649612426 and perplexity is 108.85945533193231
At time: 230.45339179039001 and batch: 600, loss is 4.71145450592041 and perplexity is 111.21380345700997
At time: 231.61330366134644 and batch: 650, loss is 4.714858770370483 and perplexity is 111.59304981566676
At time: 232.77353882789612 and batch: 700, loss is 4.710866842269898 and perplexity is 111.14846634728556
At time: 233.93364143371582 and batch: 750, loss is 4.706403551101684 and perplexity is 110.65348382634497
At time: 235.09344172477722 and batch: 800, loss is 4.725020227432251 and perplexity is 112.73277864564955
At time: 236.25325918197632 and batch: 850, loss is 4.761345739364624 and perplexity is 116.90314124884759
At time: 237.4128758907318 and batch: 900, loss is 4.739943151473999 and perplexity is 114.4276964493772
At time: 238.57243418693542 and batch: 950, loss is 4.719183301925659 and perplexity is 112.07668246954523
At time: 239.73396468162537 and batch: 1000, loss is 4.707481126785279 and perplexity is 110.77278559662047
At time: 240.89691185951233 and batch: 1050, loss is 4.689361801147461 and perplexity is 108.7837319960816
At time: 242.05705499649048 and batch: 1100, loss is 4.674309387207031 and perplexity is 107.1585364755756
At time: 243.21790719032288 and batch: 1150, loss is 4.680756530761719 and perplexity is 107.8516347947688
At time: 244.37840604782104 and batch: 1200, loss is 4.696566562652588 and perplexity is 109.57032303861652
At time: 245.53813886642456 and batch: 1250, loss is 4.699964094161987 and perplexity is 109.94322477763217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.58847023622833 and perplexity of 98.34387213943855
Finished 8 epochs...
Completing Train Step...
At time: 248.43481349945068 and batch: 50, loss is 4.666021108627319 and perplexity is 106.27404718326783
At time: 249.6301679611206 and batch: 100, loss is 4.689139204025269 and perplexity is 108.75951974528655
At time: 250.79372477531433 and batch: 150, loss is 4.598774843215942 and perplexity is 99.36250639066941
At time: 251.95648050308228 and batch: 200, loss is 4.644177694320678 and perplexity is 103.97782906220718
At time: 253.12032270431519 and batch: 250, loss is 4.673801651000977 and perplexity is 107.10414201700574
At time: 254.28408026695251 and batch: 300, loss is 4.684470977783203 and perplexity is 108.25298892125357
At time: 255.44757556915283 and batch: 350, loss is 4.682087678909301 and perplexity is 107.99529689519547
At time: 256.6105146408081 and batch: 400, loss is 4.676635341644287 and perplexity is 107.40807244112355
At time: 257.7726273536682 and batch: 450, loss is 4.6178453826904295 and perplexity is 101.27558674870936
At time: 258.93550992012024 and batch: 500, loss is 4.636495990753174 and perplexity is 103.1821621530194
At time: 260.09864020347595 and batch: 550, loss is 4.633906269073487 and perplexity is 102.9152947759997
At time: 261.2615923881531 and batch: 600, loss is 4.6559529304504395 and perplexity is 105.20942951402456
At time: 262.42541909217834 and batch: 650, loss is 4.6613006591796875 and perplexity is 105.77356808841014
At time: 263.58852887153625 and batch: 700, loss is 4.6548542976379395 and perplexity is 105.09390645290846
At time: 264.7514913082123 and batch: 750, loss is 4.651173105239868 and perplexity is 104.70774676352336
At time: 265.91596603393555 and batch: 800, loss is 4.67150806427002 and perplexity is 106.85877087560742
At time: 267.078652381897 and batch: 850, loss is 4.707595319747925 and perplexity is 110.78543579145614
At time: 268.24220395088196 and batch: 900, loss is 4.685873250961304 and perplexity is 108.4048956665523
At time: 269.4056420326233 and batch: 950, loss is 4.665996198654175 and perplexity is 106.27139993257812
At time: 270.56857800483704 and batch: 1000, loss is 4.654307250976562 and perplexity is 105.03643090458908
At time: 271.7320861816406 and batch: 1050, loss is 4.638530006408692 and perplexity is 103.39224987463466
At time: 272.895391702652 and batch: 1100, loss is 4.6231889820098875 and perplexity is 101.81821139831447
At time: 274.05911111831665 and batch: 1150, loss is 4.627450494766236 and perplexity is 102.25303685407675
At time: 275.2242479324341 and batch: 1200, loss is 4.64334246635437 and perplexity is 103.89102012916922
At time: 276.3888773918152 and batch: 1250, loss is 4.649603710174561 and perplexity is 104.54354782265882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.563758126140511 and perplexity of 95.94337043779048
Finished 9 epochs...
Completing Train Step...
At time: 279.3034520149231 and batch: 50, loss is 4.614410896301269 and perplexity is 100.92835374929305
At time: 280.4715893268585 and batch: 100, loss is 4.636318769454956 and perplexity is 103.16387769653514
At time: 281.6317148208618 and batch: 150, loss is 4.548640699386596 and perplexity is 94.50386181071768
At time: 282.7921631336212 and batch: 200, loss is 4.59431643486023 and perplexity is 98.9204938303505
At time: 283.96039366722107 and batch: 250, loss is 4.624551582336426 and perplexity is 101.95704349126038
At time: 285.1205458641052 and batch: 300, loss is 4.635091257095337 and perplexity is 103.03732045277813
At time: 286.2804448604584 and batch: 350, loss is 4.630069665908813 and perplexity is 102.52120609470938
At time: 287.43982911109924 and batch: 400, loss is 4.62520697593689 and perplexity is 102.02388738722495
At time: 288.5994691848755 and batch: 450, loss is 4.565965156555176 and perplexity is 96.15535421570733
At time: 289.75923776626587 and batch: 500, loss is 4.587829828262329 and perplexity is 98.28091210251942
At time: 290.91844296455383 and batch: 550, loss is 4.58334490776062 and perplexity is 97.84111698516178
At time: 292.078275680542 and batch: 600, loss is 4.606251010894775 and perplexity is 100.10814092084136
At time: 293.23827290534973 and batch: 650, loss is 4.614084749221802 and perplexity is 100.89544162887002
At time: 294.39792919158936 and batch: 700, loss is 4.604923686981201 and perplexity is 99.97535313714938
At time: 295.5579664707184 and batch: 750, loss is 4.601361637115478 and perplexity is 99.61986944512093
At time: 296.7184793949127 and batch: 800, loss is 4.6243953514099125 and perplexity is 101.94111589211539
At time: 297.8774468898773 and batch: 850, loss is 4.659264688491821 and perplexity is 105.55843528059009
At time: 299.03690910339355 and batch: 900, loss is 4.637873182296753 and perplexity is 103.32436164968159
At time: 300.1966714859009 and batch: 950, loss is 4.618943557739258 and perplexity is 101.3868661620864
At time: 301.4026906490326 and batch: 1000, loss is 4.606967916488648 and perplexity is 100.1799347386796
At time: 302.56282210350037 and batch: 1050, loss is 4.593116121292114 and perplexity is 98.80182945092076
At time: 303.7237367630005 and batch: 1100, loss is 4.57712007522583 and perplexity is 97.23396408998285
At time: 304.88329339027405 and batch: 1150, loss is 4.579472379684448 and perplexity is 97.46295720240923
At time: 306.0489823818207 and batch: 1200, loss is 4.595691699981689 and perplexity is 99.05662932505504
At time: 307.2105059623718 and batch: 1250, loss is 4.604692850112915 and perplexity is 99.9522778031468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.541948527315237 and perplexity of 93.87353717895179
Finished 10 epochs...
Completing Train Step...
At time: 310.091659784317 and batch: 50, loss is 4.5695248794555665 and perplexity is 96.49825057803302
At time: 311.27990102767944 and batch: 100, loss is 4.589456529617309 and perplexity is 98.44091589931887
At time: 312.4431164264679 and batch: 150, loss is 4.504217042922973 and perplexity is 90.39753894094643
At time: 313.6066081523895 and batch: 200, loss is 4.5493786430358885 and perplexity is 94.57362607325581
At time: 314.76926136016846 and batch: 250, loss is 4.579811601638794 and perplexity is 97.49602438546789
At time: 315.93424248695374 and batch: 300, loss is 4.591590204238892 and perplexity is 98.65118102220632
At time: 317.11005902290344 and batch: 350, loss is 4.58356201171875 and perplexity is 97.8623609849219
At time: 318.2774648666382 and batch: 400, loss is 4.579358167648316 and perplexity is 97.4518263952678
At time: 319.4448103904724 and batch: 450, loss is 4.519522972106934 and perplexity is 91.79180028353632
At time: 320.60803294181824 and batch: 500, loss is 4.544250230789185 and perplexity is 94.08985507991896
At time: 321.7711672782898 and batch: 550, loss is 4.538207359313965 and perplexity is 93.52299662989121
At time: 322.9470753669739 and batch: 600, loss is 4.561909446716308 and perplexity is 95.76616575077097
At time: 324.11674189567566 and batch: 650, loss is 4.571875391006469 and perplexity is 96.72533761145333
At time: 325.2871401309967 and batch: 700, loss is 4.560412750244141 and perplexity is 95.6229400777552
At time: 326.45123505592346 and batch: 750, loss is 4.557323303222656 and perplexity is 95.32797394607798
At time: 327.614718914032 and batch: 800, loss is 4.58183666229248 and perplexity is 97.6936597926454
At time: 328.77757573127747 and batch: 850, loss is 4.615622787475586 and perplexity is 101.05074207612763
At time: 329.9683253765106 and batch: 900, loss is 4.594128484725952 and perplexity is 98.90190345733858
At time: 331.1324906349182 and batch: 950, loss is 4.576925296783447 and perplexity is 97.21502685425318
At time: 332.29624605178833 and batch: 1000, loss is 4.564266080856323 and perplexity is 95.99211770490922
At time: 333.45915627479553 and batch: 1050, loss is 4.551816329956055 and perplexity is 94.80444818624866
At time: 334.6224956512451 and batch: 1100, loss is 4.535085792541504 and perplexity is 93.2315135298189
At time: 335.7855432033539 and batch: 1150, loss is 4.5356903076171875 and perplexity is 93.28789042390079
At time: 336.94850397109985 and batch: 1200, loss is 4.55250412940979 and perplexity is 94.86967706354444
At time: 338.1111011505127 and batch: 1250, loss is 4.564601831436157 and perplexity is 96.0243525252151
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.522901103444343 and perplexity of 92.10240938437666
Finished 11 epochs...
Completing Train Step...
At time: 341.0091586112976 and batch: 50, loss is 4.529242420196534 and perplexity is 92.68831568124546
At time: 342.16857385635376 and batch: 100, loss is 4.547713441848755 and perplexity is 94.41627300746336
At time: 343.32912826538086 and batch: 150, loss is 4.462480812072754 and perplexity is 86.70233471385335
At time: 344.49821305274963 and batch: 200, loss is 4.508365869522095 and perplexity is 90.77336172770802
At time: 345.65937900543213 and batch: 250, loss is 4.539452028274536 and perplexity is 93.63947427391282
At time: 346.8205716609955 and batch: 300, loss is 4.551328525543213 and perplexity is 94.75821343574084
At time: 347.9797410964966 and batch: 350, loss is 4.541146984100342 and perplexity is 93.7983236296497
At time: 349.13889050483704 and batch: 400, loss is 4.539013919830322 and perplexity is 93.59845901474334
At time: 350.3003103733063 and batch: 450, loss is 4.477957067489624 and perplexity is 88.05459919591213
At time: 351.4744050502777 and batch: 500, loss is 4.5049426460266115 and perplexity is 90.46315547866602
At time: 352.64713978767395 and batch: 550, loss is 4.497940254211426 and perplexity is 89.83190971360222
At time: 353.8080987930298 and batch: 600, loss is 4.521693315505981 and perplexity is 91.99123635546435
At time: 354.96737360954285 and batch: 650, loss is 4.533837814331054 and perplexity is 93.11523520390644
At time: 356.12683963775635 and batch: 700, loss is 4.5205211353302 and perplexity is 91.88346922544895
At time: 357.30393266677856 and batch: 750, loss is 4.516635007858277 and perplexity is 91.52709126506858
At time: 358.47314405441284 and batch: 800, loss is 4.543542966842652 and perplexity is 94.02333224506076
At time: 359.68191504478455 and batch: 850, loss is 4.576124248504638 and perplexity is 97.13718410637937
At time: 360.84147810935974 and batch: 900, loss is 4.554666938781739 and perplexity is 95.07508413831424
At time: 362.00040769577026 and batch: 950, loss is 4.5385050964355464 and perplexity is 93.55084604340583
At time: 363.16647028923035 and batch: 1000, loss is 4.525343265533447 and perplexity is 92.32761327716574
At time: 364.3263931274414 and batch: 1050, loss is 4.514581270217896 and perplexity is 91.33931152378636
At time: 365.4977602958679 and batch: 1100, loss is 4.496553115844726 and perplexity is 89.70738681029968
At time: 366.6659083366394 and batch: 1150, loss is 4.495716581344604 and perplexity is 89.63237486573124
At time: 367.8314142227173 and batch: 1200, loss is 4.513275785446167 and perplexity is 91.22014724403236
At time: 368.9897723197937 and batch: 1250, loss is 4.527843790054321 and perplexity is 92.55876962373101
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.508065550866788 and perplexity of 90.74610488685003
Finished 12 epochs...
Completing Train Step...
At time: 371.8732430934906 and batch: 50, loss is 4.491062364578247 and perplexity is 89.21617565579268
At time: 373.0329704284668 and batch: 100, loss is 4.508963680267334 and perplexity is 90.82764324215002
At time: 374.1961467266083 and batch: 150, loss is 4.425084972381592 and perplexity is 83.51990380566684
At time: 375.3584268093109 and batch: 200, loss is 4.471197710037232 and perplexity is 87.4614137194536
At time: 376.52131390571594 and batch: 250, loss is 4.502250890731812 and perplexity is 90.21997823456856
At time: 377.684100151062 and batch: 300, loss is 4.514545259475708 and perplexity is 91.33602238660993
At time: 378.8476073741913 and batch: 350, loss is 4.5022280979156495 and perplexity is 90.2179218906255
At time: 380.01866245269775 and batch: 400, loss is 4.501084995269776 and perplexity is 90.11485246609628
At time: 381.1886250972748 and batch: 450, loss is 4.43903470993042 and perplexity is 84.69314874778176
At time: 382.3532028198242 and batch: 500, loss is 4.468957147598267 and perplexity is 87.26567033054536
At time: 383.5145161151886 and batch: 550, loss is 4.461230955123901 and perplexity is 86.59403689082086
At time: 384.677757024765 and batch: 600, loss is 4.485578756332398 and perplexity is 88.72828801421352
At time: 385.83999967575073 and batch: 650, loss is 4.499138126373291 and perplexity is 89.93958133303825
At time: 387.00169825553894 and batch: 700, loss is 4.484833183288575 and perplexity is 88.66215924942105
At time: 388.1648998260498 and batch: 750, loss is 4.480027952194214 and perplexity is 88.23713906282882
At time: 389.37437653541565 and batch: 800, loss is 4.510897741317749 and perplexity is 91.00347943339914
At time: 390.5368266105652 and batch: 850, loss is 4.540152797698974 and perplexity is 93.70511695190896
At time: 391.705260515213 and batch: 900, loss is 4.51898886680603 and perplexity is 91.74278688674804
At time: 392.868643283844 and batch: 950, loss is 4.503583192825317 and perplexity is 90.34025860755109
At time: 394.0314040184021 and batch: 1000, loss is 4.4898716259002684 and perplexity is 89.11000572760247
At time: 395.1953001022339 and batch: 1050, loss is 4.4804377937316895 and perplexity is 88.27330971917718
At time: 396.3585112094879 and batch: 1100, loss is 4.461298265457153 and perplexity is 86.59986576047099
At time: 397.5207748413086 and batch: 1150, loss is 4.458856582641602 and perplexity is 86.38867429262717
At time: 398.68340373039246 and batch: 1200, loss is 4.477325057983398 and perplexity is 87.9989654345383
At time: 399.84639286994934 and batch: 1250, loss is 4.494097900390625 and perplexity is 89.48740600849331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.495235248203695 and perplexity of 89.58924221462992
Finished 13 epochs...
Completing Train Step...
At time: 402.754923582077 and batch: 50, loss is 4.455849237442017 and perplexity is 86.12926399136843
At time: 403.94677209854126 and batch: 100, loss is 4.473572235107422 and perplexity is 87.66933980415818
At time: 405.1120946407318 and batch: 150, loss is 4.390686721801758 and perplexity is 80.69581553312696
At time: 406.27277159690857 and batch: 200, loss is 4.437367839813232 and perplexity is 84.55209386175035
At time: 407.4411246776581 and batch: 250, loss is 4.467942008972168 and perplexity is 87.17712852656662
At time: 408.60206484794617 and batch: 300, loss is 4.480925045013428 and perplexity is 88.3163314828347
At time: 409.7626359462738 and batch: 350, loss is 4.466568965911865 and perplexity is 87.05751271284966
At time: 410.9227292537689 and batch: 400, loss is 4.466156234741211 and perplexity is 87.02158877768736
At time: 412.083039522171 and batch: 450, loss is 4.403729448318481 and perplexity is 81.75520261627814
At time: 413.2492446899414 and batch: 500, loss is 4.435655698776245 and perplexity is 84.40745261053118
At time: 414.4151711463928 and batch: 550, loss is 4.427839879989624 and perplexity is 83.75031065314867
At time: 415.5751609802246 and batch: 600, loss is 4.452643022537232 and perplexity is 85.8535572850811
At time: 416.7448163032532 and batch: 650, loss is 4.4678006267547605 and perplexity is 87.16480410207609
At time: 417.9064610004425 and batch: 700, loss is 4.451840753555298 and perplexity is 85.78470726089486
At time: 419.1136374473572 and batch: 750, loss is 4.4457406806945805 and perplexity is 85.26300711845109
At time: 420.2727162837982 and batch: 800, loss is 4.478123025894165 and perplexity is 88.06921380938265
At time: 421.4327657222748 and batch: 850, loss is 4.507070589065552 and perplexity is 90.65586088099069
At time: 422.59321117401123 and batch: 900, loss is 4.485919513702393 and perplexity is 88.75852798423509
At time: 423.762193441391 and batch: 950, loss is 4.471621189117432 and perplexity is 87.49845964202147
At time: 424.92210578918457 and batch: 1000, loss is 4.4573914909362795 and perplexity is 86.2621996335973
At time: 426.0822925567627 and batch: 1050, loss is 4.448988170623779 and perplexity is 85.54034796245963
At time: 427.242538690567 and batch: 1100, loss is 4.4289216041564945 and perplexity is 83.8409544050949
At time: 428.40281558036804 and batch: 1150, loss is 4.425216493606567 and perplexity is 83.53088916811348
At time: 429.56299114227295 and batch: 1200, loss is 4.445319242477417 and perplexity is 85.22708159946617
At time: 430.7239263057709 and batch: 1250, loss is 4.463285598754883 and perplexity is 86.77213968342784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.48318503720917 and perplexity of 88.5161514134349
Finished 14 epochs...
Completing Train Step...
At time: 433.6239538192749 and batch: 50, loss is 4.42406982421875 and perplexity is 83.43516174890684
At time: 434.7842240333557 and batch: 100, loss is 4.441849565505981 and perplexity is 84.93188357414199
At time: 435.9448871612549 and batch: 150, loss is 4.359069843292236 and perplexity is 78.18437686778005
At time: 437.1054894924164 and batch: 200, loss is 4.406553964614869 and perplexity is 81.98644794274128
At time: 438.26680183410645 and batch: 250, loss is 4.436281080245972 and perplexity is 84.46025597670302
At time: 439.4270055294037 and batch: 300, loss is 4.449073066711426 and perplexity is 85.54761031160575
At time: 440.58745074272156 and batch: 350, loss is 4.433132839202881 and perplexity is 84.19477285403495
At time: 441.7470977306366 and batch: 400, loss is 4.433932476043701 and perplexity is 84.26212502126175
At time: 442.90774297714233 and batch: 450, loss is 4.371147556304932 and perplexity is 79.1343907831053
At time: 444.06845784187317 and batch: 500, loss is 4.405179557800293 and perplexity is 81.87384261047981
At time: 445.228618144989 and batch: 550, loss is 4.396971025466919 and perplexity is 81.2045293242199
At time: 446.3884332180023 and batch: 600, loss is 4.42211163520813 and perplexity is 83.27193979394629
At time: 447.5489604473114 and batch: 650, loss is 4.438353519439698 and perplexity is 84.63547622544246
At time: 448.75758147239685 and batch: 700, loss is 4.4207111263275145 and perplexity is 83.15539833048501
At time: 449.9185287952423 and batch: 750, loss is 4.414668502807618 and perplexity is 82.6544366504547
At time: 451.079407453537 and batch: 800, loss is 4.448669290542602 and perplexity is 85.51307519795925
At time: 452.2385485172272 and batch: 850, loss is 4.477167463302612 and perplexity is 87.98509835838856
At time: 453.3978626728058 and batch: 900, loss is 4.455612030029297 and perplexity is 86.10883591443897
At time: 454.5575394630432 and batch: 950, loss is 4.441884326934814 and perplexity is 84.93483597908313
At time: 455.7178740501404 and batch: 1000, loss is 4.4272018241882325 and perplexity is 83.6968903259553
At time: 456.8779695034027 and batch: 1050, loss is 4.420043745040894 and perplexity is 83.09992048825374
At time: 458.03863763809204 and batch: 1100, loss is 4.399007272720337 and perplexity is 81.37005028763336
At time: 459.19899439811707 and batch: 1150, loss is 4.394079875946045 and perplexity is 80.97009394533686
At time: 460.3582937717438 and batch: 1200, loss is 4.415415477752686 and perplexity is 82.71620050888681
At time: 461.5179343223572 and batch: 1250, loss is 4.434945402145385 and perplexity is 84.34751956896098
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.472333253735173 and perplexity of 87.56078638694085
Finished 15 epochs...
Completing Train Step...
At time: 464.3999240398407 and batch: 50, loss is 4.393760442733765 and perplexity is 80.94423353868552
At time: 465.595577955246 and batch: 100, loss is 4.4121096801757815 and perplexity is 82.44320896966927
At time: 466.7597243785858 and batch: 150, loss is 4.3301143455505375 and perplexity is 75.95297095694634
At time: 467.92344188690186 and batch: 200, loss is 4.377679357528686 and perplexity is 79.65297268687162
At time: 469.0870912075043 and batch: 250, loss is 4.406986112594605 and perplexity is 82.02188587724987
At time: 470.25331115722656 and batch: 300, loss is 4.419837322235107 and perplexity is 83.0827685398435
At time: 471.4218626022339 and batch: 350, loss is 4.401940641403198 and perplexity is 81.60908906790216
At time: 472.5860540866852 and batch: 400, loss is 4.404516096115112 and perplexity is 81.81954046857648
At time: 473.7491931915283 and batch: 450, loss is 4.340570154190064 and perplexity is 76.75128693351107
At time: 474.91295528411865 and batch: 500, loss is 4.3770680809021 and perplexity is 79.60429756492782
At time: 476.07624411582947 and batch: 550, loss is 4.368247594833374 and perplexity is 78.9052365286083
At time: 477.2747576236725 and batch: 600, loss is 4.3940283298492435 and perplexity is 80.96592036060323
At time: 478.4559907913208 and batch: 650, loss is 4.411023187637329 and perplexity is 82.35368368136331
At time: 479.6222381591797 and batch: 700, loss is 4.392022275924683 and perplexity is 80.80366116307252
At time: 480.7859389781952 and batch: 750, loss is 4.385876760482788 and perplexity is 80.30860376515827
At time: 481.9510409832001 and batch: 800, loss is 4.420774230957031 and perplexity is 83.16064598666291
At time: 483.1168758869171 and batch: 850, loss is 4.44912787437439 and perplexity is 85.55229910468879
At time: 484.28089904785156 and batch: 900, loss is 4.427512998580933 and perplexity is 83.72293870755784
At time: 485.44791316986084 and batch: 950, loss is 4.414187841415405 and perplexity is 82.61471740038147
At time: 486.61700320243835 and batch: 1000, loss is 4.399480743408203 and perplexity is 81.4085857432992
At time: 487.78100299835205 and batch: 1050, loss is 4.393254909515381 and perplexity is 80.90332388126176
At time: 488.9445390701294 and batch: 1100, loss is 4.371205539703369 and perplexity is 79.13897939704661
At time: 490.1084053516388 and batch: 1150, loss is 4.365342082977295 and perplexity is 78.67630916500268
At time: 491.27247500419617 and batch: 1200, loss is 4.387895030975342 and perplexity is 80.47085192569773
At time: 492.43571758270264 and batch: 1250, loss is 4.408431348800659 and perplexity is 82.14051257756162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.462716875285127 and perplexity of 86.72280436147649
Finished 16 epochs...
Completing Train Step...
At time: 495.3552014827728 and batch: 50, loss is 4.36593279838562 and perplexity is 78.72279820264012
At time: 496.5155930519104 and batch: 100, loss is 4.3850243949890135 and perplexity is 80.2401806473505
At time: 497.67616605758667 and batch: 150, loss is 4.303100461959839 and perplexity is 73.92865170584895
At time: 498.83686351776123 and batch: 200, loss is 4.3509237098693845 and perplexity is 77.55006361016827
At time: 499.9974932670593 and batch: 250, loss is 4.379877071380616 and perplexity is 79.82821962907299
At time: 501.15798568725586 and batch: 300, loss is 4.392536973953247 and perplexity is 80.84526135302339
At time: 502.31792092323303 and batch: 350, loss is 4.373467864990235 and perplexity is 79.31822018534929
At time: 503.4779999256134 and batch: 400, loss is 4.376577033996582 and perplexity is 79.56521771674716
At time: 504.6381673812866 and batch: 450, loss is 4.312592306137085 and perplexity is 74.63371181488925
At time: 505.79854941368103 and batch: 500, loss is 4.35071572303772 and perplexity is 77.53393589537687
At time: 506.98659896850586 and batch: 550, loss is 4.341495676040649 and perplexity is 76.82235480899139
At time: 508.14970564842224 and batch: 600, loss is 4.368067188262939 and perplexity is 78.89100278946549
At time: 509.3110020160675 and batch: 650, loss is 4.385702676773072 and perplexity is 80.2946245623037
At time: 510.4708921909332 and batch: 700, loss is 4.365116901397705 and perplexity is 78.6585947039886
At time: 511.6312041282654 and batch: 750, loss is 4.359607610702515 and perplexity is 78.22643318489733
At time: 512.7909178733826 and batch: 800, loss is 4.394915771484375 and perplexity is 81.03780478125417
At time: 513.9515454769135 and batch: 850, loss is 4.4230843353271485 and perplexity is 83.35297782615709
At time: 515.1116259098053 and batch: 900, loss is 4.401144495010376 and perplexity is 81.5441421430798
At time: 516.2725224494934 and batch: 950, loss is 4.388567743301391 and perplexity is 80.52500387197456
At time: 517.4320523738861 and batch: 1000, loss is 4.3740473556518555 and perplexity is 79.36419767371902
At time: 518.5925884246826 and batch: 1050, loss is 4.368148260116577 and perplexity is 78.89739888856525
At time: 519.7531878948212 and batch: 1100, loss is 4.345306949615479 and perplexity is 77.11570448266583
At time: 520.9137609004974 and batch: 1150, loss is 4.339013156890869 and perplexity is 76.63187837057376
At time: 522.0746257305145 and batch: 1200, loss is 4.362316198348999 and perplexity is 78.43860354674632
At time: 523.2399942874908 and batch: 1250, loss is 4.383640737533569 and perplexity is 80.12923249798953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.454368869753649 and perplexity of 86.00185534102519
Finished 17 epochs...
Completing Train Step...
At time: 526.1395840644836 and batch: 50, loss is 4.339836664199829 and perplexity is 76.6950112741472
At time: 527.3372690677643 and batch: 100, loss is 4.359660415649414 and perplexity is 78.23056403661155
At time: 528.5014927387238 and batch: 150, loss is 4.27789270401001 and perplexity is 72.08836830410335
At time: 529.664867401123 and batch: 200, loss is 4.325797004699707 and perplexity is 75.62576293521892
At time: 530.8280084133148 and batch: 250, loss is 4.354566421508789 and perplexity is 77.83307127422968
At time: 531.9908809661865 and batch: 300, loss is 4.367183141708374 and perplexity is 78.82129028935125
At time: 533.1543090343475 and batch: 350, loss is 4.346742095947266 and perplexity is 77.22645625656925
At time: 534.3177003860474 and batch: 400, loss is 4.350885305404663 and perplexity is 77.5470853986749
At time: 535.4816951751709 and batch: 450, loss is 4.287176971435547 and perplexity is 72.76077255418349
At time: 536.6798894405365 and batch: 500, loss is 4.326552534103394 and perplexity is 75.68292201275564
At time: 537.8449492454529 and batch: 550, loss is 4.316551036834717 and perplexity is 74.9297521663251
At time: 539.013623714447 and batch: 600, loss is 4.343777008056641 and perplexity is 76.99781216871014
At time: 540.1777603626251 and batch: 650, loss is 4.362085485458374 and perplexity is 78.42050883720711
At time: 541.3414781093597 and batch: 700, loss is 4.340195808410645 and perplexity is 76.72256079027322
At time: 542.5048177242279 and batch: 750, loss is 4.334901704788208 and perplexity is 76.31745688048044
At time: 543.6686799526215 and batch: 800, loss is 4.37098916053772 and perplexity is 79.121857223222
At time: 544.8314499855042 and batch: 850, loss is 4.39853102684021 and perplexity is 81.33130736272962
At time: 545.9942100048065 and batch: 900, loss is 4.376684942245483 and perplexity is 79.57380392331739
At time: 547.1568763256073 and batch: 950, loss is 4.36495361328125 and perplexity is 78.64575173879668
At time: 548.3207383155823 and batch: 1000, loss is 4.349811391830444 and perplexity is 77.4638512321742
At time: 549.4830396175385 and batch: 1050, loss is 4.344851274490356 and perplexity is 77.08057277930634
At time: 550.6461505889893 and batch: 1100, loss is 4.3209010601043705 and perplexity is 75.2564082976229
At time: 551.8105719089508 and batch: 1150, loss is 4.314354734420776 and perplexity is 74.76536435951508
At time: 552.9764831066132 and batch: 1200, loss is 4.338561906814575 and perplexity is 76.59730603058468
At time: 554.14040184021 and batch: 1250, loss is 4.360368480682373 and perplexity is 78.28597597882879
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4469439652714415 and perplexity of 85.36566453054743
Finished 18 epochs...
Completing Train Step...
At time: 557.0298683643341 and batch: 50, loss is 4.315917434692383 and perplexity is 74.88229155198918
At time: 558.2171671390533 and batch: 100, loss is 4.3358284282684325 and perplexity is 76.38821484119042
At time: 559.3766548633575 and batch: 150, loss is 4.254229369163514 and perplexity is 70.40254190825478
At time: 560.536479473114 and batch: 200, loss is 4.302178630828857 and perplexity is 73.8605333748578
At time: 561.6964492797852 and batch: 250, loss is 4.330987911224366 and perplexity is 76.01934985414138
At time: 562.856541633606 and batch: 300, loss is 4.3432349109649655 and perplexity is 76.95608319026894
At time: 564.0160677433014 and batch: 350, loss is 4.321704025268555 and perplexity is 75.31686083926195
At time: 565.20241522789 and batch: 400, loss is 4.326976051330567 and perplexity is 75.71498182249246
At time: 566.3622441291809 and batch: 450, loss is 4.262567863464356 and perplexity is 70.9920474814392
At time: 567.5224144458771 and batch: 500, loss is 4.30350136756897 and perplexity is 73.95829605888996
At time: 568.6840734481812 and batch: 550, loss is 4.292935309410095 and perplexity is 73.180962310084
At time: 569.8436958789825 and batch: 600, loss is 4.321385679244995 and perplexity is 75.29288783216495
At time: 571.0040802955627 and batch: 650, loss is 4.3404385852813725 and perplexity is 76.74118951471632
At time: 572.163361787796 and batch: 700, loss is 4.316933031082153 and perplexity is 74.95838036817055
At time: 573.3239676952362 and batch: 750, loss is 4.3117398929595945 and perplexity is 74.57012016248497
At time: 574.4845411777496 and batch: 800, loss is 4.3480987548828125 and perplexity is 77.33129731921234
At time: 575.6449394226074 and batch: 850, loss is 4.375913763046265 and perplexity is 79.51246191680886
At time: 576.8058114051819 and batch: 900, loss is 4.353639221191406 and perplexity is 77.76093787206577
At time: 577.966694355011 and batch: 950, loss is 4.3421554851531985 and perplexity is 76.87305962465204
At time: 579.1351344585419 and batch: 1000, loss is 4.3273416519165036 and perplexity is 75.74266832500335
At time: 580.2945101261139 and batch: 1050, loss is 4.322622127532959 and perplexity is 75.38604117219707
At time: 581.4535591602325 and batch: 1100, loss is 4.298029165267945 and perplexity is 73.55468662442775
At time: 582.6132607460022 and batch: 1150, loss is 4.2913611888885494 and perplexity is 73.06585727389286
At time: 583.7754056453705 and batch: 1200, loss is 4.31587857246399 and perplexity is 74.87938151581787
At time: 584.9370062351227 and batch: 1250, loss is 4.33899845123291 and perplexity is 76.63075145666772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.441295317489735 and perplexity of 84.88482328889457
Finished 19 epochs...
Completing Train Step...
At time: 587.8466956615448 and batch: 50, loss is 4.294013900756836 and perplexity is 73.25993724596272
At time: 589.0082957744598 and batch: 100, loss is 4.313653135299683 and perplexity is 74.71292744259058
At time: 590.1689279079437 and batch: 150, loss is 4.232011089324951 and perplexity is 68.85556772185878
At time: 591.3296928405762 and batch: 200, loss is 4.279889783859253 and perplexity is 72.23247838357966
At time: 592.4901413917542 and batch: 250, loss is 4.308446903228759 and perplexity is 74.3249653901703
At time: 593.6509561538696 and batch: 300, loss is 4.320909061431885 and perplexity is 75.25701045120225
At time: 594.8465509414673 and batch: 350, loss is 4.298382234573364 and perplexity is 73.58066111167184
At time: 596.0074486732483 and batch: 400, loss is 4.304608583450317 and perplexity is 74.04022920931611
At time: 597.1686413288116 and batch: 450, loss is 4.239606704711914 and perplexity is 69.3805594250115
At time: 598.3306527137756 and batch: 500, loss is 4.281714286804199 and perplexity is 72.36438705039522
At time: 599.4919154644012 and batch: 550, loss is 4.270965332984924 and perplexity is 71.59071114829743
At time: 600.6527171134949 and batch: 600, loss is 4.299784812927246 and perplexity is 73.68393616295666
At time: 601.8136305809021 and batch: 650, loss is 4.319099254608155 and perplexity is 75.12093297426964
At time: 602.973876953125 and batch: 700, loss is 4.294483776092529 and perplexity is 73.29436837210405
At time: 604.1334707736969 and batch: 750, loss is 4.290017833709717 and perplexity is 72.96776977405435
At time: 605.2931749820709 and batch: 800, loss is 4.326725730895996 and perplexity is 75.6960311873037
At time: 606.4531781673431 and batch: 850, loss is 4.354572553634643 and perplexity is 77.83354855788173
At time: 607.6127061843872 and batch: 900, loss is 4.331971836090088 and perplexity is 76.0941839922863
At time: 608.7712721824646 and batch: 950, loss is 4.320870485305786 and perplexity is 75.25410738327218
At time: 609.9307837486267 and batch: 1000, loss is 4.30620846748352 and perplexity is 74.15877979815275
At time: 611.090252161026 and batch: 1050, loss is 4.301792678833007 and perplexity is 73.83203225497309
At time: 612.2499308586121 and batch: 1100, loss is 4.276555404663086 and perplexity is 71.99202900784599
At time: 613.4242904186249 and batch: 1150, loss is 4.269599466323853 and perplexity is 71.49299453182351
At time: 614.5961999893188 and batch: 1200, loss is 4.294985370635986 and perplexity is 73.33114164921219
At time: 615.7722454071045 and batch: 1250, loss is 4.318578777313232 and perplexity is 75.08184440752129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.437106612825046 and perplexity of 84.53000945757276
Finished 20 epochs...
Completing Train Step...
At time: 618.6865985393524 and batch: 50, loss is 4.272784423828125 and perplexity is 71.72105967736876
At time: 619.8711121082306 and batch: 100, loss is 4.292721223831177 and perplexity is 73.16529699832058
At time: 621.0324368476868 and batch: 150, loss is 4.211055402755737 and perplexity is 67.42766561443193
At time: 622.194441318512 and batch: 200, loss is 4.258870725631714 and perplexity is 70.73006468854283
At time: 623.3548765182495 and batch: 250, loss is 4.287650270462036 and perplexity is 72.79521830792784
At time: 624.5422406196594 and batch: 300, loss is 4.299805374145508 and perplexity is 73.68545121002602
At time: 625.7025926113129 and batch: 350, loss is 4.276351823806762 and perplexity is 71.97737430069161
At time: 626.8719987869263 and batch: 400, loss is 4.283841857910156 and perplexity is 72.51851132642442
At time: 628.0316364765167 and batch: 450, loss is 4.217966423034668 and perplexity is 67.89527354162412
At time: 629.192581653595 and batch: 500, loss is 4.261339731216431 and perplexity is 70.90491337563574
At time: 630.3537132740021 and batch: 550, loss is 4.250099935531616 and perplexity is 70.11241871842681
At time: 631.52166223526 and batch: 600, loss is 4.279568185806275 and perplexity is 72.20925229410247
At time: 632.6823749542236 and batch: 650, loss is 4.29883038520813 and perplexity is 73.61364372169231
At time: 633.8429255485535 and batch: 700, loss is 4.274269485473633 and perplexity is 71.82764899853774
At time: 635.0024361610413 and batch: 750, loss is 4.269437127113342 and perplexity is 71.48138935754706
At time: 636.1624987125397 and batch: 800, loss is 4.306610960960388 and perplexity is 74.18863423097817
At time: 637.3227696418762 and batch: 850, loss is 4.334512386322022 and perplexity is 76.28775086814929
At time: 638.489853143692 and batch: 900, loss is 4.311431770324707 and perplexity is 74.54714696004021
At time: 639.6525630950928 and batch: 950, loss is 4.300583276748657 and perplexity is 73.74279361484791
At time: 640.8213534355164 and batch: 1000, loss is 4.286214160919189 and perplexity is 72.69075143113601
At time: 641.9815304279327 and batch: 1050, loss is 4.281846904754639 and perplexity is 72.37398450347389
At time: 643.1419095993042 and batch: 1100, loss is 4.256294794082642 and perplexity is 70.5481033440436
At time: 644.3033876419067 and batch: 1150, loss is 4.249194526672364 and perplexity is 70.04896704256487
At time: 645.4634244441986 and batch: 1200, loss is 4.274885959625244 and perplexity is 71.87194253902497
At time: 646.6233994960785 and batch: 1250, loss is 4.298601589202881 and perplexity is 73.59680314067924
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4326087227703015 and perplexity of 84.15065655209413
Finished 21 epochs...
Completing Train Step...
At time: 649.514398097992 and batch: 50, loss is 4.2528129577636715 and perplexity is 70.30289353352477
At time: 650.6745026111603 and batch: 100, loss is 4.272725429534912 and perplexity is 71.7168286689485
At time: 651.8348741531372 and batch: 150, loss is 4.190957441329956 and perplexity is 66.08603418019946
At time: 652.9947640895844 and batch: 200, loss is 4.238881568908692 and perplexity is 69.33026733382653
At time: 654.1898543834686 and batch: 250, loss is 4.267771034240723 and perplexity is 71.36239388049792
At time: 655.3512616157532 and batch: 300, loss is 4.2797191619873045 and perplexity is 72.22015499425216
At time: 656.5125160217285 and batch: 350, loss is 4.255380382537842 and perplexity is 70.48362282923858
At time: 657.6716027259827 and batch: 400, loss is 4.264323387145996 and perplexity is 71.11678515993994
At time: 658.8319730758667 and batch: 450, loss is 4.197295479774475 and perplexity is 66.50622017638089
At time: 659.9913449287415 and batch: 500, loss is 4.2423828125 and perplexity is 69.57343493410383
At time: 661.1510760784149 and batch: 550, loss is 4.230297451019287 and perplexity is 68.7376752248516
At time: 662.3110239505768 and batch: 600, loss is 4.260342235565186 and perplexity is 70.83422129627404
At time: 663.470386505127 and batch: 650, loss is 4.283264818191529 and perplexity is 72.47667733615114
At time: 664.6299073696136 and batch: 700, loss is 4.25493556022644 and perplexity is 70.45227711336945
At time: 665.7901463508606 and batch: 750, loss is 4.249927468299866 and perplexity is 70.10032766634424
At time: 666.9492835998535 and batch: 800, loss is 4.287669081687927 and perplexity is 72.79658768810302
At time: 668.1085443496704 and batch: 850, loss is 4.3163760375976565 and perplexity is 74.91664066414775
At time: 669.2681312561035 and batch: 900, loss is 4.292435121536255 and perplexity is 73.14436723311042
At time: 670.4272134304047 and batch: 950, loss is 4.281504220962525 and perplexity is 72.34918736104589
At time: 671.58638215065 and batch: 1000, loss is 4.26722638130188 and perplexity is 71.32353672574037
At time: 672.7460374832153 and batch: 1050, loss is 4.262680377960205 and perplexity is 71.00003556525006
At time: 673.9193649291992 and batch: 1100, loss is 4.236702275276184 and perplexity is 69.17934083999256
At time: 675.0885982513428 and batch: 1150, loss is 4.229751830101013 and perplexity is 68.7001807411929
At time: 676.2483386993408 and batch: 1200, loss is 4.25570650100708 and perplexity is 70.50661258891088
At time: 677.407310962677 and batch: 1250, loss is 4.280560941696167 and perplexity is 72.28097404982933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.428793802748632 and perplexity of 83.83024009775176
Finished 22 epochs...
Completing Train Step...
At time: 680.2888216972351 and batch: 50, loss is 4.233913421630859 and perplexity is 68.98667856143122
At time: 681.4784560203552 and batch: 100, loss is 4.253807544708252 and perplexity is 70.37285065705349
At time: 682.6414544582367 and batch: 150, loss is 4.1716379547119145 and perplexity is 64.8215399469358
At time: 683.8309512138367 and batch: 200, loss is 4.21982629776001 and perplexity is 68.02166774711391
At time: 684.9930515289307 and batch: 250, loss is 4.248961672782898 and perplexity is 70.03265776704879
At time: 686.155948638916 and batch: 300, loss is 4.260490407943726 and perplexity is 70.84471774894853
At time: 687.3183023929596 and batch: 350, loss is 4.235424671173096 and perplexity is 69.09101346600866
At time: 688.4811670780182 and batch: 400, loss is 4.245350646972656 and perplexity is 69.78022407887465
At time: 689.6457643508911 and batch: 450, loss is 4.17797128200531 and perplexity is 65.23337875345277
At time: 690.8098990917206 and batch: 500, loss is 4.2241671276092525 and perplexity is 68.3175800206294
At time: 691.9741199016571 and batch: 550, loss is 4.211297512054443 and perplexity is 67.44399245562752
At time: 693.1368877887726 and batch: 600, loss is 4.242698421478272 and perplexity is 69.5953964002539
At time: 694.299592256546 and batch: 650, loss is 4.262329969406128 and perplexity is 70.97516090384983
At time: 695.4625890254974 and batch: 700, loss is 4.236138219833374 and perplexity is 69.14033085919202
At time: 696.6249988079071 and batch: 750, loss is 4.2309155988693234 and perplexity is 68.78017840628006
At time: 697.7889125347137 and batch: 800, loss is 4.2697034931182865 and perplexity is 71.5004321057159
At time: 698.9520137310028 and batch: 850, loss is 4.298671998977661 and perplexity is 73.60198525744663
At time: 700.1150913238525 and batch: 900, loss is 4.2741237449646 and perplexity is 71.81718156319323
At time: 701.2774567604065 and batch: 950, loss is 4.263477602005005 and perplexity is 71.05666106939195
At time: 702.4404270648956 and batch: 1000, loss is 4.248928880691528 and perplexity is 70.03036128738978
At time: 703.6060128211975 and batch: 1050, loss is 4.244734869003296 and perplexity is 69.73726818119496
At time: 704.769415140152 and batch: 1100, loss is 4.218611001968384 and perplexity is 67.93905151231098
At time: 705.9325320720673 and batch: 1150, loss is 4.211344566345215 and perplexity is 67.44716605952456
At time: 707.0961530208588 and batch: 1200, loss is 4.237366933822631 and perplexity is 69.2253367642182
At time: 708.2601687908173 and batch: 1250, loss is 4.26319197177887 and perplexity is 71.0363680375119
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.426235226819115 and perplexity of 83.61602821880876
Finished 23 epochs...
Completing Train Step...
At time: 711.1341152191162 and batch: 50, loss is 4.215799007415772 and perplexity is 67.74827562563615
At time: 712.3201932907104 and batch: 100, loss is 4.236132183074951 and perplexity is 69.13991347697716
At time: 713.4788506031036 and batch: 150, loss is 4.153513917922973 and perplexity is 63.65729425725699
At time: 714.637945652008 and batch: 200, loss is 4.2021260070800786 and perplexity is 66.82825746769886
At time: 715.8128957748413 and batch: 250, loss is 4.2310822439193725 and perplexity is 68.79164123763849
At time: 716.9852440357208 and batch: 300, loss is 4.242443017959594 and perplexity is 69.57762376082347
At time: 718.1467568874359 and batch: 350, loss is 4.216731901168823 and perplexity is 67.81150705826578
At time: 719.3134586811066 and batch: 400, loss is 4.227279453277588 and perplexity is 68.53053780361077
At time: 720.4729351997375 and batch: 450, loss is 4.159038434028625 and perplexity is 64.00994321513558
At time: 721.6386680603027 and batch: 500, loss is 4.206979174613952 and perplexity is 67.15337448272793
At time: 722.7982964515686 and batch: 550, loss is 4.19382073879242 and perplexity is 66.2755293151773
At time: 723.9581050872803 and batch: 600, loss is 4.22477352142334 and perplexity is 68.35901994173341
At time: 725.1194491386414 and batch: 650, loss is 4.245295915603638 and perplexity is 69.77640501619264
At time: 726.2793037891388 and batch: 700, loss is 4.218802890777588 and perplexity is 67.95208950688676
At time: 727.4387834072113 and batch: 750, loss is 4.213189873695374 and perplexity is 67.57174171563854
At time: 728.5976212024689 and batch: 800, loss is 4.252025294303894 and perplexity is 70.24754031588705
At time: 729.7566864490509 and batch: 850, loss is 4.281315107345581 and perplexity is 72.33550643820625
At time: 730.9161870479584 and batch: 900, loss is 4.256285538673401 and perplexity is 70.54745039549763
At time: 732.0768814086914 and batch: 950, loss is 4.24633038520813 and perplexity is 69.84862393398998
At time: 733.2362804412842 and batch: 1000, loss is 4.233164615631104 and perplexity is 68.93504025856956
At time: 734.3961002826691 and batch: 1050, loss is 4.227015271186828 and perplexity is 68.51243565409005
At time: 735.5545792579651 and batch: 1100, loss is 4.200766377449035 and perplexity is 66.7374575297855
At time: 736.7131242752075 and batch: 1150, loss is 4.193595962524414 and perplexity is 66.260633823177
At time: 737.8721401691437 and batch: 1200, loss is 4.219904308319092 and perplexity is 68.02697436242786
At time: 739.0327498912811 and batch: 1250, loss is 4.246264171600342 and perplexity is 69.84399915771355
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.4231692126197535 and perplexity of 83.36005290149791
Finished 24 epochs...
Completing Train Step...
At time: 741.9382798671722 and batch: 50, loss is 4.198053331375122 and perplexity is 66.5566411251797
At time: 743.100353717804 and batch: 100, loss is 4.218941030502319 and perplexity is 67.96147703820671
At time: 744.2624094486237 and batch: 150, loss is 4.137179489135742 and perplexity is 62.62593498136542
At time: 745.4250762462616 and batch: 200, loss is 4.185100288391113 and perplexity is 65.70008954262704
At time: 746.5886199474335 and batch: 250, loss is 4.213641390800476 and perplexity is 67.60225840172917
At time: 747.7505621910095 and batch: 300, loss is 4.225388288497925 and perplexity is 68.40105773684667
At time: 748.9139268398285 and batch: 350, loss is 4.199040060043335 and perplexity is 66.62234688258403
At time: 750.0768325328827 and batch: 400, loss is 4.210201253890991 and perplexity is 67.3700969400039
At time: 751.23952293396 and batch: 450, loss is 4.141233587265015 and perplexity is 62.88034201430887
At time: 752.402407169342 and batch: 500, loss is 4.1906085872650145 and perplexity is 66.06298381937965
At time: 753.5654788017273 and batch: 550, loss is 4.1765057516098025 and perplexity is 65.13784727333022
At time: 754.7282075881958 and batch: 600, loss is 4.208301181793213 and perplexity is 67.24221043385002
At time: 755.8911926746368 and batch: 650, loss is 4.228755235671997 and perplexity is 68.63174862898862
At time: 757.0539219379425 and batch: 700, loss is 4.201711883544922 and perplexity is 66.80058804314338
At time: 758.2159514427185 and batch: 750, loss is 4.19604501247406 and perplexity is 66.42310829794921
At time: 759.3800137042999 and batch: 800, loss is 4.235073666572571 and perplexity is 69.0667664580818
At time: 760.5452167987823 and batch: 850, loss is 4.264533548355103 and perplexity is 71.13173272014058
At time: 761.708616733551 and batch: 900, loss is 4.239371075630188 and perplexity is 69.3642132733945
At time: 762.8716464042664 and batch: 950, loss is 4.230735988616943 and perplexity is 68.7678258904302
At time: 764.0369575023651 and batch: 1000, loss is 4.2165306949615475 and perplexity is 67.7978643346673
At time: 765.1992437839508 and batch: 1050, loss is 4.210595998764038 and perplexity is 67.39669618998101
At time: 766.362555027008 and batch: 1100, loss is 4.184276065826416 and perplexity is 65.645960356636
At time: 767.5249752998352 and batch: 1150, loss is 4.177019867897034 and perplexity is 65.17134431147726
At time: 768.6876995563507 and batch: 1200, loss is 4.203468132019043 and perplexity is 66.91800955445822
At time: 769.8502943515778 and batch: 1250, loss is 4.229935350418091 and perplexity is 68.71278977711759
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.420262691748404 and perplexity of 83.11811693419227
Finished 25 epochs...
Completing Train Step...
At time: 772.730889081955 and batch: 50, loss is 4.18198028087616 and perplexity is 65.49542421422757
At time: 773.9173438549042 and batch: 100, loss is 4.202773733139038 and perplexity is 66.8715578934287
At time: 775.0760765075684 and batch: 150, loss is 4.1209741926193235 and perplexity is 61.61924204482004
At time: 776.2539682388306 and batch: 200, loss is 4.168842992782593 and perplexity is 64.64061916171416
At time: 777.4187128543854 and batch: 250, loss is 4.197395577430725 and perplexity is 66.51287762633861
At time: 778.577930688858 and batch: 300, loss is 4.208606157302857 and perplexity is 67.26272078866563
At time: 779.7375376224518 and batch: 350, loss is 4.182283248901367 and perplexity is 65.51527023976563
At time: 780.8970901966095 and batch: 400, loss is 4.1939645195007325 and perplexity is 66.2850591428123
At time: 782.0564067363739 and batch: 450, loss is 4.124647059440613 and perplexity is 61.8459774440546
At time: 783.2165155410767 and batch: 500, loss is 4.175147590637207 and perplexity is 65.0494396408125
At time: 784.380562543869 and batch: 550, loss is 4.159922132492065 and perplexity is 64.06653370438114
At time: 785.5433783531189 and batch: 600, loss is 4.192034296989441 and perplexity is 66.15723763115815
At time: 786.7026889324188 and batch: 650, loss is 4.2128493595123295 and perplexity is 67.5487364962354
At time: 787.8624386787415 and batch: 700, loss is 4.186586318016052 and perplexity is 65.79779440022496
At time: 789.0217349529266 and batch: 750, loss is 4.179724669456482 and perplexity is 65.3478584754812
At time: 790.1812222003937 and batch: 800, loss is 4.219249248504639 and perplexity is 67.98242721733945
At time: 791.3414013385773 and batch: 850, loss is 4.248721513748169 and perplexity is 70.01584081101274
At time: 792.5006394386292 and batch: 900, loss is 4.2232475137710574 and perplexity is 68.25478310753113
At time: 793.6692187786102 and batch: 950, loss is 4.214396805763244 and perplexity is 67.65334545277472
At time: 794.8306732177734 and batch: 1000, loss is 4.200543718338013 and perplexity is 66.72259948102136
At time: 795.9896109104156 and batch: 1050, loss is 4.195174112319946 and perplexity is 66.36528558525522
At time: 797.1498513221741 and batch: 1100, loss is 4.167954797744751 and perplexity is 64.58323117416734
At time: 798.3094525337219 and batch: 1150, loss is 4.160644941329956 and perplexity is 64.11285830105275
At time: 799.4680733680725 and batch: 1200, loss is 4.187874584197998 and perplexity is 65.88261409704013
At time: 800.6545872688293 and batch: 1250, loss is 4.214777836799621 and perplexity is 67.67912838884445
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.418580438098768 and perplexity of 82.97840872389982
Finished 26 epochs...
Completing Train Step...
At time: 803.5503633022308 and batch: 50, loss is 4.165838623046875 and perplexity is 64.44670628066089
At time: 804.7105631828308 and batch: 100, loss is 4.187149572372436 and perplexity is 65.83486573387637
At time: 805.8706545829773 and batch: 150, loss is 4.1048485279083256 and perplexity is 60.63355955989671
At time: 807.0305876731873 and batch: 200, loss is 4.153611226081848 and perplexity is 63.663488932751456
At time: 808.1910197734833 and batch: 250, loss is 4.181759819984436 and perplexity is 65.48098662612173
At time: 809.3521912097931 and batch: 300, loss is 4.192999386787415 and perplexity is 66.22111612554174
At time: 810.511876821518 and batch: 350, loss is 4.166101689338684 and perplexity is 64.46366226687786
At time: 811.67187666893 and batch: 400, loss is 4.178507986068726 and perplexity is 65.26839916985935
At time: 812.8365135192871 and batch: 450, loss is 4.1083863162994385 and perplexity is 60.84844815407871
At time: 813.9974265098572 and batch: 500, loss is 4.159618468284607 and perplexity is 64.04708194475025
At time: 815.1580183506012 and batch: 550, loss is 4.14471203327179 and perplexity is 63.099448743431644
At time: 816.3184123039246 and batch: 600, loss is 4.1776296377182005 and perplexity is 65.21109594888598
At time: 817.4774956703186 and batch: 650, loss is 4.198169589042664 and perplexity is 66.56437929483839
At time: 818.6370961666107 and batch: 700, loss is 4.170999307632446 and perplexity is 64.78015507633205
At time: 819.7976553440094 and batch: 750, loss is 4.164473099708557 and perplexity is 64.35876285722082
At time: 820.9577724933624 and batch: 800, loss is 4.204119176864624 and perplexity is 66.96159036464677
At time: 822.1183335781097 and batch: 850, loss is 4.233557786941528 and perplexity is 68.96214886749661
At time: 823.2804999351501 and batch: 900, loss is 4.207943773269653 and perplexity is 67.21818178897497
At time: 824.4404621124268 and batch: 950, loss is 4.1990760135650635 and perplexity is 66.62474223364067
At time: 825.6005990505219 and batch: 1000, loss is 4.1860305690765385 and perplexity is 65.7612375049335
At time: 826.7607028484344 and batch: 1050, loss is 4.180175113677978 and perplexity is 65.37730067125433
At time: 827.9220962524414 and batch: 1100, loss is 4.152424836158753 and perplexity is 63.58800399714991
At time: 829.0843369960785 and batch: 1150, loss is 4.145495986938476 and perplexity is 63.148935182643214
At time: 830.2835555076599 and batch: 1200, loss is 4.172529425621033 and perplexity is 64.87935222923795
At time: 831.4432752132416 and batch: 1250, loss is 4.19952419757843 and perplexity is 66.65460907042392
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.416713296932025 and perplexity of 82.82362087137504
Finished 27 epochs...
Completing Train Step...
At time: 834.3114058971405 and batch: 50, loss is 4.151190433502197 and perplexity is 63.50955922227088
At time: 835.4979145526886 and batch: 100, loss is 4.172185254096985 and perplexity is 64.85702644587182
At time: 836.6572229862213 and batch: 150, loss is 4.090113615989685 and perplexity is 59.74667949665841
At time: 837.8176045417786 and batch: 200, loss is 4.138862977027893 and perplexity is 62.73145377952953
At time: 838.9763805866241 and batch: 250, loss is 4.166559314727783 and perplexity is 64.49316922645745
At time: 840.1363685131073 and batch: 300, loss is 4.178246808052063 and perplexity is 65.25135472472749
At time: 841.2960019111633 and batch: 350, loss is 4.150386009216309 and perplexity is 63.458491133405694
At time: 842.4559330940247 and batch: 400, loss is 4.163583345413208 and perplexity is 64.30152483918758
At time: 843.6158034801483 and batch: 450, loss is 4.0934048271179195 and perplexity is 59.94364237851424
At time: 844.7752850055695 and batch: 500, loss is 4.144700722694397 and perplexity is 63.0987350562693
At time: 845.9348766803741 and batch: 550, loss is 4.131062760353088 and perplexity is 62.24403829226068
At time: 847.0942199230194 and batch: 600, loss is 4.162595014572144 and perplexity is 64.23800505351365
At time: 848.2534575462341 and batch: 650, loss is 4.183801860809326 and perplexity is 65.61483809263325
At time: 849.4133071899414 and batch: 700, loss is 4.156131315231323 and perplexity is 63.82412892896763
At time: 850.5729458332062 and batch: 750, loss is 4.14981629371643 and perplexity is 63.422348143996324
At time: 851.7328128814697 and batch: 800, loss is 4.19005853176117 and perplexity is 66.02665550374034
At time: 852.8917284011841 and batch: 850, loss is 4.219336128234863 and perplexity is 67.98833376885217
At time: 854.0516045093536 and batch: 900, loss is 4.193390398025513 and perplexity is 66.24701438907229
At time: 855.2109246253967 and batch: 950, loss is 4.184194455146789 and perplexity is 65.64060316380159
At time: 856.3706560134888 and batch: 1000, loss is 4.171466302871704 and perplexity is 64.81041416522643
At time: 857.5303997993469 and batch: 1050, loss is 4.165441212654113 and perplexity is 64.42109957832751
At time: 858.7179536819458 and batch: 1100, loss is 4.137427005767822 and perplexity is 62.64143786040393
At time: 859.8771929740906 and batch: 1150, loss is 4.1306230831146244 and perplexity is 62.216677020898416
At time: 861.0451190471649 and batch: 1200, loss is 4.158201088905335 and perplexity is 63.956367235265404
At time: 862.2055516242981 and batch: 1250, loss is 4.18480929851532 and perplexity is 65.6809742630427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.415815590071852 and perplexity of 82.749302901601
Finished 28 epochs...
Completing Train Step...
At time: 865.076910495758 and batch: 50, loss is 4.135903339385987 and perplexity is 62.54606588341471
At time: 866.2753596305847 and batch: 100, loss is 4.158023457527161 and perplexity is 63.94500758655532
At time: 867.4345436096191 and batch: 150, loss is 4.07578064918518 and perplexity is 58.89644010486168
At time: 868.5940246582031 and batch: 200, loss is 4.125070171356201 and perplexity is 61.87215075077089
At time: 869.7540566921234 and batch: 250, loss is 4.152183184623718 and perplexity is 63.57263971485017
At time: 870.9139699935913 and batch: 300, loss is 4.1639506101608275 and perplexity is 64.3251448596141
At time: 872.0734143257141 and batch: 350, loss is 4.135853500366211 and perplexity is 62.542948726478905
At time: 873.2328400611877 and batch: 400, loss is 4.149051403999328 and perplexity is 63.373855590158655
At time: 874.3918662071228 and batch: 450, loss is 4.079409317970276 and perplexity is 59.11054399975386
At time: 875.5509009361267 and batch: 500, loss is 4.130812788009644 and perplexity is 62.228480948677166
At time: 876.7105894088745 and batch: 550, loss is 4.11691951751709 and perplexity is 61.369901876572534
At time: 877.8704867362976 and batch: 600, loss is 4.14792311668396 and perplexity is 63.30239199608493
At time: 879.0303866863251 and batch: 650, loss is 4.169811015129089 and perplexity is 64.7032230216331
At time: 880.1899492740631 and batch: 700, loss is 4.142308592796326 and perplexity is 62.94797507622952
At time: 881.349365234375 and batch: 750, loss is 4.136057353019714 and perplexity is 62.55569957213742
At time: 882.508759021759 and batch: 800, loss is 4.176156764030456 and perplexity is 65.11511893987021
At time: 883.6702601909637 and batch: 850, loss is 4.205533237457275 and perplexity is 67.05634508947482
At time: 884.8300955295563 and batch: 900, loss is 4.179214925765991 and perplexity is 65.31455630544288
At time: 885.9908394813538 and batch: 950, loss is 4.170144710540772 and perplexity is 64.72481779311416
At time: 887.1510059833527 and batch: 1000, loss is 4.15769257068634 and perplexity is 63.923852525167455
At time: 888.3369295597076 and batch: 1050, loss is 4.1515524959564205 and perplexity is 63.53255781236127
At time: 889.496253490448 and batch: 1100, loss is 4.123011803627014 and perplexity is 61.744926094812776
At time: 890.6558606624603 and batch: 1150, loss is 4.1164949512481686 and perplexity is 61.3438518166884
At time: 891.8151824474335 and batch: 1200, loss is 4.144483861923217 and perplexity is 63.08505289953968
At time: 892.9751722812653 and batch: 1250, loss is 4.170966458320618 and perplexity is 64.7780271277687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.415015450359261 and perplexity of 82.6831183801233
Finished 29 epochs...
Completing Train Step...
At time: 895.868902683258 and batch: 50, loss is 4.121848750114441 and perplexity is 61.673155186428524
At time: 897.0331404209137 and batch: 100, loss is 4.144241843223572 and perplexity is 63.069786984462844
At time: 898.1979970932007 and batch: 150, loss is 4.062008395195007 and perplexity is 58.09086340915646
At time: 899.3618133068085 and batch: 200, loss is 4.1112464237213135 and perplexity is 61.022730366372734
At time: 900.5269453525543 and batch: 250, loss is 4.138330798149109 and perplexity is 62.6980783064413
At time: 901.6894400119781 and batch: 300, loss is 4.150219349861145 and perplexity is 63.44791606343541
At time: 902.8525619506836 and batch: 350, loss is 4.1214724111557 and perplexity is 61.649949542291296
At time: 904.0161004066467 and batch: 400, loss is 4.1354296875 and perplexity is 62.51644783622368
At time: 905.1789174079895 and batch: 450, loss is 4.065433135032654 and perplexity is 58.2901505618186
At time: 906.3420321941376 and batch: 500, loss is 4.117510714530945 and perplexity is 61.4061943062335
At time: 907.5049126148224 and batch: 550, loss is 4.102796249389648 and perplexity is 60.509250210435766
At time: 908.6674265861511 and batch: 600, loss is 4.13491575717926 and perplexity is 62.48432699278035
At time: 909.8304896354675 and batch: 650, loss is 4.156305875778198 and perplexity is 63.835271076278666
At time: 910.9935154914856 and batch: 700, loss is 4.128676042556763 and perplexity is 62.09565648154466
At time: 912.1566784381866 and batch: 750, loss is 4.122441706657409 and perplexity is 61.70973553152752
At time: 913.3193752765656 and batch: 800, loss is 4.1628398418426515 and perplexity is 64.25373419433649
At time: 914.4819934368134 and batch: 850, loss is 4.192554407119751 and perplexity is 66.19165563044261
At time: 915.644385099411 and batch: 900, loss is 4.165707607269287 and perplexity is 64.43826329841852
At time: 916.8072097301483 and batch: 950, loss is 4.15662919998169 and perplexity is 63.855913901436736
At time: 918.005040884018 and batch: 1000, loss is 4.144391150474548 and perplexity is 63.07920446400865
At time: 919.1852858066559 and batch: 1050, loss is 4.138093390464783 and perplexity is 62.683195067626386
At time: 920.3541789054871 and batch: 1100, loss is 4.109305462837219 and perplexity is 60.90440250571382
At time: 921.5229167938232 and batch: 1150, loss is 4.102937026023865 and perplexity is 60.5177690986355
At time: 922.6966032981873 and batch: 1200, loss is 4.131754751205444 and perplexity is 62.287125503631344
At time: 923.8653275966644 and batch: 1250, loss is 4.157683095932007 and perplexity is 63.92324686523799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.413511401545392 and perplexity of 82.55885240846614
Finished 30 epochs...
Completing Train Step...
At time: 926.7429449558258 and batch: 50, loss is 4.108364019393921 and perplexity is 60.84709143710468
At time: 927.9442663192749 and batch: 100, loss is 4.13104742527008 and perplexity is 62.24308378208549
At time: 929.1037199497223 and batch: 150, loss is 4.049038577079773 and perplexity is 57.34230033339214
At time: 930.2630033493042 and batch: 200, loss is 4.0977127599716185 and perplexity is 60.20243259008926
At time: 931.4231581687927 and batch: 250, loss is 4.124945673942566 and perplexity is 61.86444830750345
At time: 932.5839784145355 and batch: 300, loss is 4.136830945014953 and perplexity is 62.60411088347019
At time: 933.744635105133 and batch: 350, loss is 4.1083989381790165 and perplexity is 60.849216180710776
At time: 934.9049034118652 and batch: 400, loss is 4.121757788658142 and perplexity is 61.667545561551925
At time: 936.0645425319672 and batch: 450, loss is 4.0520923042297365 and perplexity is 57.51767571075232
At time: 937.2247009277344 and batch: 500, loss is 4.104121570587158 and perplexity is 60.58949756740066
At time: 938.3836176395416 and batch: 550, loss is 4.089490036964417 and perplexity is 59.7094343343528
At time: 939.5429351329803 and batch: 600, loss is 4.121670808792114 and perplexity is 61.662181959966766
At time: 940.7019410133362 and batch: 650, loss is 4.143621716499329 and perplexity is 63.03068784852399
At time: 941.8616497516632 and batch: 700, loss is 4.116528692245484 and perplexity is 61.34592165434685
At time: 943.0212352275848 and batch: 750, loss is 4.109388055801392 and perplexity is 60.909432988586325
At time: 944.1932179927826 and batch: 800, loss is 4.150778803825379 and perplexity is 63.483422182692934
At time: 945.3523383140564 and batch: 850, loss is 4.179430999755859 and perplexity is 65.32867060703211
At time: 946.5129373073578 and batch: 900, loss is 4.1524265146255495 and perplexity is 63.588110727592856
At time: 947.7164671421051 and batch: 950, loss is 4.143721809387207 and perplexity is 63.03699708784544
At time: 948.8758573532104 and batch: 1000, loss is 4.131535663604736 and perplexity is 62.273480661512174
At time: 950.0342307090759 and batch: 1050, loss is 4.125225028991699 and perplexity is 61.88173286765216
At time: 951.1922595500946 and batch: 1100, loss is 4.0959496545791625 and perplexity is 60.096382872436884
At time: 952.3514177799225 and batch: 1150, loss is 4.089807367324829 and perplexity is 59.728384957315086
At time: 953.509968996048 and batch: 1200, loss is 4.119216799736023 and perplexity is 61.51104792501001
At time: 954.6687183380127 and batch: 1250, loss is 4.144849677085876 and perplexity is 63.108134589986236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.413942657247947 and perplexity of 82.59446406267624
Annealing...
Finished 31 epochs...
Completing Train Step...
At time: 957.5706760883331 and batch: 50, loss is 4.1194400882720945 and perplexity is 61.5247841703694
At time: 958.7334470748901 and batch: 100, loss is 4.15656238079071 and perplexity is 63.8516472434795
At time: 959.8958539962769 and batch: 150, loss is 4.071965594291687 and perplexity is 58.67217501664953
At time: 961.0579001903534 and batch: 200, loss is 4.12432065486908 and perplexity is 61.825793928461
At time: 962.2207789421082 and batch: 250, loss is 4.150921277999878 and perplexity is 63.49246757521491
At time: 963.3838632106781 and batch: 300, loss is 4.154290962219238 and perplexity is 63.70677801772231
At time: 964.5464353561401 and batch: 350, loss is 4.1317105388641355 and perplexity is 62.28437170485582
At time: 965.7090063095093 and batch: 400, loss is 4.134126968383789 and perplexity is 62.435059489137814
At time: 966.8726794719696 and batch: 450, loss is 4.062302045822143 and perplexity is 58.107924332480266
At time: 968.0385913848877 and batch: 500, loss is 4.108245053291321 and perplexity is 60.8398531263472
At time: 969.2011303901672 and batch: 550, loss is 4.0911212921142575 and perplexity is 59.80691514302404
At time: 970.3644373416901 and batch: 600, loss is 4.11840341091156 and perplexity is 61.46103586842808
At time: 971.527185678482 and batch: 650, loss is 4.137282209396362 and perplexity is 62.632368264136865
At time: 972.6896855831146 and batch: 700, loss is 4.108019199371338 and perplexity is 60.826113758630854
At time: 973.852411031723 and batch: 750, loss is 4.0997495222091676 and perplexity is 60.32517558811674
At time: 975.0150718688965 and batch: 800, loss is 4.1326072406768795 and perplexity is 62.34024726198186
At time: 976.2246642112732 and batch: 850, loss is 4.159980225563049 and perplexity is 64.07025563417945
At time: 977.3872201442719 and batch: 900, loss is 4.123427386283875 and perplexity is 61.77059154793615
At time: 978.5506107807159 and batch: 950, loss is 4.111592135429382 and perplexity is 61.043830285753145
At time: 979.7138223648071 and batch: 1000, loss is 4.091328735351563 and perplexity is 59.81932297002995
At time: 980.8853223323822 and batch: 1050, loss is 4.080599966049195 and perplexity is 59.180965770864844
At time: 982.0494656562805 and batch: 1100, loss is 4.043829350471497 and perplexity is 57.04436796904072
At time: 983.212420463562 and batch: 1150, loss is 4.03159848690033 and perplexity is 56.35091549289629
At time: 984.3755025863647 and batch: 1200, loss is 4.055048723220825 and perplexity is 57.68797367157007
At time: 985.5378322601318 and batch: 1250, loss is 4.079909563064575 and perplexity is 59.14012115670813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.370256187271898 and perplexity of 79.06388426604458
Finished 32 epochs...
Completing Train Step...
At time: 988.4252769947052 and batch: 50, loss is 4.107820706367493 and perplexity is 60.8140413987778
At time: 989.6116144657135 and batch: 100, loss is 4.134612212181091 and perplexity is 62.465363066205875
At time: 990.7718060016632 and batch: 150, loss is 4.0488803100585935 and perplexity is 57.33322565646079
At time: 991.9311776161194 and batch: 200, loss is 4.100524573326111 and perplexity is 60.371948806312226
At time: 993.0893061161041 and batch: 250, loss is 4.126426067352295 and perplexity is 61.956099852507634
At time: 994.248610496521 and batch: 300, loss is 4.132059106826782 and perplexity is 62.306085825609266
At time: 995.4078290462494 and batch: 350, loss is 4.109078359603882 and perplexity is 60.890572489460446
At time: 996.5667207241058 and batch: 400, loss is 4.1142320728302 and perplexity is 61.205195079174125
At time: 997.7258501052856 and batch: 450, loss is 4.042322273254395 and perplexity is 56.95846245109468
At time: 998.886312007904 and batch: 500, loss is 4.090601773262024 and perplexity is 59.77585239265416
At time: 1000.0447046756744 and batch: 550, loss is 4.074126615524292 and perplexity is 58.79910393126676
At time: 1001.2046360969543 and batch: 600, loss is 4.1027531242370605 and perplexity is 60.50664079605358
At time: 1002.3656687736511 and batch: 650, loss is 4.123339471817016 and perplexity is 61.7651612580167
At time: 1003.5312304496765 and batch: 700, loss is 4.094661092758178 and perplexity is 60.01899483822923
At time: 1004.690512418747 and batch: 750, loss is 4.0877307462692265 and perplexity is 59.604480431396006
At time: 1005.8846468925476 and batch: 800, loss is 4.1227876615524295 and perplexity is 61.7310880098893
At time: 1007.0440783500671 and batch: 850, loss is 4.151248869895935 and perplexity is 63.513270600318386
At time: 1008.202959060669 and batch: 900, loss is 4.116408786773682 and perplexity is 61.338566383645166
At time: 1009.3617527484894 and batch: 950, loss is 4.106122336387634 and perplexity is 60.71084431473268
At time: 1010.521064043045 and batch: 1000, loss is 4.087378315925598 and perplexity is 59.58347770509218
At time: 1011.6800174713135 and batch: 1050, loss is 4.078472046852112 and perplexity is 59.05516734960526
At time: 1012.8402106761932 and batch: 1100, loss is 4.043858027458191 and perplexity is 57.04600385307793
At time: 1013.9987461566925 and batch: 1150, loss is 4.034034128189087 and perplexity is 56.48833339174997
At time: 1015.1575720310211 and batch: 1200, loss is 4.060009717941284 and perplexity is 57.974874472647855
At time: 1016.3166575431824 and batch: 1250, loss is 4.086942834854126 and perplexity is 59.55753587737658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.368417113366788 and perplexity of 78.91861356237887
Finished 33 epochs...
Completing Train Step...
At time: 1019.1949117183685 and batch: 50, loss is 4.101822423934936 and perplexity is 60.45035344457471
At time: 1020.3814146518707 and batch: 100, loss is 4.126972279548645 and perplexity is 61.98995027380025
At time: 1021.5424196720123 and batch: 150, loss is 4.0409769821167 and perplexity is 56.881888255184776
At time: 1022.7021591663361 and batch: 200, loss is 4.091932406425476 and perplexity is 59.85544506680357
At time: 1023.8621423244476 and batch: 250, loss is 4.117817807197571 and perplexity is 61.425054593970145
At time: 1025.0230479240417 and batch: 300, loss is 4.124195818901062 and perplexity is 61.818076327354596
At time: 1026.1829566955566 and batch: 350, loss is 4.101248626708984 and perplexity is 60.41567714899291
At time: 1027.3435513973236 and batch: 400, loss is 4.106945071220398 and perplexity is 60.76081379407387
At time: 1028.5032052993774 and batch: 450, loss is 4.034933996200562 and perplexity is 56.53918831391786
At time: 1029.663239479065 and batch: 500, loss is 4.084013848304749 and perplexity is 59.38334787756129
At time: 1030.8232862949371 and batch: 550, loss is 4.067732996940613 and perplexity is 58.42436413588948
At time: 1031.9833817481995 and batch: 600, loss is 4.096600275039673 and perplexity is 60.13549553110466
At time: 1033.1445779800415 and batch: 650, loss is 4.117458539009094 and perplexity is 61.402990489581846
At time: 1034.3046853542328 and batch: 700, loss is 4.089261155128479 and perplexity is 59.695769493276174
At time: 1035.4922943115234 and batch: 750, loss is 4.082841067314148 and perplexity is 59.31374503843331
At time: 1036.6540877819061 and batch: 800, loss is 4.118602681159973 and perplexity is 61.473284444661196
At time: 1037.8153574466705 and batch: 850, loss is 4.147687950134277 and perplexity is 63.28750714125114
At time: 1038.9918148517609 and batch: 900, loss is 4.113233442306519 and perplexity is 61.14410421183958
At time: 1040.160700082779 and batch: 950, loss is 4.103627128601074 and perplexity is 60.55954698091151
At time: 1041.3269758224487 and batch: 1000, loss is 4.0853982925415036 and perplexity is 59.46561774717255
At time: 1042.488135099411 and batch: 1050, loss is 4.077218537330627 and perplexity is 58.98118751194207
At time: 1043.6474719047546 and batch: 1100, loss is 4.0436363697052 and perplexity is 57.033360565339905
At time: 1044.8072528839111 and batch: 1150, loss is 4.034783782958985 and perplexity is 56.53069601700879
At time: 1045.966673374176 and batch: 1200, loss is 4.061446838378906 and perplexity is 58.05825124650398
At time: 1047.1265830993652 and batch: 1250, loss is 4.089397206306457 and perplexity is 59.70389172554313
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.367723451043568 and perplexity of 78.86388967569331
Finished 34 epochs...
Completing Train Step...
At time: 1050.0362136363983 and batch: 50, loss is 4.096723232269287 and perplexity is 60.142890079633915
At time: 1051.1969442367554 and batch: 100, loss is 4.121489415168762 and perplexity is 61.650997847751235
At time: 1052.3656141757965 and batch: 150, loss is 4.035438923835755 and perplexity is 56.56774372116938
At time: 1053.525149822235 and batch: 200, loss is 4.0860632085800175 and perplexity is 59.505170538338156
At time: 1054.6852090358734 and batch: 250, loss is 4.111959233283996 and perplexity is 61.06624345855059
At time: 1055.844500541687 and batch: 300, loss is 4.1188500165939335 and perplexity is 61.488490846609785
At time: 1057.003886461258 and batch: 350, loss is 4.095854315757752 and perplexity is 60.090653627236854
At time: 1058.16300034523 and batch: 400, loss is 4.102046098709106 and perplexity is 60.46387617602049
At time: 1059.3222246170044 and batch: 450, loss is 4.030021138191223 and perplexity is 56.26210051359761
At time: 1060.4818062782288 and batch: 500, loss is 4.079639368057251 and perplexity is 59.124143949817
At time: 1061.6423304080963 and batch: 550, loss is 4.063487963676453 and perplexity is 58.17687643510662
At time: 1062.801786184311 and batch: 600, loss is 4.092425765991211 and perplexity is 59.88498260890395
At time: 1063.9669346809387 and batch: 650, loss is 4.113556962013245 and perplexity is 61.16388873467119
At time: 1065.153713941574 and batch: 700, loss is 4.0856091403961186 and perplexity is 59.47815726701687
At time: 1066.3133563995361 and batch: 750, loss is 4.079538621902466 and perplexity is 59.11818771969779
At time: 1067.4729447364807 and batch: 800, loss is 4.115633773803711 and perplexity is 61.29104661573519
At time: 1068.6329109668732 and batch: 850, loss is 4.145185508728027 and perplexity is 63.12933185761607
At time: 1069.7930688858032 and batch: 900, loss is 4.11088641166687 and perplexity is 61.00076540190902
At time: 1070.953149318695 and batch: 950, loss is 4.101650619506836 and perplexity is 60.43996869827097
At time: 1072.1138331890106 and batch: 1000, loss is 4.08373010635376 and perplexity is 59.36650072081384
At time: 1073.2731430530548 and batch: 1050, loss is 4.075981030464172 and perplexity is 58.90824303136123
At time: 1074.4336895942688 and batch: 1100, loss is 4.04297173500061 and perplexity is 56.99546680873659
At time: 1075.5931849479675 and batch: 1150, loss is 4.034748344421387 and perplexity is 56.52869268731031
At time: 1076.751936674118 and batch: 1200, loss is 4.062220759391785 and perplexity is 58.10320113870413
At time: 1077.911306142807 and batch: 1250, loss is 4.090204176902771 and perplexity is 59.75209045551447
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.367336301037865 and perplexity of 78.83336342985504
Finished 35 epochs...
Completing Train Step...
At time: 1080.7749395370483 and batch: 50, loss is 4.092366805076599 and perplexity is 59.88145183964752
At time: 1081.9710760116577 and batch: 100, loss is 4.116925311088562 and perplexity is 61.37025742851526
At time: 1083.130743265152 and batch: 150, loss is 4.030945777893066 and perplexity is 56.31414674373881
At time: 1084.290231704712 and batch: 200, loss is 4.081375141143798 and perplexity is 59.22685916701577
At time: 1085.4491295814514 and batch: 250, loss is 4.1073063325881956 and perplexity is 60.7827682941912
At time: 1086.6080524921417 and batch: 300, loss is 4.114609661102295 and perplexity is 61.22830980667771
At time: 1087.7668342590332 and batch: 350, loss is 4.09157009601593 and perplexity is 59.83376274409065
At time: 1088.9266095161438 and batch: 400, loss is 4.098096036911011 and perplexity is 60.22551121666653
At time: 1090.0861608982086 and batch: 450, loss is 4.026157751083374 and perplexity is 56.045157576921625
At time: 1091.2451395988464 and batch: 500, loss is 4.076162958145142 and perplexity is 58.91896104632817
At time: 1092.403990983963 and batch: 550, loss is 4.060081186294556 and perplexity is 57.97901798952096
At time: 1093.5908243656158 and batch: 600, loss is 4.089115319252014 and perplexity is 59.687064343187856
At time: 1094.750286102295 and batch: 650, loss is 4.110420522689819 and perplexity is 60.97235243687412
At time: 1095.9096088409424 and batch: 700, loss is 4.082552518844604 and perplexity is 59.29663261707979
At time: 1097.069232225418 and batch: 750, loss is 4.0768369197845455 and perplexity is 58.95868355012522
At time: 1098.2286367416382 and batch: 800, loss is 4.113139767646789 and perplexity is 61.138376826942675
At time: 1099.388441324234 and batch: 850, loss is 4.14306492805481 and perplexity is 62.99560285824483
At time: 1100.546863079071 and batch: 900, loss is 4.1088491678237915 and perplexity is 60.876618469895234
At time: 1101.7062311172485 and batch: 950, loss is 4.099874486923218 and perplexity is 60.33271457747817
At time: 1102.8656611442566 and batch: 1000, loss is 4.0821516227722165 and perplexity is 59.272865594329126
At time: 1104.0249638557434 and batch: 1050, loss is 4.074667119979859 and perplexity is 58.83089369940669
At time: 1105.183756828308 and batch: 1100, loss is 4.042058267593384 and perplexity is 56.94342707936408
At time: 1106.3459017276764 and batch: 1150, loss is 4.034252614974975 and perplexity is 56.50067669452809
At time: 1107.5089421272278 and batch: 1200, loss is 4.062021489143372 and perplexity is 58.091624052902304
At time: 1108.6685588359833 and batch: 1250, loss is 4.090230717658996 and perplexity is 59.75367634222644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.367090378364507 and perplexity of 78.81397890201573
Finished 36 epochs...
Completing Train Step...
At time: 1111.5818073749542 and batch: 50, loss is 4.088654923439026 and perplexity is 59.659590993467575
At time: 1112.7444429397583 and batch: 100, loss is 4.113022079467774 and perplexity is 61.13118198608811
At time: 1113.9072575569153 and batch: 150, loss is 4.027075490951538 and perplexity is 56.09661606158726
At time: 1115.0694613456726 and batch: 200, loss is 4.0773846054077145 and perplexity is 58.990983217690996
At time: 1116.232442855835 and batch: 250, loss is 4.103364181518555 and perplexity is 60.54362511810962
At time: 1117.3957724571228 and batch: 300, loss is 4.111003832817078 and perplexity is 61.00792860249345
At time: 1118.5592377185822 and batch: 350, loss is 4.087872738838196 and perplexity is 59.61294442559247
At time: 1119.7215685844421 and batch: 400, loss is 4.094707450866699 and perplexity is 60.02177726979889
At time: 1120.8845205307007 and batch: 450, loss is 4.02275698184967 and perplexity is 55.85488464987329
At time: 1122.0472960472107 and batch: 500, loss is 4.073215837478638 and perplexity is 58.745575378317696
At time: 1123.2369480133057 and batch: 550, loss is 4.057219948768616 and perplexity is 57.813363349182254
At time: 1124.4031982421875 and batch: 600, loss is 4.086239151954651 and perplexity is 59.51564099992795
At time: 1125.5741398334503 and batch: 650, loss is 4.1076630687713624 and perplexity is 60.80445557504369
At time: 1126.7500429153442 and batch: 700, loss is 4.07990345954895 and perplexity is 59.139760195156136
At time: 1127.9182524681091 and batch: 750, loss is 4.074456725120545 and perplexity is 58.818517283816234
At time: 1129.0826244354248 and batch: 800, loss is 4.110876140594482 and perplexity is 61.00013886184947
At time: 1130.2474386692047 and batch: 850, loss is 4.141144700050354 and perplexity is 62.874753004249754
At time: 1131.4169034957886 and batch: 900, loss is 4.106976308822632 and perplexity is 60.762711845851705
At time: 1132.5849244594574 and batch: 950, loss is 4.098192067146301 and perplexity is 60.23129496438201
At time: 1133.7506470680237 and batch: 1000, loss is 4.081093344688416 and perplexity is 59.21017159939818
At time: 1134.921635389328 and batch: 1050, loss is 4.07338888168335 and perplexity is 58.7557418392876
At time: 1136.0869433879852 and batch: 1100, loss is 4.040899748802185 and perplexity is 56.87749524806442
At time: 1137.2492430210114 and batch: 1150, loss is 4.033444571495056 and perplexity is 56.45504013176068
At time: 1138.4118995666504 and batch: 1200, loss is 4.061435313224792 and perplexity is 58.05758212006669
At time: 1139.5741693973541 and batch: 1250, loss is 4.089847121238709 and perplexity is 59.73075944158408
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.366925093379334 and perplexity of 78.80095321118668
Finished 37 epochs...
Completing Train Step...
At time: 1142.4356453418732 and batch: 50, loss is 4.085126695632934 and perplexity is 59.449469262264806
At time: 1143.622230052948 and batch: 100, loss is 4.109448246955871 and perplexity is 60.91309930801548
At time: 1144.7809669971466 and batch: 150, loss is 4.02353786945343 and perplexity is 55.898518071087445
At time: 1145.9400775432587 and batch: 200, loss is 4.0738728857040405 and perplexity is 58.78418673774379
At time: 1147.0989220142365 and batch: 250, loss is 4.099817328453064 and perplexity is 60.32926615036701
At time: 1148.2575778961182 and batch: 300, loss is 4.107757391929627 and perplexity is 60.81019111382289
At time: 1149.4182283878326 and batch: 350, loss is 4.084530844688415 and perplexity is 59.41405679117864
At time: 1150.579196214676 and batch: 400, loss is 4.091628127098083 and perplexity is 59.837235062842204
At time: 1151.7461540699005 and batch: 450, loss is 4.01987042427063 and perplexity is 55.693888782968365
At time: 1152.9330036640167 and batch: 500, loss is 4.070483479499817 and perplexity is 58.58528052789625
At time: 1154.0923955440521 and batch: 550, loss is 4.054609971046448 and perplexity is 57.662668499441864
At time: 1155.25159740448 and batch: 600, loss is 4.083651094436646 and perplexity is 59.361810245083724
At time: 1156.4105923175812 and batch: 650, loss is 4.105219764709473 and perplexity is 60.656073147267406
At time: 1157.5698606967926 and batch: 700, loss is 4.077468113899231 and perplexity is 58.995909671410075
At time: 1158.7378010749817 and batch: 750, loss is 4.072226734161377 and perplexity is 58.687498661508045
At time: 1159.8978109359741 and batch: 800, loss is 4.10877785205841 and perplexity is 60.87227716205892
At time: 1161.0568253993988 and batch: 850, loss is 4.1394201850891115 and perplexity is 62.766417991533196
At time: 1162.2162277698517 and batch: 900, loss is 4.105225443840027 and perplexity is 60.65641762200387
At time: 1163.3756394386292 and batch: 950, loss is 4.096624526977539 and perplexity is 60.136953951090504
At time: 1164.5351059436798 and batch: 1000, loss is 4.079305090904236 and perplexity is 59.10438340223681
At time: 1165.6948554515839 and batch: 1050, loss is 4.071863770484924 and perplexity is 58.666201096587116
At time: 1166.8611285686493 and batch: 1100, loss is 4.03974709033966 and perplexity is 56.811972691658646
At time: 1168.0191090106964 and batch: 1150, loss is 4.032492771148681 and perplexity is 56.401331768882514
At time: 1169.177567243576 and batch: 1200, loss is 4.0606408786773684 and perplexity is 58.01147748707206
At time: 1170.336867570877 and batch: 1250, loss is 4.089127407073975 and perplexity is 59.68778583415564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.366822180086679 and perplexity of 78.79284396290925
Finished 38 epochs...
Completing Train Step...
At time: 1173.225003004074 and batch: 50, loss is 4.082052211761475 and perplexity is 59.26697351172465
At time: 1174.4212765693665 and batch: 100, loss is 4.106475677490234 and perplexity is 60.73229974172152
At time: 1175.5825500488281 and batch: 150, loss is 4.020501165390015 and perplexity is 55.72902828952309
At time: 1176.7443964481354 and batch: 200, loss is 4.070677604675293 and perplexity is 58.59665450971142
At time: 1177.9058141708374 and batch: 250, loss is 4.096649675369263 and perplexity is 60.13846631778225
At time: 1179.068083524704 and batch: 300, loss is 4.104843292236328 and perplexity is 60.63324210329788
At time: 1180.2299864292145 and batch: 350, loss is 4.08152494430542 and perplexity is 59.23573220235966
At time: 1181.445532798767 and batch: 400, loss is 4.088775610923767 and perplexity is 59.66679159394769
At time: 1182.6107153892517 and batch: 450, loss is 4.017138080596924 and perplexity is 55.54192164605579
At time: 1183.781219959259 and batch: 500, loss is 4.068001070022583 and perplexity is 58.440028234713346
At time: 1184.9566760063171 and batch: 550, loss is 4.052142415046692 and perplexity is 57.520558040688925
At time: 1186.1216659545898 and batch: 600, loss is 4.081397347450256 and perplexity is 59.22817439140404
At time: 1187.3049812316895 and batch: 650, loss is 4.102921400070191 and perplexity is 60.51682345816739
At time: 1188.478495836258 and batch: 700, loss is 4.07518349647522 and perplexity is 58.86128043493695
At time: 1189.6437079906464 and batch: 750, loss is 4.070114049911499 and perplexity is 58.56364138914451
At time: 1190.8056886196136 and batch: 800, loss is 4.10690933227539 and perplexity is 60.75864230549463
At time: 1191.9686908721924 and batch: 850, loss is 4.137550492286682 and perplexity is 62.64917371112846
At time: 1193.13108253479 and batch: 900, loss is 4.103456530570984 and perplexity is 60.54921652269638
At time: 1194.2938227653503 and batch: 950, loss is 4.095005178451538 and perplexity is 60.03965006906384
At time: 1195.4568238258362 and batch: 1000, loss is 4.077720251083374 and perplexity is 59.01078660938638
At time: 1196.6196556091309 and batch: 1050, loss is 4.070404453277588 and perplexity is 58.58065093742982
At time: 1197.7819447517395 and batch: 1100, loss is 4.038492932319641 and perplexity is 56.74076616192791
At time: 1198.946358203888 and batch: 1150, loss is 4.031380529403687 and perplexity is 56.33863472681305
At time: 1200.1158845424652 and batch: 1200, loss is 4.059667444229126 and perplexity is 57.95503459267855
At time: 1201.2783427238464 and batch: 1250, loss is 4.088263273239136 and perplexity is 59.63622987772107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.366782975022811 and perplexity of 78.78975494498229
Finished 39 epochs...
Completing Train Step...
At time: 1204.1922161579132 and batch: 50, loss is 4.079123921394348 and perplexity is 59.09367645997865
At time: 1205.3569028377533 and batch: 100, loss is 4.103424091339111 and perplexity is 60.54725238447961
At time: 1206.5163185596466 and batch: 150, loss is 4.017608122825623 and perplexity is 55.568034831361636
At time: 1207.6745200157166 and batch: 200, loss is 4.067714304924011 and perplexity is 58.423272076911516
At time: 1208.834148645401 and batch: 250, loss is 4.093643593788147 and perplexity is 59.95795663122148
At time: 1209.9929749965668 and batch: 300, loss is 4.102117214202881 and perplexity is 60.46817624732932
At time: 1211.1969950199127 and batch: 350, loss is 4.078766770362854 and perplexity is 59.07257486092943
At time: 1212.356162071228 and batch: 400, loss is 4.0861239528656 and perplexity is 59.508785247196336
At time: 1213.5256378650665 and batch: 450, loss is 4.01465473651886 and perplexity is 55.40416306562564
At time: 1214.68354845047 and batch: 500, loss is 4.065644702911377 and perplexity is 58.30248418997717
At time: 1215.841629743576 and batch: 550, loss is 4.049927802085876 and perplexity is 57.39331321836114
At time: 1216.9992864131927 and batch: 600, loss is 4.079207472801208 and perplexity is 59.098614026050825
At time: 1218.1569240093231 and batch: 650, loss is 4.100690779685974 and perplexity is 60.38198384208142
At time: 1219.3158631324768 and batch: 700, loss is 4.072988181114197 and perplexity is 58.73220309640038
At time: 1220.4747128486633 and batch: 750, loss is 4.068110022544861 and perplexity is 58.44639577006473
At time: 1221.6331281661987 and batch: 800, loss is 4.10493999004364 and perplexity is 60.63910548834313
At time: 1222.791826248169 and batch: 850, loss is 4.135821070671081 and perplexity is 62.540920510606604
At time: 1223.9505252838135 and batch: 900, loss is 4.101784195899963 and perplexity is 60.44804259051908
At time: 1225.108184337616 and batch: 950, loss is 4.0934528589248655 and perplexity is 59.946521649120335
At time: 1226.266363143921 and batch: 1000, loss is 4.0761350154876705 and perplexity is 58.917314716982595
At time: 1227.4252836704254 and batch: 1050, loss is 4.068958129882812 and perplexity is 58.495985612976526
At time: 1228.5839223861694 and batch: 1100, loss is 4.03723753452301 and perplexity is 56.669578622798404
At time: 1229.7428216934204 and batch: 1150, loss is 4.030251469612121 and perplexity is 56.27506093569053
At time: 1230.9016363620758 and batch: 1200, loss is 4.058633332252502 and perplexity is 57.89513357481875
At time: 1232.0602707862854 and batch: 1250, loss is 4.087271666526794 and perplexity is 59.57712350185829
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.36672951357208 and perplexity of 78.7855428429738
Finished 40 epochs...
Completing Train Step...
At time: 1234.9413211345673 and batch: 50, loss is 4.076339607238769 and perplexity is 58.92936994672882
At time: 1236.1285254955292 and batch: 100, loss is 4.1006788063049315 and perplexity is 60.38126086990901
At time: 1237.2888493537903 and batch: 150, loss is 4.014928660392761 and perplexity is 55.419341667398605
At time: 1238.4495513439178 and batch: 200, loss is 4.064948854446411 and perplexity is 58.26192860775611
At time: 1239.608878850937 and batch: 250, loss is 4.0909241724014285 and perplexity is 59.79512718294368
At time: 1240.7959985733032 and batch: 300, loss is 4.099558296203614 and perplexity is 60.313640948651354
At time: 1241.9563705921173 and batch: 350, loss is 4.076174654960632 and perplexity is 58.91965021457494
At time: 1243.1180913448334 and batch: 400, loss is 4.083646545410156 and perplexity is 59.36154020725065
At time: 1244.278414964676 and batch: 450, loss is 4.012266955375671 and perplexity is 55.27202786755125
At time: 1245.438064813614 and batch: 500, loss is 4.063426666259765 and perplexity is 58.1733104521641
At time: 1246.60124874115 and batch: 550, loss is 4.047729234695435 and perplexity is 57.2672687608786
At time: 1247.778371810913 and batch: 600, loss is 4.077050271034241 and perplexity is 58.97126380089944
At time: 1248.9384982585907 and batch: 650, loss is 4.098599371910095 and perplexity is 60.25583245452956
At time: 1250.0989165306091 and batch: 700, loss is 4.070865998268127 and perplexity is 58.607694783908386
At time: 1251.260704278946 and batch: 750, loss is 4.066156287193298 and perplexity is 58.33231845520561
At time: 1252.422287940979 and batch: 800, loss is 4.103051509857178 and perplexity is 60.524697801430115
At time: 1253.582081079483 and batch: 850, loss is 4.134111952781677 and perplexity is 62.43412199616522
At time: 1254.7421298027039 and batch: 900, loss is 4.100131201744079 and perplexity is 60.348204867701476
At time: 1255.9019570350647 and batch: 950, loss is 4.091908178329468 and perplexity is 59.853994900901306
At time: 1257.0622668266296 and batch: 1000, loss is 4.074597449302673 and perplexity is 58.826795053982565
At time: 1258.2229442596436 and batch: 1050, loss is 4.067501540184021 and perplexity is 58.410842986901656
At time: 1259.3824932575226 and batch: 1100, loss is 4.035887432098389 and perplexity is 56.59312051205169
At time: 1260.5424165725708 and batch: 1150, loss is 4.029018144607544 and perplexity is 56.20569827804672
At time: 1261.70192360878 and batch: 1200, loss is 4.0572134160995486 and perplexity is 57.81298567484546
At time: 1262.8625226020813 and batch: 1250, loss is 4.0861512517929075 and perplexity is 59.51040979537305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.36671436616104 and perplexity of 78.78434945501073
Finished 41 epochs...
Completing Train Step...
At time: 1265.7511117458344 and batch: 50, loss is 4.073662447929382 and perplexity is 58.771817625811444
At time: 1266.9105179309845 and batch: 100, loss is 4.09802649974823 and perplexity is 60.22132345109386
At time: 1268.0705575942993 and batch: 150, loss is 4.012392296791076 and perplexity is 55.27895617594939
At time: 1269.2297337055206 and batch: 200, loss is 4.06236617565155 and perplexity is 58.111650903245796
At time: 1270.4223539829254 and batch: 250, loss is 4.088291320800781 and perplexity is 59.63790255201199
At time: 1271.5824325084686 and batch: 300, loss is 4.097124724388123 and perplexity is 60.16704182404797
At time: 1272.7432465553284 and batch: 350, loss is 4.0736663007736205 and perplexity is 58.772044064906574
At time: 1273.9032311439514 and batch: 400, loss is 4.081235780715942 and perplexity is 59.2186058616871
At time: 1275.0627908706665 and batch: 450, loss is 4.009973621368408 and perplexity is 55.14541588366808
At time: 1276.2216374874115 and batch: 500, loss is 4.06127820968628 and perplexity is 58.04846178491536
At time: 1277.3811810016632 and batch: 550, loss is 4.045648174285889 and perplexity is 57.14821603601991
At time: 1278.5418751239777 and batch: 600, loss is 4.074995331764221 and perplexity is 58.85020586106954
At time: 1279.7021884918213 and batch: 650, loss is 4.0965714931488035 and perplexity is 60.133764742742684
At time: 1280.8614161014557 and batch: 700, loss is 4.068836922645569 and perplexity is 58.48889590584113
At time: 1282.0211420059204 and batch: 750, loss is 4.064247646331787 and perplexity is 58.22108919078139
At time: 1283.1798043251038 and batch: 800, loss is 4.101216397285461 and perplexity is 60.41373001792428
At time: 1284.338300704956 and batch: 850, loss is 4.13245316028595 and perplexity is 62.33064259227771
At time: 1285.4974772930145 and batch: 900, loss is 4.098521814346314 and perplexity is 60.25115934018081
At time: 1286.6562979221344 and batch: 950, loss is 4.090390782356263 and perplexity is 59.76324156184728
At time: 1287.8146855831146 and batch: 1000, loss is 4.073060550689697 and perplexity is 58.73645367481109
At time: 1288.9730269908905 and batch: 1050, loss is 4.066053128242492 and perplexity is 58.32630126480457
At time: 1290.131377696991 and batch: 1100, loss is 4.034524631500244 and perplexity is 56.51604790279443
At time: 1291.2906258106232 and batch: 1150, loss is 4.027757325172424 and perplexity is 56.134877696657725
At time: 1292.449414730072 and batch: 1200, loss is 4.056314830780029 and perplexity is 57.76105910835965
At time: 1293.6085760593414 and batch: 1250, loss is 4.084994187355042 and perplexity is 59.44159223736827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.366697436701642 and perplexity of 78.78301568985542
Finished 42 epochs...
Completing Train Step...
At time: 1296.478438615799 and batch: 50, loss is 4.071096711158752 and perplexity is 58.62121789450283
At time: 1297.6632659435272 and batch: 100, loss is 4.095562863349914 and perplexity is 60.0731426134862
At time: 1298.8220582008362 and batch: 150, loss is 4.009886093139649 and perplexity is 55.140589314325204
At time: 1300.008629322052 and batch: 200, loss is 4.059823331832885 and perplexity is 57.96406976836466
At time: 1301.1673166751862 and batch: 250, loss is 4.085820560455322 and perplexity is 59.49073347193173
At time: 1302.3259947299957 and batch: 300, loss is 4.094740424156189 and perplexity is 60.02375641786583
At time: 1303.4900472164154 and batch: 350, loss is 4.07129620552063 and perplexity is 58.6329136635404
At time: 1304.6538381576538 and batch: 400, loss is 4.078949799537659 and perplexity is 59.083387855076325
At time: 1305.8196711540222 and batch: 450, loss is 4.007776293754578 and perplexity is 55.02437636900545
At time: 1306.9857602119446 and batch: 500, loss is 4.059228200912475 and perplexity is 57.92958382101734
At time: 1308.1437904834747 and batch: 550, loss is 4.043640031814575 and perplexity is 57.03356942812675
At time: 1309.3020589351654 and batch: 600, loss is 4.073009185791015 and perplexity is 58.73343676030156
At time: 1310.4602200984955 and batch: 650, loss is 4.094642124176025 and perplexity is 60.01785637379246
At time: 1311.6198236942291 and batch: 700, loss is 4.066849021911621 and perplexity is 58.372741276929325
At time: 1312.7781031131744 and batch: 750, loss is 4.062359375953674 and perplexity is 58.11125576292
At time: 1313.93736577034 and batch: 800, loss is 4.099425239562988 and perplexity is 60.30561635207761
At time: 1315.096028327942 and batch: 850, loss is 4.130794086456299 and perplexity is 62.227317190303204
At time: 1316.254582643509 and batch: 900, loss is 4.096939702033996 and perplexity is 60.155910606122674
At time: 1317.4138078689575 and batch: 950, loss is 4.088874616622925 and perplexity is 59.6726992388064
At time: 1318.5731513500214 and batch: 1000, loss is 4.0716108179092405 and perplexity is 58.65136320663451
At time: 1319.7316300868988 and batch: 1050, loss is 4.064625072479248 and perplexity is 58.24306749950893
At time: 1320.8902554512024 and batch: 1100, loss is 4.033146009445191 and perplexity is 56.438187315184635
At time: 1322.0493264198303 and batch: 1150, loss is 4.026471610069275 and perplexity is 56.06275061396536
At time: 1323.2075264453888 and batch: 1200, loss is 4.054746918678283 and perplexity is 57.67056580608456
At time: 1324.366042137146 and batch: 1250, loss is 4.083753943443298 and perplexity is 59.367915862273485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.36668362582687 and perplexity of 78.7819276350051
Finished 43 epochs...
Completing Train Step...
At time: 1327.230173110962 and batch: 50, loss is 4.068659319877624 and perplexity is 58.47850903842973
At time: 1328.431621313095 and batch: 100, loss is 4.093196506500244 and perplexity is 59.93115618251943
At time: 1329.5930650234222 and batch: 150, loss is 4.007664527893066 and perplexity is 55.018226865835096
At time: 1330.7553660869598 and batch: 200, loss is 4.057429070472717 and perplexity is 57.82545464247756
At time: 1331.9170215129852 and batch: 250, loss is 4.083443064689636 and perplexity is 59.34946250710932
At time: 1333.0859982967377 and batch: 300, loss is 4.092518963813782 and perplexity is 59.89056401897136
At time: 1334.250431060791 and batch: 350, loss is 4.068994946479798 and perplexity is 58.49813927574912
At time: 1335.4122359752655 and batch: 400, loss is 4.076725296974182 and perplexity is 58.95210278346068
At time: 1336.573888540268 and batch: 450, loss is 4.0056454992294315 and perplexity is 54.9072555535984
At time: 1337.735508441925 and batch: 500, loss is 4.057235465049744 and perplexity is 57.81426040454042
At time: 1338.8986113071442 and batch: 550, loss is 4.042245535850525 and perplexity is 56.95409177425713
At time: 1340.0615572929382 and batch: 600, loss is 4.0712900686264035 and perplexity is 58.63255384065515
At time: 1341.2236273288727 and batch: 650, loss is 4.092730617523193 and perplexity is 59.903241420566914
At time: 1342.385670185089 and batch: 700, loss is 4.064950499534607 and perplexity is 58.262024453845996
At time: 1343.5480728149414 and batch: 750, loss is 4.06053026676178 and perplexity is 58.00506108129309
At time: 1344.7099714279175 and batch: 800, loss is 4.097682490348816 and perplexity is 60.2006103127429
At time: 1345.8720858097076 and batch: 850, loss is 4.1291975402832035 and perplexity is 62.12804767045321
At time: 1347.0335335731506 and batch: 900, loss is 4.0953080701828 and perplexity is 60.05783833701654
At time: 1348.1953711509705 and batch: 950, loss is 4.087344670295716 and perplexity is 59.581473015178936
At time: 1349.3576638698578 and batch: 1000, loss is 4.070037798881531 and perplexity is 58.5591760214165
At time: 1350.519464969635 and batch: 1050, loss is 4.063111839294433 and perplexity is 58.15499880802204
At time: 1351.6811242103577 and batch: 1100, loss is 4.031745929718017 and perplexity is 56.35922464320305
At time: 1352.8427515029907 and batch: 1150, loss is 4.025086860656739 and perplexity is 55.98517147921484
At time: 1354.00439620018 and batch: 1200, loss is 4.053765206336975 and perplexity is 57.61397768107763
At time: 1355.1663210391998 and batch: 1250, loss is 4.082498264312744 and perplexity is 59.29341559330612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3666916450444795 and perplexity of 78.78255940695962
Annealing...
Finished 44 epochs...
Completing Train Step...
At time: 1358.0593445301056 and batch: 50, loss is 4.077314305305481 and perplexity is 58.986836291306304
At time: 1359.2178893089294 and batch: 100, loss is 4.115757350921631 and perplexity is 61.29862125464665
At time: 1360.3765852451324 and batch: 150, loss is 4.042894821166993 and perplexity is 56.9910832374663
At time: 1361.5352458953857 and batch: 200, loss is 4.101186900138855 and perplexity is 60.4119480115551
At time: 1362.694133758545 and batch: 250, loss is 4.127371525764465 and perplexity is 62.01470446804631
At time: 1363.85897898674 and batch: 300, loss is 4.131940336227417 and perplexity is 62.29868613389326
At time: 1365.0176303386688 and batch: 350, loss is 4.1053623723983765 and perplexity is 60.6647237864861
At time: 1366.176601409912 and batch: 400, loss is 4.114282341003418 and perplexity is 61.208271829852855
At time: 1367.3350207805634 and batch: 450, loss is 4.0394082403182985 and perplexity is 56.79272521468663
At time: 1368.4936604499817 and batch: 500, loss is 4.078489246368409 and perplexity is 59.05618307865349
At time: 1369.6521151065826 and batch: 550, loss is 4.061160397529602 and perplexity is 58.0416233732726
At time: 1370.8111157417297 and batch: 600, loss is 4.088942875862122 and perplexity is 59.67677259087765
At time: 1371.9689421653748 and batch: 650, loss is 4.1110245847702025 and perplexity is 61.009194649304476
At time: 1373.1275277137756 and batch: 700, loss is 4.080150408744812 and perplexity is 59.15436651481503
At time: 1374.2861061096191 and batch: 750, loss is 4.066331758499145 and perplexity is 58.34255500138184
At time: 1375.444977760315 and batch: 800, loss is 4.103421635627747 and perplexity is 60.54710369808643
At time: 1376.603247642517 and batch: 850, loss is 4.1331052446365355 and perplexity is 62.37130068368596
At time: 1377.7614572048187 and batch: 900, loss is 4.10039146900177 and perplexity is 60.36391357363117
At time: 1378.9196457862854 and batch: 950, loss is 4.094153084754944 and perplexity is 59.98851245180901
At time: 1380.0785405635834 and batch: 1000, loss is 4.0865818119049075 and perplexity is 59.53603812095116
At time: 1381.2368609905243 and batch: 1050, loss is 4.077452507019043 and perplexity is 58.99498893650116
At time: 1382.3952434062958 and batch: 1100, loss is 4.038969039916992 and perplexity is 56.76778730375217
At time: 1383.5535027980804 and batch: 1150, loss is 4.026900568008423 and perplexity is 56.086804334580066
At time: 1384.7125816345215 and batch: 1200, loss is 4.050175347328186 and perplexity is 57.40752241862615
At time: 1385.871162891388 and batch: 1250, loss is 4.073623561859131 and perplexity is 58.7695322652171
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.357023587192062 and perplexity of 78.02455518349089
Finished 45 epochs...
Completing Train Step...
At time: 1388.7523896694183 and batch: 50, loss is 4.074805364608765 and perplexity is 58.839027316675875
At time: 1389.940750837326 and batch: 100, loss is 4.106535844802856 and perplexity is 60.7359539509172
At time: 1391.1028711795807 and batch: 150, loss is 4.031748728752136 and perplexity is 56.35938239481653
At time: 1392.2646217346191 and batch: 200, loss is 4.087406349182129 and perplexity is 59.58514804742016
At time: 1393.4420883655548 and batch: 250, loss is 4.113355693817138 and perplexity is 61.1515796278761
At time: 1394.6173121929169 and batch: 300, loss is 4.119155344963073 and perplexity is 61.50726789367756
At time: 1395.7812061309814 and batch: 350, loss is 4.095424585342407 and perplexity is 60.06483639331792
At time: 1396.9447331428528 and batch: 400, loss is 4.104811253547669 and perplexity is 60.63129952485072
At time: 1398.109016418457 and batch: 450, loss is 4.030187282562256 and perplexity is 56.27144892147158
At time: 1399.27286195755 and batch: 500, loss is 4.070372486114502 and perplexity is 58.57877831013911
At time: 1400.43546128273 and batch: 550, loss is 4.053902440071106 and perplexity is 57.62188480492256
At time: 1401.598661661148 and batch: 600, loss is 4.082955274581909 and perplexity is 59.32051948603304
At time: 1402.7639939785004 and batch: 650, loss is 4.105741963386536 and perplexity is 60.68775594005704
At time: 1403.9354462623596 and batch: 700, loss is 4.075569167137146 and perplexity is 58.88398588205506
At time: 1405.1035404205322 and batch: 750, loss is 4.064187226295471 and perplexity is 58.217571576726364
At time: 1406.26607131958 and batch: 800, loss is 4.1021626806259155 and perplexity is 60.47092558151142
At time: 1407.4296481609344 and batch: 850, loss is 4.132853937149048 and perplexity is 62.35562827819872
At time: 1408.5923416614532 and batch: 900, loss is 4.100826740264893 and perplexity is 60.39019396967506
At time: 1409.7552299499512 and batch: 950, loss is 4.095258212089538 and perplexity is 60.05484404235715
At time: 1410.9182152748108 and batch: 1000, loss is 4.08804301738739 and perplexity is 59.62309609556373
At time: 1412.0814754962921 and batch: 1050, loss is 4.079797439575195 and perplexity is 59.133490531693035
At time: 1413.2443997859955 and batch: 1100, loss is 4.041407451629639 and perplexity is 56.906379444894114
At time: 1414.4082279205322 and batch: 1150, loss is 4.029406471252441 and perplexity is 56.22752868667346
At time: 1415.5734837055206 and batch: 1200, loss is 4.052807393074036 and perplexity is 57.55882066839993
At time: 1416.7656185626984 and batch: 1250, loss is 4.076286697387696 and perplexity is 58.92625208502459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.356748706232892 and perplexity of 78.003110666403
Finished 46 epochs...
Completing Train Step...
At time: 1419.652461528778 and batch: 50, loss is 4.0733768129348755 and perplexity is 58.75503273529692
At time: 1420.8102843761444 and batch: 100, loss is 4.103900103569031 and perplexity is 60.57608047782084
At time: 1421.9685757160187 and batch: 150, loss is 4.028856029510498 and perplexity is 56.196587224355454
At time: 1423.126279592514 and batch: 200, loss is 4.083792328834534 and perplexity is 59.37019476668872
At time: 1424.2872450351715 and batch: 250, loss is 4.10961489200592 and perplexity is 60.92325102034112
At time: 1425.4457635879517 and batch: 300, loss is 4.115753149986267 and perplexity is 61.29836374364175
At time: 1426.604897260666 and batch: 350, loss is 4.09209041595459 and perplexity is 59.86490354473859
At time: 1427.7637014389038 and batch: 400, loss is 4.101513614654541 and perplexity is 60.431688696499904
At time: 1428.922451019287 and batch: 450, loss is 4.026916279792785 and perplexity is 56.087685565278186
At time: 1430.0803294181824 and batch: 500, loss is 4.067633776664734 and perplexity is 58.418567541936454
At time: 1431.2387931346893 and batch: 550, loss is 4.051442046165466 and perplexity is 57.48028653590699
At time: 1432.397494316101 and batch: 600, loss is 4.080946259498596 and perplexity is 59.20146330051582
At time: 1433.5563514232635 and batch: 650, loss is 4.1040322923660275 and perplexity is 60.58408848629881
At time: 1434.7141797542572 and batch: 700, loss is 4.074175696372986 and perplexity is 58.8019899120128
At time: 1435.8726818561554 and batch: 750, loss is 4.06366913318634 and perplexity is 58.18741726610691
At time: 1437.030652999878 and batch: 800, loss is 4.101964201927185 and perplexity is 60.45892458190176
At time: 1438.190595626831 and batch: 850, loss is 4.1329529666900635 and perplexity is 62.36180363321308
At time: 1439.3495321273804 and batch: 900, loss is 4.101159977912903 and perplexity is 60.41032160933378
At time: 1440.5100786685944 and batch: 950, loss is 4.095751075744629 and perplexity is 60.084450187594946
At time: 1441.6683626174927 and batch: 1000, loss is 4.088587970733642 and perplexity is 59.65559675616249
At time: 1442.8269011974335 and batch: 1050, loss is 4.080666332244873 and perplexity is 59.18489351675297
At time: 1443.9852011203766 and batch: 1100, loss is 4.042421751022339 and perplexity is 56.96412883363971
At time: 1445.1431167125702 and batch: 1150, loss is 4.030513401031494 and perplexity is 56.289803072906885
At time: 1446.3331410884857 and batch: 1200, loss is 4.053934135437012 and perplexity is 57.62371118058932
At time: 1447.492266178131 and batch: 1250, loss is 4.0773983335495 and perplexity is 58.99179305983145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.356630200017108 and perplexity of 77.99386736064392
Finished 47 epochs...
Completing Train Step...
At time: 1450.3629570007324 and batch: 50, loss is 4.072364773750305 and perplexity is 58.69560041886727
At time: 1451.5488815307617 and batch: 100, loss is 4.102266645431518 and perplexity is 60.477212756350916
At time: 1452.7087752819061 and batch: 150, loss is 4.027101068496704 and perplexity is 56.09805089366792
At time: 1453.8683047294617 and batch: 200, loss is 4.081753211021423 and perplexity is 59.24925529179646
At time: 1455.027554512024 and batch: 250, loss is 4.107505459785461 and perplexity is 60.79487300163189
At time: 1456.1869206428528 and batch: 300, loss is 4.1138531112670895 and perplexity is 61.18200505711747
At time: 1457.3459477424622 and batch: 350, loss is 4.090184259414673 and perplexity is 59.75090035581593
At time: 1458.5059270858765 and batch: 400, loss is 4.099621849060059 and perplexity is 60.31747417462118
At time: 1459.6653289794922 and batch: 450, loss is 4.024983687400818 and perplexity is 55.979395604753044
At time: 1460.8268022537231 and batch: 500, loss is 4.066024928092957 and perplexity is 58.32465647757878
At time: 1461.9933216571808 and batch: 550, loss is 4.049998946189881 and perplexity is 57.39739655945701
At time: 1463.1521382331848 and batch: 600, loss is 4.079780497550964 and perplexity is 59.132488699150166
At time: 1464.3110570907593 and batch: 650, loss is 4.1031284379959105 and perplexity is 60.529354032874664
At time: 1465.470701932907 and batch: 700, loss is 4.073405208587647 and perplexity is 58.756701146492745
At time: 1466.6298415660858 and batch: 750, loss is 4.063334140777588 and perplexity is 58.167928187566815
At time: 1467.7894098758698 and batch: 800, loss is 4.101827673912048 and perplexity is 60.45067080837977
At time: 1468.9491317272186 and batch: 850, loss is 4.132963433265686 and perplexity is 62.3624563511626
At time: 1470.1083943843842 and batch: 900, loss is 4.101386008262634 and perplexity is 60.42397771874396
At time: 1471.2675716876984 and batch: 950, loss is 4.096005368232727 and perplexity is 60.09973115476437
At time: 1472.4270539283752 and batch: 1000, loss is 4.088838310241699 and perplexity is 59.67053277836747
At time: 1473.5866763591766 and batch: 1050, loss is 4.080873546600341 and perplexity is 59.19715874703851
At time: 1474.7731788158417 and batch: 1100, loss is 4.042903099060059 and perplexity is 56.99155500551168
At time: 1475.932849407196 and batch: 1150, loss is 4.0311361598968505 and perplexity is 56.32486896446382
At time: 1477.091413974762 and batch: 1200, loss is 4.0545036458969115 and perplexity is 57.656537833518755
At time: 1478.2507779598236 and batch: 1250, loss is 4.077933311462402 and perplexity is 59.02336080943249
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.356554462961907 and perplexity of 77.98796055849104
Finished 48 epochs...
Completing Train Step...
At time: 1481.1031930446625 and batch: 50, loss is 4.0715289878845216 and perplexity is 58.6465639604974
At time: 1482.2980196475983 and batch: 100, loss is 4.101178531646728 and perplexity is 60.41144245675916
At time: 1483.4565937519073 and batch: 150, loss is 4.025945677757263 and perplexity is 56.033273154170956
At time: 1484.6361200809479 and batch: 200, loss is 4.080387263298035 and perplexity is 59.16837915528042
At time: 1485.7961349487305 and batch: 250, loss is 4.106103415489197 and perplexity is 60.70969562188054
At time: 1486.9638249874115 and batch: 300, loss is 4.11260027885437 and perplexity is 61.10540225336406
At time: 1488.1235032081604 and batch: 350, loss is 4.088892335891724 and perplexity is 59.673756604772024
At time: 1489.2838385105133 and batch: 400, loss is 4.098334074020386 and perplexity is 60.23984882964143
At time: 1490.4442965984344 and batch: 450, loss is 4.023647246360778 and perplexity is 55.904632412497186
At time: 1491.6033816337585 and batch: 500, loss is 4.064870004653931 and perplexity is 58.25733484788678
At time: 1492.7676384449005 and batch: 550, loss is 4.048980174064636 and perplexity is 57.33895146795065
At time: 1493.9309995174408 and batch: 600, loss is 4.078979134559631 and perplexity is 59.08512109297942
At time: 1495.0908477306366 and batch: 650, loss is 4.102505927085876 and perplexity is 60.49168557534121
At time: 1496.2507843971252 and batch: 700, loss is 4.07288321018219 and perplexity is 58.726038245873234
At time: 1497.4116275310516 and batch: 750, loss is 4.06303804397583 and perplexity is 58.150707399701496
At time: 1498.5710611343384 and batch: 800, loss is 4.101669421195984 and perplexity is 60.44110508245747
At time: 1499.7304329872131 and batch: 850, loss is 4.132906556129456 and perplexity is 62.3589094541067
At time: 1500.8899207115173 and batch: 900, loss is 4.101312737464905 and perplexity is 60.419550567886986
At time: 1502.0497789382935 and batch: 950, loss is 4.096020317077636 and perplexity is 60.10062958303972
At time: 1503.2093832492828 and batch: 1000, loss is 4.088854079246521 and perplexity is 59.6714737307055
At time: 1504.4041061401367 and batch: 1050, loss is 4.081100478172302 and perplexity is 59.21059397570969
At time: 1505.5721881389618 and batch: 1100, loss is 4.043143329620361 and perplexity is 57.00524776334674
At time: 1506.7321848869324 and batch: 1150, loss is 4.031420154571533 and perplexity is 56.340867198901094
At time: 1507.892740726471 and batch: 1200, loss is 4.054778771400452 and perplexity is 57.67240279985094
At time: 1509.0548202991486 and batch: 1250, loss is 4.078211088180542 and perplexity is 59.039758402221004
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3565148123859485 and perplexity of 77.98486835224135
Finished 49 epochs...
Completing Train Step...
At time: 1511.9703829288483 and batch: 50, loss is 4.07082097530365 and perplexity is 58.605056151148
At time: 1513.1332821846008 and batch: 100, loss is 4.1002248096466065 and perplexity is 60.35385420098741
At time: 1514.2917504310608 and batch: 150, loss is 4.024953050613403 and perplexity is 55.97768060218153
At time: 1515.450343132019 and batch: 200, loss is 4.079305601119995 and perplexity is 59.104413558232366
At time: 1516.6084923744202 and batch: 250, loss is 4.105046539306641 and perplexity is 60.645566884564346
At time: 1517.7675051689148 and batch: 300, loss is 4.111649580001831 and perplexity is 61.04733702320512
At time: 1518.9277131557465 and batch: 350, loss is 4.087931337356568 and perplexity is 59.61643775816265
At time: 1520.086674451828 and batch: 400, loss is 4.0973747444152835 and perplexity is 60.18208667015704
At time: 1521.2457032203674 and batch: 450, loss is 4.0226462984085085 and perplexity is 55.84870278115611
At time: 1522.4038496017456 and batch: 500, loss is 4.064037079811096 and perplexity is 58.208831069219954
At time: 1523.5625336170197 and batch: 550, loss is 4.048215970993042 and perplexity is 57.29514960400809
At time: 1524.7217700481415 and batch: 600, loss is 4.078339915275574 and perplexity is 59.04736481273275
At time: 1525.8799917697906 and batch: 650, loss is 4.1019928789138795 and perplexity is 60.460658386537595
At time: 1527.0384905338287 and batch: 700, loss is 4.072424039840699 and perplexity is 58.69907918071269
At time: 1528.1973085403442 and batch: 750, loss is 4.062739939689636 and perplexity is 58.133375008138906
At time: 1529.360545873642 and batch: 800, loss is 4.101508731842041 and perplexity is 60.43139362061533
At time: 1530.5239362716675 and batch: 850, loss is 4.132822365760803 and perplexity is 62.35365965552532
At time: 1531.6833047866821 and batch: 900, loss is 4.1012708187103275 and perplexity is 60.417017908658366
At time: 1532.8416819572449 and batch: 950, loss is 4.0959917116165165 and perplexity is 60.098910401406016
At time: 1534.026864528656 and batch: 1000, loss is 4.088806095123291 and perplexity is 59.66861051605169
At time: 1535.1852266788483 and batch: 1050, loss is 4.081080236434937 and perplexity is 59.20939546254722
At time: 1536.3436946868896 and batch: 1100, loss is 4.043329405784607 and perplexity is 57.01585606813815
At time: 1537.5026323795319 and batch: 1150, loss is 4.031613488197326 and perplexity is 56.351760836056236
At time: 1538.6622605323792 and batch: 1200, loss is 4.054935894012451 and perplexity is 57.68146515035058
At time: 1539.8206706047058 and batch: 1250, loss is 4.0783393716812135 and perplexity is 59.04733271492695
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 138 batches
Done Evaluating: Achieved loss of 4.3564617964473085 and perplexity of 77.98073402083963
Finished Training.
<pretraining.langmodel.trainer.TrainLangModel object at 0x7f9c6396a6a0>
Saving Model Parameters and Results...
/home-nfs/siddsach/Interpreting-Attention/interpreting_language/trained_models/langmodel/



RESULTS:
[{'params': {'seq_len': 35, 'anneal': 7.763089457080676, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 8.650584662440988, 'dropout': 0.05567825749440558, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -81.51500638816472}, {'params': {'seq_len': 35, 'anneal': 4.05052866350349, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 29.672411203532512, 'dropout': 0.25293993859191477, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -160.21150864158307}, {'params': {'seq_len': 35, 'anneal': 2.7403205097693326, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 22.362301406722917, 'dropout': 0.46601156043889025, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -179.00895745900365}, {'params': {'seq_len': 35, 'anneal': 2.0663804460052724, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 2.827637190594716, 'dropout': 0.21859992004272255, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -77.05599144545246}, {'params': {'seq_len': 35, 'anneal': 7.00904806048869, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 17.082850391630302, 'dropout': 0.9255275613624224, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -136.63254043581617}, {'params': {'seq_len': 35, 'anneal': 8.0, 'tune_wordvecs': True, 'wordvec_source': 'glove', 'lr': 1.1269305926304014, 'dropout': 0.6554049880811403, 'batch_size': 50, 'wordvec_dim': 200, 'data': 'wikitext', 'num_layers': 1}, 'best_accuracy': -77.98073402083963}]
