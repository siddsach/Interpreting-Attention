Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 40], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [4, 5], 'name': 'anneal'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}, {'type': 'continuous', 'domain': [0, 0.5], 'name': 'clip'}]
SETTINGS FOR THIS RUN
{'clip': 0.3191624920806612, 'rnn_dropout': 0.08296680426935077, 'anneal': 4.659714106128586, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 32.16017481632707, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.2679381198303693, 'seq_len': 35}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.07913475036621094 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.5353634357452393 and batch: 50, loss is 7.199973611831665 and perplexity is 1339.3954197362768
At time: 2.4057931900024414 and batch: 100, loss is 6.145222311019897 and perplexity is 466.4833419241363
At time: 3.278162717819214 and batch: 150, loss is 6.172558088302612 and perplexity is 479.4109142895745
At time: 4.151445150375366 and batch: 200, loss is 5.694632568359375 and perplexity is 297.2675480143258
At time: 5.024325609207153 and batch: 250, loss is 5.725995779037476 and perplexity is 306.73855704163856
At time: 5.8971662521362305 and batch: 300, loss is 5.650658502578735 and perplexity is 284.47873413884537
At time: 6.772672653198242 and batch: 350, loss is 5.662181549072265 and perplexity is 287.7757552006181
At time: 7.647647142410278 and batch: 400, loss is 5.532271480560302 and perplexity is 252.71730199248086
At time: 8.518107414245605 and batch: 450, loss is 5.527527503967285 and perplexity is 251.52125627673234
At time: 9.386746406555176 and batch: 500, loss is 5.4879839229583744 and perplexity is 241.76928966962456
At time: 10.259806156158447 and batch: 550, loss is 5.556389474868775 and perplexity is 258.88643091760486
At time: 11.135384559631348 and batch: 600, loss is 5.481101980209351 and perplexity is 240.1111593827378
At time: 12.020819187164307 and batch: 650, loss is 5.405537176132202 and perplexity is 222.63578299667654
At time: 12.902010440826416 and batch: 700, loss is 5.4891194820404055 and perplexity is 242.04398892136155
At time: 13.782755374908447 and batch: 750, loss is 5.470592832565307 and perplexity is 237.60100863239268
At time: 14.667030096054077 and batch: 800, loss is 5.449417676925659 and perplexity is 232.62266491341956
At time: 15.549485445022583 and batch: 850, loss is 5.4633036231994625 and perplexity is 235.87538200240706
At time: 16.431593418121338 and batch: 900, loss is 5.391833620071411 and perplexity is 219.60569000546838
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.364118602177868 and perplexity of 213.60288257106473
finished 1 epochs...
Completing Train Step...
At time: 18.395569324493408 and batch: 50, loss is 5.37467170715332 and perplexity is 215.86899243053696
At time: 19.147058248519897 and batch: 100, loss is 5.321768274307251 and perplexity is 204.74560848188707
At time: 19.887824058532715 and batch: 150, loss is 5.366790714263916 and perplexity is 214.17441667672185
At time: 20.63249182701111 and batch: 200, loss is 5.226063451766968 and perplexity is 186.05893005138125
At time: 21.37927556037903 and batch: 250, loss is 5.341275939941406 and perplexity is 208.77892978837352
At time: 22.131053686141968 and batch: 300, loss is 5.28581335067749 and perplexity is 197.51476690344862
At time: 22.87467932701111 and batch: 350, loss is 5.302272214889526 and perplexity is 200.79253581478162
At time: 23.627281427383423 and batch: 400, loss is 5.180071411132812 and perplexity is 177.69549997724067
At time: 24.37500286102295 and batch: 450, loss is 5.1655494976043705 and perplexity is 175.13366766687074
At time: 25.11833691596985 and batch: 500, loss is 5.119243698120117 and perplexity is 167.2088614117172
At time: 25.863601446151733 and batch: 550, loss is 5.196533451080322 and perplexity is 180.64494069421454
At time: 26.608446836471558 and batch: 600, loss is 5.145053100585938 and perplexity is 171.58059545932036
At time: 27.354268789291382 and batch: 650, loss is 5.0527699851989745 and perplexity is 156.4552435227729
At time: 28.122119426727295 and batch: 700, loss is 5.133720903396607 and perplexity is 169.64718589736424
At time: 28.86522603034973 and batch: 750, loss is 5.150843458175659 and perplexity is 172.576990420103
At time: 29.61035180091858 and batch: 800, loss is 5.129874000549316 and perplexity is 168.99582332244722
At time: 30.35623526573181 and batch: 850, loss is 5.136429233551025 and perplexity is 170.10726923410166
At time: 31.103187799453735 and batch: 900, loss is 5.108312349319458 and perplexity is 165.39099698215415
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.278038547463613 and perplexity of 195.9850826613864
finished 2 epochs...
Completing Train Step...
At time: 33.01758337020874 and batch: 50, loss is 5.133952302932739 and perplexity is 169.68644671977995
At time: 33.78253531455994 and batch: 100, loss is 5.071141109466553 and perplexity is 159.3560663473891
At time: 34.534913539886475 and batch: 150, loss is 5.103850517272949 and perplexity is 164.65469398329614
At time: 35.28705024719238 and batch: 200, loss is 4.9834997177124025 and perplexity is 145.98439285084183
At time: 36.038787841796875 and batch: 250, loss is 5.112551441192627 and perplexity is 166.09359274522018
At time: 36.788968086242676 and batch: 300, loss is 5.079956340789795 and perplexity is 160.7670368135296
At time: 37.537893533706665 and batch: 350, loss is 5.107598323822021 and perplexity is 165.27294574407483
At time: 38.288086891174316 and batch: 400, loss is 4.986517343521118 and perplexity is 146.42558446318296
At time: 39.04625415802002 and batch: 450, loss is 5.0056093883514405 and perplexity is 149.24800544807786
At time: 39.81270146369934 and batch: 500, loss is 4.959117393493653 and perplexity is 142.4679972085025
At time: 40.56164193153381 and batch: 550, loss is 5.064197549819946 and perplexity is 158.25340063845096
At time: 41.30898141860962 and batch: 600, loss is 5.020759038925171 and perplexity is 151.52627449544215
At time: 42.05815386772156 and batch: 650, loss is 4.907790775299072 and perplexity is 135.34008722598045
At time: 42.80809569358826 and batch: 700, loss is 4.9762617874145505 and perplexity is 144.9315826703471
At time: 43.55763339996338 and batch: 750, loss is 5.008188591003418 and perplexity is 149.6334431468292
At time: 44.30112934112549 and batch: 800, loss is 4.977309808731079 and perplexity is 145.0835536789424
At time: 45.045902729034424 and batch: 850, loss is 5.0507190990448 and perplexity is 156.13470044112194
At time: 45.790653705596924 and batch: 900, loss is 4.999752798080444 and perplexity is 148.3764756190597
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.179705894156678 and perplexity of 177.6305611242659
finished 3 epochs...
Completing Train Step...
At time: 47.680095911026 and batch: 50, loss is 5.0556538295745845 and perplexity is 156.9070873072703
At time: 48.4380362033844 and batch: 100, loss is 4.952741031646728 and perplexity is 141.5624597919204
At time: 49.18203067779541 and batch: 150, loss is 4.98568585395813 and perplexity is 146.3038837214074
At time: 49.924795389175415 and batch: 200, loss is 4.899883165359497 and perplexity is 134.27409090331716
At time: 50.66795539855957 and batch: 250, loss is 5.034095726013184 and perplexity is 153.5606688904797
At time: 51.41238236427307 and batch: 300, loss is 4.998017930984497 and perplexity is 148.11928531370154
At time: 52.156652212142944 and batch: 350, loss is 5.011075220108032 and perplexity is 150.06600341901242
At time: 52.90250611305237 and batch: 400, loss is 4.932523994445801 and perplexity is 138.72922260825297
At time: 53.64777207374573 and batch: 450, loss is 4.941555089950562 and perplexity is 139.98777396165005
At time: 54.393332719802856 and batch: 500, loss is 4.88476559638977 and perplexity is 132.25945960569075
At time: 55.13847899436951 and batch: 550, loss is 4.966848888397217 and perplexity is 143.57375688232912
At time: 55.88299083709717 and batch: 600, loss is 4.92869779586792 and perplexity is 138.19943124381754
At time: 56.63153672218323 and batch: 650, loss is 4.85487928390503 and perplexity is 128.36519453437526
At time: 57.38230347633362 and batch: 700, loss is 4.930193634033203 and perplexity is 138.40630991742546
At time: 58.133652687072754 and batch: 750, loss is 4.951091737747192 and perplexity is 141.32917412177994
At time: 58.88364124298096 and batch: 800, loss is 4.915611238479614 and perplexity is 136.4026588806972
At time: 59.63176345825195 and batch: 850, loss is 4.973160657882691 and perplexity is 144.48282724322289
At time: 60.380598306655884 and batch: 900, loss is 4.926548709869385 and perplexity is 137.90274769454467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.107699041497217 and perplexity of 165.2895924892397
finished 4 epochs...
Completing Train Step...
At time: 62.27499294281006 and batch: 50, loss is 4.991768455505371 and perplexity is 147.196503925317
At time: 63.01419377326965 and batch: 100, loss is 4.915235338211059 and perplexity is 136.3513947203018
At time: 63.76006579399109 and batch: 150, loss is 4.954978256225586 and perplexity is 141.87952134311243
At time: 64.50492024421692 and batch: 200, loss is 4.85879693031311 and perplexity is 128.86907033682414
At time: 65.2502429485321 and batch: 250, loss is 4.991532239913941 and perplexity is 147.16173792238345
At time: 66.02230858802795 and batch: 300, loss is 4.959579744338989 and perplexity is 142.53388263736343
At time: 66.7837266921997 and batch: 350, loss is 4.9672778224945064 and perplexity is 143.63535377169057
At time: 67.53700423240662 and batch: 400, loss is 4.878340444564819 and perplexity is 131.41239666551255
At time: 68.28836584091187 and batch: 450, loss is 4.883342294692993 and perplexity is 132.07134839367893
At time: 69.03580069541931 and batch: 500, loss is 4.8267245388031 and perplexity is 124.80150795100825
At time: 69.78781032562256 and batch: 550, loss is 4.929886417388916 and perplexity is 138.36379572621246
At time: 70.53507733345032 and batch: 600, loss is 4.894684467315674 and perplexity is 133.57785178614284
At time: 71.28264331817627 and batch: 650, loss is 4.790847845077515 and perplexity is 120.40340883747206
At time: 72.03046536445618 and batch: 700, loss is 4.833517045974731 and perplexity is 125.6521086741254
At time: 72.77454090118408 and batch: 750, loss is 4.903284416198731 and perplexity is 134.73156832439315
At time: 73.51712989807129 and batch: 800, loss is 4.85598578453064 and perplexity is 128.50730931298008
At time: 74.26224637031555 and batch: 850, loss is 4.921569318771362 and perplexity is 137.2177827497262
At time: 75.00675964355469 and batch: 900, loss is 4.848388500213623 and perplexity is 127.53470201254326
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.084372324486301 and perplexity of 161.47855128893065
finished 5 epochs...
Completing Train Step...
At time: 76.88855075836182 and batch: 50, loss is 4.939073572158813 and perplexity is 139.64082247112714
At time: 77.62740349769592 and batch: 100, loss is 4.850238933563232 and perplexity is 127.77091495913523
At time: 78.36672329902649 and batch: 150, loss is 4.862366752624512 and perplexity is 129.32993212729224
At time: 79.10653448104858 and batch: 200, loss is 4.786890840530395 and perplexity is 119.92791339047399
At time: 79.844642162323 and batch: 250, loss is 4.931818885803223 and perplexity is 138.63143791288195
At time: 80.58545851707458 and batch: 300, loss is 4.907493934631348 and perplexity is 135.29991874623255
At time: 81.33456921577454 and batch: 350, loss is 4.9063489055633545 and perplexity is 135.14508506781894
At time: 82.08442878723145 and batch: 400, loss is 4.83351580619812 and perplexity is 125.6519528936765
At time: 82.83274030685425 and batch: 450, loss is 4.8428076171875 and perplexity is 126.82492818342952
At time: 83.58465790748596 and batch: 500, loss is 4.772902393341065 and perplexity is 118.26198713337034
At time: 84.36015701293945 and batch: 550, loss is 4.863967189788818 and perplexity is 129.53708227830992
At time: 85.11589169502258 and batch: 600, loss is 4.851608924865722 and perplexity is 127.94607996120821
At time: 85.86005163192749 and batch: 650, loss is 4.780440587997436 and perplexity is 119.15683755372018
At time: 86.60268020629883 and batch: 700, loss is 4.810363264083862 and perplexity is 122.77620960472504
At time: 87.34412789344788 and batch: 750, loss is 4.862483930587769 and perplexity is 129.3450876332554
At time: 88.09324097633362 and batch: 800, loss is 4.796989059448242 and perplexity is 121.14510711510555
At time: 88.83758497238159 and batch: 850, loss is 4.870426721572876 and perplexity is 130.3765395099361
At time: 89.5856671333313 and batch: 900, loss is 4.8415907192230225 and perplexity is 126.6706890521542
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.080765397581335 and perplexity of 160.89715910758443
finished 6 epochs...
Completing Train Step...
At time: 91.47482943534851 and batch: 50, loss is 4.904158983230591 and perplexity is 134.84945165312317
At time: 92.24939203262329 and batch: 100, loss is 4.807678480148315 and perplexity is 122.44702410321656
At time: 92.99942564964294 and batch: 150, loss is 4.8505986881256105 and perplexity is 127.81688939799226
At time: 93.74988055229187 and batch: 200, loss is 4.768488826751709 and perplexity is 117.741180131783
At time: 94.49776434898376 and batch: 250, loss is 4.908771533966064 and perplexity is 135.4728883017768
At time: 95.24773406982422 and batch: 300, loss is 4.885961542129516 and perplexity is 132.41772936510532
At time: 95.99855613708496 and batch: 350, loss is 4.892472772598267 and perplexity is 133.28274482053854
At time: 96.74806571006775 and batch: 400, loss is 4.79860562324524 and perplexity is 121.3411042877351
At time: 97.49833178520203 and batch: 450, loss is 4.8132382583618165 and perplexity is 123.12969840045727
At time: 98.242342710495 and batch: 500, loss is 4.759048099517822 and perplexity is 116.63484827164532
At time: 98.99544739723206 and batch: 550, loss is 4.842577934265137 and perplexity is 126.7958020083164
At time: 99.74870133399963 and batch: 600, loss is 4.801540956497193 and perplexity is 121.69780412631233
At time: 100.50914335250854 and batch: 650, loss is 4.723055095672607 and perplexity is 112.51146141196507
At time: 101.27213168144226 and batch: 700, loss is 4.751984643936157 and perplexity is 115.8139059596117
At time: 102.03640294075012 and batch: 750, loss is 4.837309417724609 and perplexity is 126.12953289424016
At time: 102.79164600372314 and batch: 800, loss is 4.786080684661865 and perplexity is 119.83079243450848
At time: 103.57230234146118 and batch: 850, loss is 4.835324296951294 and perplexity is 125.87939889360564
At time: 104.31424522399902 and batch: 900, loss is 4.791573066711425 and perplexity is 120.49075966488395
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.06030733291417 and perplexity of 157.63895651924614
finished 7 epochs...
Completing Train Step...
At time: 106.16205620765686 and batch: 50, loss is 4.879915733337402 and perplexity is 131.61957227642202
At time: 106.92828059196472 and batch: 100, loss is 4.785739488601685 and perplexity is 119.7899136144843
At time: 107.67020344734192 and batch: 150, loss is 4.802952518463135 and perplexity is 121.86970941689715
At time: 108.41504001617432 and batch: 200, loss is 4.7220855140686036 and perplexity is 112.40242523701714
At time: 109.1572470664978 and batch: 250, loss is 4.858712110519409 and perplexity is 128.85814015241831
At time: 109.8992989063263 and batch: 300, loss is 4.856408157348633 and perplexity is 128.56159877173602
At time: 110.64067673683167 and batch: 350, loss is 4.845121726989746 and perplexity is 127.11875483535302
At time: 111.38364124298096 and batch: 400, loss is 4.7660001850128175 and perplexity is 117.44852881969645
At time: 112.12639784812927 and batch: 450, loss is 4.78041932106018 and perplexity is 119.15430347967843
At time: 112.86789226531982 and batch: 500, loss is 4.70415355682373 and perplexity is 110.40479400115731
At time: 113.60925507545471 and batch: 550, loss is 4.802090950012207 and perplexity is 121.76475553910706
At time: 114.35050344467163 and batch: 600, loss is 4.786931505203247 and perplexity is 119.93279031899654
At time: 115.09168267250061 and batch: 650, loss is 4.7003695583343506 and perplexity is 109.98781185489393
At time: 115.83209109306335 and batch: 700, loss is 4.751661758422852 and perplexity is 115.77651736358091
At time: 116.57288599014282 and batch: 750, loss is 4.789965133666993 and perplexity is 120.29717426875641
At time: 117.31404495239258 and batch: 800, loss is 4.762135210037232 and perplexity is 116.99546929074454
At time: 118.05472207069397 and batch: 850, loss is 4.800923442840576 and perplexity is 121.62267726860166
At time: 118.7960958480835 and batch: 900, loss is 4.757082872390747 and perplexity is 116.40585938514381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.051700957833904 and perplexity of 156.28807795424194
finished 8 epochs...
Completing Train Step...
At time: 120.64365124702454 and batch: 50, loss is 4.848610992431641 and perplexity is 127.56308064816395
At time: 121.38377714157104 and batch: 100, loss is 4.7592589378356935 and perplexity is 116.65944195941485
At time: 122.12396001815796 and batch: 150, loss is 4.797565336227417 and perplexity is 121.21494034694904
At time: 122.87736749649048 and batch: 200, loss is 4.70055890083313 and perplexity is 110.00863919371378
At time: 123.61815142631531 and batch: 250, loss is 4.840339975357056 and perplexity is 126.51235550282311
At time: 124.35854625701904 and batch: 300, loss is 4.806820058822632 and perplexity is 122.34195806837424
At time: 125.09693551063538 and batch: 350, loss is 4.833469486236572 and perplexity is 125.6461328348435
At time: 125.8356192111969 and batch: 400, loss is 4.767122468948364 and perplexity is 117.58041340897971
At time: 126.5755398273468 and batch: 450, loss is 4.759846467971801 and perplexity is 116.72800303610467
At time: 127.31910157203674 and batch: 500, loss is 4.697222557067871 and perplexity is 109.64222413939243
At time: 128.06008577346802 and batch: 550, loss is 4.7880567359924315 and perplexity is 120.06781834189356
At time: 128.80282473564148 and batch: 600, loss is 4.7496081066131595 and perplexity is 115.53899668504128
At time: 129.5443708896637 and batch: 650, loss is 4.666116514205933 and perplexity is 106.28418680391142
At time: 130.28791522979736 and batch: 700, loss is 4.708590650558472 and perplexity is 110.89575884391799
At time: 131.038094997406 and batch: 750, loss is 4.801790657043457 and perplexity is 121.72819592874882
At time: 131.78785300254822 and batch: 800, loss is 4.741104898452758 and perplexity is 114.56070972894132
At time: 132.53775453567505 and batch: 850, loss is 4.783094387054444 and perplexity is 119.47347581818909
At time: 133.288494348526 and batch: 900, loss is 4.737780561447144 and perplexity is 114.18050363893224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.006808346264983 and perplexity of 149.42705484018447
finished 9 epochs...
Completing Train Step...
At time: 135.1635901927948 and batch: 50, loss is 4.8220811939239505 and perplexity is 124.2233548291022
At time: 135.91171789169312 and batch: 100, loss is 4.723810796737671 and perplexity is 112.59651857803357
At time: 136.65873789787292 and batch: 150, loss is 4.751084699630737 and perplexity is 115.7097267792162
At time: 137.40514278411865 and batch: 200, loss is 4.667518234252929 and perplexity is 106.4332719426366
At time: 138.1533784866333 and batch: 250, loss is 4.820937051773071 and perplexity is 124.08130692970892
At time: 138.89975690841675 and batch: 300, loss is 4.784687957763672 and perplexity is 119.6640170299068
At time: 139.64649319648743 and batch: 350, loss is 4.791079759597778 and perplexity is 120.43133537443022
At time: 140.39317274093628 and batch: 400, loss is 4.717845907211304 and perplexity is 111.92689189369682
At time: 141.1529562473297 and batch: 450, loss is 4.7522553443908695 and perplexity is 115.84526108034824
At time: 141.9027328491211 and batch: 500, loss is 4.667890281677246 and perplexity is 106.47287753444678
At time: 142.64557147026062 and batch: 550, loss is 4.758287878036499 and perplexity is 116.54621364975884
At time: 143.39263010025024 and batch: 600, loss is 4.734387502670288 and perplexity is 113.7937390075414
At time: 144.1473150253296 and batch: 650, loss is 4.644009408950805 and perplexity is 103.96033258702654
At time: 144.9036796092987 and batch: 700, loss is 4.696694660186767 and perplexity is 109.5843596258239
At time: 145.64963340759277 and batch: 750, loss is 4.762639703750611 and perplexity is 117.05450766048655
At time: 146.39284539222717 and batch: 800, loss is 4.70199517250061 and perplexity is 110.16675500681595
At time: 147.14958333969116 and batch: 850, loss is 4.759637184143067 and perplexity is 116.70357630885738
At time: 147.90380334854126 and batch: 900, loss is 4.710803804397583 and perplexity is 111.14146000529072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.00710098057577 and perplexity of 149.47078872209565
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.8000946044922 and batch: 50, loss is 4.774634323120117 and perplexity is 118.4669860612479
At time: 150.57041454315186 and batch: 100, loss is 4.642722969055176 and perplexity is 103.82667985415966
At time: 151.32237267494202 and batch: 150, loss is 4.662651510238647 and perplexity is 105.91654897607147
At time: 152.06736707687378 and batch: 200, loss is 4.563380165100098 and perplexity is 95.90711443378912
At time: 152.8129642009735 and batch: 250, loss is 4.715613822937012 and perplexity is 111.67734025218753
At time: 153.55915188789368 and batch: 300, loss is 4.678030414581299 and perplexity is 107.55801910515002
At time: 154.3152573108673 and batch: 350, loss is 4.670719451904297 and perplexity is 106.77453394701641
At time: 155.07117676734924 and batch: 400, loss is 4.5933292198181155 and perplexity is 98.82288621864637
At time: 155.82331943511963 and batch: 450, loss is 4.60487732887268 and perplexity is 99.9707185763048
At time: 156.57553362846375 and batch: 500, loss is 4.5269925498962404 and perplexity is 92.48001340702754
At time: 157.32842659950256 and batch: 550, loss is 4.630013313293457 and perplexity is 102.51542891939746
At time: 158.07619643211365 and batch: 600, loss is 4.592005176544189 and perplexity is 98.69212702535653
At time: 158.829735994339 and batch: 650, loss is 4.475697412490844 and perplexity is 87.85585081662606
At time: 159.58361053466797 and batch: 700, loss is 4.529374561309814 and perplexity is 92.70056442773138
At time: 160.36387062072754 and batch: 750, loss is 4.604149827957153 and perplexity is 99.89801623573082
At time: 161.1167438030243 and batch: 800, loss is 4.532034034729004 and perplexity is 92.94742723180272
At time: 161.87414717674255 and batch: 850, loss is 4.581296672821045 and perplexity is 97.64092048554818
At time: 162.6287603378296 and batch: 900, loss is 4.535942640304565 and perplexity is 93.31143297814434
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.8808046105789815 and perplexity of 131.7366179306676
finished 11 epochs...
Completing Train Step...
At time: 164.47338128089905 and batch: 50, loss is 4.68967679977417 and perplexity is 108.8180041198212
At time: 165.23831391334534 and batch: 100, loss is 4.579691953659058 and perplexity is 97.48435988094897
At time: 165.99632859230042 and batch: 150, loss is 4.603051624298096 and perplexity is 99.7883680877869
At time: 166.75910568237305 and batch: 200, loss is 4.516385917663574 and perplexity is 91.50429560329059
At time: 167.51919317245483 and batch: 250, loss is 4.6761516571044925 and perplexity is 107.35613337910789
At time: 168.2778503894806 and batch: 300, loss is 4.639858331680298 and perplexity is 103.5296796685582
At time: 169.03076696395874 and batch: 350, loss is 4.626152973175049 and perplexity is 102.12044736845628
At time: 169.77979397773743 and batch: 400, loss is 4.561387138366699 and perplexity is 95.7161593433082
At time: 170.5320405960083 and batch: 450, loss is 4.570647411346435 and perplexity is 96.60663376212926
At time: 171.2940857410431 and batch: 500, loss is 4.488777799606323 and perplexity is 89.0125881489515
At time: 172.05516123771667 and batch: 550, loss is 4.589904375076294 and perplexity is 98.48501208988503
At time: 172.8004767894745 and batch: 600, loss is 4.565433645248413 and perplexity is 96.10426013747892
At time: 173.54338097572327 and batch: 650, loss is 4.441586484909058 and perplexity is 84.90954258238307
At time: 174.28457903862 and batch: 700, loss is 4.503831272125244 and perplexity is 90.36267293581213
At time: 175.0320246219635 and batch: 750, loss is 4.581587057113648 and perplexity is 97.66927799226046
At time: 175.77758526802063 and batch: 800, loss is 4.507424097061158 and perplexity is 90.68791411786462
At time: 176.5200936794281 and batch: 850, loss is 4.5633914756774905 and perplexity is 95.90819920476413
At time: 177.26345944404602 and batch: 900, loss is 4.518676099777221 and perplexity is 91.71409725469822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.871189065175514 and perplexity of 130.47596912571586
finished 12 epochs...
Completing Train Step...
At time: 179.12255954742432 and batch: 50, loss is 4.654942646026611 and perplexity is 105.10319174036682
At time: 179.8663330078125 and batch: 100, loss is 4.552544317245483 and perplexity is 94.87348974714976
At time: 180.61846470832825 and batch: 150, loss is 4.57234486579895 and perplexity is 96.77075838037487
At time: 181.37269377708435 and batch: 200, loss is 4.4782569885253904 and perplexity is 88.08101258327426
At time: 182.12628769874573 and batch: 250, loss is 4.633681316375732 and perplexity is 102.89214630655256
At time: 182.8713116645813 and batch: 300, loss is 4.5925406551361085 and perplexity is 98.7449886984536
At time: 183.61228895187378 and batch: 350, loss is 4.578845252990723 and perplexity is 97.4018547417928
At time: 184.3541338443756 and batch: 400, loss is 4.522196235656739 and perplexity is 92.037512237482
At time: 185.09708619117737 and batch: 450, loss is 4.539550981521606 and perplexity is 93.64874066240822
At time: 185.83929228782654 and batch: 500, loss is 4.459215297698974 and perplexity is 86.41966876964285
At time: 186.5814232826233 and batch: 550, loss is 4.559160680770874 and perplexity is 95.50328843525556
At time: 187.32991456985474 and batch: 600, loss is 4.536694049835205 and perplexity is 93.38157442737824
At time: 188.07982540130615 and batch: 650, loss is 4.41167423248291 and perplexity is 82.40731707961798
At time: 188.82947826385498 and batch: 700, loss is 4.465800485610962 and perplexity is 86.99063642912876
At time: 189.580641746521 and batch: 750, loss is 4.555894727706909 and perplexity is 95.19188796422772
At time: 190.33135771751404 and batch: 800, loss is 4.482063045501709 and perplexity is 88.41689271970111
At time: 191.07954668998718 and batch: 850, loss is 4.545215034484864 and perplexity is 94.18067712550344
At time: 191.82966136932373 and batch: 900, loss is 4.498967218399048 and perplexity is 89.92421125486023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.864273698362585 and perplexity of 129.57679259011366
finished 13 epochs...
Completing Train Step...
At time: 193.7248067855835 and batch: 50, loss is 4.627638359069824 and perplexity is 102.27224835415616
At time: 194.47771286964417 and batch: 100, loss is 4.512544012069702 and perplexity is 91.15341918676752
At time: 195.23157238960266 and batch: 150, loss is 4.537938947677612 and perplexity is 93.49789733795086
At time: 195.98427867889404 and batch: 200, loss is 4.449283151626587 and perplexity is 85.56558446204313
At time: 196.73685669898987 and batch: 250, loss is 4.6212064647674564 and perplexity is 101.61655499831761
At time: 197.4914050102234 and batch: 300, loss is 4.5920641994476314 and perplexity is 98.69795229315085
At time: 198.26087641716003 and batch: 350, loss is 4.557821283340454 and perplexity is 95.37545720365091
At time: 199.0143928527832 and batch: 400, loss is 4.499938507080078 and perplexity is 90.01159605446608
At time: 199.76739239692688 and batch: 450, loss is 4.522601690292358 and perplexity is 92.07483683967475
At time: 200.52133107185364 and batch: 500, loss is 4.433027982711792 and perplexity is 84.18594494842513
At time: 201.27227401733398 and batch: 550, loss is 4.535533218383789 and perplexity is 93.27323705168241
At time: 202.01827907562256 and batch: 600, loss is 4.527364778518677 and perplexity is 92.51444352256061
At time: 202.76374506950378 and batch: 650, loss is 4.398936061859131 and perplexity is 81.36425606258433
At time: 203.50809001922607 and batch: 700, loss is 4.447608861923218 and perplexity is 85.42244274880233
At time: 204.25383687019348 and batch: 750, loss is 4.5180228805541995 and perplexity is 91.65420740607911
At time: 204.999169588089 and batch: 800, loss is 4.450156345367431 and perplexity is 85.64033242480427
At time: 205.74379897117615 and batch: 850, loss is 4.517400693893433 and perplexity is 91.59719911756531
At time: 206.48931503295898 and batch: 900, loss is 4.476324977874756 and perplexity is 87.91100341149229
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.842172701064855 and perplexity of 126.74443054909966
finished 14 epochs...
Completing Train Step...
At time: 208.34152030944824 and batch: 50, loss is 4.595950765609741 and perplexity is 99.08229481732376
At time: 209.10235571861267 and batch: 100, loss is 4.488494729995727 and perplexity is 88.98739495616789
At time: 209.84243488311768 and batch: 150, loss is 4.507809257507324 and perplexity is 90.72285024290301
At time: 210.58257222175598 and batch: 200, loss is 4.416977586746216 and perplexity is 82.84551320357852
At time: 211.32213854789734 and batch: 250, loss is 4.589574022293091 and perplexity is 98.4524826654261
At time: 212.06316494941711 and batch: 300, loss is 4.573076372146606 and perplexity is 96.8415727018027
At time: 212.8037462234497 and batch: 350, loss is 4.53294340133667 and perplexity is 93.03198896137502
At time: 213.55943036079407 and batch: 400, loss is 4.4706808280944825 and perplexity is 87.41621817540123
At time: 214.29923391342163 and batch: 450, loss is 4.503493347167969 and perplexity is 90.33214229224475
At time: 215.03805875778198 and batch: 500, loss is 4.408491134643555 and perplexity is 82.14542356414424
At time: 215.78062653541565 and batch: 550, loss is 4.512296657562256 and perplexity is 91.1308747660096
At time: 216.53631019592285 and batch: 600, loss is 4.513752403259278 and perplexity is 91.26363475376111
At time: 217.30163168907166 and batch: 650, loss is 4.387602586746215 and perplexity is 80.44732213018334
At time: 218.04503202438354 and batch: 700, loss is 4.4349026775360105 and perplexity is 84.34391593111822
At time: 218.7890169620514 and batch: 750, loss is 4.507395782470703 and perplexity is 90.68534636306984
At time: 219.53273105621338 and batch: 800, loss is 4.43639533996582 and perplexity is 84.46990693323652
At time: 220.28762078285217 and batch: 850, loss is 4.505824012756348 and perplexity is 90.5429218407171
At time: 221.03959965705872 and batch: 900, loss is 4.462095203399659 and perplexity is 86.66890798684217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.849321757277397 and perplexity of 127.6537802307241
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 222.8949692249298 and batch: 50, loss is 4.5911973857879635 and perplexity is 98.61243662834954
At time: 223.6532723903656 and batch: 100, loss is 4.461363501548767 and perplexity is 86.60551538152511
At time: 224.40772676467896 and batch: 150, loss is 4.471915864944458 and perplexity is 87.52424712226697
At time: 225.1718966960907 and batch: 200, loss is 4.370381679534912 and perplexity is 79.07380679439105
At time: 225.9257583618164 and batch: 250, loss is 4.53686339378357 and perplexity is 93.39738937094097
At time: 226.67533993721008 and batch: 300, loss is 4.508686857223511 and perplexity is 90.80250353726983
At time: 227.42090940475464 and batch: 350, loss is 4.466186590194702 and perplexity is 87.02423039757181
At time: 228.16857242584229 and batch: 400, loss is 4.385708470344543 and perplexity is 80.29508975629743
At time: 228.9160351753235 and batch: 450, loss is 4.4148001384735105 and perplexity is 82.66531763840952
At time: 229.6641833782196 and batch: 500, loss is 4.320051784515381 and perplexity is 75.19252199951735
At time: 230.4147276878357 and batch: 550, loss is 4.403399591445923 and perplexity is 81.7282395480489
At time: 231.16547417640686 and batch: 600, loss is 4.397718725204467 and perplexity is 81.26526863403666
At time: 231.9136998653412 and batch: 650, loss is 4.277259278297424 and perplexity is 72.04272013692886
At time: 232.66345357894897 and batch: 700, loss is 4.319741420745849 and perplexity is 75.16918858605938
At time: 233.41427540779114 and batch: 750, loss is 4.384868612289429 and perplexity is 80.2276815889879
At time: 234.16215085983276 and batch: 800, loss is 4.30438349723816 and perplexity is 74.02356565001517
At time: 234.9055459499359 and batch: 850, loss is 4.380781106948852 and perplexity is 79.90041980981508
At time: 235.65205645561218 and batch: 900, loss is 4.348988819122314 and perplexity is 77.4001577821222
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.792784808433219 and perplexity of 120.63685184057697
finished 16 epochs...
Completing Train Step...
At time: 237.56448245048523 and batch: 50, loss is 4.541705436706543 and perplexity is 93.85072017707049
At time: 238.3102366924286 and batch: 100, loss is 4.421595048904419 and perplexity is 83.22893375948588
At time: 239.05711221694946 and batch: 150, loss is 4.436824979782105 and perplexity is 84.50620636581532
At time: 239.81072545051575 and batch: 200, loss is 4.340192890167236 and perplexity is 76.72233689549259
At time: 240.56316494941711 and batch: 250, loss is 4.504382400512696 and perplexity is 90.4124880960466
At time: 241.31597542762756 and batch: 300, loss is 4.481818475723267 and perplexity is 88.39527126392318
At time: 242.06835341453552 and batch: 350, loss is 4.449748983383179 and perplexity is 85.6054529138314
At time: 242.82385635375977 and batch: 400, loss is 4.369883027076721 and perplexity is 79.03438627564064
At time: 243.57815885543823 and batch: 450, loss is 4.401439437866211 and perplexity is 81.56819655240376
At time: 244.33077001571655 and batch: 500, loss is 4.308444013595581 and perplexity is 74.32475061859466
At time: 245.07918238639832 and batch: 550, loss is 4.391548776626587 and perplexity is 80.7654097429532
At time: 245.82808780670166 and batch: 600, loss is 4.386086053848267 and perplexity is 80.32541358214738
At time: 246.5780839920044 and batch: 650, loss is 4.264375658035278 and perplexity is 71.12050259469909
At time: 247.3277714252472 and batch: 700, loss is 4.306582741737365 and perplexity is 74.18654071490192
At time: 248.0765311717987 and batch: 750, loss is 4.376974382400513 and perplexity is 79.59683911095453
At time: 248.83138346672058 and batch: 800, loss is 4.294971914291382 and perplexity is 73.33015488673901
At time: 249.5867211818695 and batch: 850, loss is 4.374219017028809 and perplexity is 79.37782261057673
At time: 250.34254217147827 and batch: 900, loss is 4.345256938934326 and perplexity is 77.11184797019129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.791587516053082 and perplexity of 120.4925006896151
finished 17 epochs...
Completing Train Step...
At time: 252.3161497116089 and batch: 50, loss is 4.522809991836548 and perplexity is 92.09401816805024
At time: 253.06980514526367 and batch: 100, loss is 4.400417666435242 and perplexity is 81.48489506427859
At time: 253.82776594161987 and batch: 150, loss is 4.419481706619263 and perplexity is 83.05322826274637
At time: 254.59150099754333 and batch: 200, loss is 4.321380462646484 and perplexity is 75.2924950604229
At time: 255.3637616634369 and batch: 250, loss is 4.483988447189331 and perplexity is 88.58729474769157
At time: 256.1317811012268 and batch: 300, loss is 4.46161750793457 and perplexity is 86.62751652957553
At time: 256.8953664302826 and batch: 350, loss is 4.428671522140503 and perplexity is 83.81998991172526
At time: 257.6537253856659 and batch: 400, loss is 4.3523644351959225 and perplexity is 77.6618724744773
At time: 258.40224504470825 and batch: 450, loss is 4.38392050743103 and perplexity is 80.15165338134702
At time: 259.15846157073975 and batch: 500, loss is 4.293594083786011 and perplexity is 73.22918793601796
At time: 259.917715549469 and batch: 550, loss is 4.378545789718628 and perplexity is 79.72201649300087
At time: 260.6761176586151 and batch: 600, loss is 4.372636880874634 and perplexity is 79.25233538270106
At time: 261.43492007255554 and batch: 650, loss is 4.2492144632339475 and perplexity is 70.05036359203135
At time: 262.19794392585754 and batch: 700, loss is 4.289958066940308 and perplexity is 72.96340885650423
At time: 262.96481370925903 and batch: 750, loss is 4.368360643386841 and perplexity is 78.91415715568131
At time: 263.7269916534424 and batch: 800, loss is 4.285121726989746 and perplexity is 72.61138494712897
At time: 264.49356412887573 and batch: 850, loss is 4.364686603546143 and perplexity is 78.62475536070103
At time: 265.2588336467743 and batch: 900, loss is 4.336065950393677 and perplexity is 76.40636088728154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.789649440817637 and perplexity of 120.25920330493835
finished 18 epochs...
Completing Train Step...
At time: 267.1917028427124 and batch: 50, loss is 4.50713963508606 and perplexity is 90.66212052351948
At time: 267.95949363708496 and batch: 100, loss is 4.382629880905151 and perplexity is 80.04827425766963
At time: 268.7093765735626 and batch: 150, loss is 4.396987438201904 and perplexity is 81.20586212357672
At time: 269.4663915634155 and batch: 200, loss is 4.302329821586609 and perplexity is 73.87170124908509
At time: 270.2189202308655 and batch: 250, loss is 4.467536869049073 and perplexity is 87.14181674500894
At time: 270.96591663360596 and batch: 300, loss is 4.444449491500855 and perplexity is 85.152987488399
At time: 271.7197365760803 and batch: 350, loss is 4.415414142608642 and perplexity is 82.71609007091814
At time: 272.4732344150543 and batch: 400, loss is 4.338789639472961 and perplexity is 76.61475172511405
At time: 273.22403359413147 and batch: 450, loss is 4.373764400482178 and perplexity is 79.34174434049307
At time: 273.9674141407013 and batch: 500, loss is 4.287318477630615 and perplexity is 72.7710693827732
At time: 274.7260434627533 and batch: 550, loss is 4.371841449737548 and perplexity is 79.18932067270465
At time: 275.473744392395 and batch: 600, loss is 4.366067667007446 and perplexity is 78.73341615393815
At time: 276.2207417488098 and batch: 650, loss is 4.24295597076416 and perplexity is 69.61332295328427
At time: 276.972021818161 and batch: 700, loss is 4.275782432556152 and perplexity is 71.93640267907128
At time: 277.7273530960083 and batch: 750, loss is 4.355638046264648 and perplexity is 77.91652382715625
At time: 278.4806761741638 and batch: 800, loss is 4.280933980941772 and perplexity is 72.30794271973392
At time: 279.24197793006897 and batch: 850, loss is 4.360057277679443 and perplexity is 78.26161693851671
At time: 279.999685049057 and batch: 900, loss is 4.329294624328614 and perplexity is 75.89073620584273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.78877279203232 and perplexity of 120.15382441732356
finished 19 epochs...
Completing Train Step...
At time: 281.93210530281067 and batch: 50, loss is 4.483138828277588 and perplexity is 88.51206127115165
At time: 282.7038149833679 and batch: 100, loss is 4.362308039665222 and perplexity is 78.43796359359463
At time: 283.45182943344116 and batch: 150, loss is 4.377444620132446 and perplexity is 79.63427734979355
At time: 284.20031332969666 and batch: 200, loss is 4.285221714973449 and perplexity is 72.61864557608551
At time: 284.9514436721802 and batch: 250, loss is 4.447540369033813 and perplexity is 85.416592119244
At time: 285.70259976387024 and batch: 300, loss is 4.424301481246948 and perplexity is 83.45449232947074
At time: 286.46182560920715 and batch: 350, loss is 4.39664439201355 and perplexity is 81.1780095397377
At time: 287.2187316417694 and batch: 400, loss is 4.319653468132019 and perplexity is 75.16257755017693
At time: 287.9725956916809 and batch: 450, loss is 4.349776201248169 and perplexity is 77.46112528210818
At time: 288.7357966899872 and batch: 500, loss is 4.263142538070679 and perplexity is 71.03285653321745
At time: 289.490083694458 and batch: 550, loss is 4.34992744922638 and perplexity is 77.47284200674018
At time: 290.2396364212036 and batch: 600, loss is 4.349030900001526 and perplexity is 77.40341491734385
At time: 290.9867444038391 and batch: 650, loss is 4.228702459335327 and perplexity is 68.6281265922965
At time: 291.7327733039856 and batch: 700, loss is 4.260754823684692 and perplexity is 70.86345268428242
At time: 292.4806728363037 and batch: 750, loss is 4.346315631866455 and perplexity is 77.19352896853935
At time: 293.2427363395691 and batch: 800, loss is 4.270712223052978 and perplexity is 71.57259312129374
At time: 293.99046874046326 and batch: 850, loss is 4.346401543617248 and perplexity is 77.20016108464723
At time: 294.7403266429901 and batch: 900, loss is 4.32751238822937 and perplexity is 75.75560145296569
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.793422333181721 and perplexity of 120.71378534010591
Annealing...
finished 20 epochs...
Completing Train Step...
At time: 296.6333875656128 and batch: 50, loss is 4.47165057182312 and perplexity is 87.50103062128028
At time: 297.385027885437 and batch: 100, loss is 4.336651077270508 and perplexity is 76.45108138490416
At time: 298.13509702682495 and batch: 150, loss is 4.349994411468506 and perplexity is 77.47802993564059
At time: 298.89237785339355 and batch: 200, loss is 4.259065127372741 and perplexity is 70.74381607286449
At time: 299.6396269798279 and batch: 250, loss is 4.4206446933746335 and perplexity is 83.14987425531827
At time: 300.3844668865204 and batch: 300, loss is 4.388258328437805 and perplexity is 80.50009209312233
At time: 301.1349365711212 and batch: 350, loss is 4.359439134597778 and perplexity is 78.21325501028186
At time: 301.8915331363678 and batch: 400, loss is 4.282811846733093 and perplexity is 72.44385490428925
At time: 302.6457931995392 and batch: 450, loss is 4.309375648498535 and perplexity is 74.39402641528872
At time: 303.39941453933716 and batch: 500, loss is 4.213923001289368 and perplexity is 67.62129858760005
At time: 304.15295481681824 and batch: 550, loss is 4.2947096300125125 and perplexity is 73.31092406202693
At time: 304.8983385562897 and batch: 600, loss is 4.29706446647644 and perplexity is 73.4837627227332
At time: 305.64692902565 and batch: 650, loss is 4.167550444602966 and perplexity is 64.5571220207503
At time: 306.39680004119873 and batch: 700, loss is 4.201305270195007 and perplexity is 66.7734315537326
At time: 307.1557755470276 and batch: 750, loss is 4.283226490020752 and perplexity is 72.47389949092008
At time: 307.90495562553406 and batch: 800, loss is 4.204691262245178 and perplexity is 66.9999090712939
At time: 308.65768361091614 and batch: 850, loss is 4.269514389038086 and perplexity is 71.4869123606284
At time: 309.40641927719116 and batch: 900, loss is 4.2589672088623045 and perplexity is 70.73688928290804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.778949476268194 and perplexity of 118.97929379730931
finished 21 epochs...
Completing Train Step...
At time: 311.29944467544556 and batch: 50, loss is 4.446952867507934 and perplexity is 85.36642447926407
At time: 312.0464520454407 and batch: 100, loss is 4.31712206363678 and perplexity is 74.97255128164198
At time: 312.80923795700073 and batch: 150, loss is 4.3341852569580075 and perplexity is 76.26279898619488
At time: 313.56054735183716 and batch: 200, loss is 4.243459606170655 and perplexity is 69.64839151763137
At time: 314.3103594779968 and batch: 250, loss is 4.40650505065918 and perplexity is 81.98243775933732
At time: 315.06097650527954 and batch: 300, loss is 4.376643886566162 and perplexity is 79.57053703380379
At time: 315.8167622089386 and batch: 350, loss is 4.346716241836548 and perplexity is 77.22445966102904
At time: 316.5734896659851 and batch: 400, loss is 4.273890242576599 and perplexity is 71.8004140375036
At time: 317.33716201782227 and batch: 450, loss is 4.308056535720826 and perplexity is 74.29595700098835
At time: 318.08369731903076 and batch: 500, loss is 4.2062251138687134 and perplexity is 67.10275584628205
At time: 318.8335933685303 and batch: 550, loss is 4.2862601518630985 and perplexity is 72.69409462428551
At time: 319.5850942134857 and batch: 600, loss is 4.289152374267578 and perplexity is 72.90464644801091
At time: 320.3461444377899 and batch: 650, loss is 4.16217013835907 and perplexity is 64.21071765048579
At time: 321.10164070129395 and batch: 700, loss is 4.198173742294312 and perplexity is 66.56465575403053
At time: 321.8570613861084 and batch: 750, loss is 4.280167989730835 and perplexity is 72.25257667878712
At time: 322.60904812812805 and batch: 800, loss is 4.202481551170349 and perplexity is 66.8520220841387
At time: 323.3559648990631 and batch: 850, loss is 4.272472043037414 and perplexity is 71.69865889500532
At time: 324.1025142669678 and batch: 900, loss is 4.261016044616699 and perplexity is 70.88196611938096
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.778548567262415 and perplexity of 118.93160348730089
finished 22 epochs...
Completing Train Step...
At time: 325.9818422794342 and batch: 50, loss is 4.436878776550293 and perplexity is 84.51075264889617
At time: 326.76143050193787 and batch: 100, loss is 4.309434795379639 and perplexity is 74.39842672005481
At time: 327.51318764686584 and batch: 150, loss is 4.325379676818848 and perplexity is 75.59420878050703
At time: 328.2674491405487 and batch: 200, loss is 4.233850898742676 and perplexity is 68.9823654498769
At time: 329.0144567489624 and batch: 250, loss is 4.397658357620239 and perplexity is 81.26036299415986
At time: 329.75988388061523 and batch: 300, loss is 4.3684013032913205 and perplexity is 78.91736586300578
At time: 330.50708055496216 and batch: 350, loss is 4.338447227478027 and perplexity is 76.58852240600748
At time: 331.25288486480713 and batch: 400, loss is 4.266544246673584 and perplexity is 71.2749010614485
At time: 332.01544713974 and batch: 450, loss is 4.299379529953003 and perplexity is 73.65407936878846
At time: 332.76384019851685 and batch: 500, loss is 4.199285359382629 and perplexity is 66.63869130481258
At time: 333.51738953590393 and batch: 550, loss is 4.280552172660828 and perplexity is 72.28034021819262
At time: 334.27275919914246 and batch: 600, loss is 4.284062299728394 and perplexity is 72.53449920105044
At time: 335.02359914779663 and batch: 650, loss is 4.157766547203064 and perplexity is 63.92858156402959
At time: 335.7763533592224 and batch: 700, loss is 4.193105764389038 and perplexity is 66.2281609437508
At time: 336.52142310142517 and batch: 750, loss is 4.275062136650085 and perplexity is 71.88460583948957
At time: 337.2665579319 and batch: 800, loss is 4.200172333717346 and perplexity is 66.69782433456218
At time: 338.01407861709595 and batch: 850, loss is 4.272028632164002 and perplexity is 71.666873977452
At time: 338.76194643974304 and batch: 900, loss is 4.260015244483948 and perplexity is 70.81106292416065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.778642210241866 and perplexity of 118.94274111847474
Annealing...
finished 23 epochs...
Completing Train Step...
At time: 340.6418833732605 and batch: 50, loss is 4.429509391784668 and perplexity is 83.89024956695236
At time: 341.4047842025757 and batch: 100, loss is 4.299829840660095 and perplexity is 73.68725405824756
At time: 342.15797233581543 and batch: 150, loss is 4.318825283050537 and perplexity is 75.10035479429972
At time: 342.91362166404724 and batch: 200, loss is 4.223595795631408 and perplexity is 68.27855915051157
At time: 343.671489238739 and batch: 250, loss is 4.385423583984375 and perplexity is 80.27221803851229
At time: 344.43403792381287 and batch: 300, loss is 4.354987678527832 and perplexity is 77.86586590881764
At time: 345.1806764602661 and batch: 350, loss is 4.325988807678223 and perplexity is 75.6402695729661
At time: 345.9335834980011 and batch: 400, loss is 4.251517696380615 and perplexity is 70.21189185861412
At time: 346.67940306663513 and batch: 450, loss is 4.280558838844299 and perplexity is 72.28082205380785
At time: 347.428275346756 and batch: 500, loss is 4.178343963623047 and perplexity is 65.25769456532367
At time: 348.17440843582153 and batch: 550, loss is 4.260427527427673 and perplexity is 70.84026313659248
At time: 348.9220201969147 and batch: 600, loss is 4.26536937713623 and perplexity is 71.19121152318456
At time: 349.6727559566498 and batch: 650, loss is 4.135322079658509 and perplexity is 62.50972093815428
At time: 350.45058727264404 and batch: 700, loss is 4.167487654685974 and perplexity is 64.5530686116753
At time: 351.1975166797638 and batch: 750, loss is 4.247045078277588 and perplexity is 69.89856210453335
At time: 351.9493887424469 and batch: 800, loss is 4.174264435768127 and perplexity is 64.99201627205903
At time: 352.7018346786499 and batch: 850, loss is 4.243768291473389 and perplexity is 69.66989427107316
At time: 353.454833984375 and batch: 900, loss is 4.237679853439331 and perplexity is 69.24700211964492
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774898999357877 and perplexity of 118.49834560729757
finished 24 epochs...
Completing Train Step...
At time: 355.36470222473145 and batch: 50, loss is 4.421430149078369 and perplexity is 83.21521045430305
At time: 356.11444306373596 and batch: 100, loss is 4.293377704620362 and perplexity is 73.21334437960078
At time: 356.86551547050476 and batch: 150, loss is 4.312378196716309 and perplexity is 74.61773374467091
At time: 357.6190011501312 and batch: 200, loss is 4.218553252220154 and perplexity is 67.9351281624785
At time: 358.37436032295227 and batch: 250, loss is 4.38044020652771 and perplexity is 79.87318636526254
At time: 359.1287078857422 and batch: 300, loss is 4.350853910446167 and perplexity is 77.5446508493638
At time: 359.88147950172424 and batch: 350, loss is 4.321841192245484 and perplexity is 75.3271925339425
At time: 360.6353886127472 and batch: 400, loss is 4.24741147518158 and perplexity is 69.92417741368035
At time: 361.38254499435425 and batch: 450, loss is 4.27662567615509 and perplexity is 71.99708817289222
At time: 362.132328748703 and batch: 500, loss is 4.17507999420166 and perplexity is 65.04504267916961
At time: 362.8838586807251 and batch: 550, loss is 4.257648210525513 and perplexity is 70.64364894903468
At time: 363.6401357650757 and batch: 600, loss is 4.263231830596924 and perplexity is 71.03919951961012
At time: 364.398503780365 and batch: 650, loss is 4.13379075050354 and perplexity is 62.4140712342941
At time: 365.15180683135986 and batch: 700, loss is 4.166753125190735 and perplexity is 64.50566988878293
At time: 365.89943981170654 and batch: 750, loss is 4.246913256645203 and perplexity is 69.88934856926035
At time: 366.64662623405457 and batch: 800, loss is 4.175003328323364 and perplexity is 65.04005613499517
At time: 367.39745020866394 and batch: 850, loss is 4.246328883171081 and perplexity is 69.84851901884784
At time: 368.14985513687134 and batch: 900, loss is 4.239066314697266 and perplexity is 69.34307699196593
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774452941058433 and perplexity of 118.44550022370626
finished 25 epochs...
Completing Train Step...
At time: 370.04187393188477 and batch: 50, loss is 4.41729549407959 and perplexity is 82.87185458660062
At time: 370.7940945625305 and batch: 100, loss is 4.289514775276184 and perplexity is 72.93107195344142
At time: 371.54550337791443 and batch: 150, loss is 4.308598051071167 and perplexity is 74.33620029737064
At time: 372.29202008247375 and batch: 200, loss is 4.215743432044983 and perplexity is 67.74451059472042
At time: 373.0417125225067 and batch: 250, loss is 4.377201814651489 and perplexity is 79.61494405799051
At time: 373.7992708683014 and batch: 300, loss is 4.347974901199341 and perplexity is 77.32172014628802
At time: 374.5470972061157 and batch: 350, loss is 4.319523944854736 and perplexity is 75.15284287725058
At time: 375.29238295555115 and batch: 400, loss is 4.244974021911621 and perplexity is 69.75394804613872
At time: 376.053941488266 and batch: 450, loss is 4.273978610038757 and perplexity is 71.80675913822004
At time: 376.8012728691101 and batch: 500, loss is 4.172845439910889 and perplexity is 64.89985827158955
At time: 377.54764318466187 and batch: 550, loss is 4.255800156593323 and perplexity is 70.51321623627629
At time: 378.29865288734436 and batch: 600, loss is 4.26190037727356 and perplexity is 70.94467708139173
At time: 379.0428845882416 and batch: 650, loss is 4.132830295562744 and perplexity is 62.354154109657166
At time: 379.79202699661255 and batch: 700, loss is 4.166044158935547 and perplexity is 64.4599537530771
At time: 380.53584694862366 and batch: 750, loss is 4.24657913684845 and perplexity is 69.86600105497172
At time: 381.2817270755768 and batch: 800, loss is 4.175066795349121 and perplexity is 65.04418416490861
At time: 382.0290343761444 and batch: 850, loss is 4.246444435119629 and perplexity is 69.85659061765912
At time: 382.7759909629822 and batch: 900, loss is 4.2390726375579835 and perplexity is 69.34351543996961
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774297008775685 and perplexity of 118.42703218639416
finished 26 epochs...
Completing Train Step...
At time: 384.6392970085144 and batch: 50, loss is 4.413639698028565 and perplexity is 82.5694450983728
At time: 385.4229736328125 and batch: 100, loss is 4.28615662574768 and perplexity is 72.68656927659696
At time: 386.1653914451599 and batch: 150, loss is 4.3053929805755615 and perplexity is 74.09832893591062
At time: 386.90501952171326 and batch: 200, loss is 4.213292698860169 and perplexity is 67.5786901483467
At time: 387.65332412719727 and batch: 250, loss is 4.37468373298645 and perplexity is 79.41471932400844
At time: 388.40037536621094 and batch: 300, loss is 4.345549840927124 and perplexity is 77.1344374922266
At time: 389.1638185977936 and batch: 350, loss is 4.31748911857605 and perplexity is 75.00007537801831
At time: 389.91486120224 and batch: 400, loss is 4.2426747751235965 and perplexity is 69.59375074228383
At time: 390.66247725486755 and batch: 450, loss is 4.271521282196045 and perplexity is 71.63052301334596
At time: 391.4146399497986 and batch: 500, loss is 4.170772500038147 and perplexity is 64.7654641112751
At time: 392.1661217212677 and batch: 550, loss is 4.254215269088745 and perplexity is 70.4015492341484
At time: 392.9196512699127 and batch: 600, loss is 4.260807900428772 and perplexity is 70.86721398544309
At time: 393.6726756095886 and batch: 650, loss is 4.132023720741272 and perplexity is 62.30388109613706
At time: 394.433833360672 and batch: 700, loss is 4.1651427984237674 and perplexity is 64.40187827357688
At time: 395.18641543388367 and batch: 750, loss is 4.245778193473816 and perplexity is 69.81006474821207
At time: 395.9346046447754 and batch: 800, loss is 4.174563350677491 and perplexity is 65.01144625852397
At time: 396.6969769001007 and batch: 850, loss is 4.246345639228821 and perplexity is 69.84968941447113
At time: 397.44676995277405 and batch: 900, loss is 4.238772068023682 and perplexity is 69.32267602383067
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774258966315283 and perplexity of 118.42252701640606
finished 27 epochs...
Completing Train Step...
At time: 399.3099820613861 and batch: 50, loss is 4.410347890853882 and perplexity is 82.29808927714215
At time: 400.06851053237915 and batch: 100, loss is 4.283185844421387 and perplexity is 72.4709538057019
At time: 400.8127040863037 and batch: 150, loss is 4.302636194229126 and perplexity is 73.89433698471201
At time: 401.5555160045624 and batch: 200, loss is 4.211287522315979 and perplexity is 67.4433187111472
At time: 402.3007605075836 and batch: 250, loss is 4.37292724609375 and perplexity is 79.27535084571318
At time: 403.04294323921204 and batch: 300, loss is 4.343464193344116 and perplexity is 76.9737298870739
At time: 403.7854688167572 and batch: 350, loss is 4.315736894607544 and perplexity is 74.86877351702974
At time: 404.52716755867004 and batch: 400, loss is 4.240772647857666 and perplexity is 69.46150038983367
At time: 405.270094871521 and batch: 450, loss is 4.269484801292419 and perplexity is 71.48479725533771
At time: 406.0110306739807 and batch: 500, loss is 4.169027161598206 and perplexity is 64.65252504429816
At time: 406.7533447742462 and batch: 550, loss is 4.2528186655044555 and perplexity is 70.3032948053626
At time: 407.5183758735657 and batch: 600, loss is 4.259788570404052 and perplexity is 70.79501371066496
At time: 408.26035499572754 and batch: 650, loss is 4.131288189888 and perplexity is 62.25807151855636
At time: 409.00479340553284 and batch: 700, loss is 4.164334683418274 and perplexity is 64.3498551725175
At time: 409.7496235370636 and batch: 750, loss is 4.245027852058411 and perplexity is 69.7577030124655
At time: 410.49281311035156 and batch: 800, loss is 4.174025912284851 and perplexity is 64.97651599861574
At time: 411.23692440986633 and batch: 850, loss is 4.246052823066711 and perplexity is 69.82923929070175
At time: 411.97981429100037 and batch: 900, loss is 4.238325591087341 and perplexity is 69.29173195624091
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774256039972174 and perplexity of 118.42218047196732
finished 28 epochs...
Completing Train Step...
At time: 413.81354212760925 and batch: 50, loss is 4.407273893356323 and perplexity is 82.04549359480575
At time: 414.55292296409607 and batch: 100, loss is 4.2803317832946775 and perplexity is 72.26441215507911
At time: 415.29346537590027 and batch: 150, loss is 4.299968681335449 and perplexity is 73.69748555662362
At time: 416.03446984291077 and batch: 200, loss is 4.209209017753601 and perplexity is 67.30328304830114
At time: 416.7745592594147 and batch: 250, loss is 4.37057469367981 and perplexity is 79.08907063061406
At time: 417.5151045322418 and batch: 300, loss is 4.341507663726807 and perplexity is 76.82327573679062
At time: 418.25988364219666 and batch: 350, loss is 4.314071063995361 and perplexity is 74.74415864466016
At time: 419.0101270675659 and batch: 400, loss is 4.2387999200820925 and perplexity is 69.3246068299408
At time: 419.7604298591614 and batch: 450, loss is 4.26736701965332 and perplexity is 71.33356825575675
At time: 420.5119984149933 and batch: 500, loss is 4.167064599990844 and perplexity is 64.52576490880116
At time: 421.2610821723938 and batch: 550, loss is 4.251066074371338 and perplexity is 70.18018978215437
At time: 422.01179933547974 and batch: 600, loss is 4.2585999345779415 and perplexity is 70.71091421279822
At time: 422.7646379470825 and batch: 650, loss is 4.130267214775086 and perplexity is 62.19454001451884
At time: 423.5153474807739 and batch: 700, loss is 4.163314113616943 and perplexity is 64.28421515441586
At time: 424.2667441368103 and batch: 750, loss is 4.2440159893035885 and perplexity is 69.68715349016166
At time: 425.01784443855286 and batch: 800, loss is 4.17335636138916 and perplexity is 64.93302547531418
At time: 425.766619682312 and batch: 850, loss is 4.245731806755066 and perplexity is 69.8068265634776
At time: 426.53936409950256 and batch: 900, loss is 4.23803216457367 and perplexity is 69.27140290759351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774383126872859 and perplexity of 118.43723133621943
Annealing...
finished 29 epochs...
Completing Train Step...
At time: 428.3870460987091 and batch: 50, loss is 4.404845542907715 and perplexity is 81.84650009439336
At time: 429.1297390460968 and batch: 100, loss is 4.2769215965270995 and perplexity is 72.0183967306706
At time: 429.87223410606384 and batch: 150, loss is 4.297067050933838 and perplexity is 73.48395263863279
At time: 430.6135425567627 and batch: 200, loss is 4.206677827835083 and perplexity is 67.13314107842609
At time: 431.3540589809418 and batch: 250, loss is 4.36700325012207 and perplexity is 78.80711227769734
At time: 432.09468626976013 and batch: 300, loss is 4.337499823570251 and perplexity is 76.51599650167664
At time: 432.83779215812683 and batch: 350, loss is 4.310544815063476 and perplexity is 74.48105628990082
At time: 433.58087611198425 and batch: 400, loss is 4.234354915618897 and perplexity is 69.01714248959651
At time: 434.3274955749512 and batch: 450, loss is 4.262295346260071 and perplexity is 70.97270356302812
At time: 435.0738136768341 and batch: 500, loss is 4.160860443115235 and perplexity is 64.12667622531616
At time: 435.8156430721283 and batch: 550, loss is 4.245375437736511 and perplexity is 69.78195400537456
At time: 436.5562570095062 and batch: 600, loss is 4.253450984954834 and perplexity is 70.34776300366545
At time: 437.2981538772583 and batch: 650, loss is 4.1237259864807125 and perplexity is 61.789039012802405
At time: 438.0439190864563 and batch: 700, loss is 4.156303086280823 and perplexity is 63.83509300820591
At time: 438.78790402412415 and batch: 750, loss is 4.236776051521301 and perplexity is 69.18444482027331
At time: 439.5334038734436 and batch: 800, loss is 4.166024355888367 and perplexity is 64.45867726221097
At time: 440.2781345844269 and batch: 850, loss is 4.2382254934310915 and perplexity is 69.28479636339867
At time: 441.0233314037323 and batch: 900, loss is 4.232151165008545 and perplexity is 68.86521338812253
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.773099298346533 and perplexity of 118.28527580334607
finished 30 epochs...
Completing Train Step...
At time: 442.85606837272644 and batch: 50, loss is 4.40317138671875 and perplexity is 81.70959090537565
At time: 443.6101026535034 and batch: 100, loss is 4.275802526473999 and perplexity is 71.93784817775973
At time: 444.35414123535156 and batch: 150, loss is 4.296307735443115 and perplexity is 73.42817631366837
At time: 445.11343479156494 and batch: 200, loss is 4.205875272750855 and perplexity is 67.07928464899972
At time: 445.8588001728058 and batch: 250, loss is 4.3659475231170655 and perplexity is 78.72395738323661
At time: 446.6020541191101 and batch: 300, loss is 4.336679458618164 and perplexity is 76.45325120041463
At time: 447.35422110557556 and batch: 350, loss is 4.309695796966553 and perplexity is 74.41784736179154
At time: 448.0990500450134 and batch: 400, loss is 4.233663568496704 and perplexity is 68.96944417669985
At time: 448.8481032848358 and batch: 450, loss is 4.261629471778869 and perplexity is 70.92546038162365
At time: 449.60133147239685 and batch: 500, loss is 4.160409889221191 and perplexity is 64.0977902094729
At time: 450.3577799797058 and batch: 550, loss is 4.245084481239319 and perplexity is 69.76165344590298
At time: 451.11005568504333 and batch: 600, loss is 4.253176259994507 and perplexity is 70.32843937172868
At time: 451.8706679344177 and batch: 650, loss is 4.123527464866638 and perplexity is 61.776773770543585
At time: 452.63094830513 and batch: 700, loss is 4.156331624984741 and perplexity is 63.8369148050206
At time: 453.389888048172 and batch: 750, loss is 4.23648696899414 and perplexity is 69.16444769667324
At time: 454.1481008529663 and batch: 800, loss is 4.166121592521668 and perplexity is 64.46494531171204
At time: 454.90670704841614 and batch: 850, loss is 4.238507113456726 and perplexity is 69.30431109726757
At time: 455.6639521121979 and batch: 900, loss is 4.232321772575379 and perplexity is 68.87696331690306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772926226054152 and perplexity of 118.2648056709654
finished 31 epochs...
Completing Train Step...
At time: 457.57592487335205 and batch: 50, loss is 4.402207517623902 and perplexity is 81.63087149964494
At time: 458.3457124233246 and batch: 100, loss is 4.2749984121322635 and perplexity is 71.88002517359556
At time: 459.0998213291168 and batch: 150, loss is 4.295678148269653 and perplexity is 73.38196142536759
At time: 459.85679030418396 and batch: 200, loss is 4.205235686302185 and perplexity is 67.03639536471633
At time: 460.6129539012909 and batch: 250, loss is 4.365159730911255 and perplexity is 78.66196368548171
At time: 461.3694202899933 and batch: 300, loss is 4.336031498908997 and perplexity is 76.40372862005299
At time: 462.1260998249054 and batch: 350, loss is 4.309058351516724 and perplexity is 74.37042515974393
At time: 462.8811764717102 and batch: 400, loss is 4.233145751953125 and perplexity is 68.93373990243346
At time: 463.65018701553345 and batch: 450, loss is 4.261038403511048 and perplexity is 70.88355097949042
At time: 464.4337420463562 and batch: 500, loss is 4.1600673341751095 and perplexity is 64.07583694830882
At time: 465.19653272628784 and batch: 550, loss is 4.244857892990113 and perplexity is 69.74584806571127
At time: 465.96388840675354 and batch: 600, loss is 4.252977142333984 and perplexity is 70.31443713150571
At time: 466.73054027557373 and batch: 650, loss is 4.1233601379394536 and perplexity is 61.76643771759131
At time: 467.4900772571564 and batch: 700, loss is 4.156320061683655 and perplexity is 63.83617664382208
At time: 468.2559678554535 and batch: 750, loss is 4.23622034072876 and perplexity is 69.14600895821157
At time: 469.01865220069885 and batch: 800, loss is 4.166151971817016 and perplexity is 64.46690374107301
At time: 469.7777051925659 and batch: 850, loss is 4.238670029640198 and perplexity is 69.31560281090614
At time: 470.52457070350647 and batch: 900, loss is 4.23239565372467 and perplexity is 68.8820522140971
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7728513952803935 and perplexity of 118.2559561551608
finished 32 epochs...
Completing Train Step...
At time: 472.38712906837463 and batch: 50, loss is 4.4013316822052 and perplexity is 81.55940759100547
At time: 473.13603615760803 and batch: 100, loss is 4.274293780326843 and perplexity is 71.82939406192453
At time: 473.89361023902893 and batch: 150, loss is 4.295106277465821 and perplexity is 73.34000842109295
At time: 474.650897026062 and batch: 200, loss is 4.204659452438355 and perplexity is 66.9977778510262
At time: 475.4102580547333 and batch: 250, loss is 4.364456338882446 and perplexity is 78.60665294210332
At time: 476.16963481903076 and batch: 300, loss is 4.335443425178528 and perplexity is 76.3588108031302
At time: 476.92556953430176 and batch: 350, loss is 4.3085018920898435 and perplexity is 74.32905254774026
At time: 477.6909728050232 and batch: 400, loss is 4.232669882774353 and perplexity is 68.90094426407201
At time: 478.45055532455444 and batch: 450, loss is 4.260491800308228 and perplexity is 70.84481639068733
At time: 479.2037420272827 and batch: 500, loss is 4.159684848785401 and perplexity is 64.05133356323483
At time: 479.95238876342773 and batch: 550, loss is 4.244632725715637 and perplexity is 69.7301453511314
At time: 480.70386958122253 and batch: 600, loss is 4.252776389122009 and perplexity is 70.30032269920956
At time: 481.458624124527 and batch: 650, loss is 4.123196306228638 and perplexity is 61.756319245315005
At time: 482.21291518211365 and batch: 700, loss is 4.1562813997268675 and perplexity is 63.833708660028066
At time: 482.96523356437683 and batch: 750, loss is 4.235920310020447 and perplexity is 69.125266144063
At time: 483.7446639537811 and batch: 800, loss is 4.1661289072036745 and perplexity is 64.46541685401218
At time: 484.4920198917389 and batch: 850, loss is 4.238761911392212 and perplexity is 69.32197194253332
At time: 485.24040365219116 and batch: 900, loss is 4.232416563034057 and perplexity is 68.88349250529576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772805827937714 and perplexity of 118.25056766825323
finished 33 epochs...
Completing Train Step...
At time: 487.1667814254761 and batch: 50, loss is 4.400539970397949 and perplexity is 81.49486159930665
At time: 487.9231126308441 and batch: 100, loss is 4.2736373615264895 and perplexity is 71.78225936898603
At time: 488.6827666759491 and batch: 150, loss is 4.294587354660035 and perplexity is 73.30196049096983
At time: 489.442373752594 and batch: 200, loss is 4.204143042564392 and perplexity is 66.9631884689283
At time: 490.2039909362793 and batch: 250, loss is 4.363794031143189 and perplexity is 78.55460838416187
At time: 490.97034096717834 and batch: 300, loss is 4.3348871612548825 and perplexity is 76.31634696307408
At time: 491.73955368995667 and batch: 350, loss is 4.307986936569214 and perplexity is 74.2907862453551
At time: 492.50289726257324 and batch: 400, loss is 4.232201595306396 and perplexity is 68.86868636891629
At time: 493.2681519985199 and batch: 450, loss is 4.259928617477417 and perplexity is 70.80492903943414
At time: 494.03363704681396 and batch: 500, loss is 4.1592788410186765 and perplexity is 64.02533350280953
At time: 494.8006749153137 and batch: 550, loss is 4.244371385574341 and perplexity is 69.71192444612336
At time: 495.56810903549194 and batch: 600, loss is 4.252566161155701 and perplexity is 70.28554515871826
At time: 496.3318793773651 and batch: 650, loss is 4.123015146255494 and perplexity is 61.74513248550617
At time: 497.10128450393677 and batch: 700, loss is 4.1562007761001585 and perplexity is 63.828562362388986
At time: 497.8585307598114 and batch: 750, loss is 4.235610918998718 and perplexity is 69.10388271543442
At time: 498.61428236961365 and batch: 800, loss is 4.166073389053345 and perplexity is 64.46183795265613
At time: 499.3837676048279 and batch: 850, loss is 4.238805222511291 and perplexity is 69.32497441973482
At time: 500.1502959728241 and batch: 900, loss is 4.232411332130432 and perplexity is 68.88313218332752
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7727903601241435 and perplexity of 118.24873860466379
finished 34 epochs...
Completing Train Step...
At time: 502.0367534160614 and batch: 50, loss is 4.399799718856811 and perplexity is 81.43455722536959
At time: 502.79900670051575 and batch: 100, loss is 4.273009071350097 and perplexity is 71.73717344558979
At time: 503.54960584640503 and batch: 150, loss is 4.294070272445679 and perplexity is 73.26406714875351
At time: 504.3057129383087 and batch: 200, loss is 4.203639488220215 and perplexity is 66.92947735287663
At time: 505.0700330734253 and batch: 250, loss is 4.3631714344024655 and perplexity is 78.50571576278578
At time: 505.83064103126526 and batch: 300, loss is 4.334367737770081 and perplexity is 76.27671675350899
At time: 506.59483647346497 and batch: 350, loss is 4.307514572143555 and perplexity is 74.25570220765657
At time: 507.3690769672394 and batch: 400, loss is 4.231781458854675 and perplexity is 68.83975820070505
At time: 508.1260585784912 and batch: 450, loss is 4.259468970298767 and perplexity is 70.77239123209516
At time: 508.8851761817932 and batch: 500, loss is 4.158902978897094 and perplexity is 64.00127332706177
At time: 509.63362216949463 and batch: 550, loss is 4.244121527671814 and perplexity is 69.69450854674078
At time: 510.3834536075592 and batch: 600, loss is 4.252374787330627 and perplexity is 70.27209563207856
At time: 511.13545751571655 and batch: 650, loss is 4.122821860313415 and perplexity is 61.73319917271291
At time: 511.8845269680023 and batch: 700, loss is 4.156113286018371 and perplexity is 63.822978240528734
At time: 512.6374588012695 and batch: 750, loss is 4.235300750732422 and perplexity is 69.08245220764174
At time: 513.3858866691589 and batch: 800, loss is 4.165998497009277 and perplexity is 64.45701045462036
At time: 514.1470470428467 and batch: 850, loss is 4.238761672973633 and perplexity is 69.32195541488925
At time: 514.9114303588867 and batch: 900, loss is 4.232343835830688 and perplexity is 68.87848298369401
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772725144477739 and perplexity of 118.24102718819445
finished 35 epochs...
Completing Train Step...
At time: 516.8448195457458 and batch: 50, loss is 4.399071502685547 and perplexity is 81.37527685098163
At time: 517.6114540100098 and batch: 100, loss is 4.272389588356018 and perplexity is 71.69274724865438
At time: 518.3718402385712 and batch: 150, loss is 4.2935467624664305 and perplexity is 73.22572271620308
At time: 519.1337633132935 and batch: 200, loss is 4.203115153312683 and perplexity is 66.8943930903132
At time: 519.8840792179108 and batch: 250, loss is 4.3625338268280025 and perplexity is 78.45567587837674
At time: 520.6328530311584 and batch: 300, loss is 4.333826107978821 and perplexity is 76.23541419769283
At time: 521.390576839447 and batch: 350, loss is 4.3070219612121585 and perplexity is 74.21913204520098
At time: 522.1743268966675 and batch: 400, loss is 4.231316561698914 and perplexity is 68.8077622309082
At time: 522.9327917098999 and batch: 450, loss is 4.258914227485657 and perplexity is 70.73314164441246
At time: 523.6912174224854 and batch: 500, loss is 4.15851047039032 and perplexity is 63.97615721230331
At time: 524.4492516517639 and batch: 550, loss is 4.243866238594055 and perplexity is 69.67671857081896
At time: 525.2048366069794 and batch: 600, loss is 4.252160763740539 and perplexity is 70.25705735521879
At time: 525.9593765735626 and batch: 650, loss is 4.122629947662354 and perplexity is 61.721352927559685
At time: 526.7150530815125 and batch: 700, loss is 4.15610710144043 and perplexity is 63.82258352356594
At time: 527.4766085147858 and batch: 750, loss is 4.23526309967041 and perplexity is 69.07985122891489
At time: 528.2304117679596 and batch: 800, loss is 4.165995149612427 and perplexity is 64.45679469178769
At time: 528.9804730415344 and batch: 850, loss is 4.238784589767456 and perplexity is 69.3235440700523
At time: 529.7404339313507 and batch: 900, loss is 4.232314672470093 and perplexity is 68.87647428494782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.77274061229131 and perplexity of 118.24285613250422
Annealing...
finished 36 epochs...
Completing Train Step...
At time: 531.6330606937408 and batch: 50, loss is 4.3983840656280515 and perplexity is 81.31935569345055
At time: 532.3796274662018 and batch: 100, loss is 4.271649694442749 and perplexity is 71.63972184034704
At time: 533.1387300491333 and batch: 150, loss is 4.293106880187988 and perplexity is 73.193519101873
At time: 533.9025430679321 and batch: 200, loss is 4.202447485923767 and perplexity is 66.84974479231039
At time: 534.6582446098328 and batch: 250, loss is 4.36172945022583 and perplexity is 78.39259334285167
At time: 535.4168195724487 and batch: 300, loss is 4.3328020381927494 and perplexity is 76.15738377450307
At time: 536.1684782505035 and batch: 350, loss is 4.3060857200622555 and perplexity is 74.1496775578181
At time: 536.9196650981903 and batch: 400, loss is 4.230028042793274 and perplexity is 68.71915922400247
At time: 537.665376663208 and batch: 450, loss is 4.257683005332947 and perplexity is 70.64610702396014
At time: 538.4181303977966 and batch: 500, loss is 4.1570530080795285 and perplexity is 63.88298229033912
At time: 539.1745522022247 and batch: 550, loss is 4.242484316825867 and perplexity is 69.58049729713922
At time: 539.9317417144775 and batch: 600, loss is 4.250896401405335 and perplexity is 70.16828311134799
At time: 540.6913139820099 and batch: 650, loss is 4.121107740402222 and perplexity is 61.627471707493356
At time: 541.4744830131531 and batch: 700, loss is 4.154351258277893 and perplexity is 63.71061940115538
At time: 542.2301225662231 and batch: 750, loss is 4.233291330337525 and perplexity is 68.94377589541318
At time: 542.9872810840607 and batch: 800, loss is 4.164166355133057 and perplexity is 64.33902418334881
At time: 543.7458302974701 and batch: 850, loss is 4.237099022865295 and perplexity is 69.20679302212055
At time: 544.4972765445709 and batch: 900, loss is 4.230964794158935 and perplexity is 68.78356215030776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772514865822988 and perplexity of 118.21616623801604
finished 37 epochs...
Completing Train Step...
At time: 546.4125196933746 and batch: 50, loss is 4.3981639003753665 and perplexity is 81.30145396769743
At time: 547.1679856777191 and batch: 100, loss is 4.271536173820496 and perplexity is 71.63158971613635
At time: 547.9247369766235 and batch: 150, loss is 4.292818956375122 and perplexity is 73.17244797836075
At time: 548.6812603473663 and batch: 200, loss is 4.202265329360962 and perplexity is 66.83756878157841
At time: 549.4334309101105 and batch: 250, loss is 4.361549825668335 and perplexity is 78.3785133725536
At time: 550.1902852058411 and batch: 300, loss is 4.332635703086853 and perplexity is 76.14471718148693
At time: 550.9511380195618 and batch: 350, loss is 4.305991268157959 and perplexity is 74.14267431030999
At time: 551.7039716243744 and batch: 400, loss is 4.229932188987732 and perplexity is 68.7125725467613
At time: 552.4587032794952 and batch: 450, loss is 4.257528443336486 and perplexity is 70.63518866441984
At time: 553.2108583450317 and batch: 500, loss is 4.156968812942505 and perplexity is 63.8776038803129
At time: 553.9649469852448 and batch: 550, loss is 4.242380628585815 and perplexity is 69.57328299185832
At time: 554.7172708511353 and batch: 600, loss is 4.2508573770523075 and perplexity is 70.16554489292548
At time: 555.468995809555 and batch: 650, loss is 4.121071963310242 and perplexity is 61.62526689521073
At time: 556.2178773880005 and batch: 700, loss is 4.1543149614334105 and perplexity is 63.70830694867868
At time: 556.9644317626953 and batch: 750, loss is 4.233236656188965 and perplexity is 68.94000655621126
At time: 557.7089428901672 and batch: 800, loss is 4.164189581871033 and perplexity is 64.34051858636012
At time: 558.4533846378326 and batch: 850, loss is 4.237138195037842 and perplexity is 69.2095040556564
At time: 559.1985697746277 and batch: 900, loss is 4.230998134613037 and perplexity is 68.78585546373442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7724847662938785 and perplexity of 118.21260804062949
finished 38 epochs...
Completing Train Step...
At time: 561.0556302070618 and batch: 50, loss is 4.397986125946045 and perplexity is 81.2870019327545
At time: 561.8158688545227 and batch: 100, loss is 4.271395411491394 and perplexity is 71.62150739635267
At time: 562.5643668174744 and batch: 150, loss is 4.292662162780761 and perplexity is 73.16097590663125
At time: 563.3129732608795 and batch: 200, loss is 4.20214231967926 and perplexity is 66.82934761916852
At time: 564.0615763664246 and batch: 250, loss is 4.361424245834351 and perplexity is 78.36867122985662
At time: 564.8105888366699 and batch: 300, loss is 4.33248471736908 and perplexity is 76.13322128458857
At time: 565.5580866336823 and batch: 350, loss is 4.305892572402954 and perplexity is 74.13535710418543
At time: 566.3050980567932 and batch: 400, loss is 4.229815382957458 and perplexity is 68.70454697265905
At time: 567.0570611953735 and batch: 450, loss is 4.257405996322632 and perplexity is 70.62654012599948
At time: 567.8139584064484 and batch: 500, loss is 4.156878962516784 and perplexity is 63.87186470824761
At time: 568.5650141239166 and batch: 550, loss is 4.242301321029663 and perplexity is 69.56776552360209
At time: 569.3162145614624 and batch: 600, loss is 4.250822701454163 and perplexity is 70.1631119028701
At time: 570.0755310058594 and batch: 650, loss is 4.121032028198242 and perplexity is 61.622805932415005
At time: 570.8271470069885 and batch: 700, loss is 4.154294719696045 and perplexity is 63.70701739491288
At time: 571.5798273086548 and batch: 750, loss is 4.23318498134613 and perplexity is 68.93644418425073
At time: 572.334867477417 and batch: 800, loss is 4.1641948032379155 and perplexity is 64.34085453269013
At time: 573.0868403911591 and batch: 850, loss is 4.2371652317047115 and perplexity is 69.21137527525742
At time: 573.8325009346008 and batch: 900, loss is 4.2310190725326535 and perplexity is 68.78729571152473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772464281892123 and perplexity of 118.2101865508753
finished 39 epochs...
Completing Train Step...
At time: 575.6684176921844 and batch: 50, loss is 4.39782862663269 and perplexity is 81.27420029391655
At time: 576.4234557151794 and batch: 100, loss is 4.271257061958313 and perplexity is 71.61159927965326
At time: 577.1640520095825 and batch: 150, loss is 4.292520666122437 and perplexity is 73.15062460537524
At time: 577.905647277832 and batch: 200, loss is 4.202023968696595 and perplexity is 66.82143876822626
At time: 578.6431128978729 and batch: 250, loss is 4.361298942565918 and perplexity is 78.35885199441282
At time: 579.3958745002747 and batch: 300, loss is 4.33234543800354 and perplexity is 76.12261823624164
At time: 580.1380660533905 and batch: 350, loss is 4.305792570114136 and perplexity is 74.12794376947403
At time: 580.8800354003906 and batch: 400, loss is 4.229697790145874 and perplexity is 68.69646828681891
At time: 581.6226892471313 and batch: 450, loss is 4.257293310165405 and perplexity is 70.61858194099153
At time: 582.3728201389313 and batch: 500, loss is 4.156791672706604 and perplexity is 63.86628958863049
At time: 583.1217586994171 and batch: 550, loss is 4.242228360176086 and perplexity is 69.56268998520812
At time: 583.8707220554352 and batch: 600, loss is 4.250790977478028 and perplexity is 70.16088608528862
At time: 584.6203064918518 and batch: 650, loss is 4.120991425514221 and perplexity is 61.62030393189154
At time: 585.3693580627441 and batch: 700, loss is 4.154274001121521 and perplexity is 63.70569748999858
At time: 586.1179983615875 and batch: 750, loss is 4.233130536079407 and perplexity is 68.932691023332
At time: 586.866473197937 and batch: 800, loss is 4.164194140434265 and perplexity is 64.34081188735102
At time: 587.6157867908478 and batch: 850, loss is 4.237186207771301 and perplexity is 69.2128270729004
At time: 588.3661410808563 and batch: 900, loss is 4.231034870147705 and perplexity is 68.78838239532631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7724542487157535 and perplexity of 118.20900053317472
finished 40 epochs...
Completing Train Step...
At time: 590.2585716247559 and batch: 50, loss is 4.397683601379395 and perplexity is 81.2624143370839
At time: 591.0095155239105 and batch: 100, loss is 4.271121435165405 and perplexity is 71.60188748671254
At time: 591.7549901008606 and batch: 150, loss is 4.292384128570557 and perplexity is 73.1406374799975
At time: 592.4953441619873 and batch: 200, loss is 4.201911053657532 and perplexity is 66.81389404882167
At time: 593.2342100143433 and batch: 250, loss is 4.361175880432129 and perplexity is 78.34920958020531
At time: 593.9736647605896 and batch: 300, loss is 4.332213320732117 and perplexity is 76.11256178795654
At time: 594.716344833374 and batch: 350, loss is 4.305694417953491 and perplexity is 74.12066830868585
At time: 595.4560067653656 and batch: 400, loss is 4.229580807685852 and perplexity is 68.68843247499758
At time: 596.1979548931122 and batch: 450, loss is 4.257180995941162 and perplexity is 70.61065091513555
At time: 596.937705039978 and batch: 500, loss is 4.1567054271698 and perplexity is 63.86078164372243
At time: 597.6774108409882 and batch: 550, loss is 4.242157273292541 and perplexity is 69.55774516612405
At time: 598.441080570221 and batch: 600, loss is 4.25076000213623 and perplexity is 70.15871286151967
At time: 599.1920778751373 and batch: 650, loss is 4.120950808525086 and perplexity is 61.61780115150426
At time: 599.9457161426544 and batch: 700, loss is 4.15425130367279 and perplexity is 63.70425154960561
At time: 600.6985185146332 and batch: 750, loss is 4.2330739879608155 and perplexity is 68.92879311955579
At time: 601.452764749527 and batch: 800, loss is 4.164189977645874 and perplexity is 64.3405440507237
At time: 602.2064924240112 and batch: 850, loss is 4.237201929092407 and perplexity is 69.21391519853285
At time: 602.960108757019 and batch: 900, loss is 4.231046943664551 and perplexity is 68.78921291803361
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772447141882491 and perplexity of 118.20816044450301
finished 41 epochs...
Completing Train Step...
At time: 604.8522620201111 and batch: 50, loss is 4.397544231414795 and perplexity is 81.2510895864578
At time: 605.6056132316589 and batch: 100, loss is 4.270987954139709 and perplexity is 71.5923306311727
At time: 606.3608026504517 and batch: 150, loss is 4.292248830795288 and perplexity is 73.13074238387206
At time: 607.1149625778198 and batch: 200, loss is 4.201800060272217 and perplexity is 66.80647856007784
At time: 607.8640508651733 and batch: 250, loss is 4.361055612564087 and perplexity is 78.33978725441918
At time: 608.6119430065155 and batch: 300, loss is 4.332084856033325 and perplexity is 76.10278463865491
At time: 609.3806464672089 and batch: 350, loss is 4.305598430633545 and perplexity is 74.11355400582914
At time: 610.1357581615448 and batch: 400, loss is 4.229465055465698 and perplexity is 68.68048209658558
At time: 610.8923144340515 and batch: 450, loss is 4.257068891525268 and perplexity is 70.60273559303837
At time: 611.6501877307892 and batch: 500, loss is 4.156621499061584 and perplexity is 63.85542215403899
At time: 612.4047338962555 and batch: 550, loss is 4.242086510658265 and perplexity is 69.55282325098764
At time: 613.1639020442963 and batch: 600, loss is 4.250728487968445 and perplexity is 70.15650190290938
At time: 613.9121499061584 and batch: 650, loss is 4.1209097003936765 and perplexity is 61.61526821089996
At time: 614.6636440753937 and batch: 700, loss is 4.154226570129395 and perplexity is 63.70267593722075
At time: 615.4194962978363 and batch: 750, loss is 4.233014922142029 and perplexity is 68.92472190418826
At time: 616.1764025688171 and batch: 800, loss is 4.164181952476501 and perplexity is 64.34002770903203
At time: 616.9320640563965 and batch: 850, loss is 4.237215728759765 and perplexity is 69.2148703341294
At time: 617.720112323761 and batch: 900, loss is 4.2310563087463375 and perplexity is 68.7898571376552
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772442543343322 and perplexity of 118.20761686089696
finished 42 epochs...
Completing Train Step...
At time: 619.6244380474091 and batch: 50, loss is 4.397408647537231 and perplexity is 81.24007399546055
At time: 620.3764357566833 and batch: 100, loss is 4.270856456756592 and perplexity is 71.5829170459879
At time: 621.126451253891 and batch: 150, loss is 4.292116775512695 and perplexity is 73.12108572064102
At time: 621.8765935897827 and batch: 200, loss is 4.201690950393677 and perplexity is 66.7991897109664
At time: 622.6226887702942 and batch: 250, loss is 4.360936794281006 and perplexity is 78.3304796083709
At time: 623.3752610683441 and batch: 300, loss is 4.331959543228149 and perplexity is 76.09324858273756
At time: 624.1274676322937 and batch: 350, loss is 4.305503902435302 and perplexity is 74.10654851621702
At time: 624.8741607666016 and batch: 400, loss is 4.229351730346679 and perplexity is 68.67269931377844
At time: 625.6215581893921 and batch: 450, loss is 4.256956791877746 and perplexity is 70.5948214948564
At time: 626.3671927452087 and batch: 500, loss is 4.156539211273193 and perplexity is 63.850167848758694
At time: 627.1194982528687 and batch: 550, loss is 4.242016277313232 and perplexity is 69.54793849509268
At time: 627.8736310005188 and batch: 600, loss is 4.250696506500244 and perplexity is 70.15425823085285
At time: 628.61931681633 and batch: 650, loss is 4.120868101119995 and perplexity is 61.61270511380654
At time: 629.3653028011322 and batch: 700, loss is 4.15420163154602 and perplexity is 63.701087302534944
At time: 630.1172451972961 and batch: 750, loss is 4.232955837249756 and perplexity is 68.92064961472619
At time: 630.8702368736267 and batch: 800, loss is 4.164172191619873 and perplexity is 64.33939969831104
At time: 631.625646352768 and batch: 850, loss is 4.2372297143936155 and perplexity is 69.21583835473206
At time: 632.3759596347809 and batch: 900, loss is 4.231063828468323 and perplexity is 68.7903744202012
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772440453098245 and perplexity of 118.20736977826593
finished 43 epochs...
Completing Train Step...
At time: 634.2568323612213 and batch: 50, loss is 4.397275924682617 and perplexity is 81.22929229643557
At time: 635.0242605209351 and batch: 100, loss is 4.270726571083069 and perplexity is 71.5736200543807
At time: 635.7796726226807 and batch: 150, loss is 4.2919902515411374 and perplexity is 73.11183473571906
At time: 636.5412490367889 and batch: 200, loss is 4.201584386825561 and perplexity is 66.79207173022903
At time: 637.2957928180695 and batch: 250, loss is 4.360819759368897 and perplexity is 78.32131274400652
At time: 638.0508391857147 and batch: 300, loss is 4.331836762428284 and perplexity is 76.08390636634638
At time: 638.806928396225 and batch: 350, loss is 4.305409746170044 and perplexity is 74.09957124885942
At time: 639.56325507164 and batch: 400, loss is 4.229237380027771 and perplexity is 68.66484701767648
At time: 640.3215317726135 and batch: 450, loss is 4.2568468618392945 and perplexity is 70.58706142995491
At time: 641.0781197547913 and batch: 500, loss is 4.156456923484802 and perplexity is 63.84491397582577
At time: 641.8359370231628 and batch: 550, loss is 4.241947340965271 and perplexity is 69.54314427945432
At time: 642.5916068553925 and batch: 600, loss is 4.250663313865662 and perplexity is 70.15192966484084
At time: 643.3469316959381 and batch: 650, loss is 4.120826153755188 and perplexity is 61.610120677393944
At time: 644.107430934906 and batch: 700, loss is 4.154176201820373 and perplexity is 63.69946742195807
At time: 644.8656225204468 and batch: 750, loss is 4.232896633148194 and perplexity is 68.91656935037201
At time: 645.624265909195 and batch: 800, loss is 4.164158883094788 and perplexity is 64.338543441494
At time: 646.3757965564728 and batch: 850, loss is 4.237239408493042 and perplexity is 69.21650934320324
At time: 647.1265277862549 and batch: 900, loss is 4.231068782806396 and perplexity is 68.7907152318165
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772439198951199 and perplexity of 118.20722152893529
finished 44 epochs...
Completing Train Step...
At time: 649.0208349227905 and batch: 50, loss is 4.397144327163696 and perplexity is 81.21860342643545
At time: 649.788642168045 and batch: 100, loss is 4.2705978631973265 and perplexity is 71.56440855787741
At time: 650.540753364563 and batch: 150, loss is 4.291866226196289 and perplexity is 73.1027675774938
At time: 651.2961511611938 and batch: 200, loss is 4.201479406356811 and perplexity is 66.78506023527146
At time: 652.0528156757355 and batch: 250, loss is 4.36070366859436 and perplexity is 78.31222088989774
At time: 652.8094885349274 and batch: 300, loss is 4.331714639663696 and perplexity is 76.07461535669371
At time: 653.5696496963501 and batch: 350, loss is 4.305315904617309 and perplexity is 74.09261795629543
At time: 654.3180229663849 and batch: 400, loss is 4.229123635292053 and perplexity is 68.65703719697073
At time: 655.0683059692383 and batch: 450, loss is 4.2567364740371705 and perplexity is 70.57926990943758
At time: 655.839751958847 and batch: 500, loss is 4.156373391151428 and perplexity is 63.83958108392486
At time: 656.5885117053986 and batch: 550, loss is 4.241879234313965 and perplexity is 69.53840809006098
At time: 657.3376019001007 and batch: 600, loss is 4.250628485679626 and perplexity is 70.14948644293045
At time: 658.0907142162323 and batch: 650, loss is 4.120783400535584 and perplexity is 61.60748670268061
At time: 658.8377819061279 and batch: 700, loss is 4.1541507530212405 and perplexity is 63.69784636763381
At time: 659.5915329456329 and batch: 750, loss is 4.23283860206604 and perplexity is 68.91257016331399
At time: 660.3560283184052 and batch: 800, loss is 4.164150395393372 and perplexity is 64.3379973574652
At time: 661.1124927997589 and batch: 850, loss is 4.237248873710632 and perplexity is 69.21716449562558
At time: 661.8731813430786 and batch: 900, loss is 4.231074061393738 and perplexity is 68.79107835057351
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772439617000214 and perplexity of 118.20727094535815
Annealing...
finished 45 epochs...
Completing Train Step...
At time: 663.7907691001892 and batch: 50, loss is 4.39702431678772 and perplexity is 81.20885693615365
At time: 664.5656940937042 and batch: 100, loss is 4.2704821300506595 and perplexity is 71.55612666293977
At time: 665.3265714645386 and batch: 150, loss is 4.291703586578369 and perplexity is 73.09087913809792
At time: 666.0892519950867 and batch: 200, loss is 4.201323609352112 and perplexity is 66.77465613341315
At time: 666.856883764267 and batch: 250, loss is 4.360498409271241 and perplexity is 78.29614822603438
At time: 667.6227192878723 and batch: 300, loss is 4.331483597755432 and perplexity is 76.0570409626802
At time: 668.388288974762 and batch: 350, loss is 4.3050830078125 and perplexity is 74.07536403158339
At time: 669.1547708511353 and batch: 400, loss is 4.228820400238037 and perplexity is 68.6362211328274
At time: 669.9191515445709 and batch: 450, loss is 4.256458325386047 and perplexity is 70.55964111070352
At time: 670.6673808097839 and batch: 500, loss is 4.1560322952270505 and perplexity is 63.81780937633462
At time: 671.4251329898834 and batch: 550, loss is 4.241524677276612 and perplexity is 69.51375712845054
At time: 672.1837811470032 and batch: 600, loss is 4.25035174369812 and perplexity is 70.13007582104147
At time: 672.9443998336792 and batch: 650, loss is 4.120454277992248 and perplexity is 61.58721362631476
At time: 673.7034358978271 and batch: 700, loss is 4.153703322410584 and perplexity is 63.66935237635334
At time: 674.4545466899872 and batch: 750, loss is 4.232423658370972 and perplexity is 68.88398125861686
At time: 675.2367935180664 and batch: 800, loss is 4.163739295005798 and perplexity is 64.3115534177452
At time: 675.9948463439941 and batch: 850, loss is 4.236882567405701 and perplexity is 69.19181445508467
At time: 676.7512946128845 and batch: 900, loss is 4.230677766799927 and perplexity is 68.76382221920623
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772437944804152 and perplexity of 118.20707327979048
finished 46 epochs...
Completing Train Step...
At time: 678.650472164154 and batch: 50, loss is 4.396985006332398 and perplexity is 81.205664641757
At time: 679.397718667984 and batch: 100, loss is 4.2704462766647335 and perplexity is 71.55356117950608
At time: 680.1427335739136 and batch: 150, loss is 4.291674633026123 and perplexity is 73.08876292814611
At time: 680.8868310451508 and batch: 200, loss is 4.20129909992218 and perplexity is 66.77301954471343
At time: 681.6388750076294 and batch: 250, loss is 4.360466184616089 and perplexity is 78.29362520031013
At time: 682.3944323062897 and batch: 300, loss is 4.331455955505371 and perplexity is 76.05493860399208
At time: 683.1461117267609 and batch: 350, loss is 4.305067987442016 and perplexity is 74.07425140052803
At time: 683.8993339538574 and batch: 400, loss is 4.228790965080261 and perplexity is 68.6342008445631
At time: 684.6546452045441 and batch: 450, loss is 4.256432580947876 and perplexity is 70.55782461576796
At time: 685.4091141223907 and batch: 500, loss is 4.156017680168151 and perplexity is 63.816876682107484
At time: 686.1578505039215 and batch: 550, loss is 4.241511960029602 and perplexity is 69.51287311045168
At time: 686.9101090431213 and batch: 600, loss is 4.250344724655151 and perplexity is 70.1295835767534
At time: 687.6624000072479 and batch: 650, loss is 4.120443706512451 and perplexity is 61.586562561771494
At time: 688.416033744812 and batch: 700, loss is 4.153706007003784 and perplexity is 63.66952330289322
At time: 689.1684663295746 and batch: 750, loss is 4.232416334152222 and perplexity is 68.88347673911738
At time: 689.9300956726074 and batch: 800, loss is 4.163743185997009 and perplexity is 64.31180365392115
At time: 690.6899526119232 and batch: 850, loss is 4.2368901824951175 and perplexity is 69.19234135894486
At time: 691.4561371803284 and batch: 900, loss is 4.230669016838074 and perplexity is 68.76322054101729
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772435018461045 and perplexity of 118.20672736584248
finished 47 epochs...
Completing Train Step...
At time: 693.3669965267181 and batch: 50, loss is 4.396951599121094 and perplexity is 81.20295183227317
At time: 694.1313719749451 and batch: 100, loss is 4.270417375564575 and perplexity is 71.55149323275084
At time: 694.8778052330017 and batch: 150, loss is 4.291645975112915 and perplexity is 73.08666838673439
At time: 695.6320984363556 and batch: 200, loss is 4.201277174949646 and perplexity is 66.7715555641428
At time: 696.3899011611938 and batch: 250, loss is 4.3604372215271 and perplexity is 78.29135760791456
At time: 697.1409111022949 and batch: 300, loss is 4.331426110267639 and perplexity is 76.05266876014095
At time: 697.888650894165 and batch: 350, loss is 4.305049962997437 and perplexity is 74.07291626532147
At time: 698.6386158466339 and batch: 400, loss is 4.22876615524292 and perplexity is 68.63249806232709
At time: 699.3859899044037 and batch: 450, loss is 4.256407256126404 and perplexity is 70.55603777408194
At time: 700.1297326087952 and batch: 500, loss is 4.156001410484314 and perplexity is 63.81583841014657
At time: 700.8756926059723 and batch: 550, loss is 4.241499567031861 and perplexity is 69.51201164291035
At time: 701.6230437755585 and batch: 600, loss is 4.250339918136596 and perplexity is 70.1292464984188
At time: 702.3712327480316 and batch: 650, loss is 4.120436897277832 and perplexity is 61.586143205845396
At time: 703.116224527359 and batch: 700, loss is 4.153704514503479 and perplexity is 63.66942827618121
At time: 703.8582241535187 and batch: 750, loss is 4.232408032417298 and perplexity is 68.8829048891265
At time: 704.6101458072662 and batch: 800, loss is 4.163745675086975 and perplexity is 64.31196373198556
At time: 705.3637223243713 and batch: 850, loss is 4.236894183158874 and perplexity is 69.19261817479091
At time: 706.1207141876221 and batch: 900, loss is 4.230662546157837 and perplexity is 68.76277559764465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7724325101669525 and perplexity of 118.20643086897839
finished 48 epochs...
Completing Train Step...
At time: 708.007532119751 and batch: 50, loss is 4.39692006111145 and perplexity is 81.20039089317885
At time: 708.7715601921082 and batch: 100, loss is 4.270390634536743 and perplexity is 71.54957989786126
At time: 709.5235781669617 and batch: 150, loss is 4.29161789894104 and perplexity is 73.08461642167681
At time: 710.2740321159363 and batch: 200, loss is 4.201255512237549 and perplexity is 66.77010912682529
At time: 711.0235733985901 and batch: 250, loss is 4.360409812927246 and perplexity is 78.28921178082908
At time: 711.7750916481018 and batch: 300, loss is 4.331395683288574 and perplexity is 76.0503547423852
At time: 712.5245428085327 and batch: 350, loss is 4.305030870437622 and perplexity is 74.07150203723766
At time: 713.29052734375 and batch: 400, loss is 4.2287421226501465 and perplexity is 68.63084866526984
At time: 714.0442109107971 and batch: 450, loss is 4.256381764411926 and perplexity is 70.55423920263674
At time: 714.8002142906189 and batch: 500, loss is 4.155984444618225 and perplexity is 63.814755728362094
At time: 715.5663816928864 and batch: 550, loss is 4.241487164497375 and perplexity is 69.51114952313502
At time: 716.3306334018707 and batch: 600, loss is 4.250335311889648 and perplexity is 70.12892346653511
At time: 717.0811076164246 and batch: 650, loss is 4.12043098449707 and perplexity is 61.5857790615592
At time: 717.8365111351013 and batch: 700, loss is 4.153702239990235 and perplexity is 63.66928345938802
At time: 718.5936484336853 and batch: 750, loss is 4.23239812374115 and perplexity is 68.88222235411133
At time: 719.3511669635773 and batch: 800, loss is 4.163747425079346 and perplexity is 64.3120762775299
At time: 720.1077401638031 and batch: 850, loss is 4.236896872520447 and perplexity is 69.19280425900955
At time: 720.8608477115631 and batch: 900, loss is 4.23065637588501 and perplexity is 68.76235131386785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772431256019906 and perplexity of 118.20628262082519
finished 49 epochs...
Completing Train Step...
At time: 722.7594616413116 and batch: 50, loss is 4.396889820098877 and perplexity is 81.19793534826617
At time: 723.5123538970947 and batch: 100, loss is 4.270364370346069 and perplexity is 71.5477007307297
At time: 724.2654731273651 and batch: 150, loss is 4.291590480804444 and perplexity is 73.08261260515124
At time: 725.0184078216553 and batch: 200, loss is 4.201233940124512 and perplexity is 66.76866877001952
At time: 725.7749605178833 and batch: 250, loss is 4.360382976531983 and perplexity is 78.28711080878826
At time: 726.5219087600708 and batch: 300, loss is 4.331365461349487 and perplexity is 76.0480563879272
At time: 727.2746994495392 and batch: 350, loss is 4.305011281967163 and perplexity is 74.07005110401899
At time: 728.026734828949 and batch: 400, loss is 4.228718280792236 and perplexity is 68.62921239783361
At time: 728.7801010608673 and batch: 450, loss is 4.256356434822083 and perplexity is 70.55245211532922
At time: 729.5338113307953 and batch: 500, loss is 4.155966963768005 and perplexity is 63.813640201925594
At time: 730.2847454547882 and batch: 550, loss is 4.241474914550781 and perplexity is 69.51029802048112
At time: 731.0334403514862 and batch: 600, loss is 4.250330519676209 and perplexity is 70.12858739457084
At time: 731.7850527763367 and batch: 650, loss is 4.120425033569336 and perplexity is 61.58541257012901
At time: 732.5653605461121 and batch: 700, loss is 4.153700375556946 and perplexity is 63.66916475236714
At time: 733.3390228748322 and batch: 750, loss is 4.232386956214905 and perplexity is 68.88145311438063
At time: 734.0976195335388 and batch: 800, loss is 4.163748521804809 and perplexity is 64.31214681026023
At time: 734.8588011264801 and batch: 850, loss is 4.236898951530456 and perplexity is 69.19294811169172
At time: 735.6252839565277 and batch: 900, loss is 4.230650339126587 and perplexity is 68.76193621341731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772428747725813 and perplexity of 118.20598612507665
finished 50 epochs...
Completing Train Step...
At time: 737.5494656562805 and batch: 50, loss is 4.396860160827637 and perplexity is 81.195527112391
At time: 738.3065676689148 and batch: 100, loss is 4.270338735580444 and perplexity is 71.54586664569874
At time: 739.0658090114594 and batch: 150, loss is 4.2915636157989505 and perplexity is 73.08064926673481
At time: 739.831374168396 and batch: 200, loss is 4.201212420463562 and perplexity is 66.76723194636553
At time: 740.5929834842682 and batch: 250, loss is 4.360355978012085 and perplexity is 78.28499720120159
At time: 741.3521568775177 and batch: 300, loss is 4.331335053443909 and perplexity is 76.04574396096736
At time: 742.111730337143 and batch: 350, loss is 4.304991111755371 and perplexity is 74.06855711046794
At time: 742.8702993392944 and batch: 400, loss is 4.228694295883178 and perplexity is 68.62756635215574
At time: 743.626487493515 and batch: 450, loss is 4.25633113861084 and perplexity is 70.5506674281698
At time: 744.3857445716858 and batch: 500, loss is 4.155949497222901 and perplexity is 63.812525607834814
At time: 745.1422600746155 and batch: 550, loss is 4.241462416648865 and perplexity is 69.50942929302295
At time: 745.8957371711731 and batch: 600, loss is 4.250325684547424 and perplexity is 70.12824831463907
At time: 746.654618024826 and batch: 650, loss is 4.120418977737427 and perplexity is 61.585039620351715
At time: 747.4161131381989 and batch: 700, loss is 4.1536987781524655 and perplexity is 63.66906304703931
At time: 748.1745822429657 and batch: 750, loss is 4.232374606132507 and perplexity is 68.88060242801203
At time: 748.9298956394196 and batch: 800, loss is 4.163749074935913 and perplexity is 64.3121823833188
At time: 749.6848723888397 and batch: 850, loss is 4.236900525093079 and perplexity is 69.19305699121428
At time: 750.4353377819061 and batch: 900, loss is 4.230644736289978 and perplexity is 68.76155095260307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772427493578767 and perplexity of 118.20583787748122
finished 51 epochs...
Completing Train Step...
At time: 752.3490908145905 and batch: 50, loss is 4.3968311786651615 and perplexity is 81.19317392453236
At time: 753.1097316741943 and batch: 100, loss is 4.2703137731552125 and perplexity is 71.5440807096427
At time: 753.8657381534576 and batch: 150, loss is 4.2915373802185055 and perplexity is 73.0787319786327
At time: 754.6146311759949 and batch: 200, loss is 4.201191172599793 and perplexity is 66.76581330038856
At time: 755.357278585434 and batch: 250, loss is 4.360328979492188 and perplexity is 78.28288365067851
At time: 756.1016931533813 and batch: 300, loss is 4.331305155754089 and perplexity is 76.04347040288953
At time: 756.8454818725586 and batch: 350, loss is 4.304970798492431 and perplexity is 74.06705255167306
At time: 757.5896406173706 and batch: 400, loss is 4.2286704158782955 and perplexity is 68.6259275451036
At time: 758.3336045742035 and batch: 450, loss is 4.256306219100952 and perplexity is 70.54890936202041
At time: 759.0781395435333 and batch: 500, loss is 4.155931797027588 and perplexity is 63.81139612366421
At time: 759.8279919624329 and batch: 550, loss is 4.241449909210205 and perplexity is 69.50855991353666
At time: 760.5768351554871 and batch: 600, loss is 4.250320506095886 and perplexity is 70.12788515984403
At time: 761.3445246219635 and batch: 650, loss is 4.1204127597808835 and perplexity is 61.58465668844216
At time: 762.1053400039673 and batch: 700, loss is 4.153697009086609 and perplexity is 63.66895041237339
At time: 762.8770015239716 and batch: 750, loss is 4.2323627853393555 and perplexity is 68.87978820947095
At time: 763.64337682724 and batch: 800, loss is 4.163749122619629 and perplexity is 64.31218544996275
At time: 764.3919992446899 and batch: 850, loss is 4.236901760101318 and perplexity is 69.19314244526255
At time: 765.1611297130585 and batch: 900, loss is 4.230639514923095 and perplexity is 68.76119192425543
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772426657480736 and perplexity of 118.20573904585426
finished 52 epochs...
Completing Train Step...
At time: 767.1599223613739 and batch: 50, loss is 4.396802787780762 and perplexity is 81.19086881123967
At time: 767.9368827342987 and batch: 100, loss is 4.270288987159729 and perplexity is 71.54230744035756
At time: 768.7095439434052 and batch: 150, loss is 4.291511430740356 and perplexity is 73.07683564827853
At time: 769.4750497341156 and batch: 200, loss is 4.201170415878296 and perplexity is 66.764427475379
At time: 770.2275824546814 and batch: 250, loss is 4.360301246643067 and perplexity is 78.28071267338129
At time: 771.0170783996582 and batch: 300, loss is 4.3312756633758545 and perplexity is 76.04122773316912
At time: 771.7805869579315 and batch: 350, loss is 4.304950456619263 and perplexity is 74.06554590440818
At time: 772.546472787857 and batch: 400, loss is 4.22864670753479 and perplexity is 68.62430055732665
At time: 773.3029880523682 and batch: 450, loss is 4.256281456947327 and perplexity is 70.54716244071754
At time: 774.053774356842 and batch: 500, loss is 4.155914354324341 and perplexity is 63.810283090125026
At time: 774.8152782917023 and batch: 550, loss is 4.241437191963196 and perplexity is 69.5076759616317
At time: 775.5785267353058 and batch: 600, loss is 4.250315337181092 and perplexity is 70.12752267571773
At time: 776.3373184204102 and batch: 650, loss is 4.12040647983551 and perplexity is 61.58426994137668
At time: 777.0917418003082 and batch: 700, loss is 4.153694343566895 and perplexity is 63.66878070175706
At time: 777.8639841079712 and batch: 750, loss is 4.232352571487427 and perplexity is 68.87908468510615
At time: 778.6253132820129 and batch: 800, loss is 4.163749022483826 and perplexity is 64.3121790100107
At time: 779.3757104873657 and batch: 850, loss is 4.236903128623962 and perplexity is 69.19323713770957
At time: 780.1271488666534 and batch: 900, loss is 4.23063467502594 and perplexity is 68.7608591279636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772425821382705 and perplexity of 118.20564021430992
finished 53 epochs...
Completing Train Step...
At time: 781.9959440231323 and batch: 50, loss is 4.396774959564209 and perplexity is 81.18860944559752
At time: 782.7602782249451 and batch: 100, loss is 4.270264883041381 and perplexity is 71.54058299689534
At time: 783.5051352977753 and batch: 150, loss is 4.291486024856567 and perplexity is 73.07497909026826
At time: 784.2503616809845 and batch: 200, loss is 4.201149663925171 and perplexity is 66.76304199748536
At time: 784.9924411773682 and batch: 250, loss is 4.360273962020874 and perplexity is 78.27857684284882
At time: 785.7382941246033 and batch: 300, loss is 4.33124656200409 and perplexity is 76.03901486133036
At time: 786.4807727336884 and batch: 350, loss is 4.304930419921875 and perplexity is 74.06406189034541
At time: 787.2211532592773 and batch: 400, loss is 4.228623356819153 and perplexity is 68.62269814950727
At time: 787.9616222381592 and batch: 450, loss is 4.2562569856643675 and perplexity is 70.54543608226669
At time: 788.7013914585114 and batch: 500, loss is 4.15589684009552 and perplexity is 63.80916551201264
At time: 789.4411170482635 and batch: 550, loss is 4.241424722671509 and perplexity is 69.50680925554926
At time: 790.2038631439209 and batch: 600, loss is 4.250310068130493 and perplexity is 70.12715317122587
At time: 790.9465918540955 and batch: 650, loss is 4.120399689674377 and perplexity is 61.58385177568024
At time: 791.6897647380829 and batch: 700, loss is 4.153690886497498 and perplexity is 63.668560594744214
At time: 792.438901424408 and batch: 750, loss is 4.232343335151672 and perplexity is 68.87844849769154
At time: 793.188967704773 and batch: 800, loss is 4.163748774528504 and perplexity is 64.31216306346562
At time: 793.9383544921875 and batch: 850, loss is 4.236904406547547 and perplexity is 69.19332556143571
At time: 794.6864717006683 and batch: 900, loss is 4.230630164146423 and perplexity is 68.76054895671216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7724241491866435 and perplexity of 118.20544255146916
finished 54 epochs...
Completing Train Step...
At time: 796.5258958339691 and batch: 50, loss is 4.396747465133667 and perplexity is 81.18637724170104
At time: 797.2666118144989 and batch: 100, loss is 4.270240955352783 and perplexity is 71.53887121658279
At time: 798.0128037929535 and batch: 150, loss is 4.291460666656494 and perplexity is 73.07312606382295
At time: 798.753711938858 and batch: 200, loss is 4.201128363609314 and perplexity is 66.76161993874845
At time: 799.4945240020752 and batch: 250, loss is 4.360247783660888 and perplexity is 78.27652766490725
At time: 800.237553358078 and batch: 300, loss is 4.331217927932739 and perplexity is 76.03683758592568
At time: 800.9803035259247 and batch: 350, loss is 4.304910459518433 and perplexity is 74.06258355654364
At time: 801.7225186824799 and batch: 400, loss is 4.228600001335144 and perplexity is 68.62109545189395
At time: 802.4636054039001 and batch: 450, loss is 4.256232957839966 and perplexity is 70.54374104928026
At time: 803.2185678482056 and batch: 500, loss is 4.155879487991333 and perplexity is 63.8080582983309
At time: 803.9649143218994 and batch: 550, loss is 4.24141227722168 and perplexity is 69.5059442174248
At time: 804.7064597606659 and batch: 600, loss is 4.25030475616455 and perplexity is 70.12678065916595
At time: 805.4489533901215 and batch: 650, loss is 4.120392880439758 and perplexity is 61.58343243821245
At time: 806.1904726028442 and batch: 700, loss is 4.153686966896057 and perplexity is 63.66831103985146
At time: 806.9329390525818 and batch: 750, loss is 4.232334623336792 and perplexity is 68.87784844401277
At time: 807.6770720481873 and batch: 800, loss is 4.163748416900635 and perplexity is 64.31214006364793
At time: 808.4382834434509 and batch: 850, loss is 4.2369057035446165 and perplexity is 69.1934153050344
At time: 809.2387943267822 and batch: 900, loss is 4.230625834465027 and perplexity is 68.76025124608702
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7724241491866435 and perplexity of 118.20544255146916
finished 55 epochs...
Completing Train Step...
At time: 811.1466219425201 and batch: 50, loss is 4.396720209121704 and perplexity is 81.18416445498771
At time: 811.8980808258057 and batch: 100, loss is 4.270217237472534 and perplexity is 71.53717448632362
At time: 812.6523542404175 and batch: 150, loss is 4.291435327529907 and perplexity is 73.07127447809037
At time: 813.3994522094727 and batch: 200, loss is 4.201106576919556 and perplexity is 66.76016543989151
At time: 814.1455612182617 and batch: 250, loss is 4.360222034454345 and perplexity is 78.27451213237823
At time: 814.8918409347534 and batch: 300, loss is 4.331189494132996 and perplexity is 76.03467560044953
At time: 815.6370434761047 and batch: 350, loss is 4.304890441894531 and perplexity is 74.06110101443936
At time: 816.3794152736664 and batch: 400, loss is 4.228576636314392 and perplexity is 68.6194921373055
At time: 817.1221241950989 and batch: 450, loss is 4.256208992004394 and perplexity is 70.54205042984026
At time: 817.8674392700195 and batch: 500, loss is 4.15586217880249 and perplexity is 63.80695384215875
At time: 818.6119570732117 and batch: 550, loss is 4.2413995790481565 and perplexity is 69.5050616244879
At time: 819.3545870780945 and batch: 600, loss is 4.250299386978149 and perplexity is 70.12640413641968
At time: 820.1064486503601 and batch: 650, loss is 4.120385856628418 and perplexity is 61.58299988932038
At time: 820.8616495132446 and batch: 700, loss is 4.153682675361633 and perplexity is 63.66803780568922
At time: 821.6138918399811 and batch: 750, loss is 4.232325901985169 and perplexity is 68.87724773869694
At time: 822.3662493228912 and batch: 800, loss is 4.163748111724853 and perplexity is 64.31212043714332
At time: 823.117883682251 and batch: 850, loss is 4.2369070243835445 and perplexity is 69.19350669845127
At time: 823.871607542038 and batch: 900, loss is 4.230622239112854 and perplexity is 68.76000402921275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772423313088613 and perplexity of 118.20534372017272
finished 56 epochs...
Completing Train Step...
At time: 825.717848777771 and batch: 50, loss is 4.39669319152832 and perplexity is 81.1819710838732
At time: 826.4737312793732 and batch: 100, loss is 4.270193810462952 and perplexity is 71.53549860388199
At time: 827.2155277729034 and batch: 150, loss is 4.291410007476807 and perplexity is 73.06942433296346
At time: 827.970540523529 and batch: 200, loss is 4.201084809303284 and perplexity is 66.7587122460443
At time: 828.7143802642822 and batch: 250, loss is 4.36019624710083 and perplexity is 78.27249366588818
At time: 829.4569280147552 and batch: 300, loss is 4.331161346435547 and perplexity is 76.03253542952567
At time: 830.1996755599976 and batch: 350, loss is 4.304870109558106 and perplexity is 74.05959519452598
At time: 830.944408416748 and batch: 400, loss is 4.2285531854629514 and perplexity is 68.61788297065772
At time: 831.6901507377625 and batch: 450, loss is 4.25618510723114 and perplexity is 70.54036556908218
At time: 832.4344325065613 and batch: 500, loss is 4.155844826698303 and perplexity is 63.805846666853746
At time: 833.1791648864746 and batch: 550, loss is 4.241386818885803 and perplexity is 69.50417473427564
At time: 833.9243967533112 and batch: 600, loss is 4.250293874740601 and perplexity is 70.12601758408707
At time: 834.6749832630157 and batch: 650, loss is 4.120378642082215 and perplexity is 61.58255559752503
At time: 835.4251413345337 and batch: 700, loss is 4.153678097724915 and perplexity is 63.667746357208635
At time: 836.1746287345886 and batch: 750, loss is 4.232317185401916 and perplexity is 68.87664736704942
At time: 836.924510717392 and batch: 800, loss is 4.163747744560242 and perplexity is 64.31209682401293
At time: 837.6806552410126 and batch: 850, loss is 4.236908559799194 and perplexity is 69.19361293932587
At time: 838.4407215118408 and batch: 900, loss is 4.230619797706604 and perplexity is 68.75983615831404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772422895039598 and perplexity of 118.20529430455555
finished 57 epochs...
Completing Train Step...
At time: 840.3176362514496 and batch: 50, loss is 4.3966661643981935 and perplexity is 81.17977699782682
At time: 841.0798895359039 and batch: 100, loss is 4.270170502662658 and perplexity is 71.53383128819746
At time: 841.8297176361084 and batch: 150, loss is 4.29138503074646 and perplexity is 73.0675993204468
At time: 842.5845983028412 and batch: 200, loss is 4.2010631132125855 and perplexity is 66.75726385868073
At time: 843.3357203006744 and batch: 250, loss is 4.360170230865479 and perplexity is 78.27045733676037
At time: 844.0835726261139 and batch: 300, loss is 4.331133384704589 and perplexity is 76.03040945794905
At time: 844.8290832042694 and batch: 350, loss is 4.304849901199341 and perplexity is 74.05809858677834
At time: 845.5758340358734 and batch: 400, loss is 4.228529834747315 and perplexity is 68.61628071269189
At time: 846.3284089565277 and batch: 450, loss is 4.256161327362061 and perplexity is 70.53868814836865
At time: 847.0983786582947 and batch: 500, loss is 4.155827240943909 and perplexity is 63.80472460277156
At time: 847.854841709137 and batch: 550, loss is 4.241373987197876 and perplexity is 69.50328288411778
At time: 848.609016418457 and batch: 600, loss is 4.2502881002426145 and perplexity is 70.1256126427089
At time: 849.3543469905853 and batch: 650, loss is 4.120371265411377 and perplexity is 61.58210132495855
At time: 850.1163322925568 and batch: 700, loss is 4.153673253059387 and perplexity is 63.66743790901982
At time: 850.8707408905029 and batch: 750, loss is 4.232308373451233 and perplexity is 68.87604043210375
At time: 851.6183359622955 and batch: 800, loss is 4.163746972084045 and perplexity is 64.31204714446815
At time: 852.362601518631 and batch: 850, loss is 4.236910548210144 and perplexity is 69.19375052480028
At time: 853.1201000213623 and batch: 900, loss is 4.2306183195114135 and perplexity is 68.75973451793008
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772422058941567 and perplexity of 118.20519547338307
finished 58 epochs...
Completing Train Step...
At time: 854.9773788452148 and batch: 50, loss is 4.396639442443847 and perplexity is 81.17760774451553
At time: 855.731477022171 and batch: 100, loss is 4.270146422386169 and perplexity is 71.53210875450132
At time: 856.4967210292816 and batch: 150, loss is 4.29136025428772 and perplexity is 73.06578898651395
At time: 857.2689456939697 and batch: 200, loss is 4.201041474342346 and perplexity is 66.75581932253966
At time: 858.0236902236938 and batch: 250, loss is 4.360144968032837 and perplexity is 78.2684800282722
At time: 858.7741224765778 and batch: 300, loss is 4.331105604171753 and perplexity is 76.02829732200077
At time: 859.5273284912109 and batch: 350, loss is 4.304830245971679 and perplexity is 74.0566429722957
At time: 860.2796654701233 and batch: 400, loss is 4.228506278991699 and perplexity is 68.61466442338872
At time: 861.0274028778076 and batch: 450, loss is 4.256137690544128 and perplexity is 70.53702085794451
At time: 861.7757039070129 and batch: 500, loss is 4.155810089111328 and perplexity is 63.80363024420246
At time: 862.5191476345062 and batch: 550, loss is 4.241361398696899 and perplexity is 69.50240794748039
At time: 863.2726306915283 and batch: 600, loss is 4.250282573699951 and perplexity is 70.12522509158974
At time: 864.0318808555603 and batch: 650, loss is 4.120364055633545 and perplexity is 61.58165733329014
At time: 864.7818019390106 and batch: 700, loss is 4.153668732643127 and perplexity is 63.66715010634876
At time: 865.5408947467804 and batch: 750, loss is 4.232299680709839 and perplexity is 68.87544171309828
At time: 866.312609910965 and batch: 800, loss is 4.163746366500854 and perplexity is 64.31200819818524
At time: 867.0673809051514 and batch: 850, loss is 4.23691164970398 and perplexity is 69.19382674133192
At time: 867.8224189281464 and batch: 900, loss is 4.2306144285202025 and perplexity is 68.7594669749279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.77242080479452 and perplexity of 118.20504722677923
finished 59 epochs...
Completing Train Step...
At time: 869.6742577552795 and batch: 50, loss is 4.396612882614136 and perplexity is 81.17545170970952
At time: 870.4179401397705 and batch: 100, loss is 4.2701225852966305 and perplexity is 71.53040365754241
At time: 871.1625802516937 and batch: 150, loss is 4.291335697174072 and perplexity is 73.06399472366111
At time: 871.9081315994263 and batch: 200, loss is 4.201020069122315 and perplexity is 66.7543904148318
At time: 872.6529943943024 and batch: 250, loss is 4.36011981010437 and perplexity is 78.26651098021914
At time: 873.4017009735107 and batch: 300, loss is 4.331078085899353 and perplexity is 76.02620518339114
At time: 874.1448445320129 and batch: 350, loss is 4.304810914993286 and perplexity is 74.05521139876745
At time: 874.8966565132141 and batch: 400, loss is 4.228482913970947 and perplexity is 68.61306125905963
At time: 875.6462631225586 and batch: 450, loss is 4.256114473342896 and perplexity is 70.53538320474783
At time: 876.3961825370789 and batch: 500, loss is 4.155792856216431 and perplexity is 63.80253073242233
At time: 877.1525635719299 and batch: 550, loss is 4.2413489818573 and perplexity is 69.501544952587
At time: 877.9118056297302 and batch: 600, loss is 4.250277042388916 and perplexity is 70.12483720823111
At time: 878.6677505970001 and batch: 650, loss is 4.120356736183166 and perplexity is 61.58120659105462
At time: 879.4309742450714 and batch: 700, loss is 4.153664150238037 and perplexity is 63.66685835834449
At time: 880.1984119415283 and batch: 750, loss is 4.23229103565216 and perplexity is 68.87484628350579
At time: 880.9552748203278 and batch: 800, loss is 4.163745827674866 and perplexity is 64.31197354521319
At time: 881.7134072780609 and batch: 850, loss is 4.2369124221801755 and perplexity is 69.19388019193663
At time: 882.4777276515961 and batch: 900, loss is 4.2306096506118775 and perplexity is 68.75913844928306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.77242080479452 and perplexity of 118.20504722677923
finished 60 epochs...
Completing Train Step...
At time: 884.3686652183533 and batch: 50, loss is 4.396586713790893 and perplexity is 81.17332747145663
At time: 885.113073348999 and batch: 100, loss is 4.270098719596863 and perplexity is 71.52869655477515
At time: 885.8787040710449 and batch: 150, loss is 4.2913111877441406 and perplexity is 73.06220398874693
At time: 886.6206154823303 and batch: 200, loss is 4.2009986305236815 and perplexity is 66.7529593095892
At time: 887.365730047226 and batch: 250, loss is 4.360094890594483 and perplexity is 78.26456064142575
At time: 888.111691236496 and batch: 300, loss is 4.33105089187622 and perplexity is 76.02413775311969
At time: 888.8537585735321 and batch: 350, loss is 4.304791488647461 and perplexity is 74.05377279059414
At time: 889.5961570739746 and batch: 400, loss is 4.22845959186554 and perplexity is 68.61146107667248
At time: 890.3488273620605 and batch: 450, loss is 4.256091003417969 and perplexity is 70.53372776402593
At time: 891.1057665348053 and batch: 500, loss is 4.155775785446167 and perplexity is 63.80144158337427
At time: 891.8568408489227 and batch: 550, loss is 4.241336407661438 and perplexity is 69.50067103204252
At time: 892.6016137599945 and batch: 600, loss is 4.25027156829834 and perplexity is 70.12445333957125
At time: 893.3502724170685 and batch: 650, loss is 4.120349459648132 and perplexity is 61.58075849487771
At time: 894.1108198165894 and batch: 700, loss is 4.153659424781799 and perplexity is 63.66655750410236
At time: 894.8768343925476 and batch: 750, loss is 4.232282238006592 and perplexity is 68.87424034968501
At time: 895.6340439319611 and batch: 800, loss is 4.1637451410293576 and perplexity is 64.31192938570058
At time: 896.3892250061035 and batch: 850, loss is 4.2369131660461425 and perplexity is 69.19393166292838
At time: 897.1401500701904 and batch: 900, loss is 4.230604496002197 and perplexity is 68.75878402367584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772421222843536 and perplexity of 118.20509664229321
Annealing...
finished 61 epochs...
Completing Train Step...
At time: 899.0106906890869 and batch: 50, loss is 4.396561470031738 and perplexity is 81.17127837739166
At time: 899.7671372890472 and batch: 100, loss is 4.27007764339447 and perplexity is 71.52718901737629
At time: 900.5110399723053 and batch: 150, loss is 4.291279315948486 and perplexity is 73.05987540221967
At time: 901.2542889118195 and batch: 200, loss is 4.200968942642212 and perplexity is 66.75097758506222
At time: 901.9988133907318 and batch: 250, loss is 4.360047206878662 and perplexity is 78.26082878533241
At time: 902.7474675178528 and batch: 300, loss is 4.331001467704773 and perplexity is 76.02038041595367
At time: 903.4960024356842 and batch: 350, loss is 4.304730253219605 and perplexity is 74.04923821497269
At time: 904.2628948688507 and batch: 400, loss is 4.228401823043823 and perplexity is 68.60749758789397
At time: 905.0071620941162 and batch: 450, loss is 4.256029238700867 and perplexity is 70.52937140282054
At time: 905.7574260234833 and batch: 500, loss is 4.155701293945312 and perplexity is 63.79668909524621
At time: 906.5039744377136 and batch: 550, loss is 4.241264500617981 and perplexity is 69.49567362394687
At time: 907.2578337192535 and batch: 600, loss is 4.250207934379578 and perplexity is 70.11999118777783
At time: 908.0063905715942 and batch: 650, loss is 4.120273385047913 and perplexity is 61.57607394148403
At time: 908.7495865821838 and batch: 700, loss is 4.153559107780456 and perplexity is 63.660170986311215
At time: 909.5014932155609 and batch: 750, loss is 4.232188878059387 and perplexity is 68.86781055438952
At time: 910.2507150173187 and batch: 800, loss is 4.163658647537232 and perplexity is 64.30636706289849
At time: 911.0012490749359 and batch: 850, loss is 4.236827597618103 and perplexity is 69.18801110027636
At time: 911.7507820129395 and batch: 900, loss is 4.230521931648254 and perplexity is 68.7531072334485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772422476990582 and perplexity of 118.20524488895893
Annealing...
finished 62 epochs...
Completing Train Step...
At time: 913.6180794239044 and batch: 50, loss is 4.3965560340881344 and perplexity is 81.17083713609944
At time: 914.3889350891113 and batch: 100, loss is 4.270072569847107 and perplexity is 71.52682612171567
At time: 915.1398167610168 and batch: 150, loss is 4.291273345947266 and perplexity is 73.0594392359763
At time: 915.8901786804199 and batch: 200, loss is 4.200963134765625 and perplexity is 66.75058990474815
At time: 916.6407170295715 and batch: 250, loss is 4.360037612915039 and perplexity is 78.2600779573896
At time: 917.3907816410065 and batch: 300, loss is 4.330992298126221 and perplexity is 76.01968334429978
At time: 918.141330242157 and batch: 350, loss is 4.304719696044922 and perplexity is 74.04845646835624
At time: 918.8924467563629 and batch: 400, loss is 4.228389949798584 and perplexity is 68.60668299908576
At time: 919.6446621417999 and batch: 450, loss is 4.256017422676086 and perplexity is 70.52853803094385
At time: 920.3971633911133 and batch: 500, loss is 4.155686917304993 and perplexity is 63.7957719197865
At time: 921.1525673866272 and batch: 550, loss is 4.241250386238098 and perplexity is 69.49469274253143
At time: 921.9048590660095 and batch: 600, loss is 4.250195188522339 and perplexity is 70.11909745407625
At time: 922.6643400192261 and batch: 650, loss is 4.120257921218872 and perplexity is 61.575121746965934
At time: 923.4527263641357 and batch: 700, loss is 4.153538975715637 and perplexity is 63.65888938852319
At time: 924.2173051834106 and batch: 750, loss is 4.232170057296753 and perplexity is 68.8665144218711
At time: 924.9740087985992 and batch: 800, loss is 4.163641138076782 and perplexity is 64.30524110296525
At time: 925.7465534210205 and batch: 850, loss is 4.236810169219971 and perplexity is 69.18680527458073
At time: 926.5083191394806 and batch: 900, loss is 4.230505023002625 and perplexity is 68.75194472135065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772422058941567 and perplexity of 118.20519547338307
Annealing...
finished 63 epochs...
Completing Train Step...
At time: 928.4406599998474 and batch: 50, loss is 4.396555166244507 and perplexity is 81.17076669253628
At time: 929.187819480896 and batch: 100, loss is 4.270071601867675 and perplexity is 71.52675688525268
At time: 929.9436569213867 and batch: 150, loss is 4.29127254486084 and perplexity is 73.05938070907472
At time: 930.693437576294 and batch: 200, loss is 4.200962228775024 and perplexity is 66.7505294293685
At time: 931.439590215683 and batch: 250, loss is 4.360036191940307 and perplexity is 78.25996675187534
At time: 932.190924167633 and batch: 300, loss is 4.330991044044494 and perplexity is 76.01958800946383
At time: 932.948299407959 and batch: 350, loss is 4.304718084335327 and perplexity is 74.04833712384469
At time: 933.7021250724792 and batch: 400, loss is 4.228388090133667 and perplexity is 68.60655541376295
At time: 934.4592242240906 and batch: 450, loss is 4.256015634536743 and perplexity is 70.52841191620293
At time: 935.2058444023132 and batch: 500, loss is 4.155684595108032 and perplexity is 63.79562377361084
At time: 935.9661893844604 and batch: 550, loss is 4.241248021125793 and perplexity is 69.49452837997285
At time: 936.7365570068359 and batch: 600, loss is 4.250193166732788 and perplexity is 70.11895568816102
At time: 937.4989128112793 and batch: 650, loss is 4.120255422592163 and perplexity is 61.57496789391433
At time: 938.247237443924 and batch: 700, loss is 4.153535456657409 and perplexity is 63.65866536957888
At time: 938.9996132850647 and batch: 750, loss is 4.2321669960021975 and perplexity is 68.86630360150814
At time: 939.762357711792 and batch: 800, loss is 4.163638334274292 and perplexity is 64.30506080402292
At time: 940.511381149292 and batch: 850, loss is 4.236807169914246 and perplexity is 69.1865977625108
At time: 941.2608697414398 and batch: 900, loss is 4.230502080917359 and perplexity is 68.75174244756464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772422058941567 and perplexity of 118.20519547338307
Annealing...
finished 64 epochs...
Completing Train Step...
At time: 943.1270153522491 and batch: 50, loss is 4.396555185317993 and perplexity is 81.17076824074577
At time: 943.8706951141357 and batch: 100, loss is 4.270071611404419 and perplexity is 71.52675756738505
At time: 944.6166145801544 and batch: 150, loss is 4.291272344589234 and perplexity is 73.05936607735664
At time: 945.3631665706635 and batch: 200, loss is 4.200962166786194 and perplexity is 66.7505252915814
At time: 946.1098592281342 and batch: 250, loss is 4.360035810470581 and perplexity is 78.25993689807295
At time: 946.8603672981262 and batch: 300, loss is 4.330990886688232 and perplexity is 76.01957604730654
At time: 947.6070578098297 and batch: 350, loss is 4.304717874526977 and perplexity is 74.04832158788686
At time: 948.3534581661224 and batch: 400, loss is 4.22838794708252 and perplexity is 68.6065455995172
At time: 949.1002614498138 and batch: 450, loss is 4.256015481948853 and perplexity is 70.5284011544222
At time: 949.8475587368011 and batch: 500, loss is 4.155684418678284 and perplexity is 63.795612518165996
At time: 950.5954358577728 and batch: 550, loss is 4.241247811317444 and perplexity is 69.49451379944209
At time: 951.3497664928436 and batch: 600, loss is 4.250192995071411 and perplexity is 70.11894365144559
At time: 952.1079525947571 and batch: 650, loss is 4.1202552080154415 and perplexity is 61.57495468136099
At time: 952.8540539741516 and batch: 700, loss is 4.153535084724426 and perplexity is 63.65864169282597
At time: 953.5986909866333 and batch: 750, loss is 4.232166738510132 and perplexity is 68.86628586898364
At time: 954.3519334793091 and batch: 800, loss is 4.163638029098511 and perplexity is 64.30504117967871
At time: 955.1009345054626 and batch: 850, loss is 4.236806888580322 and perplexity is 69.18657829797648
At time: 955.8566555976868 and batch: 900, loss is 4.230501861572265 and perplexity is 68.75172736720893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772422058941567 and perplexity of 118.20519547338307
Annealing...
Model not improving. Stopping early with 118.20504722677923 lossat 64 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -118.20504722677923
langmodel
SETTINGS FOR THIS RUN
{'clip': 0.1631625429098229, 'rnn_dropout': 0.36001066296425077, 'anneal': 4.533431404677296, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 23.639581435799197, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.3130703929951033, 'seq_len': 35}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.07647880315780639 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.0776822566986084 and batch: 50, loss is 6.65484540939331 and perplexity is 776.5378683470991
At time: 1.9681942462921143 and batch: 100, loss is 5.686034660339356 and perplexity is 294.7226251630173
At time: 2.845400333404541 and batch: 150, loss is 5.4704928207397465 and perplexity is 237.5772469100107
At time: 3.7274398803710938 and batch: 200, loss is 5.2894345474243165 and perplexity is 198.23130331146555
At time: 4.604260206222534 and batch: 250, loss is 5.32713906288147 and perplexity is 205.84821213166882
At time: 5.482685565948486 and batch: 300, loss is 5.252956924438476 and perplexity is 191.13059243548784
At time: 6.358205795288086 and batch: 350, loss is 5.223953027725219 and perplexity is 185.6666808638629
At time: 7.236435174942017 and batch: 400, loss is 5.0784211921691895 and perplexity is 160.52042486015867
At time: 8.113504648208618 and batch: 450, loss is 5.0864440441131595 and perplexity is 161.81343634714867
At time: 9.00364065170288 and batch: 500, loss is 5.0182134056091305 and perplexity is 151.1410347104842
At time: 9.881593704223633 and batch: 550, loss is 5.084802207946777 and perplexity is 161.54798317009076
At time: 10.75846004486084 and batch: 600, loss is 5.01592752456665 and perplexity is 150.79593885879896
At time: 11.633126974105835 and batch: 650, loss is 4.908883981704712 and perplexity is 135.48812277823956
At time: 12.510650873184204 and batch: 700, loss is 4.990224561691284 and perplexity is 146.9694234925951
At time: 13.38945484161377 and batch: 750, loss is 4.991309614181518 and perplexity is 147.12897957925432
At time: 14.267674207687378 and batch: 800, loss is 4.936927747726441 and perplexity is 139.34149904512677
At time: 15.144527435302734 and batch: 850, loss is 5.002437295913697 and perplexity is 148.77532706475773
At time: 16.021713495254517 and batch: 900, loss is 4.920445604324341 and perplexity is 137.06367574721102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.8605409387039815 and perplexity of 129.09401517264058
finished 1 epochs...
Completing Train Step...
At time: 17.95835590362549 and batch: 50, loss is 4.837045965194702 and perplexity is 126.09630812646988
At time: 18.70071268081665 and batch: 100, loss is 4.67867678642273 and perplexity is 107.62756405354659
At time: 19.44323420524597 and batch: 150, loss is 4.6398521900177006 and perplexity is 103.52904382614945
At time: 20.186375856399536 and batch: 200, loss is 4.525083093643189 and perplexity is 92.30359535202635
At time: 20.928327322006226 and batch: 250, loss is 4.646624870300293 and perplexity is 104.23259270649484
At time: 21.67048192024231 and batch: 300, loss is 4.610159845352173 and perplexity is 100.50021284445613
At time: 22.41096067428589 and batch: 350, loss is 4.581023902893066 and perplexity is 97.61429061077916
At time: 23.15058398246765 and batch: 400, loss is 4.482193574905396 and perplexity is 88.42843447723662
At time: 23.891334056854248 and batch: 450, loss is 4.499520559310913 and perplexity is 89.973983769229
At time: 24.633010149002075 and batch: 500, loss is 4.393442420959473 and perplexity is 80.9184956027458
At time: 25.377944707870483 and batch: 550, loss is 4.455960054397583 and perplexity is 86.13880910305942
At time: 26.1159725189209 and batch: 600, loss is 4.450426998138428 and perplexity is 85.66351435506945
At time: 26.85520052909851 and batch: 650, loss is 4.302905807495117 and perplexity is 73.9142625642243
At time: 27.59666609764099 and batch: 700, loss is 4.316205835342407 and perplexity is 74.9038907680123
At time: 28.342649698257446 and batch: 750, loss is 4.410758991241455 and perplexity is 82.33192900882729
At time: 29.111042261123657 and batch: 800, loss is 4.3452695369720455 and perplexity is 77.11281943427991
At time: 29.861960649490356 and batch: 850, loss is 4.426626653671264 and perplexity is 83.64876418397364
At time: 30.611589193344116 and batch: 900, loss is 4.364761528968811 and perplexity is 78.63064657442668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.526071626846105 and perplexity of 92.39488563508425
finished 2 epochs...
Completing Train Step...
At time: 32.45771336555481 and batch: 50, loss is 4.39726243019104 and perplexity is 81.22819615583077
At time: 33.198636293411255 and batch: 100, loss is 4.241153416633606 and perplexity is 69.4879541963842
At time: 33.939385414123535 and batch: 150, loss is 4.233387451171875 and perplexity is 68.95040314717892
At time: 34.68574070930481 and batch: 200, loss is 4.123813900947571 and perplexity is 61.79447140201421
At time: 35.427775859832764 and batch: 250, loss is 4.278574948310852 and perplexity is 72.13756696337995
At time: 36.17001175880432 and batch: 300, loss is 4.2550584411621095 and perplexity is 70.46093488702795
At time: 36.912837266922 and batch: 350, loss is 4.243924555778503 and perplexity is 69.68078203935143
At time: 37.65945243835449 and batch: 400, loss is 4.160405912399292 and perplexity is 64.09753530448396
At time: 38.41236114501953 and batch: 450, loss is 4.190592193603516 and perplexity is 66.06190081406257
At time: 39.16578149795532 and batch: 500, loss is 4.066452255249024 and perplexity is 58.349585513207515
At time: 39.916735887527466 and batch: 550, loss is 4.143898220062256 and perplexity is 63.048118467990975
At time: 40.66678714752197 and batch: 600, loss is 4.162231049537659 and perplexity is 64.21462892009474
At time: 41.41391158103943 and batch: 650, loss is 4.004058113098145 and perplexity is 54.82016567854291
At time: 42.16034722328186 and batch: 700, loss is 4.019818735122681 and perplexity is 55.691010087710474
At time: 42.919952630996704 and batch: 750, loss is 4.130108346939087 and perplexity is 62.18466008735792
At time: 43.66451048851013 and batch: 800, loss is 4.077758302688599 and perplexity is 59.013032107264586
At time: 44.406826972961426 and batch: 850, loss is 4.151194987297058 and perplexity is 63.50984843243374
At time: 45.1462676525116 and batch: 900, loss is 4.10565327167511 and perplexity is 60.68237367780468
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.422446054955051 and perplexity of 83.29979223192088
finished 3 epochs...
Completing Train Step...
At time: 46.97348141670227 and batch: 50, loss is 4.169056158065796 and perplexity is 64.65439976632528
At time: 47.71605658531189 and batch: 100, loss is 4.026177139282226 and perplexity is 56.046244202115275
At time: 48.472460985183716 and batch: 150, loss is 4.021698288917541 and perplexity is 55.79578276916155
At time: 49.21430778503418 and batch: 200, loss is 3.91681779384613 and perplexity is 50.2403150905021
At time: 49.959943532943726 and batch: 250, loss is 4.076268396377563 and perplexity is 58.92517368495607
At time: 50.706069469451904 and batch: 300, loss is 4.0529553508758545 and perplexity is 57.56733757503714
At time: 51.46199035644531 and batch: 350, loss is 4.042470788955688 and perplexity is 56.966922305285244
At time: 52.213979721069336 and batch: 400, loss is 3.9757290124893188 and perplexity is 53.28895105958927
At time: 52.95928907394409 and batch: 450, loss is 4.000322017669678 and perplexity is 54.61573443327846
At time: 53.716752767562866 and batch: 500, loss is 3.8836291074752807 and perplexity is 48.600271022000946
At time: 54.473836183547974 and batch: 550, loss is 3.956934404373169 and perplexity is 52.29675924212044
At time: 55.229512453079224 and batch: 600, loss is 3.9786310720443727 and perplexity is 53.44382338485233
At time: 55.987257957458496 and batch: 650, loss is 3.821270270347595 and perplexity is 45.662174803161584
At time: 56.727237939834595 and batch: 700, loss is 3.8325595617294312 and perplexity is 46.18058915910451
At time: 57.47515511512756 and batch: 750, loss is 3.9513498830795286 and perplexity is 52.00552084695505
At time: 58.22395038604736 and batch: 800, loss is 3.90375837802887 and perplexity is 49.58847153716058
At time: 58.976479053497314 and batch: 850, loss is 3.9749396514892577 and perplexity is 53.24690343744849
At time: 59.72834777832031 and batch: 900, loss is 3.92781756401062 and perplexity is 50.795997596528046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393400427413313 and perplexity of 80.91509761951274
finished 4 epochs...
Completing Train Step...
At time: 61.58365845680237 and batch: 50, loss is 4.008951239585876 and perplexity is 55.08906502603208
At time: 62.34060716629028 and batch: 100, loss is 3.872944941520691 and perplexity is 48.083781702918934
At time: 63.09081315994263 and batch: 150, loss is 3.87320894241333 and perplexity is 48.09647753999238
At time: 63.83420753479004 and batch: 200, loss is 3.7759486770629884 and perplexity is 43.63888789669231
At time: 64.57786846160889 and batch: 250, loss is 3.923769063949585 and perplexity is 50.59076571811279
At time: 65.32414984703064 and batch: 300, loss is 3.902570343017578 and perplexity is 49.52959367822205
At time: 66.07756447792053 and batch: 350, loss is 3.8922119426727293 and perplexity is 49.01919434197752
At time: 66.84984874725342 and batch: 400, loss is 3.830266485214233 and perplexity is 46.07481485528029
At time: 67.60149002075195 and batch: 450, loss is 3.8550184535980225 and perplexity is 47.22948848722847
At time: 68.36185216903687 and batch: 500, loss is 3.7427391099929808 and perplexity is 42.213459259623356
At time: 69.11776518821716 and batch: 550, loss is 3.818391752243042 and perplexity is 45.530924400254854
At time: 69.8735032081604 and batch: 600, loss is 3.8510171937942506 and perplexity is 47.0408886035834
At time: 70.62757205963135 and batch: 650, loss is 3.688484239578247 and perplexity is 39.98419454205151
At time: 71.37603783607483 and batch: 700, loss is 3.698414945602417 and perplexity is 40.38324396543829
At time: 72.12866926193237 and batch: 750, loss is 3.8171739959716797 and perplexity is 45.47551257742055
At time: 72.87515306472778 and batch: 800, loss is 3.7678775072097777 and perplexity is 43.288088604970824
At time: 73.62579679489136 and batch: 850, loss is 3.8383302593231203 and perplexity is 46.44785378382337
At time: 74.38083696365356 and batch: 900, loss is 3.793988347053528 and perplexity is 44.4332626193825
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.390028026005993 and perplexity of 80.64267904101271
finished 5 epochs...
Completing Train Step...
At time: 76.22642350196838 and batch: 50, loss is 3.8806491184234617 and perplexity is 48.45565832558341
At time: 76.9886257648468 and batch: 100, loss is 3.752806930541992 and perplexity is 42.64060338945212
At time: 77.73902583122253 and batch: 150, loss is 3.752471480369568 and perplexity is 42.62630199053034
At time: 78.49054956436157 and batch: 200, loss is 3.6529220294952394 and perplexity is 38.58725457142939
At time: 79.23954582214355 and batch: 250, loss is 3.8010244226455687 and perplexity is 44.74700086263585
At time: 79.9874975681305 and batch: 300, loss is 3.778929090499878 and perplexity is 43.76914383640654
At time: 80.74438953399658 and batch: 350, loss is 3.7718446111679076 and perplexity is 43.46015803560739
At time: 81.50051856040955 and batch: 400, loss is 3.71380211353302 and perplexity is 41.009433004952044
At time: 82.25764036178589 and batch: 450, loss is 3.735359287261963 and perplexity is 41.90307810104697
At time: 83.01564979553223 and batch: 500, loss is 3.626096920967102 and perplexity is 37.5659073990178
At time: 83.78193783760071 and batch: 550, loss is 3.7034715461730956 and perplexity is 40.58796305504386
At time: 84.54092931747437 and batch: 600, loss is 3.732261161804199 and perplexity is 41.77345800136736
At time: 85.30662274360657 and batch: 650, loss is 3.575881795883179 and perplexity is 35.72611006684837
At time: 86.09820699691772 and batch: 700, loss is 3.593658652305603 and perplexity is 36.36688661782995
At time: 86.86605668067932 and batch: 750, loss is 3.7076253032684328 and perplexity is 40.75690622610899
At time: 87.63039803504944 and batch: 800, loss is 3.6567104053497315 and perplexity is 38.73371484305336
At time: 88.38381624221802 and batch: 850, loss is 3.7282060527801515 and perplexity is 41.60440507055051
At time: 89.14313530921936 and batch: 900, loss is 3.6786127853393555 and perplexity is 39.59143414289711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.405487060546875 and perplexity of 81.89902291326351
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 91.01781678199768 and batch: 50, loss is 3.782311787605286 and perplexity is 43.91745229244061
At time: 91.76231527328491 and batch: 100, loss is 3.634747180938721 and perplexity is 37.89227179746529
At time: 92.5070526599884 and batch: 150, loss is 3.63457528591156 and perplexity is 37.88575886416194
At time: 93.25243782997131 and batch: 200, loss is 3.501090259552002 and perplexity is 33.151576085267884
At time: 94.00117802619934 and batch: 250, loss is 3.624657039642334 and perplexity is 37.51185587373546
At time: 94.75428295135498 and batch: 300, loss is 3.591068263053894 and perplexity is 36.272804133353986
At time: 95.52863693237305 and batch: 350, loss is 3.5572508573532104 and perplexity is 35.06666125650147
At time: 96.28375506401062 and batch: 400, loss is 3.4769498205184934 and perplexity is 32.36086494025979
At time: 97.04320049285889 and batch: 450, loss is 3.472309513092041 and perplexity is 32.211048444476354
At time: 97.80764627456665 and batch: 500, loss is 3.3539817762374877 and perplexity is 28.616451400378796
At time: 98.57061386108398 and batch: 550, loss is 3.3951766872406006 and perplexity is 29.81992180920004
At time: 99.33771133422852 and batch: 600, loss is 3.399982991218567 and perplexity is 29.963590398902728
At time: 100.1046895980835 and batch: 650, loss is 3.2226376342773437 and perplexity is 25.094222347371694
At time: 100.8705005645752 and batch: 700, loss is 3.1984545278549192 and perplexity is 24.494645137734008
At time: 101.6369240283966 and batch: 750, loss is 3.2776062440872193 and perplexity is 26.51223286660334
At time: 102.4007625579834 and batch: 800, loss is 3.1960389518737795 and perplexity is 24.435547866982727
At time: 103.15204238891602 and batch: 850, loss is 3.2123764848709104 and perplexity is 24.838043375756712
At time: 103.90748071670532 and batch: 900, loss is 3.1306612062454224 and perplexity is 22.88910896185403
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36095731552333 and perplexity of 78.33208706361773
finished 7 epochs...
Completing Train Step...
At time: 105.78859400749207 and batch: 50, loss is 3.5754437589645387 and perplexity is 35.71046413867695
At time: 106.53524470329285 and batch: 100, loss is 3.4318072843551635 and perplexity is 30.93249607928019
At time: 107.2826783657074 and batch: 150, loss is 3.4341913223266602 and perplexity is 31.00632829892432
At time: 108.02733945846558 and batch: 200, loss is 3.311424427032471 and perplexity is 27.424161380214223
At time: 108.7914650440216 and batch: 250, loss is 3.4312480878829956 and perplexity is 30.915203572002458
At time: 109.5405375957489 and batch: 300, loss is 3.4106813049316407 and perplexity is 30.28587114537301
At time: 110.29039287567139 and batch: 350, loss is 3.379795665740967 and perplexity is 29.364770271646417
At time: 111.05271792411804 and batch: 400, loss is 3.3082762241363524 and perplexity is 27.337960316349054
At time: 111.80211400985718 and batch: 450, loss is 3.312231249809265 and perplexity is 27.446296746708413
At time: 112.54979491233826 and batch: 500, loss is 3.1993017864227293 and perplexity is 24.515407229880648
At time: 113.2999222278595 and batch: 550, loss is 3.2462857866287234 and perplexity is 25.694726765564276
At time: 114.05369472503662 and batch: 600, loss is 3.2598250198364256 and perplexity is 26.04497938901796
At time: 114.80865597724915 and batch: 650, loss is 3.0935784339904786 and perplexity is 22.055862376922054
At time: 115.56365466117859 and batch: 700, loss is 3.0779292678833006 and perplexity is 21.713393190633987
At time: 116.32075119018555 and batch: 750, loss is 3.1651134634017946 and perplexity is 23.69143199080284
At time: 117.09235882759094 and batch: 800, loss is 3.0928468990325926 and perplexity is 22.039733642652752
At time: 117.8499653339386 and batch: 850, loss is 3.1251570844650267 and perplexity is 22.76347060029249
At time: 118.6029486656189 and batch: 900, loss is 3.0608305740356445 and perplexity is 21.3452786357286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.386814483224529 and perplexity of 80.3839462889444
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 120.50094175338745 and batch: 50, loss is 3.469895348548889 and perplexity is 32.13337946399581
At time: 121.26407241821289 and batch: 100, loss is 3.338402786254883 and perplexity is 28.174090703994022
At time: 122.01175022125244 and batch: 150, loss is 3.3450565195083617 and perplexity is 28.362178637746233
At time: 122.7654402256012 and batch: 200, loss is 3.213377194404602 and perplexity is 24.86291148336198
At time: 123.51660752296448 and batch: 250, loss is 3.332266745567322 and perplexity is 28.001742645205844
At time: 124.26700615882874 and batch: 300, loss is 3.308976354598999 and perplexity is 27.35710715701424
At time: 125.03381252288818 and batch: 350, loss is 3.2702146482467653 and perplexity is 26.316987632301956
At time: 125.79626369476318 and batch: 400, loss is 3.1942971324920655 and perplexity is 24.393022602521715
At time: 126.55857110023499 and batch: 450, loss is 3.193440861701965 and perplexity is 24.37214450971108
At time: 127.3161084651947 and batch: 500, loss is 3.0718812799453734 and perplexity is 21.582467169110846
At time: 128.07262539863586 and batch: 550, loss is 3.107831587791443 and perplexity is 22.372479008263742
At time: 128.82813906669617 and batch: 600, loss is 3.1171110820770265 and perplexity is 22.581050521804137
At time: 129.58878254890442 and batch: 650, loss is 2.9413059425354002 and perplexity is 18.940565457926013
At time: 130.34526443481445 and batch: 700, loss is 2.917252235412598 and perplexity is 18.490410297903047
At time: 131.107351064682 and batch: 750, loss is 2.9931536960601806 and perplexity is 19.948494883629124
At time: 131.8720965385437 and batch: 800, loss is 2.911933422088623 and perplexity is 18.392324338933232
At time: 132.62659358978271 and batch: 850, loss is 2.932367730140686 and perplexity is 18.772025008234895
At time: 133.38040208816528 and batch: 900, loss is 2.869064836502075 and perplexity is 17.62053241385868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38774087984268 and perplexity of 80.45844820877517
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.2522885799408 and batch: 50, loss is 3.4265476655960083 and perplexity is 30.77023004544346
At time: 136.01018595695496 and batch: 100, loss is 3.2925617599487307 and perplexity is 26.911716783859557
At time: 136.75415873527527 and batch: 150, loss is 3.301520709991455 and perplexity is 27.15390074724352
At time: 137.50634336471558 and batch: 200, loss is 3.1705245923995973 and perplexity is 23.819976858224837
At time: 138.26170682907104 and batch: 250, loss is 3.289694833755493 and perplexity is 26.83467336976787
At time: 139.01375317573547 and batch: 300, loss is 3.2679463386535645 and perplexity is 26.2573602090859
At time: 139.76140117645264 and batch: 350, loss is 3.227751512527466 and perplexity is 25.222879834214734
At time: 140.50354981422424 and batch: 400, loss is 3.153643102645874 and perplexity is 23.42123530821257
At time: 141.24943923950195 and batch: 450, loss is 3.1520494270324706 and perplexity is 23.383939183522013
At time: 142.00279545783997 and batch: 500, loss is 3.0293389844894407 and perplexity is 20.683555918366096
At time: 142.75650453567505 and batch: 550, loss is 3.06332396030426 and perplexity is 21.398567067092486
At time: 143.5242154598236 and batch: 600, loss is 3.072408218383789 and perplexity is 21.593842797522814
At time: 144.2983136177063 and batch: 650, loss is 2.895742998123169 and perplexity is 18.096942441589594
At time: 145.05993485450745 and batch: 700, loss is 2.869622144699097 and perplexity is 17.63035521791918
At time: 145.82521438598633 and batch: 750, loss is 2.941320915222168 and perplexity is 18.94084905120289
At time: 146.58721160888672 and batch: 800, loss is 2.8592926359176634 and perplexity is 17.44917964713168
At time: 147.3483190536499 and batch: 850, loss is 2.876327495574951 and perplexity is 17.748970168725965
At time: 148.104585647583 and batch: 900, loss is 2.814189715385437 and perplexity is 16.679655035393882
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38312227432042 and perplexity of 80.08769920631373
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 149.9996621608734 and batch: 50, loss is 3.412234859466553 and perplexity is 30.33295846472337
At time: 150.74295282363892 and batch: 100, loss is 3.279286708831787 and perplexity is 26.556823194979376
At time: 151.49295449256897 and batch: 150, loss is 3.2876584911346436 and perplexity is 26.780084380474843
At time: 152.24724674224854 and batch: 200, loss is 3.1576342248916625 and perplexity is 23.51489910898316
At time: 153.00803136825562 and batch: 250, loss is 3.2771670913696287 and perplexity is 26.50059250362617
At time: 153.75055360794067 and batch: 300, loss is 3.2553911638259887 and perplexity is 25.929755332815837
At time: 154.49413466453552 and batch: 350, loss is 3.2143906688690187 and perplexity is 24.88812218230333
At time: 155.24275732040405 and batch: 400, loss is 3.141174931526184 and perplexity is 23.13102827354793
At time: 156.00017189979553 and batch: 450, loss is 3.1401105976104735 and perplexity is 23.106422232495966
At time: 156.75324130058289 and batch: 500, loss is 3.017432146072388 and perplexity is 20.438740541065776
At time: 157.51362872123718 and batch: 550, loss is 3.0513738584518433 and perplexity is 21.14437385353761
At time: 158.26580691337585 and batch: 600, loss is 3.0608949851989746 and perplexity is 21.34665355423671
At time: 159.02789282798767 and batch: 650, loss is 2.8835702466964723 and perplexity is 17.87798820107474
At time: 159.78984570503235 and batch: 700, loss is 2.857345280647278 and perplexity is 17.415232959040615
At time: 160.54635310173035 and batch: 750, loss is 2.928197350502014 and perplexity is 18.693901552825114
At time: 161.3016767501831 and batch: 800, loss is 2.845849928855896 and perplexity is 17.216184984788022
At time: 162.05356216430664 and batch: 850, loss is 2.8629804754257204 and perplexity is 17.513648223032945
At time: 162.83777451515198 and batch: 900, loss is 2.7999714183807374 and perplexity is 16.444176763180955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380954272126498 and perplexity of 79.91425697822872
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 164.79926133155823 and batch: 50, loss is 3.4084930419921875 and perplexity is 30.219670154960397
At time: 165.5426139831543 and batch: 100, loss is 3.275996289253235 and perplexity is 26.469583709960038
At time: 166.28459215164185 and batch: 150, loss is 3.2840730142593384 and perplexity is 26.6842369392836
At time: 167.02782607078552 and batch: 200, loss is 3.1543496227264405 and perplexity is 23.437788728242737
At time: 167.78687000274658 and batch: 250, loss is 3.2741482782363893 and perplexity is 26.420712798552493
At time: 168.53737330436707 and batch: 300, loss is 3.252318720817566 and perplexity is 25.850209899314027
At time: 169.28870725631714 and batch: 350, loss is 3.211187958717346 and perplexity is 24.808540247693916
At time: 170.03519773483276 and batch: 400, loss is 3.138166208267212 and perplexity is 23.061538001697187
At time: 170.78229880332947 and batch: 450, loss is 3.137251305580139 and perplexity is 23.04044858746334
At time: 171.53134965896606 and batch: 500, loss is 3.0146887493133545 and perplexity is 20.38274580950831
At time: 172.27965021133423 and batch: 550, loss is 3.0484760856628417 and perplexity is 21.083190952230805
At time: 173.0336458683014 and batch: 600, loss is 3.0581715726852416 and perplexity is 21.288596902810006
At time: 173.79045796394348 and batch: 650, loss is 2.880784215927124 and perplexity is 17.828248895643092
At time: 174.54179334640503 and batch: 700, loss is 2.854547176361084 and perplexity is 17.366571432814574
At time: 175.2915482521057 and batch: 750, loss is 2.9253338384628296 and perplexity is 18.6404479097698
At time: 176.04082369804382 and batch: 800, loss is 2.842824821472168 and perplexity is 17.164182872115546
At time: 176.78959727287292 and batch: 850, loss is 2.859937686920166 and perplexity is 17.46043888895812
At time: 177.53987383842468 and batch: 900, loss is 2.7967489671707155 and perplexity is 16.39127149397854
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380496090405608 and perplexity of 79.87765011338136
Annealing...
Model not improving. Stopping early with 78.33208706361773 lossat 11 epochs.
Finished Training.
Improved accuracyfrom -118.20504722677923 to -78.33208706361773
langmodel
SETTINGS FOR THIS RUN
{'clip': 0.08412413531206164, 'rnn_dropout': 0.979596838791089, 'anneal': 4.6703586663889, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 22.165990174463836, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.40945704415214257, 'seq_len': 35}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.0789737621943156 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1249580383300781 and batch: 50, loss is 7.003380241394043 and perplexity is 1100.346315372508
At time: 2.0337159633636475 and batch: 100, loss is 6.267614917755127 and perplexity is 527.21841782534
At time: 2.9205477237701416 and batch: 150, loss is 6.1865394496917725 and perplexity is 486.16080793280065
At time: 3.8030009269714355 and batch: 200, loss is 6.037862663269043 and perplexity is 418.9965404766353
At time: 4.688662052154541 and batch: 250, loss is 6.102858915328979 and perplexity is 447.1342635198375
At time: 5.575117111206055 and batch: 300, loss is 6.025459098815918 and perplexity is 413.8315880097482
At time: 6.46248197555542 and batch: 350, loss is 6.014505548477173 and perplexity is 409.3233982946043
At time: 7.348480939865112 and batch: 400, loss is 5.897970733642578 and perplexity is 364.29746070235205
At time: 8.237605571746826 and batch: 450, loss is 5.906948566436768 and perplexity is 367.5827878841182
At time: 9.122740030288696 and batch: 500, loss is 5.864404773712158 and perplexity is 352.2724118912265
At time: 10.006164789199829 and batch: 550, loss is 5.912141313552857 and perplexity is 369.4965168007376
At time: 10.889972686767578 and batch: 600, loss is 5.839448671340943 and perplexity is 343.58985750061527
At time: 11.78485369682312 and batch: 650, loss is 5.765087594985962 and perplexity is 318.9669827209589
At time: 12.675699472427368 and batch: 700, loss is 5.86337028503418 and perplexity is 351.90817849970347
At time: 13.56054949760437 and batch: 750, loss is 5.824181566238403 and perplexity is 338.3840747652793
At time: 14.449653625488281 and batch: 800, loss is 5.825773372650146 and perplexity is 338.9231456395325
At time: 15.339356422424316 and batch: 850, loss is 5.8737424373626705 and perplexity is 355.57721876163305
At time: 16.23120903968811 and batch: 900, loss is 5.740591897964477 and perplexity is 311.24858387347706
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.6814332726883565 and perplexity of 293.36960738203675
finished 1 epochs...
Completing Train Step...
At time: 18.180543899536133 and batch: 50, loss is 5.643910503387451 and perplexity is 282.5655342664043
At time: 18.956830739974976 and batch: 100, loss is 5.459813461303711 and perplexity is 235.05357368824784
At time: 19.714228630065918 and batch: 150, loss is 5.380586004257202 and perplexity is 217.14948867236595
At time: 20.466152906417847 and batch: 200, loss is 5.182952508926392 and perplexity is 178.20819629962992
At time: 21.20663356781006 and batch: 250, loss is 5.208713970184326 and perplexity is 182.85874511805548
At time: 21.954802751541138 and batch: 300, loss is 5.117346296310425 and perplexity is 166.8918998123261
At time: 22.694953441619873 and batch: 350, loss is 5.066002531051636 and perplexity is 158.53930300303404
At time: 23.435666799545288 and batch: 400, loss is 4.907750120162964 and perplexity is 135.3345850681596
At time: 24.175618648529053 and batch: 450, loss is 4.903723115921021 and perplexity is 134.79068799294365
At time: 24.936175107955933 and batch: 500, loss is 4.811064901351929 and perplexity is 122.86238419713324
At time: 25.673616647720337 and batch: 550, loss is 4.865705280303955 and perplexity is 129.7624252288696
At time: 26.415966272354126 and batch: 600, loss is 4.786421394348144 and perplexity is 119.87162690215973
At time: 27.158282041549683 and batch: 650, loss is 4.653373727798462 and perplexity is 104.93842271537038
At time: 27.8956561088562 and batch: 700, loss is 4.7110062599182125 and perplexity is 111.16396348533969
At time: 28.63202166557312 and batch: 750, loss is 4.721459131240845 and perplexity is 112.33204033429881
At time: 29.373557806015015 and batch: 800, loss is 4.6532541942596435 and perplexity is 104.92587980400943
At time: 30.11434245109558 and batch: 850, loss is 4.702032537460327 and perplexity is 110.17087146008396
At time: 30.852675199508667 and batch: 900, loss is 4.629333248138428 and perplexity is 102.44573544907291
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7174444329248715 and perplexity of 111.8819651437121
finished 2 epochs...
Completing Train Step...
At time: 32.66852021217346 and batch: 50, loss is 4.691039886474609 and perplexity is 108.96643363217048
At time: 33.42031383514404 and batch: 100, loss is 4.552769422531128 and perplexity is 94.89484867507299
At time: 34.15843415260315 and batch: 150, loss is 4.548650188446045 and perplexity is 94.5047585677352
At time: 34.8976628780365 and batch: 200, loss is 4.431773452758789 and perplexity is 84.08039737900212
At time: 35.63599991798401 and batch: 250, loss is 4.558464517593384 and perplexity is 95.43682569965658
At time: 36.37462592124939 and batch: 300, loss is 4.521750373840332 and perplexity is 91.9964853719343
At time: 37.11494445800781 and batch: 350, loss is 4.507758140563965 and perplexity is 90.71821288663052
At time: 37.854095220565796 and batch: 400, loss is 4.407177014350891 and perplexity is 82.03754549399437
At time: 38.59645104408264 and batch: 450, loss is 4.423656816482544 and perplexity is 83.40070949664599
At time: 39.33863353729248 and batch: 500, loss is 4.316486520767212 and perplexity is 74.92491814931374
At time: 40.077115535736084 and batch: 550, loss is 4.396251840591431 and perplexity is 81.14614925045812
At time: 40.81600308418274 and batch: 600, loss is 4.375778932571411 and perplexity is 79.50174193651803
At time: 41.554306745529175 and batch: 650, loss is 4.229699172973633 and perplexity is 68.69656328226789
At time: 42.29229426383972 and batch: 700, loss is 4.259888081550598 and perplexity is 70.80205895418337
At time: 43.04407620429993 and batch: 750, loss is 4.333560361862182 and perplexity is 76.21515762409157
At time: 43.78269028663635 and batch: 800, loss is 4.269328260421753 and perplexity is 71.47360783875934
At time: 44.52153563499451 and batch: 850, loss is 4.341825947761536 and perplexity is 76.84773125064658
At time: 45.26107096672058 and batch: 900, loss is 4.286285820007325 and perplexity is 72.6959605707383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499301022046233 and perplexity of 89.95423329500186
finished 3 epochs...
Completing Train Step...
At time: 47.089627742767334 and batch: 50, loss is 4.381196594238281 and perplexity is 79.93362431621357
At time: 47.842013120651245 and batch: 100, loss is 4.2347766780853275 and perplexity is 69.04625746921013
At time: 48.582191705703735 and batch: 150, loss is 4.2386097764968875 and perplexity is 69.31142645377693
At time: 49.321823596954346 and batch: 200, loss is 4.126659274101257 and perplexity is 61.97055011801687
At time: 50.06142830848694 and batch: 250, loss is 4.273918981552124 and perplexity is 71.80247753749659
At time: 50.80104947090149 and batch: 300, loss is 4.247195782661438 and perplexity is 69.9090969180687
At time: 51.539984941482544 and batch: 350, loss is 4.237786202430725 and perplexity is 69.25436686008686
At time: 52.28123354911804 and batch: 400, loss is 4.152378549575806 and perplexity is 63.585060793844406
At time: 53.02114796638489 and batch: 450, loss is 4.171105437278747 and perplexity is 64.78703053611626
At time: 53.760637044906616 and batch: 500, loss is 4.056907696723938 and perplexity is 57.795313826413626
At time: 54.49993705749512 and batch: 550, loss is 4.142562098503113 and perplexity is 62.96393476999331
At time: 55.24122643470764 and batch: 600, loss is 4.145404772758484 and perplexity is 63.14317536699557
At time: 55.982168674468994 and batch: 650, loss is 3.986363048553467 and perplexity is 53.85865142550039
At time: 56.725157260894775 and batch: 700, loss is 4.004588918685913 and perplexity is 54.84927225309026
At time: 57.46795415878296 and batch: 750, loss is 4.099685282707214 and perplexity is 60.321300453351405
At time: 58.212878465652466 and batch: 800, loss is 4.04403995513916 and perplexity is 57.05638304436998
At time: 58.958871364593506 and batch: 850, loss is 4.12087728023529 and perplexity is 61.61327066652608
At time: 59.70605134963989 and batch: 900, loss is 4.0777058506011965 and perplexity is 59.00993683172416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.411005725599315 and perplexity of 82.35224563076396
finished 4 epochs...
Completing Train Step...
At time: 61.549808502197266 and batch: 50, loss is 4.174679617881775 and perplexity is 65.01900539705841
At time: 62.298378705978394 and batch: 100, loss is 4.02766369342804 and perplexity is 56.12962193619503
At time: 63.045456647872925 and batch: 150, loss is 4.03825044631958 and perplexity is 56.727008988529306
At time: 63.79242181777954 and batch: 200, loss is 3.9282813596725465 and perplexity is 50.81956202397375
At time: 64.54442977905273 and batch: 250, loss is 4.07790472984314 and perplexity is 59.02167385031437
At time: 65.29140758514404 and batch: 300, loss is 4.057694611549377 and perplexity is 57.84081171483818
At time: 66.04558801651001 and batch: 350, loss is 4.0430547761917115 and perplexity is 57.00019997670899
At time: 66.79727435112 and batch: 400, loss is 3.970929570198059 and perplexity is 53.033806579584244
At time: 67.54648971557617 and batch: 450, loss is 3.9954798555374147 and perplexity is 54.35191543483182
At time: 68.29068779945374 and batch: 500, loss is 3.878351469039917 and perplexity is 48.344452017578185
At time: 69.04280543327332 and batch: 550, loss is 3.96173330783844 and perplexity is 52.54832948914094
At time: 69.79536056518555 and batch: 600, loss is 3.9764657735824587 and perplexity is 53.32822675204864
At time: 70.54841303825378 and batch: 650, loss is 3.8155474138259886 and perplexity is 45.401603046853005
At time: 71.31034994125366 and batch: 700, loss is 3.8268200874328615 and perplexity is 45.916296031949116
At time: 72.0706627368927 and batch: 750, loss is 3.9285786724090577 and perplexity is 50.834673573344126
At time: 72.82741355895996 and batch: 800, loss is 3.8789777755737305 and perplexity is 48.374739947524574
At time: 73.58296275138855 and batch: 850, loss is 3.957926082611084 and perplexity is 52.348646523666005
At time: 74.33958029747009 and batch: 900, loss is 3.917915596961975 and perplexity is 50.29549935013337
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.379429647367295 and perplexity of 79.79251055581445
finished 5 epochs...
Completing Train Step...
At time: 76.2693030834198 and batch: 50, loss is 4.018320231437683 and perplexity is 55.60761940010372
At time: 77.02934002876282 and batch: 100, loss is 3.8735249519348143 and perplexity is 48.11167888660351
At time: 77.78843808174133 and batch: 150, loss is 3.883799424171448 and perplexity is 48.60854916452718
At time: 78.54597520828247 and batch: 200, loss is 3.775987458229065 and perplexity is 43.64058029646764
At time: 79.30271244049072 and batch: 250, loss is 3.9249421882629396 and perplexity is 50.650149801053246
At time: 80.06657481193542 and batch: 300, loss is 3.9065448427200318 and perplexity is 49.726840753125565
At time: 80.84547305107117 and batch: 350, loss is 3.89063355922699 and perplexity is 48.94188428561141
At time: 81.60423183441162 and batch: 400, loss is 3.8276310968399048 and perplexity is 45.953549684457
At time: 82.37145471572876 and batch: 450, loss is 3.8532038021087645 and perplexity is 47.14386114100119
At time: 83.14147090911865 and batch: 500, loss is 3.736368708610535 and perplexity is 41.94539731802021
At time: 83.8907380104065 and batch: 550, loss is 3.817224359512329 and perplexity is 45.47780294292177
At time: 84.64338850975037 and batch: 600, loss is 3.838063540458679 and perplexity is 46.435466916985376
At time: 85.40403175354004 and batch: 650, loss is 3.6757106161117554 and perplexity is 39.476699671007935
At time: 86.1586241722107 and batch: 700, loss is 3.6856859731674194 and perplexity is 39.87246451160272
At time: 86.91771507263184 and batch: 750, loss is 3.7911612367630005 and perplexity is 44.30782228576758
At time: 87.68501162528992 and batch: 800, loss is 3.7449636888504028 and perplexity is 42.30747095803924
At time: 88.45010757446289 and batch: 850, loss is 3.822140612602234 and perplexity is 45.701933822799106
At time: 89.20907783508301 and batch: 900, loss is 3.786756377220154 and perplexity is 44.11308176913653
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377529614592252 and perplexity of 79.64104610982837
finished 6 epochs...
Completing Train Step...
At time: 91.08477568626404 and batch: 50, loss is 3.8891555643081666 and perplexity is 48.86960185900748
At time: 91.83136796951294 and batch: 100, loss is 3.7449244165420534 and perplexity is 42.30580947861958
At time: 92.57840394973755 and batch: 150, loss is 3.7574008655548097 and perplexity is 42.83694218887482
At time: 93.32602787017822 and batch: 200, loss is 3.6527947187423706 and perplexity is 38.58234231169714
At time: 94.07390904426575 and batch: 250, loss is 3.797465138435364 and perplexity is 44.58801667181243
At time: 94.8242998123169 and batch: 300, loss is 3.78099467754364 and perplexity is 43.85964625096054
At time: 95.57786226272583 and batch: 350, loss is 3.765747127532959 and perplexity is 43.195966702913836
At time: 96.33760929107666 and batch: 400, loss is 3.706330714225769 and perplexity is 40.70417692065069
At time: 97.09990167617798 and batch: 450, loss is 3.7324639558792114 and perplexity is 41.78193027017674
At time: 97.86225605010986 and batch: 500, loss is 3.6153466367721556 and perplexity is 37.164226180535444
At time: 98.61883115768433 and batch: 550, loss is 3.6942410707473754 and perplexity is 40.21504063290839
At time: 99.37116408348083 and batch: 600, loss is 3.7221795558929442 and perplexity is 41.354430245712805
At time: 100.15866208076477 and batch: 650, loss is 3.557830882072449 and perplexity is 35.08700668670723
At time: 100.92960381507874 and batch: 700, loss is 3.5669730615615847 and perplexity is 35.40924915441222
At time: 101.69110441207886 and batch: 750, loss is 3.6750184059143067 and perplexity is 39.449382952480356
At time: 102.45771527290344 and batch: 800, loss is 3.6316894578933714 and perplexity is 37.776584684461625
At time: 103.23327493667603 and batch: 850, loss is 3.705325446128845 and perplexity is 40.66327887037757
At time: 104.00670433044434 and batch: 900, loss is 3.670512089729309 and perplexity is 39.272011505659236
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391689352793236 and perplexity of 80.7767642327193
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 105.90576887130737 and batch: 50, loss is 3.800359525680542 and perplexity is 44.7172586064319
At time: 106.657794713974 and batch: 100, loss is 3.6592474603652954 and perplexity is 38.832109171702385
At time: 107.40998339653015 and batch: 150, loss is 3.671548914909363 and perplexity is 39.31275083218987
At time: 108.16029143333435 and batch: 200, loss is 3.5449422502517702 and perplexity is 34.63768496691472
At time: 108.93234014511108 and batch: 250, loss is 3.6859823751449583 and perplexity is 39.884284540586755
At time: 109.68922543525696 and batch: 300, loss is 3.6530467462539673 and perplexity is 38.59206734885851
At time: 110.44313716888428 and batch: 350, loss is 3.626154274940491 and perplexity is 37.56806201485839
At time: 111.1999568939209 and batch: 400, loss is 3.559470171928406 and perplexity is 35.14457163076802
At time: 111.96301770210266 and batch: 450, loss is 3.5616746044158933 and perplexity is 35.222130921931864
At time: 112.72437906265259 and batch: 500, loss is 3.4333398008346556 and perplexity is 30.9799369819711
At time: 113.47820591926575 and batch: 550, loss is 3.4891764879226685 and perplexity is 32.758959196536125
At time: 114.24804854393005 and batch: 600, loss is 3.5075212717056274 and perplexity is 33.36546128599338
At time: 115.01545524597168 and batch: 650, loss is 3.3196244192123414 and perplexity is 27.649963812920213
At time: 115.78409695625305 and batch: 700, loss is 3.3075638008117676 and perplexity is 27.318491051775673
At time: 116.56038808822632 and batch: 750, loss is 3.3975795888900757 and perplexity is 29.891662306663676
At time: 117.32017207145691 and batch: 800, loss is 3.3281030368804934 and perplexity is 27.885393935264418
At time: 118.08333730697632 and batch: 850, loss is 3.3799391317367555 and perplexity is 29.368983419869068
At time: 118.83656024932861 and batch: 900, loss is 3.3275581979751587 and perplexity is 27.870205025888016
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.360751217358733 and perplexity of 78.31594462776523
finished 8 epochs...
Completing Train Step...
At time: 120.71713662147522 and batch: 50, loss is 3.681916422843933 and perplexity is 39.72244617857324
At time: 121.48660349845886 and batch: 100, loss is 3.5367567682266237 and perplexity is 34.35531605864467
At time: 122.23451852798462 and batch: 150, loss is 3.5510986852645874 and perplexity is 34.8515873859741
At time: 122.98843169212341 and batch: 200, loss is 3.430849232673645 and perplexity is 30.902875340762694
At time: 123.74166536331177 and batch: 250, loss is 3.570921516418457 and perplexity is 35.54933736019108
At time: 124.49619889259338 and batch: 300, loss is 3.5456542444229124 and perplexity is 34.66235557833659
At time: 125.24594378471375 and batch: 350, loss is 3.523165202140808 and perplexity is 33.891532422570585
At time: 126.00406241416931 and batch: 400, loss is 3.460194730758667 and perplexity is 31.823172861933003
At time: 126.76827216148376 and batch: 450, loss is 3.4709457635879515 and perplexity is 32.16715058278104
At time: 127.54580569267273 and batch: 500, loss is 3.348251266479492 and perplexity is 28.452933514287206
At time: 128.3123757839203 and batch: 550, loss is 3.405972805023193 and perplexity is 30.14360531601571
At time: 129.07434749603271 and batch: 600, loss is 3.434061894416809 and perplexity is 31.002315474351775
At time: 129.83696246147156 and batch: 650, loss is 3.2515623426437377 and perplexity is 25.830664757448492
At time: 130.6063425540924 and batch: 700, loss is 3.246957054138184 and perplexity is 25.711980591124835
At time: 131.36893153190613 and batch: 750, loss is 3.345384874343872 and perplexity is 28.371493025375965
At time: 132.1368865966797 and batch: 800, loss is 3.2838335418701172 and perplexity is 26.677847566378937
At time: 132.9006745815277 and batch: 850, loss is 3.3448773002624512 and perplexity is 28.357096044941226
At time: 133.65049719810486 and batch: 900, loss is 3.3028511381149293 and perplexity is 27.190051102852276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3753858592412245 and perplexity of 79.47049806302843
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 135.51145100593567 and batch: 50, loss is 3.636793808937073 and perplexity is 37.96990259543189
At time: 136.2833490371704 and batch: 100, loss is 3.499837942123413 and perplexity is 33.11008577369368
At time: 137.03929138183594 and batch: 150, loss is 3.519416289329529 and perplexity is 33.764713886823685
At time: 137.79676914215088 and batch: 200, loss is 3.3981157207489012 and perplexity is 29.907692475897775
At time: 138.56480884552002 and batch: 250, loss is 3.5361996936798095 and perplexity is 34.33618291630874
At time: 139.34029078483582 and batch: 300, loss is 3.506199474334717 and perplexity is 33.32138804135266
At time: 140.0966157913208 and batch: 350, loss is 3.4773487424850464 and perplexity is 32.3737769754171
At time: 140.85335731506348 and batch: 400, loss is 3.417907991409302 and perplexity is 30.505530389112224
At time: 141.60934257507324 and batch: 450, loss is 3.4213496017456055 and perplexity is 30.610699409399682
At time: 142.3629710674286 and batch: 500, loss is 3.294317436218262 and perplexity is 26.959006546999632
At time: 143.11352849006653 and batch: 550, loss is 3.3426928186416625 and perplexity is 28.295218100035388
At time: 143.87004017829895 and batch: 600, loss is 3.368046736717224 and perplexity is 29.02178447322192
At time: 144.63101243972778 and batch: 650, loss is 3.1827989053726196 and perplexity is 24.114152417298936
At time: 145.38420009613037 and batch: 700, loss is 3.169387731552124 and perplexity is 23.792912246410747
At time: 146.14953446388245 and batch: 750, loss is 3.258792772293091 and perplexity is 26.018108394172653
At time: 146.90568566322327 and batch: 800, loss is 3.1905957412719728 and perplexity is 24.30290137268426
At time: 147.65486574172974 and batch: 850, loss is 3.245658030509949 and perplexity is 25.678601805417696
At time: 148.40423941612244 and batch: 900, loss is 3.204095802307129 and perplexity is 24.633216646029037
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367678289544092 and perplexity of 78.86032814460377
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 150.26963257789612 and batch: 50, loss is 3.615357689857483 and perplexity is 37.16463696216875
At time: 151.04997396469116 and batch: 100, loss is 3.4777073955535887 and perplexity is 32.385390012270804
At time: 151.81019711494446 and batch: 150, loss is 3.4992822122573854 and perplexity is 33.091690622003526
At time: 152.5705292224884 and batch: 200, loss is 3.3773948907852174 and perplexity is 29.294356624078098
At time: 153.3225769996643 and batch: 250, loss is 3.5165742588043214 and perplexity is 33.668889771311804
At time: 154.07913899421692 and batch: 300, loss is 3.4860822439193724 and perplexity is 32.65775164466791
At time: 154.8508894443512 and batch: 350, loss is 3.456570825576782 and perplexity is 31.708057410616156
At time: 155.61183619499207 and batch: 400, loss is 3.3964553260803223 and perplexity is 29.85807510636843
At time: 156.36991047859192 and batch: 450, loss is 3.4007029867172243 and perplexity is 29.985171817443348
At time: 157.14528393745422 and batch: 500, loss is 3.2762156438827517 and perplexity is 26.47539057254601
At time: 157.907306432724 and batch: 550, loss is 3.322230577468872 and perplexity is 27.72211797614853
At time: 158.69404220581055 and batch: 600, loss is 3.347255859375 and perplexity is 28.424625353533006
At time: 159.4495494365692 and batch: 650, loss is 3.161454610824585 and perplexity is 23.60490692156915
At time: 160.2055628299713 and batch: 700, loss is 3.1487141132354735 and perplexity is 23.306076329242064
At time: 160.96564078330994 and batch: 750, loss is 3.2343943023681643 and perplexity is 25.390987859100566
At time: 161.7295365333557 and batch: 800, loss is 3.165825786590576 and perplexity is 23.708313959180273
At time: 162.48939037322998 and batch: 850, loss is 3.2185476493835448 and perplexity is 24.991796958975705
At time: 163.25027132034302 and batch: 900, loss is 3.178282594680786 and perplexity is 24.005490972033506
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366936252541738 and perplexity of 78.80183256872759
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 165.16748356819153 and batch: 50, loss is 3.608082609176636 and perplexity is 36.89524235173932
At time: 165.91431212425232 and batch: 100, loss is 3.469734764099121 and perplexity is 32.12821975723082
At time: 166.6578369140625 and batch: 150, loss is 3.492355008125305 and perplexity is 32.86324986735779
At time: 167.40706872940063 and batch: 200, loss is 3.370290470123291 and perplexity is 29.086974727975182
At time: 168.15979170799255 and batch: 250, loss is 3.510257239341736 and perplexity is 33.45687310109747
At time: 168.9126455783844 and batch: 300, loss is 3.480379567146301 and perplexity is 32.472045058493585
At time: 169.66265988349915 and batch: 350, loss is 3.449665846824646 and perplexity is 31.489868111078028
At time: 170.4257836341858 and batch: 400, loss is 3.3889317464828492 and perplexity is 29.63427843337238
At time: 171.17995810508728 and batch: 450, loss is 3.395039005279541 and perplexity is 29.815816426511756
At time: 171.9385724067688 and batch: 500, loss is 3.270470576286316 and perplexity is 26.32372374929741
At time: 172.71079921722412 and batch: 550, loss is 3.316209969520569 and perplexity is 27.555715397304475
At time: 173.47331881523132 and batch: 600, loss is 3.340815224647522 and perplexity is 28.24214101267108
At time: 174.23279571533203 and batch: 650, loss is 3.1555248975753782 and perplexity is 23.465350765161592
At time: 174.98219656944275 and batch: 700, loss is 3.1433923292160033 and perplexity is 23.182375870186874
At time: 175.73352336883545 and batch: 750, loss is 3.2288993549346925 and perplexity is 25.2518483477844
At time: 176.48630023002625 and batch: 800, loss is 3.1601396703720095 and perplexity is 23.573888272886112
At time: 177.23924732208252 and batch: 850, loss is 3.211651840209961 and perplexity is 24.82005114001215
At time: 178.0146131515503 and batch: 900, loss is 3.1712188720703125 and perplexity is 23.836520326146612
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366660340191567 and perplexity of 78.78009316912812
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 179.88853406906128 and batch: 50, loss is 3.606308355331421 and perplexity is 36.82983886447838
At time: 180.63445830345154 and batch: 100, loss is 3.4676417398452757 and perplexity is 32.06104493783219
At time: 181.37732362747192 and batch: 150, loss is 3.4904363918304444 and perplexity is 32.80025834826079
At time: 182.1256709098816 and batch: 200, loss is 3.3686504936218262 and perplexity is 29.039311866591458
At time: 182.867427110672 and batch: 250, loss is 3.5084861946105956 and perplexity is 33.39767192171395
At time: 183.60887169837952 and batch: 300, loss is 3.4791963720321655 and perplexity is 32.43364701410326
At time: 184.35343289375305 and batch: 350, loss is 3.447995467185974 and perplexity is 31.437311983123866
At time: 185.1038715839386 and batch: 400, loss is 3.3872787666320803 and perplexity is 29.585334031429454
At time: 185.8557538986206 and batch: 450, loss is 3.393805079460144 and perplexity is 29.779048609840988
At time: 186.6094024181366 and batch: 500, loss is 3.2688677644729616 and perplexity is 26.281565568740852
At time: 187.3610134124756 and batch: 550, loss is 3.3145867204666137 and perplexity is 27.511021892513146
At time: 188.11374282836914 and batch: 600, loss is 3.3394627380371094 and perplexity is 28.203969714005872
At time: 188.86475920677185 and batch: 650, loss is 3.1540158796310425 and perplexity is 23.429967833240735
At time: 189.62388134002686 and batch: 700, loss is 3.141996884346008 and perplexity is 23.150048703339895
At time: 190.3757016658783 and batch: 750, loss is 3.2276355695724486 and perplexity is 25.21995558851883
At time: 191.1269657611847 and batch: 800, loss is 3.1589030981063844 and perplexity is 23.544755472573634
At time: 191.87722778320312 and batch: 850, loss is 3.2101823472976685 and perplexity is 24.783605035983445
At time: 192.63625931739807 and batch: 900, loss is 3.1695400142669676 and perplexity is 23.796535771574785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366447553242723 and perplexity of 78.76333157686028
Annealing...
Model not improving. Stopping early with 78.31594462776523 lossat 12 epochs.
Finished Training.
Improved accuracyfrom -78.33208706361773 to -78.31594462776523
langmodel
SETTINGS FOR THIS RUN
{'clip': 0.25372525675611113, 'rnn_dropout': 0.49930070350316336, 'anneal': 4.765827761833172, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 36.59443577805188, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.524166368804321, 'seq_len': 35}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.0743114709854126 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.100825309753418 and batch: 50, loss is 7.357531900405884 and perplexity is 1567.9618972719518
At time: 1.989227294921875 and batch: 100, loss is 6.848070888519287 and perplexity is 942.0618103620233
At time: 2.8681581020355225 and batch: 150, loss is 6.506577482223511 and perplexity is 669.5310100114252
At time: 3.751377820968628 and batch: 200, loss is 5.8946576595306395 and perplexity is 363.09251335835125
At time: 4.6373960971832275 and batch: 250, loss is 5.943904504776001 and perplexity is 381.42128714196224
At time: 5.521669149398804 and batch: 300, loss is 5.8781102180480955 and perplexity is 357.1336987771125
At time: 6.402559995651245 and batch: 350, loss is 5.908219337463379 and perplexity is 368.0501983637897
At time: 7.304204940795898 and batch: 400, loss is 5.795714616775513 and perplexity is 328.887128283505
At time: 8.18180227279663 and batch: 450, loss is 5.809072113037109 and perplexity is 333.3097084116055
At time: 9.059656620025635 and batch: 500, loss is 5.785665702819824 and perplexity is 325.5987199684487
At time: 9.939094305038452 and batch: 550, loss is 5.841990242004394 and perplexity is 344.4642260667755
At time: 10.817108869552612 and batch: 600, loss is 5.799607648849487 and perplexity is 330.1699919170418
At time: 11.695107698440552 and batch: 650, loss is 5.721022987365723 and perplexity is 305.21699643723855
At time: 12.578163623809814 and batch: 700, loss is 5.82630072593689 and perplexity is 339.1019250101402
At time: 13.459158897399902 and batch: 750, loss is 5.795808773040772 and perplexity is 328.9180965251001
At time: 14.339722394943237 and batch: 800, loss is 5.79032112121582 and perplexity is 327.11805205835367
At time: 15.217875719070435 and batch: 850, loss is 5.833561878204346 and perplexity is 341.5731568593711
At time: 16.10024857521057 and batch: 900, loss is 5.729636287689209 and perplexity is 307.8572765307979
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.7367269437607025 and perplexity of 310.0479440543576
finished 1 epochs...
Completing Train Step...
At time: 18.021487712860107 and batch: 50, loss is 5.787258720397949 and perplexity is 326.11781780898923
At time: 18.783360719680786 and batch: 100, loss is 5.660416641235352 and perplexity is 287.26830544773486
At time: 19.525800228118896 and batch: 150, loss is 5.650662298202515 and perplexity is 284.4798139151427
At time: 20.268489360809326 and batch: 200, loss is 5.5075788497924805 and perplexity is 246.55346087680456
At time: 21.00749659538269 and batch: 250, loss is 5.599751701354981 and perplexity is 270.35926925111517
At time: 21.744681119918823 and batch: 300, loss is 5.672726268768311 and perplexity is 290.82632531158816
At time: 22.481505155563354 and batch: 350, loss is 5.9043505764007564 and perplexity is 366.62905089981854
At time: 23.216992616653442 and batch: 400, loss is 5.709124994277954 and perplexity is 301.6070448985542
At time: 23.95421004295349 and batch: 450, loss is 5.6606150722503665 and perplexity is 287.32531404511633
At time: 24.691603660583496 and batch: 500, loss is 5.476620740890503 and perplexity is 239.037571114004
At time: 25.44721484184265 and batch: 550, loss is 5.432257270812988 and perplexity is 228.6648217354679
At time: 26.203816175460815 and batch: 600, loss is 5.343179702758789 and perplexity is 209.17677393224247
At time: 26.960444688796997 and batch: 650, loss is 5.258464403152466 and perplexity is 192.18614415111136
At time: 27.72763228416443 and batch: 700, loss is 5.349909200668335 and perplexity is 210.5891756421407
At time: 28.4837543964386 and batch: 750, loss is 5.335234670639038 and perplexity is 207.5214422837261
At time: 29.239020586013794 and batch: 800, loss is 5.292534961700439 and perplexity is 198.84685621564574
At time: 29.989113092422485 and batch: 850, loss is 5.313094129562378 and perplexity is 202.97729582542866
At time: 30.75068712234497 and batch: 900, loss is 5.234705095291138 and perplexity is 187.67375230923145
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.368659868632277 and perplexity of 214.57511609111313
finished 2 epochs...
Completing Train Step...
At time: 32.646358489990234 and batch: 50, loss is 5.311946935653687 and perplexity is 202.74457502152217
At time: 33.40511441230774 and batch: 100, loss is 5.209730939865112 and perplexity is 183.04480150852578
At time: 34.16419982910156 and batch: 150, loss is 5.235926666259766 and perplexity is 187.903149200391
At time: 34.92148756980896 and batch: 200, loss is 5.155273962020874 and perplexity is 173.34328973228244
At time: 35.683308839797974 and batch: 250, loss is 5.272894411087036 and perplexity is 194.9794973199629
At time: 36.43779516220093 and batch: 300, loss is 5.249075727462769 and perplexity is 190.39021466354822
At time: 37.18779373168945 and batch: 350, loss is 5.248928985595703 and perplexity is 190.3622784977302
At time: 37.93635106086731 and batch: 400, loss is 5.141067142486572 and perplexity is 170.8980436103812
At time: 38.68548846244812 and batch: 450, loss is 5.1544771480560305 and perplexity is 173.2052223926091
At time: 39.437944173812866 and batch: 500, loss is 5.106795692443848 and perplexity is 165.14034571344882
At time: 40.18885588645935 and batch: 550, loss is 5.16352632522583 and perplexity is 174.7797002571768
At time: 40.947813272476196 and batch: 600, loss is 5.128096418380737 and perplexity is 168.69568619857577
At time: 41.70340442657471 and batch: 650, loss is 5.033288412094116 and perplexity is 153.43674725362476
At time: 42.45511436462402 and batch: 700, loss is 5.106978120803833 and perplexity is 165.170474743996
At time: 43.20515513420105 and batch: 750, loss is 5.12011420249939 and perplexity is 167.35448082980426
At time: 43.957024812698364 and batch: 800, loss is 5.078666152954102 and perplexity is 160.5597508858973
At time: 44.7083215713501 and batch: 850, loss is 5.129480676651001 and perplexity is 168.929366296844
At time: 45.457275390625 and batch: 900, loss is 5.059920883178711 and perplexity is 157.57804875585373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.210242702536387 and perplexity of 183.13850097900945
finished 3 epochs...
Completing Train Step...
At time: 47.31006932258606 and batch: 50, loss is 5.139024572372437 and perplexity is 170.5493286325736
At time: 48.067445278167725 and batch: 100, loss is 5.033699827194214 and perplexity is 153.4998864356707
At time: 48.819820165634155 and batch: 150, loss is 5.062704467773438 and perplexity is 158.01729163607808
At time: 49.571576833724976 and batch: 200, loss is 4.992628574371338 and perplexity is 147.3231648792769
At time: 50.31363368034363 and batch: 250, loss is 5.126131868362426 and perplexity is 168.36460040894667
At time: 51.057847023010254 and batch: 300, loss is 5.086875467300415 and perplexity is 161.8832614766053
At time: 51.80235958099365 and batch: 350, loss is 5.102106132507324 and perplexity is 164.36772321001365
At time: 52.549519062042236 and batch: 400, loss is 5.005359888076782 and perplexity is 149.2107726747131
At time: 53.287938833236694 and batch: 450, loss is 5.016577119827271 and perplexity is 150.8939270088757
At time: 54.02779579162598 and batch: 500, loss is 4.944307823181152 and perplexity is 140.37365382740703
At time: 54.76704978942871 and batch: 550, loss is 5.02565975189209 and perplexity is 152.27068385211248
At time: 55.5149028301239 and batch: 600, loss is 5.0130962562561034 and perplexity is 150.3695989219232
At time: 56.2532594203949 and batch: 650, loss is 4.885662698745728 and perplexity is 132.37816311513873
At time: 56.99116039276123 and batch: 700, loss is 4.971606512069702 and perplexity is 144.25845426154103
At time: 57.72840070724487 and batch: 750, loss is 5.012087078094482 and perplexity is 150.21792575201732
At time: 58.46685528755188 and batch: 800, loss is 4.964379558563232 and perplexity is 143.2196632879471
At time: 59.20461988449097 and batch: 850, loss is 5.023121757507324 and perplexity is 151.88471171627197
At time: 59.942779779434204 and batch: 900, loss is 4.960849981307984 and perplexity is 142.71504948247642
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.115945476375214 and perplexity of 166.65827798130584
finished 4 epochs...
Completing Train Step...
At time: 61.82555937767029 and batch: 50, loss is 5.042985696792602 and perplexity is 154.93190484265713
At time: 62.56816911697388 and batch: 100, loss is 4.936376132965088 and perplexity is 139.26465741282226
At time: 63.31035375595093 and batch: 150, loss is 4.971828479766845 and perplexity is 144.2904785324723
At time: 64.05299758911133 and batch: 200, loss is 4.86649001121521 and perplexity is 129.86429377953877
At time: 64.79595947265625 and batch: 250, loss is 5.005046358108521 and perplexity is 149.16399795891286
At time: 65.53874850273132 and batch: 300, loss is 5.011367597579956 and perplexity is 150.10988575250406
At time: 66.3061294555664 and batch: 350, loss is 4.995836915969849 and perplexity is 147.79658696136752
At time: 67.04564070701599 and batch: 400, loss is 4.898465127944946 and perplexity is 134.08382015601427
At time: 67.78623008728027 and batch: 450, loss is 4.933559103012085 and perplexity is 138.87289676131365
At time: 68.52637696266174 and batch: 500, loss is 4.860936117172241 and perplexity is 129.14504042919535
At time: 69.26646757125854 and batch: 550, loss is 4.937237148284912 and perplexity is 139.38461805292815
At time: 70.00906133651733 and batch: 600, loss is 4.914145011901855 and perplexity is 136.2028082259437
At time: 70.75292181968689 and batch: 650, loss is 4.821675968170166 and perplexity is 124.17302652435527
At time: 71.49583768844604 and batch: 700, loss is 4.8733709526062015 and perplexity is 130.76096380284702
At time: 72.2373878955841 and batch: 750, loss is 4.92148684501648 and perplexity is 137.2064663506056
At time: 72.99055504798889 and batch: 800, loss is 4.8636534309387205 and perplexity is 129.49644524777665
At time: 73.7321629524231 and batch: 850, loss is 4.933637228012085 and perplexity is 138.8837466301906
At time: 74.47438859939575 and batch: 900, loss is 4.880982837677002 and perplexity is 131.76009905820885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.056608853274828 and perplexity of 157.05700887231396
finished 5 epochs...
Completing Train Step...
At time: 76.29204106330872 and batch: 50, loss is 4.965616760253906 and perplexity is 143.3969645536017
At time: 77.05960202217102 and batch: 100, loss is 4.857317762374878 and perplexity is 128.67859224902188
At time: 77.8051598072052 and batch: 150, loss is 4.88169340133667 and perplexity is 131.85375626715782
At time: 78.54890537261963 and batch: 200, loss is 4.796446743011475 and perplexity is 121.07942594388228
At time: 79.2911741733551 and batch: 250, loss is 4.949817523956299 and perplexity is 141.14920522283504
At time: 80.03264784812927 and batch: 300, loss is 4.924720678329468 and perplexity is 137.65088739673863
At time: 80.78027868270874 and batch: 350, loss is 4.926386632919312 and perplexity is 137.8803986489731
At time: 81.52512454986572 and batch: 400, loss is 4.841503868103027 and perplexity is 126.65968803867152
At time: 82.26334929466248 and batch: 450, loss is 4.848182287216186 and perplexity is 127.50840541080775
At time: 83.00033473968506 and batch: 500, loss is 4.7889541912078855 and perplexity is 120.1756221990039
At time: 83.73984026908875 and batch: 550, loss is 4.871598634719849 and perplexity is 130.5294190539215
At time: 84.47612929344177 and batch: 600, loss is 4.862043333053589 and perplexity is 129.2881110593766
At time: 85.23615908622742 and batch: 650, loss is 4.739990882873535 and perplexity is 114.433158373826
At time: 85.97472381591797 and batch: 700, loss is 4.786284122467041 and perplexity is 119.85517302779698
At time: 86.71294975280762 and batch: 750, loss is 4.859104585647583 and perplexity is 128.90872369323242
At time: 87.4506950378418 and batch: 800, loss is 4.818569583892822 and perplexity is 123.78789587867811
At time: 88.1877555847168 and batch: 850, loss is 4.869049711227417 and perplexity is 130.19713321673515
At time: 88.93120908737183 and batch: 900, loss is 4.817820978164673 and perplexity is 123.69526222811459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.023024049523759 and perplexity of 151.86987209234243
finished 6 epochs...
Completing Train Step...
At time: 90.75419402122498 and batch: 50, loss is 4.911107873916626 and perplexity is 135.78976904936857
At time: 91.50321912765503 and batch: 100, loss is 4.801722555160523 and perplexity is 121.71990629167297
At time: 92.23888063430786 and batch: 150, loss is 4.8264054012298585 and perplexity is 124.76168545538913
At time: 92.97684407234192 and batch: 200, loss is 4.7293240833282475 and perplexity is 113.21900986369776
At time: 93.72175455093384 and batch: 250, loss is 4.886669998168945 and perplexity is 132.511574743939
At time: 94.4696102142334 and batch: 300, loss is 4.876774940490723 and perplexity is 131.20683097209456
At time: 95.21620178222656 and batch: 350, loss is 4.863098058700562 and perplexity is 129.4245464842824
At time: 95.9732780456543 and batch: 400, loss is 4.795604114532471 and perplexity is 120.97744394386338
At time: 96.72029304504395 and batch: 450, loss is 4.794201860427856 and perplexity is 120.80792171092868
At time: 97.46595668792725 and batch: 500, loss is 4.730054597854615 and perplexity is 113.30174821216737
At time: 98.21218276023865 and batch: 550, loss is 4.818238019943237 and perplexity is 123.74685907854749
At time: 98.95866084098816 and batch: 600, loss is 4.7959683322906494 and perplexity is 121.02151410236681
At time: 99.7050096988678 and batch: 650, loss is 4.687206172943116 and perplexity is 108.54948727818324
At time: 100.4504542350769 and batch: 700, loss is 4.736828050613403 and perplexity is 114.07179725243851
At time: 101.1954882144928 and batch: 750, loss is 4.805855865478516 and perplexity is 122.22405361717641
At time: 101.94043111801147 and batch: 800, loss is 4.745223407745361 and perplexity is 115.03350200792799
At time: 102.69513273239136 and batch: 850, loss is 4.817495241165161 and perplexity is 123.65497666614777
At time: 103.45687794685364 and batch: 900, loss is 4.761661901473999 and perplexity is 116.94010743592325
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.997632170376712 and perplexity of 148.06215774766494
finished 7 epochs...
Completing Train Step...
At time: 105.32549142837524 and batch: 50, loss is 4.892287044525147 and perplexity is 133.25799277180917
At time: 106.08835768699646 and batch: 100, loss is 4.752611722946167 and perplexity is 115.88655320450685
At time: 106.84107208251953 and batch: 150, loss is 4.773514432907104 and perplexity is 118.33439030319661
At time: 107.60618853569031 and batch: 200, loss is 4.7002466201782225 and perplexity is 109.97429098724024
At time: 108.36503481864929 and batch: 250, loss is 4.836045017242432 and perplexity is 125.9701554317353
At time: 109.11795926094055 and batch: 300, loss is 4.815633964538574 and perplexity is 123.42503460759083
At time: 109.87020087242126 and batch: 350, loss is 4.8107373237609865 and perplexity is 122.82214382458247
At time: 110.62234163284302 and batch: 400, loss is 4.735012216567993 and perplexity is 113.86484974739224
At time: 111.38457202911377 and batch: 450, loss is 4.754789361953735 and perplexity is 116.1391872562656
At time: 112.13806653022766 and batch: 500, loss is 4.685001087188721 and perplexity is 108.31039006195422
At time: 112.8885235786438 and batch: 550, loss is 4.770536003112793 and perplexity is 117.9824639835066
At time: 113.6390221118927 and batch: 600, loss is 4.754016485214233 and perplexity is 116.0494606581573
At time: 114.38892412185669 and batch: 650, loss is 4.637700805664062 and perplexity is 103.30655247913205
At time: 115.14038014411926 and batch: 700, loss is 4.679876375198364 and perplexity is 107.75675034105257
At time: 115.89705348014832 and batch: 750, loss is 4.758426065444946 and perplexity is 116.562319981808
At time: 116.65709185600281 and batch: 800, loss is 4.724231157302857 and perplexity is 112.64385966368663
At time: 117.41393256187439 and batch: 850, loss is 4.763321294784546 and perplexity is 117.13431815936005
At time: 118.18170189857483 and batch: 900, loss is 4.71952709197998 and perplexity is 112.11521994231997
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.978110535504067 and perplexity of 145.19977248823974
finished 8 epochs...
Completing Train Step...
At time: 120.05436778068542 and batch: 50, loss is 4.832291612625122 and perplexity is 125.49822469624375
At time: 120.82218480110168 and batch: 100, loss is 4.715339622497559 and perplexity is 111.64672247430929
At time: 121.57126021385193 and batch: 150, loss is 4.746141595840454 and perplexity is 115.13917290546458
At time: 122.32334280014038 and batch: 200, loss is 4.653979034423828 and perplexity is 105.00196186628993
At time: 123.07663440704346 and batch: 250, loss is 4.809443712234497 and perplexity is 122.66336240650163
At time: 123.84931778907776 and batch: 300, loss is 4.791873931884766 and perplexity is 120.52701659212627
At time: 124.60277056694031 and batch: 350, loss is 4.767432155609131 and perplexity is 117.61683213348547
At time: 125.35674166679382 and batch: 400, loss is 4.699759254455566 and perplexity is 109.92070634615892
At time: 126.11655807495117 and batch: 450, loss is 4.717379808425903 and perplexity is 111.87473506139385
At time: 126.86823678016663 and batch: 500, loss is 4.64290678024292 and perplexity is 103.84576611358348
At time: 127.61140632629395 and batch: 550, loss is 4.745704183578491 and perplexity is 115.08882063255706
At time: 128.35857343673706 and batch: 600, loss is 4.729668350219726 and perplexity is 113.25799413039097
At time: 129.1029360294342 and batch: 650, loss is 4.612170429229736 and perplexity is 100.70248022171879
At time: 129.845210313797 and batch: 700, loss is 4.64712571144104 and perplexity is 104.28480975225983
At time: 130.6002230644226 and batch: 750, loss is 4.725870342254638 and perplexity is 112.82865519901061
At time: 131.3621735572815 and batch: 800, loss is 4.682497472763061 and perplexity is 108.03956177321612
At time: 132.12418603897095 and batch: 850, loss is 4.752127895355224 and perplexity is 115.83049765435169
At time: 132.8738579750061 and batch: 900, loss is 4.689406709671021 and perplexity is 108.78861742257065
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.9549798834813785 and perplexity of 141.8797522175732
finished 9 epochs...
Completing Train Step...
At time: 134.72710943222046 and batch: 50, loss is 4.780760688781738 and perplexity is 119.19498585620111
At time: 135.46682500839233 and batch: 100, loss is 4.674408769607544 and perplexity is 107.1691866773785
At time: 136.20579409599304 and batch: 150, loss is 4.688852777481079 and perplexity is 108.72837259279679
At time: 136.9464020729065 and batch: 200, loss is 4.61302321434021 and perplexity is 100.78839442540978
At time: 137.68551015853882 and batch: 250, loss is 4.763476867675781 and perplexity is 117.15254250146901
At time: 138.42358565330505 and batch: 300, loss is 4.755113277435303 and perplexity is 116.17681263042621
At time: 139.16364884376526 and batch: 350, loss is 4.726299896240234 and perplexity is 112.8771316084128
At time: 139.90413236618042 and batch: 400, loss is 4.659749813079834 and perplexity is 105.60965669639644
At time: 140.64672708511353 and batch: 450, loss is 4.685347585678101 and perplexity is 108.34792595118374
At time: 141.3897190093994 and batch: 500, loss is 4.5949952507019045 and perplexity is 98.98766542462064
At time: 142.1307601928711 and batch: 550, loss is 4.6839210891723635 and perplexity is 108.19347819919014
At time: 142.88548183441162 and batch: 600, loss is 4.701374483108521 and perplexity is 110.09839688739667
At time: 143.62740302085876 and batch: 650, loss is 4.556749467849731 and perplexity is 95.27328707472971
At time: 144.3691370487213 and batch: 700, loss is 4.631624813079834 and perplexity is 102.68076569551044
At time: 145.11428475379944 and batch: 750, loss is 4.699025354385376 and perplexity is 109.84006512697384
At time: 145.86199045181274 and batch: 800, loss is 4.6507430934906 and perplexity is 104.66273088154841
At time: 146.61029744148254 and batch: 850, loss is 4.713432140350342 and perplexity is 111.4339613279492
At time: 147.35788655281067 and batch: 900, loss is 4.651276016235352 and perplexity is 104.7185228964594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.915068743980094 and perplexity of 136.32868125657595
finished 10 epochs...
Completing Train Step...
At time: 149.18145275115967 and batch: 50, loss is 4.766679048538208 and perplexity is 117.52828741155923
At time: 149.91838002204895 and batch: 100, loss is 4.631087188720703 and perplexity is 102.62557685142109
At time: 150.656583070755 and batch: 150, loss is 4.653577079772949 and perplexity is 104.95976432068441
At time: 151.3946840763092 and batch: 200, loss is 4.57976372718811 and perplexity is 97.49135692858344
At time: 152.13412523269653 and batch: 250, loss is 4.736871671676636 and perplexity is 114.07677329404885
At time: 152.8764772415161 and batch: 300, loss is 4.719209280014038 and perplexity is 112.07959404532609
At time: 153.62217235565186 and batch: 350, loss is 4.711801691055298 and perplexity is 111.25242193985822
At time: 154.36477088928223 and batch: 400, loss is 4.642609100341797 and perplexity is 103.8148579167975
At time: 155.10650539398193 and batch: 450, loss is 4.647221899032592 and perplexity is 104.29484113938513
At time: 155.84781169891357 and batch: 500, loss is 4.56978856086731 and perplexity is 96.5236987279308
At time: 156.59155368804932 and batch: 550, loss is 4.660326290130615 and perplexity is 105.67055579160163
At time: 157.3376486301422 and batch: 600, loss is 4.662170257568359 and perplexity is 105.86558861743693
At time: 158.08283948898315 and batch: 650, loss is 4.5416225147247316 and perplexity is 93.84293821201139
At time: 158.8335428237915 and batch: 700, loss is 4.59151930809021 and perplexity is 98.64418728132654
At time: 159.58278965950012 and batch: 750, loss is 4.672786302566529 and perplexity is 106.9954491839727
At time: 160.33154129981995 and batch: 800, loss is 4.616212520599365 and perplexity is 101.11035262134128
At time: 161.0808970928192 and batch: 850, loss is 4.692104330062866 and perplexity is 109.08248400736692
At time: 161.84949254989624 and batch: 900, loss is 4.614548444747925 and perplexity is 100.94223724237935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.941154166443707 and perplexity of 139.9316608216866
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 163.67169070243835 and batch: 50, loss is 4.720945549011231 and perplexity is 112.27436340675706
At time: 164.41126012802124 and batch: 100, loss is 4.560987720489502 and perplexity is 95.67793623213491
At time: 165.15165424346924 and batch: 150, loss is 4.570714406967163 and perplexity is 96.61310620033464
At time: 165.89223098754883 and batch: 200, loss is 4.468355932235718 and perplexity is 87.21322063728007
At time: 166.63346552848816 and batch: 250, loss is 4.623567934036255 and perplexity is 101.85680292755225
At time: 167.37401413917542 and batch: 300, loss is 4.594092493057251 and perplexity is 98.8983438768534
At time: 168.11616373062134 and batch: 350, loss is 4.575499639511109 and perplexity is 97.07653029201128
At time: 168.86066818237305 and batch: 400, loss is 4.509393615722656 and perplexity is 90.86670166200132
At time: 169.60682320594788 and batch: 450, loss is 4.5159267234802245 and perplexity is 91.46228700878734
At time: 170.34894967079163 and batch: 500, loss is 4.42009934425354 and perplexity is 83.1045409068485
At time: 171.09223055839539 and batch: 550, loss is 4.503350706100464 and perplexity is 90.31925813796477
At time: 171.83555364608765 and batch: 600, loss is 4.496627197265625 and perplexity is 89.71403270714544
At time: 172.5802538394928 and batch: 650, loss is 4.354411153793335 and perplexity is 77.82098724921994
At time: 173.33019828796387 and batch: 700, loss is 4.39557110786438 and perplexity is 81.0909292081666
At time: 174.07975506782532 and batch: 750, loss is 4.48696873664856 and perplexity is 88.85170434130221
At time: 174.82858872413635 and batch: 800, loss is 4.426067352294922 and perplexity is 83.60199239601813
At time: 175.57623028755188 and batch: 850, loss is 4.477309789657593 and perplexity is 87.9976218479207
At time: 176.32346367835999 and batch: 900, loss is 4.420380973815918 and perplexity is 83.12794889837224
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.808865899909033 and perplexity of 122.59250647650646
finished 12 epochs...
Completing Train Step...
At time: 178.1373372077942 and batch: 50, loss is 4.626169719696045 and perplexity is 102.12215754499194
At time: 178.88681054115295 and batch: 100, loss is 4.479450759887695 and perplexity is 88.18622396032876
At time: 179.62751626968384 and batch: 150, loss is 4.50277009010315 and perplexity is 90.26683255286382
At time: 180.39337182044983 and batch: 200, loss is 4.400664672851563 and perplexity is 81.5050248421825
At time: 181.14485669136047 and batch: 250, loss is 4.563828763961792 and perplexity is 95.95014790781454
At time: 181.90383434295654 and batch: 300, loss is 4.539614191055298 and perplexity is 93.65466034272444
At time: 182.65749788284302 and batch: 350, loss is 4.533668966293335 and perplexity is 93.0995142064276
At time: 183.4188051223755 and batch: 400, loss is 4.457811269760132 and perplexity is 86.29841827968089
At time: 184.18358206748962 and batch: 450, loss is 4.4688954925537105 and perplexity is 87.26029012761295
At time: 184.94680404663086 and batch: 500, loss is 4.369819421768188 and perplexity is 79.02935942898563
At time: 185.7165195941925 and batch: 550, loss is 4.454256200790406 and perplexity is 85.99216614699287
At time: 186.48259449005127 and batch: 600, loss is 4.455356206893921 and perplexity is 86.0868100995284
At time: 187.25130820274353 and batch: 650, loss is 4.317178244590759 and perplexity is 74.97676342941541
At time: 188.01345205307007 and batch: 700, loss is 4.358189635276794 and perplexity is 78.11558863099863
At time: 188.7755115032196 and batch: 750, loss is 4.465584602355957 and perplexity is 86.97185863436013
At time: 189.53322911262512 and batch: 800, loss is 4.397237567901612 and perplexity is 81.22617666201295
At time: 190.29569673538208 and batch: 850, loss is 4.458204441070556 and perplexity is 86.33235501292123
At time: 191.051828622818 and batch: 900, loss is 4.405193595886231 and perplexity is 81.87499197058585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.791120555302868 and perplexity of 120.43624855588936
finished 13 epochs...
Completing Train Step...
At time: 192.93085861206055 and batch: 50, loss is 4.5752636814117436 and perplexity is 97.05362700064543
At time: 193.6939148902893 and batch: 100, loss is 4.432047786712647 and perplexity is 84.10346665105465
At time: 194.44335889816284 and batch: 150, loss is 4.463393573760986 and perplexity is 86.78150941157888
At time: 195.19422101974487 and batch: 200, loss is 4.357181730270386 and perplexity is 78.03689520255107
At time: 195.941885471344 and batch: 250, loss is 4.531640872955323 and perplexity is 92.91089103924091
At time: 196.68790435791016 and batch: 300, loss is 4.502692995071411 and perplexity is 90.25987369679324
At time: 197.42749071121216 and batch: 350, loss is 4.492673807144165 and perplexity is 89.36005829698351
At time: 198.19048810005188 and batch: 400, loss is 4.4197594356536865 and perplexity is 83.07629775902373
At time: 198.95040583610535 and batch: 450, loss is 4.4404885196685795 and perplexity is 84.81636601771788
At time: 199.712664604187 and batch: 500, loss is 4.34133978843689 and perplexity is 76.81038008956445
At time: 200.481999874115 and batch: 550, loss is 4.423990097045898 and perplexity is 83.42850996451163
At time: 201.22878623008728 and batch: 600, loss is 4.427859506607056 and perplexity is 83.75195440458624
At time: 201.97076439857483 and batch: 650, loss is 4.285672721862793 and perplexity is 72.65140447222375
At time: 202.7123420238495 and batch: 700, loss is 4.322105174064636 and perplexity is 75.3470801681307
At time: 203.46518206596375 and batch: 750, loss is 4.427099771499634 and perplexity is 83.68834926909652
At time: 204.2185275554657 and batch: 800, loss is 4.359431381225586 and perplexity is 78.21264859615627
At time: 204.9683108329773 and batch: 850, loss is 4.433385238647461 and perplexity is 84.21602624999971
At time: 205.7272434234619 and batch: 900, loss is 4.382785949707031 and perplexity is 80.06076827086308
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.788597211445848 and perplexity of 120.13272959034067
finished 14 epochs...
Completing Train Step...
At time: 207.6916847229004 and batch: 50, loss is 4.5472814559936525 and perplexity is 94.3754953213586
At time: 208.47734785079956 and batch: 100, loss is 4.403943324089051 and perplexity is 81.77268994322685
At time: 209.2434720993042 and batch: 150, loss is 4.421519584655762 and perplexity is 83.2226531875155
At time: 210.0046706199646 and batch: 200, loss is 4.326720662117005 and perplexity is 75.69564750182352
At time: 210.76519918441772 and batch: 250, loss is 4.502718591690064 and perplexity is 90.26218407392872
At time: 211.5297977924347 and batch: 300, loss is 4.475205583572388 and perplexity is 87.8126513927726
At time: 212.29750633239746 and batch: 350, loss is 4.455661487579346 and perplexity is 86.11309475181585
At time: 213.0510458946228 and batch: 400, loss is 4.387448501586914 and perplexity is 80.43492734668817
At time: 213.79258942604065 and batch: 450, loss is 4.406021890640258 and perplexity is 81.94283669075519
At time: 214.5388925075531 and batch: 500, loss is 4.304257507324219 and perplexity is 74.01424001482967
At time: 215.28006839752197 and batch: 550, loss is 4.385735135078431 and perplexity is 80.29723083204375
At time: 216.02089047431946 and batch: 600, loss is 4.398125410079956 and perplexity is 81.2983247109403
At time: 216.76663732528687 and batch: 650, loss is 4.251125354766845 and perplexity is 70.18435021487625
At time: 217.51914930343628 and batch: 700, loss is 4.292986822128296 and perplexity is 73.18473215746988
At time: 218.26297569274902 and batch: 750, loss is 4.399724979400634 and perplexity is 81.42847107828905
At time: 219.00288724899292 and batch: 800, loss is 4.337762689590454 and perplexity is 76.5361126009625
At time: 219.75829362869263 and batch: 850, loss is 4.410808038711548 and perplexity is 82.33596728068578
At time: 220.49897503852844 and batch: 900, loss is 4.352568969726563 and perplexity is 77.6777586336912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.780206549657534 and perplexity of 119.1289535483616
finished 15 epochs...
Completing Train Step...
At time: 222.31086826324463 and batch: 50, loss is 4.515810823440551 and perplexity is 91.45168714036888
At time: 223.07159805297852 and batch: 100, loss is 4.365772972106933 and perplexity is 78.71021723616693
At time: 223.80880546569824 and batch: 150, loss is 4.394048871994019 and perplexity is 80.96758359134427
At time: 224.55076575279236 and batch: 200, loss is 4.286436138153076 and perplexity is 72.70688891407848
At time: 225.29038310050964 and batch: 250, loss is 4.462135496139527 and perplexity is 86.67240018496099
At time: 226.02985310554504 and batch: 300, loss is 4.444031534194946 and perplexity is 85.1174046117379
At time: 226.77031826972961 and batch: 350, loss is 4.424406404495239 and perplexity is 83.4632491052771
At time: 227.50993824005127 and batch: 400, loss is 4.364586615562439 and perplexity is 78.61689422295954
At time: 228.25714349746704 and batch: 450, loss is 4.381688575744629 and perplexity is 79.97295985649832
At time: 229.00618052482605 and batch: 500, loss is 4.276010665893555 and perplexity is 71.95282283807803
At time: 229.7567582130432 and batch: 550, loss is 4.371122078895569 and perplexity is 79.13237466951945
At time: 230.50683903694153 and batch: 600, loss is 4.376550703048706 and perplexity is 79.56312271672854
At time: 231.25642561912537 and batch: 650, loss is 4.229897723197937 and perplexity is 68.7102043544907
At time: 232.00489115715027 and batch: 700, loss is 4.276514081954956 and perplexity is 71.98905416370835
At time: 232.75249886512756 and batch: 750, loss is 4.3812328672409055 and perplexity is 79.93652380136439
At time: 233.50049996376038 and batch: 800, loss is 4.306493854522705 and perplexity is 74.17994677299536
At time: 234.2482078075409 and batch: 850, loss is 4.374528045654297 and perplexity is 79.40235642062189
At time: 234.99538135528564 and batch: 900, loss is 4.323044233322143 and perplexity is 75.41786877343431
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.774915303269478 and perplexity of 118.5002776095988
finished 16 epochs...
Completing Train Step...
At time: 236.82181072235107 and batch: 50, loss is 4.48547396659851 and perplexity is 88.7189906876649
At time: 237.55989527702332 and batch: 100, loss is 4.338234052658081 and perplexity is 76.57219740163544
At time: 238.3112087249756 and batch: 150, loss is 4.356705141067505 and perplexity is 77.999712522007
At time: 239.05704951286316 and batch: 200, loss is 4.247009701728821 and perplexity is 69.89608937838085
At time: 239.79644775390625 and batch: 250, loss is 4.426462678909302 and perplexity is 83.63504902227903
At time: 240.5423059463501 and batch: 300, loss is 4.412606945037842 and perplexity is 82.48421527525787
At time: 241.29340600967407 and batch: 350, loss is 4.395251750946045 and perplexity is 81.06503639365454
At time: 242.03341674804688 and batch: 400, loss is 4.33096688747406 and perplexity is 76.01775165911171
At time: 242.77265119552612 and batch: 450, loss is 4.3515957164764405 and perplexity is 77.60219527974326
At time: 243.51336288452148 and batch: 500, loss is 4.251748309135437 and perplexity is 70.22808548357338
At time: 244.25557351112366 and batch: 550, loss is 4.340401973724365 and perplexity is 76.73837995171314
At time: 245.01097893714905 and batch: 600, loss is 4.345603694915772 and perplexity is 77.13859160120434
At time: 245.7659249305725 and batch: 650, loss is 4.202368612289429 and perplexity is 66.84447231791636
At time: 246.51448726654053 and batch: 700, loss is 4.239982585906983 and perplexity is 69.40664317449108
At time: 247.26668190956116 and batch: 750, loss is 4.36486918926239 and perplexity is 78.63911242863124
At time: 248.01725935935974 and batch: 800, loss is 4.2959159469604495 and perplexity is 73.39941363469664
At time: 248.76799607276917 and batch: 850, loss is 4.361539950370789 and perplexity is 78.37773936523462
At time: 249.51887559890747 and batch: 900, loss is 4.306215906143189 and perplexity is 74.15933144212886
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.773042443680437 and perplexity of 118.27855092465838
finished 17 epochs...
Completing Train Step...
At time: 251.35850954055786 and batch: 50, loss is 4.461520185470581 and perplexity is 86.61908613645758
At time: 252.10357904434204 and batch: 100, loss is 4.310557208061218 and perplexity is 74.48197933918289
At time: 252.84224462509155 and batch: 150, loss is 4.331007118225098 and perplexity is 76.02080997187188
At time: 253.5805013179779 and batch: 200, loss is 4.229850063323974 and perplexity is 68.70692971284632
At time: 254.31867146492004 and batch: 250, loss is 4.407777967453003 and perplexity is 82.08686102812611
At time: 255.05647158622742 and batch: 300, loss is 4.39389811038971 and perplexity is 80.95537770865735
At time: 255.80667066574097 and batch: 350, loss is 4.378414697647095 and perplexity is 79.71156625369872
At time: 256.5625035762787 and batch: 400, loss is 4.307821760177612 and perplexity is 74.27851617474471
At time: 257.3305170536041 and batch: 450, loss is 4.330876512527466 and perplexity is 76.01088186929846
At time: 258.08708930015564 and batch: 500, loss is 4.218254189491272 and perplexity is 67.914814335369
At time: 258.85111713409424 and batch: 550, loss is 4.310000805854798 and perplexity is 74.44054892859477
At time: 259.61641359329224 and batch: 600, loss is 4.317648849487305 and perplexity is 75.0120561652282
At time: 260.373991727829 and batch: 650, loss is 4.173874049186707 and perplexity is 64.96664921280889
At time: 261.13227248191833 and batch: 700, loss is 4.1998638248443605 and perplexity is 66.6772506376933
At time: 261.8925004005432 and batch: 750, loss is 4.3301307964324955 and perplexity is 75.95422046058361
At time: 262.6670928001404 and batch: 800, loss is 4.271922397613525 and perplexity is 71.6592608837049
At time: 263.42626881599426 and batch: 850, loss is 4.335889573097229 and perplexity is 76.3928857283078
At time: 264.1853530406952 and batch: 900, loss is 4.2894685125350955 and perplexity is 72.92769804018616
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.772032437259203 and perplexity of 118.15914913716661
finished 18 epochs...
Completing Train Step...
At time: 266.1003260612488 and batch: 50, loss is 4.430690031051636 and perplexity is 83.98935218041365
At time: 266.86054968833923 and batch: 100, loss is 4.282406558990479 and perplexity is 72.41450024681205
At time: 267.6218147277832 and batch: 150, loss is 4.301614851951599 and perplexity is 73.81890410223295
At time: 268.38388323783875 and batch: 200, loss is 4.197368245124817 and perplexity is 66.5110597008647
At time: 269.15309262275696 and batch: 250, loss is 4.37044207572937 and perplexity is 79.07858269562442
At time: 269.9101815223694 and batch: 300, loss is 4.356102442741394 and perplexity is 77.95271638950062
At time: 270.6647264957428 and batch: 350, loss is 4.344744443893433 and perplexity is 77.07233865554113
At time: 271.41551327705383 and batch: 400, loss is 4.277748913764953 and perplexity is 72.07800344515984
At time: 272.1658308506012 and batch: 450, loss is 4.2910513591766355 and perplexity is 73.04322280697919
At time: 272.9167511463165 and batch: 500, loss is 4.204737248420716 and perplexity is 67.00299021171786
At time: 273.6640636920929 and batch: 550, loss is 4.290705828666687 and perplexity is 73.01798850481615
At time: 274.40926933288574 and batch: 600, loss is 4.2943430995941165 and perplexity is 73.28405830221733
At time: 275.1640114784241 and batch: 650, loss is 4.1606905555725096 and perplexity is 64.11578282722164
At time: 275.91749024391174 and batch: 700, loss is 4.185876240730286 and perplexity is 65.7510894649871
At time: 276.6917176246643 and batch: 750, loss is 4.302416849136352 and perplexity is 73.87813040199283
At time: 277.4497182369232 and batch: 800, loss is 4.24554012298584 and perplexity is 69.79344701020682
At time: 278.1985909938812 and batch: 850, loss is 4.316060137748718 and perplexity is 74.89297824635288
At time: 278.94296622276306 and batch: 900, loss is 4.266167087554932 and perplexity is 71.24802415133658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.77232423547196 and perplexity of 118.19363279659615
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 280.75207448005676 and batch: 50, loss is 4.413447971343994 and perplexity is 82.55361584991041
At time: 281.51225066185 and batch: 100, loss is 4.247702379226684 and perplexity is 69.94452159869998
At time: 282.2737627029419 and batch: 150, loss is 4.2631159687042235 and perplexity is 71.03096926029379
At time: 283.03606605529785 and batch: 200, loss is 4.1487776565551755 and perplexity is 63.35650953349241
At time: 283.791868686676 and batch: 250, loss is 4.324153747558594 and perplexity is 75.50159241021585
At time: 284.5462498664856 and batch: 300, loss is 4.295799932479858 and perplexity is 73.39089873378357
At time: 285.3040072917938 and batch: 350, loss is 4.265367484092712 and perplexity is 71.19107675525059
At time: 286.06200647354126 and batch: 400, loss is 4.213437881469726 and perplexity is 67.58850211117887
At time: 286.82029700279236 and batch: 450, loss is 4.207418699264526 and perplexity is 67.18289653354157
At time: 287.5609362125397 and batch: 500, loss is 4.112175860404968 and perplexity is 61.07947349600032
At time: 288.29877614974976 and batch: 550, loss is 4.197191572189331 and perplexity is 66.49931003465959
At time: 289.0376486778259 and batch: 600, loss is 4.192709856033325 and perplexity is 66.20194585117913
At time: 289.77540016174316 and batch: 650, loss is 4.044387102127075 and perplexity is 57.07619343424479
At time: 290.51327633857727 and batch: 700, loss is 4.067256116867066 and perplexity is 58.39650936304435
At time: 291.2552742958069 and batch: 750, loss is 4.171859736442566 and perplexity is 64.83591777455955
At time: 291.99792551994324 and batch: 800, loss is 4.101715641021729 and perplexity is 60.44389872436251
At time: 292.74545431137085 and batch: 850, loss is 4.147323160171509 and perplexity is 63.26442470425401
At time: 293.4927291870117 and batch: 900, loss is 4.094550943374633 and perplexity is 60.01238414703528
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.72346433874679 and perplexity of 112.55751537130327
finished 20 epochs...
Completing Train Step...
At time: 295.3144311904907 and batch: 50, loss is 4.353711090087891 and perplexity is 77.76652666568806
At time: 296.0649857521057 and batch: 100, loss is 4.197837896347046 and perplexity is 66.54230403774115
At time: 296.8035228252411 and batch: 150, loss is 4.212154788970947 and perplexity is 67.50183542369473
At time: 297.54203724861145 and batch: 200, loss is 4.105181322097779 and perplexity is 60.653741414219766
At time: 298.27935767173767 and batch: 250, loss is 4.280360312461853 and perplexity is 72.26647382798308
At time: 299.0173089504242 and batch: 300, loss is 4.260846090316773 and perplexity is 70.86992044808751
At time: 299.75616455078125 and batch: 350, loss is 4.231549806594849 and perplexity is 68.8238131620754
At time: 300.49678587913513 and batch: 400, loss is 4.179251613616944 and perplexity is 65.31695260010684
At time: 301.24705815315247 and batch: 450, loss is 4.175248322486877 and perplexity is 65.05599252122389
At time: 301.9934630393982 and batch: 500, loss is 4.0839442539215085 and perplexity is 59.37921527389568
At time: 302.74749732017517 and batch: 550, loss is 4.171490831375122 and perplexity is 64.81200388718845
At time: 303.4978232383728 and batch: 600, loss is 4.168720569610596 and perplexity is 64.63270613645523
At time: 304.2508807182312 and batch: 650, loss is 4.026297378540039 and perplexity is 56.05298356608126
At time: 305.0013554096222 and batch: 700, loss is 4.050253810882569 and perplexity is 57.41202699360359
At time: 305.7569489479065 and batch: 750, loss is 4.155492291450501 and perplexity is 63.78335682135238
At time: 306.5070745944977 and batch: 800, loss is 4.090570287704468 and perplexity is 59.773970346241995
At time: 307.2546663284302 and batch: 850, loss is 4.146236777305603 and perplexity is 63.19573263693242
At time: 308.00443410873413 and batch: 900, loss is 4.095202469825745 and perplexity is 60.051496542697954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.725663694616866 and perplexity of 112.8053418328497
Annealing...
finished 21 epochs...
Completing Train Step...
At time: 309.86943101882935 and batch: 50, loss is 4.3328576755523684 and perplexity is 76.16162108812716
At time: 310.6232645511627 and batch: 100, loss is 4.1738004398345945 and perplexity is 64.9618672358527
At time: 311.36563539505005 and batch: 150, loss is 4.187429022789002 and perplexity is 65.85326588537548
At time: 312.10643100738525 and batch: 200, loss is 4.080581555366516 and perplexity is 59.17987621891314
At time: 312.8472149372101 and batch: 250, loss is 4.253479475975037 and perplexity is 70.34976731175465
At time: 313.58693838119507 and batch: 300, loss is 4.2271719169616695 and perplexity is 68.52316867827885
At time: 314.32702231407166 and batch: 350, loss is 4.1938141822814945 and perplexity is 66.27509478036978
At time: 315.079656124115 and batch: 400, loss is 4.138044662475586 and perplexity is 62.680140715991136
At time: 315.82157850265503 and batch: 450, loss is 4.132344279289246 and perplexity is 62.32385633924076
At time: 316.5635025501251 and batch: 500, loss is 4.038437514305115 and perplexity is 56.73762178845281
At time: 317.30562376976013 and batch: 550, loss is 4.12165445804596 and perplexity is 61.66117374552476
At time: 318.04570150375366 and batch: 600, loss is 4.121186079978943 and perplexity is 61.632299766652935
At time: 318.7864513397217 and batch: 650, loss is 3.974241075515747 and perplexity is 53.2097194194862
At time: 319.5303294658661 and batch: 700, loss is 3.992948069572449 and perplexity is 54.21448206751178
At time: 320.27848529815674 and batch: 750, loss is 4.0962381839752195 and perplexity is 60.11372494722116
At time: 321.0272295475006 and batch: 800, loss is 4.032043380737305 and perplexity is 56.37599124551728
At time: 321.77664136886597 and batch: 850, loss is 4.075933585166931 and perplexity is 58.90544817856248
At time: 322.52576875686646 and batch: 900, loss is 4.024386663436889 and perplexity is 55.94598453868674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.714646430864726 and perplexity of 111.5693567182023
finished 22 epochs...
Completing Train Step...
At time: 324.33937788009644 and batch: 50, loss is 4.311046276092529 and perplexity is 74.51841500321946
At time: 325.09252667427063 and batch: 100, loss is 4.154658703804016 and perplexity is 63.73020995741691
At time: 325.83078122138977 and batch: 150, loss is 4.168016004562378 and perplexity is 64.58718422919527
At time: 326.56888246536255 and batch: 200, loss is 4.062757539749145 and perplexity is 58.13439816800234
At time: 327.30663299560547 and batch: 250, loss is 4.237462258338928 and perplexity is 69.23193595048784
At time: 328.045095205307 and batch: 300, loss is 4.211615428924561 and perplexity is 67.46543744729894
At time: 328.7831139564514 and batch: 350, loss is 4.178338823318481 and perplexity is 65.25735912176047
At time: 329.52125239372253 and batch: 400, loss is 4.125847358703613 and perplexity is 61.92025569434691
At time: 330.2594130039215 and batch: 450, loss is 4.120649933815002 and perplexity is 61.59926470216108
At time: 330.99750685691833 and batch: 500, loss is 4.027558507919312 and perplexity is 56.1237182238549
At time: 331.74359822273254 and batch: 550, loss is 4.111104121208191 and perplexity is 61.01404729631001
At time: 332.4897119998932 and batch: 600, loss is 4.1121268606185915 and perplexity is 61.076480688171074
At time: 333.23586535453796 and batch: 650, loss is 3.968045253753662 and perplexity is 52.881060688794754
At time: 333.9939901828766 and batch: 700, loss is 3.987070822715759 and perplexity is 53.89678468066975
At time: 334.7405023574829 and batch: 750, loss is 4.090814375877381 and perplexity is 59.788562246234214
At time: 335.486305475235 and batch: 800, loss is 4.028581557273864 and perplexity is 56.18116493796034
At time: 336.23508310317993 and batch: 850, loss is 4.076055107116699 and perplexity is 58.9126069184403
At time: 336.9938311576843 and batch: 900, loss is 4.027503352165223 and perplexity is 56.120622763220986
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.714561148865582 and perplexity of 111.55984226612964
finished 23 epochs...
Completing Train Step...
At time: 338.87893080711365 and batch: 50, loss is 4.2987961578369145 and perplexity is 73.6111241633013
At time: 339.6411738395691 and batch: 100, loss is 4.14303071975708 and perplexity is 62.99344792276515
At time: 340.39749121665955 and batch: 150, loss is 4.156479554176331 and perplexity is 63.84635884672878
At time: 341.15276050567627 and batch: 200, loss is 4.052859787940979 and perplexity is 57.56183653415749
At time: 341.90845251083374 and batch: 250, loss is 4.226467585563659 and perplexity is 68.47492265167777
At time: 342.6698806285858 and batch: 300, loss is 4.2005904150009155 and perplexity is 66.72571527650538
At time: 343.42229676246643 and batch: 350, loss is 4.168186130523682 and perplexity is 64.59817312072157
At time: 344.1721420288086 and batch: 400, loss is 4.117642297744751 and perplexity is 61.414274862249
At time: 344.9207670688629 and batch: 450, loss is 4.112088894844055 and perplexity is 61.07416191629304
At time: 345.6706335544586 and batch: 500, loss is 4.01879967212677 and perplexity is 55.63428634756638
At time: 346.4199411869049 and batch: 550, loss is 4.102073111534119 and perplexity is 60.46550949818743
At time: 347.17174768447876 and batch: 600, loss is 4.105065031051636 and perplexity is 60.64668833729102
At time: 347.92424988746643 and batch: 650, loss is 3.9625511980056762 and perplexity is 52.59132583188094
At time: 348.6770601272583 and batch: 700, loss is 3.9816880416870117 and perplexity is 53.607449503139875
At time: 349.4421603679657 and batch: 750, loss is 4.085343012809753 and perplexity is 59.46233059463241
At time: 350.20601773262024 and batch: 800, loss is 4.0244732856750485 and perplexity is 55.950830914982454
At time: 350.9621500968933 and batch: 850, loss is 4.073495712280273 and perplexity is 58.76201908555598
At time: 351.7236351966858 and batch: 900, loss is 4.027003479003906 and perplexity is 56.09257658045632
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.715054864752783 and perplexity of 111.61493473152365
Annealing...
finished 24 epochs...
Completing Train Step...
At time: 353.599312543869 and batch: 50, loss is 4.289456634521485 and perplexity is 72.92683180914078
At time: 354.34839510917664 and batch: 100, loss is 4.133332080841065 and perplexity is 62.385450357573085
At time: 355.1040692329407 and batch: 150, loss is 4.1495918893814085 and perplexity is 63.4081174909054
At time: 355.8610460758209 and batch: 200, loss is 4.045319218635559 and perplexity is 57.12941989916579
At time: 356.6151683330536 and batch: 250, loss is 4.217514300346375 and perplexity is 67.864583486386
At time: 357.3691430091858 and batch: 300, loss is 4.1912312507629395 and perplexity is 66.10413163726064
At time: 358.12784123420715 and batch: 350, loss is 4.15755298614502 and perplexity is 63.91493036624553
At time: 358.88827443122864 and batch: 400, loss is 4.1058667325973515 and perplexity is 60.69532837586544
At time: 359.63911294937134 and batch: 450, loss is 4.0995423412323 and perplexity is 60.3126786539169
At time: 360.39089798927307 and batch: 500, loss is 4.002981791496277 and perplexity is 54.76119329232483
At time: 361.15542125701904 and batch: 550, loss is 4.083757343292237 and perplexity is 59.368117704562316
At time: 361.90165758132935 and batch: 600, loss is 4.090219855308533 and perplexity is 59.7530272803777
At time: 362.64946842193604 and batch: 650, loss is 3.9460376405715945 and perplexity is 51.72998740665224
At time: 363.4114725589752 and batch: 700, loss is 3.962466516494751 and perplexity is 52.586872507507714
At time: 364.17170548439026 and batch: 750, loss is 4.065576786994934 and perplexity is 58.298524657791674
At time: 364.9410192966461 and batch: 800, loss is 4.00400511264801 and perplexity is 54.81726026208037
At time: 365.7125458717346 and batch: 850, loss is 4.050233607292175 and perplexity is 57.41086707624383
At time: 366.47380542755127 and batch: 900, loss is 4.004918584823608 and perplexity is 54.867357181659806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.713142290507277 and perplexity of 111.40166689224087
finished 25 epochs...
Completing Train Step...
At time: 368.46697068214417 and batch: 50, loss is 4.283708181381225 and perplexity is 72.50881795144988
At time: 369.20746970176697 and batch: 100, loss is 4.1279483413696285 and perplexity is 62.05048583596669
At time: 369.949022769928 and batch: 150, loss is 4.143539457321167 and perplexity is 63.025503209187626
At time: 370.69849610328674 and batch: 200, loss is 4.040227899551391 and perplexity is 56.83929497934285
At time: 371.4394483566284 and batch: 250, loss is 4.212497582435608 and perplexity is 67.52497857816502
At time: 372.1792469024658 and batch: 300, loss is 4.18619815826416 and perplexity is 65.77225930084506
At time: 372.93285179138184 and batch: 350, loss is 4.153062248229981 and perplexity is 63.62854867894504
At time: 373.6885814666748 and batch: 400, loss is 4.102054662704468 and perplexity is 60.464393990592896
At time: 374.43718457221985 and batch: 450, loss is 4.096005134582519 and perplexity is 60.099717112451344
At time: 375.18258786201477 and batch: 500, loss is 4.000160775184631 and perplexity is 54.606928766478674
At time: 375.93568325042725 and batch: 550, loss is 4.081135606765747 and perplexity is 59.212673997126984
At time: 376.6765558719635 and batch: 600, loss is 4.088375215530395 and perplexity is 59.642906067602695
At time: 377.41779804229736 and batch: 650, loss is 3.944760212898254 and perplexity is 51.66394827828283
At time: 378.15861916542053 and batch: 700, loss is 3.96182475566864 and perplexity is 52.553135139583176
At time: 378.8999948501587 and batch: 750, loss is 4.065247740745544 and perplexity is 58.27934490259437
At time: 379.64193749427795 and batch: 800, loss is 4.0039679193496704 and perplexity is 54.81522146528029
At time: 380.3829393386841 and batch: 850, loss is 4.051093702316284 and perplexity is 57.46026711866586
At time: 381.1250822544098 and batch: 900, loss is 4.006469559669495 and perplexity is 54.95252109898363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.712965037724743 and perplexity of 111.38192238674122
finished 26 epochs...
Completing Train Step...
At time: 382.93646001815796 and batch: 50, loss is 4.279851179122925 and perplexity is 72.22968992162164
At time: 383.6877257823944 and batch: 100, loss is 4.124523019790649 and perplexity is 61.838306566417096
At time: 384.4255156517029 and batch: 150, loss is 4.139802079200745 and perplexity is 62.79039269458008
At time: 385.16387248039246 and batch: 200, loss is 4.036986737251282 and perplexity is 56.65536782917845
At time: 385.90069794654846 and batch: 250, loss is 4.209108138084412 and perplexity is 67.29649385782331
At time: 386.63674569129944 and batch: 300, loss is 4.183123092651368 and perplexity is 65.57031594166496
At time: 387.3736107349396 and batch: 350, loss is 4.150479121208191 and perplexity is 63.464400155013074
At time: 388.1130964756012 and batch: 400, loss is 4.099298553466797 and perplexity is 60.2979769528766
At time: 388.8600363731384 and batch: 450, loss is 4.093562536239624 and perplexity is 59.95309678320884
At time: 389.6070477962494 and batch: 500, loss is 3.9981604957580568 and perplexity is 54.497808821892235
At time: 390.35652709007263 and batch: 550, loss is 4.079148631095887 and perplexity is 59.09513666512732
At time: 391.1051700115204 and batch: 600, loss is 4.086851572990417 and perplexity is 59.55210079366621
At time: 391.8663115501404 and batch: 650, loss is 3.9436519002914427 and perplexity is 51.60672019225101
At time: 392.61381816864014 and batch: 700, loss is 3.9610871028900148 and perplexity is 52.514383467818334
At time: 393.3611559867859 and batch: 750, loss is 4.064707865715027 and perplexity is 58.24788983115439
At time: 394.10891914367676 and batch: 800, loss is 4.003496837615967 and perplexity is 54.789405097006274
At time: 394.8558783531189 and batch: 850, loss is 4.051225724220276 and perplexity is 57.46785363331681
At time: 395.6023964881897 and batch: 900, loss is 4.007230958938599 and perplexity is 54.99437784120815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.713028999224101 and perplexity of 111.38904676933915
Annealing...
finished 27 epochs...
Completing Train Step...
At time: 397.4192850589752 and batch: 50, loss is 4.2768880558013915 and perplexity is 72.01598122188912
At time: 398.16865372657776 and batch: 100, loss is 4.121542201042176 and perplexity is 61.65425223541154
At time: 398.9065968990326 and batch: 150, loss is 4.1376683807373045 and perplexity is 62.65655976050647
At time: 399.64467000961304 and batch: 200, loss is 4.034928331375122 and perplexity is 56.53886803019273
At time: 400.3820700645447 and batch: 250, loss is 4.206712102890014 and perplexity is 67.13544210995802
At time: 401.12091994285583 and batch: 300, loss is 4.180921802520752 and perplexity is 65.42613540214158
At time: 401.86106729507446 and batch: 350, loss is 4.148538551330566 and perplexity is 63.34136247199289
At time: 402.61072993278503 and batch: 400, loss is 4.096975622177124 and perplexity is 60.158071453850354
At time: 403.3545067310333 and batch: 450, loss is 4.090907244682312 and perplexity is 59.79411499639324
At time: 404.09737157821655 and batch: 500, loss is 3.9949641275405883 and perplexity is 54.32389185725383
At time: 404.8413279056549 and batch: 550, loss is 4.074402885437012 and perplexity is 58.815350598707234
At time: 405.584757566452 and batch: 600, loss is 4.08207145690918 and perplexity is 59.26811412435949
At time: 406.32935428619385 and batch: 650, loss is 3.938414716720581 and perplexity is 51.33715282814229
At time: 407.08107829093933 and batch: 700, loss is 3.9555467176437378 and perplexity is 52.22423805331122
At time: 407.8266475200653 and batch: 750, loss is 4.059258251190186 and perplexity is 57.93132464725485
At time: 408.571741104126 and batch: 800, loss is 3.9976360607147217 and perplexity is 54.46923575417493
At time: 409.3211393356323 and batch: 850, loss is 4.045281863212585 and perplexity is 57.12728584538071
At time: 410.07211923599243 and batch: 900, loss is 4.002209172248841 and perplexity is 54.71890008075315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.712103020654966 and perplexity of 111.28595063896448
finished 28 epochs...
Completing Train Step...
At time: 411.94230365753174 and batch: 50, loss is 4.275427007675171 and perplexity is 71.91083923492202
At time: 412.69758582115173 and batch: 100, loss is 4.120443572998047 and perplexity is 61.58655433907885
At time: 413.43830585479736 and batch: 150, loss is 4.136580247879028 and perplexity is 62.588418179314736
At time: 414.17819142341614 and batch: 200, loss is 4.0338482093811034 and perplexity is 56.4778321243626
At time: 414.92341113090515 and batch: 250, loss is 4.205608992576599 and perplexity is 67.06142514331613
At time: 415.68070888519287 and batch: 300, loss is 4.179834642410278 and perplexity is 65.35504536767668
At time: 416.44525170326233 and batch: 350, loss is 4.147604904174805 and perplexity is 63.28225158772722
At time: 417.20334672927856 and batch: 400, loss is 4.095925893783569 and perplexity is 60.094954951531896
At time: 417.96701526641846 and batch: 450, loss is 4.090084133148193 and perplexity is 59.74491802074394
At time: 418.73106145858765 and batch: 500, loss is 3.9941160392761232 and perplexity is 54.277839932898175
At time: 419.49821186065674 and batch: 550, loss is 4.073936805725098 and perplexity is 58.78794434428978
At time: 420.2636411190033 and batch: 600, loss is 4.081895928382874 and perplexity is 59.25771179261
At time: 421.0224177837372 and batch: 650, loss is 3.9383195495605468 and perplexity is 51.33226744957088
At time: 421.78371357917786 and batch: 700, loss is 3.9555948209762573 and perplexity is 52.22675027362247
At time: 422.544828414917 and batch: 750, loss is 4.059234442710877 and perplexity is 57.92994540692952
At time: 423.3044080734253 and batch: 800, loss is 3.9977330064773557 and perplexity is 54.47451657174753
At time: 424.0700707435608 and batch: 850, loss is 4.045640749931335 and perplexity is 57.147791748977
At time: 424.8317470550537 and batch: 900, loss is 4.002765655517578 and perplexity is 54.74935870720379
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711890651755137 and perplexity of 111.26231947341206
finished 29 epochs...
Completing Train Step...
At time: 426.71683859825134 and batch: 50, loss is 4.274346294403077 and perplexity is 71.8331662152448
At time: 427.4805088043213 and batch: 100, loss is 4.119515900611877 and perplexity is 61.52944868502364
At time: 428.22810792922974 and batch: 150, loss is 4.13568422794342 and perplexity is 62.53236282599529
At time: 428.97664403915405 and batch: 200, loss is 4.032980823516846 and perplexity is 56.428865290773714
At time: 429.73061060905457 and batch: 250, loss is 4.204709877967835 and perplexity is 67.00115633462852
At time: 430.50751066207886 and batch: 300, loss is 4.178982915878296 and perplexity is 65.29940444032886
At time: 431.26017570495605 and batch: 350, loss is 4.146864614486694 and perplexity is 63.23542172540718
At time: 432.0096709728241 and batch: 400, loss is 4.095142402648926 and perplexity is 60.047889527169566
At time: 432.76174306869507 and batch: 450, loss is 4.089573936462402 and perplexity is 59.714444136075436
At time: 433.528023481369 and batch: 500, loss is 3.9936450004577635 and perplexity is 54.25227898388529
At time: 434.2912440299988 and batch: 550, loss is 4.073542428016663 and perplexity is 58.76476426067023
At time: 435.0417881011963 and batch: 600, loss is 4.081752467155456 and perplexity is 59.24921121830827
At time: 435.79174065589905 and batch: 650, loss is 3.938210577964783 and perplexity is 51.326673995242075
At time: 436.5344521999359 and batch: 700, loss is 3.9556479597091676 and perplexity is 52.22952561069433
At time: 437.2807354927063 and batch: 750, loss is 4.059190859794617 and perplexity is 57.92742070598722
At time: 438.0333960056305 and batch: 800, loss is 3.9977766942977904 and perplexity is 54.476896496632264
At time: 438.78817915916443 and batch: 850, loss is 4.045927448272705 and perplexity is 57.164178274967036
At time: 439.5338637828827 and batch: 900, loss is 4.003206267356872 and perplexity is 54.77348723811178
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.71178195901113 and perplexity of 111.25022672381287
finished 30 epochs...
Completing Train Step...
At time: 441.3870937824249 and batch: 50, loss is 4.273448839187622 and perplexity is 71.76872808507531
At time: 442.13000321388245 and batch: 100, loss is 4.118728213310241 and perplexity is 61.48100180261974
At time: 442.8696963787079 and batch: 150, loss is 4.134902443885803 and perplexity is 62.48349512613609
At time: 443.60819959640503 and batch: 200, loss is 4.032232513427735 and perplexity is 56.38665479679848
At time: 444.3466680049896 and batch: 250, loss is 4.203933048248291 and perplexity is 66.94912805631684
At time: 445.0859200954437 and batch: 300, loss is 4.178265376091003 and perplexity is 65.25256632567141
At time: 445.82412099838257 and batch: 350, loss is 4.146241083145141 and perplexity is 63.19600474820249
At time: 446.56067991256714 and batch: 400, loss is 4.09448194026947 and perplexity is 60.00824324904994
At time: 447.30051589012146 and batch: 450, loss is 4.089033389091492 and perplexity is 59.68217437274592
At time: 448.04099559783936 and batch: 500, loss is 3.993127117156982 and perplexity is 54.22418990862951
At time: 448.7906656265259 and batch: 550, loss is 4.073173360824585 and perplexity is 58.74308011583079
At time: 449.5599000453949 and batch: 600, loss is 4.081572771072388 and perplexity is 59.23856532366886
At time: 450.30833435058594 and batch: 650, loss is 3.9380689430236817 and perplexity is 51.31940485958779
At time: 451.06758666038513 and batch: 700, loss is 3.95563090801239 and perplexity is 52.228635016253875
At time: 451.8148510456085 and batch: 750, loss is 4.059091720581055 and perplexity is 57.92167811171759
At time: 452.56364369392395 and batch: 800, loss is 3.997762770652771 and perplexity is 54.47613798494433
At time: 453.31377506256104 and batch: 850, loss is 4.046132001876831 and perplexity is 57.17587260967847
At time: 454.06206369400024 and batch: 900, loss is 4.003552451133728 and perplexity is 54.79245221328938
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711721759952911 and perplexity of 111.24352976651471
finished 31 epochs...
Completing Train Step...
At time: 455.88576579093933 and batch: 50, loss is 4.27264518737793 and perplexity is 71.7110741868033
At time: 456.62559151649475 and batch: 100, loss is 4.118000359535217 and perplexity is 61.436268904844525
At time: 457.36518359184265 and batch: 150, loss is 4.134204573631287 and perplexity is 62.43990496539676
At time: 458.1063904762268 and batch: 200, loss is 4.0315728282928465 and perplexity is 56.34946962542389
At time: 458.84658908843994 and batch: 250, loss is 4.203228449821472 and perplexity is 66.90197242085563
At time: 459.586385011673 and batch: 300, loss is 4.177630410194397 and perplexity is 65.21114632292482
At time: 460.3296194076538 and batch: 350, loss is 4.145678248405456 and perplexity is 63.16044584915146
At time: 461.0731873512268 and batch: 400, loss is 4.093892412185669 and perplexity is 59.972877130079496
At time: 461.8158049583435 and batch: 450, loss is 4.088487315177917 and perplexity is 59.6495923911103
At time: 462.5692753791809 and batch: 500, loss is 3.992660756111145 and perplexity is 54.198907754478206
At time: 463.3152902126312 and batch: 550, loss is 4.072806763648987 and perplexity is 58.72154901543581
At time: 464.0605068206787 and batch: 600, loss is 4.081369209289551 and perplexity is 59.22650784296163
At time: 464.81082797050476 and batch: 650, loss is 3.9378983783721924 and perplexity is 51.310652329640604
At time: 465.56128430366516 and batch: 700, loss is 3.955569100379944 and perplexity is 52.22540698773707
At time: 466.3039548397064 and batch: 750, loss is 4.058944969177246 and perplexity is 57.91317864781313
At time: 467.0485944747925 and batch: 800, loss is 3.997695317268372 and perplexity is 54.47246350899755
At time: 467.8013586997986 and batch: 850, loss is 4.046267132759095 and perplexity is 57.18359935783779
At time: 468.5570387840271 and batch: 900, loss is 4.003826208114624 and perplexity is 54.80745408292279
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711684971639555 and perplexity of 111.23943737995924
finished 32 epochs...
Completing Train Step...
At time: 470.3778793811798 and batch: 50, loss is 4.271896209716797 and perplexity is 71.65738430295319
At time: 471.11979365348816 and batch: 100, loss is 4.117322263717651 and perplexity is 61.39462334928757
At time: 471.8605787754059 and batch: 150, loss is 4.13356572151184 and perplexity is 62.40002783892122
At time: 472.60063099861145 and batch: 200, loss is 4.0309775447845455 and perplexity is 56.315935697541754
At time: 473.3388662338257 and batch: 250, loss is 4.202598943710327 and perplexity is 66.85987047347597
At time: 474.07910799980164 and batch: 300, loss is 4.177040433883667 and perplexity is 65.17268463825573
At time: 474.81972002983093 and batch: 350, loss is 4.14515869140625 and perplexity is 63.12763892071017
At time: 475.5625309944153 and batch: 400, loss is 4.093333768844604 and perplexity is 59.939383038123076
At time: 476.3113257884979 and batch: 450, loss is 4.087989668846131 and perplexity is 59.61991537520583
At time: 477.05989837646484 and batch: 500, loss is 3.992233214378357 and perplexity is 54.17574041239732
At time: 477.8085126876831 and batch: 550, loss is 4.072446746826172 and perplexity is 58.70041207498389
At time: 478.55751180648804 and batch: 600, loss is 4.081149206161499 and perplexity is 59.21347925918977
At time: 479.3068034648895 and batch: 650, loss is 3.9377045249938964 and perplexity is 51.300706550386536
At time: 480.05492401123047 and batch: 700, loss is 3.955503783226013 and perplexity is 52.221995884192744
At time: 480.8131191730499 and batch: 750, loss is 4.058794012069702 and perplexity is 57.90443690170674
At time: 481.5619742870331 and batch: 800, loss is 3.9976030826568603 and perplexity is 54.467439494185314
At time: 482.31537556648254 and batch: 850, loss is 4.046361465454101 and perplexity is 57.18899389531202
At time: 483.0662009716034 and batch: 900, loss is 4.004055042266845 and perplexity is 54.81999733532077
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711689988227739 and perplexity of 111.23999542380623
Annealing...
finished 33 epochs...
Completing Train Step...
At time: 484.8807809352875 and batch: 50, loss is 4.271221809387207 and perplexity is 71.60907483115457
At time: 485.6324405670166 and batch: 100, loss is 4.116674823760986 and perplexity is 61.35488688188329
At time: 486.37080931663513 and batch: 150, loss is 4.133110756874085 and perplexity is 62.37164449005914
At time: 487.12929010391235 and batch: 200, loss is 4.030464272499085 and perplexity is 56.28703770542209
At time: 487.8650884628296 and batch: 250, loss is 4.2020862007141115 and perplexity is 66.8255973305707
At time: 488.60236501693726 and batch: 300, loss is 4.176394324302674 and perplexity is 65.1305895427786
At time: 489.33859848976135 and batch: 350, loss is 4.144622793197632 and perplexity is 63.09381799519439
At time: 490.08242893218994 and batch: 400, loss is 4.0926590204238895 and perplexity is 59.898952675775
At time: 490.82863426208496 and batch: 450, loss is 4.087455606460571 and perplexity is 59.58808312193483
At time: 491.5661597251892 and batch: 500, loss is 3.991580286026001 and perplexity is 54.140379080936384
At time: 492.3039209842682 and batch: 550, loss is 4.071325163841248 and perplexity is 58.63461159883759
At time: 493.0519349575043 and batch: 600, loss is 4.080013656616211 and perplexity is 59.14627758238007
At time: 493.8107056617737 and batch: 650, loss is 3.936539120674133 and perplexity is 51.240955309305726
At time: 494.57021260261536 and batch: 700, loss is 3.954084062576294 and perplexity is 52.147907842871845
At time: 495.33021235466003 and batch: 750, loss is 4.05742561340332 and perplexity is 57.825254736213495
At time: 496.1006808280945 and batch: 800, loss is 3.9961574125289916 and perplexity is 54.38875443399166
At time: 496.86592960357666 and batch: 850, loss is 4.0450156927108765 and perplexity is 57.11208227050733
At time: 497.62598490715027 and batch: 900, loss is 4.002827501296997 and perplexity is 54.752744828673286
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711570426209332 and perplexity of 111.22669614048613
finished 34 epochs...
Completing Train Step...
At time: 499.53443217277527 and batch: 50, loss is 4.271018419265747 and perplexity is 71.59451173377234
At time: 500.30191469192505 and batch: 100, loss is 4.116536021232605 and perplexity is 61.34637125946417
At time: 501.0601987838745 and batch: 150, loss is 4.132870678901672 and perplexity is 62.356672229440846
At time: 501.82219886779785 and batch: 200, loss is 4.030321207046509 and perplexity is 56.27898555090488
At time: 502.5788369178772 and batch: 250, loss is 4.201881222724914 and perplexity is 66.8119009577784
At time: 503.33878469467163 and batch: 300, loss is 4.1762401437759396 and perplexity is 65.12054844826689
At time: 504.09158873558044 and batch: 350, loss is 4.1444633674621585 and perplexity is 63.08376001862813
At time: 504.8448677062988 and batch: 400, loss is 4.09252818107605 and perplexity is 59.8911160485514
At time: 505.5935950279236 and batch: 450, loss is 4.087334175109863 and perplexity is 59.58084769982766
At time: 506.3473310470581 and batch: 500, loss is 3.9914653396606443 and perplexity is 54.13415619879769
At time: 507.1240944862366 and batch: 550, loss is 4.07124418258667 and perplexity is 58.629863486685274
At time: 507.8782811164856 and batch: 600, loss is 4.079983258247376 and perplexity is 59.14447965934598
At time: 508.6344542503357 and batch: 650, loss is 3.9365278625488282 and perplexity is 51.24037843545737
At time: 509.3923239707947 and batch: 700, loss is 3.9540617942810057 and perplexity is 52.146746610790714
At time: 510.1490852832794 and batch: 750, loss is 4.0574052619934085 and perplexity is 57.82407792272604
At time: 510.89792823791504 and batch: 800, loss is 3.996167707443237 and perplexity is 54.389314364436686
At time: 511.65789127349854 and batch: 850, loss is 4.045065155029297 and perplexity is 57.114907236370364
At time: 512.413161277771 and batch: 900, loss is 4.002907752990723 and perplexity is 54.75713900549964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7115478515625 and perplexity of 111.22418526544365
finished 35 epochs...
Completing Train Step...
At time: 514.2961194515228 and batch: 50, loss is 4.27084324836731 and perplexity is 71.58197155719782
At time: 515.0489666461945 and batch: 100, loss is 4.11638186454773 and perplexity is 61.33691503513052
At time: 515.7969551086426 and batch: 150, loss is 4.132702031135559 and perplexity is 62.346156802693635
At time: 516.55317735672 and batch: 200, loss is 4.030177459716797 and perplexity is 56.27089617843984
At time: 517.3193392753601 and batch: 250, loss is 4.201731252670288 and perplexity is 66.80188192463834
At time: 518.0754337310791 and batch: 300, loss is 4.17608904838562 and perplexity is 65.11070977688894
At time: 518.8206269741058 and batch: 350, loss is 4.144351983070374 and perplexity is 63.076733863696234
At time: 519.5646305084229 and batch: 400, loss is 4.092393517494202 and perplexity is 59.88305143936019
At time: 520.308159828186 and batch: 450, loss is 4.087217431068421 and perplexity is 59.57389239687763
At time: 521.0768930912018 and batch: 500, loss is 3.9913561296463014 and perplexity is 54.128244529635396
At time: 521.8444664478302 and batch: 550, loss is 4.071168384552002 and perplexity is 58.62541962668016
At time: 522.5920283794403 and batch: 600, loss is 4.079955239295959 and perplexity is 59.14282251625965
At time: 523.3633880615234 and batch: 650, loss is 3.936510453224182 and perplexity is 51.23948638283923
At time: 524.1099331378937 and batch: 700, loss is 3.9540432786941526 and perplexity is 52.14578109211334
At time: 524.8537681102753 and batch: 750, loss is 4.057388963699341 and perplexity is 57.82313549657988
At time: 525.5974791049957 and batch: 800, loss is 3.996175141334534 and perplexity is 54.389718690190236
At time: 526.3724701404572 and batch: 850, loss is 4.0451145362854 and perplexity is 57.11772771187063
At time: 527.1164734363556 and batch: 900, loss is 4.0029737710952755 and perplexity is 54.76075408735663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711533637895976 and perplexity of 111.22260437320007
finished 36 epochs...
Completing Train Step...
At time: 528.9346635341644 and batch: 50, loss is 4.270670938491821 and perplexity is 71.5696383391897
At time: 529.687239408493 and batch: 100, loss is 4.1162314176559445 and perplexity is 61.32768778103503
At time: 530.4275438785553 and batch: 150, loss is 4.1325459480285645 and perplexity is 62.336426380227735
At time: 531.1669683456421 and batch: 200, loss is 4.030038056373596 and perplexity is 56.2630523741266
At time: 531.9048087596893 and batch: 250, loss is 4.201587944030762 and perplexity is 66.79230932375653
At time: 532.645069360733 and batch: 300, loss is 4.175943322181702 and perplexity is 65.1012221316348
At time: 533.3844349384308 and batch: 350, loss is 4.144242267608643 and perplexity is 63.06981375034456
At time: 534.1234760284424 and batch: 400, loss is 4.092261829376221 and perplexity is 59.87516607223316
At time: 534.8618569374084 and batch: 450, loss is 4.087104878425598 and perplexity is 59.567187575174906
At time: 535.6013264656067 and batch: 500, loss is 3.9912511205673216 and perplexity is 54.12256087095365
At time: 536.3455083370209 and batch: 550, loss is 4.071094665527344 and perplexity is 58.62109797722096
At time: 537.0865042209625 and batch: 600, loss is 4.079926815032959 and perplexity is 59.14114144900954
At time: 537.8252081871033 and batch: 650, loss is 3.936490969657898 and perplexity is 51.23848806463537
At time: 538.5660009384155 and batch: 700, loss is 3.9540231704711912 and perplexity is 52.1447325436629
At time: 539.3073217868805 and batch: 750, loss is 4.05737069606781 and perplexity is 57.82207921449457
At time: 540.0520384311676 and batch: 800, loss is 3.996178297996521 and perplexity is 54.389890380418684
At time: 540.8068270683289 and batch: 850, loss is 4.04515917301178 and perplexity is 57.12027731715655
At time: 541.5594713687897 and batch: 900, loss is 4.0030347442626955 and perplexity is 54.76409312577849
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7115194242294525 and perplexity of 111.22102350342661
finished 37 epochs...
Completing Train Step...
At time: 543.4110157489777 and batch: 50, loss is 4.27050407409668 and perplexity is 71.55769691110504
At time: 544.1577882766724 and batch: 100, loss is 4.1160845279693605 and perplexity is 61.31868003778644
At time: 544.9138512611389 and batch: 150, loss is 4.1323971891403195 and perplexity is 62.32715397243589
At time: 545.6549301147461 and batch: 200, loss is 4.029902663230896 and perplexity is 56.25543525831204
At time: 546.4014112949371 and batch: 250, loss is 4.201448307037354 and perplexity is 66.78298329764405
At time: 547.1453499794006 and batch: 300, loss is 4.175802617073059 and perplexity is 65.0920627015064
At time: 547.8870642185211 and batch: 350, loss is 4.1441339492797855 and perplexity is 63.06298250349907
At time: 548.6278903484344 and batch: 400, loss is 4.092132787704468 and perplexity is 59.86744017919853
At time: 549.3693659305573 and batch: 450, loss is 4.086994843482971 and perplexity is 59.56063346370488
At time: 550.1132254600525 and batch: 500, loss is 3.9911498260498046 and perplexity is 54.117078829918476
At time: 550.8546206951141 and batch: 550, loss is 4.071021490097046 and perplexity is 58.61680851009564
At time: 551.5961484909058 and batch: 600, loss is 4.079897379875183 and perplexity is 59.139400645800464
At time: 552.3383133411407 and batch: 650, loss is 3.9364690160751343 and perplexity is 51.23736320859431
At time: 553.0824847221375 and batch: 700, loss is 3.9540023517608645 and perplexity is 52.14364696888118
At time: 553.8267779350281 and batch: 750, loss is 4.057349886894226 and perplexity is 57.82087599733022
At time: 554.5719795227051 and batch: 800, loss is 3.996177635192871 and perplexity is 54.38985433061278
At time: 555.3157444000244 and batch: 850, loss is 4.045199408531189 and perplexity is 57.122575627419714
At time: 556.0610313415527 and batch: 900, loss is 4.003091425895691 and perplexity is 54.767197331981265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711510227151113 and perplexity of 111.22000059966429
finished 38 epochs...
Completing Train Step...
At time: 557.8921599388123 and batch: 50, loss is 4.270341520309448 and perplexity is 71.54606588182611
At time: 558.6317684650421 and batch: 100, loss is 4.115940904617309 and perplexity is 61.30987387581644
At time: 559.3714671134949 and batch: 150, loss is 4.132253613471985 and perplexity is 62.31820595202327
At time: 560.1118547916412 and batch: 200, loss is 4.029770312309265 and perplexity is 56.24799029229377
At time: 560.8515825271606 and batch: 250, loss is 4.201311750411987 and perplexity is 66.7738642614596
At time: 561.5919554233551 and batch: 300, loss is 4.175666012763977 and perplexity is 65.08317145255963
At time: 562.3311905860901 and batch: 350, loss is 4.1440272760391235 and perplexity is 63.05625572957937
At time: 563.0689640045166 and batch: 400, loss is 4.092006683349609 and perplexity is 59.85989111027292
At time: 563.8299176692963 and batch: 450, loss is 4.086886954307556 and perplexity is 59.554207862705915
At time: 564.5724239349365 and batch: 500, loss is 3.9910514974594116 and perplexity is 54.111757835448174
At time: 565.312252998352 and batch: 550, loss is 4.070948343276978 and perplexity is 58.61252103376015
At time: 566.0529074668884 and batch: 600, loss is 4.07986668586731 and perplexity is 59.13758544842941
At time: 566.7943856716156 and batch: 650, loss is 3.936444253921509 and perplexity is 51.23609447684351
At time: 567.5374655723572 and batch: 700, loss is 3.9539812850952147 and perplexity is 52.14254848767544
At time: 568.28022813797 and batch: 750, loss is 4.057326879501343 and perplexity is 57.81954570502263
At time: 569.0324354171753 and batch: 800, loss is 3.996173372268677 and perplexity is 54.389622471281044
At time: 569.7822775840759 and batch: 850, loss is 4.045235495567322 and perplexity is 57.1246370490655
At time: 570.5421736240387 and batch: 900, loss is 4.003144397735595 and perplexity is 54.77009852803053
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711501866170805 and perplexity of 111.21907069531687
finished 39 epochs...
Completing Train Step...
At time: 572.4602973461151 and batch: 50, loss is 4.270182275772095 and perplexity is 71.53467346877918
At time: 573.2128999233246 and batch: 100, loss is 4.115799927711487 and perplexity is 61.301231208723635
At time: 573.9716472625732 and batch: 150, loss is 4.132114148139953 and perplexity is 62.309515328772974
At time: 574.7333188056946 and batch: 200, loss is 4.029640583992005 and perplexity is 56.2406938084544
At time: 575.4914503097534 and batch: 250, loss is 4.201177697181702 and perplexity is 66.76491360920204
At time: 576.2597575187683 and batch: 300, loss is 4.175532875061035 and perplexity is 65.07450700540733
At time: 577.0170724391937 and batch: 350, loss is 4.143922305107116 and perplexity is 63.049637003039585
At time: 577.7783770561218 and batch: 400, loss is 4.091882658004761 and perplexity is 59.85246742700733
At time: 578.5360059738159 and batch: 450, loss is 4.086781024932861 and perplexity is 59.54789965682462
At time: 579.2869186401367 and batch: 500, loss is 3.990956230163574 and perplexity is 54.10660300015369
At time: 580.0439863204956 and batch: 550, loss is 4.070869565010071 and perplexity is 58.60790382280438
At time: 580.8045825958252 and batch: 600, loss is 4.0798351860046385 and perplexity is 59.13572265194815
At time: 581.5731105804443 and batch: 650, loss is 3.936416425704956 and perplexity is 51.234668687549764
At time: 582.3361237049103 and batch: 700, loss is 3.953958716392517 and perplexity is 52.14137171127992
At time: 583.1216294765472 and batch: 750, loss is 4.0573009634017945 and perplexity is 57.81804726733722
At time: 583.8786566257477 and batch: 800, loss is 3.9961654138565064 and perplexity is 54.389189617970025
At time: 584.6381649971008 and batch: 850, loss is 4.045266642570495 and perplexity is 57.126416338026544
At time: 585.397275686264 and batch: 900, loss is 4.003193325996399 and perplexity is 54.77277839925574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711495177386558 and perplexity of 111.21832677743686
finished 40 epochs...
Completing Train Step...
At time: 587.3021714687347 and batch: 50, loss is 4.2700252342224125 and perplexity is 71.52344043485121
At time: 588.0697357654572 and batch: 100, loss is 4.115660195350647 and perplexity is 61.292666041393886
At time: 588.8238546848297 and batch: 150, loss is 4.131977171897888 and perplexity is 62.3009809900325
At time: 589.5645182132721 and batch: 200, loss is 4.029512252807617 and perplexity is 56.23347683669836
At time: 590.3096966743469 and batch: 250, loss is 4.201044921875 and perplexity is 66.75604946580334
At time: 591.057727098465 and batch: 300, loss is 4.175401892662048 and perplexity is 65.06598394856428
At time: 591.8061919212341 and batch: 350, loss is 4.143818926811218 and perplexity is 63.043119375905626
At time: 592.5629451274872 and batch: 400, loss is 4.091760058403015 and perplexity is 59.84512998812999
At time: 593.3054618835449 and batch: 450, loss is 4.0866765213012695 and perplexity is 59.54167701020706
At time: 594.0477879047394 and batch: 500, loss is 3.9908653926849365 and perplexity is 54.10168831598164
At time: 594.7870087623596 and batch: 550, loss is 4.07080367565155 and perplexity is 58.60404231283488
At time: 595.5247704982758 and batch: 600, loss is 4.079801273345947 and perplexity is 59.13371723637409
At time: 596.2714869976044 and batch: 650, loss is 3.9363874053955077 and perplexity is 51.23318186318412
At time: 597.0357224941254 and batch: 700, loss is 3.953936061859131 and perplexity is 52.14019048621379
At time: 597.7868278026581 and batch: 750, loss is 4.057274837493896 and perplexity is 57.81653673809151
At time: 598.5301034450531 and batch: 800, loss is 3.9961566352844238 and perplexity is 54.388712160644154
At time: 599.2731463909149 and batch: 850, loss is 4.045296897888184 and perplexity is 57.12814474204783
At time: 600.0156481266022 and batch: 900, loss is 4.003240652084351 and perplexity is 54.775370641923494
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711488488602312 and perplexity of 111.21758286453273
finished 41 epochs...
Completing Train Step...
At time: 601.8481833934784 and batch: 50, loss is 4.269871425628662 and perplexity is 71.51244036103238
At time: 602.6103358268738 and batch: 100, loss is 4.115524053573608 and perplexity is 61.28432211691083
At time: 603.346462726593 and batch: 150, loss is 4.131844892501831 and perplexity is 62.29274039893559
At time: 604.0834197998047 and batch: 200, loss is 4.029388136863709 and perplexity is 56.226497798755425
At time: 604.820609331131 and batch: 250, loss is 4.200916352272034 and perplexity is 66.74746721874783
At time: 605.5578827857971 and batch: 300, loss is 4.175274567604065 and perplexity is 65.05769994577706
At time: 606.3041281700134 and batch: 350, loss is 4.14371766090393 and perplexity is 63.03673558045963
At time: 607.04154753685 and batch: 400, loss is 4.091639542579651 and perplexity is 59.837918137594954
At time: 607.777988910675 and batch: 450, loss is 4.086574754714966 and perplexity is 59.53561796530432
At time: 608.5149002075195 and batch: 500, loss is 3.990773377418518 and perplexity is 54.09671036374488
At time: 609.251307964325 and batch: 550, loss is 4.07073073387146 and perplexity is 58.59976778556578
At time: 609.9909434318542 and batch: 600, loss is 4.079766244888305 and perplexity is 59.13164590974256
At time: 610.7284364700317 and batch: 650, loss is 3.93635703086853 and perplexity is 51.2316257031534
At time: 611.4646143913269 and batch: 700, loss is 3.953912072181702 and perplexity is 52.13893967486628
At time: 612.2014684677124 and batch: 750, loss is 4.057247123718262 and perplexity is 57.81493444576728
At time: 612.9386827945709 and batch: 800, loss is 3.9961457872390747 and perplexity is 54.38812215262838
At time: 613.6785833835602 and batch: 850, loss is 4.045324039459229 and perplexity is 57.12969531068935
At time: 614.421407699585 and batch: 900, loss is 4.0032856798172 and perplexity is 54.777837108208736
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711483472014127 and perplexity of 111.21702493312003
finished 42 epochs...
Completing Train Step...
At time: 616.2348384857178 and batch: 50, loss is 4.269719686508179 and perplexity is 71.50158994946513
At time: 616.9865474700928 and batch: 100, loss is 4.11538987159729 and perplexity is 61.276099417133224
At time: 617.7246978282928 and batch: 150, loss is 4.131714973449707 and perplexity is 62.284647910844356
At time: 618.4683165550232 and batch: 200, loss is 4.029266080856323 and perplexity is 56.21963543572967
At time: 619.2131578922272 and batch: 250, loss is 4.200789427757263 and perplexity is 66.73899586648139
At time: 619.9528532028198 and batch: 300, loss is 4.175149416923523 and perplexity is 65.04955843982246
At time: 620.6933786869049 and batch: 350, loss is 4.143618125915527 and perplexity is 63.03046153196349
At time: 621.4582688808441 and batch: 400, loss is 4.091520547866821 and perplexity is 59.83079816533776
At time: 622.1983253955841 and batch: 450, loss is 4.086474604606629 and perplexity is 59.52965576527765
At time: 622.9402713775635 and batch: 500, loss is 3.9906829404830932 and perplexity is 54.091818244260494
At time: 623.6829252243042 and batch: 550, loss is 4.070657625198364 and perplexity is 58.59548379089974
At time: 624.4260406494141 and batch: 600, loss is 4.079730157852173 and perplexity is 59.12951206240238
At time: 625.168377161026 and batch: 650, loss is 3.93632550239563 and perplexity is 51.23001047369379
At time: 625.9121248722076 and batch: 700, loss is 3.953886728286743 and perplexity is 52.13761828780052
At time: 626.6573178768158 and batch: 750, loss is 4.057217683792114 and perplexity is 57.81323240342109
At time: 627.4086394309998 and batch: 800, loss is 3.9961329650878907 and perplexity is 54.38742478437442
At time: 628.1596088409424 and batch: 850, loss is 4.045348420143127 and perplexity is 57.13108818871152
At time: 628.9097771644592 and batch: 900, loss is 4.003328523635864 and perplexity is 54.78018405020425
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711480963720034 and perplexity of 111.21674596846329
finished 43 epochs...
Completing Train Step...
At time: 630.732414484024 and batch: 50, loss is 4.269569711685181 and perplexity is 71.49086731525107
At time: 631.4854769706726 and batch: 100, loss is 4.11525755405426 and perplexity is 61.26799205059714
At time: 632.2277791500092 and batch: 150, loss is 4.131587328910828 and perplexity is 62.276698123066815
At time: 632.9714097976685 and batch: 200, loss is 4.029145188331604 and perplexity is 56.21283931287157
At time: 633.712388753891 and batch: 250, loss is 4.200664653778076 and perplexity is 66.73066909589313
At time: 634.4537398815155 and batch: 300, loss is 4.175026245117188 and perplexity is 65.04154666163086
At time: 635.19664311409 and batch: 350, loss is 4.1435203981399535 and perplexity is 63.024302006147884
At time: 635.9451215267181 and batch: 400, loss is 4.091402816772461 and perplexity is 59.823754634622716
At time: 636.6874995231628 and batch: 450, loss is 4.086375684738159 and perplexity is 59.52376739080272
At time: 637.431886434555 and batch: 500, loss is 3.990593795776367 and perplexity is 54.0869964599083
At time: 638.1824312210083 and batch: 550, loss is 4.07058427810669 and perplexity is 58.591186140190466
At time: 638.9282207489014 and batch: 600, loss is 4.079693088531494 and perplexity is 59.1273202121836
At time: 639.6704921722412 and batch: 650, loss is 3.9362927436828614 and perplexity is 51.22833227198356
At time: 640.4262869358063 and batch: 700, loss is 3.9538605070114134 and perplexity is 52.13625119087995
At time: 641.1695861816406 and batch: 750, loss is 4.057186880111694 and perplexity is 57.81145157051435
At time: 641.9115746021271 and batch: 800, loss is 3.996118392944336 and perplexity is 54.38663224878736
At time: 642.6638917922974 and batch: 850, loss is 4.045370135307312 and perplexity is 57.13232881314174
At time: 643.4197235107422 and batch: 900, loss is 4.003369064331054 and perplexity is 54.78240492196583
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7114801276220035 and perplexity of 111.21665298039986
finished 44 epochs...
Completing Train Step...
At time: 645.290284872055 and batch: 50, loss is 4.26942141532898 and perplexity is 71.48026626619446
At time: 646.038492679596 and batch: 100, loss is 4.115126504898071 and perplexity is 61.25996345801914
At time: 646.7762944698334 and batch: 150, loss is 4.131461720466614 and perplexity is 62.26887613516895
At time: 647.5138103961945 and batch: 200, loss is 4.029026718139648 and perplexity is 56.20618016147104
At time: 648.2598216533661 and batch: 250, loss is 4.200541334152222 and perplexity is 66.72244040213738
At time: 649.0011434555054 and batch: 300, loss is 4.174905061721802 and perplexity is 65.03366518372702
At time: 649.7388217449188 and batch: 350, loss is 4.143424277305603 and perplexity is 63.01824434879289
At time: 650.4769670963287 and batch: 400, loss is 4.091286644935608 and perplexity is 59.81680520283134
At time: 651.2148566246033 and batch: 450, loss is 4.086277990341187 and perplexity is 59.51795253628594
At time: 651.9527544975281 and batch: 500, loss is 3.990505995750427 and perplexity is 54.08224782868413
At time: 652.7014164924622 and batch: 550, loss is 4.070510993003845 and perplexity is 58.58689243642256
At time: 653.456093788147 and batch: 600, loss is 4.079655623435974 and perplexity is 59.125105042980024
At time: 654.2118117809296 and batch: 650, loss is 3.936259183883667 and perplexity is 51.226613088287344
At time: 654.9688858985901 and batch: 700, loss is 3.953833327293396 and perplexity is 52.13483416153141
At time: 655.727294921875 and batch: 750, loss is 4.057154889106751 and perplexity is 57.80960215366386
At time: 656.4799942970276 and batch: 800, loss is 3.9961028051376344 and perplexity is 54.385784487084116
At time: 657.2271156311035 and batch: 850, loss is 4.04539002418518 and perplexity is 57.13346512235175
At time: 657.9799189567566 and batch: 900, loss is 4.0034077501297 and perplexity is 54.784524264045906
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711479709572988 and perplexity of 111.21660648639725
finished 45 epochs...
Completing Train Step...
At time: 659.8707242012024 and batch: 50, loss is 4.269274616241455 and perplexity is 71.46977379849179
At time: 660.6277849674225 and batch: 100, loss is 4.11499680519104 and perplexity is 61.2520185739416
At time: 661.389636516571 and batch: 150, loss is 4.131336679458618 and perplexity is 62.261090458904796
At time: 662.1545956134796 and batch: 200, loss is 4.028910903930664 and perplexity is 56.19967106410589
At time: 662.9132783412933 and batch: 250, loss is 4.200419945716858 and perplexity is 66.71434156105607
At time: 663.6714813709259 and batch: 300, loss is 4.1747850847244266 and perplexity is 65.02586310789415
At time: 664.4310290813446 and batch: 350, loss is 4.143329358100891 and perplexity is 63.0122629910343
At time: 665.1851131916046 and batch: 400, loss is 4.091172804832459 and perplexity is 59.80999603914228
At time: 665.9509184360504 and batch: 450, loss is 4.086181197166443 and perplexity is 59.512191883506134
At time: 666.7065649032593 and batch: 500, loss is 3.990419101715088 and perplexity is 54.07754860810016
At time: 667.4558520317078 and batch: 550, loss is 4.0704377746582034 and perplexity is 58.58260295811825
At time: 668.2089416980743 and batch: 600, loss is 4.079617629051208 and perplexity is 59.12285866366489
At time: 668.9556231498718 and batch: 650, loss is 3.936224799156189 and perplexity is 51.22485170543918
At time: 669.6992950439453 and batch: 700, loss is 3.9538056325912474 and perplexity is 52.13339032282118
At time: 670.4507367610931 and batch: 750, loss is 4.0571218824386595 and perplexity is 57.8076940828028
At time: 671.2004837989807 and batch: 800, loss is 3.996085877418518 and perplexity is 54.384863867592415
At time: 671.949844121933 and batch: 850, loss is 4.045408029556274 and perplexity is 57.13449384085435
At time: 672.7125082015991 and batch: 900, loss is 4.003444976806641 and perplexity is 54.78656374779342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711481799818065 and perplexity of 111.21683895660445
Annealing...
finished 46 epochs...
Completing Train Step...
At time: 674.6162526607513 and batch: 50, loss is 4.269136228561401 and perplexity is 71.45988394663443
At time: 675.3777477741241 and batch: 100, loss is 4.114878625869751 and perplexity is 61.24478027967572
At time: 676.1342227458954 and batch: 150, loss is 4.131220169067383 and perplexity is 62.253836817467196
At time: 676.8888514041901 and batch: 200, loss is 4.0288135433197025 and perplexity is 56.19419969614742
At time: 677.6361103057861 and batch: 250, loss is 4.200295338630676 and perplexity is 66.70602899926043
At time: 678.3827967643738 and batch: 300, loss is 4.174645042419433 and perplexity is 65.01675737374921
At time: 679.1640605926514 and batch: 350, loss is 4.143175506591797 and perplexity is 63.00256920500266
At time: 679.9119000434875 and batch: 400, loss is 4.091024293899536 and perplexity is 59.80111426036934
At time: 680.66015791893 and batch: 450, loss is 4.086067218780517 and perplexity is 59.50540916648126
At time: 681.4168829917908 and batch: 500, loss is 3.9902748966217043 and perplexity is 54.069750912400394
At time: 682.1710338592529 and batch: 550, loss is 4.07018358707428 and perplexity is 58.56771388020192
At time: 682.9153168201447 and batch: 600, loss is 4.079357447624207 and perplexity is 59.10747799489832
At time: 683.6589450836182 and batch: 650, loss is 3.9359607458114625 and perplexity is 51.21132739766117
At time: 684.4039070606232 and batch: 700, loss is 3.9534875440597532 and perplexity is 52.11680992640836
At time: 685.149245262146 and batch: 750, loss is 4.056826643943786 and perplexity is 57.79062954538783
At time: 685.8943750858307 and batch: 800, loss is 3.9957802963256834 and perplexity is 54.36824742042344
At time: 686.6386091709137 and batch: 850, loss is 4.045124187469482 and perplexity is 57.118278968235266
At time: 687.3840668201447 and batch: 900, loss is 4.003134722709656 and perplexity is 54.76956862846996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7114805456710185 and perplexity of 111.2166994744218
Annealing...
finished 47 epochs...
Completing Train Step...
At time: 689.2598378658295 and batch: 50, loss is 4.269106283187866 and perplexity is 71.45774408575647
At time: 690.0265626907349 and batch: 100, loss is 4.114853186607361 and perplexity is 61.24322227745747
At time: 690.773485660553 and batch: 150, loss is 4.131192808151245 and perplexity is 62.25213351876077
At time: 691.5185210704803 and batch: 200, loss is 4.028794021606445 and perplexity is 56.193102699801884
At time: 692.263237953186 and batch: 250, loss is 4.200266432762146 and perplexity is 66.70410083142383
At time: 693.0104491710663 and batch: 300, loss is 4.174616804122925 and perplexity is 65.01492143719848
At time: 693.756279706955 and batch: 350, loss is 4.14314199924469 and perplexity is 63.00045819141506
At time: 694.4975929260254 and batch: 400, loss is 4.090995397567749 and perplexity is 59.79938625249711
At time: 695.2498550415039 and batch: 450, loss is 4.086045627593994 and perplexity is 59.504124387962804
At time: 695.9993991851807 and batch: 500, loss is 3.9902473974227903 and perplexity is 54.06826405800857
At time: 696.7477169036865 and batch: 550, loss is 4.070131478309631 and perplexity is 58.564662068497064
At time: 697.4957242012024 and batch: 600, loss is 4.079302716255188 and perplexity is 59.104243050235674
At time: 698.2515561580658 and batch: 650, loss is 3.935905156135559 and perplexity is 51.208480655694025
At time: 698.9937088489532 and batch: 700, loss is 3.953421173095703 and perplexity is 52.113350998277795
At time: 699.735847234726 and batch: 750, loss is 4.056763110160827 and perplexity is 57.78695800470789
At time: 700.4810614585876 and batch: 800, loss is 3.9957177686691283 and perplexity is 54.36484800760099
At time: 701.2254784107208 and batch: 850, loss is 4.04506443977356 and perplexity is 57.11486638461989
At time: 701.9787797927856 and batch: 900, loss is 4.003070087432861 and perplexity is 54.7660286966452
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711481381769049 and perplexity of 111.2167924625241
Annealing...
finished 48 epochs...
Completing Train Step...
At time: 703.8228714466095 and batch: 50, loss is 4.2691011714935305 and perplexity is 71.45737881654439
At time: 704.5835599899292 and batch: 100, loss is 4.114848976135254 and perplexity is 61.24296441512119
At time: 705.3219969272614 and batch: 150, loss is 4.131188163757324 and perplexity is 62.25184439600169
At time: 706.0709161758423 and batch: 200, loss is 4.028790860176087 and perplexity is 56.19292504950189
At time: 706.8254156112671 and batch: 250, loss is 4.200261535644532 and perplexity is 66.70377417439653
At time: 707.5869369506836 and batch: 300, loss is 4.174612107276917 and perplexity is 65.01461607284138
At time: 708.3478636741638 and batch: 350, loss is 4.143136148452759 and perplexity is 63.00008958992094
At time: 709.1053054332733 and batch: 400, loss is 4.0909906768798825 and perplexity is 59.79910395892632
At time: 709.8670015335083 and batch: 450, loss is 4.086042141914367 and perplexity is 59.5039169760102
At time: 710.6247828006744 and batch: 500, loss is 3.9902427530288698 and perplexity is 54.06801294427482
At time: 711.376523733139 and batch: 550, loss is 4.070121865272522 and perplexity is 58.56409908693334
At time: 712.1230194568634 and batch: 600, loss is 4.079292259216309 and perplexity is 59.10362499809967
At time: 712.8707282543182 and batch: 650, loss is 3.9358944654464723 and perplexity is 51.207933204675044
At time: 713.6215553283691 and batch: 700, loss is 3.9534083652496337 and perplexity is 52.112683542774384
At time: 714.3659284114838 and batch: 750, loss is 4.056750745773315 and perplexity is 57.78624350878314
At time: 715.1089537143707 and batch: 800, loss is 3.995705647468567 and perplexity is 54.36418904436852
At time: 715.8498198986053 and batch: 850, loss is 4.0450526714324955 and perplexity is 57.11419424134744
At time: 716.5898768901825 and batch: 900, loss is 4.0030570983886715 and perplexity is 54.765317342898285
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711481381769049 and perplexity of 111.2167924625241
Annealing...
finished 49 epochs...
Completing Train Step...
At time: 718.4311740398407 and batch: 50, loss is 4.269100723266601 and perplexity is 71.45734678743008
At time: 719.214451789856 and batch: 100, loss is 4.114848470687866 and perplexity is 61.242933460032624
At time: 719.9660685062408 and batch: 150, loss is 4.131187524795532 and perplexity is 62.25180461946435
At time: 720.7174625396729 and batch: 200, loss is 4.028790702819824 and perplexity is 56.19291620719393
At time: 721.4602377414703 and batch: 250, loss is 4.200260944366455 and perplexity is 66.70373473392891
At time: 722.2022280693054 and batch: 300, loss is 4.174611830711365 and perplexity is 65.0145980920407
At time: 722.9431080818176 and batch: 350, loss is 4.14313542842865 and perplexity is 63.00004422835391
At time: 723.6841459274292 and batch: 400, loss is 4.090990309715271 and perplexity is 59.79908200281557
At time: 724.423392534256 and batch: 450, loss is 4.086041960716248 and perplexity is 59.50390619401332
At time: 725.1635074615479 and batch: 500, loss is 3.9902424907684324 and perplexity is 54.06799876437596
At time: 725.9028339385986 and batch: 550, loss is 4.070120444297791 and perplexity is 58.56401586888747
At time: 726.6424813270569 and batch: 600, loss is 4.079290728569031 and perplexity is 59.10353453136622
At time: 727.3820700645447 and batch: 650, loss is 3.9358928966522218 and perplexity is 51.20785287002686
At time: 728.1240322589874 and batch: 700, loss is 3.9534063386917113 and perplexity is 52.112577933509705
At time: 728.865211725235 and batch: 750, loss is 4.056748909950256 and perplexity is 57.786137423562195
At time: 729.6168715953827 and batch: 800, loss is 3.99570387840271 and perplexity is 54.36409287062291
At time: 730.3764941692352 and batch: 850, loss is 4.045050892829895 and perplexity is 57.114092657983385
At time: 731.1286144256592 and batch: 900, loss is 4.003055038452149 and perplexity is 54.76520452993711
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.711481381769049 and perplexity of 111.2167924625241
Annealing...
Model not improving. Stopping early with 111.21660648639725 lossat 49 epochs.
Finished Training.
langmodel
SETTINGS FOR THIS RUN
{'clip': 0.02016468994209586, 'rnn_dropout': 0.7966620185396394, 'anneal': 4.113211753950871, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 7.996266918850972, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.2547448230514421, 'seq_len': 35}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.07905596892038981 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 2 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.1320133209228516 and batch: 50, loss is 7.907852754592896 and perplexity is 2718.5468075547788
At time: 2.0328097343444824 and batch: 100, loss is 6.501353120803833 and perplexity is 666.0422592161184
At time: 2.9191994667053223 and batch: 150, loss is 6.285500106811523 and perplexity is 536.73264718126
At time: 3.8028676509857178 and batch: 200, loss is 6.113185424804687 and perplexity is 451.77552248733264
At time: 4.686878442764282 and batch: 250, loss is 6.168414478302002 and perplexity is 477.42853237585933
At time: 5.568672180175781 and batch: 300, loss is 6.061783781051636 and perplexity is 429.1402467704625
At time: 6.448758602142334 and batch: 350, loss is 6.070397939682007 and perplexity is 432.85289665064613
At time: 7.3303937911987305 and batch: 400, loss is 5.951782159805298 and perplexity is 384.43785861758016
At time: 8.20963978767395 and batch: 450, loss is 5.943875761032104 and perplexity is 381.4103238237319
At time: 9.123131513595581 and batch: 500, loss is 5.911062593460083 and perplexity is 369.09814838649083
At time: 10.021862030029297 and batch: 550, loss is 5.943010234832764 and perplexity is 381.0803460185813
At time: 10.920021057128906 and batch: 600, loss is 5.871574087142944 and perplexity is 354.80703813363453
At time: 11.82032585144043 and batch: 650, loss is 5.7806319236755375 and perplexity is 323.9638461714381
At time: 12.702337265014648 and batch: 700, loss is 5.884121866226196 and perplexity is 359.2871272685484
At time: 13.579424142837524 and batch: 750, loss is 5.820988121032715 and perplexity is 337.30518736481986
At time: 14.468714714050293 and batch: 800, loss is 5.818274755477905 and perplexity is 336.3911956455502
At time: 15.3549325466156 and batch: 850, loss is 5.852365760803223 and perplexity is 348.0568264372178
At time: 16.23752784729004 and batch: 900, loss is 5.718313980102539 and perplexity is 304.39128031750056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.734750408015839 and perplexity of 309.4357284426443
finished 1 epochs...
Completing Train Step...
At time: 18.16126799583435 and batch: 50, loss is 5.67619836807251 and perplexity is 291.8378582485397
At time: 18.915322303771973 and batch: 100, loss is 5.616166353225708 and perplexity is 274.83374555455526
At time: 19.656253814697266 and batch: 150, loss is 5.648391389846802 and perplexity is 283.834519308465
At time: 20.398016214370728 and batch: 200, loss is 5.560019083023072 and perplexity is 259.82779457544
At time: 21.138776540756226 and batch: 250, loss is 5.665815143585205 and perplexity is 288.82331766169636
At time: 21.879728078842163 and batch: 300, loss is 5.601551952362061 and perplexity is 270.84642216505193
At time: 22.621599912643433 and batch: 350, loss is 5.603224449157715 and perplexity is 271.2997909614849
At time: 23.363914012908936 and batch: 400, loss is 5.487309398651123 and perplexity is 241.6062653950946
At time: 24.107401371002197 and batch: 450, loss is 5.509551486968994 and perplexity is 247.04030142173713
At time: 24.850513458251953 and batch: 500, loss is 5.468213233947754 and perplexity is 237.03628577406636
At time: 25.593408346176147 and batch: 550, loss is 5.5280262470245365 and perplexity is 251.64673204453464
At time: 26.348712682724 and batch: 600, loss is 5.4623746490478515 and perplexity is 235.65636161741472
At time: 27.090054512023926 and batch: 650, loss is 5.368221874237061 and perplexity is 214.4811539718552
At time: 27.833364963531494 and batch: 700, loss is 5.48571738243103 and perplexity is 241.22193031672407
At time: 28.575955390930176 and batch: 750, loss is 5.4422433567047115 and perplexity is 230.95972777916379
At time: 29.340107679367065 and batch: 800, loss is 5.437080936431885 and perplexity is 229.77048891483923
At time: 30.08310341835022 and batch: 850, loss is 5.472733573913574 and perplexity is 238.1101957604741
At time: 30.825552463531494 and batch: 900, loss is 5.36656867980957 and perplexity is 214.12686785591376
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.451359997056935 and perplexity of 233.07493167970893
finished 2 epochs...
Completing Train Step...
At time: 32.671931743621826 and batch: 50, loss is 5.413717298507691 and perplexity is 224.4644400691233
At time: 33.413612842559814 and batch: 100, loss is 5.350253963470459 and perplexity is 210.66179147333168
At time: 34.1597158908844 and batch: 150, loss is 5.393985052108764 and perplexity is 220.0786653272244
At time: 34.9032244682312 and batch: 200, loss is 5.304558420181275 and perplexity is 201.2521139174758
At time: 35.644956827163696 and batch: 250, loss is 5.425135459899902 and perplexity is 227.04209933237976
At time: 36.38733196258545 and batch: 300, loss is 5.374879226684571 and perplexity is 215.91379411110879
At time: 37.128490686416626 and batch: 350, loss is 5.375555496215821 and perplexity is 216.05985941563344
At time: 37.8702175617218 and batch: 400, loss is 5.260164461135864 and perplexity is 192.51314962514817
At time: 38.60801076889038 and batch: 450, loss is 5.290989379882813 and perplexity is 198.53975951286006
At time: 39.34680128097534 and batch: 500, loss is 5.2485224151611325 and perplexity is 190.28489855469994
At time: 40.092052936553955 and batch: 550, loss is 5.321460294723511 and perplexity is 204.68256072382357
At time: 40.837074995040894 and batch: 600, loss is 5.258416614532471 and perplexity is 192.1769600599494
At time: 41.57568907737732 and batch: 650, loss is 5.164777088165283 and perplexity is 174.99844499921826
At time: 42.32359766960144 and batch: 700, loss is 5.281727590560913 and perplexity is 196.7094153034632
At time: 43.065598011016846 and batch: 750, loss is 5.256212863922119 and perplexity is 191.7539162796263
At time: 43.805426359176636 and batch: 800, loss is 5.2412244319915775 and perplexity is 188.9012576029353
At time: 44.541964292526245 and batch: 850, loss is 5.28031138420105 and perplexity is 196.43103134955663
At time: 45.2827365398407 and batch: 900, loss is 5.192481050491333 and perplexity is 179.91437629971276
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.287509029858733 and perplexity of 197.84997270210317
finished 3 epochs...
Completing Train Step...
At time: 47.13782095909119 and batch: 50, loss is 5.251360816955566 and perplexity is 190.82577079545146
At time: 47.8807430267334 and batch: 100, loss is 5.18057279586792 and perplexity is 177.78461612729834
At time: 48.63542580604553 and batch: 150, loss is 5.226824836730957 and perplexity is 186.20064646666603
At time: 49.37720966339111 and batch: 200, loss is 5.135443143844604 and perplexity is 169.93961088359484
At time: 50.11625266075134 and batch: 250, loss is 5.262956981658935 and perplexity is 193.05149787085557
At time: 50.85620713233948 and batch: 300, loss is 5.221914644241333 and perplexity is 185.28860642933202
At time: 51.5975296497345 and batch: 350, loss is 5.219653959274292 and perplexity is 184.87020038253883
At time: 52.338250160217285 and batch: 400, loss is 5.106704521179199 and perplexity is 165.12529034560444
At time: 53.07746601104736 and batch: 450, loss is 5.139106779098511 and perplexity is 170.56334951081203
At time: 53.816213607788086 and batch: 500, loss is 5.094621171951294 and perplexity is 163.14203013965678
At time: 54.56019926071167 and batch: 550, loss is 5.176853237152099 and perplexity is 177.12456412093107
At time: 55.30435085296631 and batch: 600, loss is 5.115133600234985 and perplexity is 166.52302701278754
At time: 56.04462647438049 and batch: 650, loss is 5.019094324111938 and perplexity is 151.27423630575518
At time: 56.78791666030884 and batch: 700, loss is 5.132322368621826 and perplexity is 169.41009423755256
At time: 57.53187966346741 and batch: 750, loss is 5.120492086410523 and perplexity is 167.41773334586725
At time: 58.27469873428345 and batch: 800, loss is 5.099165019989013 and perplexity is 163.8850094473813
At time: 59.01375365257263 and batch: 850, loss is 5.141257219314575 and perplexity is 170.93053045582295
At time: 59.756187200546265 and batch: 900, loss is 5.063981800079346 and perplexity is 158.21926119123498
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.165593395494435 and perplexity of 175.14135583410646
finished 4 epochs...
Completing Train Step...
At time: 61.59992742538452 and batch: 50, loss is 5.132443218231201 and perplexity is 169.430568618401
At time: 62.33682680130005 and batch: 100, loss is 5.053921785354614 and perplexity is 156.63555251664286
At time: 63.075998067855835 and batch: 150, loss is 5.098640108108521 and perplexity is 163.79900683275952
At time: 63.8113374710083 and batch: 200, loss is 5.0070252609252925 and perplexity is 149.45947127469898
At time: 64.54828572273254 and batch: 250, loss is 5.139444589614868 and perplexity is 170.62097733707674
At time: 65.28372287750244 and batch: 300, loss is 5.10505410194397 and perplexity is 164.85298915756422
At time: 66.02077436447144 and batch: 350, loss is 5.099313449859619 and perplexity is 163.9093366835274
At time: 66.75649833679199 and batch: 400, loss is 4.9909893417358395 and perplexity is 147.08186576616504
At time: 67.51506662368774 and batch: 450, loss is 5.023029499053955 and perplexity is 151.87069971405134
At time: 68.25036334991455 and batch: 500, loss is 4.974113550186157 and perplexity is 144.62056943360002
At time: 69.01326990127563 and batch: 550, loss is 5.062826375961304 and perplexity is 158.03655641199606
At time: 69.76964116096497 and batch: 600, loss is 5.004632301330567 and perplexity is 149.10224837932944
At time: 70.52417469024658 and batch: 650, loss is 4.904431772232056 and perplexity is 134.88624211817668
At time: 71.27435255050659 and batch: 700, loss is 5.013254528045654 and perplexity is 150.39340007091414
At time: 72.02608180046082 and batch: 750, loss is 5.012853965759278 and perplexity is 150.3331702104364
At time: 72.78256392478943 and batch: 800, loss is 4.987554359436035 and perplexity is 146.57750888502892
At time: 73.53625154495239 and batch: 850, loss is 5.031996364593506 and perplexity is 153.23862770532912
At time: 74.28872418403625 and batch: 900, loss is 4.9622336101531985 and perplexity is 142.9126508135129
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.072989267845676 and perplexity of 159.65085391982558
finished 5 epochs...
Completing Train Step...
At time: 76.14323949813843 and batch: 50, loss is 5.038335905075074 and perplexity is 154.21317601934675
At time: 76.90355348587036 and batch: 100, loss is 4.952867975234986 and perplexity is 141.58043137919364
At time: 77.65009260177612 and batch: 150, loss is 4.9951997661590575 and perplexity is 147.70244838732148
At time: 78.39599800109863 and batch: 200, loss is 4.9030045223236085 and perplexity is 134.69386306061588
At time: 79.14284086227417 and batch: 250, loss is 5.039710569381714 and perplexity is 154.42531314329403
At time: 79.89762163162231 and batch: 300, loss is 5.009344520568848 and perplexity is 149.8065088743648
At time: 80.65647101402283 and batch: 350, loss is 5.001877517700195 and perplexity is 148.69206918310852
At time: 81.41454696655273 and batch: 400, loss is 4.897746849060058 and perplexity is 133.98754515947894
At time: 82.16727495193481 and batch: 450, loss is 4.929609603881836 and perplexity is 138.3255000592881
At time: 82.91453552246094 and batch: 500, loss is 4.875085535049439 and perplexity is 130.98535657062183
At time: 83.66036057472229 and batch: 550, loss is 4.969217462539673 and perplexity is 143.91422502326043
At time: 84.41903877258301 and batch: 600, loss is 4.915509738922119 and perplexity is 136.38881477377694
At time: 85.15666794776917 and batch: 650, loss is 4.81065505027771 and perplexity is 122.81203923466211
At time: 85.90081787109375 and batch: 700, loss is 4.914825868606568 and perplexity is 136.29557439777645
At time: 86.68285059928894 and batch: 750, loss is 4.924459781646728 and perplexity is 137.6149794211802
At time: 87.43400526046753 and batch: 800, loss is 4.89651273727417 and perplexity is 133.82229154252744
At time: 88.18379211425781 and batch: 850, loss is 4.942835111618042 and perplexity is 140.16707607631756
At time: 88.92951464653015 and batch: 900, loss is 4.877957801818848 and perplexity is 131.36212228437483
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.999999163901969 and perplexity of 148.4130350146784
finished 6 epochs...
Completing Train Step...
At time: 90.74837064743042 and batch: 50, loss is 4.960279550552368 and perplexity is 142.633663643662
At time: 91.51509261131287 and batch: 100, loss is 4.868751382827758 and perplexity is 130.158297507528
At time: 92.26867580413818 and batch: 150, loss is 4.908984527587891 and perplexity is 135.50174623608441
At time: 93.02064776420593 and batch: 200, loss is 4.816438856124878 and perplexity is 123.52441837069765
At time: 93.7744779586792 and batch: 250, loss is 4.956840028762818 and perplexity is 142.1439147833977
At time: 94.52455043792725 and batch: 300, loss is 4.928105783462525 and perplexity is 138.11763967930193
At time: 95.26522421836853 and batch: 350, loss is 4.9204271793365475 and perplexity is 137.06115037392354
At time: 96.00987982749939 and batch: 400, loss is 4.81939287185669 and perplexity is 123.88985092685071
At time: 96.7602891921997 and batch: 450, loss is 4.85096173286438 and perplexity is 127.86330107146122
At time: 97.51226139068604 and batch: 500, loss is 4.791469268798828 and perplexity is 120.47825362460512
At time: 98.26378655433655 and batch: 550, loss is 4.889506549835205 and perplexity is 132.8879842722316
At time: 99.02811121940613 and batch: 600, loss is 4.840082426071167 and perplexity is 126.47977653153269
At time: 99.7850694656372 and batch: 650, loss is 4.731123514175415 and perplexity is 113.42292305135055
At time: 100.52865433692932 and batch: 700, loss is 4.830817947387695 and perplexity is 125.31341852980682
At time: 101.28138184547424 and batch: 750, loss is 4.849150018692017 and perplexity is 127.63185903350039
At time: 102.01760244369507 and batch: 800, loss is 4.819205150604248 and perplexity is 123.86659635162725
At time: 102.75961995124817 and batch: 850, loss is 4.867044048309326 and perplexity is 129.9362633505589
At time: 103.49658989906311 and batch: 900, loss is 4.805194501876831 and perplexity is 122.14324580148312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.939200205345676 and perplexity of 139.65850675317768
finished 7 epochs...
Completing Train Step...
At time: 105.3003249168396 and batch: 50, loss is 4.892612447738648 and perplexity is 133.30136240680844
At time: 106.04786038398743 and batch: 100, loss is 4.796568689346313 and perplexity is 121.09419203641758
At time: 106.79328870773315 and batch: 150, loss is 4.834874172210693 and perplexity is 125.82275021227464
At time: 107.52897095680237 and batch: 200, loss is 4.741820611953735 and perplexity is 114.64273172419924
At time: 108.2643494606018 and batch: 250, loss is 4.8854703617095945 and perplexity is 132.35270434000753
At time: 108.99936842918396 and batch: 300, loss is 4.857607221603393 and perplexity is 128.71584484635463
At time: 109.7340738773346 and batch: 350, loss is 4.85023458480835 and perplexity is 127.77035931595314
At time: 110.4689359664917 and batch: 400, loss is 4.751543769836426 and perplexity is 115.7628578618043
At time: 111.20479416847229 and batch: 450, loss is 4.783120956420898 and perplexity is 119.4766501949201
At time: 111.94113969802856 and batch: 500, loss is 4.719266204833985 and perplexity is 112.08597433763359
At time: 112.67597222328186 and batch: 550, loss is 4.820234174728394 and perplexity is 123.99412367053955
At time: 113.4118127822876 and batch: 600, loss is 4.774777526855469 and perplexity is 118.4839521909453
At time: 114.14949202537537 and batch: 650, loss is 4.662139272689819 and perplexity is 105.8623084358503
At time: 114.88686227798462 and batch: 700, loss is 4.758072757720948 and perplexity is 116.52114488799886
At time: 115.62505221366882 and batch: 750, loss is 4.783203716278076 and perplexity is 119.486538474597
At time: 116.36253213882446 and batch: 800, loss is 4.752236080169678 and perplexity is 115.84302943311033
At time: 117.10001993179321 and batch: 850, loss is 4.801622486114502 and perplexity is 121.70772650618851
At time: 117.83867049217224 and batch: 900, loss is 4.741710939407349 and perplexity is 114.63015925332635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.889529293530608 and perplexity of 132.8910066704388
finished 8 epochs...
Completing Train Step...
At time: 119.66330909729004 and batch: 50, loss is 4.8336653137207035 and perplexity is 125.670740210249
At time: 120.40118503570557 and batch: 100, loss is 4.733584566116333 and perplexity is 113.70240652688152
At time: 121.13821744918823 and batch: 150, loss is 4.77036563873291 and perplexity is 117.96236568625864
At time: 121.87515568733215 and batch: 200, loss is 4.676561365127563 and perplexity is 107.40012705994576
At time: 122.61346983909607 and batch: 250, loss is 4.82331992149353 and perplexity is 124.37732906989444
At time: 123.35078811645508 and batch: 300, loss is 4.795876064300537 and perplexity is 121.01034820563558
At time: 124.08792686462402 and batch: 350, loss is 4.788878211975097 and perplexity is 120.16649169429816
At time: 124.83724236488342 and batch: 400, loss is 4.692383909225464 and perplexity is 109.11298546048647
At time: 125.5746955871582 and batch: 450, loss is 4.723976345062256 and perplexity is 112.61516028604743
At time: 126.31162405014038 and batch: 500, loss is 4.6562032127380375 and perplexity is 105.23576486621884
At time: 127.04791831970215 and batch: 550, loss is 4.759217386245727 and perplexity is 116.65459467482377
At time: 127.7847204208374 and batch: 600, loss is 4.717494421005249 and perplexity is 111.88755804816657
At time: 128.52214097976685 and batch: 650, loss is 4.60223819732666 and perplexity is 99.70723054195663
At time: 129.25943732261658 and batch: 700, loss is 4.69417329788208 and perplexity is 109.30840578827464
At time: 129.99676871299744 and batch: 750, loss is 4.724918575286865 and perplexity is 112.72131969926805
At time: 130.7335946559906 and batch: 800, loss is 4.693422946929932 and perplexity is 109.22641688598587
At time: 131.47043871879578 and batch: 850, loss is 4.744461088180542 and perplexity is 114.94584313501974
At time: 132.20865201950073 and batch: 900, loss is 4.686412582397461 and perplexity is 108.46337760377456
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.84720810145548 and perplexity of 127.38424902339759
finished 9 epochs...
Completing Train Step...
At time: 134.03178596496582 and batch: 50, loss is 4.781369524002075 and perplexity is 119.2675780577801
At time: 134.7710897922516 and batch: 100, loss is 4.678038539886475 and perplexity is 107.55889305042994
At time: 135.50860834121704 and batch: 150, loss is 4.7133912754058835 and perplexity is 111.4294076783517
At time: 136.24601793289185 and batch: 200, loss is 4.61880316734314 and perplexity is 101.37263341887834
At time: 136.98306512832642 and batch: 250, loss is 4.768494901657104 and perplexity is 117.74189540048596
At time: 137.72137928009033 and batch: 300, loss is 4.741106491088868 and perplexity is 114.56089218260962
At time: 138.45922589302063 and batch: 350, loss is 4.734689521789551 and perplexity is 113.82811208277569
At time: 139.1959149837494 and batch: 400, loss is 4.64004056930542 and perplexity is 103.54854839075398
At time: 139.93325233459473 and batch: 450, loss is 4.671509704589844 and perplexity is 106.85894615831141
At time: 140.6705677509308 and batch: 500, loss is 4.6003961181640625 and perplexity is 99.52373099245166
At time: 141.4081048965454 and batch: 550, loss is 4.704849433898926 and perplexity is 110.48164890397555
At time: 142.14569854736328 and batch: 600, loss is 4.666575593948364 and perplexity is 106.3329909226479
At time: 142.88460540771484 and batch: 650, loss is 4.548935441970825 and perplexity is 94.53172022849674
At time: 143.63817048072815 and batch: 700, loss is 4.637004451751709 and perplexity is 103.23463959845533
At time: 144.37391471862793 and batch: 750, loss is 4.672968521118164 and perplexity is 107.01494751617953
At time: 145.10936284065247 and batch: 800, loss is 4.640937919616699 and perplexity is 103.64150941594944
At time: 145.84476399421692 and batch: 850, loss is 4.693507900238037 and perplexity is 109.23569642559107
At time: 146.58036160469055 and batch: 900, loss is 4.637097749710083 and perplexity is 103.24427162888075
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.810903888859161 and perplexity of 122.84260341090582
finished 10 epochs...
Completing Train Step...
At time: 148.384859085083 and batch: 50, loss is 4.73405571937561 and perplexity is 113.75599040842332
At time: 149.11962509155273 and batch: 100, loss is 4.628133010864258 and perplexity is 102.32285001939584
At time: 149.85660815238953 and batch: 150, loss is 4.662102746963501 and perplexity is 105.85844180876107
At time: 150.5929183959961 and batch: 200, loss is 4.567068452835083 and perplexity is 96.26150060499754
At time: 151.3291940689087 and batch: 250, loss is 4.719408311843872 and perplexity is 112.10190367210527
At time: 152.06546640396118 and batch: 300, loss is 4.69185133934021 and perplexity is 109.05489064148868
At time: 152.80125617980957 and batch: 350, loss is 4.6859582328796385 and perplexity is 108.41410851400006
At time: 153.5369255542755 and batch: 400, loss is 4.593004636764526 and perplexity is 98.79081518961084
At time: 154.27296829223633 and batch: 450, loss is 4.624274320602417 and perplexity is 101.92877862315187
At time: 155.0090663433075 and batch: 500, loss is 4.550227947235108 and perplexity is 94.65398196948509
At time: 155.74536538124084 and batch: 550, loss is 4.655796461105346 and perplexity is 105.19296875132686
At time: 156.48154258728027 and batch: 600, loss is 4.620689353942871 and perplexity is 101.56402156174589
At time: 157.2178773880005 and batch: 650, loss is 4.500738830566406 and perplexity is 90.08366328353168
At time: 157.9537501335144 and batch: 700, loss is 4.585325565338135 and perplexity is 98.03509877724818
At time: 158.689302444458 and batch: 750, loss is 4.62608383178711 and perplexity is 102.11338686307761
At time: 159.42510271072388 and batch: 800, loss is 4.593390655517578 and perplexity is 98.82895765828381
At time: 160.16089987754822 and batch: 850, loss is 4.647378053665161 and perplexity is 104.31112853362498
At time: 160.89638805389404 and batch: 900, loss is 4.592348661422729 and perplexity is 98.72603210122776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.779762163554152 and perplexity of 119.0760260578859
finished 11 epochs...
Completing Train Step...
At time: 162.70423793792725 and batch: 50, loss is 4.691079044342041 and perplexity is 108.97070060887552
At time: 163.45267271995544 and batch: 100, loss is 4.582765130996704 and perplexity is 97.78440742001085
At time: 164.1863055229187 and batch: 150, loss is 4.6154687881469725 and perplexity is 101.03518152787973
At time: 164.91937613487244 and batch: 200, loss is 4.520029907226562 and perplexity is 91.83834456726323
At time: 165.65248227119446 and batch: 250, loss is 4.674763126373291 and perplexity is 107.20716953310078
At time: 166.3911576271057 and batch: 300, loss is 4.6469714546203615 and perplexity is 104.26872434973579
At time: 167.12870049476624 and batch: 350, loss is 4.64147159576416 and perplexity is 103.69683517911851
At time: 167.86239743232727 and batch: 400, loss is 4.550072402954101 and perplexity is 94.63926022888622
At time: 168.59547591209412 and batch: 450, loss is 4.581397409439087 and perplexity is 97.65075699710053
At time: 169.32836270332336 and batch: 500, loss is 4.504515800476074 and perplexity is 90.42454992315335
At time: 170.06278562545776 and batch: 550, loss is 4.611026248931885 and perplexity is 100.58732432002431
At time: 170.79500246047974 and batch: 600, loss is 4.578799629211426 and perplexity is 97.39741100243982
At time: 171.530118227005 and batch: 650, loss is 4.456688718795776 and perplexity is 86.20159825988904
At time: 172.26383185386658 and batch: 700, loss is 4.538101558685303 and perplexity is 93.5131023614725
At time: 172.9975233078003 and batch: 750, loss is 4.583168697357178 and perplexity is 97.82387788136393
At time: 173.73074865341187 and batch: 800, loss is 4.549833011627197 and perplexity is 94.61660712238961
At time: 174.46458220481873 and batch: 850, loss is 4.605057544708252 and perplexity is 99.98873650639517
At time: 175.1976239681244 and batch: 900, loss is 4.551214933395386 and perplexity is 94.74745025807019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.752423012093322 and perplexity of 115.86468621755218
finished 12 epochs...
Completing Train Step...
At time: 176.98422741889954 and batch: 50, loss is 4.651331148147583 and perplexity is 104.72429638802305
At time: 177.73033142089844 and batch: 100, loss is 4.54101408958435 and perplexity is 93.78585917507728
At time: 178.46351408958435 and batch: 150, loss is 4.572568693161011 and perplexity is 96.79242074817265
At time: 179.19666504859924 and batch: 200, loss is 4.476853494644165 and perplexity is 87.95747813126337
At time: 179.92987728118896 and batch: 250, loss is 4.633582887649536 and perplexity is 102.88201926206023
At time: 180.67632269859314 and batch: 300, loss is 4.605757522583008 and perplexity is 100.05875091108274
At time: 181.409982919693 and batch: 350, loss is 4.600432825088501 and perplexity is 99.52738426957492
At time: 182.14426112174988 and batch: 400, loss is 4.510485439300537 and perplexity is 90.96596624916762
At time: 182.87856030464172 and batch: 450, loss is 4.542033720016479 and perplexity is 93.88153485982637
At time: 183.61127614974976 and batch: 500, loss is 4.462670679092407 and perplexity is 86.71879819062862
At time: 184.34471821784973 and batch: 550, loss is 4.5697705078125 and perplexity is 96.52195619603624
At time: 185.07883167266846 and batch: 600, loss is 4.540229148864746 and perplexity is 93.71227171996101
At time: 185.81266117095947 and batch: 650, loss is 4.416024389266968 and perplexity is 82.7665826933838
At time: 186.54621529579163 and batch: 700, loss is 4.494446840286255 and perplexity is 89.5186371831906
At time: 187.2800431251526 and batch: 750, loss is 4.543496713638306 and perplexity is 94.01898346523444
At time: 188.01354050636292 and batch: 800, loss is 4.509446611404419 and perplexity is 90.8715173324092
At time: 188.7463140487671 and batch: 850, loss is 4.56585431098938 and perplexity is 96.14469641176102
At time: 189.47950315475464 and batch: 900, loss is 4.5130761528015135 and perplexity is 91.20193854237846
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.728523985980308 and perplexity of 113.12845986340754
finished 13 epochs...
Completing Train Step...
At time: 191.26994705200195 and batch: 50, loss is 4.614404039382935 and perplexity is 100.92766169418645
At time: 192.0187509059906 and batch: 100, loss is 4.502304172515869 and perplexity is 90.22478544403367
At time: 192.75452136993408 and batch: 150, loss is 4.532833614349365 and perplexity is 93.02177582022924
At time: 193.49076080322266 and batch: 200, loss is 4.436806907653809 and perplexity is 84.50467917261192
At time: 194.22683358192444 and batch: 250, loss is 4.595252876281738 and perplexity is 99.01317046455634
At time: 194.96377158164978 and batch: 300, loss is 4.567598695755005 and perplexity is 96.31255611887055
At time: 195.699613571167 and batch: 350, loss is 4.562257070541381 and perplexity is 95.79946213859537
At time: 196.43580150604248 and batch: 400, loss is 4.473703861236572 and perplexity is 87.68088013948991
At time: 197.17202997207642 and batch: 450, loss is 4.505595607757568 and perplexity is 90.52224374634456
At time: 197.90781211853027 and batch: 500, loss is 4.423993186950684 and perplexity is 83.42876775106208
At time: 198.6436710357666 and batch: 550, loss is 4.53138801574707 and perplexity is 92.88740082067717
At time: 199.3925120830536 and batch: 600, loss is 4.504400796890259 and perplexity is 90.41415137361317
At time: 200.1286690235138 and batch: 650, loss is 4.378232831954956 and perplexity is 79.69707077268582
At time: 200.86533117294312 and batch: 700, loss is 4.453846073150634 and perplexity is 85.95690561400602
At time: 201.60224556922913 and batch: 750, loss is 4.506530275344849 and perplexity is 90.60689150609848
At time: 202.3372631072998 and batch: 800, loss is 4.4717257118225096 and perplexity is 87.5076056956902
At time: 203.07337832450867 and batch: 850, loss is 4.529252843856812 and perplexity is 92.6892818377953
At time: 203.80959391593933 and batch: 900, loss is 4.477454061508179 and perplexity is 88.01031834352331
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.7076478722977315 and perplexity of 110.79125800157311
finished 14 epochs...
Completing Train Step...
At time: 205.6089882850647 and batch: 50, loss is 4.579873704910279 and perplexity is 97.50207939555499
At time: 206.35614824295044 and batch: 100, loss is 4.466146869659424 and perplexity is 87.02077381720727
At time: 207.09113597869873 and batch: 150, loss is 4.495749359130859 and perplexity is 89.6353128647064
At time: 207.82688093185425 and batch: 200, loss is 4.3995102024078365 and perplexity is 81.41098399412164
At time: 208.5628936290741 and batch: 250, loss is 4.559292659759522 and perplexity is 95.51589369447218
At time: 209.29816889762878 and batch: 300, loss is 4.5320421314239505 and perplexity is 92.94817980181374
At time: 210.03475427627563 and batch: 350, loss is 4.526474533081054 and perplexity is 92.4321196109809
At time: 210.77031826972961 and batch: 400, loss is 4.43928316116333 and perplexity is 84.71419347919371
At time: 211.50506448745728 and batch: 450, loss is 4.471561994552612 and perplexity is 87.4932803620747
At time: 212.24019479751587 and batch: 500, loss is 4.3879186725616455 and perplexity is 80.47275440677721
At time: 212.97512936592102 and batch: 550, loss is 4.495425615310669 and perplexity is 89.60629868293007
At time: 213.7100338935852 and batch: 600, loss is 4.470813770294189 and perplexity is 87.4278402522506
At time: 214.44591641426086 and batch: 650, loss is 4.3427889919281 and perplexity is 76.9217746577542
At time: 215.1810917854309 and batch: 700, loss is 4.415780572891236 and perplexity is 82.74640530504854
At time: 215.91612553596497 and batch: 750, loss is 4.471851100921631 and perplexity is 87.51857888347936
At time: 216.6511914730072 and batch: 800, loss is 4.436354455947876 and perplexity is 84.46645353464058
At time: 217.38550567626953 and batch: 850, loss is 4.494849815368652 and perplexity is 89.5547182327793
At time: 218.13523530960083 and batch: 900, loss is 4.444032974243164 and perplexity is 85.11752718499302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.68953966114619 and perplexity of 108.8030819912602
finished 15 epochs...
Completing Train Step...
At time: 219.93829989433289 and batch: 50, loss is 4.547313051223755 and perplexity is 94.37847718395537
At time: 220.6731493473053 and batch: 100, loss is 4.432171964645386 and perplexity is 84.11391109415082
At time: 221.40769600868225 and batch: 150, loss is 4.460910120010376 and perplexity is 86.56625893947374
At time: 222.156888961792 and batch: 200, loss is 4.364579925537109 and perplexity is 78.61636827570511
At time: 222.8982446193695 and batch: 250, loss is 4.525332460403442 and perplexity is 92.32661567069087
At time: 223.63393425941467 and batch: 300, loss is 4.498644123077392 and perplexity is 89.89516185601536
At time: 224.36928987503052 and batch: 350, loss is 4.492744674682617 and perplexity is 89.3663912487487
At time: 225.1042046546936 and batch: 400, loss is 4.406914758682251 and perplexity is 82.01603350359125
At time: 225.8403720855713 and batch: 450, loss is 4.439550762176514 and perplexity is 84.73686611667482
At time: 226.58084917068481 and batch: 500, loss is 4.354046649932862 and perplexity is 77.79262636808107
At time: 227.31955790519714 and batch: 550, loss is 4.461458196640015 and perplexity is 86.61371688702172
At time: 228.06547164916992 and batch: 600, loss is 4.43908016204834 and perplexity is 84.69699831825056
At time: 228.80093002319336 and batch: 650, loss is 4.309385032653808 and perplexity is 74.39472454365968
At time: 229.53617000579834 and batch: 700, loss is 4.379933519363403 and perplexity is 79.83272589822428
At time: 230.27136206626892 and batch: 750, loss is 4.439150056838989 and perplexity is 84.70291840410583
At time: 231.0068597793579 and batch: 800, loss is 4.403009567260742 and perplexity is 81.69636977340839
At time: 231.7442591190338 and batch: 850, loss is 4.462414083480835 and perplexity is 86.69654938216742
At time: 232.48199796676636 and batch: 900, loss is 4.4125582885742185 and perplexity is 82.48020198267491
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6735902551102315 and perplexity of 107.0815030344798
finished 16 epochs...
Completing Train Step...
At time: 234.29123878479004 and batch: 50, loss is 4.516623458862305 and perplexity is 91.52603422516412
At time: 235.03185629844666 and batch: 100, loss is 4.400174255371094 and perplexity is 81.46506315301022
At time: 235.7662615776062 and batch: 150, loss is 4.428049755096436 and perplexity is 83.76788960317
At time: 236.5017066001892 and batch: 200, loss is 4.331772017478943 and perplexity is 76.07898047714795
At time: 237.2494764328003 and batch: 250, loss is 4.49309928894043 and perplexity is 89.39808746488768
At time: 237.98461961746216 and batch: 300, loss is 4.467078094482422 and perplexity is 87.10184746494019
At time: 238.72046732902527 and batch: 350, loss is 4.460777101516723 and perplexity is 86.55474479192272
At time: 239.45577836036682 and batch: 400, loss is 4.37631781578064 and perplexity is 79.54459563588456
At time: 240.19079399108887 and batch: 450, loss is 4.409219446182251 and perplexity is 82.20527281578188
At time: 240.92582964897156 and batch: 500, loss is 4.322053213119506 and perplexity is 75.34316516434693
At time: 241.66139030456543 and batch: 550, loss is 4.429215040206909 and perplexity is 83.86555997352238
At time: 242.39655184745789 and batch: 600, loss is 4.408955249786377 and perplexity is 82.18355734768261
At time: 243.13228130340576 and batch: 650, loss is 4.277758221626282 and perplexity is 72.07867434034306
At time: 243.86762619018555 and batch: 700, loss is 4.346038188934326 and perplexity is 77.17211514021596
At time: 244.60687589645386 and batch: 750, loss is 4.408149480819702 and perplexity is 82.11736305983382
At time: 245.3537721633911 and batch: 800, loss is 4.3714776134490965 and perplexity is 79.16051396496665
At time: 246.09182977676392 and batch: 850, loss is 4.431710996627808 and perplexity is 84.07514620667608
At time: 246.8277027606964 and batch: 900, loss is 4.382725563049316 and perplexity is 80.05593381462289
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.659573489672517 and perplexity of 105.59103688348478
finished 17 epochs...
Completing Train Step...
At time: 248.63098001480103 and batch: 50, loss is 4.487408399581909 and perplexity is 88.89077773119718
At time: 249.36696100234985 and batch: 100, loss is 4.36983332157135 and perplexity is 79.03045792916014
At time: 250.10349702835083 and batch: 150, loss is 4.396854829788208 and perplexity is 81.19509425698833
At time: 250.83915066719055 and batch: 200, loss is 4.3007748460769655 and perplexity is 73.75692182551097
At time: 251.57530426979065 and batch: 250, loss is 4.462419757843017 and perplexity is 86.69704133118435
At time: 252.31280827522278 and batch: 300, loss is 4.437057757377625 and perplexity is 84.525879807019
At time: 253.05089712142944 and batch: 350, loss is 4.4303904628753665 and perplexity is 83.96419541162669
At time: 253.78847241401672 and batch: 400, loss is 4.347324066162109 and perplexity is 77.27141283433421
At time: 254.52618384361267 and batch: 450, loss is 4.38035626411438 and perplexity is 79.8664818986372
At time: 255.2617893218994 and batch: 500, loss is 4.291688466072083 and perplexity is 73.08977397535578
At time: 256.0184829235077 and batch: 550, loss is 4.398502426147461 and perplexity is 81.32898126426102
At time: 256.75431394577026 and batch: 600, loss is 4.380292387008667 and perplexity is 79.86138042186553
At time: 257.4897141456604 and batch: 650, loss is 4.24764274597168 and perplexity is 69.94035070357059
At time: 258.22622990608215 and batch: 700, loss is 4.313829045295716 and perplexity is 74.726071349399
At time: 258.96252512931824 and batch: 750, loss is 4.37867998123169 and perplexity is 79.7327152288421
At time: 259.69788217544556 and batch: 800, loss is 4.341465978622437 and perplexity is 76.82007341726845
At time: 260.4343090057373 and batch: 850, loss is 4.402557573318481 and perplexity is 81.65945185313278
At time: 261.1710934638977 and batch: 900, loss is 4.354349403381348 and perplexity is 77.8161819195631
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.647075496307791 and perplexity of 104.27957320812028
finished 18 epochs...
Completing Train Step...
At time: 262.9622085094452 and batch: 50, loss is 4.459696273803711 and perplexity is 86.4612445629804
At time: 263.7083156108856 and batch: 100, loss is 4.341033444404602 and perplexity is 76.78685329183733
At time: 264.4416344165802 and batch: 150, loss is 4.367185010910034 and perplexity is 78.82143762237561
At time: 265.1755015850067 and batch: 200, loss is 4.271348190307617 and perplexity is 71.61812542384055
At time: 265.90870475769043 and batch: 250, loss is 4.433098011016845 and perplexity is 84.19184055388641
At time: 266.64291501045227 and batch: 300, loss is 4.408455181121826 and perplexity is 82.14247019996546
At time: 267.3765981197357 and batch: 350, loss is 4.401429042816162 and perplexity is 81.56734865132525
At time: 268.11053347587585 and batch: 400, loss is 4.319710302352905 and perplexity is 75.16684947810649
At time: 268.84325647354126 and batch: 450, loss is 4.3527685737609865 and perplexity is 77.69326497521214
At time: 269.5769729614258 and batch: 500, loss is 4.262733216285706 and perplexity is 71.00378718735367
At time: 270.311559677124 and batch: 550, loss is 4.369156398773193 and perplexity is 78.97697851319923
At time: 271.0451674461365 and batch: 600, loss is 4.352974367141724 and perplexity is 77.70925538017524
At time: 271.7792296409607 and batch: 650, loss is 4.218922533988953 and perplexity is 67.96021999946373
At time: 272.51292610168457 and batch: 700, loss is 4.283143825531006 and perplexity is 72.46790872061416
At time: 273.24606013298035 and batch: 750, loss is 4.350556607246399 and perplexity is 77.52160000325632
At time: 273.97984504699707 and batch: 800, loss is 4.31287760257721 and perplexity is 74.65500758484256
At time: 274.72640800476074 and batch: 850, loss is 4.374758224487305 and perplexity is 79.42063526598182
At time: 275.4599018096924 and batch: 900, loss is 4.327339406013489 and perplexity is 75.74249821450724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.636079135006422 and perplexity of 103.13915903943385
finished 19 epochs...
Completing Train Step...
At time: 277.24867725372314 and batch: 50, loss is 4.4331106758117675 and perplexity is 84.19290683303323
At time: 277.99395513534546 and batch: 100, loss is 4.313554501533508 and perplexity is 74.70555858858984
At time: 278.7276351451874 and batch: 150, loss is 4.338776626586914 and perplexity is 76.61375475256709
At time: 279.46073627471924 and batch: 200, loss is 4.243285751342773 and perplexity is 69.63628386102961
At time: 280.1934988498688 and batch: 250, loss is 4.404959449768066 and perplexity is 81.85582350323988
At time: 280.92678928375244 and batch: 300, loss is 4.381099519729614 and perplexity is 79.92586517552058
At time: 281.6595423221588 and batch: 350, loss is 4.373739690780639 and perplexity is 79.33978385389251
At time: 282.407479763031 and batch: 400, loss is 4.293337011337281 and perplexity is 73.2103651488704
At time: 283.14033341407776 and batch: 450, loss is 4.326297950744629 and perplexity is 75.66365685266712
At time: 283.87353467941284 and batch: 500, loss is 4.2350741958618165 and perplexity is 69.06680301438817
At time: 284.6071586608887 and batch: 550, loss is 4.341043000221252 and perplexity is 76.78758705643438
At time: 285.3403284549713 and batch: 600, loss is 4.326865835189819 and perplexity is 75.7066372692599
At time: 286.07353949546814 and batch: 650, loss is 4.19143807888031 and perplexity is 66.11780524435206
At time: 286.80679512023926 and batch: 700, loss is 4.25389066696167 and perplexity is 70.37870045010018
At time: 287.54013228416443 and batch: 750, loss is 4.323636412620544 and perplexity is 75.4625429002925
At time: 288.2731580734253 and batch: 800, loss is 4.285550470352173 and perplexity is 72.64252327115958
At time: 289.00655221939087 and batch: 850, loss is 4.348202133178711 and perplexity is 77.33929211018565
At time: 289.73998951911926 and batch: 900, loss is 4.301531939506531 and perplexity is 73.81278385012764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.6263799797998715 and perplexity of 102.14363201797369
finished 20 epochs...
Completing Train Step...
At time: 291.5315670967102 and batch: 50, loss is 4.407592859268188 and perplexity is 82.07166748455205
At time: 292.2800281047821 and batch: 100, loss is 4.287230343818664 and perplexity is 72.7646560736476
At time: 293.01604080200195 and batch: 150, loss is 4.311540641784668 and perplexity is 74.55526345858519
At time: 293.76446199417114 and batch: 200, loss is 4.216419410705567 and perplexity is 67.79031991957281
At time: 294.5002341270447 and batch: 250, loss is 4.378004627227783 and perplexity is 79.67888559944383
At time: 295.23646998405457 and batch: 300, loss is 4.354861507415771 and perplexity is 77.85604210567749
At time: 295.97288274765015 and batch: 350, loss is 4.347174873352051 and perplexity is 77.2598853550462
At time: 296.70926880836487 and batch: 400, loss is 4.268024539947509 and perplexity is 71.38048694784759
At time: 297.44583678245544 and batch: 450, loss is 4.300886583328247 and perplexity is 73.76516368167137
At time: 298.18163537979126 and batch: 500, loss is 4.20860221862793 and perplexity is 67.2624558631955
At time: 298.9195382595062 and batch: 550, loss is 4.314042086601257 and perplexity is 74.74199278509879
At time: 299.65976905822754 and batch: 600, loss is 4.3018230438232425 and perplexity is 73.83427419794967
At time: 300.40249824523926 and batch: 650, loss is 4.165071687698364 and perplexity is 64.39729877212328
At time: 301.14406299591064 and batch: 700, loss is 4.225777473449707 and perplexity is 68.4276835800565
At time: 301.8826823234558 and batch: 750, loss is 4.297828936576844 and perplexity is 73.5399603401621
At time: 302.6227333545685 and batch: 800, loss is 4.259362320899964 and perplexity is 70.76484380159003
At time: 303.3623344898224 and batch: 850, loss is 4.322725720405579 and perplexity is 75.3938510332735
At time: 304.12111616134644 and batch: 900, loss is 4.276826214790344 and perplexity is 72.0115278185017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.617962980923587 and perplexity of 101.28749727908794
finished 21 epochs...
Completing Train Step...
At time: 305.93748784065247 and batch: 50, loss is 4.383069133758545 and perplexity is 80.0834434140572
At time: 306.69580459594727 and batch: 100, loss is 4.261993579864502 and perplexity is 70.9512896172572
At time: 307.43483328819275 and batch: 150, loss is 4.285385904312133 and perplexity is 72.63056976236413
At time: 308.1789183616638 and batch: 200, loss is 4.190639863014221 and perplexity is 66.06505002100425
At time: 308.92056155204773 and batch: 250, loss is 4.352020883560181 and perplexity is 77.63519619375627
At time: 309.66109347343445 and batch: 300, loss is 4.32966655254364 and perplexity is 75.91896736155303
At time: 310.4088315963745 and batch: 350, loss is 4.3216614913940425 and perplexity is 75.3136573894824
At time: 311.15767550468445 and batch: 400, loss is 4.24368803024292 and perplexity is 69.66430270402836
At time: 311.8991525173187 and batch: 450, loss is 4.27640817642212 and perplexity is 71.98143052826843
At time: 312.66125321388245 and batch: 500, loss is 4.183166971206665 and perplexity is 65.57319313552199
At time: 313.4015243053436 and batch: 550, loss is 4.2881108665466305 and perplexity is 72.82875522335073
At time: 314.14234948158264 and batch: 600, loss is 4.277752895355224 and perplexity is 72.07829043080842
At time: 314.883220911026 and batch: 650, loss is 4.139676952362061 and perplexity is 62.78253642276795
At time: 315.6276738643646 and batch: 700, loss is 4.198821053504944 and perplexity is 66.60775775062976
At time: 316.37442088127136 and batch: 750, loss is 4.273025650978088 and perplexity is 71.73836283109837
At time: 317.12601351737976 and batch: 800, loss is 4.234128456115723 and perplexity is 69.00151467139861
At time: 317.8741829395294 and batch: 850, loss is 4.298220086097717 and perplexity is 73.56873108687928
At time: 318.621146440506 and batch: 900, loss is 4.2531089591979985 and perplexity is 70.32370637101093
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.610609080693493 and perplexity of 100.545371234489
finished 22 epochs...
Completing Train Step...
At time: 320.50881266593933 and batch: 50, loss is 4.359601020812988 and perplexity is 78.22591768304315
At time: 321.25526547431946 and batch: 100, loss is 4.237731585502624 and perplexity is 69.25058450260269
At time: 322.0023572444916 and batch: 150, loss is 4.260239424705506 and perplexity is 70.82693914343642
At time: 322.75008368492126 and batch: 200, loss is 4.1658762216567995 and perplexity is 64.44912943278453
At time: 323.4974527359009 and batch: 250, loss is 4.326998844146728 and perplexity is 75.7167075998214
At time: 324.2460126876831 and batch: 300, loss is 4.305443801879883 and perplexity is 74.10209480532755
At time: 324.9941704273224 and batch: 350, loss is 4.297058477401733 and perplexity is 73.48332262430638
At time: 325.7499670982361 and batch: 400, loss is 4.220159673690796 and perplexity is 68.04434831428028
At time: 326.49669432640076 and batch: 450, loss is 4.252796640396118 and perplexity is 70.30174638473018
At time: 327.2429835796356 and batch: 500, loss is 4.158713541030884 and perplexity is 63.989150210732824
At time: 327.99241733551025 and batch: 550, loss is 4.263119950294494 and perplexity is 71.03125207707295
At time: 328.7395143508911 and batch: 600, loss is 4.254655203819275 and perplexity is 70.43252813458706
At time: 329.4836382865906 and batch: 650, loss is 4.115248918533325 and perplexity is 61.26746297185359
At time: 330.2301323413849 and batch: 700, loss is 4.172865300178528 and perplexity is 64.9011472129439
At time: 330.9764564037323 and batch: 750, loss is 4.249155139923095 and perplexity is 70.0462080957968
At time: 331.7357075214386 and batch: 800, loss is 4.209828114509582 and perplexity is 67.34496319319321
At time: 332.4815745353699 and batch: 850, loss is 4.274546728134156 and perplexity is 71.84756544776228
At time: 333.2278800010681 and batch: 900, loss is 4.230264568328858 and perplexity is 68.73541498231799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.60450681921554 and perplexity of 99.93368532065408
finished 23 epochs...
Completing Train Step...
At time: 335.10512685775757 and batch: 50, loss is 4.337232599258423 and perplexity is 76.49555229888325
At time: 335.8641812801361 and batch: 100, loss is 4.214487752914429 and perplexity is 67.65949861161363
At time: 336.6108486652374 and batch: 150, loss is 4.236042041778564 and perplexity is 69.13368139643238
At time: 337.35634899139404 and batch: 200, loss is 4.141960072517395 and perplexity is 62.92604025298521
At time: 338.0994026660919 and batch: 250, loss is 4.302849521636963 and perplexity is 73.91010235360767
At time: 338.8501226902008 and batch: 300, loss is 4.2820320987701415 and perplexity is 72.3873889734575
At time: 339.59330797195435 and batch: 350, loss is 4.273246021270752 and perplexity is 71.75417357715605
At time: 340.3347747325897 and batch: 400, loss is 4.197407388687134 and perplexity is 66.51366323163026
At time: 341.080224275589 and batch: 450, loss is 4.230007882118225 and perplexity is 68.71777381332919
At time: 341.8221960067749 and batch: 500, loss is 4.13503933429718 and perplexity is 62.49204910295352
At time: 342.5695354938507 and batch: 550, loss is 4.239063668251037 and perplexity is 69.34289347948419
At time: 343.3109085559845 and batch: 600, loss is 4.232432770729065 and perplexity is 68.88460895698086
At time: 344.0558912754059 and batch: 650, loss is 4.0917111682891845 and perplexity is 59.84220422443372
At time: 344.7977247238159 and batch: 700, loss is 4.147825803756714 and perplexity is 63.296232154739066
At time: 345.5427088737488 and batch: 750, loss is 4.226038827896118 and perplexity is 68.44556979663706
At time: 346.28909254074097 and batch: 800, loss is 4.186451759338379 and perplexity is 65.7889413316586
At time: 347.036306142807 and batch: 850, loss is 4.25169988155365 and perplexity is 70.22468458956905
At time: 347.79279088974 and batch: 900, loss is 4.208151631355285 and perplexity is 67.23215508374217
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.598968505859375 and perplexity of 99.38175105974175
finished 24 epochs...
Completing Train Step...
At time: 349.66285943984985 and batch: 50, loss is 4.315375852584839 and perplexity is 74.84174762263704
At time: 350.41420125961304 and batch: 100, loss is 4.192051429748535 and perplexity is 66.15837109688249
At time: 351.17312836647034 and batch: 150, loss is 4.212724466323852 and perplexity is 67.54030064595786
At time: 351.91748571395874 and batch: 200, loss is 4.118907041549683 and perplexity is 61.49199732505685
At time: 352.6661009788513 and batch: 250, loss is 4.279553179740906 and perplexity is 72.20816872547238
At time: 353.4123058319092 and batch: 300, loss is 4.259467282295227 and perplexity is 70.77227176814908
At time: 354.1588501930237 and batch: 350, loss is 4.250209722518921 and perplexity is 70.12011657220492
At time: 354.90604996681213 and batch: 400, loss is 4.1753311395645145 and perplexity is 65.0613804915122
At time: 355.6609642505646 and batch: 450, loss is 4.207986187934876 and perplexity is 67.22103288611618
At time: 356.4091386795044 and batch: 500, loss is 4.11215986251831 and perplexity is 61.07849636132226
At time: 357.1575663089752 and batch: 550, loss is 4.215772132873536 and perplexity is 67.74645494620651
At time: 357.9052104949951 and batch: 600, loss is 4.21100462436676 and perplexity is 67.42424183312733
At time: 358.6530578136444 and batch: 650, loss is 4.068994822502137 and perplexity is 58.49813202328709
At time: 359.40125370025635 and batch: 700, loss is 4.123653602600098 and perplexity is 61.784566644244855
At time: 360.15008568763733 and batch: 750, loss is 4.203686413764953 and perplexity is 66.93261812875117
At time: 360.8975374698639 and batch: 800, loss is 4.163907260894775 and perplexity is 64.32235647223344
At time: 361.6446418762207 and batch: 850, loss is 4.229575438499451 and perplexity is 68.68806367499009
At time: 362.3917019367218 and batch: 900, loss is 4.1867376327514645 and perplexity is 65.80775132937141
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.594708586392337 and perplexity of 98.9592932606937
finished 25 epochs...
Completing Train Step...
At time: 364.23382687568665 and batch: 50, loss is 4.294247512817383 and perplexity is 73.27705365007965
At time: 364.99273920059204 and batch: 100, loss is 4.170425086021424 and perplexity is 64.74296758927304
At time: 365.73657178878784 and batch: 150, loss is 4.190249137878418 and perplexity is 66.03924178765402
At time: 366.48096799850464 and batch: 200, loss is 4.096538319587707 and perplexity is 60.13176992471183
At time: 367.2262501716614 and batch: 250, loss is 4.257011961936951 and perplexity is 70.59871632277577
At time: 367.9757900238037 and batch: 300, loss is 4.237671875953675 and perplexity is 69.24644970488222
At time: 368.7214334011078 and batch: 350, loss is 4.227895922660828 and perplexity is 68.5727978066443
At time: 369.465856552124 and batch: 400, loss is 4.153942079544067 and perplexity is 63.68455570329662
At time: 370.2273955345154 and batch: 450, loss is 4.186672530174255 and perplexity is 65.8034672146145
At time: 370.97309947013855 and batch: 500, loss is 4.089909534454346 and perplexity is 59.7344875466936
At time: 371.7194092273712 and batch: 550, loss is 4.193216691017151 and perplexity is 66.2355078178048
At time: 372.4651746749878 and batch: 600, loss is 4.190262622833252 and perplexity is 66.04013232985128
At time: 373.2111177444458 and batch: 650, loss is 4.0470431041717525 and perplexity is 57.227989416693305
At time: 373.95624685287476 and batch: 700, loss is 4.100415873527527 and perplexity is 60.3653867442907
At time: 374.70164489746094 and batch: 750, loss is 4.1820147562026975 and perplexity is 65.49768222928674
At time: 375.45285272598267 and batch: 800, loss is 4.142081379890442 and perplexity is 62.93367410863667
At time: 376.1990637779236 and batch: 850, loss is 4.208143491744995 and perplexity is 67.23160784242795
At time: 376.9428038597107 and batch: 900, loss is 4.165960841178894 and perplexity is 64.45458331806584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.591368374759203 and perplexity of 98.6292997090994
finished 26 epochs...
Completing Train Step...
At time: 378.77626729011536 and batch: 50, loss is 4.273790674209595 and perplexity is 71.79326534342543
At time: 379.5351574420929 and batch: 100, loss is 4.1494759798049925 and perplexity is 63.40076830879421
At time: 380.2791974544525 and batch: 150, loss is 4.16855899810791 and perplexity is 64.62226417658643
At time: 381.02410411834717 and batch: 200, loss is 4.074870467185974 and perplexity is 58.84285801368744
At time: 381.7666404247284 and batch: 250, loss is 4.235215358734131 and perplexity is 69.0765533708612
At time: 382.5125391483307 and batch: 300, loss is 4.216601767539978 and perplexity is 67.80268307493576
At time: 383.2569625377655 and batch: 350, loss is 4.2062045764923095 and perplexity is 67.10137774587884
At time: 384.0016851425171 and batch: 400, loss is 4.133224124908447 and perplexity is 62.37871584161999
At time: 384.7449550628662 and batch: 450, loss is 4.166007409095764 and perplexity is 64.45758490363195
At time: 385.4886200428009 and batch: 500, loss is 4.068278241157532 and perplexity is 58.45622836866663
At time: 386.23327136039734 and batch: 550, loss is 4.171365933418274 and perplexity is 64.80390950582009
At time: 386.97663927078247 and batch: 600, loss is 4.170164141654968 and perplexity is 64.72607548065916
At time: 387.7225115299225 and batch: 650, loss is 4.025812578201294 and perplexity is 56.02581564670114
At time: 388.46748781204224 and batch: 700, loss is 4.077974371910095 and perplexity is 59.02578438480785
At time: 389.23666644096375 and batch: 750, loss is 4.161066780090332 and perplexity is 64.13990929490035
At time: 389.99286937713623 and batch: 800, loss is 4.1208452701568605 and perplexity is 61.61129845246526
At time: 390.7411587238312 and batch: 850, loss is 4.187301526069641 and perplexity is 65.84487034522918
At time: 391.4849030971527 and batch: 900, loss is 4.14577953338623 and perplexity is 63.16684337767627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.588992602204623 and perplexity of 98.39525705191765
finished 27 epochs...
Completing Train Step...
At time: 393.32369351387024 and batch: 50, loss is 4.254043970108032 and perplexity is 70.38949055337334
At time: 394.0849907398224 and batch: 100, loss is 4.129159531593323 and perplexity is 62.125686309632805
At time: 394.83005809783936 and batch: 150, loss is 4.147607660293579 and perplexity is 63.28242600136924
At time: 395.5768084526062 and batch: 200, loss is 4.053869314193726 and perplexity is 57.6199760610467
At time: 396.32324051856995 and batch: 250, loss is 4.214116034507751 and perplexity is 67.63435300442518
At time: 397.069744348526 and batch: 300, loss is 4.196205835342408 and perplexity is 66.43379151177979
At time: 397.81705832481384 and batch: 350, loss is 4.18515869140625 and perplexity is 65.7039267380019
At time: 398.5639650821686 and batch: 400, loss is 4.113125586509705 and perplexity is 61.13750982138736
At time: 399.30941915512085 and batch: 450, loss is 4.14596851348877 and perplexity is 63.178781782239795
At time: 400.0564384460449 and batch: 500, loss is 4.047209911346435 and perplexity is 57.23753625213887
At time: 400.80273270606995 and batch: 550, loss is 4.150202322006225 and perplexity is 63.44683569072395
At time: 401.5489821434021 and batch: 600, loss is 4.15067198753357 and perplexity is 63.476641481095186
At time: 402.3014659881592 and batch: 650, loss is 4.005269255638122 and perplexity is 54.88660093640732
At time: 403.05143904685974 and batch: 700, loss is 4.056074237823486 and perplexity is 57.74716387599025
At time: 403.7983009815216 and batch: 750, loss is 4.140603613853455 and perplexity is 62.84074154566088
At time: 404.5458927154541 and batch: 800, loss is 4.100267615318298 and perplexity is 60.35643774355041
At time: 405.2929117679596 and batch: 850, loss is 4.1671213006973264 and perplexity is 64.52942366898395
At time: 406.0388340950012 and batch: 900, loss is 4.126145443916321 and perplexity is 61.93871595816564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.587332529564426 and perplexity of 98.23204928361723
finished 28 epochs...
Completing Train Step...
At time: 407.87664890289307 and batch: 50, loss is 4.234895539283753 and perplexity is 69.05446487788124
At time: 408.6407136917114 and batch: 100, loss is 4.109388699531555 and perplexity is 60.9094721978382
At time: 409.3869390487671 and batch: 150, loss is 4.127255711555481 and perplexity is 62.007522699986396
At time: 410.13374280929565 and batch: 200, loss is 4.033507585525513 and perplexity is 56.45859770346627
At time: 410.8808550834656 and batch: 250, loss is 4.193694744110108 and perplexity is 66.26717947694482
At time: 411.6305470466614 and batch: 300, loss is 4.176396489143372 and perplexity is 65.13073054028213
At time: 412.3795611858368 and batch: 350, loss is 4.164685125350952 and perplexity is 64.37241001198164
At time: 413.1258497238159 and batch: 400, loss is 4.0936403512954715 and perplexity is 59.95776221830147
At time: 413.872243642807 and batch: 450, loss is 4.126500434875489 and perplexity is 61.96070754552965
At time: 414.61753153800964 and batch: 500, loss is 4.026764707565308 and perplexity is 56.07918487409361
At time: 415.3624324798584 and batch: 550, loss is 4.12960687160492 and perplexity is 62.153483831875434
At time: 416.10848927497864 and batch: 600, loss is 4.1317521619796755 and perplexity is 62.28696422840971
At time: 416.85574889183044 and batch: 650, loss is 3.9853742837905886 and perplexity is 53.80542420772988
At time: 417.6022198200226 and batch: 700, loss is 4.0348115730285645 and perplexity is 56.532267030813685
At time: 418.34907126426697 and batch: 750, loss is 4.120732684135437 and perplexity is 61.60436227196355
At time: 419.09485602378845 and batch: 800, loss is 4.08028546333313 and perplexity is 59.16235612293638
At time: 419.84330439567566 and batch: 850, loss is 4.14751971244812 and perplexity is 63.276860693079044
At time: 420.5937349796295 and batch: 900, loss is 4.107052383422851 and perplexity is 60.767334520695464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.586267758722174 and perplexity of 98.12751032665865
finished 29 epochs...
Completing Train Step...
At time: 422.45095229148865 and batch: 50, loss is 4.216234192848206 and perplexity is 67.77776510449137
At time: 423.198438167572 and batch: 100, loss is 4.09014253616333 and perplexity is 59.748407405989774
At time: 423.94395089149475 and batch: 150, loss is 4.107531843185424 and perplexity is 60.79647699824137
At time: 424.69036197662354 and batch: 200, loss is 4.0137228631973265 and perplexity is 55.352557452844245
At time: 425.43684911727905 and batch: 250, loss is 4.173945908546448 and perplexity is 64.97131784236619
At time: 426.1822826862335 and batch: 300, loss is 4.1570964050292964 and perplexity is 63.88575467706873
At time: 426.92741298675537 and batch: 350, loss is 4.144774341583252 and perplexity is 63.103380486025905
At time: 427.6859667301178 and batch: 400, loss is 4.074696803092957 and perplexity is 58.83264000939586
At time: 428.4322016239166 and batch: 450, loss is 4.107505884170532 and perplexity is 60.79489880207387
At time: 429.3426854610443 and batch: 500, loss is 4.006928339004516 and perplexity is 54.97773796411718
At time: 430.08898544311523 and batch: 550, loss is 4.109544773101806 and perplexity is 60.91897929851048
At time: 430.83553194999695 and batch: 600, loss is 4.113349742889405 and perplexity is 61.15121572032773
At time: 431.58167147636414 and batch: 650, loss is 3.9660955667495728 and perplexity is 52.778059614563254
At time: 432.3267719745636 and batch: 700, loss is 4.014331998825074 and perplexity is 55.38628493893953
At time: 433.0729787349701 and batch: 750, loss is 4.101499719619751 and perplexity is 60.43084900191683
At time: 433.8182256221771 and batch: 800, loss is 4.0608543157577515 and perplexity is 58.02386060891728
At time: 434.56425309181213 and batch: 850, loss is 4.12845196723938 and perplexity is 62.081743936397665
At time: 435.3097867965698 and batch: 900, loss is 4.088462443351745 and perplexity is 59.64810881526682
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.585897785343536 and perplexity of 98.09121247515917
finished 30 epochs...
Completing Train Step...
At time: 437.15968322753906 and batch: 50, loss is 4.197993974685669 and perplexity is 66.55269066054585
At time: 437.9076774120331 and batch: 100, loss is 4.071351203918457 and perplexity is 58.63613846853057
At time: 438.65400528907776 and batch: 150, loss is 4.088410077095031 and perplexity is 59.64498534887093
At time: 439.40555691719055 and batch: 200, loss is 3.9945006227493285 and perplexity is 54.298718307579236
At time: 440.14916157722473 and batch: 250, loss is 4.154737954139709 and perplexity is 63.73526079808759
At time: 440.9103102684021 and batch: 300, loss is 4.138302712440491 and perplexity is 62.69631741121117
At time: 441.6642048358917 and batch: 350, loss is 4.125398173332214 and perplexity is 61.89244826710806
At time: 442.42751598358154 and batch: 400, loss is 4.056247477531433 and perplexity is 57.757168844398784
At time: 443.1731147766113 and batch: 450, loss is 4.088994426727295 and perplexity is 59.67984905943207
At time: 443.91598320007324 and batch: 500, loss is 3.9876652908325196 and perplexity is 53.92883412600118
At time: 444.69427704811096 and batch: 550, loss is 4.090052905082703 and perplexity is 59.743052331662376
At time: 445.4450776576996 and batch: 600, loss is 4.095327987670898 and perplexity is 60.059034550209546
At time: 446.19656705856323 and batch: 650, loss is 3.9473481512069704 and perplexity is 51.79782454625415
At time: 446.95648860931396 and batch: 700, loss is 3.9943414735794067 and perplexity is 54.29007739924771
At time: 447.70828580856323 and batch: 750, loss is 4.082571835517883 and perplexity is 59.29777804182144
At time: 448.46187257766724 and batch: 800, loss is 4.041980538368225 and perplexity is 56.93900108291619
At time: 449.2068660259247 and batch: 850, loss is 4.109910206794739 and perplexity is 60.941245214195334
At time: 449.95245385169983 and batch: 900, loss is 4.0702828550338745 and perplexity is 58.573528066233344
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.586337154858733 and perplexity of 98.13432023305334
Annealing...
finished 31 epochs...
Completing Train Step...
At time: 451.8049249649048 and batch: 50, loss is 4.1851389122009275 and perplexity is 65.7026271793966
At time: 452.5662348270416 and batch: 100, loss is 4.0558104944229125 and perplexity is 57.73193545090271
At time: 453.3143720626831 and batch: 150, loss is 4.0712891817092896 and perplexity is 58.63250183846277
At time: 454.06199383735657 and batch: 200, loss is 3.9706145191192626 and perplexity is 53.01710085332539
At time: 454.8064239025116 and batch: 250, loss is 4.130502495765686 and perplexity is 62.209174929094495
At time: 455.55126309394836 and batch: 300, loss is 4.109927339553833 and perplexity is 60.94228931481262
At time: 456.295871257782 and batch: 350, loss is 4.096703877449036 and perplexity is 60.14172603607199
At time: 457.0412817001343 and batch: 400, loss is 4.025016179084778 and perplexity is 55.98121449913263
At time: 457.7872838973999 and batch: 450, loss is 4.052871494293213 and perplexity is 57.56251037723533
At time: 458.533567905426 and batch: 500, loss is 3.950218997001648 and perplexity is 51.94674176994063
At time: 459.28001976013184 and batch: 550, loss is 4.048618836402893 and perplexity is 57.31823648807702
At time: 460.0246567726135 and batch: 600, loss is 4.053162803649903 and perplexity is 57.57928131775009
At time: 460.7949323654175 and batch: 650, loss is 3.9025326204299926 and perplexity is 49.52772532902615
At time: 461.5440182685852 and batch: 700, loss is 3.9436946296691895 and perplexity is 51.60892536240481
At time: 462.30146646499634 and batch: 750, loss is 4.03253701210022 and perplexity is 56.40382707266492
At time: 463.05906987190247 and batch: 800, loss is 3.9887101888656615 and perplexity is 53.985213709037126
At time: 463.80344796180725 and batch: 850, loss is 4.051784162521362 and perplexity is 57.49995484630603
At time: 464.55687403678894 and batch: 900, loss is 4.011208763122559 and perplexity is 55.21357037093793
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.567595181399828 and perplexity of 96.31221764293514
finished 32 epochs...
Completing Train Step...
At time: 466.3970003128052 and batch: 50, loss is 4.165415849685669 and perplexity is 64.41946568873198
At time: 467.1528322696686 and batch: 100, loss is 4.0378079605102535 and perplexity is 56.70191364462258
At time: 467.8943910598755 and batch: 150, loss is 4.055928344726563 and perplexity is 57.73873957795225
At time: 468.63650703430176 and batch: 200, loss is 3.9563431787490844 and perplexity is 52.265849196305844
At time: 469.3786675930023 and batch: 250, loss is 4.117445979118347 and perplexity is 61.402219279572925
At time: 470.1281576156616 and batch: 300, loss is 4.097680740356445 and perplexity is 60.20050496222633
At time: 470.8811490535736 and batch: 350, loss is 4.084991564750672 and perplexity is 59.44143634579312
At time: 471.634957075119 and batch: 400, loss is 4.014604940414428 and perplexity is 55.401404222826045
At time: 472.3793432712555 and batch: 450, loss is 4.043631796836853 and perplexity is 57.03309975988696
At time: 473.1291489601135 and batch: 500, loss is 3.941448211669922 and perplexity is 51.493120265574646
At time: 473.87358379364014 and batch: 550, loss is 4.040704221725464 and perplexity is 56.866375244853614
At time: 474.61564016342163 and batch: 600, loss is 4.046308836936951 and perplexity is 57.185984202564505
At time: 475.3575792312622 and batch: 650, loss is 3.8965556383132935 and perplexity is 49.23258191256399
At time: 476.09971475601196 and batch: 700, loss is 3.9379659938812255 and perplexity is 51.31412184281182
At time: 476.8404440879822 and batch: 750, loss is 4.028044562339783 and perplexity is 56.151004035848466
At time: 477.5825309753418 and batch: 800, loss is 3.985310730934143 and perplexity is 53.802004827986046
At time: 478.3227915763855 and batch: 850, loss is 4.049461379051208 and perplexity is 57.36654989704015
At time: 479.0642650127411 and batch: 900, loss is 4.010386619567871 and perplexity is 55.16819554479242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5683355462061215 and perplexity of 96.38355022210123
Annealing...
finished 33 epochs...
Completing Train Step...
At time: 480.938672542572 and batch: 50, loss is 4.162496948242188 and perplexity is 64.23170577699285
At time: 481.6799249649048 and batch: 100, loss is 4.036205382347107 and perplexity is 56.61111716966349
At time: 482.42313170433044 and batch: 150, loss is 4.054977340698242 and perplexity is 57.683855905456724
At time: 483.16672444343567 and batch: 200, loss is 3.952487196922302 and perplexity is 52.06470109259218
At time: 483.91847562789917 and batch: 250, loss is 4.114004216194153 and perplexity is 61.19125065803955
At time: 484.66114497184753 and batch: 300, loss is 4.093050513267517 and perplexity is 59.92240727794324
At time: 485.4046127796173 and batch: 350, loss is 4.079502582550049 and perplexity is 59.11605717688822
At time: 486.14727807044983 and batch: 400, loss is 4.009216113090515 and perplexity is 55.10365859239202
At time: 486.89090633392334 and batch: 450, loss is 4.036535005569458 and perplexity is 56.629780584304335
At time: 487.6311044692993 and batch: 500, loss is 3.9341436624526978 and perplexity is 51.11835664025554
At time: 488.3735694885254 and batch: 550, loss is 4.031637344360352 and perplexity is 56.353105188885
At time: 489.11658215522766 and batch: 600, loss is 4.036189341545105 and perplexity is 56.61020908922508
At time: 489.8569416999817 and batch: 650, loss is 3.8857735109329226 and perplexity is 48.704601434491515
At time: 490.5978181362152 and batch: 700, loss is 3.9255587673187256 and perplexity is 50.68138925239901
At time: 491.3397252559662 and batch: 750, loss is 4.01469612121582 and perplexity is 55.406455997570255
At time: 492.0797371864319 and batch: 800, loss is 3.9717188596725466 and perplexity is 53.07568212870064
At time: 492.8206412792206 and batch: 850, loss is 4.035177750587463 and perplexity is 56.55297166890961
At time: 493.56798934936523 and batch: 900, loss is 3.995457859039307 and perplexity is 54.350719896175676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.564026715004281 and perplexity of 95.96914321962873
finished 34 epochs...
Completing Train Step...
At time: 495.43099331855774 and batch: 50, loss is 4.156378169059753 and perplexity is 63.83988610431947
At time: 496.1755721569061 and batch: 100, loss is 4.029884123802185 and perplexity is 56.254392324348224
At time: 496.9249851703644 and batch: 150, loss is 4.04829309463501 and perplexity is 57.299568585014406
At time: 497.6692216396332 and batch: 200, loss is 3.947217698097229 and perplexity is 51.79106779969314
At time: 498.4425575733185 and batch: 250, loss is 4.108973951339721 and perplexity is 60.8842153423582
At time: 499.1972191333771 and batch: 300, loss is 4.088648772239685 and perplexity is 59.65922401655945
At time: 499.9480528831482 and batch: 350, loss is 4.075230207443237 and perplexity is 58.86402996654093
At time: 500.698459148407 and batch: 400, loss is 4.005398440361023 and perplexity is 54.89369190475276
At time: 501.4452681541443 and batch: 450, loss is 4.033245558738709 and perplexity is 56.44380597652154
At time: 502.18990898132324 and batch: 500, loss is 3.9310690689086916 and perplexity is 50.96142983763702
At time: 502.9341311454773 and batch: 550, loss is 4.029104561805725 and perplexity is 56.2105556268899
At time: 503.6884768009186 and batch: 600, loss is 4.034200825691223 and perplexity is 56.49775064072034
At time: 504.4423360824585 and batch: 650, loss is 3.8841775608062745 and perplexity is 48.62693331337309
At time: 505.18883419036865 and batch: 700, loss is 3.9243585300445556 and perplexity is 50.620596050343195
At time: 505.9305946826935 and batch: 750, loss is 4.0140977954864505 and perplexity is 55.37331480497652
At time: 506.6733491420746 and batch: 800, loss is 3.9715071868896485 and perplexity is 53.06444864031406
At time: 507.4159445762634 and batch: 850, loss is 4.035546617507935 and perplexity is 56.5738360372637
At time: 508.16452193260193 and batch: 900, loss is 3.996423397064209 and perplexity is 54.403222925673056
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.56404051062179 and perplexity of 95.97046718235367
Annealing...
finished 35 epochs...
Completing Train Step...
At time: 510.01719880104065 and batch: 50, loss is 4.155907158851623 and perplexity is 63.809823946625855
At time: 510.77439427375793 and batch: 100, loss is 4.030878496170044 and perplexity is 56.310357958374794
At time: 511.5185589790344 and batch: 150, loss is 4.049405674934388 and perplexity is 57.36335443304417
At time: 512.2633724212646 and batch: 200, loss is 3.9469559240341185 and perplexity is 51.7775120157961
At time: 513.008120059967 and batch: 250, loss is 4.109002223014832 and perplexity is 60.88593666544594
At time: 513.7526431083679 and batch: 300, loss is 4.0882389974594116 and perplexity is 59.63478217931305
At time: 514.4981479644775 and batch: 350, loss is 4.075509834289551 and perplexity is 58.880492231140074
At time: 515.2426066398621 and batch: 400, loss is 4.004165678024292 and perplexity is 54.82606272276739
At time: 515.9858677387238 and batch: 450, loss is 4.031884546279907 and perplexity is 56.36703750663751
At time: 516.732503414154 and batch: 500, loss is 3.929302110671997 and perplexity is 50.87146262699115
At time: 517.4887397289276 and batch: 550, loss is 4.026798043251038 and perplexity is 56.08105434333642
At time: 518.2296636104584 and batch: 600, loss is 4.032171688079834 and perplexity is 56.3832251632089
At time: 518.9731831550598 and batch: 650, loss is 3.8813147258758547 and perplexity is 48.48792150899204
At time: 519.7151010036469 and batch: 700, loss is 3.9215958070755006 and perplexity is 50.480938373493174
At time: 520.4580545425415 and batch: 750, loss is 4.010543727874756 and perplexity is 55.17686360748286
At time: 521.2007188796997 and batch: 800, loss is 3.9677455043792724 and perplexity is 52.865211999372264
At time: 521.9524040222168 and batch: 850, loss is 4.031553807258606 and perplexity is 56.348397810426285
At time: 522.7107787132263 and batch: 900, loss is 3.9921303272247313 and perplexity is 54.17016671140675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561709051262842 and perplexity of 95.74697656933522
finished 36 epochs...
Completing Train Step...
At time: 524.5458607673645 and batch: 50, loss is 4.153819403648376 and perplexity is 63.67674362257085
At time: 525.2999656200409 and batch: 100, loss is 4.0283572340011595 and perplexity is 56.16856360861564
At time: 526.039880990982 and batch: 150, loss is 4.046689095497132 and perplexity is 57.20773379755267
At time: 526.7834837436676 and batch: 200, loss is 3.9448615503311157 and perplexity is 51.66918403545746
At time: 527.5269920825958 and batch: 250, loss is 4.1071691942214965 and perplexity is 60.77443321616644
At time: 528.2717092037201 and batch: 300, loss is 4.0867122507095335 and perplexity is 59.543804437099304
At time: 529.024361371994 and batch: 350, loss is 4.073662538528442 and perplexity is 58.7718229504831
At time: 529.7805397510529 and batch: 400, loss is 4.00297101020813 and perplexity is 54.7606028993033
At time: 530.5379827022552 and batch: 450, loss is 4.030776557922363 and perplexity is 56.30461807172018
At time: 531.2895669937134 and batch: 500, loss is 3.9281996297836304 and perplexity is 50.81540871654175
At time: 532.0380837917328 and batch: 550, loss is 4.025908603668213 and perplexity is 56.031195810120316
At time: 532.7920322418213 and batch: 600, loss is 4.0315741491317745 and perplexity is 56.3495440540461
At time: 533.5460467338562 and batch: 650, loss is 3.8808514785766604 and perplexity is 48.46546481221311
At time: 534.3028450012207 and batch: 700, loss is 3.921282377243042 and perplexity is 50.465118620757146
At time: 535.0500566959381 and batch: 750, loss is 4.01046926021576 and perplexity is 55.17275486860522
At time: 535.8226749897003 and batch: 800, loss is 3.967987713813782 and perplexity is 52.87801800328072
At time: 536.5642795562744 and batch: 850, loss is 4.032149815559388 and perplexity is 56.38199193345065
At time: 537.319442987442 and batch: 900, loss is 3.9929045915603636 and perplexity is 54.21212498084635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561541831656678 and perplexity of 95.7309671362047
finished 37 epochs...
Completing Train Step...
At time: 539.2896649837494 and batch: 50, loss is 4.1528950214385985 and perplexity is 63.617909170539356
At time: 540.0441401004791 and batch: 100, loss is 4.027345418930054 and perplexity is 56.11176015157718
At time: 540.7902812957764 and batch: 150, loss is 4.045535335540771 and perplexity is 57.14176786684521
At time: 541.5389451980591 and batch: 200, loss is 3.9438131427764893 and perplexity is 51.615042058961116
At time: 542.2939171791077 and batch: 250, loss is 4.106151123046875 and perplexity is 60.71259200227509
At time: 543.0490498542786 and batch: 300, loss is 4.085765347480774 and perplexity is 59.487448902254975
At time: 543.7983086109161 and batch: 350, loss is 4.072707052230835 and perplexity is 58.715694098413316
At time: 544.5499670505524 and batch: 400, loss is 4.002193746566772 and perplexity is 54.71805601090755
At time: 545.3013889789581 and batch: 450, loss is 4.030076999664306 and perplexity is 56.265243485195725
At time: 546.0598809719086 and batch: 500, loss is 3.927562870979309 and perplexity is 50.783061857312966
At time: 546.8067820072174 and batch: 550, loss is 4.02538761138916 and perplexity is 56.00201159275325
At time: 547.5529911518097 and batch: 600, loss is 4.031177191734314 and perplexity is 56.327180124747585
At time: 548.298582315445 and batch: 650, loss is 3.880540475845337 and perplexity is 48.45039426389397
At time: 549.043728351593 and batch: 700, loss is 3.921065607070923 and perplexity is 50.454180473882545
At time: 549.788544178009 and batch: 750, loss is 4.0103877782821655 and perplexity is 55.168259469006216
At time: 550.5342080593109 and batch: 800, loss is 3.9680634355545044 and perplexity is 52.88202217044923
At time: 551.2782800197601 and batch: 850, loss is 4.032362594604492 and perplexity is 56.39399011629101
At time: 552.022821187973 and batch: 900, loss is 3.993221192359924 and perplexity is 54.22929130025299
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561513822372645 and perplexity of 95.72828581790654
finished 38 epochs...
Completing Train Step...
At time: 553.8837769031525 and batch: 50, loss is 4.152106747627259 and perplexity is 63.567780598923854
At time: 554.6271691322327 and batch: 100, loss is 4.026530046463012 and perplexity is 56.06602681465798
At time: 555.3901879787445 and batch: 150, loss is 4.044649200439453 and perplexity is 57.0911549688317
At time: 556.1422839164734 and batch: 200, loss is 3.9429944896698 and perplexity is 51.57280453572639
At time: 556.885112285614 and batch: 250, loss is 4.105335230827332 and perplexity is 60.66307727292011
At time: 557.6372017860413 and batch: 300, loss is 4.08500636100769 and perplexity is 59.442315863069624
At time: 558.3818492889404 and batch: 350, loss is 4.071954917907715 and perplexity is 58.671548613324475
At time: 559.1288578510284 and batch: 400, loss is 4.00154673576355 and perplexity is 54.6826642881853
At time: 559.8846905231476 and batch: 450, loss is 4.029503045082092 and perplexity is 56.23295905666211
At time: 560.6425077915192 and batch: 500, loss is 3.927032299041748 and perplexity is 50.756124936407105
At time: 561.3873071670532 and batch: 550, loss is 4.024939656257629 and perplexity is 55.97693082223357
At time: 562.1340610980988 and batch: 600, loss is 4.030819835662842 and perplexity is 56.30705486109786
At time: 562.881409406662 and batch: 650, loss is 3.880249948501587 and perplexity is 48.43632014410217
At time: 563.6299064159393 and batch: 700, loss is 3.9208388328552246 and perplexity is 50.4427400639209
At time: 564.3772985935211 and batch: 750, loss is 4.010266056060791 and perplexity is 55.161544674592434
At time: 565.1278736591339 and batch: 800, loss is 3.9680393981933593 and perplexity is 52.88075104146159
At time: 565.8729746341705 and batch: 850, loss is 4.0324298620224 and perplexity is 56.397783721983245
At time: 566.6176404953003 and batch: 900, loss is 3.9933692264556884 and perplexity is 54.23731967857676
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561527617990154 and perplexity of 95.72960645783196
Annealing...
finished 39 epochs...
Completing Train Step...
At time: 568.4742269515991 and batch: 50, loss is 4.152088828086853 and perplexity is 63.566641503716994
At time: 569.2368793487549 and batch: 100, loss is 4.0267999792099 and perplexity is 56.081162914055696
At time: 569.9891641139984 and batch: 150, loss is 4.045473284721375 and perplexity is 57.13822228333162
At time: 570.7341020107269 and batch: 200, loss is 3.9435292053222657 and perplexity is 51.60038869573702
At time: 571.4839975833893 and batch: 250, loss is 4.105151982307434 and perplexity is 60.65196187226886
At time: 572.2292075157166 and batch: 300, loss is 4.084852614402771 and perplexity is 59.43317751133041
At time: 572.9737973213196 and batch: 350, loss is 4.07265609741211 and perplexity is 58.712702327087435
At time: 573.7170760631561 and batch: 400, loss is 4.001109290122986 and perplexity is 54.65874882631663
At time: 574.4750964641571 and batch: 450, loss is 4.028829355239868 and perplexity is 56.19508824137155
At time: 575.2168641090393 and batch: 500, loss is 3.926186270713806 and perplexity is 50.713201976474586
At time: 575.9572803974152 and batch: 550, loss is 4.024548859596252 and perplexity is 55.955059498452606
At time: 576.6998202800751 and batch: 600, loss is 4.030771870613098 and perplexity is 56.30435415518075
At time: 577.4418501853943 and batch: 650, loss is 3.879453806877136 and perplexity is 48.39777331990188
At time: 578.184255361557 and batch: 700, loss is 3.919823212623596 and perplexity is 50.391535403219294
At time: 578.9260210990906 and batch: 750, loss is 4.009510707855225 and perplexity is 55.11989423307965
At time: 579.6680614948273 and batch: 800, loss is 3.966841435432434 and perplexity is 52.817439800770096
At time: 580.4106779098511 and batch: 850, loss is 4.03085009098053 and perplexity is 56.308758474702294
At time: 581.1525630950928 and batch: 900, loss is 3.9920460557937623 and perplexity is 54.165601906286184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561217843669734 and perplexity of 95.6999564766859
finished 40 epochs...
Completing Train Step...
At time: 583.0144534111023 and batch: 50, loss is 4.1516936492919925 and perplexity is 63.54152627776341
At time: 583.7710008621216 and batch: 100, loss is 4.026128249168396 and perplexity is 56.04350416183746
At time: 584.5137503147125 and batch: 150, loss is 4.044990649223328 and perplexity is 57.11065200269563
At time: 585.2577812671661 and batch: 200, loss is 3.943016219139099 and perplexity is 51.573925197574866
At time: 586.0013840198517 and batch: 250, loss is 4.104746050834656 and perplexity is 60.62734632850917
At time: 586.7508988380432 and batch: 300, loss is 4.08445463180542 and perplexity is 59.40952884716614
At time: 587.493043422699 and batch: 350, loss is 4.0721499586105345 and perplexity is 58.68299306943263
At time: 588.2351348400116 and batch: 400, loss is 4.000847573280335 and perplexity is 54.64444558293204
At time: 588.9783601760864 and batch: 450, loss is 4.028663935661316 and perplexity is 56.18579324236661
At time: 589.7194166183472 and batch: 500, loss is 3.9259347438812258 and perplexity is 50.700447849481264
At time: 590.4659576416016 and batch: 550, loss is 4.024313230514526 and perplexity is 55.94187641238528
At time: 591.2090089321136 and batch: 600, loss is 4.030611410140991 and perplexity is 56.29532025674253
At time: 591.9583933353424 and batch: 650, loss is 3.8794100141525267 and perplexity is 48.39565389595119
At time: 592.7302303314209 and batch: 700, loss is 3.91977276802063 and perplexity is 50.388993486336545
At time: 593.473254442215 and batch: 750, loss is 4.009555344581604 and perplexity is 55.122354659628854
At time: 594.2165677547455 and batch: 800, loss is 3.966897611618042 and perplexity is 52.82040696641291
At time: 594.9593272209167 and batch: 850, loss is 4.031041960716248 and perplexity is 56.31956345785093
At time: 595.7020103931427 and batch: 900, loss is 3.9922900724411012 and perplexity is 54.178820827617294
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561168931934931 and perplexity of 95.69527574026648
finished 41 epochs...
Completing Train Step...
At time: 597.5504658222198 and batch: 50, loss is 4.151435794830323 and perplexity is 63.52514392393367
At time: 598.2936763763428 and batch: 100, loss is 4.025797142982483 and perplexity is 56.02495088265147
At time: 599.042649269104 and batch: 150, loss is 4.044658842086792 and perplexity is 57.091705424267744
At time: 599.7922158241272 and batch: 200, loss is 3.9427290868759157 and perplexity is 51.559118785512666
At time: 600.5365903377533 and batch: 250, loss is 4.104458599090576 and perplexity is 60.609921396601635
At time: 601.2811644077301 and batch: 300, loss is 4.084194869995117 and perplexity is 59.394098524594604
At time: 602.0290215015411 and batch: 350, loss is 4.071858358383179 and perplexity is 58.66588358999695
At time: 602.772768497467 and batch: 400, loss is 4.000650744438172 and perplexity is 54.63369103841433
At time: 603.5164248943329 and batch: 450, loss is 4.028507323265075 and perplexity is 56.17699453966317
At time: 604.2620558738708 and batch: 500, loss is 3.9257754993438723 and perplexity is 50.69237472293757
At time: 605.0075762271881 and batch: 550, loss is 4.024159250259399 and perplexity is 55.93326313113766
At time: 605.7695603370667 and batch: 600, loss is 4.030513067245483 and perplexity is 56.28978428416118
At time: 606.524959564209 and batch: 650, loss is 3.879352107048035 and perplexity is 48.392851524903456
At time: 607.2690505981445 and batch: 700, loss is 3.9197323322296143 and perplexity is 50.38695600872023
At time: 608.0142121315002 and batch: 750, loss is 4.009562840461731 and perplexity is 55.122767851740335
At time: 608.7575688362122 and batch: 800, loss is 3.9669465589523316 and perplexity is 52.82299244780571
At time: 609.502683877945 and batch: 850, loss is 4.031166892051697 and perplexity is 56.326599975657274
At time: 610.2464425563812 and batch: 900, loss is 3.9924332094192505 and perplexity is 54.18657637534984
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561141340699915 and perplexity of 95.69263542584848
finished 42 epochs...
Completing Train Step...
At time: 612.108761548996 and batch: 50, loss is 4.151209297180176 and perplexity is 63.51075725744426
At time: 612.8514425754547 and batch: 100, loss is 4.025530395507812 and perplexity is 56.01000836151347
At time: 613.5936460494995 and batch: 150, loss is 4.044384560585022 and perplexity is 57.07604837288328
At time: 614.3415610790253 and batch: 200, loss is 3.942485032081604 and perplexity is 51.54653707075893
At time: 615.0881824493408 and batch: 250, loss is 4.104222664833069 and perplexity is 60.59562312659151
At time: 615.837164402008 and batch: 300, loss is 4.083972206115723 and perplexity is 59.38087507645077
At time: 616.5831878185272 and batch: 350, loss is 4.071629056930542 and perplexity is 58.65243295985298
At time: 617.3279840946198 and batch: 400, loss is 4.000474348068237 and perplexity is 54.62405470357124
At time: 618.0716683864594 and batch: 450, loss is 4.028357152938843 and perplexity is 56.16855905546193
At time: 618.8162443637848 and batch: 500, loss is 3.9256405878067016 and perplexity is 50.68553619804919
At time: 619.5579330921173 and batch: 550, loss is 4.024033699035645 and perplexity is 55.9262410823256
At time: 620.3139419555664 and batch: 600, loss is 4.030428700447082 and perplexity is 56.28503549560132
At time: 621.0610916614532 and batch: 650, loss is 3.879289059638977 and perplexity is 48.389800577176075
At time: 621.8075213432312 and batch: 700, loss is 3.9196903228759767 and perplexity is 50.384839329727015
At time: 622.5663528442383 and batch: 750, loss is 4.009552888870239 and perplexity is 55.122219295202264
At time: 623.3195633888245 and batch: 800, loss is 3.9669773960113526 and perplexity is 52.82462137865707
At time: 624.0735521316528 and batch: 850, loss is 4.031249294281006 and perplexity is 56.33124160430228
At time: 624.8139817714691 and batch: 900, loss is 3.992527713775635 and perplexity is 54.191697484854636
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561125872886344 and perplexity of 95.69115528145097
finished 43 epochs...
Completing Train Step...
At time: 626.6261456012726 and batch: 50, loss is 4.150998983383179 and perplexity is 63.49740147343732
At time: 627.3858149051666 and batch: 100, loss is 4.025298590660095 and perplexity is 55.997026474744885
At time: 628.1248600482941 and batch: 150, loss is 4.044143438339233 and perplexity is 57.062287726984465
At time: 628.8651170730591 and batch: 200, loss is 3.942265486717224 and perplexity is 51.535221509680156
At time: 629.6037075519562 and batch: 250, loss is 4.1040108776092525 and perplexity is 60.582791106671024
At time: 630.3425107002258 and batch: 300, loss is 4.083770594596863 and perplexity is 59.36890441478755
At time: 631.1037344932556 and batch: 350, loss is 4.071429042816162 and perplexity is 58.64070281855433
At time: 631.8449966907501 and batch: 400, loss is 4.000310778617859 and perplexity is 54.61512060765845
At time: 632.584929227829 and batch: 450, loss is 4.02821409702301 and perplexity is 56.1605243855225
At time: 633.3259191513062 and batch: 500, loss is 3.9255151319503785 and perplexity is 50.679177799559774
At time: 634.0665059089661 and batch: 550, loss is 4.023921041488648 and perplexity is 55.91994092407936
At time: 634.807471036911 and batch: 600, loss is 4.030348858833313 and perplexity is 56.28054178693115
At time: 635.547999382019 and batch: 650, loss is 3.8792233753204344 and perplexity is 48.38662223048566
At time: 636.2883150577545 and batch: 700, loss is 3.919644823074341 and perplexity is 50.38254688168543
At time: 637.0279941558838 and batch: 750, loss is 4.009534258842468 and perplexity is 55.12119237629178
At time: 637.7706406116486 and batch: 800, loss is 3.9669937658309937 and perplexity is 52.82548611525942
At time: 638.5145568847656 and batch: 850, loss is 4.031304178237915 and perplexity is 56.33433337058253
At time: 639.2583954334259 and batch: 900, loss is 3.9925940465927123 and perplexity is 54.19529229203654
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561118766053082 and perplexity of 95.69047522278224
finished 44 epochs...
Completing Train Step...
At time: 641.132351398468 and batch: 50, loss is 4.15080002784729 and perplexity is 63.4847695705348
At time: 641.8915348052979 and batch: 100, loss is 4.02508768081665 and perplexity is 55.985217396026954
At time: 642.6458978652954 and batch: 150, loss is 4.043923392295837 and perplexity is 57.049732777727115
At time: 643.3911385536194 and batch: 200, loss is 3.9420624017715453 and perplexity is 51.5247565446937
At time: 644.1362209320068 and batch: 250, loss is 4.103813376426697 and perplexity is 60.57082711527504
At time: 644.8779301643372 and batch: 300, loss is 4.083583068847656 and perplexity is 59.35777226032303
At time: 645.6205370426178 and batch: 350, loss is 4.07124502658844 and perplexity is 58.6299129704147
At time: 646.3658518791199 and batch: 400, loss is 4.000156321525574 and perplexity is 54.60668556637733
At time: 647.1086919307709 and batch: 450, loss is 4.028077163696289 and perplexity is 56.152834664589534
At time: 647.8522155284882 and batch: 500, loss is 3.925394606590271 and perplexity is 50.6730700414828
At time: 648.5968978404999 and batch: 550, loss is 4.023814659118653 and perplexity is 55.913992344651476
At time: 649.3644485473633 and batch: 600, loss is 4.03027039527893 and perplexity is 56.27612598882184
At time: 650.1098964214325 and batch: 650, loss is 3.8791563177108763 and perplexity is 48.38337764805249
At time: 650.8622028827667 and batch: 700, loss is 3.9195961141586304 and perplexity is 50.38009286222288
At time: 651.6095645427704 and batch: 750, loss is 4.009509782791138 and perplexity is 55.11984324366861
At time: 652.3581829071045 and batch: 800, loss is 3.9669996452331544 and perplexity is 52.82579669844965
At time: 653.103266954422 and batch: 850, loss is 4.031340889930725 and perplexity is 56.33640153728661
At time: 653.8584592342377 and batch: 900, loss is 3.9926420736312864 and perplexity is 54.197895193934315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561113331415882 and perplexity of 95.68995518117904
finished 45 epochs...
Completing Train Step...
At time: 655.7453324794769 and batch: 50, loss is 4.150609831809998 and perplexity is 63.472696167127225
At time: 656.4941306114197 and batch: 100, loss is 4.024890651702881 and perplexity is 55.97418776487415
At time: 657.2404172420502 and batch: 150, loss is 4.043717818260193 and perplexity is 57.038006039325346
At time: 657.9844913482666 and batch: 200, loss is 3.9418711709976195 and perplexity is 51.51490436767307
At time: 658.7295296192169 and batch: 250, loss is 4.1036260080337525 and perplexity is 60.55947911990018
At time: 659.4733211994171 and batch: 300, loss is 4.083405885696411 and perplexity is 59.3472559948631
At time: 660.2174518108368 and batch: 350, loss is 4.071071767807007 and perplexity is 58.61975570308122
At time: 660.9615786075592 and batch: 400, loss is 4.000008659362793 and perplexity is 54.59862282038022
At time: 661.7210648059845 and batch: 450, loss is 4.027945590019226 and perplexity is 56.14544691568251
At time: 662.4758670330048 and batch: 500, loss is 3.9252772092819215 and perplexity is 50.66712150863182
At time: 663.2257857322693 and batch: 550, loss is 4.023712167739868 and perplexity is 55.90826193614646
At time: 663.9710865020752 and batch: 600, loss is 4.03019235610962 and perplexity is 56.27173441805714
At time: 664.7169306278229 and batch: 650, loss is 3.8790883111953733 and perplexity is 48.38008737501165
At time: 665.4635779857635 and batch: 700, loss is 3.9195446729660035 and perplexity is 50.377501316818055
At time: 666.207676410675 and batch: 750, loss is 4.009481158256531 and perplexity is 55.11826548638952
At time: 666.9518744945526 and batch: 800, loss is 3.966997308731079 and perplexity is 52.825673271010224
At time: 667.6946256160736 and batch: 850, loss is 4.031364006996155 and perplexity is 56.337703884620204
At time: 668.4620535373688 and batch: 900, loss is 3.9926774597167967 and perplexity is 54.19981307922115
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561114585562928 and perplexity of 95.6900751905289
Annealing...
finished 46 epochs...
Completing Train Step...
At time: 670.3216853141785 and batch: 50, loss is 4.150619864463806 and perplexity is 63.47333296990855
At time: 671.0648131370544 and batch: 100, loss is 4.024885358810425 and perplexity is 55.97389150030205
At time: 671.8051953315735 and batch: 150, loss is 4.04402452468872 and perplexity is 57.05550264547162
At time: 672.5474803447723 and batch: 200, loss is 3.9422058391571047 and perplexity is 51.53214765113187
At time: 673.2907853126526 and batch: 250, loss is 4.103498907089233 and perplexity is 60.55178244204231
At time: 674.0345635414124 and batch: 300, loss is 4.083144855499268 and perplexity is 59.3317665906199
At time: 674.7775619029999 and batch: 350, loss is 4.071149563789367 and perplexity is 58.62431626195613
At time: 675.5203218460083 and batch: 400, loss is 3.9998351383209227 and perplexity is 54.58914963238624
At time: 676.2627251148224 and batch: 450, loss is 4.02768105506897 and perplexity is 56.130596446996165
At time: 677.0046000480652 and batch: 500, loss is 3.924971480369568 and perplexity is 50.65163347237178
At time: 677.7462892532349 and batch: 550, loss is 4.023597903251648 and perplexity is 55.90187397217474
At time: 678.4923593997955 and batch: 600, loss is 4.030386109352111 and perplexity is 56.28263830535893
At time: 679.2377614974976 and batch: 650, loss is 3.87881103515625 and perplexity is 48.36667459561898
At time: 679.9798953533173 and batch: 700, loss is 3.9191216039657593 and perplexity is 50.356192665533875
At time: 680.7224910259247 and batch: 750, loss is 4.009177527427673 and perplexity is 55.10153242222033
At time: 681.4647214412689 and batch: 800, loss is 3.966660294532776 and perplexity is 52.807873268678215
At time: 682.2084794044495 and batch: 850, loss is 4.030733661651611 and perplexity is 56.302202865380856
At time: 682.9512319564819 and batch: 900, loss is 3.9920436334609986 and perplexity is 54.165470699332936
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561141340699915 and perplexity of 95.69263542584848
Annealing...
finished 47 epochs...
Completing Train Step...
At time: 684.7942636013031 and batch: 50, loss is 4.150612435340881 and perplexity is 63.47286142046706
At time: 685.5471084117889 and batch: 100, loss is 4.024830417633057 and perplexity is 55.9708163132791
At time: 686.2871029376984 and batch: 150, loss is 4.0440730333328245 and perplexity is 57.05827039767305
At time: 687.0418393611908 and batch: 200, loss is 3.9422435331344605 and perplexity is 51.53409013934836
At time: 687.782393693924 and batch: 250, loss is 4.103494372367859 and perplexity is 60.55150785720279
At time: 688.5233652591705 and batch: 300, loss is 4.083050479888916 and perplexity is 59.32616738315315
At time: 689.2647454738617 and batch: 350, loss is 4.071068215370178 and perplexity is 58.61954746047203
At time: 690.0056054592133 and batch: 400, loss is 3.9997899293899537 and perplexity is 54.586681771073955
At time: 690.7502253055573 and batch: 450, loss is 4.027626099586487 and perplexity is 56.12751184774496
At time: 691.4916722774506 and batch: 500, loss is 3.9248599433898925 and perplexity is 50.6459842572127
At time: 692.2334156036377 and batch: 550, loss is 4.023504595756531 and perplexity is 55.896658151683404
At time: 692.9759614467621 and batch: 600, loss is 4.03038791179657 and perplexity is 56.28273975177988
At time: 693.7169291973114 and batch: 650, loss is 3.8787343549728392 and perplexity is 48.362965972330784
At time: 694.4592943191528 and batch: 700, loss is 3.9189971494674682 and perplexity is 50.34992600080524
At time: 695.2004497051239 and batch: 750, loss is 4.00907470703125 and perplexity is 55.095867152070774
At time: 695.9404284954071 and batch: 800, loss is 3.9665797662734987 and perplexity is 52.80362091378742
At time: 696.6826055049896 and batch: 850, loss is 4.030584254264832 and perplexity is 56.293791528754355
At time: 697.4241764545441 and batch: 900, loss is 3.9918775224685668 and perplexity is 54.15647396648833
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.56115262802333 and perplexity of 95.69371554566882
Annealing...
finished 48 epochs...
Completing Train Step...
At time: 699.2614052295685 and batch: 50, loss is 4.150606169700622 and perplexity is 63.4724637235971
At time: 700.0199806690216 and batch: 100, loss is 4.024815940856934 and perplexity is 55.970006042167
At time: 700.7638368606567 and batch: 150, loss is 4.04407479763031 and perplexity is 57.05837106552487
At time: 701.5081734657288 and batch: 200, loss is 3.942251958847046 and perplexity is 51.5345243526095
At time: 702.2536005973816 and batch: 250, loss is 4.103493766784668 and perplexity is 60.551471188238565
At time: 703.003279209137 and batch: 300, loss is 4.083028383255005 and perplexity is 59.32485648903436
At time: 703.7670934200287 and batch: 350, loss is 4.071046328544616 and perplexity is 58.61826447870251
At time: 704.5234689712524 and batch: 400, loss is 3.99978036403656 and perplexity is 54.58615963266944
At time: 705.2694439888 and batch: 450, loss is 4.027613229751587 and perplexity is 56.12678950058239
At time: 706.0388441085815 and batch: 500, loss is 3.924833574295044 and perplexity is 50.644648786057786
At time: 706.7865169048309 and batch: 550, loss is 4.02347957611084 and perplexity is 55.89525965459615
At time: 707.5308508872986 and batch: 600, loss is 4.030384702682495 and perplexity is 56.28255913433738
At time: 708.2853300571442 and batch: 650, loss is 3.8787155866622927 and perplexity is 48.36205828968432
At time: 709.0320744514465 and batch: 700, loss is 3.918966269493103 and perplexity is 50.34837122038696
At time: 709.7769751548767 and batch: 750, loss is 4.009047813415528 and perplexity is 55.09438544491604
At time: 710.5215578079224 and batch: 800, loss is 3.9665599918365477 and perplexity is 52.802576762238665
At time: 711.2705419063568 and batch: 850, loss is 4.030548448562622 and perplexity is 56.29177592610389
At time: 712.0219120979309 and batch: 900, loss is 3.991837086677551 and perplexity is 54.15428415089864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561155554366438 and perplexity of 95.69399557872356
Annealing...
finished 49 epochs...
Completing Train Step...
At time: 713.877361536026 and batch: 50, loss is 4.150604615211487 and perplexity is 63.472365056418525
At time: 714.6205408573151 and batch: 100, loss is 4.024812564849854 and perplexity is 55.969817087349284
At time: 715.3644869327545 and batch: 150, loss is 4.04407506942749 and perplexity is 57.05838657383129
At time: 716.1084716320038 and batch: 200, loss is 3.9422543621063233 and perplexity is 51.53464820358208
At time: 716.8674132823944 and batch: 250, loss is 4.103493885993958 and perplexity is 60.55147840653686
At time: 717.6180367469788 and batch: 300, loss is 4.083023185729981 and perplexity is 59.32454814740949
At time: 718.3699266910553 and batch: 350, loss is 4.071041116714477 and perplexity is 58.61795897106113
At time: 719.1158912181854 and batch: 400, loss is 3.9997780323028564 and perplexity is 54.586032352429676
At time: 719.8596193790436 and batch: 450, loss is 4.02761028289795 and perplexity is 56.12662410339229
At time: 720.6036019325256 and batch: 500, loss is 3.9248270988464355 and perplexity is 50.64432084029908
At time: 721.3470964431763 and batch: 550, loss is 4.023473281860351 and perplexity is 55.89490783693796
At time: 722.0926806926727 and batch: 600, loss is 4.030383901596069 and perplexity is 56.282514047161314
At time: 722.83917760849 and batch: 650, loss is 3.878711051940918 and perplexity is 48.361838981722116
At time: 723.5848429203033 and batch: 700, loss is 3.918958559036255 and perplexity is 50.34798301293993
At time: 724.3297379016876 and batch: 750, loss is 4.009041147232056 and perplexity is 55.09401817685853
At time: 725.0872550010681 and batch: 800, loss is 3.9665550899505617 and perplexity is 52.802317930661985
At time: 725.8300113677979 and batch: 850, loss is 4.030539488792419 and perplexity is 56.29127156698674
At time: 726.5742275714874 and batch: 900, loss is 3.991826696395874 and perplexity is 54.153721475555464
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561155972415453 and perplexity of 95.6940355835125
Annealing...
Model not improving. Stopping early with 95.68995518117904 lossat 49 epochs.
Finished Training.
langmodel



RESULTS:
[{'params': {'clip': 0.3191624920806612, 'rnn_dropout': 0.08296680426935077, 'anneal': 4.659714106128586, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 32.16017481632707, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.2679381198303693, 'seq_len': 35}, 'best_accuracy': -118.20504722677923}, {'params': {'clip': 0.1631625429098229, 'rnn_dropout': 0.36001066296425077, 'anneal': 4.533431404677296, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 23.639581435799197, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.3130703929951033, 'seq_len': 35}, 'best_accuracy': -78.33208706361773}, {'params': {'clip': 0.08412413531206164, 'rnn_dropout': 0.979596838791089, 'anneal': 4.6703586663889, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 22.165990174463836, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.40945704415214257, 'seq_len': 35}, 'best_accuracy': -78.31594462776523}, {'params': {'clip': 0.25372525675611113, 'rnn_dropout': 0.49930070350316336, 'anneal': 4.765827761833172, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 36.59443577805188, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.524166368804321, 'seq_len': 35}, 'best_accuracy': -111.21660648639725}, {'params': {'clip': 0.02016468994209586, 'rnn_dropout': 0.7966620185396394, 'anneal': 4.113211753950871, 'num_layers': 2, 'wordvec_dim': 300, 'batch_size': 32, 'lr': 7.996266918850972, 'wordvec_source': 'None', 'hidden_size': 300, 'tune_wordvecs': True, 'dropout': 0.2547448230514421, 'seq_len': 35}, 'best_accuracy': -95.68995518117904}]
Traceback (most recent call last):
  File "bayesian_optimization.py", line 226, in <module>
    fix_pretrained = args.fix_pretrained, savepath = args.savepath)
  File "bayesian_optimization.py", line 79, in __init__
    self.save()
  File "bayesian_optimization.py", line 89, in save
    self.best_model.save_checkpoint(savepath, name = name)
TypeError: save_checkpoint() got multiple values for argument 'name'
