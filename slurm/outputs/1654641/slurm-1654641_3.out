Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'domain': [0, 40], 'name': 'lr', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'dropout', 'type': 'continuous'}, {'domain': [4, 5], 'name': 'anneal', 'type': 'continuous'}, {'domain': [0, 1], 'name': 'rnn_dropout', 'type': 'continuous'}, {'domain': [0, 0.5], 'name': 'clip', 'type': 'continuous'}]
SETTINGS FOR THIS RUN
{'batch_size': 32, 'lr': 13.100449298335306, 'dropout': 0.9571356695129988, 'hidden_size': 300, 'clip': 0.2980817543502909, 'tune_wordvecs': True, 'anneal': 4.689255290552385, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.30874817919889586}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.08347996075948079 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.983672857284546 and batch: 50, loss is 7.100544204711914 and perplexity is 1212.6268121850987
At time: 3.292468547821045 and batch: 100, loss is 6.318164587020874 and perplexity is 554.5542217886521
At time: 4.589755058288574 and batch: 150, loss is 6.247766218185425 and perplexity is 516.8569884644417
At time: 5.882551193237305 and batch: 200, loss is 6.13976146697998 and perplexity is 463.942891960379
At time: 7.178239583969116 and batch: 250, loss is 6.205408544540405 and perplexity is 495.4213162747308
At time: 8.472445249557495 and batch: 300, loss is 6.1100336456298825 and perplexity is 450.35386735225404
At time: 9.769585609436035 and batch: 350, loss is 6.1363215827941895 and perplexity is 462.3497238710425
At time: 11.07272744178772 and batch: 400, loss is 6.0418504619598385 and perplexity is 420.670750319424
At time: 12.385656118392944 and batch: 450, loss is 6.035496063232422 and perplexity is 418.00611568054256
At time: 13.703248977661133 and batch: 500, loss is 6.036383686065673 and perplexity is 418.3773121701278
At time: 15.015229225158691 and batch: 550, loss is 6.06254825592041 and perplexity is 429.46843913572206
At time: 16.321688652038574 and batch: 600, loss is 6.016642255783081 and perplexity is 410.19893764277293
At time: 17.629263401031494 and batch: 650, loss is 5.9552410697937015 and perplexity is 385.76989693895206
At time: 18.938444137573242 and batch: 700, loss is 6.0656138610839845 and perplexity is 430.78703992234205
At time: 20.256745100021362 and batch: 750, loss is 6.010386791229248 and perplexity is 407.6409617334498
At time: 21.628931999206543 and batch: 800, loss is 6.028612184524536 and perplexity is 415.1384937858734
At time: 23.030012845993042 and batch: 850, loss is 6.064780015945434 and perplexity is 430.427979964357
At time: 24.447195529937744 and batch: 900, loss is 5.942668981552124 and perplexity is 380.9503232869188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.963971281704837 and perplexity of 389.15249379771757
finished 1 epochs...
Completing Train Step...
At time: 27.695110321044922 and batch: 50, loss is 5.839216175079346 and perplexity is 343.5099834287952
At time: 28.721271276474 and batch: 100, loss is 5.58056230545044 and perplexity is 265.2206989089266
At time: 29.729533433914185 and batch: 150, loss is 5.444663095474243 and perplexity is 231.5192666826457
At time: 30.74704599380493 and batch: 200, loss is 5.238119831085205 and perplexity is 188.31570401279345
At time: 31.760634899139404 and batch: 250, loss is 5.2867168426513675 and perplexity is 197.69330054978175
At time: 32.76847243309021 and batch: 300, loss is 5.208339900970459 and perplexity is 182.79035608293415
At time: 33.76750469207764 and batch: 350, loss is 5.16992036819458 and perplexity is 175.90082962569923
At time: 34.76572275161743 and batch: 400, loss is 5.012910242080689 and perplexity is 150.34163064630079
At time: 35.78199648857117 and batch: 450, loss is 5.010502452850342 and perplexity is 149.98007513656407
At time: 36.80967903137207 and batch: 500, loss is 4.931151313781738 and perplexity is 138.53892232747717
At time: 37.84071230888367 and batch: 550, loss is 4.992360944747925 and perplexity is 147.2837421117258
At time: 38.85883593559265 and batch: 600, loss is 4.910735445022583 and perplexity is 135.73920643193352
At time: 39.872191190719604 and batch: 650, loss is 4.795176362991333 and perplexity is 120.92570672190054
At time: 40.95168375968933 and batch: 700, loss is 4.84130862236023 and perplexity is 126.63496068783147
At time: 41.97546315193176 and batch: 750, loss is 4.863048620223999 and perplexity is 129.41814809003918
At time: 42.999918699264526 and batch: 800, loss is 4.784597101211548 and perplexity is 119.65314526379927
At time: 44.023316621780396 and batch: 850, loss is 4.841620845794678 and perplexity is 126.67450526322892
At time: 45.04282879829407 and batch: 900, loss is 4.773510684967041 and perplexity is 118.33394679382546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.862037554179152 and perplexity of 129.28736392177544
finished 2 epochs...
Completing Train Step...
At time: 47.5118134021759 and batch: 50, loss is 4.840135488510132 and perplexity is 126.48648803501806
At time: 48.528440713882446 and batch: 100, loss is 4.69828351020813 and perplexity is 109.75861113105293
At time: 49.5402626991272 and batch: 150, loss is 4.701793117523193 and perplexity is 110.14449751431523
At time: 50.552948236465454 and batch: 200, loss is 4.598886728286743 and perplexity is 99.3736241936784
At time: 51.5681369304657 and batch: 250, loss is 4.746714773178101 and perplexity is 115.20518698712988
At time: 52.57686734199524 and batch: 300, loss is 4.7063890552520755 and perplexity is 110.65187982171048
At time: 53.58675670623779 and batch: 350, loss is 4.6844157791137695 and perplexity is 108.24701366521752
At time: 54.60378074645996 and batch: 400, loss is 4.584514999389649 and perplexity is 97.95566706109408
At time: 55.62268805503845 and batch: 450, loss is 4.607244262695312 and perplexity is 100.20762290921279
At time: 56.64369034767151 and batch: 500, loss is 4.518427639007569 and perplexity is 91.69131273015378
At time: 57.65472960472107 and batch: 550, loss is 4.586662187576294 and perplexity is 98.16622228216863
At time: 58.66615295410156 and batch: 600, loss is 4.55148118019104 and perplexity is 94.77267982159398
At time: 59.681658029556274 and batch: 650, loss is 4.419647731781006 and perplexity is 83.06701833311963
At time: 60.69299292564392 and batch: 700, loss is 4.444536638259888 and perplexity is 85.16040861863964
At time: 61.69716501235962 and batch: 750, loss is 4.532592430114746 and perplexity is 92.99934313973827
At time: 62.71066689491272 and batch: 800, loss is 4.472707662582398 and perplexity is 87.59357605802246
At time: 63.74030780792236 and batch: 850, loss is 4.548962326049804 and perplexity is 94.53426166089126
At time: 64.76086044311523 and batch: 900, loss is 4.475083637237549 and perplexity is 87.80194361468317
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.659285871949915 and perplexity of 105.56067139696451
finished 3 epochs...
Completing Train Step...
At time: 67.25223278999329 and batch: 50, loss is 4.577549238204956 and perplexity is 97.2757022632827
At time: 68.27249956130981 and batch: 100, loss is 4.436703605651855 and perplexity is 84.49595012095097
At time: 69.28802537918091 and batch: 150, loss is 4.444887409210205 and perplexity is 85.1902856557922
At time: 70.30152654647827 and batch: 200, loss is 4.33851845741272 and perplexity is 76.59397799575503
At time: 71.31270861625671 and batch: 250, loss is 4.500076684951782 and perplexity is 90.0240345245785
At time: 72.33615779876709 and batch: 300, loss is 4.476135592460633 and perplexity is 87.8943559261504
At time: 73.35178756713867 and batch: 350, loss is 4.456987247467041 and perplexity is 86.2273357499767
At time: 74.37015128135681 and batch: 400, loss is 4.365390334129334 and perplexity is 78.68010547914554
At time: 75.38139772415161 and batch: 450, loss is 4.406882925033569 and perplexity is 82.01342267555076
At time: 76.38910794258118 and batch: 500, loss is 4.295133724212646 and perplexity is 73.34202139336091
At time: 77.40506649017334 and batch: 550, loss is 4.365745210647583 and perplexity is 78.70803215600134
At time: 78.42048645019531 and batch: 600, loss is 4.360306224822998 and perplexity is 78.28110236982413
At time: 79.43630194664001 and batch: 650, loss is 4.215939273834229 and perplexity is 67.7577791001084
At time: 80.44574785232544 and batch: 700, loss is 4.222269325256348 and perplexity is 68.18804970686556
At time: 81.458425283432 and batch: 750, loss is 4.342829031944275 and perplexity is 76.92485466851711
At time: 82.48071050643921 and batch: 800, loss is 4.2800819253921505 and perplexity is 72.24635857613931
At time: 83.49599432945251 and batch: 850, loss is 4.3640594387054445 and perplexity is 78.57546013826126
At time: 84.51224732398987 and batch: 900, loss is 4.3040736865997316 and perplexity is 74.0006359140039
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.561642581469392 and perplexity of 95.74061249909069
finished 4 epochs...
Completing Train Step...
At time: 86.93628001213074 and batch: 50, loss is 4.411570911407471 and perplexity is 82.39880310683724
At time: 87.95663380622864 and batch: 100, loss is 4.271149301528931 and perplexity is 71.6038827987393
At time: 88.97050547599792 and batch: 150, loss is 4.27097870349884 and perplexity is 71.59166835929625
At time: 89.98463106155396 and batch: 200, loss is 4.1667787551879885 and perplexity is 64.50732319011198
At time: 91.00413084030151 and batch: 250, loss is 4.329847383499145 and perplexity is 75.93269710230567
At time: 92.019535779953 and batch: 300, loss is 4.318063969612122 and perplexity is 75.04320164345644
At time: 93.028733253479 and batch: 350, loss is 4.3015890789031985 and perplexity is 73.81700158856157
At time: 94.0378987789154 and batch: 400, loss is 4.218068819046021 and perplexity is 67.90222610277567
At time: 95.05538392066956 and batch: 450, loss is 4.25640266418457 and perplexity is 70.55571378560433
At time: 96.06811809539795 and batch: 500, loss is 4.137545962333679 and perplexity is 62.64888991395867
At time: 97.09008383750916 and batch: 550, loss is 4.212899198532105 and perplexity is 67.55210314294392
At time: 98.10170483589172 and batch: 600, loss is 4.217192645072937 and perplexity is 67.84275799555823
At time: 99.11513495445251 and batch: 650, loss is 4.08035641670227 and perplexity is 59.166554040356
At time: 100.12684392929077 and batch: 700, loss is 4.081176500320435 and perplexity is 59.21509546336017
At time: 101.13686347007751 and batch: 750, loss is 4.20255871295929 and perplexity is 66.85718070477864
At time: 102.14325857162476 and batch: 800, loss is 4.1485431146621705 and perplexity is 63.34165152029362
At time: 103.16438269615173 and batch: 850, loss is 4.224707870483399 and perplexity is 68.3545322551328
At time: 104.17959499359131 and batch: 900, loss is 4.180943760871887 and perplexity is 65.42757206796954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.508669500481592 and perplexity of 90.80092751532773
finished 5 epochs...
Completing Train Step...
At time: 106.60208797454834 and batch: 50, loss is 4.282860102653504 and perplexity is 72.44735083353471
At time: 107.61354231834412 and batch: 100, loss is 4.1511410093307495 and perplexity is 63.50642039249496
At time: 108.61001563072205 and batch: 150, loss is 4.1527126502990725 and perplexity is 63.60630815782678
At time: 109.60805535316467 and batch: 200, loss is 4.048045401573181 and perplexity is 57.285377637002334
At time: 110.60300874710083 and batch: 250, loss is 4.218448348045349 and perplexity is 67.92800185771448
At time: 111.60183191299438 and batch: 300, loss is 4.197571244239807 and perplexity is 66.52456275762891
At time: 112.60044407844543 and batch: 350, loss is 4.180908193588257 and perplexity is 65.42524502834004
At time: 113.5965883731842 and batch: 400, loss is 4.101250948905945 and perplexity is 60.41581744625768
At time: 114.59048533439636 and batch: 450, loss is 4.134550461769104 and perplexity is 62.461505923392906
At time: 115.59224343299866 and batch: 500, loss is 4.0225807428359985 and perplexity is 55.84504170747456
At time: 116.61146640777588 and batch: 550, loss is 4.096228332519531 and perplexity is 60.11313274244061
At time: 117.61439418792725 and batch: 600, loss is 4.099228324890137 and perplexity is 60.293742460472664
At time: 118.60955762863159 and batch: 650, loss is 3.970522952079773 and perplexity is 53.012246456612694
At time: 119.61887669563293 and batch: 700, loss is 3.963822965621948 and perplexity is 52.65825232542377
At time: 120.62582802772522 and batch: 750, loss is 4.091859469413757 and perplexity is 59.851079548711134
At time: 121.63067722320557 and batch: 800, loss is 4.04202287197113 and perplexity is 56.94141156699972
At time: 122.63582921028137 and batch: 850, loss is 4.109683423042298 and perplexity is 60.927426296939394
At time: 123.63824677467346 and batch: 900, loss is 4.073298487663269 and perplexity is 58.750430911621585
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.467623357903467 and perplexity of 87.14935386684292
finished 6 epochs...
Completing Train Step...
At time: 126.02505087852478 and batch: 50, loss is 4.175674939155579 and perplexity is 65.08375241302764
At time: 127.02736020088196 and batch: 100, loss is 4.045900225639343 and perplexity is 57.16262213668162
At time: 128.01965475082397 and batch: 150, loss is 4.0381111764907835 and perplexity is 56.719109177814744
At time: 129.01949095726013 and batch: 200, loss is 3.954620714187622 and perplexity is 52.175900612153676
At time: 130.01598978042603 and batch: 250, loss is 4.109443678855896 and perplexity is 60.91282105152357
At time: 131.02923941612244 and batch: 300, loss is 4.107080688476563 and perplexity is 60.76905456770587
At time: 132.03795456886292 and batch: 350, loss is 4.081351108551026 and perplexity is 59.22543580913175
At time: 133.05003952980042 and batch: 400, loss is 4.016845760345459 and perplexity is 55.5256879903821
At time: 134.06168222427368 and batch: 450, loss is 4.043252415657044 and perplexity is 57.01146657908218
At time: 135.07213973999023 and batch: 500, loss is 3.9316322374343873 and perplexity is 50.99013779389598
At time: 136.0796298980713 and batch: 550, loss is 3.9969326972961428 and perplexity is 54.430937556661966
At time: 137.08220076560974 and batch: 600, loss is 4.011335887908936 and perplexity is 55.22058983044062
At time: 138.0920865535736 and batch: 650, loss is 3.887471766471863 and perplexity is 48.787384567210324
At time: 139.12104320526123 and batch: 700, loss is 3.8822165298461915 and perplexity is 48.53166783146115
At time: 140.1514449119568 and batch: 750, loss is 3.9916526556015013 and perplexity is 54.144297338967526
At time: 141.18959879875183 and batch: 800, loss is 3.9522018671035766 and perplexity is 52.04984760004037
At time: 142.20464658737183 and batch: 850, loss is 4.0262082576751705 and perplexity is 56.047988298301995
At time: 143.21401619911194 and batch: 900, loss is 3.9895288038253782 and perplexity is 54.029424906087556
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.44874593656357 and perplexity of 85.51962968609844
finished 7 epochs...
Completing Train Step...
At time: 145.5959188938141 and batch: 50, loss is 4.0892394304275514 and perplexity is 59.694472634624255
At time: 146.60864877700806 and batch: 100, loss is 3.972844958305359 and perplexity is 53.1354842470036
At time: 147.62617921829224 and batch: 150, loss is 3.958742198944092 and perplexity is 52.391386547148215
At time: 148.63281679153442 and batch: 200, loss is 3.879350266456604 and perplexity is 48.392762453517605
At time: 149.63324069976807 and batch: 250, loss is 4.0266261434555055 and perplexity is 56.07141485009877
At time: 150.630877494812 and batch: 300, loss is 4.015872921943664 and perplexity is 55.47169673546027
At time: 151.63592410087585 and batch: 350, loss is 3.9962280702590944 and perplexity is 54.392597555694614
At time: 152.64282298088074 and batch: 400, loss is 3.930788140296936 and perplexity is 50.94711532467193
At time: 153.64001727104187 and batch: 450, loss is 3.95745418548584 and perplexity is 52.323949175623156
At time: 154.63721752166748 and batch: 500, loss is 3.861458396911621 and perplexity is 47.53462519278599
At time: 155.64161467552185 and batch: 550, loss is 3.916614866256714 and perplexity is 50.23012097883751
At time: 156.6398823261261 and batch: 600, loss is 3.9353269004821776 and perplexity is 51.17887762213943
At time: 157.6375789642334 and batch: 650, loss is 3.808491950035095 and perplexity is 45.08240106366826
At time: 158.63192224502563 and batch: 700, loss is 3.813249359130859 and perplexity is 45.2973874722141
At time: 159.63893723487854 and batch: 750, loss is 3.9323019313812257 and perplexity is 51.02429701736093
At time: 160.63620376586914 and batch: 800, loss is 3.8858172416687013 and perplexity is 48.706731369119524
At time: 161.63072848320007 and batch: 850, loss is 3.954561877250671 and perplexity is 52.172830832288106
At time: 162.62553334236145 and batch: 900, loss is 3.9026603412628176 and perplexity is 49.53405145533356
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4298990328017975 and perplexity of 83.92294301806201
finished 8 epochs...
Completing Train Step...
At time: 164.99462008476257 and batch: 50, loss is 4.019539952278137 and perplexity is 55.675486553450774
At time: 165.9931559562683 and batch: 100, loss is 3.90215190410614 and perplexity is 49.50887290445071
At time: 166.9995994567871 and batch: 150, loss is 3.9056839084625246 and perplexity is 49.68404763607704
At time: 167.99685978889465 and batch: 200, loss is 3.8187849569320678 and perplexity is 45.54883089345254
At time: 168.9854757785797 and batch: 250, loss is 3.9584075355529786 and perplexity is 52.373856001640746
At time: 169.9700963497162 and batch: 300, loss is 3.948276662826538 and perplexity is 51.84594176345435
At time: 170.96677708625793 and batch: 350, loss is 3.9334763383865354 and perplexity is 51.08425551016472
At time: 171.96013021469116 and batch: 400, loss is 3.858317379951477 and perplexity is 47.385552371553764
At time: 172.95671010017395 and batch: 450, loss is 3.885198321342468 and perplexity is 48.67659510998447
At time: 173.944176197052 and batch: 500, loss is 3.787328352928162 and perplexity is 44.13832059762416
At time: 174.93178391456604 and batch: 550, loss is 3.8548108530044556 and perplexity is 47.219684635062634
At time: 175.92065906524658 and batch: 600, loss is 3.8793368768692016 and perplexity is 48.39211449873302
At time: 176.91122841835022 and batch: 650, loss is 3.75139265537262 and perplexity is 42.58034046709342
At time: 177.91386723518372 and batch: 700, loss is 3.7487157726287843 and perplexity is 42.46651031143286
At time: 178.90800213813782 and batch: 750, loss is 3.865467948913574 and perplexity is 47.72560035095262
At time: 179.89822459220886 and batch: 800, loss is 3.8243031692504883 and perplexity is 45.80087378670507
At time: 180.8851134777069 and batch: 850, loss is 3.8890345811843874 and perplexity is 48.86368981955245
At time: 181.88121485710144 and batch: 900, loss is 3.834306025505066 and perplexity is 46.261312354758815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.425280427279538 and perplexity of 83.53622977538502
finished 9 epochs...
Completing Train Step...
At time: 184.25411438941956 and batch: 50, loss is 3.955535192489624 and perplexity is 52.22363616438762
At time: 185.2653489112854 and batch: 100, loss is 3.837316699028015 and perplexity is 46.40079993342069
At time: 186.25540208816528 and batch: 150, loss is 3.829087538719177 and perplexity is 46.02052712125113
At time: 187.24392747879028 and batch: 200, loss is 3.743763189315796 and perplexity is 42.25671133339283
At time: 188.23420977592468 and batch: 250, loss is 3.9011976814270017 and perplexity is 49.46165294786642
At time: 189.23404669761658 and batch: 300, loss is 3.8863479614257814 and perplexity is 48.732587854426804
At time: 190.2304265499115 and batch: 350, loss is 3.8669507122039795 and perplexity is 47.79641860954763
At time: 191.22755408287048 and batch: 400, loss is 3.8013227796554565 and perplexity is 44.76035343583253
At time: 192.22190284729004 and batch: 450, loss is 3.8247973775863646 and perplexity is 45.82351455448814
At time: 193.2163166999817 and batch: 500, loss is 3.7329793405532836 and perplexity is 41.80346958673118
At time: 194.20786094665527 and batch: 550, loss is 3.7975005102157593 and perplexity is 44.58959385724018
At time: 195.20088148117065 and batch: 600, loss is 3.809773178100586 and perplexity is 45.1401989193817
At time: 196.19323658943176 and batch: 650, loss is 3.682128281593323 and perplexity is 39.73086261786002
At time: 197.18697023391724 and batch: 700, loss is 3.6861935520172118 and perplexity is 39.89270806844394
At time: 198.1813187599182 and batch: 750, loss is 3.7990221214294433 and perplexity is 44.65749352861698
At time: 199.17360138893127 and batch: 800, loss is 3.763870806694031 and perplexity is 43.114993200322814
At time: 200.16745400428772 and batch: 850, loss is 3.826147084236145 and perplexity is 45.885404614115686
At time: 201.17133355140686 and batch: 900, loss is 3.7748607397079468 and perplexity is 43.59143733671562
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.451626294279752 and perplexity of 85.76631190732034
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 203.5352499485016 and batch: 50, loss is 3.9117005395889284 and perplexity is 49.98387930736515
At time: 204.51254081726074 and batch: 100, loss is 3.782385301589966 and perplexity is 43.920680958030204
At time: 205.4869899749756 and batch: 150, loss is 3.76278235912323 and perplexity is 43.068090321001904
At time: 206.46467638015747 and batch: 200, loss is 3.655206356048584 and perplexity is 38.67550121536991
At time: 207.43707013130188 and batch: 250, loss is 3.7705962753295896 and perplexity is 43.40593901160896
At time: 208.4111771583557 and batch: 300, loss is 3.7244780683517456 and perplexity is 41.4495932436306
At time: 209.4115915298462 and batch: 350, loss is 3.7036507177352904 and perplexity is 40.59523591531615
At time: 210.40918135643005 and batch: 400, loss is 3.631238350868225 and perplexity is 37.75954724486769
At time: 211.40810823440552 and batch: 450, loss is 3.663415732383728 and perplexity is 38.99430977906
At time: 212.39627146720886 and batch: 500, loss is 3.5225141048431396 and perplexity is 33.86947291961709
At time: 213.38394260406494 and batch: 550, loss is 3.5629921436309813 and perplexity is 35.268568045308456
At time: 214.37859964370728 and batch: 600, loss is 3.558734827041626 and perplexity is 35.1187377492917
At time: 215.39550518989563 and batch: 650, loss is 3.4294890689849855 and perplexity is 30.860870944751962
At time: 216.39905428886414 and batch: 700, loss is 3.39376678943634 and perplexity is 29.777908391190493
At time: 217.39934182167053 and batch: 750, loss is 3.474535641670227 and perplexity is 32.282834252525916
At time: 218.4041349887848 and batch: 800, loss is 3.405092658996582 and perplexity is 30.117086213622315
At time: 219.40484642982483 and batch: 850, loss is 3.4477464008331298 and perplexity is 31.429482981495912
At time: 220.41360330581665 and batch: 900, loss is 3.409871859550476 and perplexity is 30.26136630586239
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3710870612157535 and perplexity of 79.12960368587723
finished 11 epochs...
Completing Train Step...
At time: 222.8294117450714 and batch: 50, loss is 3.7267959976196288 and perplexity is 41.545781905141915
At time: 223.849604845047 and batch: 100, loss is 3.6052609729766845 and perplexity is 36.791284135419964
At time: 224.8456118106842 and batch: 150, loss is 3.5977021217346192 and perplexity is 36.5142327060446
At time: 225.83724451065063 and batch: 200, loss is 3.4960262298583986 and perplexity is 32.98411987904198
At time: 226.83403205871582 and batch: 250, loss is 3.6116318988800047 and perplexity is 37.02642692383115
At time: 227.83509826660156 and batch: 300, loss is 3.572633533477783 and perplexity is 35.610250559539615
At time: 228.82946681976318 and batch: 350, loss is 3.555357451438904 and perplexity is 35.000328649775156
At time: 229.83111572265625 and batch: 400, loss is 3.485418186187744 and perplexity is 32.63607221118505
At time: 230.82787775993347 and batch: 450, loss is 3.5287446212768554 and perplexity is 34.08115598988552
At time: 231.8228611946106 and batch: 500, loss is 3.3956370306015016 and perplexity is 29.833652372371812
At time: 232.81747102737427 and batch: 550, loss is 3.4376369524002075 and perplexity is 31.11334890701193
At time: 233.82817316055298 and batch: 600, loss is 3.446412229537964 and perplexity is 31.38757862748908
At time: 234.83861875534058 and batch: 650, loss is 3.3224289846420287 and perplexity is 27.727618788892404
At time: 235.83833837509155 and batch: 700, loss is 3.291303234100342 and perplexity is 26.877868996287926
At time: 236.84705781936646 and batch: 750, loss is 3.3787482500076296 and perplexity is 29.334029251382645
At time: 237.86562323570251 and batch: 800, loss is 3.3179191160202026 and perplexity is 27.602852422386743
At time: 238.8702495098114 and batch: 850, loss is 3.371134014129639 and perplexity is 29.111521222738478
At time: 239.90804839134216 and batch: 900, loss is 3.3398147201538086 and perplexity is 28.213898754284926
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.384616799550514 and perplexity of 80.20748178014153
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 242.31836414337158 and batch: 50, loss is 3.65686665058136 and perplexity is 38.739767274120275
At time: 243.30372714996338 and batch: 100, loss is 3.5345513010025025 and perplexity is 34.27963002731233
At time: 244.2881338596344 and batch: 150, loss is 3.5359934043884276 and perplexity is 34.32910046000874
At time: 245.27844405174255 and batch: 200, loss is 3.4185646390914917 and perplexity is 30.52556835315752
At time: 246.2620494365692 and batch: 250, loss is 3.5347077465057373 and perplexity is 34.28499334080485
At time: 247.2481107711792 and batch: 300, loss is 3.4830578565597534 and perplexity is 32.559131161843816
At time: 248.23956084251404 and batch: 350, loss is 3.4561226558685303 and perplexity is 31.69385000367972
At time: 249.2241916656494 and batch: 400, loss is 3.4041798734664916 and perplexity is 30.08960831573966
At time: 250.20198845863342 and batch: 450, loss is 3.4269589233398436 and perplexity is 30.782887143315605
At time: 251.19088745117188 and batch: 500, loss is 3.285992317199707 and perplexity is 26.735501253840532
At time: 252.19276785850525 and batch: 550, loss is 3.30754714012146 and perplexity is 27.318035910648078
At time: 253.18545079231262 and batch: 600, loss is 3.3330610370635987 and perplexity is 28.023993026744474
At time: 254.18473196029663 and batch: 650, loss is 3.1886844062805175 and perplexity is 24.256494750325064
At time: 255.18534302711487 and batch: 700, loss is 3.155920190811157 and perplexity is 23.47462829314546
At time: 256.181236743927 and batch: 750, loss is 3.243519425392151 and perplexity is 25.623744096575415
At time: 257.1763172149658 and batch: 800, loss is 3.1669947481155396 and perplexity is 23.736044270696205
At time: 258.1761198043823 and batch: 850, loss is 3.207625036239624 and perplexity is 24.72030662013291
At time: 259.1686806678772 and batch: 900, loss is 3.190089087486267 and perplexity is 24.29059133442723
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380754444697132 and perplexity of 79.89828951310894
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 261.59823989868164 and batch: 50, loss is 3.61048819065094 and perplexity is 36.98410370198753
At time: 262.57365226745605 and batch: 100, loss is 3.486195387840271 and perplexity is 32.661446879779476
At time: 263.5460422039032 and batch: 150, loss is 3.4845574283599854 and perplexity is 32.60799254319269
At time: 264.51918959617615 and batch: 200, loss is 3.3706034660339355 and perplexity is 29.096080257040395
At time: 265.50228548049927 and batch: 250, loss is 3.492938241958618 and perplexity is 32.88242241704939
At time: 266.48777985572815 and batch: 300, loss is 3.44227264881134 and perplexity is 31.257915772035936
At time: 267.47649812698364 and batch: 350, loss is 3.412899823188782 and perplexity is 30.353135489441538
At time: 268.4696033000946 and batch: 400, loss is 3.3588342905044555 and perplexity is 28.755650598975393
At time: 269.4672555923462 and batch: 450, loss is 3.382424650192261 and perplexity is 29.44207136326778
At time: 270.4600787162781 and batch: 500, loss is 3.2457695627212524 and perplexity is 25.68146595637981
At time: 271.4581241607666 and batch: 550, loss is 3.261973638534546 and perplexity is 26.101000280941307
At time: 272.4549152851105 and batch: 600, loss is 3.290333743095398 and perplexity is 26.851823771409606
At time: 273.450804233551 and batch: 650, loss is 3.1511082792282106 and perplexity is 23.361941793525837
At time: 274.44143080711365 and batch: 700, loss is 3.1066955518722534 and perplexity is 22.347077499753954
At time: 275.43498063087463 and batch: 750, loss is 3.1929234313964843 and perplexity is 24.359536885596885
At time: 276.42706513404846 and batch: 800, loss is 3.120525012016296 and perplexity is 22.658272386257007
At time: 277.4201054573059 and batch: 850, loss is 3.1531684255599974 and perplexity is 23.41012042268827
At time: 278.416378736496 and batch: 900, loss is 3.1302625370025634 and perplexity is 22.87998559683784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.377897915774828 and perplexity of 79.6703834034406
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 280.7876899242401 and batch: 50, loss is 3.5917887592315676 and perplexity is 36.29894796722078
At time: 281.7985951900482 and batch: 100, loss is 3.469026288986206 and perplexity is 32.1054657743776
At time: 282.79263067245483 and batch: 150, loss is 3.4683365392684937 and perplexity is 32.083328673829776
At time: 283.78948879241943 and batch: 200, loss is 3.356447944641113 and perplexity is 28.68711148266465
At time: 284.77836418151855 and batch: 250, loss is 3.478862919807434 and perplexity is 32.422833745304715
At time: 285.77575969696045 and batch: 300, loss is 3.4286301231384275 and perplexity is 30.834374508963602
At time: 286.77244234085083 and batch: 350, loss is 3.398506579399109 and perplexity is 29.919384441013733
At time: 287.7678711414337 and batch: 400, loss is 3.342637152671814 and perplexity is 28.293643063116164
At time: 288.7619264125824 and batch: 450, loss is 3.368145761489868 and perplexity is 29.024658491128243
At time: 289.7581822872162 and batch: 500, loss is 3.230777883529663 and perplexity is 25.299329250135134
At time: 290.77118277549744 and batch: 550, loss is 3.2468649673461916 and perplexity is 25.70961296633159
At time: 291.76665139198303 and batch: 600, loss is 3.2761458492279054 and perplexity is 26.473542796282285
At time: 292.75887155532837 and batch: 650, loss is 3.1375144290924073 and perplexity is 23.046511868880884
At time: 293.7472360134125 and batch: 700, loss is 3.0919272565841673 and perplexity is 22.019474285153397
At time: 294.7476484775543 and batch: 750, loss is 3.1752915287017824 and perplexity is 23.93379623996601
At time: 295.7405598163605 and batch: 800, loss is 3.1055276775360108 and perplexity is 22.32099415545083
At time: 296.7364785671234 and batch: 850, loss is 3.1355666971206664 and perplexity is 23.00166712782186
At time: 297.7219920158386 and batch: 900, loss is 3.112419648170471 and perplexity is 22.475361126993644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.369305754361087 and perplexity of 78.98877504717234
finished 15 epochs...
Completing Train Step...
At time: 300.04797744750977 and batch: 50, loss is 3.583494906425476 and perplexity is 35.99913485729279
At time: 301.0218622684479 and batch: 100, loss is 3.460958638191223 and perplexity is 31.847492107853476
At time: 302.0077247619629 and batch: 150, loss is 3.460337200164795 and perplexity is 31.82770701345302
At time: 302.98827838897705 and batch: 200, loss is 3.3495942831039427 and perplexity is 28.491171948690294
At time: 303.9691689014435 and batch: 250, loss is 3.4715778636932373 and perplexity is 32.18748986960662
At time: 304.9593873023987 and batch: 300, loss is 3.4210159397125244 and perplexity is 30.600487484961185
At time: 305.9572615623474 and batch: 350, loss is 3.391862349510193 and perplexity is 29.721252119889567
At time: 306.9461715221405 and batch: 400, loss is 3.336428875923157 and perplexity is 28.11853242683448
At time: 307.94387769699097 and batch: 450, loss is 3.36227144241333 and perplexity is 28.85465819291979
At time: 308.931143283844 and batch: 500, loss is 3.2255905628204347 and perplexity is 25.168433308715596
At time: 309.9100570678711 and batch: 550, loss is 3.242142777442932 and perplexity is 25.588493491217474
At time: 310.89766931533813 and batch: 600, loss is 3.272151780128479 and perplexity is 26.368016516957763
At time: 311.8785798549652 and batch: 650, loss is 3.133971080780029 and perplexity is 22.96499455734783
At time: 312.861759185791 and batch: 700, loss is 3.090093188285828 and perplexity is 21.979126077415547
At time: 313.852676153183 and batch: 750, loss is 3.1747667360305787 and perplexity is 23.921239254300406
At time: 314.8696093559265 and batch: 800, loss is 3.1056876850128172 and perplexity is 22.324565967156126
At time: 315.8700420856476 and batch: 850, loss is 3.1375506830215456 and perplexity is 23.047347410634806
At time: 316.87239480018616 and batch: 900, loss is 3.11600417137146 and perplexity is 22.556069143866658
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.367040764795591 and perplexity of 78.81006875624085
finished 16 epochs...
Completing Train Step...
At time: 319.245947599411 and batch: 50, loss is 3.5787018966674804 and perplexity is 35.82700349582018
At time: 320.2372360229492 and batch: 100, loss is 3.455947127342224 and perplexity is 31.68828731711497
At time: 321.22652292251587 and batch: 150, loss is 3.4550885772705078 and perplexity is 31.66109301127334
At time: 322.2184965610504 and batch: 200, loss is 3.3448344135284422 and perplexity is 28.35587992778371
At time: 323.20843958854675 and batch: 250, loss is 3.466396379470825 and perplexity is 32.02114223468297
At time: 324.19865441322327 and batch: 300, loss is 3.415968976020813 and perplexity is 30.446437006257923
At time: 325.193562746048 and batch: 350, loss is 3.3871085977554323 and perplexity is 29.580299956705637
At time: 326.192120552063 and batch: 400, loss is 3.3318725061416625 and perplexity is 27.99070543006357
At time: 327.20402574539185 and batch: 450, loss is 3.358019971847534 and perplexity is 28.73224386776314
At time: 328.20740008354187 and batch: 500, loss is 3.221677885055542 and perplexity is 25.070149740682663
At time: 329.2102019786835 and batch: 550, loss is 3.2385416889190672 and perplexity is 25.496512775712002
At time: 330.214821100235 and batch: 600, loss is 3.2691102409362793 and perplexity is 26.287939002483025
At time: 331.21505331993103 and batch: 650, loss is 3.1311917018890383 and perplexity is 22.90125475580509
At time: 332.22155809402466 and batch: 700, loss is 3.088426275253296 and perplexity is 21.94251930434329
At time: 333.2388987541199 and batch: 750, loss is 3.1741041851043703 and perplexity is 23.905395464314868
At time: 334.25027418136597 and batch: 800, loss is 3.105645360946655 and perplexity is 22.323621120744104
At time: 335.24957156181335 and batch: 850, loss is 3.1386640453338623 and perplexity is 23.073021748408607
At time: 336.25551557540894 and batch: 900, loss is 3.118260006904602 and perplexity is 22.607009360903213
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366452151781892 and perplexity of 78.76369377395841
finished 17 epochs...
Completing Train Step...
At time: 338.638475894928 and batch: 50, loss is 3.57491295337677 and perplexity is 35.69151385467358
At time: 339.6401059627533 and batch: 100, loss is 3.4519447803497316 and perplexity is 31.56171326128339
At time: 340.62826442718506 and batch: 150, loss is 3.450834593772888 and perplexity is 31.526693313813215
At time: 341.6123933792114 and batch: 200, loss is 3.34088791847229 and perplexity is 28.244194116544016
At time: 342.60459446907043 and batch: 250, loss is 3.4620879888534546 and perplexity is 31.88347941147353
At time: 343.58253359794617 and batch: 300, loss is 3.411901535987854 and perplexity is 30.32284946236619
At time: 344.5688855648041 and batch: 350, loss is 3.3831571769714355 and perplexity is 29.46364637014614
At time: 345.555052280426 and batch: 400, loss is 3.3280889272689818 and perplexity is 27.88500048596486
At time: 346.5516278743744 and batch: 450, loss is 3.354428114891052 and perplexity is 28.62922687964965
At time: 347.5375771522522 and batch: 500, loss is 3.218310923576355 and perplexity is 24.985881455871223
At time: 348.52273535728455 and batch: 550, loss is 3.2354289960861204 and perplexity is 25.41727335110272
At time: 349.51907086372375 and batch: 600, loss is 3.2664788866043093 and perplexity is 26.21885704971854
At time: 350.51369428634644 and batch: 650, loss is 3.128774952888489 and perplexity is 22.845974996819738
At time: 351.50532269477844 and batch: 700, loss is 3.08681387424469 and perplexity is 21.90716767224827
At time: 352.5185441970825 and batch: 750, loss is 3.1732858180999757 and perplexity is 23.88584008027179
At time: 353.51210832595825 and batch: 800, loss is 3.1053870630264284 and perplexity is 22.31785572046419
At time: 354.5178608894348 and batch: 850, loss is 3.1392471599578857 and perplexity is 23.08647988824825
At time: 355.5299198627472 and batch: 900, loss is 3.1196978092193604 and perplexity is 22.639537149953618
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36645047958583 and perplexity of 78.76356206573
finished 18 epochs...
Completing Train Step...
At time: 357.869793176651 and batch: 50, loss is 3.5715668106079104 and perplexity is 35.57228454407446
At time: 358.8815710544586 and batch: 100, loss is 3.4484349298477173 and perplexity is 31.451130544074413
At time: 359.86878657341003 and batch: 150, loss is 3.4470816087722778 and perplexity is 31.40859585430055
At time: 360.85407614707947 and batch: 200, loss is 3.337364048957825 and perplexity is 28.144840419479856
At time: 361.8315181732178 and batch: 250, loss is 3.458294553756714 and perplexity is 31.762760615889263
At time: 362.8300111293793 and batch: 300, loss is 3.4083586740493774 and perplexity is 30.215609872841263
At time: 363.8283178806305 and batch: 350, loss is 3.37964319229126 and perplexity is 29.360293265144232
At time: 364.8326292037964 and batch: 400, loss is 3.3247162294387818 and perplexity is 27.791111224385013
At time: 365.826203584671 and batch: 450, loss is 3.351200523376465 and perplexity is 28.53697238989509
At time: 366.8194613456726 and batch: 500, loss is 3.215262336730957 and perplexity is 24.909825816359575
At time: 367.8163478374481 and batch: 550, loss is 3.232608847618103 and perplexity is 25.345693846456342
At time: 368.81759881973267 and batch: 600, loss is 3.2640836429595947 and perplexity is 26.156131650294657
At time: 369.8109641075134 and batch: 650, loss is 3.126570472717285 and perplexity is 22.795666969845016
At time: 370.8063633441925 and batch: 700, loss is 3.08523371219635 and perplexity is 21.872578133041785
At time: 371.80068349838257 and batch: 750, loss is 3.1723601245880126 and perplexity is 23.863739343914016
At time: 372.7897469997406 and batch: 800, loss is 3.1049367427825927 and perplexity is 22.307807800794876
At time: 373.78240489959717 and batch: 850, loss is 3.1394766426086425 and perplexity is 23.091778442789565
At time: 374.77451181411743 and batch: 900, loss is 3.120590105056763 and perplexity is 22.659747330102118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.366707161681293 and perplexity of 78.78378185680532
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 377.1471121311188 and batch: 50, loss is 3.569182596206665 and perplexity is 35.48757361561519
At time: 378.122510433197 and batch: 100, loss is 3.4467288303375243 and perplexity is 31.397517533228545
At time: 379.0990250110626 and batch: 150, loss is 3.4455797719955443 and perplexity is 31.36146067347453
At time: 380.07838582992554 and batch: 200, loss is 3.335781807899475 and perplexity is 28.100343708931725
At time: 381.0782997608185 and batch: 250, loss is 3.4570540380477905 and perplexity is 31.72338284180995
At time: 382.07195711135864 and batch: 300, loss is 3.4072343158721923 and perplexity is 30.181655796650716
At time: 383.0745906829834 and batch: 350, loss is 3.3780056715011595 and perplexity is 29.312254517477683
At time: 384.0671224594116 and batch: 400, loss is 3.322314591407776 and perplexity is 27.724447118313304
At time: 385.0618999004364 and batch: 450, loss is 3.349153742790222 and perplexity is 28.47862320317884
At time: 386.05922293663025 and batch: 500, loss is 3.2127710437774657 and perplexity is 24.84784538059902
At time: 387.06211256980896 and batch: 550, loss is 3.229526162147522 and perplexity is 25.2676813500677
At time: 388.05887246131897 and batch: 600, loss is 3.261143932342529 and perplexity is 26.07935310103183
At time: 389.059011220932 and batch: 650, loss is 3.123611421585083 and perplexity is 22.72831312656746
At time: 390.044579744339 and batch: 700, loss is 3.0813580131530762 and perplexity is 21.787970665346098
At time: 391.03541588783264 and batch: 750, loss is 3.167731914520264 and perplexity is 23.753548135951664
At time: 392.0240478515625 and batch: 800, loss is 3.1005588722229005 and perplexity is 22.21036056709767
At time: 393.03049874305725 and batch: 850, loss is 3.1344634246826173 and perplexity is 22.97630401623422
At time: 394.0365788936615 and batch: 900, loss is 3.1156343126297 and perplexity is 22.547728127108122
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365681687446489 and perplexity of 78.70303252864484
finished 20 epochs...
Completing Train Step...
At time: 396.4002892971039 and batch: 50, loss is 3.5680548334121704 and perplexity is 35.447574609359535
At time: 397.3866882324219 and batch: 100, loss is 3.4456336545944213 and perplexity is 31.363150556007415
At time: 398.36438512802124 and batch: 150, loss is 3.4443846797943114 and perplexity is 31.32400322341581
At time: 399.35062074661255 and batch: 200, loss is 3.334800271987915 and perplexity is 28.07277574414122
At time: 400.3444583415985 and batch: 250, loss is 3.4560470008850097 and perplexity is 31.691452296680374
At time: 401.33143186569214 and batch: 300, loss is 3.4061568355560303 and perplexity is 30.149153170235273
At time: 402.3205964565277 and batch: 350, loss is 3.3771012353897096 and perplexity is 29.285755441151206
At time: 403.3178770542145 and batch: 400, loss is 3.321477632522583 and perplexity is 27.701252603742073
At time: 404.3090362548828 and batch: 450, loss is 3.3482518339157106 and perplexity is 28.452949659516786
At time: 405.30322432518005 and batch: 500, loss is 3.2120160055160523 and perplexity is 24.829091387505095
At time: 406.29539132118225 and batch: 550, loss is 3.228883671760559 and perplexity is 25.251452321755046
At time: 407.2788064479828 and batch: 600, loss is 3.260578441619873 and perplexity is 26.064609637839038
At time: 408.27155780792236 and batch: 650, loss is 3.1231330585479737 and perplexity is 22.71744334173078
At time: 409.26479387283325 and batch: 700, loss is 3.0810838174819946 and perplexity is 21.78199731707855
At time: 410.26378059387207 and batch: 750, loss is 3.167583646774292 and perplexity is 23.75002651198879
At time: 411.25253891944885 and batch: 800, loss is 3.1005969524383543 and perplexity is 22.211206358517227
At time: 412.2570290565491 and batch: 850, loss is 3.134708924293518 and perplexity is 22.981945382378402
At time: 413.2604811191559 and batch: 900, loss is 3.1160038137435913 and perplexity is 22.556061077189167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.365001521698416 and perplexity of 78.64951962253532
finished 21 epochs...
Completing Train Step...
At time: 415.6405427455902 and batch: 50, loss is 3.5670771741867067 and perplexity is 35.412935896210136
At time: 416.6352653503418 and batch: 100, loss is 3.4446618127822877 and perplexity is 31.332685341019523
At time: 417.61887097358704 and batch: 150, loss is 3.443350510597229 and perplexity is 31.29162564898359
At time: 418.60460209846497 and batch: 200, loss is 3.333912787437439 and perplexity is 28.04787264157325
At time: 419.58470582962036 and batch: 250, loss is 3.455123233795166 and perplexity is 31.662190293737876
At time: 420.57023882865906 and batch: 300, loss is 3.4052011442184447 and perplexity is 30.120353649633113
At time: 421.5528886318207 and batch: 350, loss is 3.3762642908096314 and perplexity is 29.261255140988112
At time: 422.5337932109833 and batch: 400, loss is 3.320698719024658 and perplexity is 27.679684125259584
At time: 423.51682806015015 and batch: 450, loss is 3.3474492168426515 and perplexity is 28.430121998501853
At time: 424.5068612098694 and batch: 500, loss is 3.2113227319717406 and perplexity is 24.81188400071905
At time: 425.50920939445496 and batch: 550, loss is 3.2282798385620115 and perplexity is 25.23620925912842
At time: 426.504301071167 and batch: 600, loss is 3.260055909156799 and perplexity is 26.050993590889007
At time: 427.5128219127655 and batch: 650, loss is 3.122677083015442 and perplexity is 22.707087104680063
At time: 428.5149221420288 and batch: 700, loss is 3.080819568634033 and perplexity is 21.776242209804934
At time: 429.52478551864624 and batch: 750, loss is 3.167442350387573 and perplexity is 23.74667095612771
At time: 430.5246388912201 and batch: 800, loss is 3.1006080961227416 and perplexity is 22.211453874569866
At time: 431.5238003730774 and batch: 850, loss is 3.1349032735824585 and perplexity is 22.986412341183012
At time: 432.53142833709717 and batch: 900, loss is 3.1163147115707397 and perplexity is 22.563074797785845
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364538323389341 and perplexity of 78.61309773396634
finished 22 epochs...
Completing Train Step...
At time: 434.9240162372589 and batch: 50, loss is 3.566194186210632 and perplexity is 35.38168050071623
At time: 435.92272305488586 and batch: 100, loss is 3.443772029876709 and perplexity is 31.30481845279615
At time: 436.90405774116516 and batch: 150, loss is 3.4424074411392214 and perplexity is 31.262129383242414
At time: 437.89783239364624 and batch: 200, loss is 3.3330835247039796 and perplexity is 28.024623227307533
At time: 438.8974344730377 and batch: 250, loss is 3.4542543125152587 and perplexity is 31.634690292221702
At time: 439.8763520717621 and batch: 300, loss is 3.4043296003341674 and perplexity is 30.09411387583565
At time: 440.867347240448 and batch: 350, loss is 3.3754735517501833 and perplexity is 29.23812626927593
At time: 441.8551678657532 and batch: 400, loss is 3.31995973110199 and perplexity is 27.65923672910768
At time: 442.8370044231415 and batch: 450, loss is 3.346707501411438 and perplexity is 28.409042756670136
At time: 443.82239151000977 and batch: 500, loss is 3.2106655073165893 and perplexity is 24.795582376314545
At time: 444.82063126564026 and batch: 550, loss is 3.227701144218445 and perplexity is 25.22160943240311
At time: 445.8174059391022 and batch: 600, loss is 3.2595612668991087 and perplexity is 26.038110855039577
At time: 446.82361221313477 and batch: 650, loss is 3.12223669052124 and perplexity is 22.697089275600003
At time: 447.824506521225 and batch: 700, loss is 3.0805524921417238 and perplexity is 21.7704270639988
At time: 448.8232409954071 and batch: 750, loss is 3.1672970342636106 and perplexity is 23.743220432661772
At time: 449.8160402774811 and batch: 800, loss is 3.1005925130844116 and perplexity is 22.211107755329575
At time: 450.8093752861023 and batch: 850, loss is 3.135053186416626 and perplexity is 22.989858557714005
At time: 451.7986204624176 and batch: 900, loss is 3.1165789413452147 and perplexity is 22.56903742166782
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364228131019906 and perplexity of 78.58871633256946
finished 23 epochs...
Completing Train Step...
At time: 454.1420421600342 and batch: 50, loss is 3.5653767681121824 and perplexity is 35.35277069203153
At time: 455.13149094581604 and batch: 100, loss is 3.442941288948059 and perplexity is 31.278823058057654
At time: 456.11845803260803 and batch: 150, loss is 3.4415292263031008 and perplexity is 31.234686569516207
At time: 457.10072660446167 and batch: 200, loss is 3.3322960329055786 and perplexity is 28.00256275372381
At time: 458.1043438911438 and batch: 250, loss is 3.4534272289276124 and perplexity is 31.608536576226268
At time: 459.101610660553 and batch: 300, loss is 3.4035183143615724 and perplexity is 30.069708844458898
At time: 460.08699464797974 and batch: 350, loss is 3.374718008041382 and perplexity is 29.21604393005275
At time: 461.0772533416748 and batch: 400, loss is 3.319250807762146 and perplexity is 27.639635399369137
At time: 462.0785503387451 and batch: 450, loss is 3.3460077953338625 and perplexity is 28.389171729545136
At time: 463.08444118499756 and batch: 500, loss is 3.210033812522888 and perplexity is 24.779924082172773
At time: 464.1000301837921 and batch: 550, loss is 3.2271404123306273 and perplexity is 25.20747083608364
At time: 465.08796310424805 and batch: 600, loss is 3.2590853548049927 and perplexity is 26.025721951424092
At time: 466.0737552642822 and batch: 650, loss is 3.1218071365356446 and perplexity is 22.687341744135626
At time: 467.06822633743286 and batch: 700, loss is 3.0802799224853517 and perplexity is 21.76449391480984
At time: 468.05973386764526 and batch: 750, loss is 3.1671440744400026 and perplexity is 23.73958895159494
At time: 469.04837226867676 and batch: 800, loss is 3.1005546188354494 and perplexity is 22.210266098029653
At time: 470.0344920158386 and batch: 850, loss is 3.135166368484497 and perplexity is 22.99246074470331
At time: 471.03525853157043 and batch: 900, loss is 3.116803755760193 and perplexity is 22.57411183699188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.364025377247431 and perplexity of 78.57278378910533
finished 24 epochs...
Completing Train Step...
At time: 473.3763687610626 and batch: 50, loss is 3.5646067380905153 and perplexity is 35.325558475705925
At time: 474.3668706417084 and batch: 100, loss is 3.442154731750488 and perplexity is 31.254230147782994
At time: 475.3434293270111 and batch: 150, loss is 3.440698881149292 and perplexity is 31.208761763650468
At time: 476.3350839614868 and batch: 200, loss is 3.331540560722351 and perplexity is 27.981415585558736
At time: 477.3142354488373 and batch: 250, loss is 3.452633104324341 and perplexity is 31.583445423727046
At time: 478.30637216567993 and batch: 300, loss is 3.4027518796920777 and perplexity is 30.046671206648792
At time: 479.29203510284424 and batch: 350, loss is 3.3739903926849366 and perplexity is 29.194793619798233
At time: 480.28465461730957 and batch: 400, loss is 3.318565411567688 and perplexity is 27.620697789074576
At time: 481.2714502811432 and batch: 450, loss is 3.3453384971618654 and perplexity is 28.370177265990968
At time: 482.2597653865814 and batch: 500, loss is 3.2094209718704225 and perplexity is 24.76474258972322
At time: 483.24694991111755 and batch: 550, loss is 3.2265929651260374 and perplexity is 25.19367485327031
At time: 484.23972058296204 and batch: 600, loss is 3.2586222791671755 and perplexity is 26.0136728636666
At time: 485.23469734191895 and batch: 650, loss is 3.121385684013367 and perplexity is 22.677782121338858
At time: 486.22218465805054 and batch: 700, loss is 3.0800014305114747 and perplexity is 21.75843352186364
At time: 487.2073495388031 and batch: 750, loss is 3.1669824171066283 and perplexity is 23.735751583127357
At time: 488.2103123664856 and batch: 800, loss is 3.1004983091354372 and perplexity is 22.209015479819783
At time: 489.1990280151367 and batch: 850, loss is 3.1352490472793577 and perplexity is 22.994361812236466
At time: 490.18728494644165 and batch: 900, loss is 3.1169950675964357 and perplexity is 22.57843094491402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36390163473887 and perplexity of 78.5630615972712
finished 25 epochs...
Completing Train Step...
At time: 492.5530390739441 and batch: 50, loss is 3.5638719844818114 and perplexity is 35.29961242728005
At time: 493.5339345932007 and batch: 100, loss is 3.4414023971557617 and perplexity is 31.230725352055416
At time: 494.51325392723083 and batch: 150, loss is 3.4399044847488405 and perplexity is 31.18397948042444
At time: 495.50438809394836 and batch: 200, loss is 3.330810561180115 and perplexity is 27.960996618815113
At time: 496.48515367507935 and batch: 250, loss is 3.4518659591674803 and perplexity is 31.559225627770655
At time: 497.46293115615845 and batch: 300, loss is 3.402019753456116 and perplexity is 30.02468130103073
At time: 498.4553117752075 and batch: 350, loss is 3.373285660743713 and perplexity is 29.174226364269114
At time: 499.4379942417145 and batch: 400, loss is 3.317899217605591 and perplexity is 27.602303174849375
At time: 500.421462059021 and batch: 450, loss is 3.344692440032959 and perplexity is 28.351854430156614
At time: 501.4178476333618 and batch: 500, loss is 3.2088229608535768 and perplexity is 24.749937428091314
At time: 502.4097442626953 and batch: 550, loss is 3.2260559797286987 and perplexity is 25.180149849458576
At time: 503.40936970710754 and batch: 600, loss is 3.2581688499450685 and perplexity is 26.00188017799207
At time: 504.4056284427643 and batch: 650, loss is 3.1209704208374025 and perplexity is 22.668366828558813
At time: 505.4088673591614 and batch: 700, loss is 3.0797176361083984 and perplexity is 21.75225947633165
At time: 506.39964413642883 and batch: 750, loss is 3.166812205314636 and perplexity is 23.73171182213334
At time: 507.3891682624817 and batch: 800, loss is 3.100426421165466 and perplexity is 22.207418976167272
At time: 508.3822033405304 and batch: 850, loss is 3.135306463241577 and perplexity is 22.995682093547785
At time: 509.37521147727966 and batch: 900, loss is 3.1171573066711424 and perplexity is 22.582094345824206
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3638326566513275 and perplexity of 78.55764265442707
finished 26 epochs...
Completing Train Step...
At time: 511.7624673843384 and batch: 50, loss is 3.563164587020874 and perplexity is 35.274650401154815
At time: 512.7499301433563 and batch: 100, loss is 3.44067729473114 and perplexity is 31.208088085540204
At time: 513.743720293045 and batch: 150, loss is 3.439138708114624 and perplexity is 31.160108658601757
At time: 514.7310242652893 and batch: 200, loss is 3.3301012468338014 and perplexity is 27.941170515077324
At time: 515.720290184021 and batch: 250, loss is 3.4511215019226076 and perplexity is 31.535739876762914
At time: 516.7050948143005 and batch: 300, loss is 3.401315016746521 and perplexity is 30.003529260109303
At time: 517.6897132396698 and batch: 350, loss is 3.3726006031036375 and perplexity is 29.15424718183212
At time: 518.6695737838745 and batch: 400, loss is 3.3172494745254517 and perplexity is 27.58437459448186
At time: 519.6536889076233 and batch: 450, loss is 3.344064483642578 and perplexity is 28.334056290800568
At time: 520.6335566043854 and batch: 500, loss is 3.2082373666763306 and perplexity is 24.735448251648947
At time: 521.6195647716522 and batch: 550, loss is 3.22552770614624 and perplexity is 25.166851354421837
At time: 522.6100952625275 and batch: 600, loss is 3.2577226877212526 and perplexity is 25.99028170890025
At time: 523.5968992710114 and batch: 650, loss is 3.1205604696273803 and perplexity is 22.659075808710753
At time: 524.5857527256012 and batch: 700, loss is 3.0794293689727783 and perplexity is 21.745989918496296
At time: 525.5745663642883 and batch: 750, loss is 3.166633896827698 and perplexity is 23.727480633745493
At time: 526.5629894733429 and batch: 800, loss is 3.100341396331787 and perplexity is 22.205530874331295
At time: 527.5569658279419 and batch: 850, loss is 3.135342583656311 and perplexity is 22.996512722123324
At time: 528.5373618602753 and batch: 900, loss is 3.1172941398620604 and perplexity is 22.58518453726671
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363807155661387 and perplexity of 78.55563938231484
finished 27 epochs...
Completing Train Step...
At time: 530.8727881908417 and batch: 50, loss is 3.562479410171509 and perplexity is 35.2504893055874
At time: 531.8561313152313 and batch: 100, loss is 3.439974331855774 and perplexity is 31.186157667242956
At time: 532.832457780838 and batch: 150, loss is 3.4383958101272585 and perplexity is 31.136968473056076
At time: 533.8229279518127 and batch: 200, loss is 3.329408974647522 and perplexity is 27.92183431360711
At time: 534.8341522216797 and batch: 250, loss is 3.450396399497986 and perplexity is 31.512881523641195
At time: 535.83469581604 and batch: 300, loss is 3.4006329154968262 and perplexity is 29.98307079347177
At time: 536.8442857265472 and batch: 350, loss is 3.371932964324951 and perplexity is 29.134789172033837
At time: 537.855278968811 and batch: 400, loss is 3.3166139411926268 and perplexity is 27.566849374482548
At time: 538.8539435863495 and batch: 450, loss is 3.3434512186050416 and perplexity is 28.316685331751188
At time: 539.8689138889313 and batch: 500, loss is 3.207662420272827 and perplexity is 24.72123078217074
At time: 540.8825435638428 and batch: 550, loss is 3.225006914138794 and perplexity is 25.15374807172248
At time: 541.8889722824097 and batch: 600, loss is 3.2572821235656737 and perplexity is 25.978833844330467
At time: 542.8971257209778 and batch: 650, loss is 3.120154685974121 and perplexity is 22.649882991423244
At time: 543.8955647945404 and batch: 700, loss is 3.079137077331543 and perplexity is 21.739634676250077
At time: 544.8895809650421 and batch: 750, loss is 3.166447944641113 and perplexity is 23.723068867041047
At time: 545.9161672592163 and batch: 800, loss is 3.1002448320388796 and perplexity is 22.203386716469986
At time: 546.9337458610535 and batch: 850, loss is 3.135360713005066 and perplexity is 22.996929637701808
At time: 547.9299454689026 and batch: 900, loss is 3.1174086236953737 and perplexity is 22.58777032378118
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363813844445634 and perplexity of 78.55616482579529
Annealing...
finished 28 epochs...
Completing Train Step...
At time: 550.3324918746948 and batch: 50, loss is 3.5619528770446776 and perplexity is 35.2319336407458
At time: 551.3079195022583 and batch: 100, loss is 3.439583783149719 and perplexity is 31.173980331795278
At time: 552.2814877033234 and batch: 150, loss is 3.438035192489624 and perplexity is 31.125741957403662
At time: 553.2528598308563 and batch: 200, loss is 3.3290446853637694 and perplexity is 27.911664541065985
At time: 554.2179868221283 and batch: 250, loss is 3.4501146554946898 and perplexity is 31.504004208869155
At time: 555.1916732788086 and batch: 300, loss is 3.4003669214248657 and perplexity is 29.975096534981297
At time: 556.1576502323151 and batch: 350, loss is 3.371572799682617 and perplexity is 29.124297740546957
At time: 557.1236517429352 and batch: 400, loss is 3.316085548400879 and perplexity is 27.552287097621754
At time: 558.0997269153595 and batch: 450, loss is 3.3429970359802246 and perplexity is 28.303827305448195
At time: 559.0857014656067 and batch: 500, loss is 3.207116651535034 and perplexity is 24.7077423883567
At time: 560.094868183136 and batch: 550, loss is 3.2243218278884886 and perplexity is 25.13652148629707
At time: 561.1058666706085 and batch: 600, loss is 3.2566225576400756 and perplexity is 25.961704740247633
At time: 562.1250433921814 and batch: 650, loss is 3.119491548538208 and perplexity is 22.63486798514921
At time: 563.1259107589722 and batch: 700, loss is 3.078273797035217 and perplexity is 21.72087537641674
At time: 564.1128098964691 and batch: 750, loss is 3.1654235887527467 and perplexity is 23.698780443875446
At time: 565.0970432758331 and batch: 800, loss is 3.0992361879348755 and perplexity is 22.18100269202436
At time: 566.1066513061523 and batch: 850, loss is 3.1342074728012084 and perplexity is 22.97042394053376
At time: 567.1035194396973 and batch: 900, loss is 3.116274886131287 and perplexity is 22.562176231309635
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363764096612799 and perplexity of 78.55225692404505
finished 29 epochs...
Completing Train Step...
At time: 569.4579229354858 and batch: 50, loss is 3.5617888879776003 and perplexity is 35.2261564625267
At time: 570.4415411949158 and batch: 100, loss is 3.439418730735779 and perplexity is 31.16883541569148
At time: 571.4215290546417 and batch: 150, loss is 3.4378587913513186 and perplexity is 31.120251825338784
At time: 572.402606010437 and batch: 200, loss is 3.3288874578475953 and perplexity is 27.907276404354867
At time: 573.3905494213104 and batch: 250, loss is 3.4499493551254274 and perplexity is 31.49879701572752
At time: 574.3684251308441 and batch: 300, loss is 3.400205945968628 and perplexity is 29.97027166849377
At time: 575.3542382717133 and batch: 350, loss is 3.371424479484558 and perplexity is 29.119978339272897
At time: 576.3417389392853 and batch: 400, loss is 3.3159441232681273 and perplexity is 27.548390787285953
At time: 577.3315732479095 and batch: 450, loss is 3.3428550863265993 and perplexity is 28.2998098721093
At time: 578.31809425354 and batch: 500, loss is 3.206988983154297 and perplexity is 24.704588192244152
At time: 579.319028377533 and batch: 550, loss is 3.2242095279693603 and perplexity is 25.13369881546281
At time: 580.3148603439331 and batch: 600, loss is 3.256527347564697 and perplexity is 25.95923304204952
At time: 581.3019735813141 and batch: 650, loss is 3.119407362937927 and perplexity is 22.632962535407437
At time: 582.2934069633484 and batch: 700, loss is 3.0782125997543335 and perplexity is 21.719546158577963
At time: 583.2824106216431 and batch: 750, loss is 3.165388026237488 and perplexity is 23.69793767061995
At time: 584.2704708576202 and batch: 800, loss is 3.09922447681427 and perplexity is 22.18074292914774
At time: 585.2629034519196 and batch: 850, loss is 3.134223027229309 and perplexity is 22.970781235120135
At time: 586.2503321170807 and batch: 900, loss is 3.116311764717102 and perplexity is 22.563008307804772
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36371852927012 and perplexity of 78.54867758798657
finished 30 epochs...
Completing Train Step...
At time: 588.5959239006042 and batch: 50, loss is 3.5616273260116578 and perplexity is 35.220465715152564
At time: 589.5707488059998 and batch: 100, loss is 3.439255704879761 and perplexity is 31.163754503788017
At time: 590.5470585823059 and batch: 150, loss is 3.437685112953186 and perplexity is 31.114847379183637
At time: 591.5193309783936 and batch: 200, loss is 3.328731713294983 and perplexity is 27.90293033652362
At time: 592.5013341903687 and batch: 250, loss is 3.449786133766174 and perplexity is 31.493656158824006
At time: 593.4952394962311 and batch: 300, loss is 3.400046968460083 and perplexity is 29.96550744808547
At time: 594.4808685779572 and batch: 350, loss is 3.3712771940231323 and perplexity is 29.115689705660934
At time: 595.4804582595825 and batch: 400, loss is 3.315804123878479 and perplexity is 27.54453429934922
At time: 596.4776182174683 and batch: 450, loss is 3.3427148962020876 and perplexity is 28.295842796318592
At time: 597.480907201767 and batch: 500, loss is 3.206862573623657 and perplexity is 24.701465494219672
At time: 598.4911081790924 and batch: 550, loss is 3.224097714424133 and perplexity is 25.130888684601857
At time: 599.4806895256042 and batch: 600, loss is 3.256432147026062 and perplexity is 25.956761826713745
At time: 600.4823544025421 and batch: 650, loss is 3.119322681427002 and perplexity is 22.63104602309095
At time: 601.4777126312256 and batch: 700, loss is 3.078152289390564 and perplexity is 21.718236284348126
At time: 602.4685471057892 and batch: 750, loss is 3.1653519344329832 and perplexity is 23.69708238472088
At time: 603.4582118988037 and batch: 800, loss is 3.0992120504379272 and perplexity is 22.180467304601056
At time: 604.446576833725 and batch: 850, loss is 3.134237599372864 and perplexity is 22.971115971080767
At time: 605.4427618980408 and batch: 900, loss is 3.1163465452194212 and perplexity is 22.563793074214754
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363678814613656 and perplexity of 78.54555811618526
finished 31 epochs...
Completing Train Step...
At time: 607.8295845985413 and batch: 50, loss is 3.5614680242538452 and perplexity is 35.2148554799252
At time: 608.803537607193 and batch: 100, loss is 3.4390944480895995 and perplexity is 31.158729541932853
At time: 609.792070388794 and batch: 150, loss is 3.4375138425827028 and perplexity is 31.109518784073728
At time: 610.7709665298462 and batch: 200, loss is 3.3285773944854737 and perplexity is 27.898624721759486
At time: 611.778082370758 and batch: 250, loss is 3.449624471664429 and perplexity is 31.48856523969316
At time: 612.7625541687012 and batch: 300, loss is 3.399890103340149 and perplexity is 29.960807273822084
At time: 613.7470495700836 and batch: 350, loss is 3.3711309671401977 and perplexity is 29.111432520375836
At time: 614.7405405044556 and batch: 400, loss is 3.3156651306152343 and perplexity is 27.540706060698252
At time: 615.735093832016 and batch: 450, loss is 3.342576332092285 and perplexity is 28.29192227967813
At time: 616.730776309967 and batch: 500, loss is 3.2067373466491698 and perplexity is 24.698372398104002
At time: 617.7265455722809 and batch: 550, loss is 3.223986277580261 and perplexity is 25.128088333717173
At time: 618.7214059829712 and batch: 600, loss is 3.2563370132446288 and perplexity is 25.954292579263686
At time: 619.7249181270599 and batch: 650, loss is 3.1192377758026124 and perplexity is 22.629124601568673
At time: 620.7197387218475 and batch: 700, loss is 3.0780922889709474 and perplexity is 21.71693322015032
At time: 621.7193474769592 and batch: 750, loss is 3.165315227508545 and perplexity is 23.696212553672886
At time: 622.7120587825775 and batch: 800, loss is 3.0991988134384156 and perplexity is 22.18017370370938
At time: 623.7017121315002 and batch: 850, loss is 3.1342510175704956 and perplexity is 22.97142420412265
At time: 624.7004408836365 and batch: 900, loss is 3.1163794565200806 and perplexity is 22.564535690212796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363642444349315 and perplexity of 78.54270144542303
finished 32 epochs...
Completing Train Step...
At time: 627.0745918750763 and batch: 50, loss is 3.5613106203079226 and perplexity is 35.209312958936344
At time: 628.1085338592529 and batch: 100, loss is 3.4389350843429565 and perplexity is 31.153764365697466
At time: 629.0969190597534 and batch: 150, loss is 3.437344717979431 and perplexity is 31.104257843941145
At time: 630.0747470855713 and batch: 200, loss is 3.3284244394302367 and perplexity is 27.89435781240513
At time: 631.0569746494293 and batch: 250, loss is 3.4494644260406493 and perplexity is 31.48352603588946
At time: 632.040397644043 and batch: 300, loss is 3.3997349071502687 and perplexity is 29.956157831484653
At time: 633.0284793376923 and batch: 350, loss is 3.3709855937957762 and perplexity is 29.10720080166645
At time: 634.0079622268677 and batch: 400, loss is 3.315527229309082 and perplexity is 27.536908423215724
At time: 634.9875769615173 and batch: 450, loss is 3.3424389696121217 and perplexity is 28.28803629796536
At time: 635.967967748642 and batch: 500, loss is 3.2066129064559936 and perplexity is 24.695299119095328
At time: 636.9630007743835 and batch: 550, loss is 3.223875308036804 and perplexity is 25.125300035937762
At time: 637.9500601291656 and batch: 600, loss is 3.2562421417236327 and perplexity is 25.951830372848796
At time: 638.9470098018646 and batch: 650, loss is 3.1191529035568237 and perplexity is 22.627204098443364
At time: 639.9599452018738 and batch: 700, loss is 3.07803249835968 and perplexity is 21.715634790255578
At time: 640.9631650447845 and batch: 750, loss is 3.1652779817581176 and perplexity is 23.69532998689007
At time: 641.9690766334534 and batch: 800, loss is 3.0991847562789916 and perplexity is 22.179861915663004
At time: 642.9623672962189 and batch: 850, loss is 3.1342631196975708 and perplexity is 22.971702208899682
At time: 643.9501552581787 and batch: 900, loss is 3.1164108228683474 and perplexity is 22.565243468397885
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3636106726241435 and perplexity of 78.54020604794023
finished 33 epochs...
Completing Train Step...
At time: 646.2765510082245 and batch: 50, loss is 3.561155219078064 and perplexity is 35.20384181352242
At time: 647.2678442001343 and batch: 100, loss is 3.4387773752212523 and perplexity is 31.148851520292023
At time: 648.2432703971863 and batch: 150, loss is 3.437177510261536 and perplexity is 31.09905740675897
At time: 649.2218723297119 and batch: 200, loss is 3.328272671699524 and perplexity is 27.89012467025557
At time: 650.1973881721497 and batch: 250, loss is 3.4493056678771974 and perplexity is 31.478528165853845
At time: 651.178026676178 and batch: 300, loss is 3.3995814895629883 and perplexity is 29.951562382546314
At time: 652.1695861816406 and batch: 350, loss is 3.370841302871704 and perplexity is 29.103001199755134
At time: 653.1660346984863 and batch: 400, loss is 3.3153900957107543 and perplexity is 27.533132446789367
At time: 654.1648528575897 and batch: 450, loss is 3.342303056716919 and perplexity is 28.284191850313423
At time: 655.159262418747 and batch: 500, loss is 3.2064894104003905 and perplexity is 24.692249535371836
At time: 656.1607096195221 and batch: 550, loss is 3.223764696121216 and perplexity is 25.12252103206936
At time: 657.1666185855865 and batch: 600, loss is 3.2561476039886474 and perplexity is 25.9493770615537
At time: 658.1729598045349 and batch: 650, loss is 3.1190678119659423 and perplexity is 22.625278795564135
At time: 659.1751565933228 and batch: 700, loss is 3.0779727935791015 and perplexity is 21.714338301748967
At time: 660.1789083480835 and batch: 750, loss is 3.1652403402328493 and perplexity is 23.69443807531419
At time: 661.1992871761322 and batch: 800, loss is 3.0991698026657106 and perplexity is 22.179530249065106
At time: 662.2072024345398 and batch: 850, loss is 3.1342741870880126 and perplexity is 22.971956447104017
At time: 663.1908555030823 and batch: 900, loss is 3.1164406967163085 and perplexity is 22.565917589119707
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363582245291096 and perplexity of 78.53797339107967
finished 34 epochs...
Completing Train Step...
At time: 665.5722804069519 and batch: 50, loss is 3.5610014820098876 and perplexity is 35.198430094095066
At time: 666.5609602928162 and batch: 100, loss is 3.4386210536956785 and perplexity is 31.14398266486587
At time: 667.5482025146484 and batch: 150, loss is 3.437012062072754 and perplexity is 31.093912549653574
At time: 668.5355906486511 and batch: 200, loss is 3.328122029304504 and perplexity is 27.885923551519078
At time: 669.5130577087402 and batch: 250, loss is 3.4491481256484984 and perplexity is 31.47356935899143
At time: 670.4846379756927 and batch: 300, loss is 3.3994297933578492 and perplexity is 29.94701918879623
At time: 671.4706833362579 and batch: 350, loss is 3.370697784423828 and perplexity is 29.098824681905253
At time: 672.4586033821106 and batch: 400, loss is 3.3152537679672243 and perplexity is 27.52937917281408
At time: 673.450528383255 and batch: 450, loss is 3.3421682024002077 and perplexity is 28.280377862119888
At time: 674.4407541751862 and batch: 500, loss is 3.2063664197921753 and perplexity is 24.68921280733162
At time: 675.4223206043243 and batch: 550, loss is 3.223654475212097 and perplexity is 25.119752157558587
At time: 676.4166288375854 and batch: 600, loss is 3.2560533809661867 and perplexity is 25.946932148001366
At time: 677.4059448242188 and batch: 650, loss is 3.1189829778671263 and perplexity is 22.623359481839827
At time: 678.3954410552979 and batch: 700, loss is 3.0779130458831787 and perplexity is 21.713040958823964
At time: 679.3811435699463 and batch: 750, loss is 3.1652024030685424 and perplexity is 23.693539192574406
At time: 680.3658542633057 and batch: 800, loss is 3.099154162406921 and perplexity is 22.179183358184922
At time: 681.3580029010773 and batch: 850, loss is 3.134284029006958 and perplexity is 22.972182536349955
At time: 682.3559622764587 and batch: 900, loss is 3.1164690494537353 and perplexity is 22.566557403726108
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363558416497217 and perplexity of 78.5361019481973
finished 35 epochs...
Completing Train Step...
At time: 684.7140543460846 and batch: 50, loss is 3.560849437713623 and perplexity is 35.19307878039045
At time: 685.7049717903137 and batch: 100, loss is 3.4384661388397215 and perplexity is 31.139158372964303
At time: 686.6800909042358 and batch: 150, loss is 3.4368483352661134 and perplexity is 31.088822059382093
At time: 687.6554386615753 and batch: 200, loss is 3.3279724407196043 and perplexity is 27.88175244765892
At time: 688.6326329708099 and batch: 250, loss is 3.4489917373657226 and perplexity is 31.468647646385676
At time: 689.6167948246002 and batch: 300, loss is 3.3992795276641847 and perplexity is 29.94251951726624
At time: 690.612596988678 and batch: 350, loss is 3.3705551719665525 and perplexity is 29.094675122909976
At time: 691.6072797775269 and batch: 400, loss is 3.315118308067322 and perplexity is 27.525650298429134
At time: 692.5937592983246 and batch: 450, loss is 3.342034521102905 and perplexity is 28.276597557202734
At time: 693.5910305976868 and batch: 500, loss is 3.2062442302703857 and perplexity is 24.68619622852625
At time: 694.5938763618469 and batch: 550, loss is 3.223544645309448 and perplexity is 25.116993409123864
At time: 695.5851626396179 and batch: 600, loss is 3.2559595823287966 and perplexity is 25.944498475260808
At time: 696.5811126232147 and batch: 650, loss is 3.118898243904114 and perplexity is 22.621442596148086
At time: 697.5896489620209 and batch: 700, loss is 3.0778531551361086 and perplexity is 21.711740587520268
At time: 698.5837693214417 and batch: 750, loss is 3.1651641225814817 and perplexity is 23.692632209713906
At time: 699.5734651088715 and batch: 800, loss is 3.099137692451477 and perplexity is 22.17881807103136
At time: 700.5757973194122 and batch: 850, loss is 3.1342927551269533 and perplexity is 22.97238299524594
At time: 701.5696170330048 and batch: 900, loss is 3.1164961290359496 and perplexity is 22.567168504946764
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363537514046447 and perplexity of 78.53446036834922
finished 36 epochs...
Completing Train Step...
At time: 703.9249103069305 and batch: 50, loss is 3.560698752403259 and perplexity is 35.18777609991984
At time: 704.9173858165741 and batch: 100, loss is 3.438312678337097 and perplexity is 31.134380108715877
At time: 705.9135575294495 and batch: 150, loss is 3.4366859197616577 and perplexity is 31.083773162685034
At time: 706.8993985652924 and batch: 200, loss is 3.3278238868713377 and perplexity is 27.87761081367185
At time: 707.8852095603943 and batch: 250, loss is 3.44883647441864 and perplexity is 31.463762110692542
At time: 708.8889110088348 and batch: 300, loss is 3.3991306161880495 and perplexity is 29.93806106445135
At time: 709.880443572998 and batch: 350, loss is 3.370413408279419 and perplexity is 29.090550846831796
At time: 710.8957977294922 and batch: 400, loss is 3.314983673095703 and perplexity is 27.52194463274411
At time: 711.9015691280365 and batch: 450, loss is 3.34190167427063 and perplexity is 28.27284135029536
At time: 712.9121949672699 and batch: 500, loss is 3.206122670173645 and perplexity is 24.68319555450936
At time: 713.8986546993256 and batch: 550, loss is 3.2234350633621216 and perplexity is 25.11424119087458
At time: 714.8989436626434 and batch: 600, loss is 3.2558661031723024 and perplexity is 25.942073318780235
At time: 715.8917191028595 and batch: 650, loss is 3.1188135957717895 and perplexity is 22.619527814324332
At time: 716.8797216415405 and batch: 700, loss is 3.0777932739257814 and perplexity is 21.71044050114134
At time: 717.8808748722076 and batch: 750, loss is 3.1651254987716673 and perplexity is 23.691717127665527
At time: 718.8822510242462 and batch: 800, loss is 3.0991204595565796 and perplexity is 22.178435869083827
At time: 719.8868141174316 and batch: 850, loss is 3.1343003034591677 and perplexity is 22.972556399078996
At time: 720.8877680301666 and batch: 900, loss is 3.116521873474121 and perplexity is 22.567749491499598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3635187018407535 and perplexity of 78.53298297582327
finished 37 epochs...
Completing Train Step...
At time: 723.2644143104553 and batch: 50, loss is 3.560549426078796 and perplexity is 35.18252203094398
At time: 724.2414538860321 and batch: 100, loss is 3.438160529136658 and perplexity is 31.12964339802868
At time: 725.2336158752441 and batch: 150, loss is 3.436524977684021 and perplexity is 31.078770878201276
At time: 726.216423034668 and batch: 200, loss is 3.3276762104034425 and perplexity is 27.873494250540766
At time: 727.207355260849 and batch: 250, loss is 3.4486821842193605 and perplexity is 31.458907935051894
At time: 728.1978363990784 and batch: 300, loss is 3.398983201980591 and perplexity is 29.933648094181933
At time: 729.1938607692719 and batch: 350, loss is 3.3702725553512574 and perplexity is 29.08645364612132
At time: 730.1924946308136 and batch: 400, loss is 3.3148496103286744 and perplexity is 27.518255212005137
At time: 731.184513092041 and batch: 450, loss is 3.341769881248474 and perplexity is 28.269115432619234
At time: 732.1789445877075 and batch: 500, loss is 3.2060016775131226 and perplexity is 24.680209249673148
At time: 733.1690096855164 and batch: 550, loss is 3.2233259153366087 and perplexity is 25.111500170627536
At time: 734.1616439819336 and batch: 600, loss is 3.2557728147506713 and perplexity is 25.9396533365864
At time: 735.1730766296387 and batch: 650, loss is 3.1187291383743285 and perplexity is 22.617617508544203
At time: 736.1656332015991 and batch: 700, loss is 3.077733178138733 and perplexity is 21.709135834335136
At time: 737.1559562683105 and batch: 750, loss is 3.1650865364074705 and perplexity is 23.69079406033691
At time: 738.1422905921936 and batch: 800, loss is 3.0991024351119996 and perplexity is 22.178036118698284
At time: 739.1307108402252 and batch: 850, loss is 3.1343068265914917 and perplexity is 22.972706252592968
At time: 740.1138820648193 and batch: 900, loss is 3.1165464878082276 and perplexity is 22.56830498846218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363504070125214 and perplexity of 78.53183391196231
finished 38 epochs...
Completing Train Step...
At time: 742.4497017860413 and batch: 50, loss is 3.5604015398025513 and perplexity is 35.17731940348
At time: 743.4243924617767 and batch: 100, loss is 3.438009572029114 and perplexity is 31.1249445117765
At time: 744.4002578258514 and batch: 150, loss is 3.4363654327392577 and perplexity is 31.073812812945835
At time: 745.3757517337799 and batch: 200, loss is 3.327529468536377 and perplexity is 27.869404342040546
At time: 746.3708236217499 and batch: 250, loss is 3.448528914451599 and perplexity is 31.454086605030163
At time: 747.3590650558472 and batch: 300, loss is 3.3988370370864867 and perplexity is 29.929273165416387
At time: 748.3663508892059 and batch: 350, loss is 3.370132188796997 and perplexity is 29.082371167375804
At time: 749.3637444972992 and batch: 400, loss is 3.3147162580490113 and perplexity is 27.514585834605622
At time: 750.3742809295654 and batch: 450, loss is 3.341638994216919 and perplexity is 28.265415614149987
At time: 751.3663053512573 and batch: 500, loss is 3.2058811616897582 and perplexity is 24.677235073155884
At time: 752.3702967166901 and batch: 550, loss is 3.223217029571533 and perplexity is 25.108766034576213
At time: 753.3650288581848 and batch: 600, loss is 3.255680046081543 and perplexity is 25.93724706108393
At time: 754.3605990409851 and batch: 650, loss is 3.118644814491272 and perplexity is 22.615710383619614
At time: 755.3666586875916 and batch: 700, loss is 3.077672824859619 and perplexity is 21.70782565633797
At time: 756.3700768947601 and batch: 750, loss is 3.1650473356246946 and perplexity is 23.68986538086776
At time: 757.3678047657013 and batch: 800, loss is 3.099083857536316 and perplexity is 22.177624108380865
At time: 758.3661847114563 and batch: 850, loss is 3.134312310218811 and perplexity is 22.97283222669797
At time: 759.3688125610352 and batch: 900, loss is 3.1165699768066406 and perplexity is 22.568835101568126
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363490692556721 and perplexity of 78.53078335400224
finished 39 epochs...
Completing Train Step...
At time: 761.8128423690796 and batch: 50, loss is 3.5602547407150267 and perplexity is 35.172155784106586
At time: 762.8036432266235 and batch: 100, loss is 3.437859768867493 and perplexity is 31.120282245903155
At time: 763.7866382598877 and batch: 150, loss is 3.4362071323394776 and perplexity is 31.068894205273907
At time: 764.7732346057892 and batch: 200, loss is 3.327383613586426 and perplexity is 27.865339747892865
At time: 765.7515621185303 and batch: 250, loss is 3.448376626968384 and perplexity is 31.449296906059114
At time: 766.7334668636322 and batch: 300, loss is 3.3986920738220214 and perplexity is 29.92493483473214
At time: 767.7261435985565 and batch: 350, loss is 3.369992799758911 and perplexity is 29.078317686145507
At time: 768.7309169769287 and batch: 400, loss is 3.3145835113525393 and perplexity is 27.510933606647246
At time: 769.7351927757263 and batch: 450, loss is 3.3415089082717895 and perplexity is 28.26173891999364
At time: 770.7555863857269 and batch: 500, loss is 3.2057611322402955 and perplexity is 24.67427325597199
At time: 771.7601680755615 and batch: 550, loss is 3.2231086015701296 and perplexity is 25.10604368884929
At time: 772.7585039138794 and batch: 600, loss is 3.2555875158309937 and perplexity is 25.93484719214676
At time: 773.7490439414978 and batch: 650, loss is 3.11856062412262 and perplexity is 22.613806438773118
At time: 774.7376551628113 and batch: 700, loss is 3.0776124477386473 and perplexity is 21.706515039888306
At time: 775.7268934249878 and batch: 750, loss is 3.1650078296661377 and perplexity is 23.688929508514196
At time: 776.7111172676086 and batch: 800, loss is 3.099064426422119 and perplexity is 22.177193176620957
At time: 777.7047073841095 and batch: 850, loss is 3.134316740036011 and perplexity is 22.972933992370695
At time: 778.7065622806549 and batch: 900, loss is 3.116592345237732 and perplexity is 22.569339936647072
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363480659380351 and perplexity of 78.52999544475503
finished 40 epochs...
Completing Train Step...
At time: 781.070797920227 and batch: 50, loss is 3.5601091623306274 and perplexity is 35.16703585117652
At time: 782.055037021637 and batch: 100, loss is 3.4377110958099366 and perplexity is 31.115655842308954
At time: 783.0382175445557 and batch: 150, loss is 3.436050090789795 and perplexity is 31.06401548107222
At time: 784.0178377628326 and batch: 200, loss is 3.327238612174988 and perplexity is 27.86129952722515
At time: 785.0226337909698 and batch: 250, loss is 3.448225111961365 and perplexity is 31.444532226586986
At time: 786.0033750534058 and batch: 300, loss is 3.3985482931137083 and perplexity is 29.92063251570803
At time: 786.9922983646393 and batch: 350, loss is 3.3698539543151855 and perplexity is 29.07428057449735
At time: 787.971018075943 and batch: 400, loss is 3.3144513845443724 and perplexity is 27.507298914925503
At time: 788.9659831523895 and batch: 450, loss is 3.3413795757293703 and perplexity is 28.258083993801474
At time: 789.9543352127075 and batch: 500, loss is 3.2056416130065917 and perplexity is 24.671324381967327
At time: 790.949547290802 and batch: 550, loss is 3.2230003595352175 and perplexity is 25.10332630666196
At time: 791.9382560253143 and batch: 600, loss is 3.255495324134827 and perplexity is 25.932456324805038
At time: 792.9351227283478 and batch: 650, loss is 3.118476619720459 and perplexity is 22.61190685927029
At time: 793.9316217899323 and batch: 700, loss is 3.0775518131256105 and perplexity is 21.705198913650293
At time: 794.9248526096344 and batch: 750, loss is 3.164968180656433 and perplexity is 23.68799028453799
At time: 795.9196462631226 and batch: 800, loss is 3.0990444564819337 and perplexity is 22.176750303821823
At time: 796.9189124107361 and batch: 850, loss is 3.1343203830718993 and perplexity is 22.97301768374614
At time: 797.9118111133575 and batch: 900, loss is 3.116613655090332 and perplexity is 22.569820891078912
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363473134498074 and perplexity of 78.5294045180074
finished 41 epochs...
Completing Train Step...
At time: 800.2766449451447 and batch: 50, loss is 3.55996461391449 and perplexity is 35.1619528792206
At time: 801.2636580467224 and batch: 100, loss is 3.4375634384155274 and perplexity is 31.111061724827998
At time: 802.2452783584595 and batch: 150, loss is 3.4358940887451173 and perplexity is 31.059169809118462
At time: 803.2280850410461 and batch: 200, loss is 3.32709445476532 and perplexity is 27.85728340393924
At time: 804.2115378379822 and batch: 250, loss is 3.4480746173858643 and perplexity is 31.439800351127456
At time: 805.2078576087952 and batch: 300, loss is 3.398405518531799 and perplexity is 29.91636091485541
At time: 806.1910378932953 and batch: 350, loss is 3.369716076850891 and perplexity is 29.070272162756723
At time: 807.1743879318237 and batch: 400, loss is 3.3143198251724244 and perplexity is 27.503680309992188
At time: 808.1603133678436 and batch: 450, loss is 3.3412511253356936 and perplexity is 28.254454464900117
At time: 809.1529557704926 and batch: 500, loss is 3.2055225563049317 and perplexity is 24.668387270305708
At time: 810.1562762260437 and batch: 550, loss is 3.2228924655914306 and perplexity is 25.100617955894517
At time: 811.1383852958679 and batch: 600, loss is 3.2554033899307253 and perplexity is 25.930072354658297
At time: 812.1241128444672 and batch: 650, loss is 3.1183927345275877 and perplexity is 22.61001013465688
At time: 813.1172020435333 and batch: 700, loss is 3.0774910163879396 and perplexity is 21.703879348478882
At time: 814.1162900924683 and batch: 750, loss is 3.164928088188171 and perplexity is 23.687040593577176
At time: 815.127959728241 and batch: 800, loss is 3.0990238428115844 and perplexity is 22.17629316431332
At time: 816.124475479126 and batch: 850, loss is 3.1343230867385863 and perplexity is 22.973079795212715
At time: 817.1176199913025 and batch: 900, loss is 3.1166339445114137 and perplexity is 22.570278824324298
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3634664457138275 and perplexity of 78.52887925352026
finished 42 epochs...
Completing Train Step...
At time: 819.4661817550659 and batch: 50, loss is 3.5598210859298707 and perplexity is 35.15690651714433
At time: 820.4496178627014 and batch: 100, loss is 3.4374167346954345 and perplexity is 31.106497951106405
At time: 821.4343264102936 and batch: 150, loss is 3.435739207267761 and perplexity is 31.054359691521647
At time: 822.4208483695984 and batch: 200, loss is 3.326950807571411 and perplexity is 27.853282070745365
At time: 823.4052391052246 and batch: 250, loss is 3.447924828529358 and perplexity is 31.43509137206917
At time: 824.3914289474487 and batch: 300, loss is 3.3982639265060426 and perplexity is 29.912125296582175
At time: 825.3842089176178 and batch: 350, loss is 3.3695785999298096 and perplexity is 29.066275945944874
At time: 826.3750784397125 and batch: 400, loss is 3.3141888999938964 and perplexity is 27.50007962145296
At time: 827.373605966568 and batch: 450, loss is 3.341123242378235 and perplexity is 28.25084143272931
At time: 828.3725757598877 and batch: 500, loss is 3.2054040002822877 and perplexity is 24.665462857782682
At time: 829.3680472373962 and batch: 550, loss is 3.2227848529815675 and perplexity is 25.097916958220416
At time: 830.3582336902618 and batch: 600, loss is 3.2553117084503174 and perplexity is 25.92769515621194
At time: 831.3477964401245 and batch: 650, loss is 3.1183091163635255 and perplexity is 22.60811960616235
At time: 832.3494672775269 and batch: 700, loss is 3.0774299716949463 and perplexity is 21.702554482265725
At time: 833.3578815460205 and batch: 750, loss is 3.164887614250183 and perplexity is 23.686081905166148
At time: 834.3706750869751 and batch: 800, loss is 3.099002685546875 and perplexity is 22.175823979571916
At time: 835.3713157176971 and batch: 850, loss is 3.1343249225616456 and perplexity is 22.97312196976106
At time: 836.3692691326141 and batch: 900, loss is 3.1166532039642334 and perplexity is 22.57071351973043
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363463101321703 and perplexity of 78.52861662259414
finished 43 epochs...
Completing Train Step...
At time: 838.72305727005 and batch: 50, loss is 3.559678568840027 and perplexity is 35.151896414160724
At time: 839.7010979652405 and batch: 100, loss is 3.437271041870117 and perplexity is 31.101966287657618
At time: 840.6741425991058 and batch: 150, loss is 3.435585298538208 and perplexity is 31.04958052226181
At time: 841.6529064178467 and batch: 200, loss is 3.3268080949783325 and perplexity is 27.849307340264094
At time: 842.6273469924927 and batch: 250, loss is 3.447775902748108 and perplexity is 31.430410225108304
At time: 843.6034939289093 and batch: 300, loss is 3.3981232118606566 and perplexity is 29.90791651860362
At time: 844.5825550556183 and batch: 350, loss is 3.369441742897034 and perplexity is 29.06229829385611
At time: 845.5625746250153 and batch: 400, loss is 3.3140584945678713 and perplexity is 27.496493695671372
At time: 846.5474829673767 and batch: 450, loss is 3.3409959745407103 and perplexity is 28.247246238013098
At time: 847.5418424606323 and batch: 500, loss is 3.2052858448028565 and perplexity is 24.662548670360326
At time: 848.5318057537079 and batch: 550, loss is 3.2226775312423706 and perplexity is 25.09522355065542
At time: 849.5382959842682 and batch: 600, loss is 3.255220365524292 and perplexity is 25.92532695283224
At time: 850.5472419261932 and batch: 650, loss is 3.118225684165955 and perplexity is 22.606233439745267
At time: 851.5471291542053 and batch: 700, loss is 3.077368764877319 and perplexity is 21.70122617862252
At time: 852.5513277053833 and batch: 750, loss is 3.1648470306396486 and perplexity is 23.685120657948595
At time: 853.5534574985504 and batch: 800, loss is 3.0989809083938598 and perplexity is 22.17534105851822
At time: 854.5521111488342 and batch: 850, loss is 3.1343258476257323 and perplexity is 22.973143221380983
At time: 855.5534887313843 and batch: 900, loss is 3.116671543121338 and perplexity is 22.571127451387195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363460174978596 and perplexity of 78.5283868212544
finished 44 epochs...
Completing Train Step...
At time: 857.9491095542908 and batch: 50, loss is 3.559536924362183 and perplexity is 35.14691769476029
At time: 858.9380125999451 and batch: 100, loss is 3.4371261978149414 and perplexity is 31.097461678977346
At time: 859.9200410842896 and batch: 150, loss is 3.4354323291778566 and perplexity is 31.04483125104589
At time: 860.9106194972992 and batch: 200, loss is 3.326665930747986 and perplexity is 27.845348446333603
At time: 861.8936469554901 and batch: 250, loss is 3.4476276350021364 and perplexity is 31.42575045448465
At time: 862.8826847076416 and batch: 300, loss is 3.397983417510986 and perplexity is 29.903735853087422
At time: 863.8810968399048 and batch: 350, loss is 3.369305624961853 and perplexity is 29.058342663043003
At time: 864.8734486103058 and batch: 400, loss is 3.313928608894348 and perplexity is 27.492922526995027
At time: 865.8666548728943 and batch: 450, loss is 3.340869359970093 and perplexity is 28.243669951469318
At time: 866.8769812583923 and batch: 500, loss is 3.2051682233810426 and perplexity is 24.659647996914174
At time: 867.8842885494232 and batch: 550, loss is 3.2225705194473266 and perplexity is 25.09253820942039
At time: 868.8927872180939 and batch: 600, loss is 3.255129170417786 and perplexity is 25.922962797680896
At time: 869.9015634059906 and batch: 650, loss is 3.118142318725586 and perplexity is 22.604348939691665
At time: 870.9008550643921 and batch: 700, loss is 3.077307448387146 and perplexity is 21.699895576395136
At time: 871.8903868198395 and batch: 750, loss is 3.164806032180786 and perplexity is 23.684149624409223
At time: 872.8879964351654 and batch: 800, loss is 3.0989585590362547 and perplexity is 22.174845459429072
At time: 873.8874657154083 and batch: 850, loss is 3.1343259811401367 and perplexity is 22.973146288626722
At time: 874.8827605247498 and batch: 900, loss is 3.1166889476776123 and perplexity is 22.571520295263724
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363458920831549 and perplexity of 78.52828833517174
finished 45 epochs...
Completing Train Step...
At time: 877.2983469963074 and batch: 50, loss is 3.5593961095809936 and perplexity is 35.14196883767992
At time: 878.2877168655396 and batch: 100, loss is 3.4369823265075685 and perplexity is 31.09298796833663
At time: 879.2587928771973 and batch: 150, loss is 3.4352804136276247 and perplexity is 31.04011541663789
At time: 880.2403819561005 and batch: 200, loss is 3.3265245008468627 and perplexity is 27.8414105599301
At time: 881.2146067619324 and batch: 250, loss is 3.4474802827835083 and perplexity is 31.421120141584943
At time: 882.2046349048615 and batch: 300, loss is 3.397844591140747 and perplexity is 29.899584714132793
At time: 883.185708284378 and batch: 350, loss is 3.3691700172424315 and perplexity is 29.05440239463574
At time: 884.2058668136597 and batch: 400, loss is 3.3137991857528686 and perplexity is 27.48936453684123
At time: 885.1939113140106 and batch: 450, loss is 3.340743479728699 and perplexity is 28.24011485524085
At time: 886.1763956546783 and batch: 500, loss is 3.2050508499145507 and perplexity is 24.65675377840186
At time: 887.1585812568665 and batch: 550, loss is 3.2224636554718016 and perplexity is 25.089856864303467
At time: 888.1446282863617 and batch: 600, loss is 3.2550381803512574 and perplexity is 25.920604172878658
At time: 889.1268858909607 and batch: 650, loss is 3.1180591773986817 and perplexity is 22.6024696622509
At time: 890.1094481945038 and batch: 700, loss is 3.0772459840774538 and perplexity is 21.6985618482819
At time: 891.0985670089722 and batch: 750, loss is 3.1647648096084593 and perplexity is 23.6831733229613
At time: 892.0859661102295 and batch: 800, loss is 3.0989357471466064 and perplexity is 22.174339615071133
At time: 893.0734779834747 and batch: 850, loss is 3.1343252992630006 and perplexity is 22.973130623768864
At time: 894.0731098651886 and batch: 900, loss is 3.116705574989319 and perplexity is 22.57189560208753
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363461011076627 and perplexity of 78.52845247871143
Annealing...
finished 46 epochs...
Completing Train Step...
At time: 896.4367561340332 and batch: 50, loss is 3.559285306930542 and perplexity is 35.13807523010561
At time: 897.4200186729431 and batch: 100, loss is 3.4369002628326415 and perplexity is 31.090436468173657
At time: 898.4104588031769 and batch: 150, loss is 3.4352039241790773 and perplexity is 31.037741266126712
At time: 899.3930230140686 and batch: 200, loss is 3.3264476823806763 and perplexity is 27.83927190761947
At time: 900.3811266422272 and batch: 250, loss is 3.4474208831787108 and perplexity is 31.419253794896914
At time: 901.365133523941 and batch: 300, loss is 3.3977877473831177 and perplexity is 29.897885157691128
At time: 902.3563275337219 and batch: 350, loss is 3.3690934944152833 and perplexity is 29.052179154688787
At time: 903.352897644043 and batch: 400, loss is 3.313686852455139 and perplexity is 27.486276739304806
At time: 904.3591945171356 and batch: 450, loss is 3.3406450176239013 and perplexity is 28.237334410978924
At time: 905.3718023300171 and batch: 500, loss is 3.204933595657349 and perplexity is 24.65386283854338
At time: 906.3585817813873 and batch: 550, loss is 3.2223161935806273 and perplexity is 25.08615733933714
At time: 907.3538513183594 and batch: 600, loss is 3.2548944997787475 and perplexity is 25.916880153172347
At time: 908.370982170105 and batch: 650, loss is 3.1179147577285766 and perplexity is 22.59920565673769
At time: 909.3514549732208 and batch: 700, loss is 3.0770595979690554 and perplexity is 21.69451791465939
At time: 910.3399691581726 and batch: 750, loss is 3.1645419454574584 and perplexity is 23.677895780754994
At time: 911.3368873596191 and batch: 800, loss is 3.0987149143218993 and perplexity is 22.169443333667648
At time: 912.3254110813141 and batch: 850, loss is 3.1340736770629882 and perplexity is 22.96735080129646
At time: 913.3239502906799 and batch: 900, loss is 3.1164578771591187 and perplexity is 22.566305284906687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36345975692958 and perplexity of 78.52835399254643
Annealing...
finished 47 epochs...
Completing Train Step...
At time: 915.6620247364044 and batch: 50, loss is 3.559263515472412 and perplexity is 35.137309528553374
At time: 916.6400310993195 and batch: 100, loss is 3.4368845081329344 and perplexity is 31.08994665154181
At time: 917.6247360706329 and batch: 150, loss is 3.4351894664764404 and perplexity is 31.037292534936785
At time: 918.6137924194336 and batch: 200, loss is 3.326433262825012 and perplexity is 27.838870480582752
At time: 919.5983431339264 and batch: 250, loss is 3.4474101781845095 and perplexity is 31.4189174537675
At time: 920.5907597541809 and batch: 300, loss is 3.397777500152588 and perplexity is 29.89757878873928
At time: 921.5820932388306 and batch: 350, loss is 3.369079384803772 and perplexity is 29.05176924261922
At time: 922.5766544342041 and batch: 400, loss is 3.3136645698547365 and perplexity is 27.48566428040728
At time: 923.5769076347351 and batch: 450, loss is 3.3406260061264037 and perplexity is 28.236797582069407
At time: 924.5698618888855 and batch: 500, loss is 3.2049105072021487 and perplexity is 24.653293625506873
At time: 925.5706622600555 and batch: 550, loss is 3.22228627204895 and perplexity is 25.085406734315335
At time: 926.5718324184418 and batch: 600, loss is 3.2548651123046874 and perplexity is 25.916118532720233
At time: 927.5637969970703 and batch: 650, loss is 3.117885179519653 and perplexity is 22.598537222596864
At time: 928.5646359920502 and batch: 700, loss is 3.0770206117630003 and perplexity is 21.6936721442005
At time: 929.5541019439697 and batch: 750, loss is 3.1644951248168947 and perplexity is 23.676787192459948
At time: 930.5480787754059 and batch: 800, loss is 3.0986682033538817 and perplexity is 22.168407801694663
At time: 931.542510509491 and batch: 850, loss is 3.134019913673401 and perplexity is 22.96611603186053
At time: 932.540461063385 and batch: 900, loss is 3.1164051294326782 and perplexity is 22.565114995001565
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.36345975692958 and perplexity of 78.52835399254643
Annealing...
finished 48 epochs...
Completing Train Step...
At time: 934.9170527458191 and batch: 50, loss is 3.559261236190796 and perplexity is 35.13722944082101
At time: 935.9057278633118 and batch: 100, loss is 3.4368836498260498 and perplexity is 31.089919966838004
At time: 936.8812844753265 and batch: 150, loss is 3.43518883228302 and perplexity is 31.03727285129631
At time: 937.8544774055481 and batch: 200, loss is 3.3264324998855592 and perplexity is 27.838849241218238
At time: 938.8276724815369 and batch: 250, loss is 3.4474101400375368 and perplexity is 31.418916255230936
At time: 939.8050887584686 and batch: 300, loss is 3.3977775287628176 and perplexity is 29.89757964411589
At time: 940.7752342224121 and batch: 350, loss is 3.369078574180603 and perplexity is 29.051745692591517
At time: 941.7804918289185 and batch: 400, loss is 3.313661842346191 and perplexity is 27.485589313125324
At time: 942.7784290313721 and batch: 450, loss is 3.340623936653137 and perplexity is 28.236739146832143
At time: 943.7653822898865 and batch: 500, loss is 3.2049077129364014 and perplexity is 24.653224737749184
At time: 944.7595953941345 and batch: 550, loss is 3.222282075881958 and perplexity is 25.08530147198046
At time: 945.7499585151672 and batch: 600, loss is 3.2548609447479246 and perplexity is 25.91601052605024
At time: 946.7410471439362 and batch: 650, loss is 3.1178810834884643 and perplexity is 22.59844465847315
At time: 947.7260267734528 and batch: 700, loss is 3.0770145654678345 and perplexity is 21.69354097825202
At time: 948.7172400951385 and batch: 750, loss is 3.1644872999191285 and perplexity is 23.676601924745587
At time: 949.7023596763611 and batch: 800, loss is 3.0986602020263674 and perplexity is 22.16823042571299
At time: 950.6911416053772 and batch: 850, loss is 3.134010367393494 and perplexity is 22.965896791934977
At time: 951.678558588028 and batch: 900, loss is 3.1163955545425415 and perplexity is 22.56489893753893
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363460593027611 and perplexity of 78.52841964997602
Annealing...
finished 49 epochs...
Completing Train Step...
At time: 954.0407662391663 and batch: 50, loss is 3.5592613363265992 and perplexity is 35.137232959315874
At time: 955.0231847763062 and batch: 100, loss is 3.436883931159973 and perplexity is 31.089928713488394
At time: 956.0116496086121 and batch: 150, loss is 3.4351891708374023 and perplexity is 31.03728335910283
At time: 957.0044009685516 and batch: 200, loss is 3.3264327526092528 and perplexity is 27.838856276755934
At time: 958.0196878910065 and batch: 250, loss is 3.447410545349121 and perplexity is 31.41892898968424
At time: 959.0055894851685 and batch: 300, loss is 3.397778024673462 and perplexity is 29.897594470647558
At time: 959.9926319122314 and batch: 350, loss is 3.3690788984298705 and perplexity is 29.0517551126003
At time: 960.9894516468048 and batch: 400, loss is 3.3136619234085085 and perplexity is 27.485591541170976
At time: 961.9793336391449 and batch: 450, loss is 3.34062415599823 and perplexity is 28.236745340422992
At time: 962.9678118228912 and batch: 500, loss is 3.2049077701568605 and perplexity is 24.65322614841806
At time: 963.9637567996979 and batch: 550, loss is 3.222281928062439 and perplexity is 25.085297763883542
At time: 964.9564604759216 and batch: 600, loss is 3.254860668182373 and perplexity is 25.91600335857548
At time: 965.946081161499 and batch: 650, loss is 3.117880845069885 and perplexity is 22.598439270584727
At time: 966.9387578964233 and batch: 700, loss is 3.0770140171051024 and perplexity is 21.693529082325885
At time: 967.9324953556061 and batch: 750, loss is 3.1644863653182984 and perplexity is 23.676579796584114
At time: 968.9273753166199 and batch: 800, loss is 3.0986592102050783 and perplexity is 22.168208438801017
At time: 969.913102388382 and batch: 850, loss is 3.1340092325210573 and perplexity is 22.96587072858652
At time: 970.9137029647827 and batch: 900, loss is 3.1163942861557006 and perplexity is 22.564870316536204
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.363461429125642 and perplexity of 78.5284853074605
Annealing...
Model not improving. Stopping early with 78.52828833517174 lossat 49 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -78.52828833517174
langmodel
SETTINGS FOR THIS RUN
{'batch_size': 32, 'lr': 1.4404124831850984, 'dropout': 0.4514114234112402, 'hidden_size': 300, 'clip': 0.016277867931793033, 'tune_wordvecs': True, 'anneal': 4.481110263320149, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.5550563291447015}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.08047505219777425 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.5780315399169922 and batch: 50, loss is 9.072147388458252 and perplexity is 8709.306002757632
At time: 2.9045324325561523 and batch: 100, loss is 8.796847591400146 and perplexity is 6613.363088303984
At time: 4.2238593101501465 and batch: 150, loss is 8.245812215805053 and perplexity is 3811.630067281105
At time: 5.537420749664307 and batch: 200, loss is 7.518232183456421 and perplexity is 1841.3093180755093
At time: 6.843502998352051 and batch: 250, loss is 7.307461481094361 and perplexity is 1491.3864670749304
At time: 8.15190601348877 and batch: 300, loss is 7.079841117858887 and perplexity is 1187.779786520766
At time: 9.453132390975952 and batch: 350, loss is 6.997023429870605 and perplexity is 1093.3738061779316
At time: 10.755727529525757 and batch: 400, loss is 6.8572916984558105 and perplexity is 950.7885552605394
At time: 12.075519561767578 and batch: 450, loss is 6.81665994644165 and perplexity is 912.9306747501306
At time: 13.389288663864136 and batch: 500, loss is 6.76389048576355 and perplexity is 866.0048298696342
At time: 14.703270673751831 and batch: 550, loss is 6.760189342498779 and perplexity is 862.8055460822285
At time: 16.034528970718384 and batch: 600, loss is 6.699932661056518 and perplexity is 812.3511204594973
At time: 17.35936999320984 and batch: 650, loss is 6.627507753372193 and perplexity is 755.5966887752551
At time: 18.67703652381897 and batch: 700, loss is 6.70060640335083 and perplexity is 812.8986201833316
At time: 19.99986457824707 and batch: 750, loss is 6.634442958831787 and perplexity is 760.8551201311781
At time: 21.31595015525818 and batch: 800, loss is 6.643476686477661 and perplexity is 767.7596177848205
At time: 22.632686853408813 and batch: 850, loss is 6.66329662322998 and perplexity is 783.1283655492934
At time: 23.958914518356323 and batch: 900, loss is 6.554677085876465 and perplexity is 702.5222588322963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.56226589255137 and perplexity of 707.8738448320476
finished 1 epochs...
Completing Train Step...
At time: 26.46890640258789 and batch: 50, loss is 6.482459936141968 and perplexity is 653.5767274152005
At time: 27.39029049873352 and batch: 100, loss is 6.443975744247436 and perplexity is 628.9021902725726
At time: 28.32183289527893 and batch: 150, loss is 6.4668510627746585 and perplexity is 643.454336117981
At time: 29.247565031051636 and batch: 200, loss is 6.385615215301514 and perplexity is 593.2495965816187
At time: 30.16510510444641 and batch: 250, loss is 6.4644615364074705 and perplexity is 641.9186205628749
At time: 31.11609935760498 and batch: 300, loss is 6.378376493453979 and perplexity is 588.9707332007517
At time: 32.08946633338928 and batch: 350, loss is 6.3898732948303225 and perplexity is 595.7810863617822
At time: 33.08315396308899 and batch: 400, loss is 6.28356463432312 and perplexity is 535.6948205754143
At time: 34.064823627471924 and batch: 450, loss is 6.25485071182251 and perplexity is 520.5316597337988
At time: 35.043973445892334 and batch: 500, loss is 6.210523834228516 and perplexity is 497.96203253491495
At time: 36.03496193885803 and batch: 550, loss is 6.204686260223388 and perplexity is 495.06361042588327
At time: 37.02409243583679 and batch: 600, loss is 6.136168327331543 and perplexity is 462.27887167958744
At time: 38.028648376464844 and batch: 650, loss is 6.053987693786621 and perplexity is 425.8076394856708
At time: 39.040289878845215 and batch: 700, loss is 6.146855688095092 and perplexity is 467.24590773000966
At time: 40.03633165359497 and batch: 750, loss is 6.084530220031739 and perplexity is 439.0135244622557
At time: 41.03209662437439 and batch: 800, loss is 6.097052917480469 and perplexity is 444.54572474467466
At time: 42.028719902038574 and batch: 850, loss is 6.123432874679565 and perplexity is 456.4288712672075
At time: 43.048619747161865 and batch: 900, loss is 5.997501621246338 and perplexity is 402.42213359892486
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.104381404510916 and perplexity of 447.8155390846603
finished 2 epochs...
Completing Train Step...
At time: 45.42650866508484 and batch: 50, loss is 6.040511112213135 and perplexity is 420.1077021998853
At time: 46.41759133338928 and batch: 100, loss is 6.012574205398559 and perplexity is 408.53361729692114
At time: 47.397823333740234 and batch: 150, loss is 6.061794319152832 and perplexity is 429.14476911763876
At time: 48.390384912490845 and batch: 200, loss is 5.983018941879273 and perplexity is 396.635983482023
At time: 49.370832681655884 and batch: 250, loss is 6.089227867126465 and perplexity is 441.08070671481545
At time: 50.366180181503296 and batch: 300, loss is 6.0103496742248534 and perplexity is 407.6258316028761
At time: 51.368839502334595 and batch: 350, loss is 6.052205352783203 and perplexity is 425.0493810085668
At time: 52.35578727722168 and batch: 400, loss is 5.958962135314941 and perplexity is 387.2080460657254
At time: 53.34458374977112 and batch: 450, loss is 5.966028671264649 and perplexity is 389.9539562531699
At time: 54.344759702682495 and batch: 500, loss is 5.96120099067688 and perplexity is 388.0759200354746
At time: 55.34005403518677 and batch: 550, loss is 5.996622953414917 and perplexity is 402.0686935164349
At time: 56.331096172332764 and batch: 600, loss is 5.958363637924195 and perplexity is 386.97637239543803
At time: 57.32746744155884 and batch: 650, loss is 5.890196905136109 and perplexity is 361.4764539355921
At time: 58.3216233253479 and batch: 700, loss is 6.001660776138306 and perplexity is 404.09935507868664
At time: 59.314268827438354 and batch: 750, loss is 5.958727073669434 and perplexity is 387.11703900181766
At time: 60.299941539764404 and batch: 800, loss is 5.980119581222534 and perplexity is 395.48765822543317
At time: 61.29198360443115 and batch: 850, loss is 6.021330099105835 and perplexity is 412.1264002852365
At time: 62.29082775115967 and batch: 900, loss is 5.898468894958496 and perplexity is 364.47898481515267
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.012113806319563 and perplexity of 408.3455720870211
finished 3 epochs...
Completing Train Step...
At time: 64.6549220085144 and batch: 50, loss is 5.955124044418335 and perplexity is 385.7247547134023
At time: 65.64178681373596 and batch: 100, loss is 5.926214723587036 and perplexity is 374.7333564495734
At time: 66.61908149719238 and batch: 150, loss is 5.97783935546875 and perplexity is 394.5868844559099
At time: 67.60979962348938 and batch: 200, loss is 5.901802835464477 and perplexity is 365.69616394017265
At time: 68.59773898124695 and batch: 250, loss is 6.018615875244141 and perplexity is 411.00931367291645
At time: 69.58027148246765 and batch: 300, loss is 5.949159851074219 and perplexity is 383.43106450380276
At time: 70.57396602630615 and batch: 350, loss is 5.9909193801879885 and perplexity is 399.7819926625964
At time: 71.56707692146301 and batch: 400, loss is 5.898180980682373 and perplexity is 364.3740612173031
At time: 72.5601007938385 and batch: 450, loss is 5.910383787155151 and perplexity is 368.8476872531429
At time: 73.54444193840027 and batch: 500, loss is 5.904819650650024 and perplexity is 366.801067487751
At time: 74.53713917732239 and batch: 550, loss is 5.948561773300171 and perplexity is 383.2018114686497
At time: 75.53440070152283 and batch: 600, loss is 5.907463092803955 and perplexity is 367.77196758540737
At time: 76.51793766021729 and batch: 650, loss is 5.83735538482666 and perplexity is 342.87137773981766
At time: 77.51724886894226 and batch: 700, loss is 5.950377063751221 and perplexity is 383.898065818574
At time: 78.52240061759949 and batch: 750, loss is 5.912307348251343 and perplexity is 369.5578711368296
At time: 79.5136296749115 and batch: 800, loss is 5.9319251441955565 and perplexity is 376.8793629993835
At time: 80.51387691497803 and batch: 850, loss is 5.97552752494812 and perplexity is 393.6757200879964
At time: 81.52134037017822 and batch: 900, loss is 5.854060344696045 and perplexity is 348.6471379540168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.967945255645334 and perplexity of 390.7020525807811
finished 4 epochs...
Completing Train Step...
At time: 83.89865827560425 and batch: 50, loss is 5.912545022964477 and perplexity is 369.64571613669267
At time: 84.89832401275635 and batch: 100, loss is 5.879975738525391 and perplexity is 357.80056083432584
At time: 85.87975668907166 and batch: 150, loss is 5.9305737113952635 and perplexity is 376.37037987218497
At time: 86.87020945549011 and batch: 200, loss is 5.854912414550781 and perplexity is 348.944336269089
At time: 87.85331153869629 and batch: 250, loss is 5.974974985122681 and perplexity is 393.4582586579222
At time: 88.83543872833252 and batch: 300, loss is 5.909392213821411 and perplexity is 368.4821289911151
At time: 89.81215906143188 and batch: 350, loss is 5.946324548721313 and perplexity is 382.3454612384425
At time: 90.79745745658875 and batch: 400, loss is 5.852777338027954 and perplexity is 348.2001081836206
At time: 91.78370976448059 and batch: 450, loss is 5.8682212638854985 and perplexity is 353.6194248854727
At time: 92.7640700340271 and batch: 500, loss is 5.859690198898315 and perplexity is 350.6155061226476
At time: 93.74868083000183 and batch: 550, loss is 5.9096627616882325 and perplexity is 368.58183453202804
At time: 94.74094676971436 and batch: 600, loss is 5.864407749176025 and perplexity is 352.27346006661895
At time: 95.73425531387329 and batch: 650, loss is 5.793332719802857 and perplexity is 328.10468524757846
At time: 96.72428035736084 and batch: 700, loss is 5.907478866577148 and perplexity is 367.7777687827642
At time: 97.72036504745483 and batch: 750, loss is 5.871582241058349 and perplexity is 354.8099312120035
At time: 98.71233892440796 and batch: 800, loss is 5.889869470596313 and perplexity is 361.35811343469175
At time: 99.71534204483032 and batch: 850, loss is 5.935867719650268 and perplexity is 378.368171266307
At time: 100.71146941184998 and batch: 900, loss is 5.814623785018921 and perplexity is 335.165280579177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.928138210348887 and perplexity of 375.4548447640921
finished 5 epochs...
Completing Train Step...
At time: 103.05667686462402 and batch: 50, loss is 5.873817396163941 and perplexity is 355.6038734026979
At time: 104.05141401290894 and batch: 100, loss is 5.838570184707642 and perplexity is 343.2881509454544
At time: 105.04903483390808 and batch: 150, loss is 5.889384660720825 and perplexity is 361.1829659127657
At time: 106.0527675151825 and batch: 200, loss is 5.81438437461853 and perplexity is 335.08504812978117
At time: 107.0492491722107 and batch: 250, loss is 5.935352840423584 and perplexity is 378.17340749909505
At time: 108.05852317810059 and batch: 300, loss is 5.874056968688965 and perplexity is 355.6890765263136
At time: 109.07194709777832 and batch: 350, loss is 5.906033535003662 and perplexity is 367.2465919173318
At time: 110.08249354362488 and batch: 400, loss is 5.811226291656494 and perplexity is 334.02849097415503
At time: 111.07925987243652 and batch: 450, loss is 5.830440368652344 and perplexity is 340.5085953713617
At time: 112.08460521697998 and batch: 500, loss is 5.818952226638794 and perplexity is 336.6191681930711
At time: 113.08926153182983 and batch: 550, loss is 5.874211177825928 and perplexity is 355.74393126127393
At time: 114.09417486190796 and batch: 600, loss is 5.825901918411255 and perplexity is 338.9667155735512
At time: 115.09687829017639 and batch: 650, loss is 5.755317554473877 and perplexity is 315.8658362092354
At time: 116.09267115592957 and batch: 700, loss is 5.869387836456299 and perplexity is 354.03218831948584
At time: 117.10069537162781 and batch: 750, loss is 5.8339494037628175 and perplexity is 341.7055508391105
At time: 118.09776329994202 and batch: 800, loss is 5.851327629089355 and perplexity is 347.69568509603897
At time: 119.0899600982666 and batch: 850, loss is 5.899545841217041 and perplexity is 364.87172053384063
At time: 120.07898330688477 and batch: 900, loss is 5.779911708831787 and perplexity is 323.730606601969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.89139755458048 and perplexity of 361.9107210883024
finished 6 epochs...
Completing Train Step...
At time: 122.41909742355347 and batch: 50, loss is 5.839172143936157 and perplexity is 343.49485862451155
At time: 123.40943551063538 and batch: 100, loss is 5.801791563034057 and perplexity is 330.8918427889836
At time: 124.39438700675964 and batch: 150, loss is 5.854098138809204 and perplexity is 348.66031501240724
At time: 125.37571883201599 and batch: 200, loss is 5.778466854095459 and perplexity is 323.26320064989085
At time: 126.35217213630676 and batch: 250, loss is 5.899998760223388 and perplexity is 365.0370153006943
At time: 127.33596158027649 and batch: 300, loss is 5.8425721836090085 and perplexity is 344.6647424699849
At time: 128.32033610343933 and batch: 350, loss is 5.871375579833984 and perplexity is 354.73661333344444
At time: 129.3131926059723 and batch: 400, loss is 5.774938278198242 and perplexity is 322.12455199714424
At time: 130.30148267745972 and batch: 450, loss is 5.797403326034546 and perplexity is 329.44299223641804
At time: 131.2783715724945 and batch: 500, loss is 5.78310305595398 and perplexity is 324.76539364579185
At time: 132.2528476715088 and batch: 550, loss is 5.843083429336548 and perplexity is 344.84099589757295
At time: 133.22969841957092 and batch: 600, loss is 5.793045721054077 and perplexity is 328.0105331248606
At time: 134.2091236114502 and batch: 650, loss is 5.722459754943848 and perplexity is 305.655837502828
At time: 135.18339776992798 and batch: 700, loss is 5.8372362613677975 and perplexity is 342.8305361480005
At time: 136.1696755886078 and batch: 750, loss is 5.801844882965088 and perplexity is 330.9094863895932
At time: 137.15194296836853 and batch: 800, loss is 5.818913087844849 and perplexity is 336.60599358263
At time: 138.1318542957306 and batch: 850, loss is 5.8692336273193355 and perplexity is 353.97759753057534
At time: 139.12923669815063 and batch: 900, loss is 5.751017999649048 and perplexity is 314.5106691244059
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.862530852017337 and perplexity of 351.6128991067486
finished 7 epochs...
Completing Train Step...
At time: 141.4891197681427 and batch: 50, loss is 5.810697593688965 and perplexity is 333.8519374657332
At time: 142.46855092048645 and batch: 100, loss is 5.771490287780762 and perplexity is 321.01578224085046
At time: 143.4464454650879 and batch: 150, loss is 5.8247775459289555 and perplexity is 338.585804909009
At time: 144.43003487586975 and batch: 200, loss is 5.748505392074585 and perplexity is 313.721419187823
At time: 145.41745257377625 and batch: 250, loss is 5.871132516860962 and perplexity is 354.6504004755692
At time: 146.40880036354065 and batch: 300, loss is 5.817011833190918 and perplexity is 335.9666278619126
At time: 147.4068694114685 and batch: 350, loss is 5.8431547737121585 and perplexity is 344.86559924075493
At time: 148.41056966781616 and batch: 400, loss is 5.746050386428833 and perplexity is 312.9521759671043
At time: 149.4018452167511 and batch: 450, loss is 5.770668659210205 and perplexity is 320.75213482751724
At time: 150.39317917823792 and batch: 500, loss is 5.7543222618103025 and perplexity is 315.55161365742197
At time: 151.38279461860657 and batch: 550, loss is 5.817690048217774 and perplexity is 336.1945627631511
At time: 152.38037419319153 and batch: 600, loss is 5.766239995956421 and perplexity is 319.3347724613148
At time: 153.37403392791748 and batch: 650, loss is 5.6945979785919185 and perplexity is 297.25726577679876
At time: 154.37177085876465 and batch: 700, loss is 5.81049069404602 and perplexity is 333.78287076425954
At time: 155.36127829551697 and batch: 750, loss is 5.77582612991333 and perplexity is 322.4106778329684
At time: 156.35657691955566 and batch: 800, loss is 5.792453680038452 and perplexity is 327.81639491025726
At time: 157.34733819961548 and batch: 850, loss is 5.84430627822876 and perplexity is 345.2629422631383
At time: 158.3419804573059 and batch: 900, loss is 5.726331720352173 and perplexity is 306.84162050642806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.839452874170591 and perplexity of 343.5913015532896
finished 8 epochs...
Completing Train Step...
At time: 160.71831011772156 and batch: 50, loss is 5.786779117584229 and perplexity is 325.96144828659254
At time: 161.70430254936218 and batch: 100, loss is 5.746354866027832 and perplexity is 313.04747802817894
At time: 162.68132519721985 and batch: 150, loss is 5.799508962631226 and perplexity is 330.13741029686105
At time: 163.65649342536926 and batch: 200, loss is 5.722249584197998 and perplexity is 305.59160433768926
At time: 164.63304209709167 and batch: 250, loss is 5.846572351455689 and perplexity is 346.0462205206123
At time: 165.60999941825867 and batch: 300, loss is 5.794521913528443 and perplexity is 328.4950973724254
At time: 166.59566044807434 and batch: 350, loss is 5.818479299545288 and perplexity is 336.4600095064331
At time: 167.58258056640625 and batch: 400, loss is 5.7207944965362545 and perplexity is 305.1472651193414
At time: 168.56884479522705 and batch: 450, loss is 5.746851768493652 and perplexity is 313.20307074593103
At time: 169.56937313079834 and batch: 500, loss is 5.728676872253418 and perplexity is 307.5620551502939
At time: 170.55844354629517 and batch: 550, loss is 5.793971767425537 and perplexity is 328.3144267769512
At time: 171.55240297317505 and batch: 600, loss is 5.741764106750488 and perplexity is 311.61364612091216
At time: 172.5481960773468 and batch: 650, loss is 5.668513040542603 and perplexity is 289.60358527918254
At time: 173.54680371284485 and batch: 700, loss is 5.785513744354248 and perplexity is 325.5492462456395
At time: 174.54278826713562 and batch: 750, loss is 5.751551733016968 and perplexity is 314.67857876858835
At time: 175.5485327243805 and batch: 800, loss is 5.76770188331604 and perplexity is 319.80194532251284
At time: 176.53967142105103 and batch: 850, loss is 5.820643224716187 and perplexity is 337.1888721076566
At time: 177.52360320091248 and batch: 900, loss is 5.702642259597778 and perplexity is 299.65813042193196
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.817003642043022 and perplexity of 335.9638759208465
finished 9 epochs...
Completing Train Step...
At time: 179.8483018875122 and batch: 50, loss is 5.763519296646118 and perplexity is 318.46713938567854
At time: 180.83397912979126 and batch: 100, loss is 5.722369318008423 and perplexity is 305.6281961755068
At time: 181.81373691558838 and batch: 150, loss is 5.774869689941406 and perplexity is 322.10245879331455
At time: 182.78619837760925 and batch: 200, loss is 5.696267242431641 and perplexity is 297.75388095722036
At time: 183.7719123363495 and batch: 250, loss is 5.822759637832641 and perplexity is 337.9032587612929
At time: 184.74625205993652 and batch: 300, loss is 5.771781034469605 and perplexity is 321.1091300862342
At time: 185.72883296012878 and batch: 350, loss is 5.793995370864868 and perplexity is 328.32217621806154
At time: 186.71914219856262 and batch: 400, loss is 5.6958886241912845 and perplexity is 297.6411672458331
At time: 187.70757913589478 and batch: 450, loss is 5.72282320022583 and perplexity is 305.7669468647423
At time: 188.69554018974304 and batch: 500, loss is 5.70302487373352 and perplexity is 299.772805795332
At time: 189.67868947982788 and batch: 550, loss is 5.769537982940673 and perplexity is 320.38967295240167
At time: 190.67657256126404 and batch: 600, loss is 5.717010850906372 and perplexity is 303.9948774912043
At time: 191.6663691997528 and batch: 650, loss is 5.6418017578125 and perplexity is 281.97030326233255
At time: 192.66078686714172 and batch: 700, loss is 5.759868144989014 and perplexity is 317.3064877090017
At time: 193.64894723892212 and batch: 750, loss is 5.7263400173187256 and perplexity is 306.8441663716519
At time: 194.63598656654358 and batch: 800, loss is 5.742103462219238 and perplexity is 311.7194118609365
At time: 195.62515568733215 and batch: 850, loss is 5.795710830688477 and perplexity is 328.88588309056956
At time: 196.61203289031982 and batch: 900, loss is 5.6782788562774655 and perplexity is 292.4456555086667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.793868391481165 and perplexity of 328.2804887172517
finished 10 epochs...
Completing Train Step...
At time: 198.97648000717163 and batch: 50, loss is 5.739065065383911 and perplexity is 310.7737220045008
At time: 199.94659638404846 and batch: 100, loss is 5.697454195022583 and perplexity is 298.107510526935
At time: 200.92717051506042 and batch: 150, loss is 5.749373207092285 and perplexity is 313.99378951320415
At time: 201.9308729171753 and batch: 200, loss is 5.669053459167481 and perplexity is 289.7601347478131
At time: 202.92396521568298 and batch: 250, loss is 5.797827463150025 and perplexity is 329.5827508731773
At time: 203.91055607795715 and batch: 300, loss is 5.747510499954224 and perplexity is 313.4094554307585
At time: 204.91490983963013 and batch: 350, loss is 5.768093767166138 and perplexity is 319.9272950997873
At time: 205.9141068458557 and batch: 400, loss is 5.6698445701599125 and perplexity is 289.98945787364306
At time: 206.9193925857544 and batch: 450, loss is 5.697650575637818 and perplexity is 298.1660588119442
At time: 207.91480255126953 and batch: 500, loss is 5.676470003128052 and perplexity is 291.9171424090655
At time: 208.9081950187683 and batch: 550, loss is 5.74373872756958 and perplexity is 312.22957282477046
At time: 209.89902997016907 and batch: 600, loss is 5.691152391433715 and perplexity is 296.23480246394877
At time: 210.8976047039032 and batch: 650, loss is 5.613553895950317 and perplexity is 274.11669118123615
At time: 211.89411187171936 and batch: 700, loss is 5.732731056213379 and perplexity is 308.8114993278847
At time: 212.8885898590088 and batch: 750, loss is 5.699692306518554 and perplexity is 298.77545556219235
At time: 213.89867210388184 and batch: 800, loss is 5.715167140960693 and perplexity is 303.4349154745296
At time: 214.89455389976501 and batch: 850, loss is 5.769128170013428 and perplexity is 320.25840002317864
At time: 215.8922882080078 and batch: 900, loss is 5.652955474853516 and perplexity is 285.13292494536927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.769609425165882 and perplexity of 320.4125631212238
finished 11 epochs...
Completing Train Step...
At time: 218.24484252929688 and batch: 50, loss is 5.713512010574341 and perplexity is 302.93310651982904
At time: 219.2350218296051 and batch: 100, loss is 5.671154594421386 and perplexity is 290.36960004239563
At time: 220.21559238433838 and batch: 150, loss is 5.723002490997314 and perplexity is 305.8217729713007
At time: 221.1890857219696 and batch: 200, loss is 5.640759286880493 and perplexity is 281.67651057926486
At time: 222.18699765205383 and batch: 250, loss is 5.771833667755127 and perplexity is 321.12603155954787
At time: 223.16576766967773 and batch: 300, loss is 5.722108764648437 and perplexity is 305.5485740954377
At time: 224.14314031600952 and batch: 350, loss is 5.740869846343994 and perplexity is 311.3351069369072
At time: 225.1278040409088 and batch: 400, loss is 5.642957973480224 and perplexity is 282.296510291296
At time: 226.11089777946472 and batch: 450, loss is 5.672003698348999 and perplexity is 290.6162587148552
At time: 227.13035607337952 and batch: 500, loss is 5.649397411346436 and perplexity is 284.12020661691986
At time: 228.12400579452515 and batch: 550, loss is 5.717315292358398 and perplexity is 304.0874402223674
At time: 229.1184206008911 and batch: 600, loss is 5.664941778182984 and perplexity is 288.5711794891009
At time: 230.11412000656128 and batch: 650, loss is 5.584834308624267 and perplexity is 266.3561461667483
At time: 231.10879611968994 and batch: 700, loss is 5.705117397308349 and perplexity is 300.40074421740013
At time: 232.08938837051392 and batch: 750, loss is 5.6727425003051755 and perplexity is 290.83104590811985
At time: 233.07572221755981 and batch: 800, loss is 5.6881091499328615 and perplexity is 295.33465879109394
At time: 234.05781865119934 and batch: 850, loss is 5.742314977645874 and perplexity is 311.785352298791
At time: 235.04440212249756 and batch: 900, loss is 5.627584152221679 and perplexity is 277.9897249015223
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.744489695927868 and perplexity of 312.46413541800285
finished 12 epochs...
Completing Train Step...
At time: 237.3981728553772 and batch: 50, loss is 5.688224239349365 and perplexity is 295.36865064066103
At time: 238.38325881958008 and batch: 100, loss is 5.644717769622803 and perplexity is 282.79373197725266
At time: 239.37192106246948 and batch: 150, loss is 5.696900825500489 and perplexity is 297.9425925508011
At time: 240.36141204833984 and batch: 200, loss is 5.612924966812134 and perplexity is 273.9443454092098
At time: 241.35162544250488 and batch: 250, loss is 5.745982246398926 and perplexity is 312.93085212298627
At time: 242.33410453796387 and batch: 300, loss is 5.697027063369751 and perplexity is 297.9802065629535
At time: 243.31439757347107 and batch: 350, loss is 5.713790044784546 and perplexity is 303.0173439967528
At time: 244.29897165298462 and batch: 400, loss is 5.61639783859253 and perplexity is 274.8973729090766
At time: 245.27827835083008 and batch: 450, loss is 5.6471133518219 and perplexity is 283.47199970627145
At time: 246.25766253471375 and batch: 500, loss is 5.622840623855591 and perplexity is 276.67419534857964
At time: 247.23976874351501 and batch: 550, loss is 5.6914957237243655 and perplexity is 296.3365268988914
At time: 248.23171877861023 and batch: 600, loss is 5.6394267082214355 and perplexity is 281.3014044572242
At time: 249.21482276916504 and batch: 650, loss is 5.557043809890747 and perplexity is 259.05588480982965
At time: 250.20268154144287 and batch: 700, loss is 5.678163795471192 and perplexity is 292.41200841152073
At time: 251.18004488945007 and batch: 750, loss is 5.64679892539978 and perplexity is 283.3828826307486
At time: 252.15748071670532 and batch: 800, loss is 5.661765365600586 and perplexity is 287.6560126069262
At time: 253.1391077041626 and batch: 850, loss is 5.716204319000244 and perplexity is 303.74979477001074
At time: 254.11866450309753 and batch: 900, loss is 5.602801961898804 and perplexity is 271.1851944659427
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.719019223565924 and perplexity of 304.6060259933475
finished 13 epochs...
Completing Train Step...
At time: 256.4592981338501 and batch: 50, loss is 5.6634971618652346 and perplexity is 288.154605821384
At time: 257.444531917572 and batch: 100, loss is 5.618762340545654 and perplexity is 275.5481373479273
At time: 258.425173997879 and batch: 150, loss is 5.671255760192871 and perplexity is 290.39897699294545
At time: 259.4060399532318 and batch: 200, loss is 5.58630485534668 and perplexity is 266.74812346431554
At time: 260.40186190605164 and batch: 250, loss is 5.7207541751861575 and perplexity is 305.134961417686
At time: 261.39394521713257 and batch: 300, loss is 5.672458658218384 and perplexity is 290.7485075316321
At time: 262.37785387039185 and batch: 350, loss is 5.6872982406616215 and perplexity is 295.09526625409376
At time: 263.35769629478455 and batch: 400, loss is 5.590304021835327 and perplexity is 267.8170295626938
At time: 264.34187483787537 and batch: 450, loss is 5.622563991546631 and perplexity is 276.597668912421
At time: 265.32532501220703 and batch: 500, loss is 5.596698226928711 and perplexity is 269.5349932298697
At time: 266.30222606658936 and batch: 550, loss is 5.666099309921265 and perplexity is 288.90540318806404
At time: 267.29258918762207 and batch: 600, loss is 5.614213857650757 and perplexity is 274.297657407688
At time: 268.2835052013397 and batch: 650, loss is 5.529911127090454 and perplexity is 252.12150325625225
At time: 269.2797226905823 and batch: 700, loss is 5.651675500869751 and perplexity is 284.7681956912982
At time: 270.2760109901428 and batch: 750, loss is 5.621510610580445 and perplexity is 276.30645959679975
At time: 271.2778100967407 and batch: 800, loss is 5.635475816726685 and perplexity is 280.1922057356304
At time: 272.2775094509125 and batch: 850, loss is 5.690006074905395 and perplexity is 295.89541817169624
At time: 273.27062344551086 and batch: 900, loss is 5.57802035331726 and perplexity is 264.54737672650776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.693631106860017 and perplexity of 296.96999502877895
finished 14 epochs...
Completing Train Step...
At time: 275.62880420684814 and batch: 50, loss is 5.638698244094849 and perplexity is 281.0965610948977
At time: 276.60435676574707 and batch: 100, loss is 5.592901697158814 and perplexity is 268.51363563808445
At time: 277.58679962158203 and batch: 150, loss is 5.64537579536438 and perplexity is 282.9798787704245
At time: 278.5670142173767 and batch: 200, loss is 5.560451850891114 and perplexity is 259.94026403098434
At time: 279.54800391197205 and batch: 250, loss is 5.695454425811768 and perplexity is 297.51195998615884
At time: 280.5295343399048 and batch: 300, loss is 5.64760576248169 and perplexity is 283.6116187128013
At time: 281.514612197876 and batch: 350, loss is 5.6608439064025875 and perplexity is 287.3910714132463
At time: 282.5003778934479 and batch: 400, loss is 5.5639333152771 and perplexity is 260.8468139476565
At time: 283.4721655845642 and batch: 450, loss is 5.59760272026062 and perplexity is 269.7788961216036
At time: 284.44273829460144 and batch: 500, loss is 5.570251588821411 and perplexity is 262.5001330323385
At time: 285.412878036499 and batch: 550, loss is 5.640514230728149 and perplexity is 281.6074924743777
At time: 286.38563537597656 and batch: 600, loss is 5.588700323104859 and perplexity is 267.387875940922
At time: 287.368061542511 and batch: 650, loss is 5.502717905044555 and perplexity is 245.35788629123746
At time: 288.35116624832153 and batch: 700, loss is 5.625100202560425 and perplexity is 277.3000693080082
At time: 289.34769797325134 and batch: 750, loss is 5.5960131359100345 and perplexity is 269.35040046543116
At time: 290.3319716453552 and batch: 800, loss is 5.608598775863648 and perplexity is 272.7617697302269
At time: 291.3203618526459 and batch: 850, loss is 5.663092021942139 and perplexity is 288.0378865319603
At time: 292.3110234737396 and batch: 900, loss is 5.552832326889038 and perplexity is 257.96716951661017
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.667667754708904 and perplexity of 289.35889090407176
finished 15 epochs...
Completing Train Step...
At time: 294.66169238090515 and batch: 50, loss is 5.613256044387818 and perplexity is 274.03505725446047
At time: 295.6424012184143 and batch: 100, loss is 5.566750974655151 and perplexity is 261.58282785050176
At time: 296.6404459476471 and batch: 150, loss is 5.618954772949219 and perplexity is 275.60116684042765
At time: 297.6409709453583 and batch: 200, loss is 5.534690856933594 and perplexity is 253.3294604843396
At time: 298.62584114074707 and batch: 250, loss is 5.669617071151733 and perplexity is 289.9234930633433
At time: 299.61852073669434 and batch: 300, loss is 5.622119474411011 and perplexity is 276.4747438320932
At time: 300.6113963127136 and batch: 350, loss is 5.634189071655274 and perplexity is 279.8319016553154
At time: 301.58757638931274 and batch: 400, loss is 5.536975402832031 and perplexity is 253.9088648501763
At time: 302.57738614082336 and batch: 450, loss is 5.5720388698577885 and perplexity is 262.9697140537797
At time: 303.560706615448 and batch: 500, loss is 5.54324143409729 and perplexity is 255.5048607941139
At time: 304.55128502845764 and batch: 550, loss is 5.614573354721069 and perplexity is 274.39628433890385
At time: 305.55338740348816 and batch: 600, loss is 5.562706756591797 and perplexity is 260.527066157277
At time: 306.55282187461853 and batch: 650, loss is 5.475247354507446 and perplexity is 238.70950550078612
At time: 307.5483913421631 and batch: 700, loss is 5.598281221389771 and perplexity is 269.9620035195065
At time: 308.5430951118469 and batch: 750, loss is 5.5700641536712645 and perplexity is 262.4509358912709
At time: 309.5286064147949 and batch: 800, loss is 5.581233148574829 and perplexity is 265.39868008328415
At time: 310.5140519142151 and batch: 850, loss is 5.635646877288818 and perplexity is 280.24013967153496
At time: 311.4900965690613 and batch: 900, loss is 5.527412166595459 and perplexity is 251.49224814896786
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.641403407266695 and perplexity of 281.85800260712597
finished 16 epochs...
Completing Train Step...
At time: 313.8590393066406 and batch: 50, loss is 5.587466478347778 and perplexity is 267.05816426042827
At time: 314.823340177536 and batch: 100, loss is 5.540437097549439 and perplexity is 254.7893429202183
At time: 315.81161618232727 and batch: 150, loss is 5.592286128997802 and perplexity is 268.3483980559032
At time: 316.78095602989197 and batch: 200, loss is 5.5088427734375 and perplexity is 246.86528264370898
At time: 317.7667968273163 and batch: 250, loss is 5.643412303924561 and perplexity is 282.42479532994383
At time: 318.7513861656189 and batch: 300, loss is 5.596275548934937 and perplexity is 269.4210907934018
At time: 319.74644589424133 and batch: 350, loss is 5.6075895881652835 and perplexity is 272.4866407593682
At time: 320.7224531173706 and batch: 400, loss is 5.509682483673096 and perplexity is 247.07266500671892
At time: 321.6915078163147 and batch: 450, loss is 5.546218099594117 and perplexity is 256.26654637669594
At time: 322.68193101882935 and batch: 500, loss is 5.516002864837646 and perplexity is 248.63920377105043
At time: 323.6722922325134 and batch: 550, loss is 5.588508462905883 and perplexity is 267.3365797708445
At time: 324.6773021221161 and batch: 600, loss is 5.536556930541992 and perplexity is 253.80263325508096
At time: 325.6676871776581 and batch: 650, loss is 5.447795629501343 and perplexity is 232.24564577308453
At time: 326.6558220386505 and batch: 700, loss is 5.571491336822509 and perplexity is 262.8257688590282
At time: 327.6384735107422 and batch: 750, loss is 5.544017553329468 and perplexity is 255.70324000349302
At time: 328.61261105537415 and batch: 800, loss is 5.5538139915466305 and perplexity is 258.22053110745713
At time: 329.58507204055786 and batch: 850, loss is 5.608193550109863 and perplexity is 272.65126202828003
At time: 330.55792212486267 and batch: 900, loss is 5.5021970844268795 and perplexity is 245.23013211673947
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.615218071088399 and perplexity of 274.5732491544783
finished 17 epochs...
Completing Train Step...
At time: 332.9001007080078 and batch: 50, loss is 5.561851749420166 and perplexity is 260.304408847671
At time: 333.8800559043884 and batch: 100, loss is 5.514185056686402 and perplexity is 248.18763595589178
At time: 334.8640196323395 and batch: 150, loss is 5.565803718566895 and perplexity is 261.33515924555326
At time: 335.85283637046814 and batch: 200, loss is 5.483049278259277 and perplexity is 240.57918291793385
At time: 336.83744955062866 and batch: 250, loss is 5.617173652648926 and perplexity is 275.11072490508343
At time: 337.8163866996765 and batch: 300, loss is 5.570515222549439 and perplexity is 262.5693460440589
At time: 338.81161403656006 and batch: 350, loss is 5.581294288635254 and perplexity is 265.41490707067567
At time: 339.7881455421448 and batch: 400, loss is 5.482454452514649 and perplexity is 240.43612277845762
At time: 340.77209520339966 and batch: 450, loss is 5.520491886138916 and perplexity is 249.75785940907235
At time: 341.75344228744507 and batch: 500, loss is 5.4889223003387455 and perplexity is 241.9962669808508
At time: 342.7292263507843 and batch: 550, loss is 5.5626121997833256 and perplexity is 260.502432714029
At time: 343.7203869819641 and batch: 600, loss is 5.51064130783081 and perplexity is 247.30967785530265
At time: 344.702595949173 and batch: 650, loss is 5.420788946151734 and perplexity is 226.0573992826311
At time: 345.68511056900024 and batch: 700, loss is 5.545068731307984 and perplexity is 255.97217094079588
At time: 346.672034740448 and batch: 750, loss is 5.518338890075683 and perplexity is 249.22071016859474
At time: 347.64865612983704 and batch: 800, loss is 5.526805429458618 and perplexity is 251.33970474394238
At time: 348.6288266181946 and batch: 850, loss is 5.581252822875976 and perplexity is 265.4039016682056
At time: 349.61678075790405 and batch: 900, loss is 5.477600526809693 and perplexity is 239.27189153351728
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.58943740635702 and perplexity of 267.585035718768
finished 18 epochs...
Completing Train Step...
At time: 351.9261977672577 and batch: 50, loss is 5.536794109344482 and perplexity is 253.86283699894904
At time: 352.89902091026306 and batch: 100, loss is 5.488334283828736 and perplexity is 241.85401100903428
At time: 353.86481738090515 and batch: 150, loss is 5.540004005432129 and perplexity is 254.6790195560392
At time: 354.8438470363617 and batch: 200, loss is 5.457684946060181 and perplexity is 234.55379066019506
At time: 355.81719398498535 and batch: 250, loss is 5.591424722671508 and perplexity is 268.11734057964037
At time: 356.78063011169434 and batch: 300, loss is 5.545333623886108 and perplexity is 256.0399850504152
At time: 357.75221133232117 and batch: 350, loss is 5.5556431484222415 and perplexity is 258.69328920990966
At time: 358.72934007644653 and batch: 400, loss is 5.4558396148681645 and perplexity is 234.12136034536928
At time: 359.7205002307892 and batch: 450, loss is 5.495361471176148 and perplexity is 243.55955000767293
At time: 360.7077350616455 and batch: 500, loss is 5.462466735839843 and perplexity is 235.67806345497948
At time: 361.6870448589325 and batch: 550, loss is 5.5373151493072506 and perplexity is 253.9951441477499
At time: 362.68777561187744 and batch: 600, loss is 5.485411834716797 and perplexity is 241.14823676631417
At time: 363.6785924434662 and batch: 650, loss is 5.394741058349609 and perplexity is 220.24510908001906
At time: 364.6845545768738 and batch: 700, loss is 5.519465465545654 and perplexity is 249.50163431869578
At time: 365.6849751472473 and batch: 750, loss is 5.493508749008178 and perplexity is 243.10871959086595
At time: 366.6755304336548 and batch: 800, loss is 5.500767507553101 and perplexity is 244.87980725893212
At time: 367.6688902378082 and batch: 850, loss is 5.555333442687989 and perplexity is 258.61318282017305
At time: 368.6692500114441 and batch: 900, loss is 5.453987865447998 and perplexity is 233.68822740270582
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.564532972361944 and perplexity of 261.0032794958478
finished 19 epochs...
Completing Train Step...
At time: 371.02656173706055 and batch: 50, loss is 5.512711486816406 and perplexity is 247.82218345946447
At time: 371.99690294265747 and batch: 100, loss is 5.463356704711914 and perplexity is 235.88790295674644
At time: 372.95748805999756 and batch: 150, loss is 5.515301141738892 and perplexity is 248.46478910106674
At time: 373.9343283176422 and batch: 200, loss is 5.433146448135376 and perplexity is 228.8682357315369
At time: 374.89581298828125 and batch: 250, loss is 5.566717758178711 and perplexity is 261.5741391349683
At time: 375.85760402679443 and batch: 300, loss is 5.5211408329010006 and perplexity is 249.91999156512003
At time: 376.835914850235 and batch: 350, loss is 5.531018524169922 and perplexity is 252.40085652157617
At time: 377.8176248073578 and batch: 400, loss is 5.430349941253662 and perplexity is 228.22909822805005
At time: 378.79222202301025 and batch: 450, loss is 5.471261682510376 and perplexity is 237.75998121243902
At time: 379.7672047615051 and batch: 500, loss is 5.4370818519592286 and perplexity is 229.7706992761009
At time: 380.73904752731323 and batch: 550, loss is 5.513036918640137 and perplexity is 247.90284580892447
At time: 381.7106490135193 and batch: 600, loss is 5.461212072372437 and perplexity is 235.38255222103376
At time: 382.6851851940155 and batch: 650, loss is 5.369973192214966 and perplexity is 214.8571077839718
At time: 383.6629614830017 and batch: 700, loss is 5.495002956390381 and perplexity is 243.47224595861172
At time: 384.6477768421173 and batch: 750, loss is 5.4698499584197995 and perplexity is 237.42456653138493
At time: 385.63426065444946 and batch: 800, loss is 5.476106128692627 and perplexity is 238.91459111037702
At time: 386.6246681213379 and batch: 850, loss is 5.530637941360474 and perplexity is 252.3048153714586
At time: 387.6287159919739 and batch: 900, loss is 5.431525621414185 and perplexity is 228.4975804445732
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.540883939560145 and perplexity of 254.90321894291608
finished 20 epochs...
Completing Train Step...
At time: 389.97451758384705 and batch: 50, loss is 5.489779453277588 and perplexity is 242.2037837163669
At time: 390.9476068019867 and batch: 100, loss is 5.439547319412231 and perplexity is 230.33789036509722
At time: 391.92224502563477 and batch: 150, loss is 5.49183723449707 and perplexity is 242.7026992672579
At time: 392.89568185806274 and batch: 200, loss is 5.409629316329956 and perplexity is 223.54870646519237
At time: 393.87594509124756 and batch: 250, loss is 5.543305110931397 and perplexity is 255.52113105276197
At time: 394.8474361896515 and batch: 300, loss is 5.498112735748291 and perplexity is 244.23056942151536
At time: 395.82055044174194 and batch: 350, loss is 5.5075851249694825 and perplexity is 246.55500804826642
At time: 396.79634618759155 and batch: 400, loss is 5.406219615936279 and perplexity is 222.78777037202397
At time: 397.774854183197 and batch: 450, loss is 5.448354558944702 and perplexity is 232.3754909863752
At time: 398.7713735103607 and batch: 500, loss is 5.412912178039551 and perplexity is 224.2837918855874
At time: 399.7517304420471 and batch: 550, loss is 5.489949779510498 and perplexity is 242.24504088793336
At time: 400.73317670822144 and batch: 600, loss is 5.438143711090088 and perplexity is 230.0148129753741
At time: 401.7183885574341 and batch: 650, loss is 5.346510801315308 and perplexity is 209.87472420713812
At time: 402.69946026802063 and batch: 700, loss is 5.471730012893676 and perplexity is 237.87135751399092
At time: 403.6729438304901 and batch: 750, loss is 5.447430429458618 and perplexity is 232.16084513886562
At time: 404.66959261894226 and batch: 800, loss is 5.452799711227417 and perplexity is 233.4107346337644
At time: 405.6575675010681 and batch: 850, loss is 5.507169723510742 and perplexity is 246.45261000788358
At time: 406.6410174369812 and batch: 900, loss is 5.410157794952393 and perplexity is 223.6668784005531
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.518559704088185 and perplexity of 249.27574766990767
finished 21 epochs...
Completing Train Step...
At time: 408.9678599834442 and batch: 50, loss is 5.467987565994263 and perplexity is 236.98280031575652
At time: 409.9483895301819 and batch: 100, loss is 5.416905002593994 and perplexity is 225.18110793764225
At time: 410.9167010784149 and batch: 150, loss is 5.4695421314239505 and perplexity is 237.351492088049
At time: 411.8918390274048 and batch: 200, loss is 5.387121505737305 and perplexity is 218.57331712550032
At time: 412.85795879364014 and batch: 250, loss is 5.521114072799683 and perplexity is 249.91330377030778
At time: 413.84665083885193 and batch: 300, loss is 5.476205196380615 and perplexity is 238.93826099898618
At time: 414.8326621055603 and batch: 350, loss is 5.485289068222046 and perplexity is 241.1186336597429
At time: 415.8161554336548 and batch: 400, loss is 5.38338194847107 and perplexity is 217.75747608213555
At time: 416.79426765441895 and batch: 450, loss is 5.42656247138977 and perplexity is 227.36632229673899
At time: 417.7831959724426 and batch: 500, loss is 5.3898677730560305 and perplexity is 219.17440287636262
At time: 418.76038336753845 and batch: 550, loss is 5.4679819679260255 and perplexity is 236.9814736735826
At time: 419.7446572780609 and batch: 600, loss is 5.416139841079712 and perplexity is 225.00887392192826
At time: 420.7279019355774 and batch: 650, loss is 5.32419602394104 and perplexity is 205.24328342854358
At time: 421.7054064273834 and batch: 700, loss is 5.4495317745208744 and perplexity is 232.64920811430767
At time: 422.6991837024689 and batch: 750, loss is 5.4261172580718995 and perplexity is 227.26511831236962
At time: 423.6820366382599 and batch: 800, loss is 5.430642576217651 and perplexity is 228.2958958151693
At time: 424.68097138404846 and batch: 850, loss is 5.484805841445922 and perplexity is 241.00214682678134
At time: 425.67279028892517 and batch: 900, loss is 5.389720516204834 and perplexity is 219.14213032016804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.497426908310145 and perplexity of 244.06312682070038
finished 22 epochs...
Completing Train Step...
At time: 428.03772354125977 and batch: 50, loss is 5.4472126770019536 and perplexity is 232.1102970481806
At time: 429.00107622146606 and batch: 100, loss is 5.395273323059082 and perplexity is 220.36236898290204
At time: 429.9748830795288 and batch: 150, loss is 5.448252077102661 and perplexity is 232.3516779382371
At time: 430.9468741416931 and batch: 200, loss is 5.365522480010986 and perplexity is 213.9029655136065
At time: 431.92270278930664 and batch: 250, loss is 5.499929447174072 and perplexity is 244.67466916590467
At time: 432.8922543525696 and batch: 300, loss is 5.455289344787598 and perplexity is 233.99256580469074
At time: 433.8621356487274 and batch: 350, loss is 5.463969449996949 and perplexity is 236.03248644898278
At time: 434.8367021083832 and batch: 400, loss is 5.361672401428223 and perplexity is 213.08100560774065
At time: 435.8065357208252 and batch: 450, loss is 5.405726947784424 and perplexity is 222.67803696623554
At time: 436.7731935977936 and batch: 500, loss is 5.367792739868164 and perplexity is 214.3891324834091
At time: 437.7407977581024 and batch: 550, loss is 5.446972951889038 and perplexity is 232.05466104995136
At time: 438.7205123901367 and batch: 600, loss is 5.3950854015350345 and perplexity is 220.32096204142965
At time: 439.71907210350037 and batch: 650, loss is 5.302826299667358 and perplexity is 200.90382273072476
At time: 440.7231459617615 and batch: 700, loss is 5.428248271942139 and perplexity is 227.74993982892968
At time: 441.717570066452 and batch: 750, loss is 5.405726938247681 and perplexity is 222.67803484261236
At time: 442.71373438835144 and batch: 800, loss is 5.40942349433899 and perplexity is 223.50269996008743
At time: 443.70417165756226 and batch: 850, loss is 5.463382797241211 and perplexity is 235.8940579490645
At time: 444.6973559856415 and batch: 900, loss is 5.3700558376312255 and perplexity is 214.8748654728667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.477292413580908 and perplexity of 239.1981800547834
finished 23 epochs...
Completing Train Step...
At time: 447.10906624794006 and batch: 50, loss is 5.42731017112732 and perplexity is 227.5363876072188
At time: 448.07592821121216 and batch: 100, loss is 5.374496870040893 and perplexity is 215.83125381838906
At time: 449.04635095596313 and batch: 150, loss is 5.427822542190552 and perplexity is 227.65300054005618
At time: 450.013548374176 and batch: 200, loss is 5.344750671386719 and perplexity is 209.50564233511773
At time: 450.9726815223694 and batch: 250, loss is 5.479554967880249 and perplexity is 239.73999163338468
At time: 451.9409339427948 and batch: 300, loss is 5.435236196517945 and perplexity is 229.34701284459024
At time: 452.9083731174469 and batch: 350, loss is 5.443482055664062 and perplexity is 231.24599461622896
At time: 453.885324716568 and batch: 400, loss is 5.340937719345093 and perplexity is 208.70832839433874
At time: 454.86347913742065 and batch: 450, loss is 5.385706405639649 and perplexity is 218.26423274733654
At time: 455.8379690647125 and batch: 500, loss is 5.346554536819458 and perplexity is 209.88390338473613
At time: 456.8317048549652 and batch: 550, loss is 5.426779699325562 and perplexity is 227.4157179784667
At time: 457.81668877601624 and batch: 600, loss is 5.374873352050781 and perplexity is 215.91252570036406
At time: 458.79951906204224 and batch: 650, loss is 5.282241344451904 and perplexity is 196.81050149545877
At time: 459.77273774147034 and batch: 700, loss is 5.40773943901062 and perplexity is 223.12662580104057
At time: 460.7748589515686 and batch: 750, loss is 5.386127109527588 and perplexity is 218.35607667684587
At time: 461.7470021247864 and batch: 800, loss is 5.388984813690185 and perplexity is 218.98096619552115
At time: 462.72934198379517 and batch: 850, loss is 5.44276837348938 and perplexity is 231.08101734957438
At time: 463.70382475852966 and batch: 900, loss is 5.351066265106201 and perplexity is 210.83298191085692
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.458042092519264 and perplexity of 234.63757567686432
finished 24 epochs...
Completing Train Step...
At time: 466.01519560813904 and batch: 50, loss is 5.408164005279541 and perplexity is 223.2213779529144
At time: 466.9926862716675 and batch: 100, loss is 5.354496078491211 and perplexity is 211.55734119270602
At time: 467.9773197174072 and batch: 150, loss is 5.408161525726318 and perplexity is 223.2208244643134
At time: 468.9475040435791 and batch: 200, loss is 5.324747514724732 and perplexity is 205.35650442506747
At time: 469.91788625717163 and batch: 250, loss is 5.459875879287719 and perplexity is 235.0682457163458
At time: 470.88744282722473 and batch: 300, loss is 5.415944490432739 and perplexity is 224.96492258593378
At time: 471.8557348251343 and batch: 350, loss is 5.423736238479615 and perplexity is 226.72463931390993
At time: 472.8265118598938 and batch: 400, loss is 5.321082696914673 and perplexity is 204.60528762738113
At time: 473.80634570121765 and batch: 450, loss is 5.366414289474488 and perplexity is 214.09381128890817
At time: 474.7874405384064 and batch: 500, loss is 5.326071062088013 and perplexity is 205.628483433878
At time: 475.7662284374237 and batch: 550, loss is 5.407300777435303 and perplexity is 223.02877018818927
At time: 476.73769450187683 and batch: 600, loss is 5.3554296875 and perplexity is 211.75494526043482
At time: 477.71874713897705 and batch: 650, loss is 5.262347936630249 and perplexity is 192.93395661338948
At time: 478.6980810165405 and batch: 700, loss is 5.387914819717407 and perplexity is 218.7467831910832
At time: 479.68052792549133 and batch: 750, loss is 5.3672440719604495 and perplexity is 214.27153631022597
At time: 480.6593279838562 and batch: 800, loss is 5.369232034683227 and perplexity is 214.69792381777475
At time: 481.6477494239807 and batch: 850, loss is 5.4228943252563475 and perplexity is 226.53383717272428
At time: 482.6354343891144 and batch: 900, loss is 5.332706823348999 and perplexity is 206.99752224214063
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.439645845596105 and perplexity of 230.3605857964652
finished 25 epochs...
Completing Train Step...
At time: 484.99370408058167 and batch: 50, loss is 5.389707641601563 and perplexity is 219.139308970342
At time: 485.97455954551697 and batch: 100, loss is 5.33524567604065 and perplexity is 207.52372615310884
At time: 486.9610104560852 and batch: 150, loss is 5.389230003356934 and perplexity is 219.0346646485308
At time: 487.94650077819824 and batch: 200, loss is 5.305472631454467 and perplexity is 201.43618499588018
At time: 488.93685817718506 and batch: 250, loss is 5.440835494995117 and perplexity is 230.6347972042921
At time: 489.9358112812042 and batch: 300, loss is 5.397357025146484 and perplexity is 220.82201722983348
At time: 490.9230215549469 and batch: 350, loss is 5.4046873855590825 and perplexity is 222.44666957183034
At time: 491.90613532066345 and batch: 400, loss is 5.302059421539306 and perplexity is 200.74981304410846
At time: 492.9026873111725 and batch: 450, loss is 5.347812376022339 and perplexity is 210.14806969100582
At time: 493.9039890766144 and batch: 500, loss is 5.30631118774414 and perplexity is 201.60517141818758
At time: 494.8970892429352 and batch: 550, loss is 5.388470354080201 and perplexity is 218.86833830679208
At time: 495.8745160102844 and batch: 600, loss is 5.336712198257446 and perplexity is 207.82828757648699
At time: 496.854688167572 and batch: 650, loss is 5.243119211196899 and perplexity is 189.25952308750882
At time: 497.83309602737427 and batch: 700, loss is 5.368732976913452 and perplexity is 214.59080388248017
At time: 498.8186991214752 and batch: 750, loss is 5.349044399261475 and perplexity is 210.40713655195185
At time: 499.7935211658478 and batch: 800, loss is 5.3501107883453365 and perplexity is 210.63163210406614
At time: 500.77230525016785 and batch: 850, loss is 5.403741817474366 and perplexity is 222.2364305138679
At time: 501.7444679737091 and batch: 900, loss is 5.314954519271851 and perplexity is 203.35526417303709
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.422078171821489 and perplexity of 226.34902623070536
finished 26 epochs...
Completing Train Step...
At time: 504.1041820049286 and batch: 50, loss is 5.371912212371826 and perplexity is 215.27412421778695
At time: 505.08578085899353 and batch: 100, loss is 5.316741580963135 and perplexity is 203.71899748549475
At time: 506.070068359375 and batch: 150, loss is 5.371020441055298 and perplexity is 215.0822345022077
At time: 507.06850004196167 and batch: 200, loss is 5.286905937194824 and perplexity is 197.730686808851
At time: 508.0388765335083 and batch: 250, loss is 5.422417249679565 and perplexity is 226.42578918727122
At time: 509.0130751132965 and batch: 300, loss is 5.37945590019226 and perplexity is 216.90422576492455
At time: 509.9978663921356 and batch: 350, loss is 5.386319761276245 and perplexity is 218.3981474092175
At time: 510.9784519672394 and batch: 400, loss is 5.28383316040039 and perplexity is 197.12403706982755
At time: 511.9717001914978 and batch: 450, loss is 5.329892511367798 and perplexity is 206.41578561336104
At time: 512.9859411716461 and batch: 500, loss is 5.287279443740845 and perplexity is 197.80455430886195
At time: 513.9826719760895 and batch: 550, loss is 5.370260705947876 and perplexity is 214.91889103441403
At time: 514.9809350967407 and batch: 600, loss is 5.318691558837891 and perplexity is 204.11663258708637
At time: 515.9666867256165 and batch: 650, loss is 5.22457763671875 and perplexity is 185.78268616773596
At time: 516.9510998725891 and batch: 700, loss is 5.350185022354126 and perplexity is 210.64726871487213
At time: 517.9364988803864 and batch: 750, loss is 5.331516580581665 and perplexity is 206.75129150468226
At time: 518.9313552379608 and batch: 800, loss is 5.33161678314209 and perplexity is 206.77200955144556
At time: 519.9077503681183 and batch: 850, loss is 5.385302238464355 and perplexity is 218.17603533336984
At time: 520.8874409198761 and batch: 900, loss is 5.297810373306274 and perplexity is 199.8986270515188
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.405294757999786 and perplexity of 222.5818185871935
finished 27 epochs...
Completing Train Step...
At time: 523.2250239849091 and batch: 50, loss is 5.354775791168213 and perplexity is 211.61652473975874
At time: 524.2025442123413 and batch: 100, loss is 5.298986902236939 and perplexity is 200.13395197559055
At time: 525.1596734523773 and batch: 150, loss is 5.35352198600769 and perplexity is 211.35136511297893
At time: 526.1228640079498 and batch: 200, loss is 5.26903639793396 and perplexity is 194.22871305374852
At time: 527.0877678394318 and batch: 250, loss is 5.40462854385376 and perplexity is 222.43358081553558
At time: 528.05477643013 and batch: 300, loss is 5.3622479820251465 and perplexity is 213.2036862030397
At time: 529.0337159633636 and batch: 350, loss is 5.368640613555908 and perplexity is 214.5709844706434
At time: 530.0121743679047 and batch: 400, loss is 5.266368961334228 and perplexity is 193.71131065166801
At time: 530.9946732521057 and batch: 450, loss is 5.3126507663726805 and perplexity is 202.88732311088398
At time: 531.9814927577972 and batch: 500, loss is 5.268988733291626 and perplexity is 194.2194554322421
At time: 532.950192451477 and batch: 550, loss is 5.352669544219971 and perplexity is 211.17127714562469
At time: 533.9227111339569 and batch: 600, loss is 5.3013384151458744 and perplexity is 200.60512331280214
At time: 534.8988041877747 and batch: 650, loss is 5.206759586334228 and perplexity is 182.5017179374677
At time: 535.8744175434113 and batch: 700, loss is 5.332272787094116 and perplexity is 206.90769730786582
At time: 536.8584389686584 and batch: 750, loss is 5.314656352996826 and perplexity is 203.29463952997307
At time: 537.8318455219269 and batch: 800, loss is 5.313766393661499 and perplexity is 203.11379605130864
At time: 538.8098948001862 and batch: 850, loss is 5.367559957504272 and perplexity is 214.33923228252564
At time: 539.79230427742 and batch: 900, loss is 5.281273555755615 and perplexity is 196.62012265490478
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.389254217278467 and perplexity of 219.0399684009258
finished 28 epochs...
Completing Train Step...
At time: 542.1430578231812 and batch: 50, loss is 5.338297481536865 and perplexity is 208.15801557285803
At time: 543.1158156394958 and batch: 100, loss is 5.281965732574463 and perplexity is 196.75626565800525
At time: 544.0880558490753 and batch: 150, loss is 5.336697816848755 and perplexity is 207.8252987344376
At time: 545.0437133312225 and batch: 200, loss is 5.251839361190796 and perplexity is 190.9171112214729
At time: 546.007242679596 and batch: 250, loss is 5.387472114562988 and perplexity is 218.64996429534534
At time: 546.9723374843597 and batch: 300, loss is 5.345720062255859 and perplexity is 209.70883366182724
At time: 547.9526302814484 and batch: 350, loss is 5.351654634475708 and perplexity is 210.95706607957754
At time: 548.9380612373352 and batch: 400, loss is 5.24963228225708 and perplexity is 190.49620674277318
At time: 549.9143946170807 and batch: 450, loss is 5.2960664176940915 and perplexity is 199.55031652633104
At time: 550.899676322937 and batch: 500, loss is 5.2514307403564455 and perplexity is 190.83911444883225
At time: 551.8668284416199 and batch: 550, loss is 5.335692005157471 and perplexity is 207.61637070796536
At time: 552.8320624828339 and batch: 600, loss is 5.284618225097656 and perplexity is 197.27885295460501
At time: 553.7927222251892 and batch: 650, loss is 5.189674234390258 and perplexity is 179.4100977711146
At time: 554.7547335624695 and batch: 700, loss is 5.314991102218628 and perplexity is 203.36270364392135
At time: 555.7233195304871 and batch: 750, loss is 5.298446264266968 and perplexity is 200.02578120531976
At time: 556.6967575550079 and batch: 800, loss is 5.296559171676636 and perplexity is 199.64866996955274
At time: 557.6808912754059 and batch: 850, loss is 5.350480298995972 and perplexity is 210.70947711688427
At time: 558.6709740161896 and batch: 900, loss is 5.265330247879028 and perplexity is 193.5102045707473
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.3739385735498715 and perplexity of 215.7107896172288
finished 29 epochs...
Completing Train Step...
At time: 560.9845945835114 and batch: 50, loss is 5.322469100952149 and perplexity is 204.8891499527055
At time: 561.9314451217651 and batch: 100, loss is 5.265642242431641 and perplexity is 193.57058811962767
At time: 562.8846340179443 and batch: 150, loss is 5.32050633430481 and perplexity is 204.48739476759408
At time: 563.8557105064392 and batch: 200, loss is 5.235288667678833 and perplexity is 187.78330549197142
At time: 564.8299188613892 and batch: 250, loss is 5.370953102111816 and perplexity is 215.0677515794125
At time: 565.8054461479187 and batch: 300, loss is 5.3298410987854 and perplexity is 206.40517351757498
At time: 566.7701478004456 and batch: 350, loss is 5.335343723297119 and perplexity is 207.54407428263326
At time: 567.728964805603 and batch: 400, loss is 5.233589496612549 and perplexity is 187.46450046139415
At time: 568.7008149623871 and batch: 450, loss is 5.2801170158386235 and perplexity is 196.392855081913
At time: 569.666829586029 and batch: 500, loss is 5.234589719772339 and perplexity is 187.65210060175644
At time: 570.6298942565918 and batch: 550, loss is 5.31933180809021 and perplexity is 204.24735995306963
At time: 571.5946967601776 and batch: 600, loss is 5.268505582809448 and perplexity is 194.12564087380125
At time: 572.5669386386871 and batch: 650, loss is 5.173301448822022 and perplexity is 176.49657107049825
At time: 573.546484708786 and batch: 700, loss is 5.29833044052124 and perplexity is 200.0026148117336
At time: 574.5249490737915 and batch: 750, loss is 5.282859516143799 and perplexity is 196.93220178811328
At time: 575.5037677288055 and batch: 800, loss is 5.2799831199646 and perplexity is 196.36656064932708
At time: 576.4823708534241 and batch: 850, loss is 5.33403528213501 and perplexity is 207.2726926550536
At time: 577.4563205242157 and batch: 900, loss is 5.249968318939209 and perplexity is 190.56023121272798
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.359335703392551 and perplexity of 212.58368091560297
finished 30 epochs...
Completing Train Step...
At time: 579.7882552146912 and batch: 50, loss is 5.3072816753387455 and perplexity is 201.8009217073069
At time: 580.7674686908722 and batch: 100, loss is 5.249976844787597 and perplexity is 190.56185590729405
At time: 581.732076883316 and batch: 150, loss is 5.304922437667846 and perplexity is 201.32538654160038
At time: 582.6994276046753 and batch: 200, loss is 5.219371385574341 and perplexity is 184.81796830605916
At time: 583.6783401966095 and batch: 250, loss is 5.355078096389771 and perplexity is 211.68050719078272
At time: 584.6634318828583 and batch: 300, loss is 5.314587326049804 and perplexity is 203.28060720597034
At time: 585.6704635620117 and batch: 350, loss is 5.319690856933594 and perplexity is 204.3207078983845
At time: 586.665046453476 and batch: 400, loss is 5.218208293914795 and perplexity is 184.6031330293662
At time: 587.6502451896667 and batch: 450, loss is 5.264787454605102 and perplexity is 193.4051970345406
At time: 588.642410993576 and batch: 500, loss is 5.218455619812012 and perplexity is 184.64879581143322
At time: 589.6255848407745 and batch: 550, loss is 5.303603105545044 and perplexity is 201.05994663220284
At time: 590.6027839183807 and batch: 600, loss is 5.252991075515747 and perplexity is 191.13711986257763
At time: 591.5797731876373 and batch: 650, loss is 5.1576110363006595 and perplexity is 173.74887963855008
At time: 592.5468380451202 and batch: 700, loss is 5.282281646728515 and perplexity is 196.8184335665692
At time: 593.5098872184753 and batch: 750, loss is 5.267869920730591 and perplexity is 194.00228177686765
At time: 594.4961776733398 and batch: 800, loss is 5.2640272235870365 and perplexity is 193.25822027992254
At time: 595.4834697246552 and batch: 850, loss is 5.318223438262939 and perplexity is 204.0211037529334
At time: 596.4548616409302 and batch: 900, loss is 5.235179843902588 and perplexity is 187.76287131543438
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.3454393360712755 and perplexity of 209.64997116358873
finished 31 epochs...
Completing Train Step...
At time: 598.7428119182587 and batch: 50, loss is 5.292726860046387 and perplexity is 198.88501825994996
At time: 599.7066416740417 and batch: 100, loss is 5.23494215965271 and perplexity is 187.71824834150988
At time: 600.6768643856049 and batch: 150, loss is 5.289928541183472 and perplexity is 198.32925252932998
At time: 601.6394908428192 and batch: 200, loss is 5.204081726074219 and perplexity is 182.01365760995137
At time: 602.6015582084656 and batch: 250, loss is 5.339843788146973 and perplexity is 208.4801406761883
At time: 603.5742280483246 and batch: 300, loss is 5.299954261779785 and perplexity is 200.32764713520749
At time: 604.5426590442657 and batch: 350, loss is 5.304692325592041 and perplexity is 201.2790644688295
At time: 605.5202958583832 and batch: 400, loss is 5.203461942672729 and perplexity is 181.90088351748557
At time: 606.5133965015411 and batch: 450, loss is 5.250067262649536 and perplexity is 190.5790868818545
At time: 607.4889869689941 and batch: 500, loss is 5.203019056320191 and perplexity is 181.82033993580407
At time: 608.4605567455292 and batch: 550, loss is 5.288516817092895 and perplexity is 198.04946388432083
At time: 609.4368977546692 and batch: 600, loss is 5.238076190948487 and perplexity is 188.3074860690415
At time: 610.4161386489868 and batch: 650, loss is 5.142577800750733 and perplexity is 171.1564072527417
At time: 611.3884983062744 and batch: 700, loss is 5.266839084625244 and perplexity is 193.80240026053417
At time: 612.3930492401123 and batch: 750, loss is 5.253456630706787 and perplexity is 191.22612545783096
At time: 613.3907923698425 and batch: 800, loss is 5.248683032989502 and perplexity is 190.3154641565018
At time: 614.3672912120819 and batch: 850, loss is 5.3030480289459225 and perplexity is 200.94837392936972
At time: 615.335517168045 and batch: 900, loss is 5.220961856842041 and perplexity is 185.11214985604252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.332220208154966 and perplexity of 206.89681860663717
finished 32 epochs...
Completing Train Step...
At time: 617.6329193115234 and batch: 50, loss is 5.2787933254241945 and perplexity is 196.13306372177826
At time: 618.5960745811462 and batch: 100, loss is 5.220527772903442 and perplexity is 185.03181308266355
At time: 619.5548388957977 and batch: 150, loss is 5.275510768890381 and perplexity is 195.49030138097868
At time: 620.510730266571 and batch: 200, loss is 5.1894047069549565 and perplexity is 179.36174834363638
At time: 621.4784610271454 and batch: 250, loss is 5.325243644714355 and perplexity is 205.45841322339217
At time: 622.4488055706024 and batch: 300, loss is 5.285934610366821 and perplexity is 197.5387189349003
At time: 623.4199013710022 and batch: 350, loss is 5.290337677001953 and perplexity is 198.41041273203317
At time: 624.3810205459595 and batch: 400, loss is 5.189324150085449 and perplexity is 179.3473001046407
At time: 625.3421456813812 and batch: 450, loss is 5.235939426422119 and perplexity is 187.90554689037893
At time: 626.3094892501831 and batch: 500, loss is 5.1882577323913575 and perplexity is 179.1561429152422
At time: 627.276127576828 and batch: 550, loss is 5.274069576263428 and perplexity is 195.20876512270743
At time: 628.2528805732727 and batch: 600, loss is 5.223760042190552 and perplexity is 185.63085333739483
At time: 629.230128288269 and batch: 650, loss is 5.128175172805786 and perplexity is 168.7089722535115
At time: 630.2106034755707 and batch: 700, loss is 5.251990280151367 and perplexity is 190.945926407778
At time: 631.1950862407684 and batch: 750, loss is 5.239597425460816 and perplexity is 188.59416391260967
At time: 632.1774101257324 and batch: 800, loss is 5.233940305709839 and perplexity is 187.53027625027303
At time: 633.3408620357513 and batch: 850, loss is 5.288490447998047 and perplexity is 198.04424156807724
At time: 634.3080558776855 and batch: 900, loss is 5.207308511734009 and perplexity is 182.60192526660515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.3196432035263275 and perplexity of 204.31097155246493
finished 33 epochs...
Completing Train Step...
At time: 636.6232614517212 and batch: 50, loss is 5.265465888977051 and perplexity is 193.53645428760387
At time: 637.5968267917633 and batch: 100, loss is 5.206727123260498 and perplexity is 182.49579346690612
At time: 638.5627775192261 and batch: 150, loss is 5.261654195785522 and perplexity is 192.80015686389382
At time: 639.5483994483948 and batch: 200, loss is 5.175314903259277 and perplexity is 176.85229687335266
At time: 640.5225877761841 and batch: 250, loss is 5.3112647914886475 and perplexity is 202.6063211525739
At time: 641.504298210144 and batch: 300, loss is 5.27251049041748 and perplexity is 194.90465502847275
At time: 642.4819014072418 and batch: 350, loss is 5.276595258712769 and perplexity is 195.70242362461892
At time: 643.4852452278137 and batch: 400, loss is 5.17576907157898 and perplexity is 176.93263582618184
At time: 644.4873836040497 and batch: 450, loss is 5.222375221252442 and perplexity is 185.37396575763694
At time: 645.4705793857574 and batch: 500, loss is 5.174133100509644 and perplexity is 176.6434157950563
At time: 646.4525089263916 and batch: 550, loss is 5.26024338722229 and perplexity is 192.528344534263
At time: 647.4296476840973 and batch: 600, loss is 5.210032653808594 and perplexity is 183.10003700966413
At time: 648.4149770736694 and batch: 650, loss is 5.114371223449707 and perplexity is 166.39612210360366
At time: 649.3965184688568 and batch: 700, loss is 5.237715215682983 and perplexity is 188.2395239913139
At time: 650.3717033863068 and batch: 750, loss is 5.22626841545105 and perplexity is 186.0970692835862
At time: 651.3496794700623 and batch: 800, loss is 5.219781513214111 and perplexity is 184.89378280893652
At time: 652.3317565917969 and batch: 850, loss is 5.274519786834717 and perplexity is 195.2966699587324
At time: 653.3138637542725 and batch: 900, loss is 5.1942065334320064 and perplexity is 180.2250834696717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.307669025577911 and perplexity of 201.8791044836751
finished 34 epochs...
Completing Train Step...
At time: 655.6746933460236 and batch: 50, loss is 5.252722425460815 and perplexity is 191.08577776166496
At time: 656.6463568210602 and batch: 100, loss is 5.1935210800170895 and perplexity is 180.10158990013343
At time: 657.6173825263977 and batch: 150, loss is 5.248339576721191 and perplexity is 190.2501103411121
At time: 658.5791113376617 and batch: 200, loss is 5.161777429580688 and perplexity is 174.47429593852263
At time: 659.5435645580292 and batch: 250, loss is 5.297883825302124 and perplexity is 199.91331054390272
At time: 660.5038712024689 and batch: 300, loss is 5.259651536941528 and perplexity is 192.41443029290897
At time: 661.4660558700562 and batch: 350, loss is 5.263425416946411 and perplexity is 193.14195118886735
At time: 662.4301073551178 and batch: 400, loss is 5.162766981124878 and perplexity is 174.6470326993587
At time: 663.3935220241547 and batch: 450, loss is 5.209340562820435 and perplexity is 182.97335896554097
At time: 664.3550019264221 and batch: 500, loss is 5.160597267150879 and perplexity is 174.26850938415333
At time: 665.3180341720581 and batch: 550, loss is 5.24700737953186 and perplexity is 189.99682842703209
At time: 666.296950340271 and batch: 600, loss is 5.196872930526734 and perplexity is 180.70627634918654
At time: 667.2774133682251 and batch: 650, loss is 5.101127290725708 and perplexity is 164.2069119322253
At time: 668.25359416008 and batch: 700, loss is 5.223988075256347 and perplexity is 185.67318813667114
At time: 669.2247884273529 and batch: 750, loss is 5.213444385528565 and perplexity is 183.72579206105894
At time: 670.200719833374 and batch: 800, loss is 5.206179475784301 and perplexity is 182.39587746806706
At time: 671.1731534004211 and batch: 850, loss is 5.261101541519165 and perplexity is 192.69363447238692
At time: 672.1505768299103 and batch: 900, loss is 5.181633558273315 and perplexity is 177.97330342284576
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.296252943065069 and perplexity of 199.58754119471436
finished 35 epochs...
Completing Train Step...
At time: 674.4698603153229 and batch: 50, loss is 5.240532112121582 and perplexity is 188.77052276921307
At time: 675.4394414424896 and batch: 100, loss is 5.180859251022339 and perplexity is 177.83555074185756
At time: 676.3978371620178 and batch: 150, loss is 5.235538759231567 and perplexity is 187.8302743834362
At time: 677.3580212593079 and batch: 200, loss is 5.1487515354156494 and perplexity is 172.21635003342345
At time: 678.3156580924988 and batch: 250, loss is 5.285071334838867 and perplexity is 197.36826217917928
At time: 679.273598909378 and batch: 300, loss is 5.247319316864013 and perplexity is 190.05610477558153
At time: 680.2350442409515 and batch: 350, loss is 5.250784893035888 and perplexity is 190.71590131082834
At time: 681.1987669467926 and batch: 400, loss is 5.150283374786377 and perplexity is 172.48035997747527
At time: 682.1674058437347 and batch: 450, loss is 5.196800708770752 and perplexity is 180.69322589586065
At time: 683.1478333473206 and batch: 500, loss is 5.14760087966919 and perplexity is 172.01830226487607
At time: 684.1333231925964 and batch: 550, loss is 5.2343213176727295 and perplexity is 187.60174114255338
At time: 685.1144461631775 and batch: 600, loss is 5.184249086380005 and perplexity is 178.43940688782794
At time: 686.0875096321106 and batch: 650, loss is 5.08840196609497 and perplexity is 162.13056478618782
At time: 687.0615510940552 and batch: 700, loss is 5.210777635574341 and perplexity is 183.23649402122555
At time: 688.0436735153198 and batch: 750, loss is 5.2010972118377685 and perplexity is 181.47124507916647
At time: 689.0258414745331 and batch: 800, loss is 5.193099155426025 and perplexity is 180.02561663908537
At time: 689.9983248710632 and batch: 850, loss is 5.248205099105835 and perplexity is 190.22452768013835
At time: 690.9760797023773 and batch: 900, loss is 5.169560585021973 and perplexity is 175.8375548504345
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.285344790105951 and perplexity of 197.42224095008055
finished 36 epochs...
Completing Train Step...
At time: 693.2822394371033 and batch: 50, loss is 5.228855142593384 and perplexity is 186.579074763375
At time: 694.266101360321 and batch: 100, loss is 5.168678541183471 and perplexity is 175.68252679942108
At time: 695.2347114086151 and batch: 150, loss is 5.22321515083313 and perplexity is 185.52973224224775
At time: 696.1974527835846 and batch: 200, loss is 5.136195802688599 and perplexity is 170.06756558174854
At time: 697.174563407898 and batch: 250, loss is 5.272791090011597 and perplexity is 194.95935286930225
At time: 698.1517102718353 and batch: 300, loss is 5.235473480224609 and perplexity is 187.81801340984424
At time: 699.1213824748993 and batch: 350, loss is 5.238629837036132 and perplexity is 188.4117706376859
At time: 700.0899634361267 and batch: 400, loss is 5.138280878067016 and perplexity is 170.42253922020205
At time: 701.0811479091644 and batch: 450, loss is 5.1847184371948245 and perplexity is 178.52317722614717
At time: 702.0664865970612 and batch: 500, loss is 5.1350954246521 and perplexity is 169.88052989171658
At time: 703.060259103775 and batch: 550, loss is 5.222138786315918 and perplexity is 185.33014205674107
At time: 704.0431082248688 and batch: 600, loss is 5.1721239757537845 and perplexity is 176.28887341462135
At time: 705.0144731998444 and batch: 650, loss is 5.076156244277954 and perplexity is 160.1572658856983
At time: 705.9780323505402 and batch: 700, loss is 5.198046703338623 and perplexity is 180.918508995391
At time: 706.9423186779022 and batch: 750, loss is 5.189198226928711 and perplexity is 179.32471754832227
At time: 707.9056837558746 and batch: 800, loss is 5.180500183105469 and perplexity is 177.7717071638835
At time: 708.8813264369965 and batch: 850, loss is 5.235791606903076 and perplexity is 187.87777283563636
At time: 709.8647594451904 and batch: 900, loss is 5.157951078414917 and perplexity is 173.8079716212442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.27488687593643 and perplexity of 195.3683743980289
finished 37 epochs...
Completing Train Step...
At time: 712.1770017147064 and batch: 50, loss is 5.217646265029908 and perplexity is 184.49940988679433
At time: 713.1685817241669 and batch: 100, loss is 5.156955547332764 and perplexity is 173.6350264835829
At time: 714.1440961360931 and batch: 150, loss is 5.211321678161621 and perplexity is 183.33620959981815
At time: 715.1147766113281 and batch: 200, loss is 5.124071807861328 and perplexity is 168.0181161576567
At time: 716.0810348987579 and batch: 250, loss is 5.260998106002807 and perplexity is 192.6737041375765
At time: 717.0444648265839 and batch: 300, loss is 5.224072942733764 and perplexity is 185.68894642044575
At time: 718.0252614021301 and batch: 350, loss is 5.226921548843384 and perplexity is 186.21865519533816
At time: 719.0017416477203 and batch: 400, loss is 5.126724548339844 and perplexity is 168.46441631303702
At time: 719.9798395633698 and batch: 450, loss is 5.1730552005767825 and perplexity is 176.45311445035924
At time: 720.9493410587311 and batch: 500, loss is 5.123040685653686 and perplexity is 167.84495823563185
At time: 721.9329407215118 and batch: 550, loss is 5.210412626266479 and perplexity is 183.16962320034634
At time: 722.903404712677 and batch: 600, loss is 5.16045693397522 and perplexity is 174.24405544670347
At time: 723.8676903247833 and batch: 650, loss is 5.064351730346679 and perplexity is 158.27780211218644
At time: 724.8284802436829 and batch: 700, loss is 5.185757503509522 and perplexity is 178.70877105144345
At time: 725.8043727874756 and batch: 750, loss is 5.17771803855896 and perplexity is 177.27780794635967
At time: 726.7827270030975 and batch: 800, loss is 5.168341636657715 and perplexity is 175.62334853031913
At time: 727.7754485607147 and batch: 850, loss is 5.223818235397339 and perplexity is 185.64165610634993
At time: 728.7534489631653 and batch: 900, loss is 5.1467683887481686 and perplexity is 171.87515818133312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.264840740047089 and perplexity of 193.4155029905234
finished 38 epochs...
Completing Train Step...
At time: 731.0565016269684 and batch: 50, loss is 5.206861610412598 and perplexity is 182.52033845689564
At time: 732.0250630378723 and batch: 100, loss is 5.145657777786255 and perplexity is 171.68437770762895
At time: 732.9802920818329 and batch: 150, loss is 5.199815740585327 and perplexity is 181.23884383501303
At time: 733.9507002830505 and batch: 200, loss is 5.1123464012146 and perplexity is 166.05954040977755
At time: 734.9328739643097 and batch: 250, loss is 5.249647798538208 and perplexity is 190.49916255840245
At time: 735.9122025966644 and batch: 300, loss is 5.213076467514038 and perplexity is 183.6582084658
At time: 736.8849632740021 and batch: 350, loss is 5.215621099472046 and perplexity is 184.12614612447564
At time: 737.8654446601868 and batch: 400, loss is 5.115579557418823 and perplexity is 166.5973057142876
At time: 738.8448095321655 and batch: 450, loss is 5.1617751121521 and perplexity is 174.47389160726976
At time: 739.8331339359283 and batch: 500, loss is 5.111400365829468 and perplexity is 165.902516495312
At time: 740.8094425201416 and batch: 550, loss is 5.199096508026123 and perplexity is 181.10853782331475
At time: 741.7930107116699 and batch: 600, loss is 5.149207706451416 and perplexity is 172.29492806534265
At time: 742.7784757614136 and batch: 650, loss is 5.052951030731201 and perplexity is 156.48357160986552
At time: 743.7573034763336 and batch: 700, loss is 5.173874950408935 and perplexity is 176.59782116486176
At time: 744.7360198497772 and batch: 750, loss is 5.166624250411988 and perplexity is 175.32199425203916
At time: 745.7395541667938 and batch: 800, loss is 5.156586227416992 and perplexity is 173.57091145043495
At time: 746.7078764438629 and batch: 850, loss is 5.212247648239136 and perplexity is 183.50605206641802
At time: 747.6782464981079 and batch: 900, loss is 5.135974197387696 and perplexity is 170.02988188331176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.255166249732449 and perplexity of 191.55332889703394
finished 39 epochs...
Completing Train Step...
At time: 750.002525806427 and batch: 50, loss is 5.196459884643555 and perplexity is 180.6316517784227
At time: 750.9536864757538 and batch: 100, loss is 5.13474850654602 and perplexity is 169.82160548159013
At time: 751.9100782871246 and batch: 150, loss is 5.188659963607788 and perplexity is 179.22821960331314
At time: 752.9178686141968 and batch: 200, loss is 5.100989170074463 and perplexity is 164.18423313285157
At time: 753.8920347690582 and batch: 250, loss is 5.238695383071899 and perplexity is 188.424120687087
At time: 754.8513290882111 and batch: 300, loss is 5.2024482727050785 and perplexity is 181.7165894772223
At time: 755.8272559642792 and batch: 350, loss is 5.204687070846558 and perplexity is 182.12387198156108
At time: 756.8054072856903 and batch: 400, loss is 5.104809989929199 and perplexity is 164.81275147369092
At time: 757.7866621017456 and batch: 450, loss is 5.150847635269165 and perplexity is 172.57771129183448
At time: 758.7653079032898 and batch: 500, loss is 5.100141515731812 and perplexity is 164.04512062263288
At time: 759.7462658882141 and batch: 550, loss is 5.1881482219696045 and perplexity is 179.13652452470063
At time: 760.7286412715912 and batch: 600, loss is 5.138338575363159 and perplexity is 170.4323724235887
At time: 761.6991589069366 and batch: 650, loss is 5.041917772293091 and perplexity is 154.76653758131084
At time: 762.676299571991 and batch: 700, loss is 5.1623671436309815 and perplexity is 174.57721622603975
At time: 763.6669528484344 and batch: 750, loss is 5.155882968902588 and perplexity is 173.44888914074772
At time: 764.6607835292816 and batch: 800, loss is 5.145199270248413 and perplexity is 171.60567717009388
At time: 765.6446695327759 and batch: 850, loss is 5.201044826507569 and perplexity is 181.46173889706563
At time: 766.6181676387787 and batch: 900, loss is 5.1255300331115725 and perplexity is 168.26330314261503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.245840412296661 and perplexity of 189.77523767318777
finished 40 epochs...
Completing Train Step...
At time: 768.9454445838928 and batch: 50, loss is 5.186405410766602 and perplexity is 178.82459527873849
At time: 769.8957126140594 and batch: 100, loss is 5.124194440841674 and perplexity is 168.03872198344442
At time: 770.855681180954 and batch: 150, loss is 5.177821369171142 and perplexity is 177.29612711723038
At time: 771.8030362129211 and batch: 200, loss is 5.089972257614136 and perplexity is 162.38535703376593
At time: 772.7495999336243 and batch: 250, loss is 5.228099050521851 and perplexity is 186.4380571221108
At time: 773.7049939632416 and batch: 300, loss is 5.1921569442749025 and perplexity is 179.85607438044642
At time: 774.6677725315094 and batch: 350, loss is 5.194082336425781 and perplexity is 180.20270144377716
At time: 775.6373674869537 and batch: 400, loss is 5.094380521774292 and perplexity is 163.10277470483035
At time: 776.6036200523376 and batch: 450, loss is 5.140246496200562 and perplexity is 170.75785429639345
At time: 777.5820698738098 and batch: 500, loss is 5.089234037399292 and perplexity is 162.2655251172226
At time: 778.5583872795105 and batch: 550, loss is 5.177529840469361 and perplexity is 177.24444774083773
At time: 779.5408589839935 and batch: 600, loss is 5.12781665802002 and perplexity is 168.64849843349424
At time: 780.5174913406372 and batch: 650, loss is 5.031218528747559 and perplexity is 153.11947955257554
At time: 781.4935877323151 and batch: 700, loss is 5.151203651428222 and perplexity is 172.63916268394271
At time: 782.4738821983337 and batch: 750, loss is 5.14546106338501 and perplexity is 171.65060823964436
At time: 783.4636063575745 and batch: 800, loss is 5.134150218963623 and perplexity is 169.72003371140192
At time: 784.4489448070526 and batch: 850, loss is 5.19017804145813 and perplexity is 179.50050861931902
At time: 785.4353165626526 and batch: 900, loss is 5.115401248931885 and perplexity is 166.5676026490107
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.2368360545537245 and perplexity of 188.0741038449667
finished 41 epochs...
Completing Train Step...
At time: 787.7818973064423 and batch: 50, loss is 5.176668186187744 and perplexity is 177.09179008205672
At time: 788.7731926441193 and batch: 100, loss is 5.113965816497803 and perplexity is 166.32867763108575
At time: 789.7605652809143 and batch: 150, loss is 5.1672714328765865 and perplexity is 175.43549629668243
At time: 790.7328667640686 and batch: 200, loss is 5.079270448684692 and perplexity is 160.65680577983784
At time: 791.7033715248108 and batch: 250, loss is 5.217821760177612 and perplexity is 184.5317914793069
At time: 792.668356180191 and batch: 300, loss is 5.18217619895935 and perplexity is 178.06990518596416
At time: 793.6365251541138 and batch: 350, loss is 5.183776397705078 and perplexity is 178.35508053265804
At time: 794.6259565353394 and batch: 400, loss is 5.0842588424682615 and perplexity is 161.46022741679494
At time: 795.5930552482605 and batch: 450, loss is 5.129948949813842 and perplexity is 169.008489909783
At time: 796.5539226531982 and batch: 500, loss is 5.078650693893433 and perplexity is 160.55726880215278
At time: 797.5165510177612 and batch: 550, loss is 5.167208709716797 and perplexity is 175.42449277310692
At time: 798.4873764514923 and batch: 600, loss is 5.1176120471954345 and perplexity is 166.93625737617208
At time: 799.4527454376221 and batch: 650, loss is 5.0208235931396485 and perplexity is 151.53605647079544
At time: 800.4217936992645 and batch: 700, loss is 5.140356922149659 and perplexity is 170.77671143565942
At time: 801.3953697681427 and batch: 750, loss is 5.135329122543335 and perplexity is 169.92023525266305
At time: 802.3561143875122 and batch: 800, loss is 5.123411808013916 and perplexity is 167.9072608129096
At time: 803.3229084014893 and batch: 850, loss is 5.179621276855468 and perplexity is 177.61553114147853
At time: 804.2878713607788 and batch: 900, loss is 5.1055583667755124 and perplexity is 164.93613968555258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.228133110150899 and perplexity of 186.4444072413175
finished 42 epochs...
Completing Train Step...
At time: 806.6307570934296 and batch: 50, loss is 5.167222747802734 and perplexity is 175.42695541449731
At time: 807.5891497135162 and batch: 100, loss is 5.104033966064453 and perplexity is 164.68490245869214
At time: 808.5550227165222 and batch: 150, loss is 5.156986808776855 and perplexity is 173.6404546501013
At time: 809.5331797599792 and batch: 200, loss is 5.068860464096069 and perplexity is 158.9930457903413
At time: 810.5097301006317 and batch: 250, loss is 5.207830972671509 and perplexity is 182.6973525660167
At time: 811.4786560535431 and batch: 300, loss is 5.172482414245605 and perplexity is 176.35207345852044
At time: 812.4367518424988 and batch: 350, loss is 5.17374584197998 and perplexity is 176.57502236940428
At time: 813.407369852066 and batch: 400, loss is 5.0744172477722165 and perplexity is 159.8789949861423
At time: 814.3804521560669 and batch: 450, loss is 5.1199345207214355 and perplexity is 167.32441298054334
At time: 815.3450894355774 and batch: 500, loss is 5.068366298675537 and perplexity is 158.91449633481747
At time: 816.315999507904 and batch: 550, loss is 5.157157392501831 and perplexity is 173.67007741217068
At time: 817.2870028018951 and batch: 600, loss is 5.107697458267212 and perplexity is 165.28933079800439
At time: 818.2515187263489 and batch: 650, loss is 5.010706653594971 and perplexity is 150.0107043067297
At time: 819.239265203476 and batch: 700, loss is 5.129802131652832 and perplexity is 168.9836782155476
At time: 820.2249503135681 and batch: 750, loss is 5.125461149215698 and perplexity is 168.25171290995664
At time: 821.2054908275604 and batch: 800, loss is 5.112961511611939 and perplexity is 166.16171678131357
At time: 822.1732633113861 and batch: 850, loss is 5.1693549156188965 and perplexity is 175.80139416419084
At time: 823.1478540897369 and batch: 900, loss is 5.09597692489624 and perplexity is 163.36336042815662
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.219703569804152 and perplexity of 184.87937211864062
finished 43 epochs...
Completing Train Step...
At time: 825.4820611476898 and batch: 50, loss is 5.158048706054688 and perplexity is 173.8249409116093
At time: 826.4469838142395 and batch: 100, loss is 5.094370765686035 and perplexity is 163.10118346752745
At time: 827.4116263389587 and batch: 150, loss is 5.146947574615479 and perplexity is 171.905958540033
At time: 828.3969669342041 and batch: 200, loss is 5.058720369338989 and perplexity is 157.38898763543855
At time: 829.3894176483154 and batch: 250, loss is 5.198095178604126 and perplexity is 180.92727928071818
At time: 830.3583629131317 and batch: 300, loss is 5.163054151535034 and perplexity is 174.6971933613558
At time: 831.3403480052948 and batch: 350, loss is 5.163970651626587 and perplexity is 174.85737674789732
At time: 832.3213760852814 and batch: 400, loss is 5.064832506179809 and perplexity is 158.35391654988786
At time: 833.2906713485718 and batch: 450, loss is 5.110186414718628 and perplexity is 165.70124114505683
At time: 834.26664686203 and batch: 500, loss is 5.058359127044678 and perplexity is 157.33214234452572
At time: 835.2444446086884 and batch: 550, loss is 5.147356214523316 and perplexity is 171.97622053003627
At time: 836.2232990264893 and batch: 600, loss is 5.098050384521485 and perplexity is 163.7024391718095
At time: 837.2025067806244 and batch: 650, loss is 5.000845422744751 and perplexity is 148.53868401623436
At time: 838.1785020828247 and batch: 700, loss is 5.1195188331604005 and perplexity is 167.25487275791397
At time: 839.1485958099365 and batch: 750, loss is 5.115836563110352 and perplexity is 166.64012767255727
At time: 840.1217992305756 and batch: 800, loss is 5.1027818775177005 and perplexity is 164.47883141501933
At time: 841.1027114391327 and batch: 850, loss is 5.159362545013428 and perplexity is 174.05346898264793
At time: 842.0691697597504 and batch: 900, loss is 5.086639614105224 and perplexity is 161.84508529430238
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.211520678376498 and perplexity of 183.37269717532058
finished 44 epochs...
Completing Train Step...
At time: 844.4348051548004 and batch: 50, loss is 5.1491272830963135 and perplexity is 172.2810720863401
At time: 845.4195861816406 and batch: 100, loss is 5.084949398040772 and perplexity is 161.5717631829665
At time: 846.3877053260803 and batch: 150, loss is 5.137137441635132 and perplexity is 170.22778324680053
At time: 847.3590505123138 and batch: 200, loss is 5.048829755783081 and perplexity is 155.83998689211987
At time: 848.3353545665741 and batch: 250, loss is 5.18859167098999 and perplexity is 179.21598005695336
At time: 849.2986423969269 and batch: 300, loss is 5.153872747421264 and perplexity is 173.10056867582244
At time: 850.2692215442657 and batch: 350, loss is 5.1544340801239015 and perplexity is 173.19776296247898
At time: 851.2394392490387 and batch: 400, loss is 5.05548623085022 and perplexity is 156.88079208317842
At time: 852.2400236129761 and batch: 450, loss is 5.100692939758301 and perplexity is 164.13560398863078
At time: 853.22039270401 and batch: 500, loss is 5.048611850738525 and perplexity is 155.80603227240837
At time: 854.1911995410919 and batch: 550, loss is 5.137789926528931 and perplexity is 170.33889054784976
At time: 855.165421962738 and batch: 600, loss is 5.08865234375 and perplexity is 162.1711637391308
At time: 856.1278510093689 and batch: 650, loss is 4.991224203109741 and perplexity is 147.11641367206045
At time: 857.1029148101807 and batch: 700, loss is 5.109490699768067 and perplexity is 165.5860004062663
At time: 858.0734963417053 and batch: 750, loss is 5.106439619064331 and perplexity is 165.08155410014146
At time: 859.045093536377 and batch: 800, loss is 5.092858409881591 and perplexity is 162.85470287615485
At time: 860.0110146999359 and batch: 850, loss is 5.149627895355224 and perplexity is 172.36733969451427
At time: 860.991810798645 and batch: 900, loss is 5.077534265518189 and perplexity is 160.3781181344627
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.203561025123074 and perplexity of 181.91890759566456
finished 45 epochs...
Completing Train Step...
At time: 863.3675482273102 and batch: 50, loss is 5.140441427230835 and perplexity is 170.79114354530702
At time: 864.3216581344604 and batch: 100, loss is 5.0757495784759525 and perplexity is 160.09214864409552
At time: 865.286141872406 and batch: 150, loss is 5.1275424385070805 and perplexity is 168.60225806469128
At time: 866.245532989502 and batch: 200, loss is 5.039173173904419 and perplexity is 154.34234797298126
At time: 867.2280879020691 and batch: 250, loss is 5.1793106842041015 and perplexity is 177.56037362893977
At time: 868.1917674541473 and batch: 300, loss is 5.144923133850098 and perplexity is 171.5582971384464
At time: 869.1757128238678 and batch: 350, loss is 5.145120534896851 and perplexity is 171.5921662686716
At time: 870.1506009101868 and batch: 400, loss is 5.04636441230774 and perplexity is 155.45626099965529
At time: 871.1448469161987 and batch: 450, loss is 5.091445388793946 and perplexity is 162.62474825039743
At time: 872.1190030574799 and batch: 500, loss is 5.0391129112243656 and perplexity is 154.3330471696952
At time: 873.0913758277893 and batch: 550, loss is 5.128447141647339 and perplexity is 168.75486207727303
At time: 874.0660483837128 and batch: 600, loss is 5.079490718841552 and perplexity is 160.6921975773829
At time: 875.0382304191589 and batch: 650, loss is 4.981834411621094 and perplexity is 145.74148646506978
At time: 876.028885602951 and batch: 700, loss is 5.099704437255859 and perplexity is 163.9734356984336
At time: 877.0147435665131 and batch: 750, loss is 5.097260189056397 and perplexity is 163.5731333419206
At time: 878.006511926651 and batch: 800, loss is 5.083179912567139 and perplexity is 161.28611709290013
At time: 878.9992425441742 and batch: 850, loss is 5.140136604309082 and perplexity is 170.73909042381737
At time: 879.9858798980713 and batch: 900, loss is 5.068652658462525 and perplexity is 158.96000957240668
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.195813322720462 and perplexity of 180.51489997797904
finished 46 epochs...
Completing Train Step...
At time: 882.357602596283 and batch: 50, loss is 5.131976146697998 and perplexity is 169.35145090119988
At time: 883.3314940929413 and batch: 100, loss is 5.066759958267212 and perplexity is 158.65943047403184
At time: 884.3225955963135 and batch: 150, loss is 5.118148565292358 and perplexity is 167.02584573002468
At time: 885.2995433807373 and batch: 200, loss is 5.029740905761718 and perplexity is 152.89339376592207
At time: 886.2695798873901 and batch: 250, loss is 5.1702454471588135 and perplexity is 175.95802058048628
At time: 887.2295744419098 and batch: 300, loss is 5.136193180084229 and perplexity is 170.0671195623927
At time: 888.1941931247711 and batch: 350, loss is 5.136019563674926 and perplexity is 170.03759568274302
At time: 889.1521065235138 and batch: 400, loss is 5.037458658218384 and perplexity is 154.07795231637974
At time: 890.1310932636261 and batch: 450, loss is 5.082434997558594 and perplexity is 161.16601738120022
At time: 891.0974276065826 and batch: 500, loss is 5.0298534107208255 and perplexity is 152.91059599858556
At time: 892.0916764736176 and batch: 550, loss is 5.11931887626648 and perplexity is 167.221432336497
At time: 893.0568673610687 and batch: 600, loss is 5.07055495262146 and perplexity is 159.26268606871093
At time: 894.0200667381287 and batch: 650, loss is 4.972671890258789 and perplexity is 144.4122259702704
At time: 894.9903643131256 and batch: 700, loss is 5.090148916244507 and perplexity is 162.41404634256426
At time: 895.9619793891907 and batch: 750, loss is 5.088291463851928 and perplexity is 162.11264998494437
At time: 896.9316682815552 and batch: 800, loss is 5.07373703956604 and perplexity is 159.77028096003545
At time: 897.9122002124786 and batch: 850, loss is 5.130875558853149 and perplexity is 169.16516728237752
At time: 898.8923096656799 and batch: 900, loss is 5.059985284805298 and perplexity is 157.58819736529802
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.188267537992295 and perplexity of 179.15789965749806
finished 47 epochs...
Completing Train Step...
At time: 901.2214512825012 and batch: 50, loss is 5.123718872070312 and perplexity is 167.95882701417145
At time: 902.1907525062561 and batch: 100, loss is 5.057973623275757 and perplexity is 157.2715018999904
At time: 903.1506390571594 and batch: 150, loss is 5.108941459655762 and perplexity is 165.49507890396083
At time: 904.1146285533905 and batch: 200, loss is 5.020525341033935 and perplexity is 151.49086726208486
At time: 905.0755839347839 and batch: 250, loss is 5.161388034820557 and perplexity is 174.40636978780842
At time: 906.0444209575653 and batch: 300, loss is 5.127672281265259 and perplexity is 168.6241512682197
At time: 907.0379147529602 and batch: 350, loss is 5.127124452590943 and perplexity is 168.53179942177727
At time: 908.0262851715088 and batch: 400, loss is 5.028762407302857 and perplexity is 152.7438609864801
At time: 909.0248563289642 and batch: 450, loss is 5.073646974563599 and perplexity is 159.75589189727603
At time: 910.0115346908569 and batch: 500, loss is 5.020822124481201 and perplexity is 151.53583391624946
At time: 911.0136704444885 and batch: 550, loss is 5.110394287109375 and perplexity is 165.73568943850387
At time: 911.9936146736145 and batch: 600, loss is 5.061838512420654 and perplexity is 157.88051494635388
At time: 912.9863827228546 and batch: 650, loss is 4.963729887008667 and perplexity is 143.12664776469595
At time: 913.9633519649506 and batch: 700, loss is 5.0808132457733155 and perplexity is 160.90485792992834
At time: 914.9434552192688 and batch: 750, loss is 5.079526691436768 and perplexity is 160.69797819673195
At time: 915.9407320022583 and batch: 800, loss is 5.064521102905274 and perplexity is 158.3046122988897
At time: 916.9239048957825 and batch: 850, loss is 5.121832399368286 and perplexity is 167.64227594829518
At time: 917.9049940109253 and batch: 900, loss is 5.051523094177246 and perplexity is 156.26028245718257
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.180917818252355 and perplexity of 177.8459663824675
finished 48 epochs...
Completing Train Step...
At time: 920.2527053356171 and batch: 50, loss is 5.11565978050232 and perplexity is 166.6106711999572
At time: 921.2188994884491 and batch: 100, loss is 5.049383058547973 and perplexity is 155.92623744688285
At time: 922.198700428009 and batch: 150, loss is 5.099910554885864 and perplexity is 164.0072369977857
At time: 923.1586475372314 and batch: 200, loss is 5.011517305374145 and perplexity is 150.13236005463315
At time: 924.1183321475983 and batch: 250, loss is 5.152729129791259 and perplexity is 172.90272096634834
At time: 925.0761604309082 and batch: 300, loss is 5.119350652694703 and perplexity is 167.22674612076506
At time: 926.0518929958344 and batch: 350, loss is 5.11842869758606 and perplexity is 167.07264161751007
At time: 927.0243558883667 and batch: 400, loss is 5.020265703201294 and perplexity is 151.4515396073384
At time: 927.9951283931732 and batch: 450, loss is 5.0650636672973635 and perplexity is 158.39052604939
At time: 928.9547250270844 and batch: 500, loss is 5.012003536224365 and perplexity is 150.20537678976413
At time: 929.9128241539001 and batch: 550, loss is 5.10166296005249 and perplexity is 164.29489610129937
At time: 930.8864011764526 and batch: 600, loss is 5.053332262039184 and perplexity is 156.54323941944415
At time: 931.8663356304169 and batch: 650, loss is 4.954997510910034 and perplexity is 141.88225321482608
At time: 932.8366038799286 and batch: 700, loss is 5.071685810089111 and perplexity is 159.44289134060406
At time: 933.8314533233643 and batch: 750, loss is 5.070959033966065 and perplexity is 159.3270541531381
At time: 934.81005692482 and batch: 800, loss is 5.0555230903625485 and perplexity is 156.88657473924061
At time: 935.7680096626282 and batch: 850, loss is 5.112993822097779 and perplexity is 166.1670856338454
At time: 936.7501294612885 and batch: 900, loss is 5.043258152008057 and perplexity is 154.97412259913463
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.173757474716395 and perplexity of 176.57707643204415
finished 49 epochs...
Completing Train Step...
At time: 939.1056036949158 and batch: 50, loss is 5.107788000106812 and perplexity is 165.30429707560802
At time: 940.0743353366852 and batch: 100, loss is 5.040980930328369 and perplexity is 154.62161369016115
At time: 941.0465922355652 and batch: 150, loss is 5.091052923202515 and perplexity is 162.56093615523895
At time: 942.0051667690277 and batch: 200, loss is 5.002707471847534 and perplexity is 148.81552800809862
At time: 942.9810283184052 and batch: 250, loss is 5.144260663986206 and perplexity is 171.44468257395923
At time: 943.9734046459198 and batch: 300, loss is 5.111215572357178 and perplexity is 165.87186162572436
At time: 944.9656636714935 and batch: 350, loss is 5.109924659729004 and perplexity is 165.65787369447114
At time: 945.9358866214752 and batch: 400, loss is 5.011953125 and perplexity is 150.19780494366847
At time: 946.9053816795349 and batch: 450, loss is 5.056675310134888 and perplexity is 157.0674467348039
At time: 947.8829524517059 and batch: 500, loss is 5.003386812210083 and perplexity is 148.91665875005663
At time: 948.8738622665405 and batch: 550, loss is 5.09312177658081 and perplexity is 162.8975990301649
At time: 949.8619318008423 and batch: 600, loss is 5.045024423599243 and perplexity is 155.2480908692115
At time: 950.8644328117371 and batch: 650, loss is 4.946464309692383 and perplexity is 140.67669435238108
At time: 951.8467171192169 and batch: 700, loss is 5.062756795883178 and perplexity is 158.0255605986034
At time: 952.8179116249084 and batch: 750, loss is 5.062581100463867 and perplexity is 157.99779867036537
At time: 953.7892217636108 and batch: 800, loss is 5.04673324584961 and perplexity is 155.51360905829935
At time: 954.7737038135529 and batch: 850, loss is 5.1043466091156 and perplexity is 164.7363980985323
At time: 955.7692651748657 and batch: 900, loss is 5.03518328666687 and perplexity is 153.72776627972158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.16678358104131 and perplexity of 175.3499306412195
finished 50 epochs...
Completing Train Step...
At time: 958.1695599555969 and batch: 50, loss is 5.100092926025391 and perplexity is 164.03714991203083
At time: 959.1307122707367 and batch: 100, loss is 5.032761478424073 and perplexity is 153.3559175630879
At time: 960.0881986618042 and batch: 150, loss is 5.08236927986145 and perplexity is 161.15542626969565
At time: 961.0471062660217 and batch: 200, loss is 4.994086265563965 and perplexity is 147.5380731560883
At time: 962.0016946792603 and batch: 250, loss is 5.1359740829467775 and perplexity is 170.0298624249371
At time: 962.9635119438171 and batch: 300, loss is 5.103254070281983 and perplexity is 164.55651546857166
At time: 963.9391443729401 and batch: 350, loss is 5.101603727340699 and perplexity is 164.2851647572796
At time: 964.9487206935883 and batch: 400, loss is 5.003805360794067 and perplexity is 148.9790006523555
At time: 965.926864862442 and batch: 450, loss is 5.04847264289856 and perplexity is 155.78434436080082
At time: 966.9155585765839 and batch: 500, loss is 4.994963607788086 and perplexity is 147.6675713361377
At time: 967.9039626121521 and batch: 550, loss is 5.084766244888305 and perplexity is 161.54217351499315
At time: 968.8755552768707 and batch: 600, loss is 5.036902112960815 and perplexity is 153.9922248205506
At time: 969.8580069541931 and batch: 650, loss is 4.938121223449707 and perplexity is 139.5078990188938
At time: 970.841579914093 and batch: 700, loss is 5.05401683807373 and perplexity is 156.65044185944487
At time: 971.822675704956 and batch: 750, loss is 5.054385166168213 and perplexity is 156.70815124553656
At time: 972.8154096603394 and batch: 800, loss is 5.038141288757324 and perplexity is 154.1831665391434
At time: 973.7956254482269 and batch: 850, loss is 5.095880489349366 and perplexity is 163.3476071527546
At time: 974.7674944400787 and batch: 900, loss is 5.027290878295898 and perplexity is 152.51925925891152
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.159994883080051 and perplexity of 174.16356442181356
finished 51 epochs...
Completing Train Step...
At time: 977.1530387401581 and batch: 50, loss is 5.092563705444336 and perplexity is 162.80671594391757
At time: 978.1200683116913 and batch: 100, loss is 5.024717388153076 and perplexity is 152.127257071848
At time: 979.0799324512482 and batch: 150, loss is 5.0738567352294925 and perplexity is 159.78940591437922
At time: 980.0438210964203 and batch: 200, loss is 4.985644645690918 and perplexity is 146.29785491609204
At time: 981.0193498134613 and batch: 250, loss is 5.127861862182617 and perplexity is 168.65612221995133
At time: 981.9861073493958 and batch: 300, loss is 5.095454339981079 and perplexity is 163.27801150327957
At time: 982.9457290172577 and batch: 350, loss is 5.0934564399719235 and perplexity is 162.95212401631002
At time: 983.9021422863007 and batch: 400, loss is 4.995815162658691 and perplexity is 147.7933719311922
At time: 984.8843245506287 and batch: 450, loss is 5.040445461273193 and perplexity is 154.53884076391674
At time: 985.8507509231567 and batch: 500, loss is 4.986724004745484 and perplexity is 146.45584808079695
At time: 986.8328535556793 and batch: 550, loss is 5.076588277816772 and perplexity is 160.22647414511442
At time: 987.8109776973724 and batch: 600, loss is 5.028954620361328 and perplexity is 152.77322317297046
At time: 988.793173789978 and batch: 650, loss is 4.929958095550537 and perplexity is 138.37371374417347
At time: 989.7821006774902 and batch: 700, loss is 5.0454575729370115 and perplexity is 155.31535104275946
At time: 990.7600638866425 and batch: 750, loss is 5.046361799240112 and perplexity is 155.45585478246295
At time: 991.7472476959229 and batch: 800, loss is 5.029738836288452 and perplexity is 152.89307735745845
At time: 992.7226707935333 and batch: 850, loss is 5.087588596343994 and perplexity is 161.9987463049319
At time: 993.6952133178711 and batch: 900, loss is 5.019573116302491 and perplexity is 151.34668257069913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.153391798881636 and perplexity of 173.01733622697566
finished 52 epochs...
Completing Train Step...
At time: 996.0220875740051 and batch: 50, loss is 5.085189733505249 and perplexity is 161.6105992743756
At time: 996.9845988750458 and batch: 100, loss is 5.016840715408325 and perplexity is 150.93370722395724
At time: 997.9488768577576 and batch: 150, loss is 5.065512065887451 and perplexity is 158.4615640634362
At time: 998.917337179184 and batch: 200, loss is 4.9773751068115235 and perplexity is 145.09302766581484
At time: 999.8778910636902 and batch: 250, loss is 5.119917402267456 and perplexity is 167.3215486697964
At time: 1000.8527781963348 and batch: 300, loss is 5.087804756164551 and perplexity is 162.03376770983755
At time: 1001.831392288208 and batch: 350, loss is 5.08547420501709 and perplexity is 161.65657942559218
At time: 1002.8186333179474 and batch: 400, loss is 4.987981119155884 and perplexity is 146.64007561118652
At time: 1003.816915512085 and batch: 450, loss is 5.032585372924805 and perplexity is 153.328913120545
At time: 1004.7979328632355 and batch: 500, loss is 4.9786584854125975 and perplexity is 145.27935649232595
At time: 1005.7762265205383 and batch: 550, loss is 5.068579368591308 and perplexity is 158.94835984068547
At time: 1006.7415931224823 and batch: 600, loss is 5.021173048019409 and perplexity is 151.5890207389608
At time: 1007.7007677555084 and batch: 650, loss is 4.921966648101806 and perplexity is 137.27231423223253
At time: 1008.6649191379547 and batch: 700, loss is 5.037071475982666 and perplexity is 154.01830761775602
At time: 1009.6374673843384 and batch: 750, loss is 5.0385018253326415 and perplexity is 154.23876523205792
At time: 1010.6169364452362 and batch: 800, loss is 5.02151647567749 and perplexity is 151.64108954176578
At time: 1011.5968947410583 and batch: 850, loss is 5.079464864730835 and perplexity is 160.688043077221
At time: 1012.5699067115784 and batch: 900, loss is 5.0120203495025635 and perplexity is 150.20790225578156
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.146973910397047 and perplexity of 171.91048587742256
finished 53 epochs...
Completing Train Step...
At time: 1014.8985042572021 and batch: 50, loss is 5.07796082496643 and perplexity is 160.44654352875585
At time: 1015.866729259491 and batch: 100, loss is 5.009120950698852 and perplexity is 149.77302039629808
At time: 1016.8300836086273 and batch: 150, loss is 5.057331314086914 and perplexity is 157.1705174042836
At time: 1017.8044588565826 and batch: 200, loss is 4.969271802902222 and perplexity is 143.92204558690838
At time: 1018.7648141384125 and batch: 250, loss is 5.112135028839111 and perplexity is 166.02444371961462
At time: 1019.7360076904297 and batch: 300, loss is 5.08029634475708 and perplexity is 160.8217075374598
At time: 1020.724841594696 and batch: 350, loss is 5.077650051116944 and perplexity is 160.39668868597857
At time: 1021.7059166431427 and batch: 400, loss is 4.980297079086304 and perplexity is 145.51760547012032
At time: 1022.6956927776337 and batch: 450, loss is 5.024880990982056 and perplexity is 152.15214755749136
At time: 1023.6759347915649 and batch: 500, loss is 4.970758543014527 and perplexity is 144.1361794063534
At time: 1024.6537606716156 and batch: 550, loss is 5.060731754302979 and perplexity is 157.705876064141
At time: 1025.635801076889 and batch: 600, loss is 5.013548917770386 and perplexity is 150.43768086014737
At time: 1026.6128368377686 and batch: 650, loss is 4.914139909744263 and perplexity is 136.20211329952443
At time: 1027.590086221695 and batch: 700, loss is 5.028850498199463 and perplexity is 152.75731692280957
At time: 1028.5640406608582 and batch: 750, loss is 5.030797119140625 and perplexity is 153.05496712691613
At time: 1029.5362424850464 and batch: 800, loss is 5.013465547561646 and perplexity is 150.42513936209258
At time: 1030.51074051857 and batch: 850, loss is 5.071502819061279 and perplexity is 159.41371739140405
At time: 1031.4822475910187 and batch: 900, loss is 5.004623193740844 and perplexity is 149.10089042340837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.140734528842038 and perplexity of 170.8412100415863
finished 54 epochs...
Completing Train Step...
At time: 1033.8567733764648 and batch: 50, loss is 5.070867204666138 and perplexity is 159.31242393304754
At time: 1034.8107697963715 and batch: 100, loss is 5.001548147201538 and perplexity is 148.64310246667387
At time: 1035.7659072875977 and batch: 150, loss is 5.049309930801392 and perplexity is 155.91483532941606
At time: 1036.7318513393402 and batch: 200, loss is 4.96132981300354 and perplexity is 142.7835451185254
At time: 1037.7224555015564 and batch: 250, loss is 5.10450909614563 and perplexity is 164.76316780140303
At time: 1038.6855924129486 and batch: 300, loss is 5.072920627593994 and perplexity is 159.63989582111896
At time: 1039.6521291732788 and batch: 350, loss is 5.069978084564209 and perplexity is 159.17083900677096
At time: 1040.623693704605 and batch: 400, loss is 4.972755641937256 and perplexity is 144.4243212430791
At time: 1041.606304883957 and batch: 450, loss is 5.017318534851074 and perplexity is 151.00584351652418
At time: 1042.5829010009766 and batch: 500, loss is 4.963016958236694 and perplexity is 143.0246450241331
At time: 1043.5718779563904 and batch: 550, loss is 5.053038873672485 and perplexity is 156.49731819081964
At time: 1044.5795645713806 and batch: 600, loss is 5.006074228286743 and perplexity is 149.31739800824147
At time: 1045.570992231369 and batch: 650, loss is 4.9064713478088375 and perplexity is 135.16163354859654
At time: 1046.57199716568 and batch: 700, loss is 5.020785312652588 and perplexity is 151.53025570777527
At time: 1047.556348323822 and batch: 750, loss is 5.023241672515869 and perplexity is 151.90292606484036
At time: 1048.5407354831696 and batch: 800, loss is 5.005577430725098 and perplexity is 149.2432359122993
At time: 1049.525863647461 and batch: 850, loss is 5.063696689605713 and perplexity is 158.17415765279006
At time: 1050.5099794864655 and batch: 900, loss is 4.99737359046936 and perplexity is 148.0238767981877
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.13466905567744 and perplexity of 169.80811354351113
finished 55 epochs...
Completing Train Step...
At time: 1052.8894531726837 and batch: 50, loss is 5.063907985687256 and perplexity is 158.20758276367653
At time: 1053.8580360412598 and batch: 100, loss is 4.99411561012268 and perplexity is 147.54240265926217
At time: 1054.8260102272034 and batch: 150, loss is 5.041443014144898 and perplexity is 154.6930783455917
At time: 1055.7843222618103 and batch: 200, loss is 4.953543033599853 and perplexity is 141.67603870032798
At time: 1056.7496552467346 and batch: 250, loss is 5.097032442092895 and perplexity is 163.53588429933072
At time: 1057.7214031219482 and batch: 300, loss is 5.06566967010498 and perplexity is 158.4865402423727
At time: 1058.6932787895203 and batch: 350, loss is 5.062452507019043 and perplexity is 157.9774824954512
At time: 1059.6698751449585 and batch: 400, loss is 4.96534797668457 and perplexity is 143.35842698500167
At time: 1060.654571056366 and batch: 450, loss is 5.009887943267822 and perplexity is 149.88793925529595
At time: 1061.6389050483704 and batch: 500, loss is 4.955428609848022 and perplexity is 141.94343168954663
At time: 1062.6265740394592 and batch: 550, loss is 5.045496063232422 and perplexity is 155.32132929155443
At time: 1063.596075296402 and batch: 600, loss is 4.998741588592529 and perplexity is 148.22651175435047
At time: 1064.5599405765533 and batch: 650, loss is 4.898955316543579 and perplexity is 134.1495626277045
At time: 1065.5226228237152 and batch: 700, loss is 5.01286639213562 and perplexity is 150.33503831859304
At time: 1066.4921386241913 and batch: 750, loss is 5.015833158493042 and perplexity is 150.78170950952722
At time: 1067.4580161571503 and batch: 800, loss is 4.99784333229065 and perplexity is 148.093426137507
At time: 1068.4203264713287 and batch: 850, loss is 5.0560398769378665 and perplexity is 156.96767256824324
At time: 1069.3827855587006 and batch: 900, loss is 4.9902650737762455 and perplexity is 146.97537765097323
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.128768293824915 and perplexity of 168.8090667722177
finished 56 epochs...
Completing Train Step...
At time: 1071.7268924713135 and batch: 50, loss is 5.057079477310181 and perplexity is 157.13094107138016
At time: 1072.6972036361694 and batch: 100, loss is 4.986816320419312 and perplexity is 146.4693688751796
At time: 1073.6654047966003 and batch: 150, loss is 5.033726243972779 and perplexity is 153.50394146174048
At time: 1074.6432874202728 and batch: 200, loss is 4.94590503692627 and perplexity is 140.59803970515426
At time: 1075.6180114746094 and batch: 250, loss is 5.08969630241394 and perplexity is 162.34055213241393
At time: 1076.5876762866974 and batch: 300, loss is 5.058536748886109 and perplexity is 157.36009045138883
At time: 1077.5731091499329 and batch: 350, loss is 5.0550689697265625 and perplexity is 156.8153454827041
At time: 1078.5523054599762 and batch: 400, loss is 4.958066835403442 and perplexity is 142.3184048929061
At time: 1079.524709224701 and batch: 450, loss is 5.0025841331481935 and perplexity is 148.7971744263091
At time: 1080.499237537384 and batch: 500, loss is 4.947995548248291 and perplexity is 140.89226893662737
At time: 1081.472862958908 and batch: 550, loss is 5.038097314834594 and perplexity is 154.17638664956203
At time: 1082.4460983276367 and batch: 600, loss is 4.991544284820557 and perplexity is 147.16351048244934
At time: 1083.4345755577087 and batch: 650, loss is 4.891586332321167 and perplexity is 133.16464997703315
At time: 1084.409653186798 and batch: 700, loss is 5.005085144042969 and perplexity is 149.16978353615852
At time: 1085.3838047981262 and batch: 750, loss is 5.008567533493042 and perplexity is 149.69015636115088
At time: 1086.388888835907 and batch: 800, loss is 4.990254688262939 and perplexity is 146.97385124415922
At time: 1087.3577861785889 and batch: 850, loss is 5.048526887893677 and perplexity is 155.79279511100336
At time: 1088.3217492103577 and batch: 900, loss is 4.983291149139404 and perplexity is 145.95394826934674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.123026808647261 and perplexity of 167.84262906622885
finished 57 epochs...
Completing Train Step...
At time: 1090.647449016571 and batch: 50, loss is 5.050375375747681 and perplexity is 156.0810425293571
At time: 1091.6079993247986 and batch: 100, loss is 4.97964391708374 and perplexity is 145.4225899331703
At time: 1092.568424463272 and batch: 150, loss is 5.026153411865234 and perplexity is 152.3458723510628
At time: 1093.5303270816803 and batch: 200, loss is 4.938410263061524 and perplexity is 139.54822815594986
At time: 1094.4930205345154 and batch: 250, loss is 5.08249264717102 and perplexity is 161.1753088074595
At time: 1095.4596149921417 and batch: 300, loss is 5.05151665687561 and perplexity is 156.25927656584832
At time: 1096.430314540863 and batch: 350, loss is 5.047822179794312 and perplexity is 155.68304534178355
At time: 1097.4026334285736 and batch: 400, loss is 4.95090805053711 and perplexity is 141.30321614422948
At time: 1098.3824167251587 and batch: 450, loss is 4.99540337562561 and perplexity is 147.73252506589048
At time: 1099.362777709961 and batch: 500, loss is 4.940734767913819 and perplexity is 139.8729859937815
At time: 1100.3322298526764 and batch: 550, loss is 5.03082968711853 and perplexity is 153.05995189887525
At time: 1101.3005633354187 and batch: 600, loss is 4.9844750308990475 and perplexity is 146.12684280960326
At time: 1102.268183708191 and batch: 650, loss is 4.884357967376709 and perplexity is 132.2055577994201
At time: 1103.2534036636353 and batch: 700, loss is 4.997434816360474 and perplexity is 148.03293996939829
At time: 1104.2266497612 and batch: 750, loss is 5.0014400291442875 and perplexity is 148.62703233196305
At time: 1105.2114622592926 and batch: 800, loss is 4.98280499458313 and perplexity is 145.88300933742906
At time: 1106.1988008022308 and batch: 850, loss is 5.041152591705322 and perplexity is 154.64815852757042
At time: 1107.1849868297577 and batch: 900, loss is 4.976444149017334 and perplexity is 144.9580150361079
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.1174383294092465 and perplexity of 166.9072600978506
finished 58 epochs...
Completing Train Step...
At time: 1109.5296132564545 and batch: 50, loss is 5.043790502548218 and perplexity is 155.0566451205168
At time: 1110.4939618110657 and batch: 100, loss is 4.972592887878418 and perplexity is 144.40081751131763
At time: 1111.468513727188 and batch: 150, loss is 5.018718004226685 and perplexity is 151.21731951263902
At time: 1112.4305846691132 and batch: 200, loss is 4.93105411529541 and perplexity is 138.52545720833461
At time: 1113.4047825336456 and batch: 250, loss is 5.07541443824768 and perplexity is 160.03850431454285
At time: 1114.3745703697205 and batch: 300, loss is 5.044605712890625 and perplexity is 155.1831004381163
At time: 1115.3499336242676 and batch: 350, loss is 5.0407070064544675 and perplexity is 154.57926493918197
At time: 1116.318644285202 and batch: 400, loss is 4.94387071609497 and perplexity is 140.3123089167302
At time: 1117.3177795410156 and batch: 450, loss is 4.9883413505554195 and perplexity is 146.69290948649106
At time: 1118.2960016727448 and batch: 500, loss is 4.933625068664551 and perplexity is 138.8820579047154
At time: 1119.2881789207458 and batch: 550, loss is 5.023679590225219 and perplexity is 151.96946161375052
At time: 1120.287320137024 and batch: 600, loss is 4.977532625198364 and perplexity is 145.11588428559628
At time: 1121.2792851924896 and batch: 650, loss is 4.877258577346802 and perplexity is 131.27030277873732
At time: 1122.2487454414368 and batch: 700, loss is 4.989910306930542 and perplexity is 146.92324490787917
At time: 1123.2204854488373 and batch: 750, loss is 4.994444341659546 and perplexity is 147.59091247295711
At time: 1124.1993639469147 and batch: 800, loss is 4.975487575531006 and perplexity is 144.8194183418235
At time: 1125.1896324157715 and batch: 850, loss is 5.033915605545044 and perplexity is 153.53301196177551
At time: 1126.173674106598 and batch: 900, loss is 4.9697158241271975 and perplexity is 143.98596421945547
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.111990732689426 and perplexity of 166.00048875997808
finished 59 epochs...
Completing Train Step...
At time: 1128.5268115997314 and batch: 50, loss is 5.037320947647094 and perplexity is 154.05673561445874
At time: 1129.4810583591461 and batch: 100, loss is 4.9656564807891845 and perplexity is 143.40266047091276
At time: 1130.4245252609253 and batch: 150, loss is 5.011413488388062 and perplexity is 150.11677457453146
At time: 1131.3798384666443 and batch: 200, loss is 4.923831510543823 and perplexity is 137.5285470606033
At time: 1132.3602132797241 and batch: 250, loss is 5.068450860977173 and perplexity is 158.92793507858838
At time: 1133.3348002433777 and batch: 300, loss is 5.03780125617981 and perplexity is 154.13074815210047
At time: 1134.301203250885 and batch: 350, loss is 5.033718185424805 and perplexity is 153.50270444784832
At time: 1135.2809753417969 and batch: 400, loss is 4.936952257156372 and perplexity is 139.34491426768662
At time: 1136.2631251811981 and batch: 450, loss is 4.981394062042236 and perplexity is 145.67732339101093
At time: 1137.2436368465424 and batch: 500, loss is 4.926604747772217 and perplexity is 137.91047569184857
At time: 1138.2362048625946 and batch: 550, loss is 5.01667332649231 and perplexity is 150.90844470870638
At time: 1139.2180244922638 and batch: 600, loss is 4.970711727142334 and perplexity is 144.12943170335092
At time: 1140.2009179592133 and batch: 650, loss is 4.8702811431884765 and perplexity is 130.35756088542286
At time: 1141.1917505264282 and batch: 700, loss is 4.982507266998291 and perplexity is 145.8395824064084
At time: 1142.1761486530304 and batch: 750, loss is 4.987573261260986 and perplexity is 146.58027949362838
At time: 1143.1532521247864 and batch: 800, loss is 4.968299036026001 and perplexity is 143.78211106107395
At time: 1144.1329474449158 and batch: 850, loss is 5.0268111991882325 and perplexity is 152.44611650065735
At time: 1145.128202676773 and batch: 900, loss is 4.963099927902221 and perplexity is 143.03651222339488
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.106676911654538 and perplexity of 165.12073137776423
finished 60 epochs...
Completing Train Step...
At time: 1147.5470395088196 and batch: 50, loss is 5.030962133407593 and perplexity is 153.08022546405806
At time: 1148.5104978084564 and batch: 100, loss is 4.958829946517945 and perplexity is 142.42705109876937
At time: 1149.4729537963867 and batch: 150, loss is 5.004232883453369 and perplexity is 149.04270616769986
At time: 1150.432095527649 and batch: 200, loss is 4.91673565864563 and perplexity is 136.5561190417221
At time: 1151.3845036029816 and batch: 250, loss is 5.06159049987793 and perplexity is 157.84136345362677
At time: 1152.3646292686462 and batch: 300, loss is 5.03109956741333 and perplexity is 153.10126533840665
At time: 1153.354701757431 and batch: 350, loss is 5.0268477439880375 and perplexity is 152.45168771526474
At time: 1154.3472900390625 and batch: 400, loss is 4.930150156021118 and perplexity is 138.40029241702567
At time: 1155.3237280845642 and batch: 450, loss is 4.974557209014892 and perplexity is 144.68474586121266
At time: 1156.2930324077606 and batch: 500, loss is 4.919635410308838 and perplexity is 136.95267255051266
At time: 1157.2697069644928 and batch: 550, loss is 5.009806299209595 and perplexity is 149.8757022952002
At time: 1158.244835615158 and batch: 600, loss is 4.9640095424652095 and perplexity is 143.16667951000704
At time: 1159.212065935135 and batch: 650, loss is 4.863415412902832 and perplexity is 129.4656264260874
At time: 1160.1977679729462 and batch: 700, loss is 4.97522050857544 and perplexity is 144.78074702480535
At time: 1161.187702178955 and batch: 750, loss is 4.980819759368896 and perplexity is 145.59368453405088
At time: 1162.1620972156525 and batch: 800, loss is 4.961236200332642 and perplexity is 142.7701793951168
At time: 1163.1561534404755 and batch: 850, loss is 5.019833126068115 and perplexity is 151.386039302508
At time: 1164.1427297592163 and batch: 900, loss is 4.956591157913208 and perplexity is 142.10854370815608
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.101485160932149 and perplexity of 164.26568721003053
finished 61 epochs...
Completing Train Step...
At time: 1166.4862761497498 and batch: 50, loss is 5.024708871841431 and perplexity is 152.12596151423372
At time: 1167.4386339187622 and batch: 100, loss is 4.952107219696045 and perplexity is 141.47276424119482
At time: 1168.41730260849 and batch: 150, loss is 4.997169532775879 and perplexity is 147.99367446892174
At time: 1169.3936517238617 and batch: 200, loss is 4.909760236740112 and perplexity is 135.60689695869803
At time: 1170.3774273395538 and batch: 250, loss is 5.054831581115723 and perplexity is 156.77812372386722
At time: 1171.3643989562988 and batch: 300, loss is 5.024497556686401 and perplexity is 152.09381838937668
At time: 1172.35285282135 and batch: 350, loss is 5.020090379714966 and perplexity is 151.424988922952
At time: 1173.3258764743805 and batch: 400, loss is 4.923458108901977 and perplexity is 137.47720326185615
At time: 1174.3273229599 and batch: 450, loss is 4.967827167510986 and perplexity is 143.71428081457933
At time: 1175.2996110916138 and batch: 500, loss is 4.912752475738525 and perplexity is 136.0132728879764
At time: 1176.2671964168549 and batch: 550, loss is 5.003051548004151 and perplexity is 148.86674069304712
At time: 1177.2500214576721 and batch: 600, loss is 4.957419080734253 and perplexity is 142.2262473325925
At time: 1178.2401325702667 and batch: 650, loss is 4.856663942337036 and perplexity is 128.59448710477858
At time: 1179.2337436676025 and batch: 700, loss is 4.968044586181641 and perplexity is 143.74553037946447
At time: 1180.207160949707 and batch: 750, loss is 4.974179830551147 and perplexity is 144.63015525539956
At time: 1181.1741151809692 and batch: 800, loss is 4.954293174743652 and perplexity is 141.78235559740145
At time: 1182.1545691490173 and batch: 850, loss is 5.012972278594971 and perplexity is 150.3509576063206
At time: 1183.1399953365326 and batch: 900, loss is 4.950185413360596 and perplexity is 141.20114207278402
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.096416316620291 and perplexity of 163.43515670893268
finished 62 epochs...
Completing Train Step...
At time: 1185.5180399417877 and batch: 50, loss is 5.01855544090271 and perplexity is 151.19273912052577
At time: 1186.488891839981 and batch: 100, loss is 4.945481958389283 and perplexity is 140.53856827364274
At time: 1187.457819700241 and batch: 150, loss is 4.990222082138062 and perplexity is 146.9690590745393
At time: 1188.4145846366882 and batch: 200, loss is 4.902899494171143 and perplexity is 134.67971715590227
At time: 1189.3723537921906 and batch: 250, loss is 5.048180503845215 and perplexity is 155.7388403169911
At time: 1190.342538356781 and batch: 300, loss is 5.017991504669189 and perplexity is 151.10750009363738
At time: 1191.3155360221863 and batch: 350, loss is 5.013443603515625 and perplexity is 150.42183846212936
At time: 1192.2890057563782 and batch: 400, loss is 4.9168715667724605 and perplexity is 136.57467938929082
At time: 1193.252852678299 and batch: 450, loss is 4.961201372146607 and perplexity is 142.7652070553379
At time: 1194.233163356781 and batch: 500, loss is 4.90597710609436 and perplexity is 135.09484753667593
At time: 1195.2132794857025 and batch: 550, loss is 4.996402940750122 and perplexity is 147.88026717232978
At time: 1196.1912455558777 and batch: 600, loss is 4.950934944152832 and perplexity is 141.30701634972513
At time: 1197.1668827533722 and batch: 650, loss is 4.8500230026245115 and perplexity is 127.74332824404674
At time: 1198.1423094272614 and batch: 700, loss is 4.960976905822754 and perplexity is 142.73316467048883
At time: 1199.11905002594 and batch: 750, loss is 4.967651243209839 and perplexity is 143.6890002039642
At time: 1200.0881972312927 and batch: 800, loss is 4.947462844848633 and perplexity is 140.81723513313477
At time: 1201.067096233368 and batch: 850, loss is 5.006224098205567 and perplexity is 149.33977787155217
At time: 1202.032919883728 and batch: 900, loss is 4.943878116607666 and perplexity is 140.31334730359603
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.09146619822881 and perplexity of 162.6281324147146
finished 63 epochs...
Completing Train Step...
At time: 1204.3361132144928 and batch: 50, loss is 5.0124979496002195 and perplexity is 150.27965869864573
At time: 1205.2784934043884 and batch: 100, loss is 4.9389485740661625 and perplexity is 139.62336872552726
At time: 1206.2327177524567 and batch: 150, loss is 4.9833893394470214 and perplexity is 145.96828023604382
At time: 1207.19144821167 and batch: 200, loss is 4.896147413253784 and perplexity is 133.77341197394773
At time: 1208.1654160022736 and batch: 250, loss is 5.0416366004943844 and perplexity is 154.72302771272024
At time: 1209.1294469833374 and batch: 300, loss is 5.011579418182373 and perplexity is 150.14168548673348
At time: 1210.1074707508087 and batch: 350, loss is 5.006904220581054 and perplexity is 149.44138174364986
At time: 1211.081092596054 and batch: 400, loss is 4.910385580062866 and perplexity is 135.69172434658645
At time: 1212.0511949062347 and batch: 450, loss is 4.954677991867065 and perplexity is 141.83692637484452
At time: 1213.0271871089935 and batch: 500, loss is 4.899309492111206 and perplexity is 134.19708354007136
At time: 1213.9988596439362 and batch: 550, loss is 4.9898575401306156 and perplexity is 146.9154924429487
At time: 1214.965262413025 and batch: 600, loss is 4.944554691314697 and perplexity is 140.40831188713557
At time: 1215.9280443191528 and batch: 650, loss is 4.843484096527099 and perplexity is 126.9107516527208
At time: 1216.896193265915 and batch: 700, loss is 4.954013566970826 and perplexity is 141.74271769051842
At time: 1217.8589117527008 and batch: 750, loss is 4.96123194694519 and perplexity is 142.7695721395188
At time: 1218.8330883979797 and batch: 800, loss is 4.940739469528198 and perplexity is 139.87364362416972
At time: 1219.8165261745453 and batch: 850, loss is 4.999586200714111 and perplexity is 148.3517585479524
At time: 1220.788284778595 and batch: 900, loss is 4.937665042877197 and perplexity is 139.44427273928935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.086638568198844 and perplexity of 161.8449160195836
finished 64 epochs...
Completing Train Step...
At time: 1223.128558397293 and batch: 50, loss is 5.006532669067383 and perplexity is 149.38586688601117
At time: 1224.090854883194 and batch: 100, loss is 4.9325020885467525 and perplexity is 138.72618365319306
At time: 1225.0570464134216 and batch: 150, loss is 4.976669073104858 and perplexity is 144.99062325241837
At time: 1226.028344154358 and batch: 200, loss is 4.889496335983276 and perplexity is 132.88662698096874
At time: 1227.0191156864166 and batch: 250, loss is 5.035196657180786 and perplexity is 153.72982171270104
At time: 1227.986060142517 and batch: 300, loss is 5.005258026123047 and perplexity is 149.19557454795873
At time: 1228.9566235542297 and batch: 350, loss is 5.000468702316284 and perplexity is 148.48273699840172
At time: 1229.9230785369873 and batch: 400, loss is 4.903994626998902 and perplexity is 134.8272901266465
At time: 1230.89097905159 and batch: 450, loss is 4.948255300521851 and perplexity is 140.92887077731052
At time: 1231.8602230548859 and batch: 500, loss is 4.892743520736694 and perplexity is 133.3188357611402
At time: 1232.8450181484222 and batch: 550, loss is 4.983410873413086 and perplexity is 145.97142354588078
At time: 1233.819952249527 and batch: 600, loss is 4.938274993896484 and perplexity is 139.52935286029592
At time: 1234.78910779953 and batch: 650, loss is 4.837042064666748 and perplexity is 126.09581628525437
At time: 1235.7462747097015 and batch: 700, loss is 4.947151575088501 and perplexity is 140.77340980723568
At time: 1236.7047622203827 and batch: 750, loss is 4.954917154312134 and perplexity is 141.8708524977229
At time: 1237.6663446426392 and batch: 800, loss is 4.934118461608887 and perplexity is 138.95059823944882
At time: 1238.6249108314514 and batch: 850, loss is 4.993056221008301 and perplexity is 147.38618060855623
At time: 1239.5931928157806 and batch: 900, loss is 4.931543807983399 and perplexity is 138.59330872366942
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.081923393354024 and perplexity of 161.08358525718273
finished 65 epochs...
Completing Train Step...
At time: 1241.9394919872284 and batch: 50, loss is 5.000656604766846 and perplexity is 148.5106398899786
At time: 1242.920913696289 and batch: 100, loss is 4.92613826751709 and perplexity is 137.84615818055406
At time: 1243.9005224704742 and batch: 150, loss is 4.970061511993408 and perplexity is 144.0357470243508
At time: 1244.8863348960876 and batch: 200, loss is 4.882939119338989 and perplexity is 132.01811121371702
At time: 1245.851627588272 and batch: 250, loss is 5.028860368728638 and perplexity is 152.75882472580426
At time: 1246.8090381622314 and batch: 300, loss is 4.999026222229004 and perplexity is 148.26870801036506
At time: 1247.780814409256 and batch: 350, loss is 4.994132614135742 and perplexity is 147.5449114935342
At time: 1248.7646594047546 and batch: 400, loss is 4.897694416046143 and perplexity is 133.98051997283653
At time: 1249.7423558235168 and batch: 450, loss is 4.9419308853149415 and perplexity is 140.0403906041006
At time: 1250.7355072498322 and batch: 500, loss is 4.8862708473205565 and perplexity is 132.45869319101908
At time: 1251.7134547233582 and batch: 550, loss is 4.977058963775635 and perplexity is 145.04716476556254
At time: 1252.6855657100677 and batch: 600, loss is 4.932093420028687 and perplexity is 138.6695022120517
At time: 1253.6603801250458 and batch: 650, loss is 4.830696525573731 and perplexity is 125.29820367094146
At time: 1254.6349084377289 and batch: 700, loss is 4.940386219024658 and perplexity is 139.8242419152305
At time: 1255.6067733764648 and batch: 750, loss is 4.948700542449951 and perplexity is 140.99163219042995
At time: 1256.6081171035767 and batch: 800, loss is 4.927598524093628 and perplexity is 138.04759597922816
At time: 1257.5861868858337 and batch: 850, loss is 4.986631450653076 and perplexity is 146.4422936199701
At time: 1258.5597195625305 and batch: 900, loss is 4.925512371063232 and perplexity is 137.75990775370474
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.077316493204195 and perplexity of 160.34319602323518
finished 66 epochs...
Completing Train Step...
At time: 1260.922500371933 and batch: 50, loss is 4.994865989685058 and perplexity is 147.65315701150612
At time: 1261.9123239517212 and batch: 100, loss is 4.919854488372803 and perplexity is 136.98267916364514
At time: 1262.8830299377441 and batch: 150, loss is 4.963570470809937 and perplexity is 143.1038328771519
At time: 1263.8643472194672 and batch: 200, loss is 4.876470861434936 and perplexity is 131.16693978816397
At time: 1264.8310642242432 and batch: 250, loss is 5.022624845504761 and perplexity is 151.809257128712
At time: 1265.81658411026 and batch: 300, loss is 4.99288387298584 and perplexity is 147.3607810806324
At time: 1266.7934663295746 and batch: 350, loss is 4.987892112731934 and perplexity is 146.62702428328322
At time: 1267.765165090561 and batch: 400, loss is 4.891488008499145 and perplexity is 133.15155736335765
At time: 1268.7317752838135 and batch: 450, loss is 4.935700082778931 and perplexity is 139.17053933311055
At time: 1269.696495294571 and batch: 500, loss is 4.879883089065552 and perplexity is 131.61527572145314
At time: 1270.664029598236 and batch: 550, loss is 4.970798425674438 and perplexity is 144.14192805521253
At time: 1271.6275701522827 and batch: 600, loss is 4.926007843017578 and perplexity is 137.82818083673217
At time: 1272.602564573288 and batch: 650, loss is 4.824449100494385 and perplexity is 124.51785266127567
At time: 1273.5876660346985 and batch: 700, loss is 4.933713912963867 and perplexity is 138.89439733197335
At time: 1274.5593774318695 and batch: 750, loss is 4.942575435638428 and perplexity is 140.13068277896497
At time: 1275.532058954239 and batch: 800, loss is 4.921178827285766 and perplexity is 137.1642108342593
At time: 1276.51775431633 and batch: 850, loss is 4.980307655334473 and perplexity is 145.5191445085673
At time: 1277.5010242462158 and batch: 900, loss is 4.919569931030273 and perplexity is 136.9437052819049
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.07281494140625 and perplexity of 159.62302498064082
finished 67 epochs...
Completing Train Step...
At time: 1279.852709054947 and batch: 50, loss is 4.98915693283081 and perplexity is 146.81259842484758
At time: 1280.82475399971 and batch: 100, loss is 4.913650064468384 and perplexity is 136.13541167583134
At time: 1281.81737947464 and batch: 150, loss is 4.9571811485290525 and perplexity is 142.19241115344525
At time: 1282.7808012962341 and batch: 200, loss is 4.870093240737915 and perplexity is 130.33306868142787
At time: 1283.7390995025635 and batch: 250, loss is 5.01648591041565 and perplexity is 150.8801646902121
At time: 1284.7126786708832 and batch: 300, loss is 4.986830520629883 and perplexity is 146.4714487858274
At time: 1285.6756672859192 and batch: 350, loss is 4.981745052337646 and perplexity is 145.72846369213218
At time: 1286.651960849762 and batch: 400, loss is 4.885381145477295 and perplexity is 132.34089685710904
At time: 1287.6135630607605 and batch: 450, loss is 4.929559421539307 and perplexity is 138.31855873583095
At time: 1288.583901643753 and batch: 500, loss is 4.873576955795288 and perplexity is 130.78790375316103
At time: 1289.5602638721466 and batch: 550, loss is 4.964625110626221 and perplexity is 143.25483548985463
At time: 1290.5436794757843 and batch: 600, loss is 4.920015640258789 and perplexity is 137.00475595955075
At time: 1291.5252931118011 and batch: 650, loss is 4.818299703598022 and perplexity is 123.754492472509
At time: 1292.4972026348114 and batch: 700, loss is 4.927130632400512 and perplexity is 137.9830197643303
At time: 1293.4752662181854 and batch: 750, loss is 4.936536750793457 and perplexity is 139.2870275961415
At time: 1294.4477212429047 and batch: 800, loss is 4.914855937957764 and perplexity is 136.29967277888707
At time: 1295.4245071411133 and batch: 850, loss is 4.974079551696778 and perplexity is 144.61565263628842
At time: 1296.397699356079 and batch: 900, loss is 4.91371642112732 and perplexity is 136.14444546663574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.068412885273973 and perplexity of 158.9218997930935
finished 68 epochs...
Completing Train Step...
At time: 1298.7103230953217 and batch: 50, loss is 4.983527812957764 and perplexity is 145.9884943757943
At time: 1299.6739625930786 and batch: 100, loss is 4.9075243854522705 and perplexity is 135.30403880255835
At time: 1300.629504442215 and batch: 150, loss is 4.95084942817688 and perplexity is 141.29493285898621
At time: 1301.5908513069153 and batch: 200, loss is 4.86381420135498 and perplexity is 129.5172661188285
At time: 1302.5641100406647 and batch: 250, loss is 5.0104374599456785 and perplexity is 149.97032781259617
At time: 1303.5262994766235 and batch: 300, loss is 4.980865240097046 and perplexity is 145.6003063914198
At time: 1304.4882698059082 and batch: 350, loss is 4.975691385269165 and perplexity is 144.8489369575446
At time: 1305.4916651248932 and batch: 400, loss is 4.879372501373291 and perplexity is 131.54809173467427
At time: 1306.4745757579803 and batch: 450, loss is 4.92350417137146 and perplexity is 137.48353594718435
At time: 1307.4459552764893 and batch: 500, loss is 4.867355289459229 and perplexity is 129.97671115676252
At time: 1308.4092082977295 and batch: 550, loss is 4.958536930084229 and perplexity is 142.38532374589036
At time: 1309.378114938736 and batch: 600, loss is 4.914114236831665 and perplexity is 136.198616639459
At time: 1310.341968536377 and batch: 650, loss is 4.812243003845214 and perplexity is 123.00721397369622
At time: 1311.3258967399597 and batch: 700, loss is 4.9206336307525635 and perplexity is 137.08944976362307
At time: 1312.2944185733795 and batch: 750, loss is 4.930580053329468 and perplexity is 138.4598031210398
At time: 1313.258870601654 and batch: 800, loss is 4.908626298904419 and perplexity is 135.45321431720387
At time: 1314.2256121635437 and batch: 850, loss is 4.967941675186157 and perplexity is 143.73073814499165
At time: 1315.1964128017426 and batch: 900, loss is 4.907950296401977 and perplexity is 135.36167854805194
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.064108652611301 and perplexity of 158.23933297817274
finished 69 epochs...
Completing Train Step...
At time: 1317.5390293598175 and batch: 50, loss is 4.977978191375732 and perplexity is 145.18055742244402
At time: 1318.4961426258087 and batch: 100, loss is 4.9014718627929685 and perplexity is 134.48758134787425
At time: 1319.4616696834564 and batch: 150, loss is 4.9446683025360105 and perplexity is 140.42426475312675
At time: 1320.4304032325745 and batch: 200, loss is 4.857627229690552 and perplexity is 128.71842022996117
At time: 1321.3980638980865 and batch: 250, loss is 5.0044626617431645 and perplexity is 149.076956880714
At time: 1322.3646926879883 and batch: 300, loss is 4.974982833862304 and perplexity is 144.74634039124405
At time: 1323.321124792099 and batch: 350, loss is 4.969725217819214 and perplexity is 143.9873167856108
At time: 1324.2772722244263 and batch: 400, loss is 4.8734665203094485 and perplexity is 130.77346092498314
At time: 1325.2372686862946 and batch: 450, loss is 4.917528018951416 and perplexity is 136.66436356863295
At time: 1326.201361656189 and batch: 500, loss is 4.861219711303711 and perplexity is 129.18167039854617
At time: 1327.1659624576569 and batch: 550, loss is 4.9525343036651615 and perplexity is 141.53319789507262
At time: 1328.1309659481049 and batch: 600, loss is 4.90830587387085 and perplexity is 135.40981866935644
At time: 1329.0903162956238 and batch: 650, loss is 4.806276178359985 and perplexity is 122.27543675907886
At time: 1330.0633883476257 and batch: 700, loss is 4.914221229553223 and perplexity is 136.2131896797164
At time: 1331.0215899944305 and batch: 750, loss is 4.924699840545654 and perplexity is 137.64801908718997
At time: 1331.9835007190704 and batch: 800, loss is 4.902489814758301 and perplexity is 134.62455294904763
At time: 1332.9449484348297 and batch: 850, loss is 4.961886177062988 and perplexity is 142.86300685408708
At time: 1333.9039244651794 and batch: 900, loss is 4.902263097763061 and perplexity is 134.5940347345472
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.059898899026113 and perplexity of 157.5745845740625
finished 70 epochs...
Completing Train Step...
At time: 1336.1803932189941 and batch: 50, loss is 4.972505521774292 and perplexity is 144.38820232553698
At time: 1337.131323337555 and batch: 100, loss is 4.8954925441741945 and perplexity is 133.685836581113
At time: 1338.0778603553772 and batch: 150, loss is 4.938575496673584 and perplexity is 139.57128811883268
At time: 1339.0230457782745 and batch: 200, loss is 4.8515301609039305 and perplexity is 127.93600281791781
At time: 1339.9690618515015 and batch: 250, loss is 4.99856279373169 and perplexity is 148.20001198489052
At time: 1340.9170367717743 and batch: 300, loss is 4.969179887771606 and perplexity is 143.908817581226
At time: 1341.8627352714539 and batch: 350, loss is 4.963842973709107 and perplexity is 143.14283440026597
At time: 1342.8095712661743 and batch: 400, loss is 4.867659587860107 and perplexity is 130.0162688804897
At time: 1343.7548382282257 and batch: 450, loss is 4.911616554260254 and perplexity is 135.85886020691692
At time: 1344.7191452980042 and batch: 500, loss is 4.855171604156494 and perplexity is 128.4027237653307
At time: 1345.6798069477081 and batch: 550, loss is 4.946628799438477 and perplexity is 140.6998361293568
At time: 1346.6366174221039 and batch: 600, loss is 4.9025845623016355 and perplexity is 134.63730889899998
At time: 1347.603369474411 and batch: 650, loss is 4.800397787094116 and perplexity is 121.55876240947384
At time: 1348.567402601242 and batch: 700, loss is 4.907893657684326 and perplexity is 135.35401205327216
At time: 1349.5314545631409 and batch: 750, loss is 4.918891468048096 and perplexity is 136.85082555851875
At time: 1350.4896824359894 and batch: 800, loss is 4.896444015502929 and perplexity is 133.81309535361396
At time: 1351.4552929401398 and batch: 850, loss is 4.955911388397217 and perplexity is 142.0119754779695
At time: 1352.429547071457 and batch: 900, loss is 4.896648797988892 and perplexity is 133.84050073791036
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.055781116224315 and perplexity of 156.92706075588237
finished 71 epochs...
Completing Train Step...
At time: 1354.757756948471 and batch: 50, loss is 4.967106809616089 and perplexity is 143.61079237661687
At time: 1355.7372782230377 and batch: 100, loss is 4.889595060348511 and perplexity is 132.89974677647672
At time: 1356.7075803279877 and batch: 150, loss is 4.932559366226196 and perplexity is 138.7341297946369
At time: 1357.6711583137512 and batch: 200, loss is 4.845524063110352 and perplexity is 127.16990959204277
At time: 1358.6310601234436 and batch: 250, loss is 4.992736101150513 and perplexity is 147.33900691640056
At time: 1359.5977289676666 and batch: 300, loss is 4.9634534358978275 and perplexity is 143.0870857126692
At time: 1360.5607719421387 and batch: 350, loss is 4.9580440425872805 and perplexity is 142.31516109263478
At time: 1361.5197525024414 and batch: 400, loss is 4.861944751739502 and perplexity is 129.27536629570065
At time: 1362.477816104889 and batch: 450, loss is 4.905788803100586 and perplexity is 135.06941116738878
At time: 1363.4449746608734 and batch: 500, loss is 4.849234218597412 and perplexity is 127.64260607640013
At time: 1364.4041171073914 and batch: 550, loss is 4.940829753875732 and perplexity is 139.88627259491216
At time: 1365.369503736496 and batch: 600, loss is 4.89693175315857 and perplexity is 133.87837695788986
At time: 1366.3283035755157 and batch: 650, loss is 4.7946080493927 and perplexity is 120.85700252296066
At time: 1367.2844016551971 and batch: 700, loss is 4.9016460514068605 and perplexity is 134.51100959366252
At time: 1368.2512862682343 and batch: 750, loss is 4.913152875900269 and perplexity is 136.06774352873902
At time: 1369.2151398658752 and batch: 800, loss is 4.89048131942749 and perplexity is 133.01758259246938
At time: 1370.1818625926971 and batch: 850, loss is 4.950017948150634 and perplexity is 141.17749777373402
At time: 1371.1486394405365 and batch: 900, loss is 4.891109590530395 and perplexity is 133.10117995394396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.051756976401969 and perplexity of 156.2968332338014
finished 72 epochs...
Completing Train Step...
At time: 1373.4276869297028 and batch: 50, loss is 4.9617781066894535 and perplexity is 142.84756842980525
At time: 1374.3733937740326 and batch: 100, loss is 4.883782653808594 and perplexity is 132.12952002311735
At time: 1375.3345098495483 and batch: 150, loss is 4.926629247665406 and perplexity is 137.91385452516303
At time: 1376.2929849624634 and batch: 200, loss is 4.839607744216919 and perplexity is 126.4197531238222
At time: 1377.2529048919678 and batch: 250, loss is 4.9869753360748295 and perplexity is 146.4926616497934
At time: 1378.1973550319672 and batch: 300, loss is 4.95780426979065 and perplexity is 142.28104187904972
At time: 1379.1582007408142 and batch: 350, loss is 4.952327432632447 and perplexity is 141.50392180455256
At time: 1380.1237914562225 and batch: 400, loss is 4.856316080093384 and perplexity is 128.5497617175608
At time: 1381.0888769626617 and batch: 450, loss is 4.9000200843811035 and perplexity is 134.29247683913422
At time: 1382.052994966507 and batch: 500, loss is 4.843390312194824 and perplexity is 126.89884997072286
At time: 1383.0124752521515 and batch: 550, loss is 4.93511926651001 and perplexity is 139.08973028958667
At time: 1383.9744136333466 and batch: 600, loss is 4.891323890686035 and perplexity is 133.1297066140486
At time: 1384.9419159889221 and batch: 650, loss is 4.7889128589630126 and perplexity is 120.17065517340929
At time: 1385.9027462005615 and batch: 700, loss is 4.895473299026489 and perplexity is 133.68326380219867
At time: 1386.8610272407532 and batch: 750, loss is 4.907483940124512 and perplexity is 135.29856649702722
At time: 1387.8196775913239 and batch: 800, loss is 4.884598007202149 and perplexity is 132.237296207525
At time: 1388.7755808830261 and batch: 850, loss is 4.944201383590698 and perplexity is 140.35871330832865
At time: 1389.7307164669037 and batch: 900, loss is 4.885641107559204 and perplexity is 132.37530494438295
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.047828987853168 and perplexity of 155.68410524472708
finished 73 epochs...
Completing Train Step...
At time: 1391.9747369289398 and batch: 50, loss is 4.956515426635742 and perplexity is 142.09778205410427
At time: 1392.9191074371338 and batch: 100, loss is 4.878057994842529 and perplexity is 131.37528451197437
At time: 1393.8647904396057 and batch: 150, loss is 4.920762796401977 and perplexity is 137.10715815506285
At time: 1394.8134615421295 and batch: 200, loss is 4.833779249191284 and perplexity is 125.68505938088832
At time: 1395.7650170326233 and batch: 250, loss is 4.981275720596313 and perplexity is 145.6600847459659
At time: 1396.7268226146698 and batch: 300, loss is 4.952233352661133 and perplexity is 141.49060974585728
At time: 1397.6812813282013 and batch: 350, loss is 4.946690406799316 and perplexity is 140.70850454194755
At time: 1398.631903886795 and batch: 400, loss is 4.850770387649536 and perplexity is 127.83883738122678
At time: 1399.5821578502655 and batch: 450, loss is 4.894299459457398 and perplexity is 133.52643316243575
At time: 1400.531105041504 and batch: 500, loss is 4.837615623474121 and perplexity is 126.16816039607536
At time: 1401.4997205734253 and batch: 550, loss is 4.929471111297607 and perplexity is 138.30634432981395
At time: 1402.4562957286835 and batch: 600, loss is 4.885800485610962 and perplexity is 132.39640434393178
At time: 1403.4128217697144 and batch: 650, loss is 4.783280191421508 and perplexity is 119.49567657417933
At time: 1404.3751463890076 and batch: 700, loss is 4.889381103515625 and perplexity is 132.87131500925892
At time: 1405.3330521583557 and batch: 750, loss is 4.901888418197632 and perplexity is 134.5436145464004
At time: 1406.2854459285736 and batch: 800, loss is 4.87879412651062 and perplexity is 131.47202962350943
At time: 1407.2399408817291 and batch: 850, loss is 4.938460235595703 and perplexity is 139.5552019087976
At time: 1408.1988785266876 and batch: 900, loss is 4.880239114761353 and perplexity is 131.66214248396076
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.043987535450556 and perplexity of 155.08719939133127
finished 74 epochs...
Completing Train Step...
At time: 1410.4582760334015 and batch: 50, loss is 4.951318693161011 and perplexity is 141.36125318309803
At time: 1411.4153726100922 and batch: 100, loss is 4.872407102584839 and perplexity is 130.63499056449905
At time: 1412.3670086860657 and batch: 150, loss is 4.914978256225586 and perplexity is 136.31634573844906
At time: 1413.3177316188812 and batch: 200, loss is 4.828029174804687 and perplexity is 124.96443474830666
At time: 1414.2706446647644 and batch: 250, loss is 4.975644826889038 and perplexity is 144.84219318266753
At time: 1415.2248377799988 and batch: 300, loss is 4.946736764907837 and perplexity is 140.71502767326976
At time: 1416.1797573566437 and batch: 350, loss is 4.941128616333008 and perplexity is 139.92808559793613
At time: 1417.1317763328552 and batch: 400, loss is 4.845301847457886 and perplexity is 127.14165358718834
At time: 1418.0879790782928 and batch: 450, loss is 4.888634748458863 and perplexity is 132.77218282990657
At time: 1419.0424213409424 and batch: 500, loss is 4.831912240982056 and perplexity is 125.4506232584276
At time: 1419.990467786789 and batch: 550, loss is 4.923882579803466 and perplexity is 137.53557072102655
At time: 1420.9395680427551 and batch: 600, loss is 4.880359325408936 and perplexity is 131.6779706267075
At time: 1421.8906483650208 and batch: 650, loss is 4.777714891433716 and perplexity is 118.83249440236958
At time: 1422.8390505313873 and batch: 700, loss is 4.883367223739624 and perplexity is 132.07464084752027
At time: 1423.7932827472687 and batch: 750, loss is 4.896369504928589 and perplexity is 133.8031252344691
At time: 1424.7605278491974 and batch: 800, loss is 4.873069496154785 and perplexity is 130.72155100763436
At time: 1425.710614681244 and batch: 850, loss is 4.932790651321411 and perplexity is 138.76622064198025
At time: 1426.6691057682037 and batch: 900, loss is 4.874900274276733 and perplexity is 130.9610923699221
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.040234291390197 and perplexity of 154.50621026040537
finished 75 epochs...
Completing Train Step...
At time: 1428.9219856262207 and batch: 50, loss is 4.946185569763184 and perplexity is 140.63748760505425
At time: 1429.8675966262817 and batch: 100, loss is 4.866815414428711 and perplexity is 129.90655891428605
At time: 1430.83384847641 and batch: 150, loss is 4.90926944732666 and perplexity is 135.54035885871275
At time: 1431.8118300437927 and batch: 200, loss is 4.82235276222229 and perplexity is 124.25709453529713
At time: 1432.7751252651215 and batch: 250, loss is 4.970083351135254 and perplexity is 144.03889267581
At time: 1433.7242138385773 and batch: 300, loss is 4.94131106376648 and perplexity is 139.95361744706352
At time: 1434.6729023456573 and batch: 350, loss is 4.935644989013672 and perplexity is 139.16287211529544
At time: 1435.6232459545135 and batch: 400, loss is 4.839906120300293 and perplexity is 126.4574793826472
At time: 1436.5719952583313 and batch: 450, loss is 4.883027410507202 and perplexity is 132.0297677615589
At time: 1437.529333114624 and batch: 500, loss is 4.826290426254272 and perplexity is 124.74734180824582
At time: 1438.4942939281464 and batch: 550, loss is 4.91834885597229 and perplexity is 136.77658879069156
At time: 1439.4590909481049 and batch: 600, loss is 4.874987936019897 and perplexity is 130.9725731507712
At time: 1440.4200944900513 and batch: 650, loss is 4.772218723297119 and perplexity is 118.18116258723836
At time: 1441.3829457759857 and batch: 700, loss is 4.877428798675537 and perplexity is 131.292649686006
At time: 1442.347080230713 and batch: 750, loss is 4.890928707122803 and perplexity is 133.07710633628392
At time: 1443.3169329166412 and batch: 800, loss is 4.867424468994141 and perplexity is 129.9857031962187
At time: 1444.2939448356628 and batch: 850, loss is 4.927192935943603 and perplexity is 137.99161686316006
At time: 1445.270424604416 and batch: 900, loss is 4.869622392654419 and perplexity is 130.27171605084084
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.036556296152611 and perplexity of 153.938980927722
finished 76 epochs...
Completing Train Step...
At time: 1447.5476937294006 and batch: 50, loss is 4.941113739013672 and perplexity is 139.92600385860808
At time: 1448.5218539237976 and batch: 100, loss is 4.861281223297119 and perplexity is 129.1896168650032
At time: 1449.4979407787323 and batch: 150, loss is 4.903635683059693 and perplexity is 134.77890337260112
At time: 1450.4774987697601 and batch: 200, loss is 4.816746711730957 and perplexity is 123.56245190949936
At time: 1451.4479439258575 and batch: 250, loss is 4.964583015441894 and perplexity is 143.24880527807133
At time: 1452.4171113967896 and batch: 300, loss is 4.935952882766724 and perplexity is 139.20572609117374
At time: 1453.3884477615356 and batch: 350, loss is 4.930243330001831 and perplexity is 138.4131883239742
At time: 1454.3497023582458 and batch: 400, loss is 4.8345818519592285 and perplexity is 125.78597504961692
At time: 1455.3308465480804 and batch: 450, loss is 4.8774742603302 and perplexity is 131.2986186027832
At time: 1456.2948637008667 and batch: 500, loss is 4.82077657699585 and perplexity is 124.06139660721706
At time: 1457.2597692012787 and batch: 550, loss is 4.912866582870484 and perplexity is 136.02879385796376
At time: 1458.2243840694427 and batch: 600, loss is 4.869680757522583 and perplexity is 130.27931956426053
At time: 1459.1880369186401 and batch: 650, loss is 4.766797895431519 and perplexity is 117.54225611344619
At time: 1460.1587574481964 and batch: 700, loss is 4.871560287475586 and perplexity is 130.52441370637683
At time: 1461.1273691654205 and batch: 750, loss is 4.885565452575683 and perplexity is 132.36529047169603
At time: 1462.1035900115967 and batch: 800, loss is 4.8618537712097165 and perplexity is 129.26360528940626
At time: 1463.0786538124084 and batch: 850, loss is 4.92166425704956 and perplexity is 137.23081058816913
At time: 1464.0500037670135 and batch: 900, loss is 4.864404544830323 and perplexity is 129.5937483650022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.032940172169306 and perplexity of 153.38332375493562
finished 77 epochs...
Completing Train Step...
At time: 1466.3915333747864 and batch: 50, loss is 4.9360973834991455 and perplexity is 139.22584287396108
At time: 1467.3570742607117 and batch: 100, loss is 4.855813255310059 and perplexity is 128.48513995955093
At time: 1468.3222835063934 and batch: 150, loss is 4.8980827808380125 and perplexity is 134.03256339485262
At time: 1469.290288925171 and batch: 200, loss is 4.811207008361817 and perplexity is 122.87984504380412
At time: 1470.2526998519897 and batch: 250, loss is 4.959131298065185 and perplexity is 142.46997817873302
At time: 1471.2047176361084 and batch: 300, loss is 4.930657396316528 and perplexity is 138.47051242994058
At time: 1472.1904430389404 and batch: 350, loss is 4.924923191070556 and perplexity is 137.67876627807766
At time: 1473.1635093688965 and batch: 400, loss is 4.8293303203582765 and perplexity is 125.1271374939589
At time: 1474.1116206645966 and batch: 450, loss is 4.872059679031372 and perplexity is 130.5896127749828
At time: 1475.087610244751 and batch: 500, loss is 4.815354824066162 and perplexity is 123.39058649327811
At time: 1476.0478446483612 and batch: 550, loss is 4.907445764541626 and perplexity is 135.2934014939767
At time: 1477.0084700584412 and batch: 600, loss is 4.864442481994629 and perplexity is 129.59866487758566
At time: 1477.9838707447052 and batch: 650, loss is 4.761433935165405 and perplexity is 116.91345206968268
At time: 1478.9594221115112 and batch: 700, loss is 4.865748815536499 and perplexity is 129.76807458919956
At time: 1479.9408297538757 and batch: 750, loss is 4.880262956619263 and perplexity is 131.66528159147492
At time: 1480.9060971736908 and batch: 800, loss is 4.856356906890869 and perplexity is 128.5550100997858
At time: 1481.877721786499 and batch: 850, loss is 4.916198987960815 and perplexity is 136.4828530374172
At time: 1482.8469812870026 and batch: 900, loss is 4.859246912002564 and perplexity is 128.92707210770138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.029388845783391 and perplexity of 152.8395755951943
finished 78 epochs...
Completing Train Step...
At time: 1485.1951158046722 and batch: 50, loss is 4.931141452789307 and perplexity is 138.5375562029483
At time: 1486.1549968719482 and batch: 100, loss is 4.850414543151856 and perplexity is 127.79335472721081
At time: 1487.0971710681915 and batch: 150, loss is 4.892605361938476 and perplexity is 133.30041786333825
At time: 1488.0332317352295 and batch: 200, loss is 4.805735940933228 and perplexity is 122.20939683199482
At time: 1488.972998380661 and batch: 250, loss is 4.953728046417236 and perplexity is 141.70225300832027
At time: 1489.9171471595764 and batch: 300, loss is 4.925424823760986 and perplexity is 137.74784777334034
At time: 1490.877362728119 and batch: 350, loss is 4.919669618606568 and perplexity is 136.9573575484426
At time: 1491.8326745033264 and batch: 400, loss is 4.824148588180542 and perplexity is 124.48043913515187
At time: 1492.7836697101593 and batch: 450, loss is 4.866614246368409 and perplexity is 129.8804284922
At time: 1493.7335057258606 and batch: 500, loss is 4.809932641983032 and perplexity is 122.72335083730665
At time: 1494.687436580658 and batch: 550, loss is 4.902097072601318 and perplexity is 134.57169059305693
At time: 1495.6486961841583 and batch: 600, loss is 4.859259538650512 and perplexity is 128.92870003472953
At time: 1496.6027808189392 and batch: 650, loss is 4.756137218475342 and perplexity is 116.29583176066656
At time: 1497.5607023239136 and batch: 700, loss is 4.860029554367065 and perplexity is 129.0280153923569
At time: 1498.5115106105804 and batch: 750, loss is 4.87504487991333 and perplexity is 130.9800314513696
At time: 1499.4696018695831 and batch: 800, loss is 4.850943002700806 and perplexity is 127.86090619334534
At time: 1500.4226396083832 and batch: 850, loss is 4.910805959701538 and perplexity is 135.74877837597393
At time: 1501.3867111206055 and batch: 900, loss is 4.854144344329834 and perplexity is 128.27088853168584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.025896464308647 and perplexity of 152.3067324800685
finished 79 epochs...
Completing Train Step...
At time: 1503.6948771476746 and batch: 50, loss is 4.92623571395874 and perplexity is 137.85959145266554
At time: 1504.6537923812866 and batch: 100, loss is 4.845094404220581 and perplexity is 127.11528164640684
At time: 1505.6157658100128 and batch: 150, loss is 4.887281141281128 and perplexity is 132.59258303143537
At time: 1506.5802626609802 and batch: 200, loss is 4.800325813293457 and perplexity is 121.55001367818333
At time: 1507.5408337116241 and batch: 250, loss is 4.948384218215942 and perplexity is 140.94704017351523
At time: 1508.5039246082306 and batch: 300, loss is 4.9202585124969485 and perplexity is 137.03803465234083
At time: 1509.4628751277924 and batch: 350, loss is 4.914491777420044 and perplexity is 136.25004685320712
At time: 1510.4188284873962 and batch: 400, loss is 4.81902551651001 and perplexity is 123.84434768615448
At time: 1511.377447128296 and batch: 450, loss is 4.86124080657959 and perplexity is 129.18439555026558
At time: 1512.3317425251007 and batch: 500, loss is 4.80458818435669 and perplexity is 122.06921065825706
At time: 1513.292837381363 and batch: 550, loss is 4.896799545288086 and perplexity is 133.86067835274173
At time: 1514.2575495243073 and batch: 600, loss is 4.854142303466797 and perplexity is 128.27062674863788
At time: 1515.2167418003082 and batch: 650, loss is 4.750891590118409 and perplexity is 115.68738428764946
At time: 1516.1793274879456 and batch: 700, loss is 4.8543704319000245 and perplexity is 128.29989226376827
At time: 1517.150931596756 and batch: 750, loss is 4.8698792648315425 and perplexity is 130.30518352840835
At time: 1518.1157596111298 and batch: 800, loss is 4.845600557327271 and perplexity is 127.17963772676094
At time: 1519.0778920650482 and batch: 850, loss is 4.905477132797241 and perplexity is 135.02732060256784
At time: 1520.0408928394318 and batch: 900, loss is 4.849090280532837 and perplexity is 127.62423476892113
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.022486020440924 and perplexity of 151.78818366169668
finished 80 epochs...
Completing Train Step...
At time: 1522.3533115386963 and batch: 50, loss is 4.921387214660644 and perplexity is 137.19279710248748
At time: 1523.3111815452576 and batch: 100, loss is 4.839840888977051 and perplexity is 126.44923066297352
At time: 1524.2716329097748 and batch: 150, loss is 4.881953773498535 and perplexity is 131.88809178453957
At time: 1525.2418448925018 and batch: 200, loss is 4.794984960556031 and perplexity is 120.90256346205216
At time: 1526.2042138576508 and batch: 250, loss is 4.943106641769409 and perplexity is 140.20514083133182
At time: 1527.1700127124786 and batch: 300, loss is 4.915186853408813 and perplexity is 136.34478391015605
At time: 1528.1406104564667 and batch: 350, loss is 4.909369430541992 and perplexity is 135.5539112970956
At time: 1529.1046557426453 and batch: 400, loss is 4.813956127166748 and perplexity is 123.21812110405263
At time: 1530.0706596374512 and batch: 450, loss is 4.855944967269897 and perplexity is 128.50206410367673
At time: 1531.0405769348145 and batch: 500, loss is 4.799280633926392 and perplexity is 121.42303847930832
At time: 1532.0134744644165 and batch: 550, loss is 4.8915433502197265 and perplexity is 133.15892640354602
At time: 1532.9875993728638 and batch: 600, loss is 4.849084157943725 and perplexity is 127.62345338056295
At time: 1533.9613635540009 and batch: 650, loss is 4.7457068729400635 and perplexity is 115.0891301484249
At time: 1534.9353981018066 and batch: 700, loss is 4.848775148391724 and perplexity is 127.58402260696553
At time: 1535.9079830646515 and batch: 750, loss is 4.864776334762573 and perplexity is 129.64193897376342
At time: 1536.89426779747 and batch: 800, loss is 4.840313692092895 and perplexity is 126.50903038886146
At time: 1537.8733494281769 and batch: 850, loss is 4.900210857391357 and perplexity is 134.31809866309382
At time: 1538.8559021949768 and batch: 900, loss is 4.844082651138305 and perplexity is 126.98673740687731
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01916044052333 and perplexity of 151.28423834604166
finished 81 epochs...
Completing Train Step...
At time: 1541.188084602356 and batch: 50, loss is 4.916595935821533 and perplexity is 136.53704036801608
At time: 1542.1516966819763 and batch: 100, loss is 4.8346568202972415 and perplexity is 125.795405368595
At time: 1543.1095979213715 and batch: 150, loss is 4.876644191741943 and perplexity is 131.18967696457057
At time: 1544.0649890899658 and batch: 200, loss is 4.789707231521606 and perplexity is 120.26615336977872
At time: 1545.0296745300293 and batch: 250, loss is 4.937902040481568 and perplexity is 139.4773246143247
At time: 1545.9926018714905 and batch: 300, loss is 4.910178050994873 and perplexity is 135.66356729130328
At time: 1546.948642730713 and batch: 350, loss is 4.904301156997681 and perplexity is 134.8686250706009
At time: 1547.9061756134033 and batch: 400, loss is 4.808934068679809 and perplexity is 122.6008637418284
At time: 1548.86754488945 and batch: 450, loss is 4.8507177352905275 and perplexity is 127.83210654206455
At time: 1549.848179101944 and batch: 500, loss is 4.794017505645752 and perplexity is 120.7856522456495
At time: 1550.8505291938782 and batch: 550, loss is 4.8863313770294186 and perplexity is 132.4667111198132
At time: 1551.826229095459 and batch: 600, loss is 4.844083156585693 and perplexity is 126.98680159200826
At time: 1552.7969949245453 and batch: 650, loss is 4.740580644607544 and perplexity is 114.50066657665734
At time: 1553.7665383815765 and batch: 700, loss is 4.843257808685303 and perplexity is 126.88203654169135
At time: 1554.7326612472534 and batch: 750, loss is 4.859754724502563 and perplexity is 128.99255951276018
At time: 1555.7063047885895 and batch: 800, loss is 4.835063304901123 and perplexity is 125.84654965809526
At time: 1556.6810064315796 and batch: 850, loss is 4.895000820159912 and perplexity is 133.62011620436456
At time: 1557.6545503139496 and batch: 900, loss is 4.83912335395813 and perplexity is 126.35853145567741
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.015956094820205 and perplexity of 150.80024719855209
finished 82 epochs...
Completing Train Step...
At time: 1560.0117959976196 and batch: 50, loss is 4.9118585109710695 and perplexity is 135.8917361469845
At time: 1560.967997789383 and batch: 100, loss is 4.8295470809936525 and perplexity is 125.15426307155738
At time: 1561.9199376106262 and batch: 150, loss is 4.871367168426514 and perplexity is 130.49920938951632
At time: 1562.8691773414612 and batch: 200, loss is 4.7844947147369385 and perplexity is 119.64089502721971
At time: 1563.8255715370178 and batch: 250, loss is 4.932757120132447 and perplexity is 138.76156772362344
At time: 1564.7897064685822 and batch: 300, loss is 4.905206985473633 and perplexity is 134.9908482599678
At time: 1565.760014295578 and batch: 350, loss is 4.899282073974609 and perplexity is 134.19340415654514
At time: 1566.724764585495 and batch: 400, loss is 4.803955678939819 and perplexity is 121.99202563382887
At time: 1567.695070028305 and batch: 450, loss is 4.845548810958863 and perplexity is 127.17305681264382
At time: 1568.6700329780579 and batch: 500, loss is 4.788817596435547 and perplexity is 120.15920795832426
At time: 1569.6648845672607 and batch: 550, loss is 4.8811651229858395 and perplexity is 131.7841191777491
At time: 1570.6456141471863 and batch: 600, loss is 4.8391366386413575 and perplexity is 126.36021009989096
At time: 1571.6201179027557 and batch: 650, loss is 4.7355101203918455 and perplexity is 113.9215576078426
At time: 1572.5918035507202 and batch: 700, loss is 4.837803974151611 and perplexity is 126.1919264926736
At time: 1573.559284210205 and batch: 750, loss is 4.854750852584839 and perplexity is 128.3487094815959
At time: 1574.5266535282135 and batch: 800, loss is 4.8299071407318115 and perplexity is 125.19933419641018
At time: 1575.5008716583252 and batch: 850, loss is 4.88986795425415 and perplexity is 132.93601925647727
At time: 1576.4674220085144 and batch: 900, loss is 4.834215478897095 and perplexity is 125.73989889781033
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012790209626498 and perplexity of 150.32358585440932
finished 83 epochs...
Completing Train Step...
At time: 1578.7659316062927 and batch: 50, loss is 4.907163944244385 and perplexity is 135.25527843953049
At time: 1579.728865146637 and batch: 100, loss is 4.824491634368896 and perplexity is 124.5231490006313
At time: 1580.680790424347 and batch: 150, loss is 4.866091394424439 and perplexity is 129.8125380075618
At time: 1581.6433446407318 and batch: 200, loss is 4.779342241287232 and perplexity is 119.02603388023667
At time: 1582.6155638694763 and batch: 250, loss is 4.927674140930176 and perplexity is 138.0580350964107
At time: 1583.5973675251007 and batch: 300, loss is 4.900282506942749 and perplexity is 134.32772283938664
At time: 1584.567652463913 and batch: 350, loss is 4.894313249588013 and perplexity is 133.52827452208592
At time: 1585.5243153572083 and batch: 400, loss is 4.799014291763306 and perplexity is 121.39070271097164
At time: 1586.476863861084 and batch: 450, loss is 4.840431098937988 and perplexity is 126.5238842869528
At time: 1587.4350006580353 and batch: 500, loss is 4.783678913116455 and perplexity is 119.54333159279538
At time: 1588.4035484790802 and batch: 550, loss is 4.876050443649292 and perplexity is 131.11180646411555
At time: 1589.370058298111 and batch: 600, loss is 4.8342461681365965 and perplexity is 125.7437578188959
At time: 1590.3450424671173 and batch: 650, loss is 4.730488510131836 and perplexity is 113.35092189950902
At time: 1591.3169779777527 and batch: 700, loss is 4.832372999191284 and perplexity is 125.50843898145837
At time: 1592.2941756248474 and batch: 750, loss is 4.849848012924195 and perplexity is 127.72097643304524
At time: 1593.2873368263245 and batch: 800, loss is 4.824760055541992 and perplexity is 124.55657813670688
At time: 1594.2654123306274 and batch: 850, loss is 4.884779148101806 and perplexity is 132.2612519599461
At time: 1595.2429192066193 and batch: 900, loss is 4.829358196258545 and perplexity is 125.13062557418097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.009697064961473 and perplexity of 149.8593316296831
finished 84 epochs...
Completing Train Step...
At time: 1597.605352640152 and batch: 50, loss is 4.902510023117065 and perplexity is 134.62727351780111
At time: 1598.5706086158752 and batch: 100, loss is 4.819517860412597 and perplexity is 123.90533670816227
At time: 1599.5247864723206 and batch: 150, loss is 4.8609059715270995 and perplexity is 129.14114732731508
At time: 1600.4768607616425 and batch: 200, loss is 4.774231977462769 and perplexity is 118.4193309714191
At time: 1601.4330575466156 and batch: 250, loss is 4.922667989730835 and perplexity is 137.36862278936914
At time: 1602.3899371623993 and batch: 300, loss is 4.895414505004883 and perplexity is 133.67540425654775
At time: 1603.3485174179077 and batch: 350, loss is 4.8893811130523686 and perplexity is 132.87131627641864
At time: 1604.3052382469177 and batch: 400, loss is 4.794130611419678 and perplexity is 120.79931457295531
At time: 1605.2641384601593 and batch: 450, loss is 4.8353555870056155 and perplexity is 125.88333772845787
At time: 1606.2275214195251 and batch: 500, loss is 4.778590116500855 and perplexity is 118.9365451075118
At time: 1607.1949276924133 and batch: 550, loss is 4.870982732772827 and perplexity is 130.44905048270286
At time: 1608.1659171581268 and batch: 600, loss is 4.829408378601074 and perplexity is 125.13690507965329
At time: 1609.1366589069366 and batch: 650, loss is 4.7255193138122555 and perplexity is 112.78905608253466
At time: 1610.117784500122 and batch: 700, loss is 4.82700623512268 and perplexity is 124.83666902861664
At time: 1611.0923557281494 and batch: 750, loss is 4.84500207901001 and perplexity is 127.10354624300719
At time: 1612.0540282726288 and batch: 800, loss is 4.819720487594605 and perplexity is 123.93044584118564
At time: 1613.0153541564941 and batch: 850, loss is 4.879774160385132 and perplexity is 131.60093982395512
At time: 1613.9807481765747 and batch: 900, loss is 4.824559812545776 and perplexity is 124.53163905132433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.006665373501712 and perplexity of 149.4056923684087
finished 85 epochs...
Completing Train Step...
At time: 1616.3058845996857 and batch: 50, loss is 4.897901124954224 and perplexity is 134.0082178024197
At time: 1617.263435602188 and batch: 100, loss is 4.814598436355591 and perplexity is 123.29729065841394
At time: 1618.230075120926 and batch: 150, loss is 4.855799894332886 and perplexity is 128.48342328399718
At time: 1619.186285495758 and batch: 200, loss is 4.769166564941406 and perplexity is 117.82100487314061
At time: 1620.1457455158234 and batch: 250, loss is 4.917707242965698 and perplexity is 136.68885929952705
At time: 1621.1387717723846 and batch: 300, loss is 4.890586080551148 and perplexity is 133.03151839384037
At time: 1622.1033313274384 and batch: 350, loss is 4.884487628936768 and perplexity is 132.22270088966866
At time: 1623.0718035697937 and batch: 400, loss is 4.789289665222168 and perplexity is 120.21594476060956
At time: 1624.054230928421 and batch: 450, loss is 4.830327110290527 and perplexity is 125.25192514807058
At time: 1625.0305669307709 and batch: 500, loss is 4.773554134368896 and perplexity is 118.33908844473282
At time: 1626.0054638385773 and batch: 550, loss is 4.865963954925537 and perplexity is 129.79599581685355
At time: 1626.9927411079407 and batch: 600, loss is 4.824636363983155 and perplexity is 124.54117249218746
At time: 1627.9727101325989 and batch: 650, loss is 4.720605592727662 and perplexity is 112.2362015184876
At time: 1628.9486265182495 and batch: 700, loss is 4.821685209274292 and perplexity is 124.17417402552503
At time: 1629.9236829280853 and batch: 750, loss is 4.840214691162109 and perplexity is 126.49650649704887
At time: 1630.89799284935 and batch: 800, loss is 4.814673366546631 and perplexity is 123.30652969409476
At time: 1631.8699345588684 and batch: 850, loss is 4.874784784317017 and perplexity is 130.94596855198125
At time: 1632.8391993045807 and batch: 900, loss is 4.819815053939819 and perplexity is 123.94216604466928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.003624484963613 and perplexity of 148.9520563887487
finished 86 epochs...
Completing Train Step...
At time: 1635.1493463516235 and batch: 50, loss is 4.8933319664001464 and perplexity is 133.39730973847176
At time: 1636.12473487854 and batch: 100, loss is 4.809736747741699 and perplexity is 122.69931239417367
At time: 1637.0695848464966 and batch: 150, loss is 4.850766239166259 and perplexity is 127.83830704504781
At time: 1638.018040895462 and batch: 200, loss is 4.764173440933227 and perplexity is 117.23417625826251
At time: 1638.9778311252594 and batch: 250, loss is 4.91279167175293 and perplexity is 136.0186041706616
At time: 1639.9327037334442 and batch: 300, loss is 4.8858123397827145 and perplexity is 132.39797380295062
At time: 1640.9075181484222 and batch: 350, loss is 4.879619617462158 and perplexity is 131.5806034015141
At time: 1641.8726580142975 and batch: 400, loss is 4.784532747268677 and perplexity is 119.6454453598868
At time: 1642.8413462638855 and batch: 450, loss is 4.8253453540802 and perplexity is 124.62950225892376
At time: 1643.8058462142944 and batch: 500, loss is 4.768566741943359 and perplexity is 117.75035431579754
At time: 1644.7769417762756 and batch: 550, loss is 4.860987873077392 and perplexity is 129.15172462063023
At time: 1645.7418415546417 and batch: 600, loss is 4.819921207427979 and perplexity is 123.95532363627474
At time: 1646.711672782898 and batch: 650, loss is 4.71575177192688 and perplexity is 111.69274709112094
At time: 1647.6704156398773 and batch: 700, loss is 4.816408519744873 and perplexity is 123.52067114384094
At time: 1648.6405210494995 and batch: 750, loss is 4.835405025482178 and perplexity is 125.88956136274199
At time: 1649.5985238552094 and batch: 800, loss is 4.809727725982666 and perplexity is 122.69820543553709
At time: 1650.557301044464 and batch: 850, loss is 4.8698866748809815 and perplexity is 130.3061490998379
At time: 1651.526822090149 and batch: 900, loss is 4.8151130104064945 and perplexity is 123.36075257125994
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.000731585776969 and perplexity of 148.52177578525675
finished 87 epochs...
Completing Train Step...
At time: 1653.9196302890778 and batch: 50, loss is 4.888807926177979 and perplexity is 132.7951780047596
At time: 1654.8987770080566 and batch: 100, loss is 4.804921207427978 and perplexity is 122.10986929145591
At time: 1655.8728444576263 and batch: 150, loss is 4.845803127288819 and perplexity is 127.20540311064325
At time: 1656.833404302597 and batch: 200, loss is 4.759258527755737 and perplexity is 116.65939411972576
At time: 1657.8219895362854 and batch: 250, loss is 4.907895107269287 and perplexity is 135.35420826055457
At time: 1658.8049530982971 and batch: 300, loss is 4.881104068756104 and perplexity is 131.77607344547675
At time: 1659.7754850387573 and batch: 350, loss is 4.874796047210693 and perplexity is 130.94744339080788
At time: 1660.7488272190094 and batch: 400, loss is 4.77981635093689 and perplexity is 119.08247865090772
At time: 1661.723643541336 and batch: 450, loss is 4.820389814376831 and perplexity is 124.01342357423212
At time: 1662.6957337856293 and batch: 500, loss is 4.763647661209107 and perplexity is 117.17255310693332
At time: 1663.6734204292297 and batch: 550, loss is 4.85606237411499 and perplexity is 128.5171520113056
At time: 1664.6420300006866 and batch: 600, loss is 4.815229110717773 and perplexity is 123.3750756244724
At time: 1665.6388130187988 and batch: 650, loss is 4.710950441360474 and perplexity is 111.15775864639961
At time: 1666.647878408432 and batch: 700, loss is 4.811184711456299 and perplexity is 122.87710523405393
At time: 1667.6154754161835 and batch: 750, loss is 4.830740976333618 and perplexity is 125.30377339509543
At time: 1668.5918009281158 and batch: 800, loss is 4.8047887802124025 and perplexity is 122.09369969214248
At time: 1669.578736782074 and batch: 850, loss is 4.865003461837769 and perplexity is 129.6713875123388
At time: 1670.5629024505615 and batch: 900, loss is 4.810450286865234 and perplexity is 122.78689439687459
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.997837014394264 and perplexity of 148.0924905015409
finished 88 epochs...
Completing Train Step...
At time: 1672.8706223964691 and batch: 50, loss is 4.8843148326873775 and perplexity is 132.19985527674567
At time: 1673.8234844207764 and batch: 100, loss is 4.800167961120605 and perplexity is 121.53082825868395
At time: 1674.7643263339996 and batch: 150, loss is 4.840992031097412 and perplexity is 126.59487551135352
At time: 1675.7082047462463 and batch: 200, loss is 4.754399347305298 and perplexity is 116.0939001038785
At time: 1676.6750047206879 and batch: 250, loss is 4.903062696456909 and perplexity is 134.70169898728227
At time: 1677.6538577079773 and batch: 300, loss is 4.87643666267395 and perplexity is 131.16245411804354
At time: 1678.6350617408752 and batch: 350, loss is 4.86999324798584 and perplexity is 130.32003697075322
At time: 1679.6238567829132 and batch: 400, loss is 4.775204095840454 and perplexity is 118.53450455143646
At time: 1680.6180322170258 and batch: 450, loss is 4.815491828918457 and perplexity is 123.40749276044909
At time: 1681.5770182609558 and batch: 500, loss is 4.758778257369995 and perplexity is 116.60337951966495
At time: 1682.5367002487183 and batch: 550, loss is 4.851172590255738 and perplexity is 127.89026483623022
At time: 1683.4977915287018 and batch: 600, loss is 4.81059063911438 and perplexity is 122.80412902309975
At time: 1684.4591574668884 and batch: 650, loss is 4.7062073802948 and perplexity is 110.63177897213721
At time: 1685.4262518882751 and batch: 700, loss is 4.806008033752441 and perplexity is 122.24265365557949
At time: 1686.3900549411774 and batch: 750, loss is 4.826030101776123 and perplexity is 124.71487124821327
At time: 1687.3584325313568 and batch: 800, loss is 4.799939403533935 and perplexity is 121.50305463992767
At time: 1688.337715625763 and batch: 850, loss is 4.860205345153808 and perplexity is 129.0506993224492
At time: 1689.3194048404694 and batch: 900, loss is 4.80581521987915 and perplexity is 122.21908584821995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.995092104559076 and perplexity of 147.68654736138717
finished 89 epochs...
Completing Train Step...
At time: 1691.6577558517456 and batch: 50, loss is 4.8798753452301025 and perplexity is 131.61425651836163
At time: 1692.6723592281342 and batch: 100, loss is 4.795463743209839 and perplexity is 120.96046337186898
At time: 1693.6594903469086 and batch: 150, loss is 4.836097602844238 and perplexity is 125.97677982234056
At time: 1694.6423749923706 and batch: 200, loss is 4.749638729095459 and perplexity is 115.54253483009526
At time: 1695.6243193149567 and batch: 250, loss is 4.898272714614868 and perplexity is 134.05802312359452
At time: 1696.597633600235 and batch: 300, loss is 4.8718048858642575 and perplexity is 130.55634367250164
At time: 1697.573715209961 and batch: 350, loss is 4.865235729217529 and perplexity is 129.70150944378184
At time: 1698.5473749637604 and batch: 400, loss is 4.770579996109009 and perplexity is 117.9876544997706
At time: 1699.5288994312286 and batch: 450, loss is 4.810680532455445 and perplexity is 122.81516879274768
At time: 1700.5264203548431 and batch: 500, loss is 4.753955249786377 and perplexity is 116.04235453735679
At time: 1701.503984928131 and batch: 550, loss is 4.846332092285156 and perplexity is 127.27270811566025
At time: 1702.47501206398 and batch: 600, loss is 4.805995807647705 and perplexity is 122.24115911322896
At time: 1703.4457507133484 and batch: 650, loss is 4.7014954566955565 and perplexity is 110.111716691051
At time: 1704.430080652237 and batch: 700, loss is 4.800880069732666 and perplexity is 121.61740222949447
At time: 1705.4101741313934 and batch: 750, loss is 4.821447305679321 and perplexity is 124.14463605685563
At time: 1706.3947451114655 and batch: 800, loss is 4.795144491195678 and perplexity is 120.92185266390476
At time: 1707.3792407512665 and batch: 850, loss is 4.855432710647583 and perplexity is 128.43625492739085
At time: 1708.3582973480225 and batch: 900, loss is 4.801235227584839 and perplexity is 121.6606032759985
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.9923685152236725 and perplexity of 147.28485712394405
finished 90 epochs...
Completing Train Step...
At time: 1710.697987794876 and batch: 50, loss is 4.8754700183868405 and perplexity is 131.03572794052252
At time: 1711.6403510570526 and batch: 100, loss is 4.790806264877319 and perplexity is 120.39840254371039
At time: 1712.5799555778503 and batch: 150, loss is 4.831305456161499 and perplexity is 125.37452481453023
At time: 1713.5289011001587 and batch: 200, loss is 4.744942350387573 and perplexity is 115.00117553880534
At time: 1714.5018684864044 and batch: 250, loss is 4.8935994529724125 and perplexity is 133.43299650025526
At time: 1715.456706047058 and batch: 300, loss is 4.8671991157531735 and perplexity is 129.95641379707848
At time: 1716.4126539230347 and batch: 350, loss is 4.860509281158447 and perplexity is 129.08992843766538
At time: 1717.361042022705 and batch: 400, loss is 4.766003551483155 and perplexity is 117.44892420735039
At time: 1718.314757823944 and batch: 450, loss is 4.805821800231934 and perplexity is 122.21989009556782
At time: 1719.275227546692 and batch: 500, loss is 4.749172163009644 and perplexity is 115.48863917579436
At time: 1720.2348997592926 and batch: 550, loss is 4.841542015075683 and perplexity is 126.664519814486
At time: 1721.2090830802917 and batch: 600, loss is 4.801441698074341 and perplexity is 121.68572519368807
At time: 1722.184511423111 and batch: 650, loss is 4.696828422546386 and perplexity is 109.59901886875038
At time: 1723.1549353599548 and batch: 700, loss is 4.795813179016113 and perplexity is 121.00273867473666
At time: 1724.1154675483704 and batch: 750, loss is 4.816843299865723 and perplexity is 123.57438715264855
At time: 1725.086546421051 and batch: 800, loss is 4.790377912521362 and perplexity is 120.34684064844538
At time: 1726.0482957363129 and batch: 850, loss is 4.850728511810303 and perplexity is 127.83348413471143
At time: 1727.0101761817932 and batch: 900, loss is 4.796690673828125 and perplexity is 121.10896454967391
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.989672517123288 and perplexity of 146.88831221127552
finished 91 epochs...
Completing Train Step...
At time: 1729.3228685855865 and batch: 50, loss is 4.871146841049194 and perplexity is 130.47046000822345
At time: 1730.2863960266113 and batch: 100, loss is 4.786194868087769 and perplexity is 119.84447590611539
At time: 1731.2397718429565 and batch: 150, loss is 4.826484870910645 and perplexity is 124.77160062067891
At time: 1732.1953394412994 and batch: 200, loss is 4.740255050659179 and perplexity is 114.46339192106673
At time: 1733.1494443416595 and batch: 250, loss is 4.888926334381104 and perplexity is 132.81090297413516
At time: 1734.1016278266907 and batch: 300, loss is 4.862636203765869 and perplexity is 129.3647849204672
At time: 1735.0535097122192 and batch: 350, loss is 4.8558095359802245 and perplexity is 128.48466208182532
At time: 1736.0337660312653 and batch: 400, loss is 4.761545906066894 and perplexity is 116.92654370723471
At time: 1737.000036239624 and batch: 450, loss is 4.801066226959229 and perplexity is 121.64004429522195
At time: 1737.9732739925385 and batch: 500, loss is 4.74441276550293 and perplexity is 114.94028877830084
At time: 1738.96590924263 and batch: 550, loss is 4.836791944503784 and perplexity is 126.06428112309044
At time: 1739.9332914352417 and batch: 600, loss is 4.796941776275634 and perplexity is 121.13937912551509
At time: 1740.9034278392792 and batch: 650, loss is 4.692205591201782 and perplexity is 109.09353038320812
At time: 1741.8691499233246 and batch: 700, loss is 4.790781278610229 and perplexity is 120.39539427465009
At time: 1742.8430500030518 and batch: 750, loss is 4.8123287582397465 and perplexity is 123.01776283515277
At time: 1743.8127071857452 and batch: 800, loss is 4.7856879234313965 and perplexity is 119.7837367864459
At time: 1744.7875623703003 and batch: 850, loss is 4.846061401367187 and perplexity is 127.23826121191041
At time: 1745.7684457302094 and batch: 900, loss is 4.79218635559082 and perplexity is 120.5646779721773
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.987008290748074 and perplexity of 146.49748934713793
finished 92 epochs...
Completing Train Step...
At time: 1748.0914030075073 and batch: 50, loss is 4.866802482604981 and perplexity is 129.90487899642702
At time: 1749.040601491928 and batch: 100, loss is 4.781624336242675 and perplexity is 119.29797276888371
At time: 1749.9936184883118 and batch: 150, loss is 4.821734981536865 and perplexity is 124.18035460892904
At time: 1750.9578306674957 and batch: 200, loss is 4.735558919906616 and perplexity is 113.92711706022394
At time: 1751.9165244102478 and batch: 250, loss is 4.884300298690796 and perplexity is 132.1979338984637
At time: 1752.875839471817 and batch: 300, loss is 4.858075256347656 and perplexity is 128.77610243410226
At time: 1753.8341808319092 and batch: 350, loss is 4.851147785186767 and perplexity is 127.88709254873487
At time: 1754.796694278717 and batch: 400, loss is 4.757114133834839 and perplexity is 116.40949845729
At time: 1755.7673163414001 and batch: 450, loss is 4.796343488693237 and perplexity is 121.066924615723
At time: 1756.7475199699402 and batch: 500, loss is 4.739702939987183 and perplexity is 114.400212903344
At time: 1757.7450587749481 and batch: 550, loss is 4.832089824676514 and perplexity is 125.47290322179843
At time: 1758.7466509342194 and batch: 600, loss is 4.792484540939331 and perplexity is 120.60063395320344
At time: 1759.7261846065521 and batch: 650, loss is 4.687630729675293 and perplexity is 108.59558247810294
At time: 1760.7071950435638 and batch: 700, loss is 4.785792636871338 and perplexity is 119.79628041030537
At time: 1761.6904275417328 and batch: 750, loss is 4.8077555274963375 and perplexity is 122.45645868514706
At time: 1762.6901786327362 and batch: 800, loss is 4.7810295295715335 and perplexity is 119.22703463815523
At time: 1763.6830945014954 and batch: 850, loss is 4.841451616287231 and perplexity is 126.65307001288679
At time: 1764.6619877815247 and batch: 900, loss is 4.78771900177002 and perplexity is 120.02727417759047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.984410952215326 and perplexity of 146.11747949385727
finished 93 epochs...
Completing Train Step...
At time: 1767.0028512477875 and batch: 50, loss is 4.862428379058838 and perplexity is 129.33790251545167
At time: 1767.9737961292267 and batch: 100, loss is 4.777148904800415 and perplexity is 118.76525582879799
At time: 1768.9380984306335 and batch: 150, loss is 4.817166299819946 and perplexity is 123.61430812093039
At time: 1769.9137270450592 and batch: 200, loss is 4.730979862213135 and perplexity is 113.4066307961219
At time: 1770.8998889923096 and batch: 250, loss is 4.879715766906738 and perplexity is 131.59325541168084
At time: 1771.873838186264 and batch: 300, loss is 4.853602151870728 and perplexity is 128.20135987386143
At time: 1772.8460268974304 and batch: 350, loss is 4.846545305252075 and perplexity is 127.29984720046502
At time: 1773.8065564632416 and batch: 400, loss is 4.752666959762573 and perplexity is 115.89295458556448
At time: 1774.7721681594849 and batch: 450, loss is 4.791653070449829 and perplexity is 120.50039976171702
At time: 1775.7361595630646 and batch: 500, loss is 4.735023288726807 and perplexity is 113.86611048407148
At time: 1776.7068150043488 and batch: 550, loss is 4.827422313690185 and perplexity is 124.88862169848487
At time: 1777.664011001587 and batch: 600, loss is 4.7880816078186035 and perplexity is 120.07080468493794
At time: 1778.6153922080994 and batch: 650, loss is 4.683090915679932 and perplexity is 108.10369611404518
At time: 1779.5829887390137 and batch: 700, loss is 4.780843391418457 and perplexity is 119.20484400345681
At time: 1780.5570611953735 and batch: 750, loss is 4.803300037384033 and perplexity is 121.91206880672226
At time: 1781.5229332447052 and batch: 800, loss is 4.776432409286499 and perplexity is 118.68019153351955
At time: 1782.49414396286 and batch: 850, loss is 4.836867055892944 and perplexity is 126.07375034198769
At time: 1783.4909901618958 and batch: 900, loss is 4.783281469345093 and perplexity is 119.49582928062027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.981806088800299 and perplexity of 145.73735871352136
finished 94 epochs...
Completing Train Step...
At time: 1785.8136057853699 and batch: 50, loss is 4.858120107650757 and perplexity is 128.78187833963216
At time: 1786.7665119171143 and batch: 100, loss is 4.772606363296509 and perplexity is 118.2269832134116
At time: 1787.7364325523376 and batch: 150, loss is 4.812582178115845 and perplexity is 123.04894193190287
At time: 1788.7201495170593 and batch: 200, loss is 4.726421203613281 and perplexity is 112.8908252672794
At time: 1789.700073003769 and batch: 250, loss is 4.875177068710327 and perplexity is 130.99734668857283
At time: 1790.679684638977 and batch: 300, loss is 4.8491209125518795 and perplexity is 127.62814421678765
At time: 1791.664387702942 and batch: 350, loss is 4.841992883682251 and perplexity is 126.72164174631159
At time: 1792.6437408924103 and batch: 400, loss is 4.748313598632812 and perplexity is 115.38952729732159
At time: 1793.618525981903 and batch: 450, loss is 4.787012491226196 and perplexity is 119.94250359202003
At time: 1794.5841588974 and batch: 500, loss is 4.73037971496582 and perplexity is 113.33859053795136
At time: 1795.5561747550964 and batch: 550, loss is 4.8227927780151365 and perplexity is 124.31178164998057
At time: 1796.5243048667908 and batch: 600, loss is 4.783726301193237 and perplexity is 119.5489966555988
At time: 1797.4922368526459 and batch: 650, loss is 4.6785996055603025 and perplexity is 107.61925758588627
At time: 1798.4677612781525 and batch: 700, loss is 4.775940446853638 and perplexity is 118.62181969731292
At time: 1799.4440791606903 and batch: 750, loss is 4.79882119178772 and perplexity is 121.3672644322802
At time: 1800.4145674705505 and batch: 800, loss is 4.771859407424927 and perplexity is 118.13870584787362
At time: 1801.3790667057037 and batch: 850, loss is 4.832338829040527 and perplexity is 125.50415041244806
At time: 1802.3794565200806 and batch: 900, loss is 4.778893995285034 and perplexity is 118.97269289222784
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.979224636130137 and perplexity of 145.36162979159397
finished 95 epochs...
Completing Train Step...
At time: 1804.7526276111603 and batch: 50, loss is 4.853852891921997 and perplexity is 128.23350911979236
At time: 1805.7485980987549 and batch: 100, loss is 4.76819016456604 and perplexity is 117.70602054426247
At time: 1806.7094204425812 and batch: 150, loss is 4.807846269607544 and perplexity is 122.46757114691549
At time: 1807.6766948699951 and batch: 200, loss is 4.72192681312561 and perplexity is 112.38458828154556
At time: 1808.659206867218 and batch: 250, loss is 4.870661954879761 and perplexity is 130.407212021922
At time: 1809.632694721222 and batch: 300, loss is 4.844740676879883 and perplexity is 127.07032544745503
At time: 1810.6066427230835 and batch: 350, loss is 4.837490253448486 and perplexity is 126.15234368206386
At time: 1811.6083364486694 and batch: 400, loss is 4.743956651687622 and perplexity is 114.88787487891405
At time: 1812.6113078594208 and batch: 450, loss is 4.782405223846435 and perplexity is 119.39116745952623
At time: 1813.599621772766 and batch: 500, loss is 4.725776491165161 and perplexity is 112.81806660367927
At time: 1814.5816299915314 and batch: 550, loss is 4.818196601867676 and perplexity is 123.74173382792745
At time: 1815.5789320468903 and batch: 600, loss is 4.77937991142273 and perplexity is 119.0305176915131
At time: 1816.560804605484 and batch: 650, loss is 4.674152412414551 and perplexity is 107.14171660673212
At time: 1817.5576553344727 and batch: 700, loss is 4.771085643768311 and perplexity is 118.04732976715307
At time: 1818.5498721599579 and batch: 750, loss is 4.794379034042358 and perplexity is 120.8293275833006
At time: 1819.5269758701324 and batch: 800, loss is 4.767312545776367 and perplexity is 117.60276484517347
At time: 1820.5181486606598 and batch: 850, loss is 4.827864618301391 and perplexity is 124.94387272974149
At time: 1821.4867639541626 and batch: 900, loss is 4.774561834335327 and perplexity is 118.45839884463281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.976700456175085 and perplexity of 144.99517357473133
finished 96 epochs...
Completing Train Step...
At time: 1823.8677163124084 and batch: 50, loss is 4.849632606506348 and perplexity is 127.69346747793702
At time: 1824.8368422985077 and batch: 100, loss is 4.763770866394043 and perplexity is 117.18699026235622
At time: 1825.815989971161 and batch: 150, loss is 4.803310394287109 and perplexity is 121.91333144474125
At time: 1826.7862470149994 and batch: 200, loss is 4.717433366775513 and perplexity is 111.88072704802576
At time: 1827.7534184455872 and batch: 250, loss is 4.866223440170288 and perplexity is 129.82968033272428
At time: 1828.7279968261719 and batch: 300, loss is 4.840396347045899 and perplexity is 126.51948741897952
At time: 1829.6977779865265 and batch: 350, loss is 4.833014163970947 and perplexity is 125.5889363753731
At time: 1830.6825540065765 and batch: 400, loss is 4.739577379226684 and perplexity is 114.38584962736151
At time: 1831.6674540042877 and batch: 450, loss is 4.777897663116455 and perplexity is 118.85421560228531
At time: 1832.6561768054962 and batch: 500, loss is 4.721206331253052 and perplexity is 112.30364638501976
At time: 1833.6557443141937 and batch: 550, loss is 4.813637523651123 and perplexity is 123.17886963064089
At time: 1834.6304204463959 and batch: 600, loss is 4.7751184177398684 and perplexity is 118.5243491752855
At time: 1835.614985704422 and batch: 650, loss is 4.669732990264893 and perplexity is 106.66925689963395
At time: 1836.6287126541138 and batch: 700, loss is 4.766286869049072 and perplexity is 117.48220426486624
At time: 1837.6111102104187 and batch: 750, loss is 4.790050640106201 and perplexity is 120.30746089155492
At time: 1838.5946853160858 and batch: 800, loss is 4.762835359573364 and perplexity is 117.07741229712953
At time: 1839.5814273357391 and batch: 850, loss is 4.823397846221924 and perplexity is 124.38702151711628
At time: 1840.580334186554 and batch: 900, loss is 4.770263662338257 and perplexity is 117.9503369228366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.97420303135702 and perplexity of 144.6335108304809
finished 97 epochs...
Completing Train Step...
At time: 1842.976520061493 and batch: 50, loss is 4.8454482364654545 and perplexity is 127.16026709005044
At time: 1843.9559381008148 and batch: 100, loss is 4.7594067668914795 and perplexity is 116.6766888893355
At time: 1844.9425976276398 and batch: 150, loss is 4.798820829391479 and perplexity is 121.36722044924775
At time: 1845.9166295528412 and batch: 200, loss is 4.7130376243591305 and perplexity is 111.39000751905161
At time: 1846.8791410923004 and batch: 250, loss is 4.8618430900573735 and perplexity is 129.26222461251936
At time: 1847.8443186283112 and batch: 300, loss is 4.835987701416015 and perplexity is 125.96293555508149
At time: 1848.8218033313751 and batch: 350, loss is 4.828599996566773 and perplexity is 125.03578753007059
At time: 1849.808000087738 and batch: 400, loss is 4.73529767036438 and perplexity is 113.89735754054341
At time: 1850.7898907661438 and batch: 450, loss is 4.773365440368653 and perplexity is 118.31676067537332
At time: 1851.7783560752869 and batch: 500, loss is 4.716667919158936 and perplexity is 111.79512097982617
At time: 1852.7557446956635 and batch: 550, loss is 4.80911771774292 and perplexity is 122.62338134319606
At time: 1853.7518343925476 and batch: 600, loss is 4.770851516723633 and perplexity is 118.01969492986022
At time: 1854.7224955558777 and batch: 650, loss is 4.665375080108642 and perplexity is 106.20541329010543
At time: 1855.6878328323364 and batch: 700, loss is 4.761538410186768 and perplexity is 116.92566724316443
At time: 1856.674488067627 and batch: 750, loss is 4.7856708335876466 and perplexity is 119.78168971859253
At time: 1857.639151096344 and batch: 800, loss is 4.758353013992309 and perplexity is 116.55380524602126
At time: 1858.6035544872284 and batch: 850, loss is 4.818992576599121 and perplexity is 123.84026833156477
At time: 1859.5775439739227 and batch: 900, loss is 4.766012802124023 and perplexity is 117.45001069019393
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.971718984107449 and perplexity of 144.27468021632006
finished 98 epochs...
Completing Train Step...
At time: 1861.9117047786713 and batch: 50, loss is 4.841295671463013 and perplexity is 126.63332066209145
At time: 1862.89022231102 and batch: 100, loss is 4.755083580017089 and perplexity is 116.17336253026461
At time: 1863.8603913784027 and batch: 150, loss is 4.794306287765503 and perplexity is 120.82053801929185
At time: 1864.8317580223083 and batch: 200, loss is 4.708635215759277 and perplexity is 110.9007010458036
At time: 1865.7913496494293 and batch: 250, loss is 4.857365074157715 and perplexity is 128.68468040665383
At time: 1866.7789587974548 and batch: 300, loss is 4.831780023574829 and perplexity is 125.43403759876607
At time: 1867.7510678768158 and batch: 350, loss is 4.824258737564087 and perplexity is 124.49415133396727
At time: 1868.738242149353 and batch: 400, loss is 4.731054773330689 and perplexity is 113.41512653178138
At time: 1869.7044310569763 and batch: 450, loss is 4.7689022445678715 and perplexity is 117.7898664965573
At time: 1870.68163728714 and batch: 500, loss is 4.712168474197387 and perplexity is 111.2932349370445
At time: 1871.6531417369843 and batch: 550, loss is 4.804650297164917 and perplexity is 122.07679295520559
At time: 1872.6375019550323 and batch: 600, loss is 4.766661787033081 and perplexity is 117.52625871393279
At time: 1873.6096351146698 and batch: 650, loss is 4.661008081436157 and perplexity is 105.74262562329272
At time: 1874.584994316101 and batch: 700, loss is 4.756835727691651 and perplexity is 116.37709384882437
At time: 1875.5733847618103 and batch: 750, loss is 4.781417379379272 and perplexity is 119.27328578928716
At time: 1876.5543994903564 and batch: 800, loss is 4.753991231918335 and perplexity is 116.04653006379225
At time: 1877.5276958942413 and batch: 850, loss is 4.814565505981445 and perplexity is 123.29323049935304
At time: 1878.4988870620728 and batch: 900, loss is 4.761775741577148 and perplexity is 116.95342066759153
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.9692884471318495 and perplexity of 143.92444107841908
finished 99 epochs...
Completing Train Step...
At time: 1880.8590984344482 and batch: 50, loss is 4.837141885757446 and perplexity is 126.10840393541447
At time: 1881.8123700618744 and batch: 100, loss is 4.750807981491089 and perplexity is 115.67771222859044
At time: 1882.7759766578674 and batch: 150, loss is 4.789824886322021 and perplexity is 120.28030409248404
At time: 1883.7404499053955 and batch: 200, loss is 4.704265918731689 and perplexity is 110.41719999142644
At time: 1884.7321083545685 and batch: 250, loss is 4.853079814910888 and perplexity is 128.13441305121964
At time: 1885.6895244121552 and batch: 300, loss is 4.827492942810059 and perplexity is 124.89744278342673
At time: 1886.6570324897766 and batch: 350, loss is 4.819875688552856 and perplexity is 123.94968145779116
At time: 1887.6368865966797 and batch: 400, loss is 4.726853981018066 and perplexity is 112.9396924392038
At time: 1888.6187763214111 and batch: 450, loss is 4.764467477798462 and perplexity is 117.26865249634194
At time: 1889.5891060829163 and batch: 500, loss is 4.70770770072937 and perplexity is 110.79788666706793
At time: 1890.5663504600525 and batch: 550, loss is 4.800186605453491 and perplexity is 121.53309414102473
At time: 1891.5501508712769 and batch: 600, loss is 4.762480220794678 and perplexity is 117.0358409501502
At time: 1892.527037858963 and batch: 650, loss is 4.656680641174316 and perplexity is 105.28601940839866
At time: 1893.5146689414978 and batch: 700, loss is 4.752171993255615 and perplexity is 115.83560564872455
At time: 1894.4923796653748 and batch: 750, loss is 4.777138586044312 and perplexity is 118.76403032541234
At time: 1895.4580023288727 and batch: 800, loss is 4.749596319198608 and perplexity is 115.53763478701718
At time: 1896.449253320694 and batch: 850, loss is 4.810211963653565 and perplexity is 122.75763491659616
At time: 1897.4187896251678 and batch: 900, loss is 4.757566976547241 and perplexity is 116.46222558796818
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.966861254548373 and perplexity of 143.57553234808663
Finished Training.
langmodel
SETTINGS FOR THIS RUN
{'batch_size': 32, 'lr': 12.08068884456746, 'dropout': 0.5041766372594111, 'hidden_size': 300, 'clip': 0.12163134647722318, 'tune_wordvecs': True, 'anneal': 4.912640445451629, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.5177780835706017}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.08796268304189046 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.5728585720062256 and batch: 50, loss is 7.0192710876464846 and perplexity is 1117.9714175007227
At time: 2.9082834720611572 and batch: 100, loss is 6.232045297622681 and perplexity is 508.7950573356432
At time: 4.235276937484741 and batch: 150, loss is 6.051360282897949 and perplexity is 424.6903363072303
At time: 5.550072431564331 and batch: 200, loss is 5.838945951461792 and perplexity is 343.41717145896496
At time: 6.870333671569824 and batch: 250, loss is 5.83429741859436 and perplexity is 341.82449013394194
At time: 8.189668655395508 and batch: 300, loss is 5.714704856872559 and perplexity is 303.2946747593445
At time: 9.500067710876465 and batch: 350, loss is 5.677019872665405 and perplexity is 292.07770289272145
At time: 10.816286087036133 and batch: 400, loss is 5.517629108428955 and perplexity is 249.0438806451314
At time: 12.13445520401001 and batch: 450, loss is 5.512492189407348 and perplexity is 247.76784265534133
At time: 13.454178094863892 and batch: 500, loss is 5.447483377456665 and perplexity is 232.1731379162768
At time: 14.771070957183838 and batch: 550, loss is 5.495151233673096 and perplexity is 243.5083500383003
At time: 16.0899498462677 and batch: 600, loss is 5.410479478836059 and perplexity is 223.73884000446813
At time: 17.406195640563965 and batch: 650, loss is 5.299917449951172 and perplexity is 200.32027284392612
At time: 18.745962381362915 and batch: 700, loss is 5.396651229858398 and perplexity is 220.66621707854097
At time: 20.062427282333374 and batch: 750, loss is 5.36496717453003 and perplexity is 213.78421699836613
At time: 21.38277578353882 and batch: 800, loss is 5.336758985519409 and perplexity is 207.83801152049762
At time: 22.710267782211304 and batch: 850, loss is 5.371110219955444 and perplexity is 215.10154521549663
At time: 24.029947757720947 and batch: 900, loss is 5.268348112106323 and perplexity is 194.09507417938082
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.199784788366866 and perplexity of 181.2332341775411
finished 1 epochs...
Completing Train Step...
At time: 26.51173973083496 and batch: 50, loss is 5.140119161605835 and perplexity is 170.7361122985038
At time: 27.435617208480835 and batch: 100, loss is 5.002044010162353 and perplexity is 148.71682735277733
At time: 28.358380556106567 and batch: 150, loss is 4.9849874305725095 and perplexity is 146.20173734247138
At time: 29.288499355316162 and batch: 200, loss is 4.849538822174072 and perplexity is 127.68149239290003
At time: 30.199565172195435 and batch: 250, loss is 4.944507923126221 and perplexity is 140.40174539829414
At time: 31.11342453956604 and batch: 300, loss is 4.893072919845581 and perplexity is 133.36275810043034
At time: 32.025612592697144 and batch: 350, loss is 4.861461658477783 and perplexity is 129.21292931999233
At time: 32.967729330062866 and batch: 400, loss is 4.7310445022583005 and perplexity is 113.4139616427892
At time: 33.92236661911011 and batch: 450, loss is 4.7471216201782225 and perplexity is 115.25206740778663
At time: 34.9101402759552 and batch: 500, loss is 4.657223682403565 and perplexity is 105.34320958470732
At time: 35.87653875350952 and batch: 550, loss is 4.726008958816529 and perplexity is 112.84429620330307
At time: 36.862409353256226 and batch: 600, loss is 4.669730892181397 and perplexity is 106.66903309886126
At time: 37.8515408039093 and batch: 650, loss is 4.535689163208008 and perplexity is 93.28778366444371
At time: 38.83408284187317 and batch: 700, loss is 4.588514318466187 and perplexity is 98.34820745295639
At time: 39.825321435928345 and batch: 750, loss is 4.626895618438721 and perplexity is 102.196314802827
At time: 40.82035970687866 and batch: 800, loss is 4.556747608184814 and perplexity is 95.27310989850494
At time: 41.81126594543457 and batch: 850, loss is 4.6252659320831295 and perplexity is 102.02990249976187
At time: 42.79839086532593 and batch: 900, loss is 4.551461095809937 and perplexity is 94.77077639008895
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.666036109401755 and perplexity of 106.27564138823514
finished 2 epochs...
Completing Train Step...
At time: 45.17118453979492 and batch: 50, loss is 4.59520806312561 and perplexity is 99.00873347130813
At time: 46.141969442367554 and batch: 100, loss is 4.461504340171814 and perplexity is 86.61771364203263
At time: 47.11384701728821 and batch: 150, loss is 4.464910154342651 and perplexity is 86.91322041352925
At time: 48.08993458747864 and batch: 200, loss is 4.358768167495728 and perplexity is 78.16079409096932
At time: 49.069992780685425 and batch: 250, loss is 4.501083602905274 and perplexity is 90.11472699346197
At time: 50.04091954231262 and batch: 300, loss is 4.47133740901947 and perplexity is 87.47363284341503
At time: 51.0187783241272 and batch: 350, loss is 4.45503830909729 and perplexity is 86.05944764174176
At time: 51.99789786338806 and batch: 400, loss is 4.358702373504639 and perplexity is 78.15565174954882
At time: 52.98024773597717 and batch: 450, loss is 4.383292646408081 and perplexity is 80.10134507722822
At time: 53.96614336967468 and batch: 500, loss is 4.270797924995422 and perplexity is 71.5787272944015
At time: 54.94905138015747 and batch: 550, loss is 4.350144710540771 and perplexity is 77.48967568681174
At time: 55.92783570289612 and batch: 600, loss is 4.339696435928345 and perplexity is 76.68425721931364
At time: 56.90177321434021 and batch: 650, loss is 4.191177039146424 and perplexity is 66.10054812256115
At time: 57.88169026374817 and batch: 700, loss is 4.222934827804566 and perplexity is 68.23344413108057
At time: 58.86504054069519 and batch: 750, loss is 4.310148272514343 and perplexity is 74.45152723712738
At time: 59.840471506118774 and batch: 800, loss is 4.242741060256958 and perplexity is 69.598363926224
At time: 60.816665172576904 and batch: 850, loss is 4.323182039260864 and perplexity is 75.42826251978049
At time: 61.80168890953064 and batch: 900, loss is 4.2654796314239505 and perplexity is 71.19906109221935
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.498024718402183 and perplexity of 89.83949761360874
finished 3 epochs...
Completing Train Step...
At time: 64.1711585521698 and batch: 50, loss is 4.337710371017456 and perplexity is 76.5321084455152
At time: 65.1575379371643 and batch: 100, loss is 4.197531385421753 and perplexity is 66.52191122002976
At time: 66.13669538497925 and batch: 150, loss is 4.206296644210815 and perplexity is 67.10755590103646
At time: 67.1182804107666 and batch: 200, loss is 4.097190341949463 and perplexity is 60.17098996813788
At time: 68.10006475448608 and batch: 250, loss is 4.253844547271728 and perplexity is 70.37545468110443
At time: 69.09772181510925 and batch: 300, loss is 4.230825324058532 and perplexity is 68.77396956894327
At time: 70.10053730010986 and batch: 350, loss is 4.217293186187744 and perplexity is 67.84957932498497
At time: 71.07807445526123 and batch: 400, loss is 4.134951014518737 and perplexity is 62.48653006274611
At time: 72.06934189796448 and batch: 450, loss is 4.163075985908509 and perplexity is 64.26890912403992
At time: 73.05780148506165 and batch: 500, loss is 4.037959027290344 and perplexity is 56.71048006717649
At time: 74.05566024780273 and batch: 550, loss is 4.117704911231995 and perplexity is 61.41812034455295
At time: 75.07296228408813 and batch: 600, loss is 4.1309201049804685 and perplexity is 62.235159479105015
At time: 76.06419658660889 and batch: 650, loss is 3.9747106504440306 and perplexity is 53.234711236972814
At time: 77.04430389404297 and batch: 700, loss is 3.9933140325546264 and perplexity is 54.23432619193242
At time: 78.0362708568573 and batch: 750, loss is 4.098984346389771 and perplexity is 60.2790338779676
At time: 79.02070713043213 and batch: 800, loss is 4.036105122566223 and perplexity is 56.60544163597911
At time: 80.00604701042175 and batch: 850, loss is 4.122580876350403 and perplexity is 61.71832425410725
At time: 80.99088263511658 and batch: 900, loss is 4.072011489868164 and perplexity is 58.67486787173968
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.42909805088827 and perplexity of 83.85574917269446
finished 4 epochs...
Completing Train Step...
At time: 83.3900158405304 and batch: 50, loss is 4.150299639701843 and perplexity is 63.453010491021395
At time: 84.37284708023071 and batch: 100, loss is 4.0129874801635745 and perplexity is 55.31186708455562
At time: 85.34503054618835 and batch: 150, loss is 4.019486021995545 and perplexity is 55.672484039691405
At time: 86.31894040107727 and batch: 200, loss is 3.910166025161743 and perplexity is 49.90723714273432
At time: 87.29426407814026 and batch: 250, loss is 4.069278073310852 and perplexity is 58.51470401339504
At time: 88.27655577659607 and batch: 300, loss is 4.051762948036194 and perplexity is 57.49873502730571
At time: 89.2728328704834 and batch: 350, loss is 4.037640509605407 and perplexity is 56.69241965278722
At time: 90.25543427467346 and batch: 400, loss is 3.963721227645874 and perplexity is 52.65289525392202
At time: 91.24211478233337 and batch: 450, loss is 3.994951434135437 and perplexity is 54.32320230646146
At time: 92.21919178962708 and batch: 500, loss is 3.8665397787094116 and perplexity is 47.77678149527091
At time: 93.20788502693176 and batch: 550, loss is 3.944779357910156 and perplexity is 51.66493739465582
At time: 94.19334292411804 and batch: 600, loss is 3.968789019584656 and perplexity is 52.92040644504253
At time: 95.17173075675964 and batch: 650, loss is 3.8090380430221558 and perplexity is 45.1070269701344
At time: 96.1578598022461 and batch: 700, loss is 3.8206336879730225 and perplexity is 45.63311631753842
At time: 97.12906813621521 and batch: 750, loss is 3.9352672338485717 and perplexity is 51.175824041899325
At time: 98.10998678207397 and batch: 800, loss is 3.872293291091919 and perplexity is 48.05245809308559
At time: 99.09718418121338 and batch: 850, loss is 3.964754376411438 and perplexity is 52.70732163809157
At time: 100.0848376750946 and batch: 900, loss is 3.9173598766326903 and perplexity is 50.26755688349046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.39970377046768 and perplexity of 81.42674408561925
finished 5 epochs...
Completing Train Step...
At time: 102.44506478309631 and batch: 50, loss is 3.9961361694335937 and perplexity is 54.387599060764536
At time: 103.44813656806946 and batch: 100, loss is 3.863647656440735 and perplexity is 47.63880482046362
At time: 104.44698071479797 and batch: 150, loss is 3.8705984115600587 and perplexity is 47.97108394457714
At time: 105.43897247314453 and batch: 200, loss is 3.759341287612915 and perplexity is 42.920144634223604
At time: 106.43340468406677 and batch: 250, loss is 3.9205059480667113 and perplexity is 50.425951237590176
At time: 107.42481279373169 and batch: 300, loss is 3.9030309677124024 and perplexity is 49.5524134874752
At time: 108.41518950462341 and batch: 350, loss is 3.889193015098572 and perplexity is 48.87143209849564
At time: 109.41647052764893 and batch: 400, loss is 3.8231397390365602 and perplexity is 45.74761865164094
At time: 110.40661215782166 and batch: 450, loss is 3.8525326538085936 and perplexity is 47.11223123409932
At time: 111.39251327514648 and batch: 500, loss is 3.72613995552063 and perplexity is 41.518535061691736
At time: 112.37343215942383 and batch: 550, loss is 3.8008237218856813 and perplexity is 44.73802100672213
At time: 113.352294921875 and batch: 600, loss is 3.8326622581481935 and perplexity is 46.18533198375886
At time: 114.34902024269104 and batch: 650, loss is 3.6704224967956542 and perplexity is 39.26849316854935
At time: 115.3445360660553 and batch: 700, loss is 3.681155481338501 and perplexity is 39.69223121794545
At time: 116.33553767204285 and batch: 750, loss is 3.7971554660797118 and perplexity is 44.57421113336295
At time: 117.32864809036255 and batch: 800, loss is 3.738596272468567 and perplexity is 42.038937513797144
At time: 118.32059478759766 and batch: 850, loss is 3.8313753175735474 and perplexity is 46.12593243611766
At time: 119.31159210205078 and batch: 900, loss is 3.786008539199829 and perplexity is 44.08010466170015
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.397986843161387 and perplexity of 81.28706023306032
finished 6 epochs...
Completing Train Step...
At time: 121.64116644859314 and batch: 50, loss is 3.8648979473114013 and perplexity is 47.6984044338824
At time: 122.62908673286438 and batch: 100, loss is 3.733573637008667 and perplexity is 41.82832062423963
At time: 123.59316897392273 and batch: 150, loss is 3.742478275299072 and perplexity is 42.20244996076451
At time: 124.5714681148529 and batch: 200, loss is 3.6351013231277465 and perplexity is 37.90569342598898
At time: 125.5435688495636 and batch: 250, loss is 3.7959838628768923 and perplexity is 44.52201842537808
At time: 126.51746606826782 and batch: 300, loss is 3.779736614227295 and perplexity is 43.8045027332358
At time: 127.4921932220459 and batch: 350, loss is 3.7643441486358644 and perplexity is 43.135406165701596
At time: 128.46973943710327 and batch: 400, loss is 3.70135627746582 and perplexity is 40.50219934552511
At time: 129.46758675575256 and batch: 450, loss is 3.729721841812134 and perplexity is 41.66751639107853
At time: 130.44577622413635 and batch: 500, loss is 3.603799557685852 and perplexity is 36.73755605929352
At time: 131.43576216697693 and batch: 550, loss is 3.6781735801696778 and perplexity is 39.57404919840481
At time: 132.42035031318665 and batch: 600, loss is 3.7186843490600587 and perplexity is 41.210140267100314
At time: 133.41719031333923 and batch: 650, loss is 3.552515048980713 and perplexity is 34.900984883950905
At time: 134.40099048614502 and batch: 700, loss is 3.5622769451141356 and perplexity is 35.24335303569473
At time: 135.3763952255249 and batch: 750, loss is 3.6786569786071777 and perplexity is 39.593183856412146
At time: 136.35712838172913 and batch: 800, loss is 3.6222440099716184 and perplexity is 37.42144777508915
At time: 137.3381268978119 and batch: 850, loss is 3.7133420372009276 and perplexity is 40.99056987500648
At time: 138.32586908340454 and batch: 900, loss is 3.671562461853027 and perplexity is 39.313283403418026
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.410390775497645 and perplexity of 82.30161867706119
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 140.70951175689697 and batch: 50, loss is 3.786390156745911 and perplexity is 44.09692961321634
At time: 141.68732929229736 and batch: 100, loss is 3.659424982070923 and perplexity is 38.83900332586856
At time: 142.67785143852234 and batch: 150, loss is 3.6739179611206056 and perplexity is 39.40599496181829
At time: 143.66334199905396 and batch: 200, loss is 3.5536617851257324 and perplexity is 34.94103006105182
At time: 144.64612817764282 and batch: 250, loss is 3.7087386798858644 and perplexity is 40.80230928316087
At time: 145.6202118396759 and batch: 300, loss is 3.6745909357070925 and perplexity is 39.43252312036988
At time: 146.60125279426575 and batch: 350, loss is 3.6499605417251586 and perplexity is 38.47314793503601
At time: 147.5774049758911 and batch: 400, loss is 3.5793115186691282 and perplexity is 35.84885108411298
At time: 148.56303310394287 and batch: 450, loss is 3.5891197681427003 and perplexity is 36.20219557163657
At time: 149.5461037158966 and batch: 500, loss is 3.446685118675232 and perplexity is 31.39614512554064
At time: 150.5371537208557 and batch: 550, loss is 3.5047668027877807 and perplexity is 33.2736836173465
At time: 151.50847864151 and batch: 600, loss is 3.538332042694092 and perplexity is 34.409477759425684
At time: 152.48266196250916 and batch: 650, loss is 3.3509922885894774 and perplexity is 28.53103061819147
At time: 153.4656593799591 and batch: 700, loss is 3.3383099937438967 and perplexity is 28.171476480664897
At time: 154.44894361495972 and batch: 750, loss is 3.437283658981323 and perplexity is 31.102358707100592
At time: 155.43176531791687 and batch: 800, loss is 3.354008255004883 and perplexity is 28.617209138771045
At time: 156.40310311317444 and batch: 850, loss is 3.422760500907898 and perplexity is 30.653918501274397
At time: 157.3753809928894 and batch: 900, loss is 3.363453593254089 and perplexity is 28.88878892122219
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.375199827429366 and perplexity of 79.45571539735015
finished 8 epochs...
Completing Train Step...
At time: 159.68786478042603 and batch: 50, loss is 3.677609753608704 and perplexity is 39.55174258746226
At time: 160.64422821998596 and batch: 100, loss is 3.5416337060928345 and perplexity is 34.523274027826744
At time: 161.61913514137268 and batch: 150, loss is 3.554414029121399 and perplexity is 34.967324129655346
At time: 162.59043765068054 and batch: 200, loss is 3.4406264209747315 and perplexity is 31.206500453253717
At time: 163.5657958984375 and batch: 250, loss is 3.5956220388412476 and perplexity is 36.438359014369404
At time: 164.55521178245544 and batch: 300, loss is 3.5661715936660765 and perplexity is 35.380881147552806
At time: 165.54049515724182 and batch: 350, loss is 3.542889747619629 and perplexity is 34.56666393770646
At time: 166.51954984664917 and batch: 400, loss is 3.4785002183914187 and perplexity is 32.4110760699857
At time: 167.5308187007904 and batch: 450, loss is 3.4940446853637694 and perplexity is 32.91882509151395
At time: 168.542405128479 and batch: 500, loss is 3.358595495223999 and perplexity is 28.74878470513005
At time: 169.52783226966858 and batch: 550, loss is 3.419916968345642 and perplexity is 30.56687689732202
At time: 170.5309181213379 and batch: 600, loss is 3.460578451156616 and perplexity is 31.835386405630647
At time: 171.52519845962524 and batch: 650, loss is 3.2792665767669678 and perplexity is 26.556288556675128
At time: 172.5408329963684 and batch: 700, loss is 3.2733591985702515 and perplexity is 26.399872974540294
At time: 173.5425980091095 and batch: 750, loss is 3.3799447202682495 and perplexity is 29.36914754981648
At time: 174.5254807472229 and batch: 800, loss is 3.3030291843414306 and perplexity is 27.194892619843642
At time: 175.5137357711792 and batch: 850, loss is 3.3808084106445313 and perplexity is 29.3945243571895
At time: 176.49232292175293 and batch: 900, loss is 3.3306539964675905 and perplexity is 27.956619256096804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.391586512735445 and perplexity of 80.76845757275349
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 178.85110473632812 and batch: 50, loss is 3.6345094776153566 and perplexity is 37.883265748955445
At time: 179.82987523078918 and batch: 100, loss is 3.514473853111267 and perplexity is 33.59824566017567
At time: 180.79876565933228 and batch: 150, loss is 3.5333769130706787 and perplexity is 34.23939607326009
At time: 181.77296543121338 and batch: 200, loss is 3.416834020614624 and perplexity is 30.472785926843567
At time: 182.7479429244995 and batch: 250, loss is 3.5730954456329345 and perplexity is 35.626703166658125
At time: 183.73798155784607 and batch: 300, loss is 3.5445610332489013 and perplexity is 34.624483009030605
At time: 184.7199239730835 and batch: 350, loss is 3.512933487892151 and perplexity is 33.54653193037799
At time: 185.68731546401978 and batch: 400, loss is 3.4473695945739746 and perplexity is 31.41764238653172
At time: 186.6661696434021 and batch: 450, loss is 3.454005494117737 and perplexity is 31.626819978457124
At time: 187.63194251060486 and batch: 500, loss is 3.314775142669678 and perplexity is 27.516206068258246
At time: 188.60037660598755 and batch: 550, loss is 3.367189717292786 and perplexity is 28.99692289514587
At time: 189.56611824035645 and batch: 600, loss is 3.410194811820984 and perplexity is 30.271140861091986
At time: 190.534437417984 and batch: 650, loss is 3.221084780693054 and perplexity is 25.055284934129798
At time: 191.51899075508118 and batch: 700, loss is 3.2052995347976685 and perplexity is 24.662886302834764
At time: 192.49373126029968 and batch: 750, loss is 3.304061713218689 and perplexity is 27.222986633231663
At time: 193.46673154830933 and batch: 800, loss is 3.220144567489624 and perplexity is 25.03173869539588
At time: 194.4450397491455 and batch: 850, loss is 3.2930859899520875 and perplexity is 26.925828411785698
At time: 195.42204809188843 and batch: 900, loss is 3.241390438079834 and perplexity is 25.569249500235312
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.382669109187714 and perplexity of 80.05141447558314
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 197.77344417572021 and batch: 50, loss is 3.6169461011886597 and perplexity is 37.22371660160113
At time: 198.75740814208984 and batch: 100, loss is 3.494826111793518 and perplexity is 32.94455878466124
At time: 199.72715759277344 and batch: 150, loss is 3.5106893396377563 and perplexity is 33.471332949695814
At time: 200.70995211601257 and batch: 200, loss is 3.39594482421875 and perplexity is 29.84283639347027
At time: 201.6951127052307 and batch: 250, loss is 3.5520780420303346 and perplexity is 34.885736243105264
At time: 202.67600107192993 and batch: 300, loss is 3.5260192728042603 and perplexity is 33.988399417867775
At time: 203.656968832016 and batch: 350, loss is 3.4953813457489016 and perplexity is 32.96285580143605
At time: 204.63164949417114 and batch: 400, loss is 3.4301517724990847 and perplexity is 30.881329330548553
At time: 205.6117024421692 and batch: 450, loss is 3.436062412261963 and perplexity is 31.064398237832453
At time: 206.60204768180847 and batch: 500, loss is 3.299195713996887 and perplexity is 27.090841371573386
At time: 207.5960443019867 and batch: 550, loss is 3.3476309394836425 and perplexity is 28.435288864809195
At time: 208.584627866745 and batch: 600, loss is 3.3916501331329347 and perplexity is 29.71494545264981
At time: 209.573166847229 and batch: 650, loss is 3.2030165576934815 and perplexity is 24.606645720490217
At time: 210.55198693275452 and batch: 700, loss is 3.185994153022766 and perplexity is 24.19132633537617
At time: 211.52879977226257 and batch: 750, loss is 3.2820640993118286 and perplexity is 26.630684386106
At time: 212.5152940750122 and batch: 800, loss is 3.1970360326766967 and perplexity is 24.459924233253563
At time: 213.49629306793213 and batch: 850, loss is 3.2692543935775755 and perplexity is 26.291728751469066
At time: 214.50016260147095 and batch: 900, loss is 3.2181243801116945 and perplexity is 24.981220937684963
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.381321319162029 and perplexity of 79.94359465317316
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 216.84156584739685 and batch: 50, loss is 3.6110376834869387 and perplexity is 37.004431786574536
At time: 217.826238155365 and batch: 100, loss is 3.487744197845459 and perplexity is 32.71207245006558
At time: 218.7898986339569 and batch: 150, loss is 3.5037802553176878 and perplexity is 33.240873735867005
At time: 219.76781702041626 and batch: 200, loss is 3.3891148281097414 and perplexity is 29.63970442196399
At time: 220.73593592643738 and batch: 250, loss is 3.545316662788391 and perplexity is 34.65065617854645
At time: 221.71804213523865 and batch: 300, loss is 3.518094801902771 and perplexity is 33.72012371112153
At time: 222.69464302062988 and batch: 350, loss is 3.4884054327011107 and perplexity is 32.73370996552149
At time: 223.65928149223328 and batch: 400, loss is 3.4237876415252684 and perplexity is 30.685420561801962
At time: 224.6215534210205 and batch: 450, loss is 3.4300945520401003 and perplexity is 30.879562337264776
At time: 225.58747267723083 and batch: 500, loss is 3.293387794494629 and perplexity is 26.9339559755182
At time: 226.56008887290955 and batch: 550, loss is 3.341811361312866 and perplexity is 28.270288061667884
At time: 227.52467823028564 and batch: 600, loss is 3.3850228929519655 and perplexity is 29.518668477875924
At time: 228.49597263336182 and batch: 650, loss is 3.1975908851623536 and perplexity is 24.473499648841628
At time: 229.47369170188904 and batch: 700, loss is 3.1808247470855715 and perplexity is 24.066594222616978
At time: 230.45965242385864 and batch: 750, loss is 3.2771634578704836 and perplexity is 26.500496213920893
At time: 231.44049215316772 and batch: 800, loss is 3.1920534706115724 and perplexity is 24.338354259130952
At time: 232.42501282691956 and batch: 850, loss is 3.2626489496231077 and perplexity is 26.11863252880974
At time: 233.40132594108582 and batch: 900, loss is 3.2119479417800902 and perplexity is 24.827401484295933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380504451385916 and perplexity of 79.878317971633
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 235.72005343437195 and batch: 50, loss is 3.609587607383728 and perplexity is 36.95081143052214
At time: 236.69556665420532 and batch: 100, loss is 3.4860120248794555 and perplexity is 32.65545852921271
At time: 237.66506814956665 and batch: 150, loss is 3.502151565551758 and perplexity is 33.186778728952746
At time: 238.6462471485138 and batch: 200, loss is 3.3876095724105837 and perplexity is 29.59512264986251
At time: 239.6314697265625 and batch: 250, loss is 3.543847613334656 and perplexity is 34.599790022612964
At time: 240.62936687469482 and batch: 300, loss is 3.516725945472717 and perplexity is 33.67399728039052
At time: 241.60199642181396 and batch: 350, loss is 3.4865808868408203 and perplexity is 32.67404026212032
At time: 242.58959007263184 and batch: 400, loss is 3.4221961164474486 and perplexity is 30.636622787195623
At time: 243.57974004745483 and batch: 450, loss is 3.42880437374115 and perplexity is 30.83974788544981
At time: 244.55623841285706 and batch: 500, loss is 3.291953411102295 and perplexity is 26.895350050844822
At time: 245.54764080047607 and batch: 550, loss is 3.3404276990890502 and perplexity is 28.23119858157551
At time: 246.5353753566742 and batch: 600, loss is 3.383769979476929 and perplexity is 29.481707299789008
At time: 247.51706457138062 and batch: 650, loss is 3.196374983787537 and perplexity is 24.443760370650562
At time: 248.5031771659851 and batch: 700, loss is 3.1796457099914552 and perplexity is 24.038235536556055
At time: 249.4969687461853 and batch: 750, loss is 3.2760072231292723 and perplexity is 26.469873126689304
At time: 250.48109889030457 and batch: 800, loss is 3.1909812784194944 and perplexity is 24.31227285036629
At time: 251.45677709579468 and batch: 850, loss is 3.261300039291382 and perplexity is 26.083424587057852
At time: 252.4406967163086 and batch: 900, loss is 3.2105465793609618 and perplexity is 24.79263366373959
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.38025362197667 and perplexity of 79.85828465290234
Annealing...
Model not improving. Stopping early with 79.45571539735015 lossat 12 epochs.
Finished Training.
langmodel
SETTINGS FOR THIS RUN
{'batch_size': 32, 'lr': 9.720646266067527, 'dropout': 0.8263516623500462, 'hidden_size': 300, 'clip': 0.2575458035136192, 'tune_wordvecs': True, 'anneal': 4.259308432670263, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.21537363069409232}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.0831637978553772 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.5410890579223633 and batch: 50, loss is 6.890347414016723 and perplexity is 982.7427765337214
At time: 2.85392427444458 and batch: 100, loss is 6.068364515304565 and perplexity is 431.9736172957593
At time: 4.164733648300171 and batch: 150, loss is 5.9679625034332275 and perplexity is 390.7087913850283
At time: 5.46970009803772 and batch: 200, loss is 5.818860855102539 and perplexity is 336.58841218767344
At time: 6.774504661560059 and batch: 250, loss is 5.872287273406982 and perplexity is 355.06017189464677
At time: 8.077582597732544 and batch: 300, loss is 5.790112819671631 and perplexity is 327.0499199592347
At time: 9.385502099990845 and batch: 350, loss is 5.78434268951416 and perplexity is 325.16823336216953
At time: 10.690022945404053 and batch: 400, loss is 5.6493962955474855 and perplexity is 284.11988959606845
At time: 11.994716167449951 and batch: 450, loss is 5.66245964050293 and perplexity is 287.85579430067054
At time: 13.310253143310547 and batch: 500, loss is 5.619836111068725 and perplexity is 275.84417172354824
At time: 14.622189998626709 and batch: 550, loss is 5.677537460327148 and perplexity is 292.22891783812804
At time: 15.935855150222778 and batch: 600, loss is 5.60861026763916 and perplexity is 272.7649042652637
At time: 17.254596948623657 and batch: 650, loss is 5.518714027404785 and perplexity is 249.3142196985253
At time: 18.574190616607666 and batch: 700, loss is 5.625886373519897 and perplexity is 277.51816028672187
At time: 19.886727571487427 and batch: 750, loss is 5.580778512954712 and perplexity is 265.2780478137274
At time: 21.201510906219482 and batch: 800, loss is 5.577200860977173 and perplexity is 264.33067098416575
At time: 22.511714220046997 and batch: 850, loss is 5.616613664627075 and perplexity is 274.9567093219195
At time: 23.830899953842163 and batch: 900, loss is 5.519841718673706 and perplexity is 249.59552775180353
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.4500188958154965 and perplexity of 232.76256410457398
finished 1 epochs...
Completing Train Step...
At time: 26.323419332504272 and batch: 50, loss is 5.358351469039917 and perplexity is 212.37455168700416
At time: 27.250600814819336 and batch: 100, loss is 5.158630628585815 and perplexity is 173.9261229984104
At time: 28.17005467414856 and batch: 150, loss is 5.080017948150635 and perplexity is 160.77694155147705
At time: 29.08842158317566 and batch: 200, loss is 4.911703491210938 and perplexity is 135.87067187537446
At time: 29.99788498878479 and batch: 250, loss is 4.981002464294433 and perplexity is 145.62028764752827
At time: 30.9048113822937 and batch: 300, loss is 4.917039766311645 and perplexity is 136.59765311945966
At time: 31.831754446029663 and batch: 350, loss is 4.873858556747437 and perplexity is 130.82473893755554
At time: 32.77875328063965 and batch: 400, loss is 4.73564193725586 and perplexity is 113.93657538008617
At time: 33.76397204399109 and batch: 450, loss is 4.737311916351318 and perplexity is 114.1270060425352
At time: 34.75890111923218 and batch: 500, loss is 4.635026245117188 and perplexity is 103.03062201049413
At time: 35.768452167510986 and batch: 550, loss is 4.68723690032959 and perplexity is 108.55282277147566
At time: 36.77382731437683 and batch: 600, loss is 4.641854667663575 and perplexity is 103.73656613215383
At time: 37.78008270263672 and batch: 650, loss is 4.498666496276855 and perplexity is 89.89717312090146
At time: 38.76760935783386 and batch: 700, loss is 4.529103326797485 and perplexity is 92.67542424494292
At time: 39.76210308074951 and batch: 750, loss is 4.586619424819946 and perplexity is 98.16202451367833
At time: 40.75333094596863 and batch: 800, loss is 4.511721906661987 and perplexity is 91.07851226283486
At time: 41.74315404891968 and batch: 850, loss is 4.578939018249511 and perplexity is 97.41098808009735
At time: 42.74304389953613 and batch: 900, loss is 4.514857339859009 and perplexity is 91.36453101574611
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.611571429526969 and perplexity of 100.64217752844883
finished 2 epochs...
Completing Train Step...
At time: 45.14521265029907 and batch: 50, loss is 4.555848131179809 and perplexity is 95.18745245618094
At time: 46.11715292930603 and batch: 100, loss is 4.4056146478652956 and perplexity is 81.90947285660422
At time: 47.09938025474548 and batch: 150, loss is 4.4009671401977535 and perplexity is 81.52968117942818
At time: 48.077576875686646 and batch: 200, loss is 4.276564745903015 and perplexity is 71.99270150580291
At time: 49.0426881313324 and batch: 250, loss is 4.424846935272217 and perplexity is 83.50002533519032
At time: 50.00740718841553 and batch: 300, loss is 4.395566277503967 and perplexity is 81.09053751069831
At time: 50.98120355606079 and batch: 350, loss is 4.377037363052368 and perplexity is 79.60185232963362
At time: 51.94573783874512 and batch: 400, loss is 4.285922741889953 and perplexity is 72.66957104925257
At time: 52.90564203262329 and batch: 450, loss is 4.305398931503296 and perplexity is 74.09876989102341
At time: 53.89380502700806 and batch: 500, loss is 4.179761428833007 and perplexity is 65.3502606661673
At time: 54.883514404296875 and batch: 550, loss is 4.2585111284255985 and perplexity is 70.70463492740204
At time: 55.881245851516724 and batch: 600, loss is 4.26095534324646 and perplexity is 70.87766361749743
At time: 56.872947454452515 and batch: 650, loss is 4.106491746902466 and perplexity is 60.73327568192323
At time: 57.8680157661438 and batch: 700, loss is 4.113926820755005 and perplexity is 61.18651491758756
At time: 58.85864591598511 and batch: 750, loss is 4.225586915016175 and perplexity is 68.41464535017504
At time: 59.86384701728821 and batch: 800, loss is 4.164857330322266 and perplexity is 64.38349621552344
At time: 60.86117076873779 and batch: 850, loss is 4.24152250289917 and perplexity is 69.51360597946947
At time: 61.85874271392822 and batch: 900, loss is 4.178106498718262 and perplexity is 65.24219999287861
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.431820386076627 and perplexity of 84.08434364362112
finished 3 epochs...
Completing Train Step...
At time: 64.227459192276 and batch: 50, loss is 4.260059604644775 and perplexity is 70.81420418397319
At time: 65.22616577148438 and batch: 100, loss is 4.112853074073792 and perplexity is 61.12085135958237
At time: 66.21043086051941 and batch: 150, loss is 4.114329624176025 and perplexity is 61.21116601955749
At time: 67.19276905059814 and batch: 200, loss is 3.9940867710113523 and perplexity is 54.27625133795565
At time: 68.18461227416992 and batch: 250, loss is 4.148172931671143 and perplexity is 63.31820785776746
At time: 69.19123005867004 and batch: 300, loss is 4.127230243682861 and perplexity is 62.00594352040606
At time: 70.17114353179932 and batch: 350, loss is 4.11546552658081 and perplexity is 61.280735434791545
At time: 71.1651542186737 and batch: 400, loss is 4.036544523239136 and perplexity is 56.63031957041478
At time: 72.15557813644409 and batch: 450, loss is 4.063708882331849 and perplexity is 58.189730212191094
At time: 73.14637684822083 and batch: 500, loss is 3.9365838527679444 and perplexity is 51.24324747579192
At time: 74.14656186103821 and batch: 550, loss is 4.019119420051575 and perplexity is 55.65207813946671
At time: 75.15756058692932 and batch: 600, loss is 4.033003849983215 and perplexity is 56.4301646631025
At time: 76.1569037437439 and batch: 650, loss is 3.880196976661682 and perplexity is 48.43375445106124
At time: 77.15790796279907 and batch: 700, loss is 3.8821630668640137 and perplexity is 48.52907325312638
At time: 78.15198874473572 and batch: 750, loss is 3.999170017242432 and perplexity is 54.55285331037135
At time: 79.1616940498352 and batch: 800, loss is 3.9449062013626097 and perplexity is 51.67149116932867
At time: 80.16511797904968 and batch: 850, loss is 4.023189663887024 and perplexity is 55.879057284314996
At time: 81.16376638412476 and batch: 900, loss is 3.9688310956954957 and perplexity is 52.922633176775584
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.361881621896404 and perplexity of 78.40452338242527
finished 4 epochs...
Completing Train Step...
At time: 83.57543540000916 and batch: 50, loss is 4.060992550849915 and perplexity is 58.03188209704903
At time: 84.57979369163513 and batch: 100, loss is 3.925798411369324 and perplexity is 50.69353620122328
At time: 85.57566905021667 and batch: 150, loss is 3.922424621582031 and perplexity is 50.52279505083502
At time: 86.55821132659912 and batch: 200, loss is 3.806383981704712 and perplexity is 44.98746888213735
At time: 87.54144310951233 and batch: 250, loss is 3.9594708681106567 and perplexity is 52.4295764473303
At time: 88.52530479431152 and batch: 300, loss is 3.942244277000427 and perplexity is 51.53412847381839
At time: 89.50644659996033 and batch: 350, loss is 3.9283043193817138 and perplexity is 50.820728839732645
At time: 90.49344611167908 and batch: 400, loss is 3.860445833206177 and perplexity is 47.48651771661636
At time: 91.47553515434265 and batch: 450, loss is 3.8872447919845583 and perplexity is 48.77631233221621
At time: 92.48421883583069 and batch: 500, loss is 3.7666078662872313 and perplexity is 43.233163151399175
At time: 93.4687385559082 and batch: 550, loss is 3.8463818502426146 and perplexity is 46.82334251388714
At time: 94.48340106010437 and batch: 600, loss is 3.8621020841598512 and perplexity is 47.56523247457367
At time: 95.47077465057373 and batch: 650, loss is 3.7144953918457033 and perplexity is 41.03787381302839
At time: 96.45673108100891 and batch: 700, loss is 3.713976788520813 and perplexity is 41.01659695282461
At time: 97.44846701622009 and batch: 750, loss is 3.830506992340088 and perplexity is 46.085897509250636
At time: 98.43925833702087 and batch: 800, loss is 3.7855979919433596 and perplexity is 44.06201140998795
At time: 99.42711162567139 and batch: 850, loss is 3.858461232185364 and perplexity is 47.39236937942554
At time: 100.41126418113708 and batch: 900, loss is 3.8093655252456666 and perplexity is 45.12180113862928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.338816603569136 and perplexity of 76.61681760050006
finished 5 epochs...
Completing Train Step...
At time: 102.7898473739624 and batch: 50, loss is 3.902102336883545 and perplexity is 49.50641894794542
At time: 103.76125431060791 and batch: 100, loss is 3.7726439094543456 and perplexity is 43.49490955201718
At time: 104.73802971839905 and batch: 150, loss is 3.772839527130127 and perplexity is 43.503418757380416
At time: 105.72784662246704 and batch: 200, loss is 3.6620081186294557 and perplexity is 38.93945946536123
At time: 106.7130057811737 and batch: 250, loss is 3.8103527927398684 and perplexity is 45.166370423456236
At time: 107.69447255134583 and batch: 300, loss is 3.792316060066223 and perplexity is 44.35901954766398
At time: 108.6785888671875 and batch: 350, loss is 3.7857249641418456 and perplexity is 44.067606415643965
At time: 109.66332507133484 and batch: 400, loss is 3.7189791440963744 and perplexity is 41.222290602738255
At time: 110.64694929122925 and batch: 450, loss is 3.745159001350403 and perplexity is 42.315734942964255
At time: 111.62882924079895 and batch: 500, loss is 3.6291877174377443 and perplexity is 37.68219559201611
At time: 112.61874890327454 and batch: 550, loss is 3.7057287645339967 and perplexity is 40.67968242686557
At time: 113.60650491714478 and batch: 600, loss is 3.728363561630249 and perplexity is 41.610958648661885
At time: 114.59892845153809 and batch: 650, loss is 3.5812461280822756 and perplexity is 35.9182717381444
At time: 115.5714807510376 and batch: 700, loss is 3.581387996673584 and perplexity is 35.923367774233306
At time: 116.55225586891174 and batch: 750, loss is 3.698307957649231 and perplexity is 40.378923675937
At time: 117.5330171585083 and batch: 800, loss is 3.653468203544617 and perplexity is 38.60833568496755
At time: 118.52935314178467 and batch: 850, loss is 3.7258721828460692 and perplexity is 41.507419020866834
At time: 119.52471876144409 and batch: 900, loss is 3.6799642610549927 and perplexity is 39.64497717759442
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3386514742080475 and perplexity of 76.60416695888601
finished 6 epochs...
Completing Train Step...
At time: 121.88560509681702 and batch: 50, loss is 3.778916392326355 and perplexity is 43.76858805175189
At time: 122.86855030059814 and batch: 100, loss is 3.644295139312744 and perplexity is 38.25579833772975
At time: 123.84181714057922 and batch: 150, loss is 3.64313973903656 and perplexity is 38.21162310271838
At time: 124.83459973335266 and batch: 200, loss is 3.539932689666748 and perplexity is 34.464599289118034
At time: 125.81658101081848 and batch: 250, loss is 3.689691061973572 and perplexity is 40.03247749209658
At time: 126.8164324760437 and batch: 300, loss is 3.6714252710342405 and perplexity is 39.30789035182576
At time: 127.80368423461914 and batch: 350, loss is 3.6647268772125243 and perplexity is 39.04547049890673
At time: 128.79922556877136 and batch: 400, loss is 3.6025148916244505 and perplexity is 36.69039087009088
At time: 129.7886025905609 and batch: 450, loss is 3.6254596519470215 and perplexity is 37.541975436380305
At time: 130.77283883094788 and batch: 500, loss is 3.5097551250457766 and perplexity is 33.440078143662284
At time: 131.75989937782288 and batch: 550, loss is 3.5927615785598754 and perplexity is 36.33427746722451
At time: 132.75417971611023 and batch: 600, loss is 3.61555371761322 and perplexity is 37.171922976652425
At time: 133.75023818016052 and batch: 650, loss is 3.4646949768066406 and perplexity is 31.96670769863827
At time: 134.74645495414734 and batch: 700, loss is 3.4640085601806643 and perplexity is 31.94477274811482
At time: 135.74139213562012 and batch: 750, loss is 3.586550784111023 and perplexity is 36.109312068503
At time: 136.72853875160217 and batch: 800, loss is 3.5430371809005736 and perplexity is 34.57176059008096
At time: 137.72757077217102 and batch: 850, loss is 3.609473104476929 and perplexity is 36.94658069742499
At time: 138.72800493240356 and batch: 900, loss is 3.5639180612564085 and perplexity is 35.30123895703756
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3591371301102315 and perplexity of 78.18963782271092
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 141.08444952964783 and batch: 50, loss is 3.6837357091903686 and perplexity is 39.79477845916773
At time: 142.07093787193298 and batch: 100, loss is 3.551477861404419 and perplexity is 34.864804782048374
At time: 143.05711102485657 and batch: 150, loss is 3.5448813962936403 and perplexity is 34.63557719081793
At time: 144.03673553466797 and batch: 200, loss is 3.4185586738586426 and perplexity is 30.52538626157756
At time: 145.00171613693237 and batch: 250, loss is 3.543560461997986 and perplexity is 34.58985607300119
At time: 145.98702716827393 and batch: 300, loss is 3.5093509578704833 and perplexity is 33.42656549260632
At time: 146.95913457870483 and batch: 350, loss is 3.4782622766494753 and perplexity is 32.40336503951174
At time: 147.94495630264282 and batch: 400, loss is 3.3950932264328 and perplexity is 29.81743311829282
At time: 148.9283905029297 and batch: 450, loss is 3.391673655509949 and perplexity is 29.715644427020443
At time: 149.91436958312988 and batch: 500, loss is 3.2730881929397584 and perplexity is 26.39271942968914
At time: 150.90622973442078 and batch: 550, loss is 3.3196335554122927 and perplexity is 27.65021642967223
At time: 151.89316487312317 and batch: 600, loss is 3.325406107902527 and perplexity is 27.810290328366996
At time: 152.88037395477295 and batch: 650, loss is 3.1477602338790893 and perplexity is 23.283855743717044
At time: 153.86323356628418 and batch: 700, loss is 3.115248432159424 and perplexity is 22.539029077679366
At time: 154.84866166114807 and batch: 750, loss is 3.209317808151245 and perplexity is 24.76218789855118
At time: 155.84442400932312 and batch: 800, loss is 3.1307750368118286 and perplexity is 22.891714590388965
At time: 156.82588005065918 and batch: 850, loss is 3.1532930517196656 and perplexity is 23.413038117900715
At time: 157.80413365364075 and batch: 900, loss is 3.0820013236999513 and perplexity is 21.801991606096106
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.380014079890839 and perplexity of 79.83915752379338
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 160.1655433177948 and batch: 50, loss is 3.522431664466858 and perplexity is 33.86668082261748
At time: 161.1456983089447 and batch: 100, loss is 3.4000918006896974 and perplexity is 29.966850898710618
At time: 162.13008999824524 and batch: 150, loss is 3.394842848777771 and perplexity is 29.809968433843757
At time: 163.1276457309723 and batch: 200, loss is 3.276915202140808 and perplexity is 26.493918130453782
At time: 164.11418414115906 and batch: 250, loss is 3.3942244625091553 and perplexity is 29.79154005721119
At time: 165.09020566940308 and batch: 300, loss is 3.359602189064026 and perplexity is 28.777740501966726
At time: 166.07065510749817 and batch: 350, loss is 3.324547839164734 and perplexity is 27.786431865540163
At time: 167.05791401863098 and batch: 400, loss is 3.2409467124938964 and perplexity is 25.557906286841952
At time: 168.05542922019958 and batch: 450, loss is 3.2382730436325073 and perplexity is 25.48966417769407
At time: 169.03215336799622 and batch: 500, loss is 3.113523063659668 and perplexity is 22.500174475785585
At time: 170.02009677886963 and batch: 550, loss is 3.1514413261413576 and perplexity is 23.36972371192531
At time: 171.01654863357544 and batch: 600, loss is 3.1563891839981078 and perplexity is 23.485640315961135
At time: 172.00389337539673 and batch: 650, loss is 2.9774551010131836 and perplexity is 19.637776840261346
At time: 172.9761257171631 and batch: 700, loss is 2.939045076370239 and perplexity is 18.89779174537278
At time: 173.95927047729492 and batch: 750, loss is 3.0246226930618287 and perplexity is 20.586235916036085
At time: 174.94170498847961 and batch: 800, loss is 2.941679039001465 and perplexity is 18.947633434400164
At time: 175.9206519126892 and batch: 850, loss is 2.9576065349578857 and perplexity is 19.251837966551598
At time: 176.90270328521729 and batch: 900, loss is 2.895848636627197 and perplexity is 18.098854276496525
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.393218576091609 and perplexity of 80.90038443991094
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 179.229398727417 and batch: 50, loss is 3.475541338920593 and perplexity is 32.315317341506486
At time: 180.20774841308594 and batch: 100, loss is 3.3524289894104005 and perplexity is 28.572050632991058
At time: 181.16710925102234 and batch: 150, loss is 3.345600891113281 and perplexity is 28.377622405643244
At time: 182.14676332473755 and batch: 200, loss is 3.2291599464416505 and perplexity is 25.258429622473724
At time: 183.1053454875946 and batch: 250, loss is 3.3483108615875246 and perplexity is 28.454629220461225
At time: 184.06883335113525 and batch: 300, loss is 3.3099807119369506 and perplexity is 27.384597270970502
At time: 185.04662895202637 and batch: 350, loss is 3.2749107122421264 and perplexity is 26.440864529652263
At time: 186.02363801002502 and batch: 400, loss is 3.1930177211761475 and perplexity is 24.361833849250925
At time: 187.0069305896759 and batch: 450, loss is 3.1881374645233156 and perplexity is 24.243231487909426
At time: 187.9915280342102 and batch: 500, loss is 3.0613341522216797 and perplexity is 21.356030359363928
At time: 188.9786458015442 and batch: 550, loss is 3.1002974939346313 and perplexity is 22.204556019695175
At time: 189.97177124023438 and batch: 600, loss is 3.106665320396423 and perplexity is 22.34640192483251
At time: 190.96062564849854 and batch: 650, loss is 2.9262279224395753 and perplexity is 18.657121488244247
At time: 191.96923542022705 and batch: 700, loss is 2.8844131088256835 and perplexity is 17.893063232469977
At time: 192.95313358306885 and batch: 750, loss is 2.96487108707428 and perplexity is 19.392203174681942
At time: 193.9356017112732 and batch: 800, loss is 2.883059148788452 and perplexity is 17.868853133361416
At time: 194.92451333999634 and batch: 850, loss is 2.8943120765686037 and perplexity is 18.07106565481948
At time: 195.9167652130127 and batch: 900, loss is 2.837736892700195 and perplexity is 17.0770745206914
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.3889268848994005 and perplexity of 80.5539289443323
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 198.3051097393036 and batch: 50, loss is 3.4581312704086304 and perplexity is 31.757574709389143
At time: 199.290385723114 and batch: 100, loss is 3.3347750663757325 and perplexity is 28.072068161560498
At time: 200.2691695690155 and batch: 150, loss is 3.3286838912963868 and perplexity is 27.901595994533942
At time: 201.24821400642395 and batch: 200, loss is 3.2140299558639525 and perplexity is 24.879146331910803
At time: 202.23186683654785 and batch: 250, loss is 3.332928099632263 and perplexity is 28.020267836709817
At time: 203.22650170326233 and batch: 300, loss is 3.2946633005142214 and perplexity is 26.96833231745133
At time: 204.1989085674286 and batch: 350, loss is 3.2599732446670533 and perplexity is 26.048840187802654
At time: 205.17779278755188 and batch: 400, loss is 3.1785656642913818 and perplexity is 24.012287158866712
At time: 206.1524043083191 and batch: 450, loss is 3.1746709299087525 and perplexity is 23.918947562918962
At time: 207.13767170906067 and batch: 500, loss is 3.045869197845459 and perplexity is 21.02830101562477
At time: 208.1238877773285 and batch: 550, loss is 3.0851819276809693 and perplexity is 21.87144550150968
At time: 209.11095809936523 and batch: 600, loss is 3.0917797660827637 and perplexity is 22.016226861338406
At time: 210.10839581489563 and batch: 650, loss is 2.911821813583374 and perplexity is 18.390271713653064
At time: 211.09943914413452 and batch: 700, loss is 2.869227604866028 and perplexity is 17.62340071251947
At time: 212.08956480026245 and batch: 750, loss is 2.9493949174880982 and perplexity is 19.09439654663434
At time: 213.07962489128113 and batch: 800, loss is 2.8670392560958864 and perplexity is 17.584876732570446
At time: 214.06435823440552 and batch: 850, loss is 2.8779255390167235 and perplexity is 17.77735646933213
At time: 215.06161975860596 and batch: 900, loss is 2.8214050912857056 and perplexity is 16.800440247693366
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.385595034246576 and perplexity of 80.28598191121851
Annealing...
Model not improving. Stopping early with 76.60416695888601 lossat 10 epochs.
Finished Training.
Improved accuracyfrom -78.52828833517174 to -76.60416695888601
langmodel
SETTINGS FOR THIS RUN
{'batch_size': 32, 'lr': 1.8158991810007974, 'dropout': 0.9936987775753, 'hidden_size': 300, 'clip': 0.19122660562142446, 'tune_wordvecs': True, 'anneal': 4.952290843617265, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.5999394469912211}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.08737071752548217 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 3 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.586374044418335 and batch: 50, loss is 9.074467144012452 and perplexity is 8729.532915406711
At time: 2.907283306121826 and batch: 100, loss is 8.732880725860596 and perplexity is 6203.573189186218
At time: 4.225153207778931 and batch: 150, loss is 8.402812976837158 and perplexity is 4459.593854424544
At time: 5.539526700973511 and batch: 200, loss is 8.079349555969237 and perplexity is 3227.1334841217426
At time: 6.852134466171265 and batch: 250, loss is 7.923373031616211 and perplexity is 2761.0685280036196
At time: 8.164122581481934 and batch: 300, loss is 7.704072484970093 and perplexity is 2217.3597936513643
At time: 9.47743558883667 and batch: 350, loss is 7.592252597808838 and perplexity is 1982.774881379351
At time: 10.78743314743042 and batch: 400, loss is 7.455019931793213 and perplexity is 1728.518447396346
At time: 12.101807117462158 and batch: 450, loss is 7.368981466293335 and perplexity is 1586.0175477573625
At time: 13.415235042572021 and batch: 500, loss is 7.300733022689819 and perplexity is 1481.3854188154482
At time: 14.74673318862915 and batch: 550, loss is 7.249620113372803 and perplexity is 1407.570029594822
At time: 16.065346479415894 and batch: 600, loss is 7.176169261932373 and perplexity is 1307.8884714842106
At time: 17.37530827522278 and batch: 650, loss is 7.06293062210083 and perplexity is 1167.8627198445854
At time: 18.686877250671387 and batch: 700, loss is 7.125611343383789 and perplexity is 1243.4080840517488
At time: 20.001344680786133 and batch: 750, loss is 7.032400007247925 and perplexity is 1132.7459489541861
At time: 21.305803060531616 and batch: 800, loss is 7.035573415756225 and perplexity is 1136.3463242957403
At time: 22.61998987197876 and batch: 850, loss is 7.04709864616394 and perplexity is 1149.5187392565722
At time: 23.93799090385437 and batch: 900, loss is 6.907109422683716 and perplexity is 999.354352221864
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 6.966090790212971 and perplexity of 1060.070601644388
finished 1 epochs...
Completing Train Step...
At time: 26.448622703552246 and batch: 50, loss is 6.532471599578858 and perplexity is 687.0943366303297
At time: 27.385917901992798 and batch: 100, loss is 6.037876710891724 and perplexity is 419.0024264232823
At time: 28.32251238822937 and batch: 150, loss is 5.96765097618103 and perplexity is 390.58709390586364
At time: 29.244142770767212 and batch: 200, loss is 5.8106693649292 and perplexity is 333.84251337260923
At time: 30.17333149909973 and batch: 250, loss is 5.855413513183594 and perplexity is 349.1192356161733
At time: 31.104123830795288 and batch: 300, loss is 5.750469369888306 and perplexity is 314.3381665356071
At time: 32.01629090309143 and batch: 350, loss is 5.722615079879761 and perplexity is 305.70331716349176
At time: 32.95677876472473 and batch: 400, loss is 5.582199611663818 and perplexity is 265.6553020993295
At time: 33.91248345375061 and batch: 450, loss is 5.571422395706176 and perplexity is 262.80764998169695
At time: 34.8898606300354 and batch: 500, loss is 5.509414787292481 and perplexity is 247.00653340053836
At time: 35.89737677574158 and batch: 550, loss is 5.55244065284729 and perplexity is 257.8661502572895
At time: 36.89506983757019 and batch: 600, loss is 5.469272804260254 and perplexity is 237.28757549151004
At time: 37.892539739608765 and batch: 650, loss is 5.354483156204224 and perplexity is 211.55460740569234
At time: 38.87495517730713 and batch: 700, loss is 5.452023639678955 and perplexity is 233.2296614754608
At time: 39.87945747375488 and batch: 750, loss is 5.404918947219849 and perplexity is 222.49818565641314
At time: 40.88022327423096 and batch: 800, loss is 5.380413274765015 and perplexity is 217.1119837906518
At time: 41.89170575141907 and batch: 850, loss is 5.409421434402466 and perplexity is 223.50223955918688
At time: 42.895525217056274 and batch: 900, loss is 5.289827566146851 and perplexity is 198.30922723683713
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.36542751364512 and perplexity of 213.88265289084688
finished 2 epochs...
Completing Train Step...
At time: 45.31323266029358 and batch: 50, loss is 5.322523174285888 and perplexity is 204.9002292916362
At time: 46.2930691242218 and batch: 100, loss is 5.248483047485352 and perplexity is 190.27740762795887
At time: 47.25053119659424 and batch: 150, loss is 5.279031629562378 and perplexity is 196.17980861202693
At time: 48.21215581893921 and batch: 200, loss is 5.174278573989868 and perplexity is 176.66911459671252
At time: 49.19110584259033 and batch: 250, loss is 5.28711311340332 and perplexity is 197.77165614663693
At time: 50.16479468345642 and batch: 300, loss is 5.225241966247559 and perplexity is 185.90614809726074
At time: 51.138861656188965 and batch: 350, loss is 5.212084093093872 and perplexity is 183.47604116170137
At time: 52.13451313972473 and batch: 400, loss is 5.094248037338257 and perplexity is 163.0811675570448
At time: 53.105971813201904 and batch: 450, loss is 5.120485401153564 and perplexity is 167.41661411904155
At time: 54.076595306396484 and batch: 500, loss is 5.062833480834961 and perplexity is 158.0376792457513
At time: 55.04753804206848 and batch: 550, loss is 5.137840938568115 and perplexity is 170.34758010364317
At time: 56.01940155029297 and batch: 600, loss is 5.062977952957153 and perplexity is 158.06051293403428
At time: 56.99205279350281 and batch: 650, loss is 4.956793661117554 and perplexity is 142.13732405758
At time: 57.96669602394104 and batch: 700, loss is 5.058257389068603 and perplexity is 157.31613650500674
At time: 58.94453310966492 and batch: 750, loss is 5.045448246002198 and perplexity is 155.31390243336025
At time: 59.919678926467896 and batch: 800, loss is 5.011387119293213 and perplexity is 150.1128161832541
At time: 60.91523814201355 and batch: 850, loss is 5.056324510574341 and perplexity is 157.01235720676917
At time: 61.89453315734863 and batch: 900, loss is 4.969621772766113 and perplexity is 143.97242278034898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.103817456389127 and perplexity of 164.6492504435721
finished 3 epochs...
Completing Train Step...
At time: 64.2372076511383 and batch: 50, loss is 5.029833307266236 and perplexity is 152.9075219982617
At time: 65.20987010002136 and batch: 100, loss is 4.942897729873657 and perplexity is 140.17585336892267
At time: 66.18480730056763 and batch: 150, loss is 4.974809761047363 and perplexity is 144.72129090241543
At time: 67.14701008796692 and batch: 200, loss is 4.879264516830444 and perplexity is 131.53388734106719
At time: 68.10771131515503 and batch: 250, loss is 5.0074582767486575 and perplexity is 149.52420360476282
At time: 69.07429718971252 and batch: 300, loss is 4.9657494163513185 and perplexity is 143.41598829708008
At time: 70.05303621292114 and batch: 350, loss is 4.9545587539672855 and perplexity is 141.82001504588808
At time: 71.04489421844482 and batch: 400, loss is 4.846653308868408 and perplexity is 127.31359678681048
At time: 72.05514430999756 and batch: 450, loss is 4.878647871017456 and perplexity is 131.45280252304843
At time: 73.04501843452454 and batch: 500, loss is 4.813909120559693 and perplexity is 123.21232917438229
At time: 74.04833483695984 and batch: 550, loss is 4.901775178909301 and perplexity is 134.5283797858428
At time: 75.04349565505981 and batch: 600, loss is 4.840573902130127 and perplexity is 126.54195359165728
At time: 76.02868294715881 and batch: 650, loss is 4.730105810165405 and perplexity is 113.30755080509073
At time: 77.06032395362854 and batch: 700, loss is 4.824316740036011 and perplexity is 124.50137251190579
At time: 78.05886483192444 and batch: 750, loss is 4.834602355957031 and perplexity is 125.78855419141422
At time: 79.03951382637024 and batch: 800, loss is 4.794346694946289 and perplexity is 120.82542013524989
At time: 80.02124094963074 and batch: 850, loss is 4.850490379333496 and perplexity is 127.80304645475948
At time: 81.00862240791321 and batch: 900, loss is 4.777592697143555 and perplexity is 118.81797463720228
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.928372317797517 and perplexity of 138.1544576789571
finished 4 epochs...
Completing Train Step...
At time: 83.40873265266418 and batch: 50, loss is 4.854437103271485 and perplexity is 128.30844647870114
At time: 84.37622427940369 and batch: 100, loss is 4.756310653686524 and perplexity is 116.31600330198475
At time: 85.34717655181885 and batch: 150, loss is 4.782309522628784 and perplexity is 119.37974212614145
At time: 86.32384419441223 and batch: 200, loss is 4.693011589050293 and perplexity is 109.18149497885884
At time: 87.29761934280396 and batch: 250, loss is 4.832649269104004 and perplexity is 125.5431179771024
At time: 88.27236747741699 and batch: 300, loss is 4.797341003417968 and perplexity is 121.18775090868941
At time: 89.246901512146 and batch: 350, loss is 4.788144979476929 and perplexity is 120.07841401205259
At time: 90.22225451469421 and batch: 400, loss is 4.683293008804322 and perplexity is 108.1255453354653
At time: 91.21190690994263 and batch: 450, loss is 4.71622712135315 and perplexity is 111.74585279525223
At time: 92.19146752357483 and batch: 500, loss is 4.64221158027649 and perplexity is 103.77359762913795
At time: 93.16822481155396 and batch: 550, loss is 4.739824790954589 and perplexity is 114.41415352927996
At time: 94.14204454421997 and batch: 600, loss is 4.690976448059082 and perplexity is 108.95952119353463
At time: 95.12295079231262 and batch: 650, loss is 4.574725084304809 and perplexity is 97.00136827238863
At time: 96.09740853309631 and batch: 700, loss is 4.659215602874756 and perplexity is 105.55325400682196
At time: 97.07177567481995 and batch: 750, loss is 4.686256828308106 and perplexity is 108.446485304724
At time: 98.03874850273132 and batch: 800, loss is 4.644265623092651 and perplexity is 103.98697210699147
At time: 99.00723600387573 and batch: 850, loss is 4.70499397277832 and perplexity is 110.49761895182036
At time: 99.9784631729126 and batch: 900, loss is 4.639894361495972 and perplexity is 103.53340989103266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.819719288447132 and perplexity of 123.9302972303938
finished 5 epochs...
Completing Train Step...
At time: 102.30728244781494 and batch: 50, loss is 4.726229991912842 and perplexity is 112.86924128423684
At time: 103.28883719444275 and batch: 100, loss is 4.619518184661866 and perplexity is 101.44514252696248
At time: 104.26003646850586 and batch: 150, loss is 4.641702108383178 and perplexity is 103.7207413634123
At time: 105.21847581863403 and batch: 200, loss is 4.555566377639771 and perplexity is 95.16063683236003
At time: 106.17870497703552 and batch: 250, loss is 4.700583219528198 and perplexity is 110.01131449279497
At time: 107.13629150390625 and batch: 300, loss is 4.669918489456177 and perplexity is 106.68904579587907
At time: 108.1037847995758 and batch: 350, loss is 4.662371740341187 and perplexity is 105.886920858746
At time: 109.06757116317749 and batch: 400, loss is 4.561697635650635 and perplexity is 95.74588356521933
At time: 110.03171491622925 and batch: 450, loss is 4.59394832611084 and perplexity is 98.88408703231913
At time: 110.9987485408783 and batch: 500, loss is 4.512472696304322 and perplexity is 91.14691874270602
At time: 111.96341133117676 and batch: 550, loss is 4.6158979129791256 and perplexity is 101.0785475372447
At time: 112.92877554893494 and batch: 600, loss is 4.575309858322144 and perplexity is 97.05810874075912
At time: 113.89774775505066 and batch: 650, loss is 4.454546804428101 and perplexity is 86.01715941467982
At time: 114.88514351844788 and batch: 700, loss is 4.530214405059814 and perplexity is 92.77845111913491
At time: 115.8632071018219 and batch: 750, loss is 4.5708705425262455 and perplexity is 96.62819211937949
At time: 116.83417296409607 and batch: 800, loss is 4.526254453659058 and perplexity is 92.41177944183143
At time: 117.80386590957642 and batch: 850, loss is 4.591897773742676 and perplexity is 98.68152778363097
At time: 118.77263307571411 and batch: 900, loss is 4.531664094924927 and perplexity is 92.91304863818027
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.744709171660959 and perplexity of 114.97436283733131
finished 6 epochs...
Completing Train Step...
At time: 121.10084128379822 and batch: 50, loss is 4.623295955657959 and perplexity is 101.82910384641991
At time: 122.05747246742249 and batch: 100, loss is 4.5099703884124756 and perplexity is 90.91912621100468
At time: 123.01125812530518 and batch: 150, loss is 4.531179056167603 and perplexity is 92.86799313624529
At time: 123.96594524383545 and batch: 200, loss is 4.4432347393035885 and perplexity is 85.04961051116094
At time: 124.94099044799805 and batch: 250, loss is 4.593749504089356 and perplexity is 98.86442865256693
At time: 125.91867399215698 and batch: 300, loss is 4.566646919250489 and perplexity is 96.22093170076761
At time: 126.88477849960327 and batch: 350, loss is 4.558730039596558 and perplexity is 95.46216964133107
At time: 127.86203503608704 and batch: 400, loss is 4.463718709945678 and perplexity is 86.80972980793818
At time: 128.83196997642517 and batch: 450, loss is 4.4952098083496095 and perplexity is 89.58696310637056
At time: 129.7939293384552 and batch: 500, loss is 4.4076916599273686 and perplexity is 82.0797766199869
At time: 130.7566397190094 and batch: 550, loss is 4.513237543106079 and perplexity is 91.21665883884143
At time: 131.71999168395996 and batch: 600, loss is 4.480528173446655 and perplexity is 88.28128819628947
At time: 132.68473601341248 and batch: 650, loss is 4.354202337265015 and perplexity is 77.8047386373805
At time: 133.6488859653473 and batch: 700, loss is 4.423663349151611 and perplexity is 83.40125432766068
At time: 134.61205744743347 and batch: 750, loss is 4.4743032550811765 and perplexity is 87.73345127316803
At time: 135.57831525802612 and batch: 800, loss is 4.428313217163086 and perplexity is 83.78996217199841
At time: 136.54977178573608 and batch: 850, loss is 4.497162418365479 and perplexity is 89.76206240250986
At time: 137.51916646957397 and batch: 900, loss is 4.441348733901978 and perplexity is 84.88935765271218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.689325202001284 and perplexity of 108.77975067722949
finished 7 epochs...
Completing Train Step...
At time: 139.8623731136322 and batch: 50, loss is 4.535867223739624 and perplexity is 93.30439601575446
At time: 140.80825757980347 and batch: 100, loss is 4.418461871147156 and perplexity is 82.96857081016503
At time: 141.75971841812134 and batch: 150, loss is 4.4380051708221435 and perplexity is 84.60599870882746
At time: 142.70998096466064 and batch: 200, loss is 4.348544826507569 and perplexity is 77.3658003114828
At time: 143.67213773727417 and batch: 250, loss is 4.503373470306396 and perplexity is 90.32131420755901
At time: 144.6287202835083 and batch: 300, loss is 4.479075212478637 and perplexity is 88.1531120703373
At time: 145.5873579978943 and batch: 350, loss is 4.469368934631348 and perplexity is 87.30161260179334
At time: 146.5462999343872 and batch: 400, loss is 4.379696478843689 and perplexity is 79.81380455003888
At time: 147.50666427612305 and batch: 450, loss is 4.411106052398682 and perplexity is 82.36050818245953
At time: 148.4671504497528 and batch: 500, loss is 4.317725009918213 and perplexity is 75.01776933330498
At time: 149.45825910568237 and batch: 550, loss is 4.424651393890381 and perplexity is 83.4836992211204
At time: 150.41903734207153 and batch: 600, loss is 4.398717699050903 and perplexity is 81.346491074818
At time: 151.38340735435486 and batch: 650, loss is 4.267921705245971 and perplexity is 71.37314693418719
At time: 152.34935855865479 and batch: 700, loss is 4.33206983089447 and perplexity is 76.1016411923387
At time: 153.31453394889832 and batch: 750, loss is 4.390809211730957 and perplexity is 80.70570056325428
At time: 154.27902054786682 and batch: 800, loss is 4.343849496841431 and perplexity is 77.00339384884788
At time: 155.24747824668884 and batch: 850, loss is 4.414911479949951 and perplexity is 82.67452222934266
At time: 156.21536350250244 and batch: 900, loss is 4.362691421508789 and perplexity is 78.46804104988969
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.649792396858947 and perplexity of 104.56327565920297
finished 8 epochs...
Completing Train Step...
At time: 158.4918406009674 and batch: 50, loss is 4.458623914718628 and perplexity is 86.36857675732765
At time: 159.50990915298462 and batch: 100, loss is 4.338264288902283 and perplexity is 76.5745126922978
At time: 160.45134258270264 and batch: 150, loss is 4.35664843082428 and perplexity is 77.9952892647616
At time: 161.40415406227112 and batch: 200, loss is 4.265743479728699 and perplexity is 71.21784932230055
At time: 162.35913825035095 and batch: 250, loss is 4.424379053115845 and perplexity is 83.46096630150441
At time: 163.31360149383545 and batch: 300, loss is 4.401959781646728 and perplexity is 81.61065110069002
At time: 164.27614736557007 and batch: 350, loss is 4.390312385559082 and perplexity is 80.66561381789164
At time: 165.2440128326416 and batch: 400, loss is 4.305433831214905 and perplexity is 74.10135596184946
At time: 166.21262860298157 and batch: 450, loss is 4.336044521331787 and perplexity is 76.40472358818823
At time: 167.16868257522583 and batch: 500, loss is 4.238957347869873 and perplexity is 69.33552130853138
At time: 168.12325167655945 and batch: 550, loss is 4.346300830841065 and perplexity is 77.1923864336125
At time: 169.07796716690063 and batch: 600, loss is 4.326236982345581 and perplexity is 75.65904390126623
At time: 170.05442094802856 and batch: 650, loss is 4.191650185585022 and perplexity is 66.13183076154576
At time: 171.02020931243896 and batch: 700, loss is 4.2510546207427975 and perplexity is 70.17938596893299
At time: 171.99076676368713 and batch: 750, loss is 4.316561260223389 and perplexity is 74.93051820622031
At time: 172.9913854598999 and batch: 800, loss is 4.269567584991455 and perplexity is 71.49071527623376
At time: 173.96730828285217 and batch: 850, loss is 4.342478919029236 and perplexity is 76.8979269975479
At time: 174.94178438186646 and batch: 900, loss is 4.292464356422425 and perplexity is 73.1465056316182
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.618751421366652 and perplexity of 101.36738792869062
finished 9 epochs...
Completing Train Step...
At time: 177.2913932800293 and batch: 50, loss is 4.388777122497559 and perplexity is 80.54186589777377
At time: 178.25830459594727 and batch: 100, loss is 4.267317171096802 and perplexity is 71.33001246897406
At time: 179.2244713306427 and batch: 150, loss is 4.284539351463318 and perplexity is 72.56911016468987
At time: 180.1916401386261 and batch: 200, loss is 4.191832962036133 and perplexity is 66.14391920758582
At time: 181.16119050979614 and batch: 250, loss is 4.353032760620117 and perplexity is 77.71379322638924
At time: 182.12592840194702 and batch: 300, loss is 4.332904453277588 and perplexity is 76.16518383883873
At time: 183.08471703529358 and batch: 350, loss is 4.319714317321777 and perplexity is 75.1671512712732
At time: 184.049409866333 and batch: 400, loss is 4.238790612220765 and perplexity is 69.32396156911682
At time: 185.00637793540955 and batch: 450, loss is 4.268528971672058 and perplexity is 71.41650261293617
At time: 185.9632008075714 and batch: 500, loss is 4.168238654136657 and perplexity is 64.6015661392715
At time: 186.9196047782898 and batch: 550, loss is 4.275683722496033 and perplexity is 71.92930218288902
At time: 187.87732934951782 and batch: 600, loss is 4.260973262786865 and perplexity is 70.87893372403428
At time: 188.8345718383789 and batch: 650, loss is 4.123335618972778 and perplexity is 61.76492328692945
At time: 189.79179406166077 and batch: 700, loss is 4.177766871452332 and perplexity is 65.2200457251814
At time: 190.75076150894165 and batch: 750, loss is 4.249521226882934 and perplexity is 70.07185579352424
At time: 191.7091782093048 and batch: 800, loss is 4.202399768829346 and perplexity is 66.84655499283066
At time: 192.666011095047 and batch: 850, loss is 4.277465033531189 and perplexity is 72.05754482872835
At time: 193.6221902370453 and batch: 900, loss is 4.228440880775452 and perplexity is 68.61017729345373
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.597611936804366 and perplexity of 99.24702425538959
finished 10 epochs...
Completing Train Step...
At time: 195.92040276527405 and batch: 50, loss is 4.325152711868286 and perplexity is 75.57705349154672
At time: 196.86739110946655 and batch: 100, loss is 4.2032259702682495 and perplexity is 66.90180653406
At time: 197.82160592079163 and batch: 150, loss is 4.218962926864624 and perplexity is 67.96296516362291
At time: 198.77170515060425 and batch: 200, loss is 4.125310959815979 and perplexity is 61.88705064444256
At time: 199.7179455757141 and batch: 250, loss is 4.2881175231933595 and perplexity is 72.82924002025952
At time: 200.6646146774292 and batch: 300, loss is 4.26922393321991 and perplexity is 71.46615158619959
At time: 201.61888694763184 and batch: 350, loss is 4.2556549215316775 and perplexity is 70.5029759886089
At time: 202.574369430542 and batch: 400, loss is 4.177562260627747 and perplexity is 65.20670236299082
At time: 203.5298731327057 and batch: 450, loss is 4.2074477481842045 and perplexity is 67.18484815245282
At time: 204.48574376106262 and batch: 500, loss is 4.103505544662475 and perplexity is 60.55218436026709
At time: 205.44708347320557 and batch: 550, loss is 4.211094965934754 and perplexity is 67.43033332000842
At time: 206.41039419174194 and batch: 600, loss is 4.201280479431152 and perplexity is 66.77177620987787
At time: 207.37512278556824 and batch: 650, loss is 4.06153085231781 and perplexity is 58.06312915378034
At time: 208.33919286727905 and batch: 700, loss is 4.110618152618408 and perplexity is 60.984403589327115
At time: 209.302250623703 and batch: 750, loss is 4.188451800346375 and perplexity is 65.92065358324089
At time: 210.2651870250702 and batch: 800, loss is 4.1404215574264525 and perplexity is 62.82930202613553
At time: 211.22883248329163 and batch: 850, loss is 4.218244886398315 and perplexity is 67.91418252047698
At time: 212.19222712516785 and batch: 900, loss is 4.169414973258972 and perplexity is 64.67760290983834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.584514461151541 and perplexity of 97.95561433763541
finished 11 epochs...
Completing Train Step...
At time: 214.5190145969391 and batch: 50, loss is 4.26711404800415 and perplexity is 71.31552516764502
At time: 215.4808452129364 and batch: 100, loss is 4.144431681632995 and perplexity is 63.08176118905267
At time: 216.4286811351776 and batch: 150, loss is 4.158580274581909 and perplexity is 63.98062317210802
At time: 217.37713074684143 and batch: 200, loss is 4.0652890968322755 and perplexity is 58.28175515807582
At time: 218.32541036605835 and batch: 250, loss is 4.228376741409302 and perplexity is 68.60577682129397
At time: 219.27699780464172 and batch: 300, loss is 4.210550441741943 and perplexity is 67.39362586714145
At time: 220.22366786003113 and batch: 350, loss is 4.196233162879944 and perplexity is 66.4356070085174
At time: 221.20217967033386 and batch: 400, loss is 4.120276455879211 and perplexity is 61.57626303150947
At time: 222.15637731552124 and batch: 450, loss is 4.151302270889282 and perplexity is 63.51666236262011
At time: 223.11238622665405 and batch: 500, loss is 4.044336156845093 and perplexity is 57.07328574554295
At time: 224.07439756393433 and batch: 550, loss is 4.1506343889236454 and perplexity is 63.47425489247933
At time: 225.03775072097778 and batch: 600, loss is 4.146901149749755 and perplexity is 63.23773209037934
At time: 226.0033142566681 and batch: 650, loss is 4.004938912391663 and perplexity is 54.86847251293284
At time: 226.965882062912 and batch: 700, loss is 4.049070143699646 and perplexity is 57.34411046455915
At time: 227.92986965179443 and batch: 750, loss is 4.132369961738586 and perplexity is 62.32545698907812
At time: 228.8925905227661 and batch: 800, loss is 4.082833294868469 and perplexity is 59.31328402736357
At time: 229.85645294189453 and batch: 850, loss is 4.163698086738586 and perplexity is 64.3089033046708
At time: 230.82021951675415 and batch: 900, loss is 4.114792342185974 and perplexity is 61.239496082394396
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.578743294493793 and perplexity of 97.39192430134015
finished 12 epochs...
Completing Train Step...
At time: 233.1096329689026 and batch: 50, loss is 4.21321581363678 and perplexity is 67.57349454539343
At time: 234.05761528015137 and batch: 100, loss is 4.089198060035706 and perplexity is 59.69200310198346
At time: 235.00547075271606 and batch: 150, loss is 4.10280855178833 and perplexity is 60.50999462393483
At time: 235.95174193382263 and batch: 200, loss is 4.009953064918518 and perplexity is 55.144282301341065
At time: 236.89873361587524 and batch: 250, loss is 4.173374772071838 and perplexity is 64.93422094764622
At time: 237.84615993499756 and batch: 300, loss is 4.155597419738769 and perplexity is 63.79006260895284
At time: 238.79464173316956 and batch: 350, loss is 4.141109619140625 and perplexity is 62.87254733940396
At time: 239.7429871559143 and batch: 400, loss is 4.066888961791992 and perplexity is 58.37507272379139
At time: 240.68982219696045 and batch: 450, loss is 4.0990658330917356 and perplexity is 60.283946017770255
At time: 241.64518022537231 and batch: 500, loss is 3.9905035877227784 and perplexity is 54.08211759729284
At time: 242.60121202468872 and batch: 550, loss is 4.094908542633057 and perplexity is 60.03384836866862
At time: 243.55734539031982 and batch: 600, loss is 4.0961034297943115 and perplexity is 60.105624917223274
At time: 244.51151967048645 and batch: 650, loss is 3.951561269760132 and perplexity is 52.01651528337771
At time: 245.55143785476685 and batch: 700, loss is 3.991597170829773 and perplexity is 54.141293238330974
At time: 246.50695300102234 and batch: 750, loss is 4.079816942214966 and perplexity is 59.13464380210313
At time: 247.46350359916687 and batch: 800, loss is 4.029722332954407 and perplexity is 56.2452916147484
At time: 248.41846227645874 and batch: 850, loss is 4.112648129463196 and perplexity is 61.10832625402146
At time: 249.37312483787537 and batch: 900, loss is 4.063238773345947 and perplexity is 58.16238112616041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.576429393193493 and perplexity of 97.16682952504071
finished 13 epochs...
Completing Train Step...
At time: 251.66022109985352 and batch: 50, loss is 4.163053140640259 and perplexity is 64.26744090034192
At time: 252.60229468345642 and batch: 100, loss is 4.037712697982788 and perplexity is 56.69651233429198
At time: 253.5432834625244 and batch: 150, loss is 4.050108065605164 and perplexity is 57.40366007153752
At time: 254.48543548583984 and batch: 200, loss is 3.9580992364883425 and perplexity is 52.35771167959162
At time: 255.429616689682 and batch: 250, loss is 4.121500511169433 and perplexity is 61.65168193106002
At time: 256.38318276405334 and batch: 300, loss is 4.103771004676819 and perplexity is 60.56826067771123
At time: 257.34076619148254 and batch: 350, loss is 4.0898230838775635 and perplexity is 59.72932368900383
At time: 258.2984707355499 and batch: 400, loss is 4.017111716270446 and perplexity is 55.54045734000314
At time: 259.2546031475067 and batch: 450, loss is 4.050205135345459 and perplexity is 57.40923250036541
At time: 260.20873403549194 and batch: 500, loss is 3.9393680763244627 and perplexity is 51.38611893327148
At time: 261.1631832122803 and batch: 550, loss is 4.04300253868103 and perplexity is 56.997222505922366
At time: 262.1189727783203 and batch: 600, loss is 4.047989339828491 and perplexity is 57.28216620880674
At time: 263.0763990879059 and batch: 650, loss is 3.9020537996292113 and perplexity is 49.50401610061208
At time: 264.032351732254 and batch: 700, loss is 3.9382450580596924 and perplexity is 51.32844377434372
At time: 264.98851132392883 and batch: 750, loss is 4.03071291923523 and perplexity is 56.30103503375768
At time: 265.94544768333435 and batch: 800, loss is 3.9802535152435303 and perplexity is 53.530603331371026
At time: 266.90249466896057 and batch: 850, loss is 4.064044370651245 and perplexity is 58.2092554620496
At time: 267.8618071079254 and batch: 900, loss is 4.014702124595642 and perplexity is 55.40678862456863
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.578383354291524 and perplexity of 97.35687534055114
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 270.146297454834 and batch: 50, loss is 4.135481486320495 and perplexity is 62.51968619835409
At time: 271.0964033603668 and batch: 100, loss is 4.009585480690003 and perplexity is 55.12401585791498
At time: 272.0462839603424 and batch: 150, loss is 4.021053881645202 and perplexity is 55.75983914339002
At time: 272.99314856529236 and batch: 200, loss is 3.9193209648132323 and perplexity is 50.36623271954278
At time: 273.9414851665497 and batch: 250, loss is 4.080528440475464 and perplexity is 59.17673296971273
At time: 274.89786314964294 and batch: 300, loss is 4.055303525924683 and perplexity is 57.702674596079795
At time: 275.85458731651306 and batch: 350, loss is 4.037333707809449 and perplexity is 56.67502898450105
At time: 276.810626745224 and batch: 400, loss is 3.9615194463729857 and perplexity is 52.53709262799785
At time: 277.77022194862366 and batch: 450, loss is 3.987477593421936 and perplexity is 53.918712773385494
At time: 278.7318263053894 and batch: 500, loss is 3.86686203956604 and perplexity is 47.792180562927626
At time: 279.6918215751648 and batch: 550, loss is 3.9660100984573363 and perplexity is 52.7735489567023
At time: 280.6487226486206 and batch: 600, loss is 3.9720497131347656 and perplexity is 53.09324530715155
At time: 281.6049802303314 and batch: 650, loss is 3.813303532600403 and perplexity is 45.29984145532452
At time: 282.55985474586487 and batch: 700, loss is 3.842086811065674 and perplexity is 46.62266568930283
At time: 283.515985250473 and batch: 750, loss is 3.931614737510681 and perplexity is 50.989245478182575
At time: 284.4761424064636 and batch: 800, loss is 3.872343072891235 and perplexity is 48.05485029045448
At time: 285.43250465393066 and batch: 850, loss is 3.9477312755584717 and perplexity is 51.81767335623104
At time: 286.38741064071655 and batch: 900, loss is 3.8987671947479248 and perplexity is 49.34158303252347
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.539470515839041 and perplexity of 93.6412054557363
finished 15 epochs...
Completing Train Step...
At time: 288.66887855529785 and batch: 50, loss is 4.0930939340591435 and perplexity is 59.92500921279203
At time: 289.61018776893616 and batch: 100, loss is 3.9620035600662233 and perplexity is 52.56253271139146
At time: 290.5508596897125 and batch: 150, loss is 3.9769511461257934 and perplexity is 53.35411709182056
At time: 291.49385356903076 and batch: 200, loss is 3.878285875320435 and perplexity is 48.34128102915362
At time: 292.4408657550812 and batch: 250, loss is 4.041588253974915 and perplexity is 56.91666918193518
At time: 293.38205647468567 and batch: 300, loss is 4.018200407028198 and perplexity is 55.60095664913429
At time: 294.32803297042847 and batch: 350, loss is 4.003229794502258 and perplexity is 54.774775917068716
At time: 295.2744789123535 and batch: 400, loss is 3.9294145202636717 and perplexity is 50.87718138874875
At time: 296.2216935157776 and batch: 450, loss is 3.9584139919281007 and perplexity is 52.37419414799328
At time: 297.1695954799652 and batch: 500, loss is 3.8391798543930054 and perplexity is 46.48733241945727
At time: 298.12407422065735 and batch: 550, loss is 3.939593696594238 and perplexity is 51.39771399127879
At time: 299.07973289489746 and batch: 600, loss is 3.949705891609192 and perplexity is 51.920094453643436
At time: 300.03744673728943 and batch: 650, loss is 3.7929622983932494 and perplexity is 44.38769531093566
At time: 300.9929852485657 and batch: 700, loss is 3.8232538080215455 and perplexity is 45.75283733370525
At time: 301.9508261680603 and batch: 750, loss is 3.916277108192444 and perplexity is 50.21315821522401
At time: 302.90451097488403 and batch: 800, loss is 3.859351487159729 and perplexity is 47.43457945809096
At time: 303.8589882850647 and batch: 850, loss is 3.9377634143829345 and perplexity is 51.30372770660866
At time: 304.81379532814026 and batch: 900, loss is 3.892592668533325 and perplexity is 49.03786077009956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5401895601455475 and perplexity of 93.7085618445855
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 307.1129050254822 and batch: 50, loss is 4.087811121940613 and perplexity is 59.60927137406333
At time: 308.0630519390106 and batch: 100, loss is 3.96346875667572 and perplexity is 52.63960360432427
At time: 309.02344131469727 and batch: 150, loss is 3.977273106575012 and perplexity is 53.37129777292861
At time: 309.9781873226166 and batch: 200, loss is 3.8751115083694456 and perplexity is 48.18807136478652
At time: 310.92638421058655 and batch: 250, loss is 4.043868985176086 and perplexity is 57.04662895052004
At time: 311.87605381011963 and batch: 300, loss is 4.0154057264328005 and perplexity is 55.44578666077203
At time: 312.8238935470581 and batch: 350, loss is 3.997597169876099 and perplexity is 54.46711744110906
At time: 313.77260732650757 and batch: 400, loss is 3.9228717708587646 and perplexity is 50.54539133367983
At time: 314.72248220443726 and batch: 450, loss is 3.9495935583114625 and perplexity is 51.914262425786625
At time: 315.6854794025421 and batch: 500, loss is 3.82599730014801 and perplexity is 45.87853222532696
At time: 316.6633903980255 and batch: 550, loss is 3.9222364902496336 and perplexity is 50.513291024116306
At time: 317.6356451511383 and batch: 600, loss is 3.9341360569000243 and perplexity is 51.117967858379984
At time: 318.60434198379517 and batch: 650, loss is 3.7748960494995116 and perplexity is 43.592976568456805
At time: 319.5691430568695 and batch: 700, loss is 3.801262674331665 and perplexity is 44.75766318114637
At time: 320.531667470932 and batch: 750, loss is 3.893676815032959 and perplexity is 49.091053824524735
At time: 321.496985912323 and batch: 800, loss is 3.8334996366500853 and perplexity is 46.224022785026555
At time: 322.46128273010254 and batch: 850, loss is 3.911146082878113 and perplexity is 49.956173091708976
At time: 323.42519330978394 and batch: 900, loss is 3.866862416267395 and perplexity is 47.79219856631019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.529218699834118 and perplexity of 92.6861171068814
finished 17 epochs...
Completing Train Step...
At time: 325.7243137359619 and batch: 50, loss is 4.073448781967163 and perplexity is 58.759261430310616
At time: 326.68607664108276 and batch: 100, loss is 3.944910798072815 and perplexity is 51.671728688745354
At time: 327.6362006664276 and batch: 150, loss is 3.960391812324524 and perplexity is 52.477883402986386
At time: 328.58574080467224 and batch: 200, loss is 3.86041100025177 and perplexity is 47.484863649717994
At time: 329.5332224369049 and batch: 250, loss is 4.029864358901977 and perplexity is 56.25328047288552
At time: 330.4795973300934 and batch: 300, loss is 4.0023762893676755 and perplexity is 54.72804530982134
At time: 331.4280915260315 and batch: 350, loss is 3.985478734970093 and perplexity is 53.81104454127223
At time: 332.3778007030487 and batch: 400, loss is 3.9116100215911866 and perplexity is 49.979355071456354
At time: 333.32681107521057 and batch: 450, loss is 3.940065789222717 and perplexity is 51.421984201617676
At time: 334.2755615711212 and batch: 500, loss is 3.8174865198135377 and perplexity is 45.48972698037592
At time: 335.2227394580841 and batch: 550, loss is 3.9148656511306763 and perplexity is 50.14233449254722
At time: 336.1807680130005 and batch: 600, loss is 3.928261523246765 and perplexity is 50.818553955501685
At time: 337.1297845840454 and batch: 650, loss is 3.7698352098464967 and perplexity is 43.37291681728605
At time: 338.07898712158203 and batch: 700, loss is 3.797309331893921 and perplexity is 44.581070108319075
At time: 339.03331112861633 and batch: 750, loss is 3.8911867713928223 and perplexity is 48.968967021975885
At time: 339.9928894042969 and batch: 800, loss is 3.832695960998535 and perplexity is 46.186888587321526
At time: 340.94621419906616 and batch: 850, loss is 3.911931800842285 and perplexity is 49.99543997865754
At time: 341.8949694633484 and batch: 900, loss is 3.8692118310928345 and perplexity is 47.9046142700138
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.528946967974101 and perplexity of 92.6609347574605
finished 18 epochs...
Completing Train Step...
At time: 344.22117471694946 and batch: 50, loss is 4.067801427841187 and perplexity is 58.42836230454033
At time: 345.1670913696289 and batch: 100, loss is 3.9384423065185548 and perplexity is 51.33856922935638
At time: 346.10912322998047 and batch: 150, loss is 3.9534060382843017 and perplexity is 52.11256227850751
At time: 347.0498638153076 and batch: 200, loss is 3.8535200452804563 and perplexity is 47.15877242284548
At time: 347.9968144893646 and batch: 250, loss is 4.022881231307983 and perplexity is 55.86182502019488
At time: 348.9473776817322 and batch: 300, loss is 3.9956362676620483 and perplexity is 54.360417398290636
At time: 349.90231561660767 and batch: 350, loss is 3.9790656471252444 and perplexity is 53.46705378603303
At time: 350.850554227829 and batch: 400, loss is 3.9055714797973633 and perplexity is 49.678462038918006
At time: 351.81159949302673 and batch: 450, loss is 3.9346470165252687 and perplexity is 51.14409375014968
At time: 352.7712574005127 and batch: 500, loss is 3.8124406242370608 and perplexity is 45.26076870380118
At time: 353.72946763038635 and batch: 550, loss is 3.9101740074157716 and perplexity is 49.90763551656901
At time: 354.6978142261505 and batch: 600, loss is 3.9243612623214723 and perplexity is 50.62073436001825
At time: 355.66459345817566 and batch: 650, loss is 3.7663236379623415 and perplexity is 43.220876808003084
At time: 356.6446006298065 and batch: 700, loss is 3.7942563009262087 and perplexity is 44.445170279457784
At time: 357.627019405365 and batch: 750, loss is 3.8889211225509643 and perplexity is 48.858146126577374
At time: 358.5958607196808 and batch: 800, loss is 3.8310949850082396 and perplexity is 46.113003647415546
At time: 359.5511691570282 and batch: 850, loss is 3.910958561897278 and perplexity is 49.94680613940955
At time: 360.50765681266785 and batch: 900, loss is 3.8688617753982544 and perplexity is 47.88784792174097
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.529180657373716 and perplexity of 92.68259116600969
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 362.78622794151306 and batch: 50, loss is 4.067788248062134 and perplexity is 58.427592236709394
At time: 363.756174325943 and batch: 100, loss is 3.940420141220093 and perplexity is 51.44020891321933
At time: 364.72451090812683 and batch: 150, loss is 3.9582840633392333 and perplexity is 52.36738968491128
At time: 365.6745615005493 and batch: 200, loss is 3.854106135368347 and perplexity is 47.18641981305887
At time: 366.62404322624207 and batch: 250, loss is 4.025654807090759 and perplexity is 56.0169770888008
At time: 367.56767988204956 and batch: 300, loss is 3.9987278985977173 and perplexity is 54.5287398077078
At time: 368.5136251449585 and batch: 350, loss is 3.981511535644531 and perplexity is 53.59798829938498
At time: 369.4681296348572 and batch: 400, loss is 3.9077575397491455 and perplexity is 49.78718092495279
At time: 370.42431592941284 and batch: 450, loss is 3.9351806116104124 and perplexity is 51.171391269472274
At time: 371.37906551361084 and batch: 500, loss is 3.8127821636199952 and perplexity is 45.27622967893108
At time: 372.33400988578796 and batch: 550, loss is 3.907446823120117 and perplexity is 49.77171362302536
At time: 373.29012537002563 and batch: 600, loss is 3.9213474082946775 and perplexity is 50.46840052720373
At time: 374.24652647972107 and batch: 650, loss is 3.7617438411712647 and perplexity is 43.02338655289736
At time: 375.202152967453 and batch: 700, loss is 3.789196457862854 and perplexity is 44.22085267743256
At time: 376.1567175388336 and batch: 750, loss is 3.882660665512085 and perplexity is 48.55322726337233
At time: 377.1120855808258 and batch: 800, loss is 3.82396164894104 and perplexity is 45.78523452883054
At time: 378.0719037055969 and batch: 850, loss is 3.902613220214844 and perplexity is 49.531717413910265
At time: 379.0290231704712 and batch: 900, loss is 3.8603625679016114 and perplexity is 47.48256390186601
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.527814473191353 and perplexity of 92.55605613074118
finished 20 epochs...
Completing Train Step...
At time: 381.2880470752716 and batch: 50, loss is 4.064745154380798 and perplexity is 58.25006185774591
At time: 382.2453022003174 and batch: 100, loss is 3.9355176496505737 and perplexity is 51.18864088162251
At time: 383.18732810020447 and batch: 150, loss is 3.9540514755249023 and perplexity is 52.14620852400705
At time: 384.12851452827454 and batch: 200, loss is 3.8510368347167967 and perplexity is 47.0418125391064
At time: 385.0694987773895 and batch: 250, loss is 4.022215828895569 and perplexity is 55.82466679102326
At time: 386.0106611251831 and batch: 300, loss is 3.9950986433029176 and perplexity is 54.33119976848331
At time: 386.95165514945984 and batch: 350, loss is 3.9775623226165773 and perplexity is 53.38673584076451
At time: 387.90811467170715 and batch: 400, loss is 3.904136052131653 and perplexity is 49.60720335569549
At time: 388.85072469711304 and batch: 450, loss is 3.932376461029053 and perplexity is 51.0280999819633
At time: 389.792400598526 and batch: 500, loss is 3.810204253196716 and perplexity is 45.15966192967832
At time: 390.74019742012024 and batch: 550, loss is 3.905239586830139 and perplexity is 49.66197684255646
At time: 391.69766068458557 and batch: 600, loss is 3.919815492630005 and perplexity is 50.39114638239055
At time: 392.65820050239563 and batch: 650, loss is 3.7605779123306275 and perplexity is 42.97325357711381
At time: 393.61286902427673 and batch: 700, loss is 3.788435306549072 and perplexity is 44.1872067237759
At time: 394.56970834732056 and batch: 750, loss is 3.8824081945419313 and perplexity is 48.5409705302808
At time: 395.52450251579285 and batch: 800, loss is 3.8242385578155518 and perplexity is 45.79791462212732
At time: 396.4809150695801 and batch: 850, loss is 3.9039175701141358 and perplexity is 49.59636625772157
At time: 397.4360394477844 and batch: 900, loss is 3.8622653913497924 and perplexity is 47.57300085332678
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.52757242281143 and perplexity of 92.53365561332676
finished 21 epochs...
Completing Train Step...
At time: 399.72916650772095 and batch: 50, loss is 4.063347654342651 and perplexity is 58.168714248960214
At time: 400.67016792297363 and batch: 100, loss is 3.933541774749756 and perplexity is 51.0875983874348
At time: 401.61238980293274 and batch: 150, loss is 3.951999306678772 and perplexity is 52.03930542854891
At time: 402.5542092323303 and batch: 200, loss is 3.849041848182678 and perplexity is 46.9480583068607
At time: 403.49808287620544 and batch: 250, loss is 4.020155854225159 and perplexity is 55.70978775602485
At time: 404.4421446323395 and batch: 300, loss is 3.9930813121795654 and perplexity is 54.22170622771821
At time: 405.3854603767395 and batch: 350, loss is 3.9755895137786865 and perplexity is 53.28151783809997
At time: 406.32850670814514 and batch: 400, loss is 3.9022799348831176 and perplexity is 49.515211969705014
At time: 407.28313088417053 and batch: 450, loss is 3.930734643936157 and perplexity is 50.944389912310335
At time: 408.2453029155731 and batch: 500, loss is 3.8087377977371215 and perplexity is 45.09348583089803
At time: 409.2097408771515 and batch: 550, loss is 3.9040422201156617 and perplexity is 49.602548830172104
At time: 410.1887192726135 and batch: 600, loss is 3.9189201736450197 and perplexity is 50.34605042300591
At time: 411.1638412475586 and batch: 650, loss is 3.759854140281677 and perplexity is 42.94216199029026
At time: 412.15150809288025 and batch: 700, loss is 3.787917981147766 and perplexity is 44.16435347111865
At time: 413.1239936351776 and batch: 750, loss is 3.882184934616089 and perplexity is 48.53013448647208
At time: 414.1163258552551 and batch: 800, loss is 3.824349341392517 and perplexity is 45.80298855997602
At time: 415.09049797058105 and batch: 850, loss is 3.9044157457351685 and perplexity is 49.62108011369213
At time: 416.06350326538086 and batch: 900, loss is 3.863028783798218 and perplexity is 47.6093315884672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.527500100331764 and perplexity of 92.52696359189501
finished 22 epochs...
Completing Train Step...
At time: 418.3685984611511 and batch: 50, loss is 4.062103753089905 and perplexity is 58.096403095726174
At time: 419.30101585388184 and batch: 100, loss is 3.9320276260375975 and perplexity is 51.01030269948003
At time: 420.2344489097595 and batch: 150, loss is 3.9503851652145388 and perplexity is 51.955374384399235
At time: 421.1732847690582 and batch: 200, loss is 3.8474445056915285 and perplexity is 46.87312604062909
At time: 422.1164200305939 and batch: 250, loss is 4.018530535697937 and perplexity is 55.61931514915594
At time: 423.05781269073486 and batch: 300, loss is 3.9915414810180665 and perplexity is 54.138278203859116
At time: 424.00031089782715 and batch: 350, loss is 3.9740990018844604 and perplexity is 53.20216025841997
At time: 424.9431383609772 and batch: 400, loss is 3.900873441696167 and perplexity is 49.44561811453347
At time: 425.88518953323364 and batch: 450, loss is 3.9294709157943726 and perplexity is 50.88005071530157
At time: 426.82652711868286 and batch: 500, loss is 3.8076114368438723 and perplexity is 45.04272288598268
At time: 427.7809135913849 and batch: 550, loss is 3.9030765390396116 and perplexity is 49.55467170817891
At time: 428.7317383289337 and batch: 600, loss is 3.9181577587127685 and perplexity is 50.30768047115429
At time: 429.67921233177185 and batch: 650, loss is 3.759213857650757 and perplexity is 42.91467567027939
At time: 430.6353921890259 and batch: 700, loss is 3.787414870262146 and perplexity is 44.14213949264673
At time: 431.59102869033813 and batch: 750, loss is 3.8818982601165772 and perplexity is 48.51622412842466
At time: 432.54749393463135 and batch: 800, loss is 3.82428062915802 and perplexity is 45.79984144240939
At time: 433.50251483917236 and batch: 850, loss is 3.9045570611953737 and perplexity is 49.628092834955574
At time: 434.4605395793915 and batch: 900, loss is 3.8633295774459837 and perplexity is 47.623654326969785
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5274984281357025 and perplexity of 92.52680886880026
finished 23 epochs...
Completing Train Step...
At time: 436.7327573299408 and batch: 50, loss is 4.060940704345703 and perplexity is 58.02887342482472
At time: 437.6956787109375 and batch: 100, loss is 3.930709218978882 and perplexity is 50.943094669839226
At time: 438.6427068710327 and batch: 150, loss is 3.9489815711975096 and perplexity is 51.88250128584714
At time: 439.58462166786194 and batch: 200, loss is 3.846055760383606 and perplexity is 46.808076385928516
At time: 440.5302839279175 and batch: 250, loss is 4.017129125595093 and perplexity is 55.54142427027277
At time: 441.4778642654419 and batch: 300, loss is 3.9902193784713744 and perplexity is 54.06674914316805
At time: 442.426527261734 and batch: 350, loss is 3.9728197383880617 and perplexity is 53.13414419138346
At time: 443.37341833114624 and batch: 400, loss is 3.8996598720550537 and perplexity is 49.38564880932767
At time: 444.3244686126709 and batch: 450, loss is 3.92838173866272 and perplexity is 50.8246634963268
At time: 445.2726047039032 and batch: 500, loss is 3.8066290712356565 and perplexity is 44.99849619106784
At time: 446.2204315662384 and batch: 550, loss is 3.902197299003601 and perplexity is 49.511120405671775
At time: 447.1683201789856 and batch: 600, loss is 3.917443776130676 and perplexity is 50.271774483202734
At time: 448.1180319786072 and batch: 650, loss is 3.7585933113098147 and perplexity is 42.88805338635421
At time: 449.066130399704 and batch: 700, loss is 3.7868981647491453 and perplexity is 44.119336897435886
At time: 450.015257358551 and batch: 750, loss is 3.881555871963501 and perplexity is 48.49961559149649
At time: 450.9637129306793 and batch: 800, loss is 3.824092993736267 and perplexity is 45.79124857603239
At time: 451.912832736969 and batch: 850, loss is 3.90451416015625 and perplexity is 49.6259637838728
At time: 452.8618025779724 and batch: 900, loss is 3.8633994674682617 and perplexity is 47.626982861545976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.5275335442529965 and perplexity of 92.53005810812334
Annealing...
finished 24 epochs...
Completing Train Step...
At time: 455.1429979801178 and batch: 50, loss is 4.061172204017639 and perplexity is 58.04230864504992
At time: 456.08316254615784 and batch: 100, loss is 3.931136975288391 and perplexity is 50.96489056134288
At time: 457.0277397632599 and batch: 150, loss is 3.951027579307556 and perplexity is 51.98876197229201
At time: 457.9725556373596 and batch: 200, loss is 3.84740264415741 and perplexity is 46.871163900733464
At time: 458.92910599708557 and batch: 250, loss is 4.018785672187805 and perplexity is 55.63350747640527
At time: 459.88335061073303 and batch: 300, loss is 3.9896612977981567 and perplexity is 54.03658395349514
At time: 460.83391308784485 and batch: 350, loss is 3.972980341911316 and perplexity is 53.14267840743993
At time: 461.781081199646 and batch: 400, loss is 3.9001677417755127 and perplexity is 49.41073665512252
At time: 462.7287428379059 and batch: 450, loss is 3.9275492572784425 and perplexity is 50.78237051660561
At time: 463.67683124542236 and batch: 500, loss is 3.8063552379608154 and perplexity is 44.98617579243746
At time: 464.6386675834656 and batch: 550, loss is 3.9016941595077514 and perplexity is 49.486215671315044
At time: 465.5934820175171 and batch: 600, loss is 3.9166359233856203 and perplexity is 50.231178692106106
At time: 466.55842113494873 and batch: 650, loss is 3.7577876949310305 and perplexity is 42.8535159819093
At time: 467.5226311683655 and batch: 700, loss is 3.7849787855148316 and perplexity is 44.034736374579126
At time: 468.4885890483856 and batch: 750, loss is 3.879496684074402 and perplexity is 48.39984852526493
At time: 469.4557595252991 and batch: 800, loss is 3.822072162628174 and perplexity is 45.69880563367789
At time: 470.42216396331787 and batch: 850, loss is 3.9016914558410645 and perplexity is 49.48608187726313
At time: 471.38920998573303 and batch: 900, loss is 3.8604923915863036 and perplexity is 47.48872866342776
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.527713723378639 and perplexity of 92.54673159515096
Annealing...
finished 25 epochs...
Completing Train Step...
At time: 473.72650814056396 and batch: 50, loss is 4.060727305412293 and perplexity is 58.01649144632634
At time: 474.6728434562683 and batch: 100, loss is 3.9306729078292846 and perplexity is 50.94124490109154
At time: 475.6180214881897 and batch: 150, loss is 3.9510037326812744 and perplexity is 51.9875222304963
At time: 476.5663626194 and batch: 200, loss is 3.847726149559021 and perplexity is 46.886329428367944
At time: 477.51802277565 and batch: 250, loss is 4.01884325504303 and perplexity is 55.63671110484806
At time: 478.46734285354614 and batch: 300, loss is 3.9896645975112914 and perplexity is 54.03676225901515
At time: 479.41569566726685 and batch: 350, loss is 3.9721921920776366 and perplexity is 53.10081051554515
At time: 480.36720180511475 and batch: 400, loss is 3.899661340713501 and perplexity is 49.38572134003123
At time: 481.31946206092834 and batch: 450, loss is 3.9274439668655394 and perplexity is 50.77702390132433
At time: 482.2713701725006 and batch: 500, loss is 3.8058894872665405 and perplexity is 44.96522832835599
At time: 483.23129892349243 and batch: 550, loss is 3.901255555152893 and perplexity is 49.46451556084549
At time: 484.18564772605896 and batch: 600, loss is 3.91644823551178 and perplexity is 50.221751793661824
At time: 485.1416015625 and batch: 650, loss is 3.757674560546875 and perplexity is 42.84866805000885
At time: 486.0964186191559 and batch: 700, loss is 3.784614849090576 and perplexity is 44.018713445920454
At time: 487.0521638393402 and batch: 750, loss is 3.878985505104065 and perplexity is 48.37511386298951
At time: 488.0086133480072 and batch: 800, loss is 3.821530747413635 and perplexity is 45.67407030166834
At time: 488.9660015106201 and batch: 850, loss is 3.9010039615631102 and perplexity is 49.45207217121285
At time: 489.92180466651917 and batch: 900, loss is 3.859743995666504 and perplexity is 47.45320158847675
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.527621334546233 and perplexity of 92.53818170563915
Annealing...
finished 26 epochs...
Completing Train Step...
At time: 492.1993947029114 and batch: 50, loss is 4.060586051940918 and perplexity is 58.00829699427359
At time: 493.1467363834381 and batch: 100, loss is 3.930555667877197 and perplexity is 50.935272902065336
At time: 494.0899832248688 and batch: 150, loss is 3.9509589147567747 and perplexity is 51.985192309861546
At time: 495.03927779197693 and batch: 200, loss is 3.8477222204208372 and perplexity is 46.88614520586261
At time: 495.98635053634644 and batch: 250, loss is 4.018843383789062 and perplexity is 55.63671826785434
At time: 496.9337306022644 and batch: 300, loss is 3.98963668346405 and perplexity is 54.03525389533305
At time: 497.88212299346924 and batch: 350, loss is 3.9720309495925905 and perplexity is 53.09224909915024
At time: 498.83754539489746 and batch: 400, loss is 3.8995366525650024 and perplexity is 49.37956390976234
At time: 499.7920603752136 and batch: 450, loss is 3.9274206447601316 and perplexity is 50.77583968802983
At time: 500.7473828792572 and batch: 500, loss is 3.805801606178284 and perplexity is 44.96127690878685
At time: 501.7025022506714 and batch: 550, loss is 3.901148462295532 and perplexity is 49.45921854817728
At time: 502.6571865081787 and batch: 600, loss is 3.916399154663086 and perplexity is 50.21928692795025
At time: 503.61102652549744 and batch: 650, loss is 3.7576306533813475 and perplexity is 42.846786727750214
At time: 504.56569743156433 and batch: 700, loss is 3.784534435272217 and perplexity is 44.015173875410106
At time: 505.5295298099518 and batch: 750, loss is 3.8788770484924315 and perplexity is 48.36986754655655
At time: 506.5002155303955 and batch: 800, loss is 3.821411061286926 and perplexity is 45.668604076225094
At time: 507.4692587852478 and batch: 850, loss is 3.900861029624939 and perplexity is 49.445004395808276
At time: 508.43229937553406 and batch: 900, loss is 3.8595924186706543 and perplexity is 47.44600931984154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.52760252234054 and perplexity of 92.53644087470488
Annealing...
finished 27 epochs...
Completing Train Step...
At time: 510.7361476421356 and batch: 50, loss is 4.060558567047119 and perplexity is 58.006702664301336
At time: 511.68199157714844 and batch: 100, loss is 3.9305322313308717 and perplexity is 50.934079169170914
At time: 512.6325845718384 and batch: 150, loss is 3.95094765663147 and perplexity is 51.984607057346956
At time: 513.5811734199524 and batch: 200, loss is 3.8477187156677246 and perplexity is 46.88598088178722
At time: 514.5324075222015 and batch: 250, loss is 4.018842725753784 and perplexity is 55.636681656943026
At time: 515.4802374839783 and batch: 300, loss is 3.9896293973922727 and perplexity is 54.03486019202893
At time: 516.4289524555206 and batch: 350, loss is 3.971999263763428 and perplexity is 53.09056685386722
At time: 517.3739020824432 and batch: 400, loss is 3.89951105594635 and perplexity is 49.378299976072014
At time: 518.3134846687317 and batch: 450, loss is 3.927415475845337 and perplexity is 50.77557723271915
At time: 519.267073392868 and batch: 500, loss is 3.805784349441528 and perplexity is 44.96050103056162
At time: 520.213526725769 and batch: 550, loss is 3.901126518249512 and perplexity is 49.45813322471757
At time: 521.161404132843 and batch: 600, loss is 3.916388454437256 and perplexity is 50.218749573113996
At time: 522.1081981658936 and batch: 650, loss is 3.7576209020614626 and perplexity is 42.84636891706389
At time: 523.0550577640533 and batch: 700, loss is 3.784517865180969 and perplexity is 44.01444454600526
At time: 524.0009007453918 and batch: 750, loss is 3.8788551092147827 and perplexity is 48.3688063582435
At time: 524.9478466510773 and batch: 800, loss is 3.8213863611221313 and perplexity is 45.66747606810951
At time: 525.908938407898 and batch: 850, loss is 3.9008318090438845 and perplexity is 49.443559605158505
At time: 526.8652989864349 and batch: 900, loss is 3.859561629295349 and perplexity is 47.44454850934268
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.527598341850386 and perplexity of 92.53605402783352
Annealing...
Model not improving. Stopping early with 92.52680886880026 lossat 27 epochs.
Finished Training.
langmodel



RESULTS:
[{'best_accuracy': -78.52828833517174, 'params': {'batch_size': 32, 'lr': 13.100449298335306, 'dropout': 0.9571356695129988, 'hidden_size': 300, 'clip': 0.2980817543502909, 'tune_wordvecs': True, 'anneal': 4.689255290552385, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.30874817919889586}}, {'best_accuracy': -143.57553234808663, 'params': {'batch_size': 32, 'lr': 1.4404124831850984, 'dropout': 0.4514114234112402, 'hidden_size': 300, 'clip': 0.016277867931793033, 'tune_wordvecs': True, 'anneal': 4.481110263320149, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.5550563291447015}}, {'best_accuracy': -79.45571539735015, 'params': {'batch_size': 32, 'lr': 12.08068884456746, 'dropout': 0.5041766372594111, 'hidden_size': 300, 'clip': 0.12163134647722318, 'tune_wordvecs': True, 'anneal': 4.912640445451629, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.5177780835706017}}, {'best_accuracy': -76.60416695888601, 'params': {'batch_size': 32, 'lr': 9.720646266067527, 'dropout': 0.8263516623500462, 'hidden_size': 300, 'clip': 0.2575458035136192, 'tune_wordvecs': True, 'anneal': 4.259308432670263, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.21537363069409232}}, {'best_accuracy': -92.52680886880026, 'params': {'batch_size': 32, 'lr': 1.8158991810007974, 'dropout': 0.9936987775753, 'hidden_size': 300, 'clip': 0.19122660562142446, 'tune_wordvecs': True, 'anneal': 4.952290843617265, 'seq_len': 35, 'wordvec_dim': 300, 'num_layers': 3, 'wordvec_source': 'None', 'rnn_dropout': 0.5999394469912211}}]
Traceback (most recent call last):
  File "bayesian_optimization.py", line 226, in <module>
    fix_pretrained = args.fix_pretrained, savepath = args.savepath)
  File "bayesian_optimization.py", line 79, in __init__
    self.save()
  File "bayesian_optimization.py", line 89, in save
    self.best_model.save_checkpoint(savepath, name = name)
TypeError: save_checkpoint() got multiple values for argument 'name'
