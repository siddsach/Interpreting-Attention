Building Bayesian Optimizer for 
 data:ptb 
 choices:[{'type': 'continuous', 'domain': [0, 40], 'name': 'lr'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'dropout'}, {'type': 'continuous', 'domain': [4, 5], 'name': 'anneal'}, {'type': 'continuous', 'domain': [0, 1], 'name': 'rnn_dropout'}, {'type': 'continuous', 'domain': [0, 0.5], 'name': 'clip'}]
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.20498933070666836, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.206438412426412, 'wordvec_source': 'None', 'dropout': 0.0105454069243337, 'lr': 33.092705634289025, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.08760933524890213, 'tune_wordvecs': True}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.07947631279627482 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 1.2293975353240967 and batch: 50, loss is 6.436904487609863 and perplexity is 624.4707478837405
At time: 1.8679289817810059 and batch: 100, loss is 5.567927570343017 and perplexity is 261.8907862134839
At time: 2.4932425022125244 and batch: 150, loss is 5.322659883499146 and perplexity is 204.92824295559828
At time: 3.11858868598938 and batch: 200, loss is 5.090873441696167 and perplexity is 162.53176209172523
At time: 3.744593858718872 and batch: 250, loss is 5.111765451431275 and perplexity is 165.96309617309916
At time: 4.3703014850616455 and batch: 300, loss is 5.031921310424805 and perplexity is 153.22712693910938
At time: 4.997030735015869 and batch: 350, loss is 4.982261695861816 and perplexity is 145.8037728114953
At time: 5.620330572128296 and batch: 400, loss is 4.839767255783081 and perplexity is 126.43992014503058
At time: 6.246555328369141 and batch: 450, loss is 4.842910957336426 and perplexity is 126.83803496761239
At time: 6.872050762176514 and batch: 500, loss is 4.725121803283692 and perplexity is 112.74423015521437
At time: 7.497865676879883 and batch: 550, loss is 4.792585983276367 and perplexity is 120.61286858390399
At time: 8.1258544921875 and batch: 600, loss is 4.74374174118042 and perplexity is 114.86318692039835
At time: 8.753750324249268 and batch: 650, loss is 4.605681571960449 and perplexity is 100.05115167524565
At time: 9.379914999008179 and batch: 700, loss is 4.63242244720459 and perplexity is 102.76270005066003
At time: 10.006945610046387 and batch: 750, loss is 4.691984663009643 and perplexity is 109.06943120895804
At time: 10.629498720169067 and batch: 800, loss is 4.616767244338989 and perplexity is 101.1664564938984
At time: 11.255634307861328 and batch: 850, loss is 4.685156011581421 and perplexity is 108.3271712832344
At time: 11.881925344467163 and batch: 900, loss is 4.62007025718689 and perplexity is 101.50116306522594
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.713789430383134 and perplexity of 111.47378268510955
finished 1 epochs...
Completing Train Step...
At time: 13.498516321182251 and batch: 50, loss is 4.668152894973755 and perplexity is 106.50084239961689
At time: 14.113483190536499 and batch: 100, loss is 4.4859678840637205 and perplexity is 88.76282137014009
At time: 14.72894024848938 and batch: 150, loss is 4.465430450439453 and perplexity is 86.95845278896469
At time: 15.34396767616272 and batch: 200, loss is 4.356106820106507 and perplexity is 77.95305761774864
At time: 15.957690000534058 and batch: 250, loss is 4.490951204299927 and perplexity is 89.20625891206036
At time: 16.571841716766357 and batch: 300, loss is 4.462224946022034 and perplexity is 86.68015336772963
At time: 17.185308694839478 and batch: 350, loss is 4.435024433135986 and perplexity is 84.35418590040715
At time: 17.798246145248413 and batch: 400, loss is 4.362951698303223 and perplexity is 78.48846711818018
At time: 18.41075348854065 and batch: 450, loss is 4.37214111328125 and perplexity is 79.21305438104675
At time: 19.02773404121399 and batch: 500, loss is 4.255629005432129 and perplexity is 70.50114885014098
At time: 19.643551349639893 and batch: 550, loss is 4.346982083320618 and perplexity is 77.24499185502535
At time: 20.260191202163696 and batch: 600, loss is 4.339905109405517 and perplexity is 76.70026085962154
At time: 20.879642724990845 and batch: 650, loss is 4.19862964630127 and perplexity is 66.59500976604265
At time: 21.50186514854431 and batch: 700, loss is 4.201227016448975 and perplexity is 66.76820648702127
At time: 22.12021827697754 and batch: 750, loss is 4.311832752227783 and perplexity is 74.57704501078518
At time: 22.75675344467163 and batch: 800, loss is 4.252319412231445 and perplexity is 70.26820441555247
At time: 23.38379979133606 and batch: 850, loss is 4.326438145637512 and perplexity is 75.67426525453924
At time: 24.015345573425293 and batch: 900, loss is 4.280669655799866 and perplexity is 72.28883243828872
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.583628615287886 and perplexity of 97.86887918451279
finished 2 epochs...
Completing Train Step...
At time: 25.604740619659424 and batch: 50, loss is 4.358803901672363 and perplexity is 78.16358715249491
At time: 26.240699529647827 and batch: 100, loss is 4.188780999183654 and perplexity is 65.94235815811761
At time: 26.86007571220398 and batch: 150, loss is 4.179172320365906 and perplexity is 65.3117736119194
At time: 27.475051879882812 and batch: 200, loss is 4.0731964635849 and perplexity is 58.744437258807665
At time: 28.101569414138794 and batch: 250, loss is 4.221312856674194 and perplexity is 68.1228611600108
At time: 28.71789312362671 and batch: 300, loss is 4.200341787338257 and perplexity is 66.70912748005493
At time: 29.339613437652588 and batch: 350, loss is 4.173989214897156 and perplexity is 64.97413157396842
At time: 29.953850984573364 and batch: 400, loss is 4.117002048492432 and perplexity is 61.374967003443096
At time: 30.568705797195435 and batch: 450, loss is 4.128508877754212 and perplexity is 62.08527714094401
At time: 31.18416690826416 and batch: 500, loss is 4.019765748977661 and perplexity is 55.68805931394941
At time: 31.80535387992859 and batch: 550, loss is 4.104927644729615 and perplexity is 60.63835688416455
At time: 32.430845737457275 and batch: 600, loss is 4.113055477142334 and perplexity is 61.13322365950294
At time: 33.048789739608765 and batch: 650, loss is 3.9672024917602537 and perplexity is 52.83651331472864
At time: 33.66725301742554 and batch: 700, loss is 3.969259796142578 and perplexity is 52.94532599714282
At time: 34.28539037704468 and batch: 750, loss is 4.090604314804077 and perplexity is 59.77600431568984
At time: 34.90318202972412 and batch: 800, loss is 4.031608667373657 and perplexity is 56.35148917480861
At time: 35.52189373970032 and batch: 850, loss is 4.1050231695175174 and perplexity is 60.64414962701555
At time: 36.139222621917725 and batch: 900, loss is 4.0687806510925295 and perplexity is 58.4856047374334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.547293049015411 and perplexity of 94.3765894248713
finished 3 epochs...
Completing Train Step...
At time: 37.75370645523071 and batch: 50, loss is 4.153855724334717 and perplexity is 63.67905644760458
At time: 38.390225648880005 and batch: 100, loss is 3.996831169128418 and perplexity is 54.42541156383102
At time: 39.00685715675354 and batch: 150, loss is 3.9835608625411987 and perplexity is 53.70794072416393
At time: 39.624070167541504 and batch: 200, loss is 3.8845185613632203 and perplexity is 48.64351795224035
At time: 40.24099278450012 and batch: 250, loss is 4.03259644985199 and perplexity is 56.40717968897235
At time: 40.858365535736084 and batch: 300, loss is 4.019517340660095 and perplexity is 55.67422765484745
At time: 41.47691750526428 and batch: 350, loss is 3.992913999557495 and perplexity is 54.212635010761836
At time: 42.09470057487488 and batch: 400, loss is 3.9397602319717406 and perplexity is 51.40627424175358
At time: 42.71238899230957 and batch: 450, loss is 3.956519241333008 and perplexity is 52.27505206688312
At time: 43.32920503616333 and batch: 500, loss is 3.8483813285827635 and perplexity is 46.917058433310196
At time: 43.94367742538452 and batch: 550, loss is 3.9302922534942626 and perplexity is 50.921857585555465
At time: 44.558722257614136 and batch: 600, loss is 3.946820034980774 and perplexity is 51.77047649673958
At time: 45.17426514625549 and batch: 650, loss is 3.8021849679946897 and perplexity is 44.798961932130815
At time: 45.790647983551025 and batch: 700, loss is 3.8019138050079344 and perplexity is 44.78681575868052
At time: 46.40579414367676 and batch: 750, loss is 3.922125301361084 and perplexity is 50.507674819665894
At time: 47.02083873748779 and batch: 800, loss is 3.86417275428772 and perplexity is 47.663826423127695
At time: 47.63493537902832 and batch: 850, loss is 3.9435063934326173 and perplexity is 51.59921160679014
At time: 48.25008463859558 and batch: 900, loss is 3.9058260774612426 and perplexity is 49.6911116695131
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.537549580613228 and perplexity of 93.46149942267283
finished 4 epochs...
Completing Train Step...
At time: 49.846789836883545 and batch: 50, loss is 3.9924234628677366 and perplexity is 54.18604824566557
At time: 50.46265268325806 and batch: 100, loss is 3.844502177238464 and perplexity is 46.73541260662545
At time: 51.07974720001221 and batch: 150, loss is 3.83076265335083 and perplexity is 46.0976813826631
At time: 51.695308446884155 and batch: 200, loss is 3.735565981864929 and perplexity is 41.91174013630526
At time: 52.31160926818848 and batch: 250, loss is 3.883060345649719 and perplexity is 48.57263690249276
At time: 52.92828059196472 and batch: 300, loss is 3.873444652557373 and perplexity is 48.10781570384892
At time: 53.569539308547974 and batch: 350, loss is 3.846942677497864 and perplexity is 46.84960968553521
At time: 54.186726331710815 and batch: 400, loss is 3.798867974281311 and perplexity is 44.65061023388057
At time: 54.80321288108826 and batch: 450, loss is 3.812957720756531 and perplexity is 45.28417894192115
At time: 55.4232120513916 and batch: 500, loss is 3.71479896068573 and perplexity is 41.05033352387328
At time: 56.04061436653137 and batch: 550, loss is 3.7896184873580934 and perplexity is 44.23951912018473
At time: 56.65959620475769 and batch: 600, loss is 3.809171533584595 and perplexity is 45.11304873445001
At time: 57.27734661102295 and batch: 650, loss is 3.663955388069153 and perplexity is 39.01535895917551
At time: 57.895169496536255 and batch: 700, loss is 3.668741030693054 and perplexity is 39.202520009745335
At time: 58.512080669403076 and batch: 750, loss is 3.7861120462417603 and perplexity is 44.084667499080524
At time: 59.127851486206055 and batch: 800, loss is 3.725610165596008 and perplexity is 41.49654478575865
At time: 59.74381732940674 and batch: 850, loss is 3.8078821849823 and perplexity is 45.054919770421144
At time: 60.357775926589966 and batch: 900, loss is 3.7744556188583376 and perplexity is 43.573781113279686
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.548195616839683 and perplexity of 94.46180915035663
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 61.916685342788696 and batch: 50, loss is 3.8710752010345457 and perplexity is 47.99396150593848
At time: 62.52618050575256 and batch: 100, loss is 3.717223448753357 and perplexity is 41.14998031509065
At time: 63.13714599609375 and batch: 150, loss is 3.6945935678482056 and perplexity is 40.22921881687882
At time: 63.74976897239685 and batch: 200, loss is 3.5701636409759523 and perplexity is 35.52240559715847
At time: 64.36100506782532 and batch: 250, loss is 3.70911651134491 and perplexity is 40.81772859197629
At time: 64.97165870666504 and batch: 300, loss is 3.6865513467788698 and perplexity is 39.90698402419797
At time: 65.58474254608154 and batch: 350, loss is 3.638474116325378 and perplexity is 38.033757336068334
At time: 66.19921946525574 and batch: 400, loss is 3.5808610916137695 and perplexity is 35.90444455579506
At time: 66.81864023208618 and batch: 450, loss is 3.5694512939453125 and perplexity is 35.49711032758488
At time: 67.4339394569397 and batch: 500, loss is 3.4546094226837156 and perplexity is 31.64592608727492
At time: 68.05142760276794 and batch: 550, loss is 3.5070407390594482 and perplexity is 33.349431944210096
At time: 68.66867184638977 and batch: 600, loss is 3.5069425773620604 and perplexity is 33.34615846803115
At time: 69.31004500389099 and batch: 650, loss is 3.3433767318725587 and perplexity is 28.314576192938485
At time: 69.9311032295227 and batch: 700, loss is 3.3267950201034546 and perplexity is 27.848943216435625
At time: 70.55993556976318 and batch: 750, loss is 3.411583948135376 and perplexity is 30.313220822774834
At time: 71.18980050086975 and batch: 800, loss is 3.330609693527222 and perplexity is 27.955380723096518
At time: 71.81550455093384 and batch: 850, loss is 3.3823594760894777 and perplexity is 29.440152565211253
At time: 72.43801045417786 and batch: 900, loss is 3.3340011978149415 and perplexity is 28.05035247420149
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.460947951225386 and perplexity of 86.56953390817588
finished 6 epochs...
Completing Train Step...
At time: 74.01088261604309 and batch: 50, loss is 3.7100415802001954 and perplexity is 40.85550527176123
At time: 74.63704442977905 and batch: 100, loss is 3.5617601013183595 and perplexity is 35.22514243375958
At time: 75.25008869171143 and batch: 150, loss is 3.545190649032593 and perplexity is 34.64628999432593
At time: 75.86425113677979 and batch: 200, loss is 3.432255072593689 and perplexity is 30.946350388879672
At time: 76.47868371009827 and batch: 250, loss is 3.5726097774505616 and perplexity is 35.60940461150618
At time: 77.09040975570679 and batch: 300, loss is 3.557261185646057 and perplexity is 35.067023437118436
At time: 77.70171546936035 and batch: 350, loss is 3.516207242012024 and perplexity is 33.65653499073237
At time: 78.31235909461975 and batch: 400, loss is 3.465017523765564 and perplexity is 31.977020126024893
At time: 78.92424464225769 and batch: 450, loss is 3.462368211746216 and perplexity is 31.892415144245476
At time: 79.53501868247986 and batch: 500, loss is 3.353583288192749 and perplexity is 28.605050358353243
At time: 80.14623069763184 and batch: 550, loss is 3.411046986579895 and perplexity is 30.29694815785892
At time: 80.7640106678009 and batch: 600, loss is 3.423548765182495 and perplexity is 30.678091416176297
At time: 81.38594794273376 and batch: 650, loss is 3.266889901161194 and perplexity is 26.229635596548942
At time: 82.00625872612 and batch: 700, loss is 3.2579452419281005 and perplexity is 25.996066599133677
At time: 82.62683629989624 and batch: 750, loss is 3.35313618183136 and perplexity is 28.592263717077497
At time: 83.24600458145142 and batch: 800, loss is 3.2835928344726564 and perplexity is 26.671426783917298
At time: 83.86656856536865 and batch: 850, loss is 3.347158975601196 and perplexity is 28.421871601958866
At time: 84.48725533485413 and batch: 900, loss is 3.3111464738845826 and perplexity is 27.41653980749953
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.469086529457406 and perplexity of 87.27696165564232
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 86.06480097770691 and batch: 50, loss is 3.6473055696487426 and perplexity is 38.3711382778934
At time: 86.69003057479858 and batch: 100, loss is 3.508630599975586 and perplexity is 33.402495072953705
At time: 87.30335116386414 and batch: 150, loss is 3.4937839794158934 and perplexity is 32.91024407662491
At time: 87.92592191696167 and batch: 200, loss is 3.373397288322449 and perplexity is 29.177483194292286
At time: 88.54190516471863 and batch: 250, loss is 3.511857647895813 and perplexity is 33.51046063653835
At time: 89.15664935112 and batch: 300, loss is 3.4930281496047972 and perplexity is 32.88537893115419
At time: 89.77444124221802 and batch: 350, loss is 3.4466138219833375 and perplexity is 31.393906764049788
At time: 90.3941764831543 and batch: 400, loss is 3.394995403289795 and perplexity is 29.814516425931174
At time: 91.01152658462524 and batch: 450, loss is 3.386159996986389 and perplexity is 29.55225336603005
At time: 91.62788581848145 and batch: 500, loss is 3.2700477838516235 and perplexity is 26.31259663043986
At time: 92.24426198005676 and batch: 550, loss is 3.3145418834686278 and perplexity is 27.509788408533023
At time: 92.86043930053711 and batch: 600, loss is 3.3246680021286013 and perplexity is 27.789770966162518
At time: 93.4775447845459 and batch: 650, loss is 3.160636076927185 and perplexity is 23.58559341056892
At time: 94.09844160079956 and batch: 700, loss is 3.1439243173599243 and perplexity is 23.194711900317685
At time: 94.72039222717285 and batch: 750, loss is 3.229985685348511 and perplexity is 25.279295104073597
At time: 95.34303307533264 and batch: 800, loss is 3.1528217029571532 and perplexity is 23.402005011781913
At time: 95.96488046646118 and batch: 850, loss is 3.206118416786194 and perplexity is 24.683090567538407
At time: 96.58782601356506 and batch: 900, loss is 3.1679515266418456 and perplexity is 23.758765275905517
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.451873361247859 and perplexity of 85.78750454786196
finished 8 epochs...
Completing Train Step...
At time: 98.18279480934143 and batch: 50, loss is 3.6039930820465087 and perplexity is 36.744666359327915
At time: 98.81272029876709 and batch: 100, loss is 3.4622514867782592 and perplexity is 31.888692720363835
At time: 99.44460606575012 and batch: 150, loss is 3.4463753652572633 and perplexity is 31.386421568307256
At time: 100.07355284690857 and batch: 200, loss is 3.3291906118392944 and perplexity is 27.915737889095933
At time: 100.69944381713867 and batch: 250, loss is 3.46707190990448 and perplexity is 32.04278079871428
At time: 101.33895826339722 and batch: 300, loss is 3.4506948947906495 and perplexity is 31.52228937446401
At time: 101.96466827392578 and batch: 350, loss is 3.406675639152527 and perplexity is 30.164798717463473
At time: 102.5899748802185 and batch: 400, loss is 3.357042784690857 and perplexity is 28.704180801740005
At time: 103.21753668785095 and batch: 450, loss is 3.3525946712493897 and perplexity is 28.57678489506342
At time: 103.83661937713623 and batch: 500, loss is 3.239014701843262 and perplexity is 25.508575808537106
At time: 104.4546947479248 and batch: 550, loss is 3.2877596712112425 and perplexity is 26.78279412854773
At time: 105.07219243049622 and batch: 600, loss is 3.3018974256515503 and perplexity is 27.164131973898336
At time: 105.68907189369202 and batch: 650, loss is 3.1419956827163698 and perplexity is 23.150020885571955
At time: 106.30622434616089 and batch: 700, loss is 3.129488959312439 and perplexity is 22.862292994613245
At time: 106.9241111278534 and batch: 750, loss is 3.2201321840286257 and perplexity is 25.031428717755325
At time: 107.54136848449707 and batch: 800, loss is 3.147908968925476 and perplexity is 23.28731912663797
At time: 108.15766191482544 and batch: 850, loss is 3.2067648792266845 and perplexity is 24.69905241731784
At time: 108.77592992782593 and batch: 900, loss is 3.173312773704529 and perplexity is 23.886483946209278
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454878715619649 and perplexity of 86.04571421112084
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 110.34264254570007 and batch: 50, loss is 3.583927149772644 and perplexity is 36.01469860726003
At time: 110.95972228050232 and batch: 100, loss is 3.444677734375 and perplexity is 31.3331842112455
At time: 111.57759642601013 and batch: 150, loss is 3.430303888320923 and perplexity is 30.88602722664223
At time: 112.1952292919159 and batch: 200, loss is 3.3117126846313476 and perplexity is 27.432067742604666
At time: 112.81410956382751 and batch: 250, loss is 3.4486881923675536 and perplexity is 31.459096945400564
At time: 113.431316614151 and batch: 300, loss is 3.43081663608551 and perplexity is 30.901868028880582
At time: 114.0497534275055 and batch: 350, loss is 3.3857432413101196 and perplexity is 29.539939862732624
At time: 114.66721558570862 and batch: 400, loss is 3.337576961517334 and perplexity is 28.150833447463366
At time: 115.28514099121094 and batch: 450, loss is 3.330864095687866 and perplexity is 27.962493537073367
At time: 115.90266823768616 and batch: 500, loss is 3.2138784599304198 and perplexity is 24.875377527898202
At time: 116.53376150131226 and batch: 550, loss is 3.258780903816223 and perplexity is 26.017799600687486
At time: 117.15223813056946 and batch: 600, loss is 3.2722487545013426 and perplexity is 26.37057366280995
At time: 117.77111744880676 and batch: 650, loss is 3.1092392683029173 and perplexity is 22.403994487615922
At time: 118.38890838623047 and batch: 700, loss is 3.093926959037781 and perplexity is 22.063550737115488
At time: 119.00630974769592 and batch: 750, loss is 3.1824672746658327 and perplexity is 24.10615674976694
At time: 119.62442708015442 and batch: 800, loss is 3.1071003675460815 and perplexity is 22.356125778309032
At time: 120.24148488044739 and batch: 850, loss is 3.1618674564361573 and perplexity is 23.61465411570801
At time: 120.85967826843262 and batch: 900, loss is 3.1281423139572144 and perplexity is 22.8315263144952
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45347574312393 and perplexity of 85.92507908437804
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 122.4216239452362 and batch: 50, loss is 3.5737453842163087 and perplexity is 35.64986586199389
At time: 123.05240082740784 and batch: 100, loss is 3.4347718715667725 and perplexity is 31.02433422541468
At time: 123.67042064666748 and batch: 150, loss is 3.4210884380340576 and perplexity is 30.602706049362066
At time: 124.28878140449524 and batch: 200, loss is 3.303048596382141 and perplexity is 27.195420533330214
At time: 124.90717673301697 and batch: 250, loss is 3.440043931007385 and perplexity is 31.18832827289389
At time: 125.52535200119019 and batch: 300, loss is 3.4217015743255614 and perplexity is 30.621475432566154
At time: 126.14389896392822 and batch: 350, loss is 3.3767126989364624 and perplexity is 29.274379067812426
At time: 126.76204586029053 and batch: 400, loss is 3.3290808963775635 and perplexity is 27.912675269035333
At time: 127.38093709945679 and batch: 450, loss is 3.322501163482666 and perplexity is 27.729620208499423
At time: 127.99904322624207 and batch: 500, loss is 3.2053503036499023 and perplexity is 24.66413844104967
At time: 128.6178855895996 and batch: 550, loss is 3.249998631477356 and perplexity is 25.790304622553037
At time: 129.23600125312805 and batch: 600, loss is 3.2638859367370605 and perplexity is 26.150960931468457
At time: 129.86782455444336 and batch: 650, loss is 3.1001583433151243 and perplexity is 22.20146645693143
At time: 130.49766039848328 and batch: 700, loss is 3.0847569513320923 and perplexity is 21.862152629221114
At time: 131.12140464782715 and batch: 750, loss is 3.172968444824219 and perplexity is 23.878260555794185
At time: 131.74470329284668 and batch: 800, loss is 3.096893548965454 and perplexity is 22.129101427435668
At time: 132.39354038238525 and batch: 850, loss is 3.1506107902526854 and perplexity is 23.350322375542518
At time: 133.01784586906433 and batch: 900, loss is 3.1165562963485716 and perplexity is 22.568526351677775
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.453238291283176 and perplexity of 85.90467843836389
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 134.6017255783081 and batch: 50, loss is 3.5712981271743773 and perplexity is 35.56272814440518
At time: 135.23805046081543 and batch: 100, loss is 3.4323653602600097 and perplexity is 30.94976357785807
At time: 135.86055517196655 and batch: 150, loss is 3.418732137680054 and perplexity is 30.530681771004918
At time: 136.48217916488647 and batch: 200, loss is 3.300770468711853 and perplexity is 27.133536410039113
At time: 137.1028356552124 and batch: 250, loss is 3.4378215074539185 and perplexity is 31.119091562692354
At time: 137.72404146194458 and batch: 300, loss is 3.4193536138534544 and perplexity is 30.549661759477683
At time: 138.34464263916016 and batch: 350, loss is 3.3743473720550536 and perplexity is 29.205217419265313
At time: 138.96497988700867 and batch: 400, loss is 3.3267829418182373 and perplexity is 27.848606850987817
At time: 139.5850429534912 and batch: 450, loss is 3.3203408336639404 and perplexity is 27.669779743944115
At time: 140.20601558685303 and batch: 500, loss is 3.2031839275360108 and perplexity is 24.61076447557771
At time: 140.82651257514954 and batch: 550, loss is 3.247739953994751 and perplexity is 25.73211837894869
At time: 141.44943165779114 and batch: 600, loss is 3.2617460107803344 and perplexity is 26.09505964501714
At time: 142.07104015350342 and batch: 650, loss is 3.0978766202926638 and perplexity is 22.15086660915606
At time: 142.69222402572632 and batch: 700, loss is 3.0824203538894652 and perplexity is 21.811129213103484
At time: 143.31541991233826 and batch: 750, loss is 3.1707011747360228 and perplexity is 23.824183416783104
At time: 143.94230008125305 and batch: 800, loss is 3.094428582191467 and perplexity is 22.07462110136114
At time: 144.5710039138794 and batch: 850, loss is 3.1478949069976805 and perplexity is 23.286991664340242
At time: 145.20452308654785 and batch: 900, loss is 3.1137148475646974 and perplexity is 22.504490060927054
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.453177674175942 and perplexity of 85.89947130308141
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 146.90536546707153 and batch: 50, loss is 3.570723433494568 and perplexity is 35.54229634088032
At time: 147.54618763923645 and batch: 100, loss is 3.4317976999282838 and perplexity is 30.932199610454056
At time: 148.1790268421173 and batch: 150, loss is 3.4181667804718017 and perplexity is 30.513425908315625
At time: 148.81530213356018 and batch: 200, loss is 3.3002375030517577 and perplexity is 27.119079019884087
At time: 149.45897126197815 and batch: 250, loss is 3.4372771739959718 and perplexity is 31.102157009413986
At time: 150.09904098510742 and batch: 300, loss is 3.4187886905670166 and perplexity is 30.532408418022985
At time: 150.73392987251282 and batch: 350, loss is 3.3737578868865965 and perplexity is 29.18800645005902
At time: 151.3577220439911 and batch: 400, loss is 3.3262362718582152 and perplexity is 27.833387014686604
At time: 151.9785327911377 and batch: 450, loss is 3.319814615249634 and perplexity is 27.65522322661268
At time: 152.61122012138367 and batch: 500, loss is 3.2026645612716673 and perplexity is 24.597985793464833
At time: 153.23319554328918 and batch: 550, loss is 3.247195110321045 and perplexity is 25.718102215694536
At time: 153.87061882019043 and batch: 600, loss is 3.261221923828125 and perplexity is 26.081387147841696
At time: 154.50248456001282 and batch: 650, loss is 3.0973219966888426 and perplexity is 22.138584621944368
At time: 155.1348214149475 and batch: 700, loss is 3.0818522214889525 and perplexity is 21.79874112327657
At time: 155.76184821128845 and batch: 750, loss is 3.170148558616638 and perplexity is 23.811021426094875
At time: 156.39303278923035 and batch: 800, loss is 3.0938409566879272 and perplexity is 22.061653301499113
At time: 157.02101612091064 and batch: 850, loss is 3.147248373031616 and perplexity is 23.271940699266143
At time: 157.641756772995 and batch: 900, loss is 3.113033485412598 and perplexity is 22.489161575865047
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.453160952215326 and perplexity of 85.89803490751504
Annealing...
Model not improving. Stopping early with 85.78750454786196 lossat 12 epochs.
Finished Training.
Improved accuracyfrom -10000000 to -85.78750454786196
langmodel
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.4153279496866563, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.8252968048649985, 'wordvec_source': 'None', 'dropout': 0.7488427949660501, 'lr': 18.815293339150582, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.045940316742695886, 'tune_wordvecs': True}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.0809261163075765 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.869257926940918 and batch: 50, loss is 7.363281517028809 and perplexity is 1577.0030437166572
At time: 1.513343095779419 and batch: 100, loss is 6.29947585105896 and perplexity is 544.2865481285442
At time: 2.1416165828704834 and batch: 150, loss is 6.176259078979492 and perplexity is 481.1884969932505
At time: 2.768744707107544 and batch: 200, loss is 6.020375728607178 and perplexity is 411.73326663450507
At time: 3.3936383724212646 and batch: 250, loss is 6.062412128448487 and perplexity is 429.40998066182414
At time: 4.021313905715942 and batch: 300, loss is 5.967883682250976 and perplexity is 390.6779964698371
At time: 4.64462947845459 and batch: 350, loss is 5.965150480270386 and perplexity is 389.61165252663534
At time: 5.272021532058716 and batch: 400, loss is 5.825242080688477 and perplexity is 338.7431263222731
At time: 5.900025129318237 and batch: 450, loss is 5.831634368896484 and perplexity is 340.91540553475403
At time: 6.526191234588623 and batch: 500, loss is 5.786404809951782 and perplexity is 325.8394612603773
At time: 7.152096748352051 and batch: 550, loss is 5.832505502700806 and perplexity is 341.21251786243624
At time: 7.7872560024261475 and batch: 600, loss is 5.76171181678772 and perplexity is 317.89203634582884
At time: 8.408345460891724 and batch: 650, loss is 5.673927488327027 and perplexity is 291.17588148656705
At time: 9.02715253829956 and batch: 700, loss is 5.779105014801026 and perplexity is 323.4695603604272
At time: 9.646204471588135 and batch: 750, loss is 5.730091695785522 and perplexity is 307.9975091561519
At time: 10.264936923980713 and batch: 800, loss is 5.727331342697144 and perplexity is 307.14849960270044
At time: 10.882718324661255 and batch: 850, loss is 5.756237373352051 and perplexity is 316.1565092311232
At time: 11.501914739608765 and batch: 900, loss is 5.653542594909668 and perplexity is 285.30038135797287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.480267668423587 and perplexity of 239.9109153571621
finished 1 epochs...
Completing Train Step...
At time: 13.08499526977539 and batch: 50, loss is 5.407226152420044 and perplexity is 223.01212728380705
At time: 13.69513988494873 and batch: 100, loss is 5.260517463684082 and perplexity is 192.58111925355155
At time: 14.306287050247192 and batch: 150, loss is 5.237229595184326 and perplexity is 188.14813321222013
At time: 14.923276424407959 and batch: 200, loss is 5.102239637374878 and perplexity is 164.38966856600425
At time: 15.547404289245605 and batch: 250, loss is 5.184792556762695 and perplexity is 178.5364097772875
At time: 16.17228412628174 and batch: 300, loss is 5.114201211929322 and perplexity is 166.3678352505122
At time: 16.795170307159424 and batch: 350, loss is 5.0951072883605955 and perplexity is 163.22135543665655
At time: 17.412478923797607 and batch: 400, loss is 4.958412399291992 and perplexity is 142.3675934927256
At time: 18.029600143432617 and batch: 450, loss is 4.959495487213135 and perplexity is 142.52187364800415
At time: 18.648035526275635 and batch: 500, loss is 4.880500583648682 and perplexity is 131.6965725388573
At time: 19.265974283218384 and batch: 550, loss is 4.951635904312134 and perplexity is 141.40610166178718
At time: 19.885135173797607 and batch: 600, loss is 4.8880602073669435 and perplexity is 132.69592166469135
At time: 20.503954648971558 and batch: 650, loss is 4.766798372268677 and perplexity is 117.54231216197492
At time: 21.12100625038147 and batch: 700, loss is 4.838768253326416 and perplexity is 126.31366942707375
At time: 21.73826265335083 and batch: 750, loss is 4.848692255020142 and perplexity is 127.5734471754943
At time: 22.360641717910767 and batch: 800, loss is 4.787770357131958 and perplexity is 120.03343837997937
At time: 22.98457384109497 and batch: 850, loss is 4.837668304443359 and perplexity is 126.17480723213141
At time: 23.632200956344604 and batch: 900, loss is 4.770509099960327 and perplexity is 117.97928992598604
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.849364816325984 and perplexity of 127.65927699939135
finished 2 epochs...
Completing Train Step...
At time: 25.23791003227234 and batch: 50, loss is 4.817110319137573 and perplexity is 123.6073883013006
At time: 25.85982584953308 and batch: 100, loss is 4.683219976425171 and perplexity is 108.11764895799163
At time: 26.484380960464478 and batch: 150, loss is 4.6961156845092775 and perplexity is 109.52093131046499
At time: 27.108454942703247 and batch: 200, loss is 4.596789751052857 and perplexity is 99.16545830194426
At time: 27.731658220291138 and batch: 250, loss is 4.729617280960083 and perplexity is 113.25221027617187
At time: 28.355597496032715 and batch: 300, loss is 4.688329334259033 and perplexity is 108.67147435591636
At time: 28.980173110961914 and batch: 350, loss is 4.679555883407593 and perplexity is 107.72222072069358
At time: 29.60455083847046 and batch: 400, loss is 4.583474979400635 and perplexity is 97.85384416741374
At time: 30.228451013565063 and batch: 450, loss is 4.600851535797119 and perplexity is 99.56906617689084
At time: 30.851603746414185 and batch: 500, loss is 4.509225130081177 and perplexity is 90.85139321714547
At time: 31.475306510925293 and batch: 550, loss is 4.60124828338623 and perplexity is 99.60857780139914
At time: 32.09899044036865 and batch: 600, loss is 4.571754655838013 and perplexity is 96.7136601664761
At time: 32.723055601119995 and batch: 650, loss is 4.4437510108947755 and perplexity is 85.09353054526679
At time: 33.34623956680298 and batch: 700, loss is 4.491272516250611 and perplexity is 89.23492655450599
At time: 33.96988606452942 and batch: 750, loss is 4.551825380325317 and perplexity is 94.80530620539511
At time: 34.59390306472778 and batch: 800, loss is 4.49239294052124 and perplexity is 89.33496356348678
At time: 35.21829581260681 and batch: 850, loss is 4.557638139724731 and perplexity is 95.35799139699174
At time: 35.841145515441895 and batch: 900, loss is 4.505018444061279 and perplexity is 90.47001266793859
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.673705636638484 and perplexity of 107.09385897475744
finished 3 epochs...
Completing Train Step...
At time: 37.417855739593506 and batch: 50, loss is 4.581440992355347 and perplexity is 97.65501299460918
At time: 38.049752712249756 and batch: 100, loss is 4.440155844688416 and perplexity is 84.78815442774065
At time: 38.668461084365845 and batch: 150, loss is 4.454636201858521 and perplexity is 86.02484947143408
At time: 39.301321268081665 and batch: 200, loss is 4.3600346994400025 and perplexity is 78.25984994893828
At time: 39.92074918746948 and batch: 250, loss is 4.504871339797973 and perplexity is 90.45670512219617
At time: 40.539400815963745 and batch: 300, loss is 4.471720571517944 and perplexity is 87.50715588110126
At time: 41.15899991989136 and batch: 350, loss is 4.463332681655884 and perplexity is 86.77622526366994
At time: 41.77848768234253 and batch: 400, loss is 4.38284658908844 and perplexity is 80.06562325352621
At time: 42.39734363555908 and batch: 450, loss is 4.4032305812835695 and perplexity is 81.71442781220877
At time: 43.01615881919861 and batch: 500, loss is 4.30266848564148 and perplexity is 73.89672317574473
At time: 43.63807129859924 and batch: 550, loss is 4.3957782649993895 and perplexity is 81.10772951282803
At time: 44.26966214179993 and batch: 600, loss is 4.385546579360962 and perplexity is 80.28209175739784
At time: 44.90149807929993 and batch: 650, loss is 4.252139525413513 and perplexity is 70.25556522870399
At time: 45.533559799194336 and batch: 700, loss is 4.283088860511779 and perplexity is 72.46392563008433
At time: 46.166627168655396 and batch: 750, loss is 4.367877831459046 and perplexity is 78.8760656555899
At time: 46.80099654197693 and batch: 800, loss is 4.309415764808655 and perplexity is 74.3970108889861
At time: 47.43536114692688 and batch: 850, loss is 4.379868602752685 and perplexity is 79.82754359644514
At time: 48.067843437194824 and batch: 900, loss is 4.33803964138031 and perplexity is 76.55731234985467
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.586273611408391 and perplexity of 98.12808463786644
finished 4 epochs...
Completing Train Step...
At time: 49.71462607383728 and batch: 50, loss is 4.423264818191528 and perplexity is 83.36802296800406
At time: 50.361419439315796 and batch: 100, loss is 4.278671317100525 and perplexity is 72.14451910837766
At time: 50.98898506164551 and batch: 150, loss is 4.292792248725891 and perplexity is 73.17049374038345
At time: 51.61796832084656 and batch: 200, loss is 4.197269291877746 and perplexity is 66.5044785411601
At time: 52.26220989227295 and batch: 250, loss is 4.349314203262329 and perplexity is 77.42534666370821
At time: 52.890974283218384 and batch: 300, loss is 4.318145999908447 and perplexity is 75.04935771201306
At time: 53.50851082801819 and batch: 350, loss is 4.310510568618774 and perplexity is 74.47850562220111
At time: 54.1286416053772 and batch: 400, loss is 4.238799004554749 and perplexity is 69.3245433613967
At time: 54.74930953979492 and batch: 450, loss is 4.261190962791443 and perplexity is 70.89436574794539
At time: 55.406736850738525 and batch: 500, loss is 4.155044975280762 and perplexity is 63.75483187479653
At time: 56.034125089645386 and batch: 550, loss is 4.2480812883377075 and perplexity is 69.97102923687555
At time: 56.659523010253906 and batch: 600, loss is 4.249951214790344 and perplexity is 70.10199232287248
At time: 57.28026294708252 and batch: 650, loss is 4.111000781059265 and perplexity is 61.00774242135479
At time: 57.901307821273804 and batch: 700, loss is 4.132046089172364 and perplexity is 62.305274751794975
At time: 58.52383470535278 and batch: 750, loss is 4.230140013694763 and perplexity is 68.72685420100909
At time: 59.16681480407715 and batch: 800, loss is 4.173382377624511 and perplexity is 64.93471481016196
At time: 59.79703211784363 and batch: 850, loss is 4.246262545585632 and perplexity is 69.84388559043589
At time: 60.43348431587219 and batch: 900, loss is 4.211986403465271 and perplexity is 67.49047004991183
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.535228990528681 and perplexity of 93.24486505083051
finished 5 epochs...
Completing Train Step...
At time: 62.073198080062866 and batch: 50, loss is 4.299687013626099 and perplexity is 73.67673027786515
At time: 62.693113803863525 and batch: 100, loss is 4.155537438392639 and perplexity is 63.786236509876275
At time: 63.31547665596008 and batch: 150, loss is 4.168871173858642 and perplexity is 64.6424408295868
At time: 63.93657851219177 and batch: 200, loss is 4.072719578742981 and perplexity is 58.716429605875256
At time: 64.55802822113037 and batch: 250, loss is 4.22821759223938 and perplexity is 68.59485913765421
At time: 65.1819577217102 and batch: 300, loss is 4.198113055229187 and perplexity is 66.5606162630054
At time: 65.80641579627991 and batch: 350, loss is 4.190158882141113 and perplexity is 66.03328163616844
At time: 66.42998456954956 and batch: 400, loss is 4.124133553504944 and perplexity is 61.81422732017603
At time: 67.04978656768799 and batch: 450, loss is 4.147319798469543 and perplexity is 63.2642120284706
At time: 67.6747236251831 and batch: 500, loss is 4.037828183174133 and perplexity is 56.70306031995791
At time: 68.29917621612549 and batch: 550, loss is 4.1322839641571045 and perplexity is 62.320097380972285
At time: 68.92212152481079 and batch: 600, loss is 4.14085738658905 and perplexity is 62.85669083621172
At time: 69.54345917701721 and batch: 650, loss is 3.9991206073760988 and perplexity is 54.55015792777099
At time: 70.16241121292114 and batch: 700, loss is 4.013267288208008 and perplexity is 55.327345955373794
At time: 70.7856719493866 and batch: 750, loss is 4.1185204124450685 and perplexity is 61.46822732457321
At time: 71.43462944030762 and batch: 800, loss is 4.064195899963379 and perplexity is 58.218076538798584
At time: 72.05793976783752 and batch: 850, loss is 4.138367381095886 and perplexity is 62.700372028858304
At time: 72.68516063690186 and batch: 900, loss is 4.109337339401245 and perplexity is 60.90634395974302
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.506308777691567 and perplexity of 90.5868245147365
finished 6 epochs...
Completing Train Step...
At time: 74.27960109710693 and batch: 50, loss is 4.197239708900452 and perplexity is 66.50251116978193
At time: 74.9055700302124 and batch: 100, loss is 4.054993772506714 and perplexity is 57.68480376331639
At time: 75.53231310844421 and batch: 150, loss is 4.0680622339248655 and perplexity is 58.443602764204655
At time: 76.15510082244873 and batch: 200, loss is 3.9704470682144164 and perplexity is 53.00822383506829
At time: 76.78331518173218 and batch: 250, loss is 4.12706353187561 and perplexity is 61.99560725911378
At time: 77.4103901386261 and batch: 300, loss is 4.098666577339173 and perplexity is 60.25988210968199
At time: 78.03958344459534 and batch: 350, loss is 4.09000717163086 and perplexity is 59.74032013813239
At time: 78.66412687301636 and batch: 400, loss is 4.028045163154602 and perplexity is 56.15103777221394
At time: 79.2879102230072 and batch: 450, loss is 4.051218495368958 and perplexity is 57.46743820824882
At time: 79.91198968887329 and batch: 500, loss is 3.939981389045715 and perplexity is 51.417644360193535
At time: 80.53574728965759 and batch: 550, loss is 4.035663785934449 and perplexity is 56.58046509296448
At time: 81.15890502929688 and batch: 600, loss is 4.048089365959168 and perplexity is 57.28789620881949
At time: 81.77826595306396 and batch: 650, loss is 3.9045576286315917 and perplexity is 49.62812099574087
At time: 82.39969778060913 and batch: 700, loss is 3.914775137901306 and perplexity is 50.137796153317105
At time: 83.02305269241333 and batch: 750, loss is 4.025349259376526 and perplexity is 55.99986384408261
At time: 83.6512062549591 and batch: 800, loss is 3.9726773500442505 and perplexity is 53.12657904719918
At time: 84.27988839149475 and batch: 850, loss is 4.046986389160156 and perplexity is 57.22474382264772
At time: 84.90758895874023 and batch: 900, loss is 4.021926455497741 and perplexity is 55.80851495457995
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.491432921527183 and perplexity of 89.24924145564204
finished 7 epochs...
Completing Train Step...
At time: 86.55623960494995 and batch: 50, loss is 4.109408478736878 and perplexity is 60.910676950709366
At time: 87.20613813400269 and batch: 100, loss is 3.969128351211548 and perplexity is 52.93836705978728
At time: 87.83514595031738 and batch: 150, loss is 3.9812344884872437 and perplexity is 53.58314118586057
At time: 88.46661448478699 and batch: 200, loss is 3.8835364246368407 and perplexity is 48.59576681966803
At time: 89.1007182598114 and batch: 250, loss is 4.039066681861877 and perplexity is 56.77333049153074
At time: 89.72601079940796 and batch: 300, loss is 4.013363637924194 and perplexity is 55.332676986271494
At time: 90.36121439933777 and batch: 350, loss is 4.003242435455323 and perplexity is 54.77546832681657
At time: 90.99230408668518 and batch: 400, loss is 3.944784097671509 and perplexity is 51.66518227470971
At time: 91.6274938583374 and batch: 450, loss is 3.9675256061553954 and perplexity is 52.85358831120975
At time: 92.26695084571838 and batch: 500, loss is 3.8554111909866333 and perplexity is 47.248040916080136
At time: 92.90407347679138 and batch: 550, loss is 3.9516128063201905 and perplexity is 52.01919610472121
At time: 93.53001141548157 and batch: 600, loss is 3.967483696937561 and perplexity is 52.851373305078795
At time: 94.16719818115234 and batch: 650, loss is 3.822478232383728 and perplexity is 45.717366304719214
At time: 94.80263066291809 and batch: 700, loss is 3.8302030611038207 and perplexity is 46.0718926938044
At time: 95.43381524085999 and batch: 750, loss is 3.9430066394805907 and perplexity is 51.57343113935
At time: 96.06552267074585 and batch: 800, loss is 3.8923888540267946 and perplexity is 49.02786716116122
At time: 96.69429445266724 and batch: 850, loss is 3.9665059661865234 and perplexity is 52.799724145764046
At time: 97.32894945144653 and batch: 900, loss is 3.9447023630142213 and perplexity is 51.660959611314105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.486140839041096 and perplexity of 88.77817466957845
finished 8 epochs...
Completing Train Step...
At time: 99.00301933288574 and batch: 50, loss is 4.032135620117187 and perplexity is 56.38119157182378
At time: 99.6612195968628 and batch: 100, loss is 3.8936315631866454 and perplexity is 49.08883241396354
At time: 100.29737758636475 and batch: 150, loss is 3.9039491033554077 and perplexity is 49.597930216563206
At time: 100.94005274772644 and batch: 200, loss is 3.807133870124817 and perplexity is 45.02121711622539
At time: 101.57684564590454 and batch: 250, loss is 3.961265196800232 and perplexity is 52.52373679257314
At time: 102.21225547790527 and batch: 300, loss is 3.937385668754578 and perplexity is 51.284351607597834
At time: 102.84625959396362 and batch: 350, loss is 3.926210298538208 and perplexity is 50.71442051902595
At time: 103.49117732048035 and batch: 400, loss is 3.870535011291504 and perplexity is 47.9680426613823
At time: 104.12223935127258 and batch: 450, loss is 3.8928790426254274 and perplexity is 49.05190595394863
At time: 104.7569329738617 and batch: 500, loss is 3.781092495918274 and perplexity is 43.86393674010975
At time: 105.38468933105469 and batch: 550, loss is 3.8763964891433718 and perplexity is 48.25003191055223
At time: 106.01335549354553 and batch: 600, loss is 3.8956016063690186 and perplexity is 49.18563485477752
At time: 106.64824438095093 and batch: 650, loss is 3.7499161624908446 and perplexity is 42.517517287891685
At time: 107.28402352333069 and batch: 700, loss is 3.755175576210022 and perplexity is 42.74172358162755
At time: 107.91937136650085 and batch: 750, loss is 3.869327163696289 and perplexity is 47.91013955251153
At time: 108.54844164848328 and batch: 800, loss is 3.8213703870773315 and perplexity is 45.666746579627365
At time: 109.1764976978302 and batch: 850, loss is 3.894096336364746 and perplexity is 49.11165288937547
At time: 109.80223846435547 and batch: 900, loss is 3.875193405151367 and perplexity is 48.1920179743634
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.485883320847603 and perplexity of 88.75531561785311
finished 9 epochs...
Completing Train Step...
At time: 111.41375851631165 and batch: 50, loss is 3.962512421607971 and perplexity is 52.58928656925252
At time: 112.03726029396057 and batch: 100, loss is 3.825226216316223 and perplexity is 45.84316966640273
At time: 112.66827058792114 and batch: 150, loss is 3.834602508544922 and perplexity is 46.27503008270985
At time: 113.30299973487854 and batch: 200, loss is 3.7386947441101075 and perplexity is 42.04307736080816
At time: 113.9454174041748 and batch: 250, loss is 3.8913109874725342 and perplexity is 48.97505013288917
At time: 114.5850145816803 and batch: 300, loss is 3.8690512132644654 and perplexity is 47.89692055279151
At time: 115.22487878799438 and batch: 350, loss is 3.856334547996521 and perplexity is 47.29168787362449
At time: 115.8548858165741 and batch: 400, loss is 3.803333387374878 and perplexity is 44.850439481475966
At time: 116.48667097091675 and batch: 450, loss is 3.8255372190475465 and perplexity is 45.85742923464891
At time: 117.11876225471497 and batch: 500, loss is 3.714800434112549 and perplexity is 41.05039400858017
At time: 117.75053358078003 and batch: 550, loss is 3.8082429456710813 and perplexity is 45.07117674657315
At time: 118.37220788002014 and batch: 600, loss is 3.8304472303390504 and perplexity is 46.08314340609204
At time: 119.00212144851685 and batch: 650, loss is 3.684819736480713 and perplexity is 39.83794047520563
At time: 119.6520447731018 and batch: 700, loss is 3.6884923505783083 and perplexity is 39.984518855171146
At time: 120.2790310382843 and batch: 750, loss is 3.8024888610839844 and perplexity is 44.81257809589381
At time: 120.90875005722046 and batch: 800, loss is 3.7570518970489504 and perplexity is 42.82199605318059
At time: 121.53429293632507 and batch: 850, loss is 3.8289442443847657 and perplexity is 46.01393311290132
At time: 122.15870761871338 and batch: 900, loss is 3.811226739883423 and perplexity is 45.205860697565164
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4903806921553935 and perplexity of 89.15538017283195
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 123.79785871505737 and batch: 50, loss is 3.9145796203613283 and perplexity is 50.12799429300234
At time: 124.42400336265564 and batch: 100, loss is 3.7774588584899904 and perplexity is 43.704840322218445
At time: 125.05047512054443 and batch: 150, loss is 3.787109227180481 and perplexity is 44.128649814719296
At time: 125.67560291290283 and batch: 200, loss is 3.6774391174316405 and perplexity is 39.544994205086404
At time: 126.30107045173645 and batch: 250, loss is 3.8260908126831055 and perplexity is 45.88282264378263
At time: 126.92569518089294 and batch: 300, loss is 3.7964511156082152 and perplexity is 44.5428263209858
At time: 127.55186605453491 and batch: 350, loss is 3.7716885089874266 and perplexity is 43.45337433966262
At time: 128.17666840553284 and batch: 400, loss is 3.718620595932007 and perplexity is 41.207513075497566
At time: 128.80205011367798 and batch: 450, loss is 3.7252522802352903 and perplexity is 41.48169643702126
At time: 129.42761278152466 and batch: 500, loss is 3.6095036697387695 and perplexity is 36.9477099965967
At time: 130.05669689178467 and batch: 550, loss is 3.6859532165527344 and perplexity is 39.883121587952814
At time: 130.68184566497803 and batch: 600, loss is 3.7032815551757814 and perplexity is 40.580252439961
At time: 131.30587887763977 and batch: 650, loss is 3.546523756980896 and perplexity is 34.692508038873044
At time: 131.9311866760254 and batch: 700, loss is 3.5415141868591307 and perplexity is 34.51914807914002
At time: 132.56627440452576 and batch: 750, loss is 3.6428029346466064 and perplexity is 38.19875542737654
At time: 133.20163917541504 and batch: 800, loss is 3.5843851566314697 and perplexity is 36.031197364224205
At time: 133.82724952697754 and batch: 850, loss is 3.6435774898529054 and perplexity is 38.22835393362575
At time: 134.45385003089905 and batch: 900, loss is 3.616816854476929 and perplexity is 37.21890586952433
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455657122886344 and perplexity of 86.11271889541408
finished 11 epochs...
Completing Train Step...
At time: 136.06468844413757 and batch: 50, loss is 3.8584638357162477 and perplexity is 47.39249276708349
At time: 136.72217392921448 and batch: 100, loss is 3.7193415689468385 and perplexity is 41.2372332928833
At time: 137.3541238307953 and batch: 150, loss is 3.7295826196670534 and perplexity is 41.661715753864335
At time: 137.9868187904358 and batch: 200, loss is 3.624211664199829 and perplexity is 37.49515273418671
At time: 138.61536049842834 and batch: 250, loss is 3.7743817949295044 and perplexity is 43.57056444429882
At time: 139.24409985542297 and batch: 300, loss is 3.747463641166687 and perplexity is 42.41336993409856
At time: 139.8667278289795 and batch: 350, loss is 3.7264731168746947 and perplexity is 41.53236973751337
At time: 140.49054646492004 and batch: 400, loss is 3.676006417274475 and perplexity is 39.48837865191386
At time: 141.1235818862915 and batch: 450, loss is 3.6870685768127442 and perplexity is 39.92763045391312
At time: 141.76253867149353 and batch: 500, loss is 3.573776273727417 and perplexity is 35.65096708592949
At time: 142.40177249908447 and batch: 550, loss is 3.653492736816406 and perplexity is 38.60928288537915
At time: 143.03568291664124 and batch: 600, loss is 3.6759048891067505 and perplexity is 39.48436967269852
At time: 143.6810839176178 and batch: 650, loss is 3.522910966873169 and perplexity is 33.882917094960895
At time: 144.31087827682495 and batch: 700, loss is 3.521051592826843 and perplexity is 33.819974613243275
At time: 144.9457552433014 and batch: 750, loss is 3.6269909858703615 and perplexity is 37.59950877705048
At time: 145.57793760299683 and batch: 800, loss is 3.573784308433533 and perplexity is 35.651253532123526
At time: 146.20956563949585 and batch: 850, loss is 3.6384737300872803 and perplexity is 38.03374264598508
At time: 146.8447060585022 and batch: 900, loss is 3.616708927154541 and perplexity is 37.21488914943245
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45851072546554 and perplexity of 86.35880131588205
Annealing...
finished 12 epochs...
Completing Train Step...
At time: 148.47947573661804 and batch: 50, loss is 3.8403308057785033 and perplexity is 46.540867881560864
At time: 149.13239884376526 and batch: 100, loss is 3.7059322357177735 and perplexity is 40.6879604121418
At time: 149.76033997535706 and batch: 150, loss is 3.719686212539673 and perplexity is 41.25144789046819
At time: 150.38598585128784 and batch: 200, loss is 3.609500708580017 and perplexity is 36.947600588723844
At time: 151.01953434944153 and batch: 250, loss is 3.760160222053528 and perplexity is 42.95530781506539
At time: 151.67849016189575 and batch: 300, loss is 3.732009596824646 and perplexity is 41.762950583963914
At time: 152.31035041809082 and batch: 350, loss is 3.707585349082947 and perplexity is 40.755277849648245
At time: 152.9420132637024 and batch: 400, loss is 3.658036231994629 and perplexity is 38.78510309267584
At time: 153.57947874069214 and batch: 450, loss is 3.663659973144531 and perplexity is 39.00383494211662
At time: 154.20652842521667 and batch: 500, loss is 3.5500370597839357 and perplexity is 34.814607685533275
At time: 154.83290147781372 and batch: 550, loss is 3.6232179260253905 and perplexity is 37.4579108769507
At time: 155.46979999542236 and batch: 600, loss is 3.646119713783264 and perplexity is 38.32566260762584
At time: 156.09864592552185 and batch: 650, loss is 3.4886506938934327 and perplexity is 32.741739258853535
At time: 156.728182554245 and batch: 700, loss is 3.483616943359375 and perplexity is 32.57733963186687
At time: 157.35154509544373 and batch: 750, loss is 3.5859522867202758 and perplexity is 36.08770720532034
At time: 157.97887992858887 and batch: 800, loss is 3.529218225479126 and perplexity is 34.09730079140171
At time: 158.60036730766296 and batch: 850, loss is 3.5899848794937133 and perplexity is 36.2335280530457
At time: 159.2233281135559 and batch: 900, loss is 3.5676830768585206 and perplexity is 35.434399190363436
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452538059182363 and perplexity of 85.8445462806019
finished 13 epochs...
Completing Train Step...
At time: 160.88391041755676 and batch: 50, loss is 3.82733021736145 and perplexity is 45.93972528423826
At time: 161.5217809677124 and batch: 100, loss is 3.6897884893417356 and perplexity is 40.03637794102186
At time: 162.1523265838623 and batch: 150, loss is 3.7024250555038454 and perplexity is 40.54551034747838
At time: 162.7818319797516 and batch: 200, loss is 3.5939469718933106 and perplexity is 36.37737341528773
At time: 163.41705799102783 and batch: 250, loss is 3.745050144195557 and perplexity is 42.31112882316223
At time: 164.04487919807434 and batch: 300, loss is 3.7174669027328493 and perplexity is 41.15999966112974
At time: 164.66628885269165 and batch: 350, loss is 3.694339995384216 and perplexity is 40.21901908797891
At time: 165.2905900478363 and batch: 400, loss is 3.6452384185791016 and perplexity is 38.29190126401364
At time: 165.91302704811096 and batch: 450, loss is 3.652308392524719 and perplexity is 38.56358326896943
At time: 166.53618335723877 and batch: 500, loss is 3.5400662136077883 and perplexity is 34.469201445483684
At time: 167.16360926628113 and batch: 550, loss is 3.6150645923614504 and perplexity is 37.153745696313514
At time: 167.80066180229187 and batch: 600, loss is 3.6395664978027344 and perplexity is 38.07532740915165
At time: 168.42878341674805 and batch: 650, loss is 3.4835732078552244 and perplexity is 32.57591487665061
At time: 169.05736207962036 and batch: 700, loss is 3.4798318004608153 and perplexity is 32.45426282470321
At time: 169.6865119934082 and batch: 750, loss is 3.5842891216278074 and perplexity is 36.02773727420091
At time: 170.31206893920898 and batch: 800, loss is 3.5292962408065796 and perplexity is 34.09996100725569
At time: 170.93929243087769 and batch: 850, loss is 3.5922790956497193 and perplexity is 36.31675102773747
At time: 171.5607626438141 and batch: 900, loss is 3.5716056776046754 and perplexity is 35.57366715881163
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452742485150899 and perplexity of 85.86209692896188
Annealing...
finished 14 epochs...
Completing Train Step...
At time: 173.17335081100464 and batch: 50, loss is 3.8222498178482054 and perplexity is 45.70692498624925
At time: 173.7966787815094 and batch: 100, loss is 3.686168975830078 and perplexity is 39.89172766983245
At time: 174.42593455314636 and batch: 150, loss is 3.700216212272644 and perplexity is 40.45605050914372
At time: 175.05141496658325 and batch: 200, loss is 3.5899434375762937 and perplexity is 36.2320264972822
At time: 175.67170572280884 and batch: 250, loss is 3.74094612121582 and perplexity is 42.13783881448922
At time: 176.29670786857605 and batch: 300, loss is 3.7128286933898926 and perplexity is 40.96953301968206
At time: 176.92181587219238 and batch: 350, loss is 3.689337577819824 and perplexity is 40.018329146423184
At time: 177.54913330078125 and batch: 400, loss is 3.639999475479126 and perplexity is 38.09181674544076
At time: 178.17934441566467 and batch: 450, loss is 3.646043701171875 and perplexity is 38.32274948464625
At time: 178.81144332885742 and batch: 500, loss is 3.532540225982666 and perplexity is 34.21076039388635
At time: 179.44036054611206 and batch: 550, loss is 3.6075711059570312 and perplexity is 36.876375142218876
At time: 180.06483840942383 and batch: 600, loss is 3.63254469871521 and perplexity is 37.80890658132611
At time: 180.6926589012146 and batch: 650, loss is 3.4744465446472166 and perplexity is 32.27995807623095
At time: 181.32133197784424 and batch: 700, loss is 3.4697705459594728 and perplexity is 32.12936938527132
At time: 181.94336223602295 and batch: 750, loss is 3.5735302448272703 and perplexity is 35.64219699660013
At time: 182.56695342063904 and batch: 800, loss is 3.517714581489563 and perplexity is 33.707305068859824
At time: 183.1895661354065 and batch: 850, loss is 3.578419680595398 and perplexity is 35.81689396622207
At time: 183.83718967437744 and batch: 900, loss is 3.5582089376449586 and perplexity is 35.10027403284687
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452616652397261 and perplexity of 85.8512933446088
Annealing...
finished 15 epochs...
Completing Train Step...
At time: 185.46164751052856 and batch: 50, loss is 3.819160671234131 and perplexity is 45.56594745592878
At time: 186.08502578735352 and batch: 100, loss is 3.682851209640503 and perplexity is 39.75959555745771
At time: 186.70929193496704 and batch: 150, loss is 3.69753782749176 and perplexity is 40.347838620393695
At time: 187.3340196609497 and batch: 200, loss is 3.587695350646973 and perplexity is 36.15066523994637
At time: 187.9605746269226 and batch: 250, loss is 3.7389370250701903 and perplexity is 42.0532648320213
At time: 188.58841490745544 and batch: 300, loss is 3.7101784563064575 and perplexity is 40.86109779697501
At time: 189.21440291404724 and batch: 350, loss is 3.6869662857055663 and perplexity is 39.923546421270665
At time: 189.84832739830017 and batch: 400, loss is 3.637294373512268 and perplexity is 37.9889137413509
At time: 190.47593688964844 and batch: 450, loss is 3.643919997215271 and perplexity is 38.24144966886398
At time: 191.10144519805908 and batch: 500, loss is 3.5301465225219726 and perplexity is 34.12896791086804
At time: 191.72365379333496 and batch: 550, loss is 3.605086965560913 and perplexity is 36.784882736106276
At time: 192.3500461578369 and batch: 600, loss is 3.6305021142959593 and perplexity is 37.731757516396314
At time: 192.97715616226196 and batch: 650, loss is 3.4722818422317503 and perplexity is 32.21015714938654
At time: 193.60326623916626 and batch: 700, loss is 3.4672189140319825 and perplexity is 32.04749156599099
At time: 194.22944688796997 and batch: 750, loss is 3.571118311882019 and perplexity is 35.55633399694681
At time: 194.85290718078613 and batch: 800, loss is 3.5154559993743897 and perplexity is 33.631260261528105
At time: 195.47807621955872 and batch: 850, loss is 3.5754411315917967 and perplexity is 35.71037031410012
At time: 196.10239720344543 and batch: 900, loss is 3.554797496795654 and perplexity is 34.980735539370976
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452559797731165 and perplexity of 85.8464124367443
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 197.68669438362122 and batch: 50, loss is 3.8186015939712523 and perplexity is 45.54047969063227
At time: 198.33874130249023 and batch: 100, loss is 3.682233281135559 and perplexity is 39.73503455926967
At time: 198.96527361869812 and batch: 150, loss is 3.6969367837905884 and perplexity is 40.3235950925747
At time: 199.6048595905304 and batch: 200, loss is 3.587184777259827 and perplexity is 36.1322123835168
At time: 200.23016595840454 and batch: 250, loss is 3.7384461975097656 and perplexity is 42.03262899536887
At time: 200.85014986991882 and batch: 300, loss is 3.7096285724639895 and perplexity is 40.83863511600961
At time: 201.47026109695435 and batch: 350, loss is 3.686438341140747 and perplexity is 39.902474564804734
At time: 202.0912709236145 and batch: 400, loss is 3.636732382774353 and perplexity is 37.96757032164865
At time: 202.71540522575378 and batch: 450, loss is 3.643460097312927 and perplexity is 38.22386647346062
At time: 203.33873653411865 and batch: 500, loss is 3.5296332168579103 and perplexity is 34.111453813757976
At time: 203.96247935295105 and batch: 550, loss is 3.6045384073257445 and perplexity is 36.76470961932221
At time: 204.58735489845276 and batch: 600, loss is 3.630051875114441 and perplexity is 37.714773024603645
At time: 205.2139880657196 and batch: 650, loss is 3.471787657737732 and perplexity is 32.19424332168073
At time: 205.84015369415283 and batch: 700, loss is 3.4666444063186646 and perplexity is 32.02908532266448
At time: 206.46688675880432 and batch: 750, loss is 3.570568141937256 and perplexity is 35.53677735086847
At time: 207.0921595096588 and batch: 800, loss is 3.514956068992615 and perplexity is 33.614451174782666
At time: 207.71088552474976 and batch: 850, loss is 3.5748218059539796 and perplexity is 35.68826081342566
At time: 208.34190487861633 and batch: 900, loss is 3.5540784978866578 and perplexity is 34.95559346832622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452539731378424 and perplexity of 85.84468982963412
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 209.95271396636963 and batch: 50, loss is 3.818487229347229 and perplexity is 45.535271768601305
At time: 210.58851504325867 and batch: 100, loss is 3.682104272842407 and perplexity is 39.729908740926504
At time: 211.2090027332306 and batch: 150, loss is 3.696813282966614 and perplexity is 40.318615402859365
At time: 211.84276008605957 and batch: 200, loss is 3.587082962989807 and perplexity is 36.128533795958305
At time: 212.4742317199707 and batch: 250, loss is 3.7383425045013428 and perplexity is 42.02827073158106
At time: 213.10269498825073 and batch: 300, loss is 3.709511513710022 and perplexity is 40.83385487605913
At time: 213.7254798412323 and batch: 350, loss is 3.686330237388611 and perplexity is 39.89816119073492
At time: 214.34739351272583 and batch: 400, loss is 3.63661470413208 and perplexity is 37.96310261240493
At time: 214.97324514389038 and batch: 450, loss is 3.643362741470337 and perplexity is 38.22014533787312
At time: 215.62095522880554 and batch: 500, loss is 3.529526462554932 and perplexity is 34.10781246365086
At time: 216.24505472183228 and batch: 550, loss is 3.6044224309921264 and perplexity is 36.760446030336524
At time: 216.86948466300964 and batch: 600, loss is 3.629955863952637 and perplexity is 37.711152159252904
At time: 217.48994064331055 and batch: 650, loss is 3.471684021949768 and perplexity is 32.19090701878937
At time: 218.10998916625977 and batch: 700, loss is 3.466524143218994 and perplexity is 32.02523363719642
At time: 218.7292242050171 and batch: 750, loss is 3.5704508018493653 and perplexity is 35.532607706928744
At time: 219.34889793395996 and batch: 800, loss is 3.5148501586914063 and perplexity is 33.61089124665364
At time: 219.96906638145447 and batch: 850, loss is 3.5746920204162596 and perplexity is 35.68362929386433
At time: 220.59107851982117 and batch: 900, loss is 3.553928589820862 and perplexity is 34.950353735669545
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452538059182363 and perplexity of 85.8445462806019
finished 18 epochs...
Completing Train Step...
At time: 222.16389989852905 and batch: 50, loss is 3.818457832336426 and perplexity is 45.53393318740044
At time: 222.78876543045044 and batch: 100, loss is 3.682068347930908 and perplexity is 39.72848147310851
At time: 223.41358518600464 and batch: 150, loss is 3.696771478652954 and perplexity is 40.316929946044674
At time: 224.03595328330994 and batch: 200, loss is 3.587048444747925 and perplexity is 36.12728672401337
At time: 224.6619439125061 and batch: 250, loss is 3.738303470611572 and perplexity is 42.02663023671173
At time: 225.28607630729675 and batch: 300, loss is 3.7094780445098876 and perplexity is 40.83248822246855
At time: 225.91036248207092 and batch: 350, loss is 3.6862977361679077 and perplexity is 39.896864472864976
At time: 226.5308198928833 and batch: 400, loss is 3.636586923599243 and perplexity is 37.96204799183525
At time: 227.1641411781311 and batch: 450, loss is 3.643339977264404 and perplexity is 38.219275296516805
At time: 227.8048357963562 and batch: 500, loss is 3.529505295753479 and perplexity is 34.107090517997115
At time: 228.4423439502716 and batch: 550, loss is 3.604400134086609 and perplexity is 36.7596263952823
At time: 229.07834243774414 and batch: 600, loss is 3.629937171936035 and perplexity is 37.71044726835861
At time: 229.7093222141266 and batch: 650, loss is 3.471673264503479 and perplexity is 32.19056072869873
At time: 230.33668279647827 and batch: 700, loss is 3.466518225669861 and perplexity is 32.02504412686359
At time: 230.96166825294495 and batch: 750, loss is 3.5704481029510498 and perplexity is 35.53251180816307
At time: 231.61495971679688 and batch: 800, loss is 3.514849443435669 and perplexity is 33.61086720627943
At time: 232.24565649032593 and batch: 850, loss is 3.57470911026001 and perplexity is 35.684239126724364
At time: 232.87813568115234 and batch: 900, loss is 3.553958578109741 and perplexity is 34.95140185268934
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452533878692209 and perplexity of 85.84418740907152
finished 19 epochs...
Completing Train Step...
At time: 234.4760081768036 and batch: 50, loss is 3.8184286308288575 and perplexity is 45.532603547319695
At time: 235.09715151786804 and batch: 100, loss is 3.6820327615737916 and perplexity is 39.72706770633466
At time: 235.7168731689453 and batch: 150, loss is 3.696730260848999 and perplexity is 40.315268204976974
At time: 236.3352346420288 and batch: 200, loss is 3.587014317512512 and perplexity is 36.126053820632436
At time: 236.9553894996643 and batch: 250, loss is 3.7382648611068725 and perplexity is 42.02500764065812
At time: 237.57300424575806 and batch: 300, loss is 3.709444622993469 and perplexity is 40.831123561597664
At time: 238.1917703151703 and batch: 350, loss is 3.686265435218811 and perplexity is 39.89557578708952
At time: 238.81005191802979 and batch: 400, loss is 3.6365593004226686 and perplexity is 37.96099937396358
At time: 239.42755913734436 and batch: 450, loss is 3.6433172035217285 and perplexity is 38.21840491048696
At time: 240.0464107990265 and batch: 500, loss is 3.529484362602234 and perplexity is 34.10637655658554
At time: 240.6636302471161 and batch: 550, loss is 3.604378318786621 and perplexity is 36.75882448175207
At time: 241.2810137271881 and batch: 600, loss is 3.62991858959198 and perplexity is 37.70974652636373
At time: 241.89826941490173 and batch: 650, loss is 3.4716627645492553 and perplexity is 32.190222731059116
At time: 242.5161395072937 and batch: 700, loss is 3.466512417793274 and perplexity is 32.02485812989974
At time: 243.13525700569153 and batch: 750, loss is 3.570445384979248 and perplexity is 35.53241523192917
At time: 243.75318694114685 and batch: 800, loss is 3.5148488998413088 and perplexity is 33.61084893560655
At time: 244.37124824523926 and batch: 850, loss is 3.5747258996963502 and perplexity is 35.68483825001499
At time: 244.9904797077179 and batch: 900, loss is 3.5539881944656373 and perplexity is 34.95243700117426
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452531370398116 and perplexity of 85.84397208687342
finished 20 epochs...
Completing Train Step...
At time: 246.57039093971252 and batch: 50, loss is 3.8183995962142943 and perplexity is 45.53128154491764
At time: 247.2015552520752 and batch: 100, loss is 3.6819974088668825 and perplexity is 39.72566327177911
At time: 247.8195276260376 and batch: 150, loss is 3.6966893815994264 and perplexity is 40.31362018075166
At time: 248.43747186660767 and batch: 200, loss is 3.5869804096221922 and perplexity is 36.12482888312943
At time: 249.05431723594666 and batch: 250, loss is 3.738226656913757 and perplexity is 42.02340213981917
At time: 249.67148327827454 and batch: 300, loss is 3.7094114446640014 and perplexity is 40.82976887560083
At time: 250.28995895385742 and batch: 350, loss is 3.6862334299087522 and perplexity is 39.89429893724958
At time: 250.90772199630737 and batch: 400, loss is 3.6365318059921266 and perplexity is 37.95995567225104
At time: 251.52504706382751 and batch: 450, loss is 3.6432945251464846 and perplexity is 38.217538188987135
At time: 252.1426887512207 and batch: 500, loss is 3.5294634342193603 and perplexity is 34.10566277274772
At time: 252.76123309135437 and batch: 550, loss is 3.60435649394989 and perplexity is 36.75802223516381
At time: 253.38003993034363 and batch: 600, loss is 3.629900212287903 and perplexity is 37.70905352925288
At time: 253.99838709831238 and batch: 650, loss is 3.4716523408889772 and perplexity is 32.18988719286187
At time: 254.61569738388062 and batch: 700, loss is 3.466506686210632 and perplexity is 32.0246745773048
At time: 255.23363065719604 and batch: 750, loss is 3.5704427766799927 and perplexity is 35.53232255287785
At time: 255.85261464118958 and batch: 800, loss is 3.5148486375808714 and perplexity is 33.61084012081176
At time: 256.46985840797424 and batch: 850, loss is 3.57474289894104 and perplexity is 35.685444870468146
At time: 257.0870442390442 and batch: 900, loss is 3.5540173768997194 and perplexity is 34.95345701324621
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452528862104024 and perplexity of 85.84375676521542
finished 21 epochs...
Completing Train Step...
At time: 258.6437873840332 and batch: 50, loss is 3.818370752334595 and perplexity is 45.529968265050336
At time: 259.2752854824066 and batch: 100, loss is 3.6819624423980715 and perplexity is 39.724274229898406
At time: 259.8933439254761 and batch: 150, loss is 3.6966489696502687 and perplexity is 40.311991061700716
At time: 260.5115282535553 and batch: 200, loss is 3.586946716308594 and perplexity is 36.12361173844612
At time: 261.1302661895752 and batch: 250, loss is 3.738188920021057 and perplexity is 42.021816337123546
At time: 261.74816489219666 and batch: 300, loss is 3.7093784475326537 and perplexity is 40.82842163258204
At time: 262.36522674560547 and batch: 350, loss is 3.6862014389038085 and perplexity is 39.89302269894923
At time: 262.9964635372162 and batch: 400, loss is 3.636504430770874 and perplexity is 37.9589165242893
At time: 263.61553621292114 and batch: 450, loss is 3.6432717180252077 and perplexity is 38.21666656689839
At time: 264.23628640174866 and batch: 500, loss is 3.529442591667175 and perplexity is 34.10495193109947
At time: 264.85532903671265 and batch: 550, loss is 3.604334864616394 and perplexity is 36.75722719224039
At time: 265.4740107059479 and batch: 600, loss is 3.6298818445205687 and perplexity is 37.70836090449226
At time: 266.09278321266174 and batch: 650, loss is 3.4716420125961305 and perplexity is 32.189554727997134
At time: 266.71047019958496 and batch: 700, loss is 3.4665010452270506 and perplexity is 32.02449392715082
At time: 267.3279311656952 and batch: 750, loss is 3.5704404306411743 and perplexity is 35.53223919276762
At time: 267.94761991500854 and batch: 800, loss is 3.5148485279083252 and perplexity is 33.61083643462555
At time: 268.5662908554077 and batch: 850, loss is 3.574759740829468 and perplexity is 35.68604588581026
At time: 269.1871988773346 and batch: 900, loss is 3.5540462064743044 and perplexity is 34.95446472106799
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452525935760916 and perplexity of 85.84350555729705
finished 22 epochs...
Completing Train Step...
At time: 270.7614977359772 and batch: 50, loss is 3.818342032432556 and perplexity is 45.52866066759906
At time: 271.3785676956177 and batch: 100, loss is 3.681927676200867 and perplexity is 39.722893191953574
At time: 271.99474811553955 and batch: 150, loss is 3.6966089487075804 and perplexity is 40.31037777009973
At time: 272.61157846450806 and batch: 200, loss is 3.586913423538208 and perplexity is 36.12240910335464
At time: 273.2282898426056 and batch: 250, loss is 3.7381516885757446 and perplexity is 42.020251833291205
At time: 273.84494042396545 and batch: 300, loss is 3.709345326423645 and perplexity is 40.8270693723728
At time: 274.46209836006165 and batch: 350, loss is 3.686169605255127 and perplexity is 39.891752778693
At time: 275.0783095359802 and batch: 400, loss is 3.636477146148682 and perplexity is 37.95788084372224
At time: 275.69494438171387 and batch: 450, loss is 3.64324912071228 and perplexity is 38.215802982682305
At time: 276.3120355606079 and batch: 500, loss is 3.5294217824935914 and perplexity is 34.10424224261871
At time: 276.9285168647766 and batch: 550, loss is 3.6043133544921875 and perplexity is 36.75643654822145
At time: 277.545134305954 and batch: 600, loss is 3.6298635625839233 and perplexity is 37.70767152892878
At time: 278.1624343395233 and batch: 650, loss is 3.4716317558288576 and perplexity is 32.18922456891886
At time: 278.80403685569763 and batch: 700, loss is 3.4664955949783325 and perplexity is 32.024319386169495
At time: 279.42063665390015 and batch: 750, loss is 3.5704378652572633 and perplexity is 35.532148039049794
At time: 280.03648567199707 and batch: 800, loss is 3.5148486852645875 and perplexity is 33.61084172350155
At time: 280.65177512168884 and batch: 850, loss is 3.5747764778137205 and perplexity is 35.68664316759662
At time: 281.26898097991943 and batch: 900, loss is 3.5540747594833375 and perplexity is 34.9554627904638
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452523009417808 and perplexity of 85.84325435011374
finished 23 epochs...
Completing Train Step...
At time: 282.86326694488525 and batch: 50, loss is 3.81831356048584 and perplexity is 45.527364396452235
At time: 283.49369525909424 and batch: 100, loss is 3.6818934059143067 and perplexity is 39.721531900346946
At time: 284.12228536605835 and batch: 150, loss is 3.6965694046020507 and perplexity is 40.30878376378423
At time: 284.745712518692 and batch: 200, loss is 3.586880307197571 and perplexity is 36.1212128811575
At time: 285.37135887145996 and batch: 250, loss is 3.73811487197876 and perplexity is 42.018704819092335
At time: 285.99675846099854 and batch: 300, loss is 3.7093125915527345 and perplexity is 40.825732925401574
At time: 286.62384128570557 and batch: 350, loss is 3.6861381101608277 and perplexity is 39.89049640396239
At time: 287.254163980484 and batch: 400, loss is 3.6364501333236694 and perplexity is 37.95685550797784
At time: 287.8851397037506 and batch: 450, loss is 3.6432264947891237 and perplexity is 38.21493832464254
At time: 288.51431345939636 and batch: 500, loss is 3.5294011211395264 and perplexity is 34.10353761007398
At time: 289.14267134666443 and batch: 550, loss is 3.604292044639587 and perplexity is 36.755653282322164
At time: 289.7715106010437 and batch: 600, loss is 3.629845390319824 and perplexity is 37.70698630138927
At time: 290.4015245437622 and batch: 650, loss is 3.4716216373443602 and perplexity is 32.1888988643969
At time: 291.03034567832947 and batch: 700, loss is 3.46649010181427 and perplexity is 32.02414347181229
At time: 291.6603510379791 and batch: 750, loss is 3.570435643196106 and perplexity is 35.53206908453152
At time: 292.29148626327515 and batch: 800, loss is 3.5148489618301393 and perplexity is 33.610851019103826
At time: 292.92196702957153 and batch: 850, loss is 3.5747932767868043 and perplexity is 35.687242671590155
At time: 293.5523302555084 and batch: 900, loss is 3.554102911949158 and perplexity is 34.956446886787546
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452519665025685 and perplexity of 85.84296725709014
finished 24 epochs...
Completing Train Step...
At time: 295.19873452186584 and batch: 50, loss is 3.8182851552963255 and perplexity is 45.52607120140528
At time: 295.82765650749207 and batch: 100, loss is 3.6818592166900634 and perplexity is 39.720173875200565
At time: 296.45805954933167 and batch: 150, loss is 3.6965301418304444 and perplexity is 40.30720116028248
At time: 297.0901381969452 and batch: 200, loss is 3.5868475246429443 and perplexity is 36.12002875493249
At time: 297.72253155708313 and batch: 250, loss is 3.738078246116638 and perplexity is 42.017165875985825
At time: 298.35527873039246 and batch: 300, loss is 3.709279866218567 and perplexity is 40.82439691150983
At time: 298.99539828300476 and batch: 350, loss is 3.686106677055359 and perplexity is 39.88924254148822
At time: 299.6364257335663 and batch: 400, loss is 3.636422953605652 and perplexity is 37.955823865368245
At time: 300.2822895050049 and batch: 450, loss is 3.6432038164138794 and perplexity is 38.214071681758334
At time: 300.9220416545868 and batch: 500, loss is 3.5293804264068602 and perplexity is 34.10283185378291
At time: 301.55821681022644 and batch: 550, loss is 3.6042708492279054 and perplexity is 36.75487423937531
At time: 302.2016043663025 and batch: 600, loss is 3.6298274993896484 and perplexity is 37.70631169436492
At time: 302.8316116333008 and batch: 650, loss is 3.4716115045547484 and perplexity is 32.18857270270933
At time: 303.4717972278595 and batch: 700, loss is 3.466484794616699 and perplexity is 32.02397351380684
At time: 304.11007165908813 and batch: 750, loss is 3.570433249473572 and perplexity is 35.531984030718874
At time: 304.7410087585449 and batch: 800, loss is 3.514849486351013 and perplexity is 33.6108686487014
At time: 305.3651657104492 and batch: 850, loss is 3.574809970855713 and perplexity is 35.687838441851376
At time: 305.9865245819092 and batch: 900, loss is 3.554130825996399 and perplexity is 34.957422676316384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452517156731592 and perplexity of 85.84275193795253
finished 25 epochs...
Completing Train Step...
At time: 307.6412351131439 and batch: 50, loss is 3.818257031440735 and perplexity is 45.52479085075749
At time: 308.28149127960205 and batch: 100, loss is 3.681825351715088 and perplexity is 39.71882877528228
At time: 308.91195130348206 and batch: 150, loss is 3.6964913845062255 and perplexity is 40.3056389912917
At time: 309.54267382621765 and batch: 200, loss is 3.5868149471282957 and perplexity is 36.11885207333341
At time: 310.170738697052 and batch: 250, loss is 3.73804217338562 and perplexity is 42.01565022939997
At time: 310.81325125694275 and batch: 300, loss is 3.709247374534607 and perplexity is 40.82307047965663
At time: 311.4405086040497 and batch: 350, loss is 3.686075448989868 and perplexity is 39.887996897059395
At time: 312.0712876319885 and batch: 400, loss is 3.6363961410522463 and perplexity is 37.95480618645715
At time: 312.7061667442322 and batch: 450, loss is 3.6431812858581543 and perplexity is 38.21321070718598
At time: 313.3370304107666 and batch: 500, loss is 3.5293598651885985 and perplexity is 34.10213066522249
At time: 313.96606516838074 and batch: 550, loss is 3.604249858856201 and perplexity is 36.75410274900005
At time: 314.59419560432434 and batch: 600, loss is 3.6298096561431885 and perplexity is 37.70563889735472
At time: 315.2204558849335 and batch: 650, loss is 3.471601576805115 and perplexity is 32.18825314420473
At time: 315.8482298851013 and batch: 700, loss is 3.466479606628418 and perplexity is 32.023807374238494
At time: 316.4802622795105 and batch: 750, loss is 3.57043110370636 and perplexity is 35.53190778743436
At time: 317.1072270870209 and batch: 800, loss is 3.51485013961792 and perplexity is 33.61089060557676
At time: 317.7341339588165 and batch: 850, loss is 3.574826488494873 and perplexity is 35.68842792555758
At time: 318.36175060272217 and batch: 900, loss is 3.554158320426941 and perplexity is 34.95838382395913
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452513812339469 and perplexity of 85.84246484660918
finished 26 epochs...
Completing Train Step...
At time: 319.96475315093994 and batch: 50, loss is 3.8182289743423463 and perplexity is 45.52351357513988
At time: 320.60467648506165 and batch: 100, loss is 3.681791787147522 and perplexity is 39.717495652343175
At time: 321.23232197761536 and batch: 150, loss is 3.696452832221985 and perplexity is 40.30408514679313
At time: 321.87688279151917 and batch: 200, loss is 3.5867825412750243 and perplexity is 36.11768163007749
At time: 322.51144337654114 and batch: 250, loss is 3.738006477355957 and perplexity is 42.01415046427105
At time: 323.1445162296295 and batch: 300, loss is 3.709214811325073 and perplexity is 40.82174117110218
At time: 323.78104615211487 and batch: 350, loss is 3.6860443687438966 and perplexity is 39.886757187569856
At time: 324.41393637657166 and batch: 400, loss is 3.636369333267212 and perplexity is 37.953788715810006
At time: 325.0456557273865 and batch: 450, loss is 3.6431586599349974 and perplexity is 38.21234610779816
At time: 325.6785454750061 and batch: 500, loss is 3.5293395376205443 and perplexity is 34.101437458886224
At time: 326.3122191429138 and batch: 550, loss is 3.6042288541793823 and perplexity is 36.753330749057874
At time: 326.95974826812744 and batch: 600, loss is 3.6297919178009033 and perplexity is 37.704970067757856
At time: 327.59958839416504 and batch: 650, loss is 3.4715917110443115 and perplexity is 32.18793558416502
At time: 328.2376570701599 and batch: 700, loss is 3.466474404335022 and perplexity is 32.02364077743023
At time: 328.8748321533203 and batch: 750, loss is 3.5704289722442626 and perplexity is 35.53183205260038
At time: 329.5113265514374 and batch: 800, loss is 3.5148509120941163 and perplexity is 33.61091656919972
At time: 330.15134048461914 and batch: 850, loss is 3.574843144416809 and perplexity is 35.6890223541775
At time: 330.78910851478577 and batch: 900, loss is 3.5541855573654173 and perplexity is 34.95933599627564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452511304045377 and perplexity of 85.84224952873177
finished 27 epochs...
Completing Train Step...
At time: 332.4670271873474 and batch: 50, loss is 3.8182011795043946 and perplexity is 45.522248274041566
At time: 333.0970335006714 and batch: 100, loss is 3.6817585468292235 and perplexity is 39.71617545208774
At time: 333.7277297973633 and batch: 150, loss is 3.6964147329330443 and perplexity is 40.30254961905908
At time: 334.3570647239685 and batch: 200, loss is 3.5867504596710207 and perplexity is 36.1165229355044
At time: 334.987535238266 and batch: 250, loss is 3.737970972061157 and perplexity is 42.012658765954804
At time: 335.6237406730652 and batch: 300, loss is 3.7091824913024904 and perplexity is 40.820421832826305
At time: 336.2530951499939 and batch: 350, loss is 3.6860133028030395 and perplexity is 39.8855180871771
At time: 336.8775062561035 and batch: 400, loss is 3.636342611312866 and perplexity is 37.95277452995127
At time: 337.5014133453369 and batch: 450, loss is 3.643136057853699 and perplexity is 38.211482439005216
At time: 338.1248962879181 and batch: 500, loss is 3.5293189764022825 and perplexity is 34.10073629899597
At time: 338.7513337135315 and batch: 550, loss is 3.6042081785202025 and perplexity is 36.75257085757323
At time: 339.38275933265686 and batch: 600, loss is 3.629774265289307 and perplexity is 37.7043044862111
At time: 340.0123300552368 and batch: 650, loss is 3.471581931114197 and perplexity is 32.18762078994381
At time: 340.64327573776245 and batch: 700, loss is 3.4664692306518554 and perplexity is 32.02347509768759
At time: 341.27417159080505 and batch: 750, loss is 3.5704268836975097 and perplexity is 35.53175784278542
At time: 341.9064247608185 and batch: 800, loss is 3.514851870536804 and perplexity is 33.610948783352384
At time: 342.5447437763214 and batch: 850, loss is 3.574859642982483 and perplexity is 35.689611176713996
At time: 343.2004404067993 and batch: 900, loss is 3.5542123985290526 and perplexity is 34.960274358127
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4525079596532535 and perplexity of 85.84196243906867
finished 28 epochs...
Completing Train Step...
At time: 344.8671064376831 and batch: 50, loss is 3.818173327445984 and perplexity is 45.520980403380136
At time: 345.4898705482483 and batch: 100, loss is 3.6817254257202148 and perplexity is 39.714860030095416
At time: 346.11705255508423 and batch: 150, loss is 3.6963769006729126 and perplexity is 40.30102491135967
At time: 346.74985909461975 and batch: 200, loss is 3.58671844959259 and perplexity is 36.1153668612757
At time: 347.3821442127228 and batch: 250, loss is 3.7379358911514284 and perplexity is 42.01118494951673
At time: 348.0047833919525 and batch: 300, loss is 3.7091501808166503 and perplexity is 40.819102926472056
At time: 348.62870717048645 and batch: 350, loss is 3.6859825038909912 and perplexity is 39.884289675530496
At time: 349.25867080688477 and batch: 400, loss is 3.6363160467147826 and perplexity is 37.951766343140825
At time: 349.91009759902954 and batch: 450, loss is 3.64311354637146 and perplexity is 38.21062225157906
At time: 350.5471510887146 and batch: 500, loss is 3.5292987394332886 and perplexity is 34.10004621043549
At time: 351.1928791999817 and batch: 550, loss is 3.604187469482422 and perplexity is 36.75180975507569
At time: 351.8266353607178 and batch: 600, loss is 3.629756798744202 and perplexity is 37.70364592802753
At time: 352.45496106147766 and batch: 650, loss is 3.4715723276138304 and perplexity is 32.187311677600036
At time: 353.08340215682983 and batch: 700, loss is 3.466464247703552 and perplexity is 32.02331552676425
At time: 353.7211170196533 and batch: 750, loss is 3.570424780845642 and perplexity is 35.53168312484063
At time: 354.3566153049469 and batch: 800, loss is 3.514852914810181 and perplexity is 33.61098388238969
At time: 354.98057651519775 and batch: 850, loss is 3.574875955581665 and perplexity is 35.69019337178463
At time: 355.60626554489136 and batch: 900, loss is 3.55423903465271 and perplexity is 34.96120557671987
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452506287457192 and perplexity of 85.84181889459718
finished 29 epochs...
Completing Train Step...
At time: 357.24654173851013 and batch: 50, loss is 3.818145709037781 and perplexity is 45.519723203722556
At time: 357.90128087997437 and batch: 100, loss is 3.681692590713501 and perplexity is 39.713556013808514
At time: 358.5335955619812 and batch: 150, loss is 3.6963394594192507 and perplexity is 40.29951601871072
At time: 359.1756646633148 and batch: 200, loss is 3.586686806678772 and perplexity is 36.114224083915104
At time: 359.80592036247253 and batch: 250, loss is 3.737901120185852 and perplexity is 42.00972420544691
At time: 360.4396061897278 and batch: 300, loss is 3.709118118286133 and perplexity is 40.817794183719705
At time: 361.0762417316437 and batch: 350, loss is 3.685951843261719 and perplexity is 39.88306681685787
At time: 361.7082984447479 and batch: 400, loss is 3.6362895822525023 and perplexity is 37.95076198334195
At time: 362.3394293785095 and batch: 450, loss is 3.6430911207199097 and perplexity is 38.209765363087094
At time: 362.9658875465393 and batch: 500, loss is 3.5292785358428955 and perplexity is 34.09935727402898
At time: 363.5913269519806 and batch: 550, loss is 3.604166879653931 and perplexity is 36.751053049406345
At time: 364.21403670310974 and batch: 600, loss is 3.629739370346069 and perplexity is 37.70298881960143
At time: 364.83828949928284 and batch: 650, loss is 3.471562581062317 and perplexity is 32.186997963837506
At time: 365.4632787704468 and batch: 700, loss is 3.4664591693878175 and perplexity is 32.02315290267007
At time: 366.08887338638306 and batch: 750, loss is 3.5704228830337525 and perplexity is 35.53161569245393
At time: 366.7110986709595 and batch: 800, loss is 3.514854187965393 and perplexity is 33.61102667441625
At time: 367.33287739753723 and batch: 850, loss is 3.5748921775817872 and perplexity is 35.69077234280189
At time: 367.96206045150757 and batch: 900, loss is 3.554265260696411 and perplexity is 34.96212248284853
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452502525016053 and perplexity of 85.8414959204139
finished 30 epochs...
Completing Train Step...
At time: 369.6093804836273 and batch: 50, loss is 3.818118243217468 and perplexity is 45.518472984353586
At time: 370.2495105266571 and batch: 100, loss is 3.6816599655151365 and perplexity is 39.71226037230119
At time: 370.87294840812683 and batch: 150, loss is 3.696302318572998 and perplexity is 40.2980192883773
At time: 371.4973542690277 and batch: 200, loss is 3.586655306816101 and perplexity is 36.11308650873281
At time: 372.1196143627167 and batch: 250, loss is 3.737866563796997 and perplexity is 42.00827252616412
At time: 372.7402238845825 and batch: 300, loss is 3.709086093902588 and perplexity is 40.81648703995365
At time: 373.3641667366028 and batch: 350, loss is 3.6859213495254517 and perplexity is 39.881850651679635
At time: 373.9866056442261 and batch: 400, loss is 3.6362632179260252 and perplexity is 37.94976145025221
At time: 374.6118235588074 and batch: 450, loss is 3.6430685997009276 and perplexity is 38.2089048499259
At time: 375.2654962539673 and batch: 500, loss is 3.5292582273483277 and perplexity is 34.09866477444886
At time: 375.89513516426086 and batch: 550, loss is 3.6041465425491332 and perplexity is 36.750305646989084
At time: 376.5240228176117 and batch: 600, loss is 3.6297221279144285 and perplexity is 37.7023387339986
At time: 377.151243686676 and batch: 650, loss is 3.4715530490875244 and perplexity is 32.18669115964649
At time: 377.7783498764038 and batch: 700, loss is 3.4664542293548584 and perplexity is 32.02299470763002
At time: 378.405127286911 and batch: 750, loss is 3.570421004295349 and perplexity is 35.531548937905704
At time: 379.03072118759155 and batch: 800, loss is 3.514855456352234 and perplexity is 33.61106930622723
At time: 379.6539840698242 and batch: 850, loss is 3.5749085187911986 and perplexity is 35.69135557795217
At time: 380.2771952152252 and batch: 900, loss is 3.5542913770675657 and perplexity is 34.963035578538964
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45250001672196 and perplexity of 85.84128060496684
finished 31 epochs...
Completing Train Step...
At time: 381.9259765148163 and batch: 50, loss is 3.8180908393859863 and perplexity is 45.51722562088196
At time: 382.5559666156769 and batch: 100, loss is 3.6816276693344117 and perplexity is 39.71097783867379
At time: 383.18643403053284 and batch: 150, loss is 3.69626549243927 and perplexity is 40.296535295455044
At time: 383.81564378738403 and batch: 200, loss is 3.5866239738464354 and perplexity is 36.11195499621562
At time: 384.44072365760803 and batch: 250, loss is 3.737832441329956 and perplexity is 42.00683912472514
At time: 385.0747787952423 and batch: 300, loss is 3.70905415058136 and perplexity is 40.815183246620585
At time: 385.7106294631958 and batch: 350, loss is 3.6858909702301026 and perplexity is 39.88063908756294
At time: 386.3389890193939 and batch: 400, loss is 3.6362369680404663 and perplexity is 37.9487652864318
At time: 386.9644260406494 and batch: 450, loss is 3.6430463218688964 and perplexity is 38.208053647843066
At time: 387.5907824039459 and batch: 500, loss is 3.5292381048202515 and perplexity is 34.097978630013074
At time: 388.2160975933075 and batch: 550, loss is 3.6041263341903687 and perplexity is 36.74956299113181
At time: 388.84248447418213 and batch: 600, loss is 3.6297048234939577 and perplexity is 37.70168632252123
At time: 389.46771144866943 and batch: 650, loss is 3.4715436410903933 and perplexity is 32.18638834877283
At time: 390.09431552886963 and batch: 700, loss is 3.4664492845535277 and perplexity is 32.022836360674674
At time: 390.7211239337921 and batch: 750, loss is 3.5704191637039187 and perplexity is 35.531483538901405
At time: 391.35752964019775 and batch: 800, loss is 3.514856958389282 and perplexity is 33.61111979133647
At time: 391.9778742790222 and batch: 850, loss is 3.5749246311187743 and perplexity is 35.691930653397755
At time: 392.59938383102417 and batch: 900, loss is 3.5543170738220216 and perplexity is 34.96393402662281
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452497926476884 and perplexity of 85.84110117584021
finished 32 epochs...
Completing Train Step...
At time: 394.1850836277008 and batch: 50, loss is 3.8180636310577394 and perplexity is 45.515987190114274
At time: 394.80476474761963 and batch: 100, loss is 3.681595492362976 and perplexity is 39.7097000802315
At time: 395.42408514022827 and batch: 150, loss is 3.6962288188934327 and perplexity is 40.29505750571886
At time: 396.0436282157898 and batch: 200, loss is 3.586592922210693 and perplexity is 36.110833678352606
At time: 396.66583585739136 and batch: 250, loss is 3.7377984952926635 and perplexity is 42.00541318320034
At time: 397.2869117259979 and batch: 300, loss is 3.7090222597122193 and perplexity is 40.81388163570758
At time: 397.90788888931274 and batch: 350, loss is 3.685860686302185 and perplexity is 39.87943136345093
At time: 398.5312216281891 and batch: 400, loss is 3.63621075630188 and perplexity is 37.947770596352775
At time: 399.15337085723877 and batch: 450, loss is 3.643023796081543 and perplexity is 38.20719299104492
At time: 399.77622413635254 and batch: 500, loss is 3.5292180442810057 and perplexity is 34.09729461303546
At time: 400.3970956802368 and batch: 550, loss is 3.604106135368347 and perplexity is 36.7488207007463
At time: 401.01770401000977 and batch: 600, loss is 3.629687728881836 and perplexity is 37.701041832325885
At time: 401.6404845714569 and batch: 650, loss is 3.471534152030945 and perplexity is 32.18608293166942
At time: 402.26390981674194 and batch: 700, loss is 3.4664445066452028 and perplexity is 32.02268335886375
At time: 402.8944354057312 and batch: 750, loss is 3.57041729927063 and perplexity is 35.53141729288246
At time: 403.5211114883423 and batch: 800, loss is 3.5148586511611937 and perplexity is 33.61117668734412
At time: 404.1442346572876 and batch: 850, loss is 3.574940676689148 and perplexity is 35.69250335537748
At time: 404.7657392024994 and batch: 900, loss is 3.5543425464630127 and perplexity is 34.96482466170527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452497090378853 and perplexity of 85.84102940429456
finished 33 epochs...
Completing Train Step...
At time: 406.3887526988983 and batch: 50, loss is 3.81803653717041 and perplexity is 45.51475400179167
At time: 407.0156214237213 and batch: 100, loss is 3.681563582420349 and perplexity is 39.70843296619709
At time: 407.6553056240082 and batch: 150, loss is 3.696192588806152 and perplexity is 40.29359763871417
At time: 408.27838587760925 and batch: 200, loss is 3.5865619230270385 and perplexity is 36.10971428933765
At time: 408.89649057388306 and batch: 250, loss is 3.7377647876739504 and perplexity is 42.00399730461195
At time: 409.51412081718445 and batch: 300, loss is 3.7089906120300293 and perplexity is 40.81258999139152
At time: 410.13110184669495 and batch: 350, loss is 3.6858305644989016 and perplexity is 39.87823014115591
At time: 410.75049471855164 and batch: 400, loss is 3.6361847591400145 and perplexity is 37.94678407484159
At time: 411.36938524246216 and batch: 450, loss is 3.6430015468597414 and perplexity is 38.2063429201904
At time: 411.98797369003296 and batch: 500, loss is 3.5291980695724487 and perplexity is 34.09661353631516
At time: 412.6079499721527 and batch: 550, loss is 3.6040861415863037 and perplexity is 36.748085960180006
At time: 413.2249720096588 and batch: 600, loss is 3.629670696258545 and perplexity is 37.70039969015138
At time: 413.842253446579 and batch: 650, loss is 3.471524829864502 and perplexity is 32.18578288904572
At time: 414.4598731994629 and batch: 700, loss is 3.4664396619796753 and perplexity is 32.02252822004939
At time: 415.07852005958557 and batch: 750, loss is 3.57041558265686 and perplexity is 35.53135629921462
At time: 415.6969184875488 and batch: 800, loss is 3.5148602533340454 and perplexity is 33.61123053830207
At time: 416.31473779678345 and batch: 850, loss is 3.5749565982818603 and perplexity is 35.69307164140278
At time: 416.9328680038452 and batch: 900, loss is 3.5543677282333372 and perplexity is 34.96570514897541
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452493327937714 and perplexity of 85.84070643308168
finished 34 epochs...
Completing Train Step...
At time: 418.4964978694916 and batch: 50, loss is 3.81800968170166 and perplexity is 45.51353169815075
At time: 419.1264066696167 and batch: 100, loss is 3.6815318489074706 and perplexity is 39.70717289812147
At time: 419.743825674057 and batch: 150, loss is 3.6961565399169922 and perplexity is 40.29214512545993
At time: 420.3613350391388 and batch: 200, loss is 3.58653115272522 and perplexity is 36.10860319962475
At time: 420.97805190086365 and batch: 250, loss is 3.7377314710617067 and perplexity is 42.002597897032956
At time: 421.59562635421753 and batch: 300, loss is 3.7089589262008666 and perplexity is 40.8112968311249
At time: 422.21353006362915 and batch: 350, loss is 3.685800561904907 and perplexity is 39.877033708755896
At time: 422.84484791755676 and batch: 400, loss is 3.636158709526062 and perplexity is 37.94579558864059
At time: 423.462739944458 and batch: 450, loss is 3.642979235649109 and perplexity is 38.205490499935316
At time: 424.0833098888397 and batch: 500, loss is 3.5291780996322633 and perplexity is 34.09593263578111
At time: 424.70071291923523 and batch: 550, loss is 3.604066252708435 and perplexity is 36.74735508925457
At time: 425.31913924217224 and batch: 600, loss is 3.629653878211975 and perplexity is 37.69976564840537
At time: 425.9425296783447 and batch: 650, loss is 3.4715155744552613 and perplexity is 32.1854849978319
At time: 426.5652587413788 and batch: 700, loss is 3.466434888839722 and perplexity is 32.02237537240531
At time: 427.18493914604187 and batch: 750, loss is 3.5704139184951784 and perplexity is 35.53129716934217
At time: 427.80266308784485 and batch: 800, loss is 3.514862117767334 and perplexity is 33.61129320425757
At time: 428.4196059703827 and batch: 850, loss is 3.5749725675582886 and perplexity is 35.69364163848161
At time: 429.0369231700897 and batch: 900, loss is 3.554392685890198 and perplexity is 34.9665778219363
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452489983545591 and perplexity of 85.8404193485793
finished 35 epochs...
Completing Train Step...
At time: 430.5931570529938 and batch: 50, loss is 3.817982807159424 and perplexity is 45.51230855925654
At time: 431.22598791122437 and batch: 100, loss is 3.681500267982483 and perplexity is 39.70591892867357
At time: 431.8430690765381 and batch: 150, loss is 3.696120791435242 and perplexity is 40.29070476819068
At time: 432.46024680137634 and batch: 200, loss is 3.5865006160736086 and perplexity is 36.107500580623906
At time: 433.07766461372375 and batch: 250, loss is 3.7376982164382935 and perplexity is 42.00120113968197
At time: 433.69571471214294 and batch: 300, loss is 3.7089274644851686 and perplexity is 40.81001285790484
At time: 434.3125002384186 and batch: 350, loss is 3.685770688056946 and perplexity is 39.875842446107626
At time: 434.92935514450073 and batch: 400, loss is 3.636132850646973 and perplexity is 37.94481436558724
At time: 435.54686188697815 and batch: 450, loss is 3.6429568672180177 and perplexity is 38.20463591261168
At time: 436.1643831729889 and batch: 500, loss is 3.529158239364624 and perplexity is 34.09525548815774
At time: 436.7813057899475 and batch: 550, loss is 3.604046468734741 and perplexity is 36.74662808773967
At time: 437.39782977104187 and batch: 600, loss is 3.629636998176575 and perplexity is 37.699129280397614
At time: 438.01422119140625 and batch: 650, loss is 3.47150634765625 and perplexity is 32.185188030200784
At time: 438.6445336341858 and batch: 700, loss is 3.4664302492141723 and perplexity is 32.022226800919036
At time: 439.262659072876 and batch: 750, loss is 3.570412278175354 and perplexity is 35.53123888669884
At time: 439.8843970298767 and batch: 800, loss is 3.5148639917373656 and perplexity is 33.611356190872776
At time: 440.5102310180664 and batch: 850, loss is 3.574988431930542 and perplexity is 35.694207900191316
At time: 441.136155128479 and batch: 900, loss is 3.554417247772217 and perplexity is 34.96743667744288
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452488729398545 and perplexity of 85.84031169213847
finished 36 epochs...
Completing Train Step...
At time: 442.75813937187195 and batch: 50, loss is 3.8179561042785646 and perplexity is 45.511093265729436
At time: 443.3929452896118 and batch: 100, loss is 3.681468963623047 and perplexity is 39.70467597977064
At time: 444.0205943584442 and batch: 150, loss is 3.696085286140442 and perplexity is 40.28927426023565
At time: 444.64723682403564 and batch: 200, loss is 3.586470170021057 and perplexity is 36.1064012664987
At time: 445.27189588546753 and batch: 250, loss is 3.7376652765274048 and perplexity is 41.9998176466454
At time: 445.8948848247528 and batch: 300, loss is 3.708896126747131 and perplexity is 40.808733984451194
At time: 446.51937913894653 and batch: 350, loss is 3.685740838050842 and perplexity is 39.87465216973218
At time: 447.1503555774689 and batch: 400, loss is 3.636107077598572 and perplexity is 37.94383642465235
At time: 447.78245186805725 and batch: 450, loss is 3.6429347515106203 and perplexity is 38.20379099940557
At time: 448.4054410457611 and batch: 500, loss is 3.529138436317444 and perplexity is 34.09458030489006
At time: 449.02377676963806 and batch: 550, loss is 3.6040267658233645 and perplexity is 36.74590407931563
At time: 449.655499458313 and batch: 600, loss is 3.6296203422546385 and perplexity is 37.69850137187247
At time: 450.2822651863098 and batch: 650, loss is 3.471497235298157 and perplexity is 32.184894748578394
At time: 450.9087812900543 and batch: 700, loss is 3.466425447463989 and perplexity is 32.02207303855479
At time: 451.53485774993896 and batch: 750, loss is 3.570410680770874 and perplexity is 35.531182128984
At time: 452.16137528419495 and batch: 800, loss is 3.5148660278320314 and perplexity is 33.6114246268455
At time: 452.7893261909485 and batch: 850, loss is 3.575004119873047 and perplexity is 35.69476787326502
At time: 453.41598558425903 and batch: 900, loss is 3.5544417476654053 and perplexity is 34.96829338640114
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452486639153467 and perplexity of 85.84013226503703
finished 37 epochs...
Completing Train Step...
At time: 455.0522029399872 and batch: 50, loss is 3.817929563522339 and perplexity is 45.50988538292664
At time: 455.67643117904663 and batch: 100, loss is 3.6814378309249878 and perplexity is 39.703439885323405
At time: 456.2991907596588 and batch: 150, loss is 3.696050009727478 and perplexity is 40.287853024227026
At time: 456.9215269088745 and batch: 200, loss is 3.5864399528503417 and perplexity is 36.10531024969151
At time: 457.5435290336609 and batch: 250, loss is 3.737632603645325 and perplexity is 41.99844541397358
At time: 458.16666984558105 and batch: 300, loss is 3.7088647079467774 and perplexity is 40.807451843127225
At time: 458.7939531803131 and batch: 350, loss is 3.6857113218307496 and perplexity is 39.873475238092
At time: 459.4180257320404 and batch: 400, loss is 3.6360812902450563 and perplexity is 37.942857966144715
At time: 460.0532569885254 and batch: 450, loss is 3.642912549972534 and perplexity is 38.202942825900074
At time: 460.6921691894531 and batch: 500, loss is 3.5291187286376955 and perplexity is 34.09390838644125
At time: 461.326514005661 and batch: 550, loss is 3.6040071392059327 and perplexity is 36.74518288859137
At time: 461.9598124027252 and batch: 600, loss is 3.6296037006378175 and perplexity is 37.69787401307806
At time: 462.5854461193085 and batch: 650, loss is 3.4714881706237795 and perplexity is 32.18460300430991
At time: 463.21133279800415 and batch: 700, loss is 3.4664208555221556 and perplexity is 32.02192599539561
At time: 463.8324797153473 and batch: 750, loss is 3.5704090547561647 and perplexity is 35.53112435480618
At time: 464.4533839225769 and batch: 800, loss is 3.5148680639266967 and perplexity is 33.61149306295754
At time: 465.08023262023926 and batch: 850, loss is 3.5750197649002073 and perplexity is 35.695326323246356
At time: 465.7098045349121 and batch: 900, loss is 3.5544658422470095 and perplexity is 34.96913594295019
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452484548908391 and perplexity of 85.83995283831072
finished 38 epochs...
Completing Train Step...
At time: 467.3322196006775 and batch: 50, loss is 3.8179029989242554 and perplexity is 45.50867644717013
At time: 467.99292826652527 and batch: 100, loss is 3.6814068937301636 and perplexity is 39.70221159126857
At time: 468.6216354370117 and batch: 150, loss is 3.696015167236328 and perplexity is 40.286449319519
At time: 469.25091338157654 and batch: 200, loss is 3.586409912109375 and perplexity is 36.104225635710165
At time: 469.87817668914795 and batch: 250, loss is 3.737600178718567 and perplexity is 41.99708363953492
At time: 470.5071828365326 and batch: 300, loss is 3.7088336515426636 and perplexity is 40.80618453009112
At time: 471.1590564250946 and batch: 350, loss is 3.6856817770004273 and perplexity is 39.87229720043428
At time: 471.78388690948486 and batch: 400, loss is 3.6360557413101198 and perplexity is 37.941888578918686
At time: 472.4148979187012 and batch: 450, loss is 3.6428904056549074 and perplexity is 38.2020968571666
At time: 473.0459671020508 and batch: 500, loss is 3.5290990447998047 and perplexity is 34.09323729408037
At time: 473.6752829551697 and batch: 550, loss is 3.60398775100708 and perplexity is 36.74447047258491
At time: 474.3063690662384 and batch: 600, loss is 3.6295872592926024 and perplexity is 37.69725421441271
At time: 474.9515573978424 and batch: 650, loss is 3.47147910118103 and perplexity is 32.18431110921922
At time: 475.5833034515381 and batch: 700, loss is 3.466416206359863 and perplexity is 32.021777120610814
At time: 476.212322473526 and batch: 750, loss is 3.570407500267029 and perplexity is 35.53106912210232
At time: 476.8525207042694 and batch: 800, loss is 3.51487024307251 and perplexity is 33.61156630738173
At time: 477.4915289878845 and batch: 850, loss is 3.5750353050231936 and perplexity is 35.6958810373176
At time: 478.12107491493225 and batch: 900, loss is 3.5544899034500124 and perplexity is 34.96997735255157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452481622565283 and perplexity of 85.83970164152386
finished 39 epochs...
Completing Train Step...
At time: 479.7652449607849 and batch: 50, loss is 3.817876601219177 and perplexity is 45.50747513840675
At time: 480.41584181785583 and batch: 100, loss is 3.681376271247864 and perplexity is 39.70099582961177
At time: 481.04999804496765 and batch: 150, loss is 3.695980372428894 and perplexity is 40.28504758465942
At time: 481.6719238758087 and batch: 200, loss is 3.586380000114441 and perplexity is 36.103145702447414
At time: 482.2957499027252 and batch: 250, loss is 3.7375678396224976 and perplexity is 41.99572551377286
At time: 482.9205951690674 and batch: 300, loss is 3.7088024711608885 and perplexity is 40.8049121975147
At time: 483.54669404029846 and batch: 350, loss is 3.6856524181365966 and perplexity is 39.871126612273805
At time: 484.1714985370636 and batch: 400, loss is 3.63603018283844 and perplexity is 37.94091885462635
At time: 484.79929876327515 and batch: 450, loss is 3.6428683233261108 and perplexity is 38.20125327521724
At time: 485.4233696460724 and batch: 500, loss is 3.5290794229507445 and perplexity is 34.0925683282874
At time: 486.0505805015564 and batch: 550, loss is 3.6039683485031127 and perplexity is 36.74375754476709
At time: 486.7039728164673 and batch: 600, loss is 3.6295707273483275 and perplexity is 37.69663101065812
At time: 487.3315360546112 and batch: 650, loss is 3.4714701747894288 and perplexity is 32.18402382073707
At time: 487.9561607837677 and batch: 700, loss is 3.466411662101746 and perplexity is 32.021631605720835
At time: 488.59215116500854 and batch: 750, loss is 3.570406103134155 and perplexity is 35.531019480512285
At time: 489.2240138053894 and batch: 800, loss is 3.5148725509643555 and perplexity is 33.61164387933104
At time: 489.8474214076996 and batch: 850, loss is 3.575050849914551 and perplexity is 35.69643593022309
At time: 490.4714846611023 and batch: 900, loss is 3.5545134592056273 and perplexity is 34.97080110649398
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452479532320205 and perplexity of 85.83952221569757
finished 40 epochs...
Completing Train Step...
At time: 492.11525535583496 and batch: 50, loss is 3.8178503799438475 and perplexity is 45.5062818900159
At time: 492.7369349002838 and batch: 100, loss is 3.6813455486297606 and perplexity is 39.69977612981486
At time: 493.35538482666016 and batch: 150, loss is 3.6959456968307496 and perplexity is 40.28365070075718
At time: 493.97909212112427 and batch: 200, loss is 3.58635018825531 and perplexity is 36.10206941659667
At time: 494.6024420261383 and batch: 250, loss is 3.73753577709198 and perplexity is 41.99437904612766
At time: 495.2259805202484 and batch: 300, loss is 3.7087714862823487 and perplexity is 40.80364788185387
At time: 495.8525199890137 and batch: 350, loss is 3.68562313079834 and perplexity is 39.86995891020156
At time: 496.47799134254456 and batch: 400, loss is 3.6360047340393065 and perplexity is 37.93995331608943
At time: 497.1098213195801 and batch: 450, loss is 3.6428462791442873 and perplexity is 38.20041116912596
At time: 497.74243927001953 and batch: 500, loss is 3.529059886932373 and perplexity is 34.091902301751986
At time: 498.3713722229004 and batch: 550, loss is 3.603949112892151 and perplexity is 36.7430507629394
At time: 499.00011014938354 and batch: 600, loss is 3.629554443359375 and perplexity is 37.69601716413315
At time: 499.62699580192566 and batch: 650, loss is 3.4714612913131715 and perplexity is 32.18373791599551
At time: 500.2608425617218 and batch: 700, loss is 3.4664070653915404 and perplexity is 32.02148441189834
At time: 500.89469957351685 and batch: 750, loss is 3.570404677391052 and perplexity is 35.530968822442425
At time: 501.52617859840393 and batch: 800, loss is 3.514874954223633 and perplexity is 33.61172465692309
At time: 502.161673784256 and batch: 850, loss is 3.5750661849975587 and perplexity is 35.696983342228464
At time: 502.82022738456726 and batch: 900, loss is 3.5545369386672974 and perplexity is 34.971622211717644
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452477024026113 and perplexity of 85.83930690520113
finished 41 epochs...
Completing Train Step...
At time: 504.4658372402191 and batch: 50, loss is 3.8178242206573487 and perplexity is 45.50509149372047
At time: 505.08920884132385 and batch: 100, loss is 3.6813152599334718 and perplexity is 39.698573693563134
At time: 505.7089955806732 and batch: 150, loss is 3.695911464691162 and perplexity is 40.28227172880601
At time: 506.3371739387512 and batch: 200, loss is 3.586320548057556 and perplexity is 36.10099935997825
At time: 506.96634340286255 and batch: 250, loss is 3.7375040531158445 and perplexity is 41.993046838580554
At time: 507.59255385398865 and batch: 300, loss is 3.70874062538147 and perplexity is 40.80238866395149
At time: 508.22453451156616 and batch: 350, loss is 3.6855939197540284 and perplexity is 39.86879428407519
At time: 508.8580975532532 and batch: 400, loss is 3.6359793758392334 and perplexity is 37.938991239360796
At time: 509.4814124107361 and batch: 450, loss is 3.642824273109436 and perplexity is 38.199570538795946
At time: 510.1154873371124 and batch: 500, loss is 3.5290405225753783 and perplexity is 34.091242140377005
At time: 510.75191855430603 and batch: 550, loss is 3.6039299726486207 and perplexity is 36.7423474987301
At time: 511.3807818889618 and batch: 600, loss is 3.6295382928848268 and perplexity is 37.69540836048361
At time: 512.012589931488 and batch: 650, loss is 3.471452488899231 and perplexity is 32.183454622659056
At time: 512.6402769088745 and batch: 700, loss is 3.466402544975281 and perplexity is 32.02133966178672
At time: 513.2643661499023 and batch: 750, loss is 3.5704032802581787 and perplexity is 35.53091918099254
At time: 513.88733959198 and batch: 800, loss is 3.5148773860931395 and perplexity is 33.61180639635074
At time: 514.5101585388184 and batch: 850, loss is 3.575081481933594 and perplexity is 35.697529400875794
At time: 515.1334617137909 and batch: 900, loss is 3.5545601844787598 and perplexity is 34.972435164902954
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452474933781036 and perplexity of 85.83912748020002
finished 42 epochs...
Completing Train Step...
At time: 516.7520570755005 and batch: 50, loss is 3.817798342704773 and perplexity is 45.50391393035736
At time: 517.3783946037292 and batch: 100, loss is 3.6812849330902098 and perplexity is 39.69736977939655
At time: 518.0074832439423 and batch: 150, loss is 3.695877318382263 and perplexity is 40.2808962613961
At time: 518.6418173313141 and batch: 200, loss is 3.586291127204895 and perplexity is 36.09993725341929
At time: 519.2877669334412 and batch: 250, loss is 3.7374722576141357 and perplexity is 41.99171166981433
At time: 519.9177660942078 and batch: 300, loss is 3.708709788322449 and perplexity is 40.801130457683854
At time: 520.5477297306061 and batch: 350, loss is 3.685564913749695 and perplexity is 39.867637866427025
At time: 521.1766514778137 and batch: 400, loss is 3.6359541606903076 and perplexity is 37.93803461410738
At time: 521.8167469501495 and batch: 450, loss is 3.6428023910522462 and perplexity is 38.19873466275416
At time: 522.447900056839 and batch: 500, loss is 3.5290209817886353 and perplexity is 34.09057597719323
At time: 523.0832712650299 and batch: 550, loss is 3.603910789489746 and perplexity is 36.741642671201035
At time: 523.7089200019836 and batch: 600, loss is 3.6295220470428466 and perplexity is 37.69479597181041
At time: 524.3578248023987 and batch: 650, loss is 3.4714436626434324 and perplexity is 32.18317056450966
At time: 525.0007829666138 and batch: 700, loss is 3.4663980627059936 and perplexity is 32.02119613384108
At time: 525.6369709968567 and batch: 750, loss is 3.570401849746704 and perplexity is 35.5308683536413
At time: 526.2633180618286 and batch: 800, loss is 3.514879879951477 and perplexity is 33.611890219538886
At time: 526.892929315567 and batch: 850, loss is 3.575096650123596 and perplexity is 35.69807087189092
At time: 527.5283055305481 and batch: 900, loss is 3.5545830821990965 and perplexity is 34.97323596311105
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452472007437928 and perplexity of 85.83887628582843
finished 43 epochs...
Completing Train Step...
At time: 529.1565053462982 and batch: 50, loss is 3.817772374153137 and perplexity is 45.50273227496184
At time: 529.7896964550018 and batch: 100, loss is 3.681254897117615 and perplexity is 39.69617744819227
At time: 530.4102528095245 and batch: 150, loss is 3.695843539237976 and perplexity is 40.27953563016989
At time: 531.0277853012085 and batch: 200, loss is 3.5862617444992066 and perplexity is 36.09887655517077
At time: 531.644121170044 and batch: 250, loss is 3.7374408769607546 and perplexity is 41.990393963140896
At time: 532.2822377681732 and batch: 300, loss is 3.7086791467666624 and perplexity is 40.79988026672278
At time: 532.9067578315735 and batch: 350, loss is 3.685535969734192 and perplexity is 39.86648395359807
At time: 533.5318419933319 and batch: 400, loss is 3.635928864479065 and perplexity is 37.93707493770779
At time: 534.152360200882 and batch: 450, loss is 3.6427804040908813 and perplexity is 38.19789479788402
At time: 534.7769515514374 and batch: 500, loss is 3.5290016746520996 and perplexity is 34.08991779214212
At time: 535.4251194000244 and batch: 550, loss is 3.603891859054565 and perplexity is 36.740947142499365
At time: 536.0508699417114 and batch: 600, loss is 3.6295059442520143 and perplexity is 37.69418898528251
At time: 536.6772305965424 and batch: 650, loss is 3.471434931755066 and perplexity is 32.18288957806681
At time: 537.3047587871552 and batch: 700, loss is 3.466393537521362 and perplexity is 32.021051232344306
At time: 537.9271535873413 and batch: 750, loss is 3.570400528907776 and perplexity is 35.530821423118226
At time: 538.5512273311615 and batch: 800, loss is 3.5148824644088745 and perplexity is 33.61197708814946
At time: 539.1818535327911 and batch: 850, loss is 3.5751118421554566 and perplexity is 35.6986132022405
At time: 539.8034219741821 and batch: 900, loss is 3.5546058797836304 and perplexity is 34.97403327750273
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452471171339897 and perplexity of 85.83880451614301
finished 44 epochs...
Completing Train Step...
At time: 541.4096930027008 and batch: 50, loss is 3.817746558189392 and perplexity is 45.50155759323797
At time: 542.0651457309723 and batch: 100, loss is 3.6812249183654786 and perplexity is 39.6949874241656
At time: 542.6902585029602 and batch: 150, loss is 3.6958099365234376 and perplexity is 40.27818215117278
At time: 543.315215587616 and batch: 200, loss is 3.5862325620651245 and perplexity is 36.09782311745627
At time: 543.9402451515198 and batch: 250, loss is 3.737409543991089 and perplexity is 41.98907830001253
At time: 544.5640342235565 and batch: 300, loss is 3.7086485195159913 and perplexity is 40.79863069769803
At time: 545.1873605251312 and batch: 350, loss is 3.6855071210861206 and perplexity is 39.86533387602182
At time: 545.8074271678925 and batch: 400, loss is 3.6359037828445433 and perplexity is 37.93612342579217
At time: 546.4315235614777 and batch: 450, loss is 3.642758493423462 and perplexity is 38.19705786568388
At time: 547.0545341968536 and batch: 500, loss is 3.528982472419739 and perplexity is 34.08926319590439
At time: 547.6782848834991 and batch: 550, loss is 3.603872985839844 and perplexity is 36.74025372925837
At time: 548.3035507202148 and batch: 600, loss is 3.6294900035858153 and perplexity is 37.69358811958737
At time: 548.9289245605469 and batch: 650, loss is 3.471426215171814 and perplexity is 32.18260905445312
At time: 549.5525333881378 and batch: 700, loss is 3.466389012336731 and perplexity is 32.020906331503255
At time: 550.1796517372131 and batch: 750, loss is 3.5703992080688476 and perplexity is 35.53077449265713
At time: 550.8321912288666 and batch: 800, loss is 3.514885196685791 and perplexity is 33.612068925504026
At time: 551.4581248760223 and batch: 850, loss is 3.575126829147339 and perplexity is 35.69914822107592
At time: 552.0774862766266 and batch: 900, loss is 3.5546284437179567 and perplexity is 34.97482243819598
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452468663045805 and perplexity of 85.83858920744677
finished 45 epochs...
Completing Train Step...
At time: 553.707615852356 and batch: 50, loss is 3.8177207469940186 and perplexity is 45.50038315880198
At time: 554.3312060832977 and batch: 100, loss is 3.6811952686309812 and perplexity is 39.693810495775494
At time: 554.9575927257538 and batch: 150, loss is 3.6957763195037843 and perplexity is 40.27682814149081
At time: 555.582905292511 and batch: 200, loss is 3.5862034702301027 and perplexity is 36.096772980816766
At time: 556.2042219638824 and batch: 250, loss is 3.7373783588409424 and perplexity is 41.987768884718484
At time: 556.826587677002 and batch: 300, loss is 3.70861798286438 and perplexity is 40.797384863148096
At time: 557.4528267383575 and batch: 350, loss is 3.68547842502594 and perplexity is 39.864189914415476
At time: 558.0787329673767 and batch: 400, loss is 3.635878643989563 and perplexity is 37.935169767073845
At time: 558.7070116996765 and batch: 450, loss is 3.64273663520813 and perplexity is 38.19622295529287
At time: 559.3361711502075 and batch: 500, loss is 3.5289631938934325 and perplexity is 34.08860601148189
At time: 559.961639881134 and batch: 550, loss is 3.603854160308838 and perplexity is 36.739562080982964
At time: 560.5896580219269 and batch: 600, loss is 3.629474153518677 and perplexity is 37.69299067841974
At time: 561.2155222892761 and batch: 650, loss is 3.4714175844192505 and perplexity is 32.182331295516164
At time: 561.840277671814 and batch: 700, loss is 3.4663846015930178 and perplexity is 32.020765095803434
At time: 562.4647014141083 and batch: 750, loss is 3.5703979396820067 and perplexity is 35.53072942591889
At time: 563.0899727344513 and batch: 800, loss is 3.5148878383636473 and perplexity is 33.6121577178795
At time: 563.7169277667999 and batch: 850, loss is 3.5751417970657347 and perplexity is 35.69968256701231
At time: 564.3432443141937 and batch: 900, loss is 3.55465078830719 and perplexity is 34.97560394496806
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452468663045805 and perplexity of 85.83858920744677
finished 46 epochs...
Completing Train Step...
At time: 565.9556796550751 and batch: 50, loss is 3.8176951599121094 and perplexity is 45.49921895166559
At time: 566.5803687572479 and batch: 100, loss is 3.681165680885315 and perplexity is 39.692636062780515
At time: 567.2319831848145 and batch: 150, loss is 3.6957430267333984 and perplexity is 40.27548723662097
At time: 567.8600375652313 and batch: 200, loss is 3.586174521446228 and perplexity is 36.09572803826216
At time: 568.486517906189 and batch: 250, loss is 3.7373474407196046 and perplexity is 41.986470721853884
At time: 569.1112742424011 and batch: 300, loss is 3.7085876941680906 and perplexity is 40.796149182262255
At time: 569.7399320602417 and batch: 350, loss is 3.685449719429016 and perplexity is 39.86304560547221
At time: 570.3683824539185 and batch: 400, loss is 3.635853681564331 and perplexity is 37.934222825053915
At time: 570.9977362155914 and batch: 450, loss is 3.642714900970459 and perplexity is 38.19539279852647
At time: 571.6309452056885 and batch: 500, loss is 3.5289439249038694 and perplexity is 34.08794916481685
At time: 572.2630105018616 and batch: 550, loss is 3.6038354730606077 and perplexity is 36.738875526081415
At time: 572.8928513526917 and batch: 600, loss is 3.6294582509994506 and perplexity is 37.69239126967685
At time: 573.5223622322083 and batch: 650, loss is 3.4714090633392334 and perplexity is 32.18205706846441
At time: 574.1532671451569 and batch: 700, loss is 3.4663801765441895 and perplexity is 32.020623402667866
At time: 574.7833857536316 and batch: 750, loss is 3.570396661758423 and perplexity is 35.53068402039082
At time: 575.4120168685913 and batch: 800, loss is 3.5148905754089355 and perplexity is 33.612249716003305
At time: 576.0439715385437 and batch: 850, loss is 3.57515664100647 and perplexity is 35.7002124949177
At time: 576.6779568195343 and batch: 900, loss is 3.5546729803085326 and perplexity is 34.97638013223031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452466154751712 and perplexity of 85.83837389929059
finished 47 epochs...
Completing Train Step...
At time: 578.3226532936096 and batch: 50, loss is 3.817669711112976 and perplexity is 45.49806106591522
At time: 578.9766879081726 and batch: 100, loss is 3.681136293411255 and perplexity is 39.691469613607424
At time: 579.6058359146118 and batch: 150, loss is 3.6957099771499635 and perplexity is 40.27415617054088
At time: 580.2355606555939 and batch: 200, loss is 3.586145625114441 and perplexity is 36.094685019198444
At time: 580.867339849472 and batch: 250, loss is 3.7373165702819824 and perplexity is 41.9851746011345
At time: 581.49662733078 and batch: 300, loss is 3.708557381629944 and perplexity is 40.794912566176514
At time: 582.1247296333313 and batch: 350, loss is 3.6854212427139283 and perplexity is 39.86191045304275
At time: 582.7553918361664 and batch: 400, loss is 3.635828847885132 and perplexity is 37.93328079043074
At time: 583.4113273620605 and batch: 450, loss is 3.6426931285858153 and perplexity is 38.19456120279579
At time: 584.0405383110046 and batch: 500, loss is 3.5289247941970827 and perplexity is 34.08729704449419
At time: 584.6720268726349 and batch: 550, loss is 3.60381685256958 and perplexity is 36.73819143654838
At time: 585.3005554676056 and batch: 600, loss is 3.6294425535202026 and perplexity is 37.691799598790965
At time: 585.9302628040314 and batch: 650, loss is 3.4714005279541014 and perplexity is 32.18178238338527
At time: 586.5595519542694 and batch: 700, loss is 3.4663757419586183 and perplexity is 32.02048140478819
At time: 587.1847145557404 and batch: 750, loss is 3.570395426750183 and perplexity is 35.530640139730394
At time: 587.818201303482 and batch: 800, loss is 3.514893403053284 and perplexity is 33.61234475962563
At time: 588.4512248039246 and batch: 850, loss is 3.575171413421631 and perplexity is 35.700739877173355
At time: 589.0763881206512 and batch: 900, loss is 3.5546949243545534 and perplexity is 34.977147663946916
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452462392310574 and perplexity of 85.83805093806895
finished 48 epochs...
Completing Train Step...
At time: 590.6444230079651 and batch: 50, loss is 3.8176441287994383 and perplexity is 45.49689713513976
At time: 591.2774455547333 and batch: 100, loss is 3.681106991767883 and perplexity is 39.69030660535902
At time: 591.8963961601257 and batch: 150, loss is 3.6956771659851073 and perplexity is 40.272834750242104
At time: 592.5160493850708 and batch: 200, loss is 3.586117091178894 and perplexity is 36.09365511047627
At time: 593.1333675384521 and batch: 250, loss is 3.7372859573364257 and perplexity is 41.9838893309433
At time: 593.7520112991333 and batch: 300, loss is 3.708527240753174 and perplexity is 40.793682990274355
At time: 594.3708868026733 and batch: 350, loss is 3.6853927993774414 and perplexity is 39.860776663435175
At time: 594.9950487613678 and batch: 400, loss is 3.6358040142059327 and perplexity is 37.93233877920147
At time: 595.6173992156982 and batch: 450, loss is 3.6426714181900026 and perplexity is 38.19373199275545
At time: 596.239253282547 and batch: 500, loss is 3.528905596733093 and perplexity is 34.08664266111795
At time: 596.8620221614838 and batch: 550, loss is 3.603798341751099 and perplexity is 36.737511388849505
At time: 597.4914438724518 and batch: 600, loss is 3.6294269657135008 and perplexity is 37.69121207088372
At time: 598.1236052513123 and batch: 650, loss is 3.4713919878005983 and perplexity is 32.181507547197285
At time: 598.7802920341492 and batch: 700, loss is 3.466371388435364 and perplexity is 32.02034200318123
At time: 599.4044489860535 and batch: 750, loss is 3.570394287109375 and perplexity is 35.530599647586016
At time: 600.0259659290314 and batch: 800, loss is 3.5148962783813475 and perplexity is 33.61244140628275
At time: 600.6452896595001 and batch: 850, loss is 3.5751861810684202 and perplexity is 35.70126709698287
At time: 601.2664439678192 and batch: 900, loss is 3.5547166633605958 and perplexity is 34.977908040636216
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452461556212543 and perplexity of 85.83797916907359
finished 49 epochs...
Completing Train Step...
At time: 602.9233858585358 and batch: 50, loss is 3.8176188850402832 and perplexity is 45.49574863692245
At time: 603.5583438873291 and batch: 100, loss is 3.6810778331756593 and perplexity is 39.689149308766126
At time: 604.1866729259491 and batch: 150, loss is 3.6956442832946776 and perplexity is 40.27151049285698
At time: 604.8139114379883 and batch: 200, loss is 3.586088433265686 and perplexity is 36.09262075646205
At time: 605.4457213878632 and batch: 250, loss is 3.73725558757782 and perplexity is 41.982614309720134
At time: 606.0732095241547 and batch: 300, loss is 3.708497099876404 and perplexity is 40.79245345143213
At time: 606.7077114582062 and batch: 350, loss is 3.685364465713501 and perplexity is 39.859647277584685
At time: 607.3304786682129 and batch: 400, loss is 3.635779218673706 and perplexity is 37.93139823833349
At time: 607.9545977115631 and batch: 450, loss is 3.6426498699188232 and perplexity is 38.19290899272827
At time: 608.5788333415985 and batch: 500, loss is 3.5288866996765136 and perplexity is 34.08599852998909
At time: 609.2120792865753 and batch: 550, loss is 3.6037799453735353 and perplexity is 36.73683555793569
At time: 609.8511629104614 and batch: 600, loss is 3.6294113540649415 and perplexity is 37.69062365352019
At time: 610.4842367172241 and batch: 650, loss is 3.4713834524154663 and perplexity is 32.18123286680849
At time: 611.1137819290161 and batch: 700, loss is 3.46636700630188 and perplexity is 32.02020168607581
At time: 611.7370367050171 and batch: 750, loss is 3.5703930377960207 and perplexity is 35.530555258761126
At time: 612.356835603714 and batch: 800, loss is 3.5148992681503297 and perplexity is 33.61254189986771
At time: 612.9856841564178 and batch: 850, loss is 3.575200695991516 and perplexity is 35.70178530189005
At time: 613.6133906841278 and batch: 900, loss is 3.5547381687164306 and perplexity is 34.97866026108334
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452459465967466 and perplexity of 85.8377997468477
finished 50 epochs...
Completing Train Step...
At time: 615.240127325058 and batch: 50, loss is 3.8175936365127563 and perplexity is 45.49459995076201
At time: 615.8635725975037 and batch: 100, loss is 3.681048855781555 and perplexity is 39.68799923730806
At time: 616.4834904670715 and batch: 150, loss is 3.695611824989319 and perplexity is 40.27020336908578
At time: 617.1011259555817 and batch: 200, loss is 3.5860600376129153 and perplexity is 36.09159589748629
At time: 617.7169923782349 and batch: 250, loss is 3.737225136756897 and perplexity is 41.98133592411395
At time: 618.3366663455963 and batch: 300, loss is 3.7084669876098633 and perplexity is 40.79122511669502
At time: 618.9582035541534 and batch: 350, loss is 3.685336289405823 and perplexity is 39.85852419572127
At time: 619.5781388282776 and batch: 400, loss is 3.635754566192627 and perplexity is 37.93046314678233
At time: 620.1960999965668 and batch: 450, loss is 3.6426281356811523 and perplexity is 38.192078907987536
At time: 620.8181095123291 and batch: 500, loss is 3.528867745399475 and perplexity is 34.08535246065272
At time: 621.4362215995789 and batch: 550, loss is 3.603761548995972 and perplexity is 36.73615973945461
At time: 622.0535633563995 and batch: 600, loss is 3.629395980834961 and perplexity is 37.69004423134846
At time: 622.6785702705383 and batch: 650, loss is 3.4713751077651978 and perplexity is 32.18096432679545
At time: 623.3131582736969 and batch: 700, loss is 3.466362705230713 and perplexity is 32.02006396520576
At time: 623.9416069984436 and batch: 750, loss is 3.5703918743133545 and perplexity is 35.53051391960001
At time: 624.5746502876282 and batch: 800, loss is 3.5149021673202516 and perplexity is 33.61263934847944
At time: 625.204080581665 and batch: 850, loss is 3.5752151823043823 and perplexity is 35.7023024928679
At time: 625.8223950862885 and batch: 900, loss is 3.554759531021118 and perplexity is 34.979407493862674
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452457793771404 and perplexity of 85.83765620933704
finished 51 epochs...
Completing Train Step...
At time: 627.4654932022095 and batch: 50, loss is 3.8175684881210326 and perplexity is 45.49345584912735
At time: 628.082218170166 and batch: 100, loss is 3.6810199356079103 and perplexity is 39.6868514700754
At time: 628.6989924907684 and batch: 150, loss is 3.695579376220703 and perplexity is 40.268896671775
At time: 629.3249487876892 and batch: 200, loss is 3.5860317087173463 and perplexity is 36.090573476917285
At time: 629.9679379463196 and batch: 250, loss is 3.7371950006484984 and perplexity is 41.980070789087044
At time: 630.6136417388916 and batch: 300, loss is 3.7084371042251587 and perplexity is 40.79000615503573
At time: 631.2900099754333 and batch: 350, loss is 3.685308156013489 and perplexity is 39.85740285599584
At time: 631.9260220527649 and batch: 400, loss is 3.6357298946380614 and perplexity is 37.929527354834875
At time: 632.5622253417969 and batch: 450, loss is 3.6426065969467163 and perplexity is 38.1912563078013
At time: 633.1856088638306 and batch: 500, loss is 3.528848886489868 and perplexity is 34.084709654133086
At time: 633.8028817176819 and batch: 550, loss is 3.60374324798584 and perplexity is 36.73548743677494
At time: 634.4207813739777 and batch: 600, loss is 3.6293804121017454 and perplexity is 37.68945744967268
At time: 635.0385735034943 and batch: 650, loss is 3.4713667535781862 and perplexity is 32.18069548212424
At time: 635.6552391052246 and batch: 700, loss is 3.4663582944869997 and perplexity is 32.01992273322139
At time: 636.2725598812103 and batch: 750, loss is 3.570390877723694 and perplexity is 35.53047851027485
At time: 636.888608455658 and batch: 800, loss is 3.5149052286148073 and perplexity is 33.61274224682678
At time: 637.5228204727173 and batch: 850, loss is 3.575229663848877 and perplexity is 35.70281952109369
At time: 638.1551198959351 and batch: 900, loss is 3.554780707359314 and perplexity is 34.98014823746875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452456121575342 and perplexity of 85.8375126720664
finished 52 epochs...
Completing Train Step...
At time: 639.8368191719055 and batch: 50, loss is 3.817543478012085 and perplexity is 45.492318067068254
At time: 640.5012533664703 and batch: 100, loss is 3.680991349220276 and perplexity is 39.68571698257082
At time: 641.1371600627899 and batch: 150, loss is 3.6955472230911255 and perplexity is 40.267601921537626
At time: 641.7722728252411 and batch: 200, loss is 3.586003499031067 and perplexity is 36.08955538752192
At time: 642.4037160873413 and batch: 250, loss is 3.737165026664734 and perplexity is 41.97881249798488
At time: 643.0388798713684 and batch: 300, loss is 3.708407311439514 and perplexity is 40.78879092522853
At time: 643.6683979034424 and batch: 350, loss is 3.685280165672302 and perplexity is 39.85628724930427
At time: 644.3021137714386 and batch: 400, loss is 3.635705442428589 and perplexity is 37.92859990542595
At time: 644.9322006702423 and batch: 450, loss is 3.642585048675537 and perplexity is 38.19043336112026
At time: 645.5613622665405 and batch: 500, loss is 3.528830075263977 and perplexity is 34.08406848499095
At time: 646.1876397132874 and batch: 550, loss is 3.6037250661849978 and perplexity is 36.734819525530455
At time: 646.8122763633728 and batch: 600, loss is 3.6293651628494263 and perplexity is 37.68888271800839
At time: 647.4629471302032 and batch: 650, loss is 3.471358313560486 and perplexity is 32.180423877630936
At time: 648.0841875076294 and batch: 700, loss is 3.4663539934158325 and perplexity is 32.019785013551115
At time: 648.704244852066 and batch: 750, loss is 3.5703897857666016 and perplexity is 35.530439712538026
At time: 649.3243412971497 and batch: 800, loss is 3.514908356666565 and perplexity is 33.61284738938869
At time: 649.9462878704071 and batch: 850, loss is 3.575244026184082 and perplexity is 35.70333230063776
At time: 650.569253206253 and batch: 900, loss is 3.554801654815674 and perplexity is 34.98088099027205
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45245361328125 and perplexity of 85.83729736661047
finished 53 epochs...
Completing Train Step...
At time: 652.1412971019745 and batch: 50, loss is 3.817518472671509 and perplexity is 45.49118053038369
At time: 652.7755427360535 and batch: 100, loss is 3.6809627199172974 and perplexity is 39.684580824419186
At time: 653.3955438137054 and batch: 150, loss is 3.6955151176452636 and perplexity is 40.266309132977035
At time: 654.0253088474274 and batch: 200, loss is 3.585975322723389 and perplexity is 36.08853853143105
At time: 654.6470785140991 and batch: 250, loss is 3.7371351146697998 and perplexity is 41.977556846737706
At time: 655.2755334377289 and batch: 300, loss is 3.7083774757385255 and perplexity is 40.787573981213086
At time: 655.9101850986481 and batch: 350, loss is 3.6852522706985473 and perplexity is 39.85517547472402
At time: 656.5304503440857 and batch: 400, loss is 3.6356808662414553 and perplexity is 37.92766777651108
At time: 657.1489539146423 and batch: 450, loss is 3.6425635528564455 and perplexity is 38.18961243529698
At time: 657.7667863368988 and batch: 500, loss is 3.528811240196228 and perplexity is 34.08342651529767
At time: 658.3844583034515 and batch: 550, loss is 3.6037069368362427 and perplexity is 36.734153553212664
At time: 659.0039501190186 and batch: 600, loss is 3.6293498945236204 and perplexity is 37.688307276260815
At time: 659.6219401359558 and batch: 650, loss is 3.4713501358032226 and perplexity is 32.18016071501187
At time: 660.2398455142975 and batch: 700, loss is 3.466349639892578 and perplexity is 32.0196456149759
At time: 660.8594233989716 and batch: 750, loss is 3.570388617515564 and perplexity is 35.53039820408921
At time: 661.4789175987244 and batch: 800, loss is 3.514911389350891 and perplexity is 33.612949326698704
At time: 662.099125623703 and batch: 850, loss is 3.5752582693099977 and perplexity is 35.70384083131685
At time: 662.7405517101288 and batch: 900, loss is 3.554822506904602 and perplexity is 34.981610422318305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452451941085188 and perplexity of 85.8371538299399
finished 54 epochs...
Completing Train Step...
At time: 664.3171994686127 and batch: 50, loss is 3.817493577003479 and perplexity is 45.49004801115239
At time: 664.9358861446381 and batch: 100, loss is 3.68093421459198 and perplexity is 39.68344961865547
At time: 665.5538864135742 and batch: 150, loss is 3.695483236312866 and perplexity is 40.26502540985465
At time: 666.1719172000885 and batch: 200, loss is 3.5859473609924315 and perplexity is 36.08752944753392
At time: 666.7867505550385 and batch: 250, loss is 3.7371053314208984 and perplexity is 41.97630663733161
At time: 667.4037089347839 and batch: 300, loss is 3.708347897529602 and perplexity is 40.786367575670134
At time: 668.0208911895752 and batch: 350, loss is 3.685224394798279 and perplexity is 39.8540644913122
At time: 668.6391620635986 and batch: 400, loss is 3.6356565713882447 and perplexity is 37.92674634058295
At time: 669.2558343410492 and batch: 450, loss is 3.6425420522689818 and perplexity is 38.1887913450216
At time: 669.8724136352539 and batch: 500, loss is 3.5287924432754516 and perplexity is 34.08278585785088
At time: 670.4885807037354 and batch: 550, loss is 3.603688988685608 and perplexity is 36.73349424900792
At time: 671.104608297348 and batch: 600, loss is 3.6293347597122194 and perplexity is 37.68773687515463
At time: 671.7214362621307 and batch: 650, loss is 3.4713418865203858 and perplexity is 32.17989525285934
At time: 672.3421080112457 and batch: 700, loss is 3.4663453483581543 and perplexity is 32.01950820185936
At time: 672.9647715091705 and batch: 750, loss is 3.570387587547302 and perplexity is 35.530361608925574
At time: 673.590234041214 and batch: 800, loss is 3.5149146509170532 and perplexity is 33.61305895773562
At time: 674.2180728912354 and batch: 850, loss is 3.5752724313735964 and perplexity is 35.704346474961895
At time: 674.8395304679871 and batch: 900, loss is 3.5548431396484377 and perplexity is 34.98233219637117
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452450686938142 and perplexity of 85.83704617759443
finished 55 epochs...
Completing Train Step...
At time: 676.4549617767334 and batch: 50, loss is 3.817468819618225 and perplexity is 45.488921810449504
At time: 677.0907635688782 and batch: 100, loss is 3.6809059810638427 and perplexity is 39.68232923068041
At time: 677.7273118495941 and batch: 150, loss is 3.695451593399048 and perplexity is 40.26375132728367
At time: 678.356511592865 and batch: 200, loss is 3.585919518470764 and perplexity is 36.08652469370086
At time: 679.0079503059387 and batch: 250, loss is 3.73707576751709 and perplexity is 41.975065672183916
At time: 679.632536649704 and batch: 300, loss is 3.708318343162537 and perplexity is 40.78516217820401
At time: 680.2574269771576 and batch: 350, loss is 3.685196590423584 and perplexity is 39.852956389375095
At time: 680.8808765411377 and batch: 400, loss is 3.6356322622299193 and perplexity is 37.92582438450743
At time: 681.5034403800964 and batch: 450, loss is 3.6425207614898683 and perplexity is 38.18797828455584
At time: 682.1294560432434 and batch: 500, loss is 3.5287738466262817 and perplexity is 34.08215203813303
At time: 682.7566986083984 and batch: 550, loss is 3.6036710691452027 and perplexity is 36.7328360075712
At time: 683.3824174404144 and batch: 600, loss is 3.62931969165802 and perplexity is 37.687168998571146
At time: 684.0070259571075 and batch: 650, loss is 3.471333646774292 and perplexity is 32.17963009978553
At time: 684.6299965381622 and batch: 700, loss is 3.466341047286987 and perplexity is 32.01937048397201
At time: 685.2535779476166 and batch: 750, loss is 3.570386543273926 and perplexity is 35.530324505534274
At time: 685.882306098938 and batch: 800, loss is 3.514917731285095 and perplexity is 33.613162498487696
At time: 686.5090317726135 and batch: 850, loss is 3.575286560058594 and perplexity is 35.704850933989945
At time: 687.1331186294556 and batch: 900, loss is 3.5548636150360107 and perplexity is 34.98304848051417
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452448596693065 and perplexity of 85.83686675731879
finished 56 epochs...
Completing Train Step...
At time: 688.7312388420105 and batch: 50, loss is 3.8174441051483154 and perplexity is 45.48779758975252
At time: 689.3599638938904 and batch: 100, loss is 3.680877766609192 and perplexity is 39.68120963119641
At time: 689.9834716320038 and batch: 150, loss is 3.6954199647903443 and perplexity is 40.26247786098709
At time: 690.611029624939 and batch: 200, loss is 3.585891695022583 and perplexity is 36.08552065611897
At time: 691.2438340187073 and batch: 250, loss is 3.7370463466644286 and perplexity is 41.97383074812768
At time: 691.8781976699829 and batch: 300, loss is 3.7082888460159302 and perplexity is 40.7839591500389
At time: 692.5109658241272 and batch: 350, loss is 3.685168967247009 and perplexity is 39.85185553932827
At time: 693.1411459445953 and batch: 400, loss is 3.6356079721450807 and perplexity is 37.924903174203735
At time: 693.769421339035 and batch: 450, loss is 3.6424994564056394 and perplexity is 38.187164695128786
At time: 694.3994288444519 and batch: 500, loss is 3.5287551641464234 and perplexity is 34.08151530496195
At time: 695.044625043869 and batch: 550, loss is 3.603653168678284 and perplexity is 36.73217847854046
At time: 695.6689429283142 and batch: 600, loss is 3.6293045806884767 and perplexity is 37.686599513210986
At time: 696.2974395751953 and batch: 650, loss is 3.4713254404067992 and perplexity is 32.17936602299871
At time: 696.9198775291443 and batch: 700, loss is 3.466336736679077 and perplexity is 32.019232461317806
At time: 697.5402798652649 and batch: 750, loss is 3.5703854894638063 and perplexity is 35.530287063338484
At time: 698.168931722641 and batch: 800, loss is 3.5149210262298585 and perplexity is 33.61327325218391
At time: 698.8001201152802 and batch: 850, loss is 3.5753004598617553 and perplexity is 35.70534722783902
At time: 699.4246106147766 and batch: 900, loss is 3.5548837900161745 and perplexity is 34.983754269942956
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452447760595034 and perplexity of 85.83679498931352
finished 57 epochs...
Completing Train Step...
At time: 701.0450119972229 and batch: 50, loss is 3.8174194288253784 and perplexity is 45.48667513201862
At time: 701.6890573501587 and batch: 100, loss is 3.6808496475219727 and perplexity is 39.68009384748931
At time: 702.3137340545654 and batch: 150, loss is 3.6953884077072146 and perplexity is 40.2612073146737
At time: 702.9457249641418 and batch: 200, loss is 3.5858640241622926 and perplexity is 36.084522152533175
At time: 703.5652306079865 and batch: 250, loss is 3.7370169734954835 and perplexity is 41.97259786181282
At time: 704.1830394268036 and batch: 300, loss is 3.7082594537734987 and perplexity is 40.78276043564079
At time: 704.8176863193512 and batch: 350, loss is 3.6851414918899534 and perplexity is 39.85076061040985
At time: 705.4436128139496 and batch: 400, loss is 3.6355838346481324 and perplexity is 37.9239877730169
At time: 706.0652582645416 and batch: 450, loss is 3.6424780797958376 and perplexity is 38.18634839173459
At time: 706.6928598880768 and batch: 500, loss is 3.528736596107483 and perplexity is 34.08088248393375
At time: 707.3193962574005 and batch: 550, loss is 3.603635377883911 and perplexity is 36.73152498971935
At time: 707.9404680728912 and batch: 600, loss is 3.629289650917053 and perplexity is 37.686036865094636
At time: 708.5647692680359 and batch: 650, loss is 3.4713173484802247 and perplexity is 32.17910563098517
At time: 709.1858878135681 and batch: 700, loss is 3.466332483291626 and perplexity is 32.01909627140589
At time: 709.8048141002655 and batch: 750, loss is 3.5703844308853148 and perplexity is 35.53024945176071
At time: 710.4520609378815 and batch: 800, loss is 3.514924292564392 and perplexity is 33.613383044558425
At time: 711.0759720802307 and batch: 850, loss is 3.5753144598007203 and perplexity is 35.705847104020044
At time: 711.6970131397247 and batch: 900, loss is 3.5549039745330813 and perplexity is 34.984460407248974
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452445670349957 and perplexity of 85.83661556956284
finished 58 epochs...
Completing Train Step...
At time: 713.2919871807098 and batch: 50, loss is 3.817395067214966 and perplexity is 45.48556701685788
At time: 713.9180088043213 and batch: 100, loss is 3.6808216857910154 and perplexity is 39.67898433889275
At time: 714.543449640274 and batch: 150, loss is 3.695357232093811 and perplexity is 40.259952166404396
At time: 715.171285867691 and batch: 200, loss is 3.585836424827576 and perplexity is 36.08352625747128
At time: 715.7944383621216 and batch: 250, loss is 3.736987729072571 and perplexity is 41.97137041535827
At time: 716.4188303947449 and batch: 300, loss is 3.7082301568984986 and perplexity is 40.781565645708035
At time: 717.0433564186096 and batch: 350, loss is 3.685113925933838 and perplexity is 39.84966210123249
At time: 717.6690104007721 and batch: 400, loss is 3.6355596780776978 and perplexity is 37.92307167060009
At time: 718.290388584137 and batch: 450, loss is 3.6424568271636963 and perplexity is 38.18553683994324
At time: 718.9125552177429 and batch: 500, loss is 3.528718123435974 and perplexity is 34.08025292480173
At time: 719.5337793827057 and batch: 550, loss is 3.6036176347732543 and perplexity is 36.730873263988705
At time: 720.1568598747253 and batch: 600, loss is 3.629274821281433 and perplexity is 37.68547799904387
At time: 720.779025554657 and batch: 650, loss is 3.4713092756271364 and perplexity is 32.17884585484147
At time: 721.3995378017426 and batch: 700, loss is 3.466328263282776 and perplexity is 32.01896115082137
At time: 722.0212731361389 and batch: 750, loss is 3.5703834438323976 and perplexity is 35.53021438154165
At time: 722.6440098285675 and batch: 800, loss is 3.5149275159835813 and perplexity is 33.61349139475698
At time: 723.2687840461731 and batch: 850, loss is 3.575328326225281 and perplexity is 35.706342219888015
At time: 723.8873600959778 and batch: 900, loss is 3.5549239253997804 and perplexity is 34.9851583845177
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452445252300942 and perplexity of 85.83657968565775
finished 59 epochs...
Completing Train Step...
At time: 725.4868612289429 and batch: 50, loss is 3.8173705625534056 and perplexity is 45.484452422088694
At time: 726.1107635498047 and batch: 100, loss is 3.680793843269348 and perplexity is 39.67787959129111
At time: 726.750198841095 and batch: 150, loss is 3.695326089859009 and perplexity is 40.25869840104354
At time: 727.3755836486816 and batch: 200, loss is 3.5858090209960936 and perplexity is 36.08253744414713
At time: 728.0001740455627 and batch: 250, loss is 3.7369585752487184 and perplexity is 41.97014680725486
At time: 728.6226668357849 and batch: 300, loss is 3.708201026916504 and perplexity is 40.78037769673762
At time: 729.2471039295197 and batch: 350, loss is 3.6850865983963015 and perplexity is 39.84857312297522
At time: 729.8732891082764 and batch: 400, loss is 3.6355356931686402 and perplexity is 37.92216210008291
At time: 730.5092506408691 and batch: 450, loss is 3.642435669898987 and perplexity is 38.18472894697867
At time: 731.1420648097992 and batch: 500, loss is 3.5286996269226076 and perplexity is 34.07962256477773
At time: 731.7746181488037 and batch: 550, loss is 3.603600058555603 and perplexity is 36.73022767983918
At time: 732.3965606689453 and batch: 600, loss is 3.6292600774765016 and perplexity is 37.68492237580351
At time: 733.0291135311127 and batch: 650, loss is 3.471301169395447 and perplexity is 32.17858500671872
At time: 733.6613535881042 and batch: 700, loss is 3.4663239669799806 and perplexity is 32.018823587964576
At time: 734.2864420413971 and batch: 750, loss is 3.570382490158081 and perplexity is 35.53018049730489
At time: 734.912876367569 and batch: 800, loss is 3.514930920600891 and perplexity is 33.61360583602644
At time: 735.5391185283661 and batch: 850, loss is 3.5753420972824097 and perplexity is 35.706833937352314
At time: 736.1727313995361 and batch: 900, loss is 3.554943618774414 and perplexity is 34.985847367132564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45244358010488 and perplexity of 85.83643615018727
finished 60 epochs...
Completing Train Step...
At time: 737.834361076355 and batch: 50, loss is 3.8173462677001955 and perplexity is 45.48334739741701
At time: 738.4692332744598 and batch: 100, loss is 3.680766153335571 and perplexity is 39.67678092864383
At time: 739.0957643985748 and batch: 150, loss is 3.695295033454895 and perplexity is 40.257448130051465
At time: 739.7167558670044 and batch: 200, loss is 3.5857815599441527 and perplexity is 36.081546593317185
At time: 740.3385760784149 and batch: 250, loss is 3.736929636001587 and perplexity is 41.96893224037866
At time: 740.9620525836945 and batch: 300, loss is 3.7081719493865966 and perplexity is 40.77919192132529
At time: 741.5857882499695 and batch: 350, loss is 3.685059418678284 and perplexity is 39.84749006471301
At time: 742.2129950523376 and batch: 400, loss is 3.6355116844177244 and perplexity is 37.92125164726832
At time: 742.8708584308624 and batch: 450, loss is 3.642414479255676 and perplexity is 38.18391979658088
At time: 743.4913156032562 and batch: 500, loss is 3.5286811542510987 and perplexity is 34.07899302891956
At time: 744.1131911277771 and batch: 550, loss is 3.603582377433777 and perplexity is 36.72957825395017
At time: 744.7334825992584 and batch: 600, loss is 3.6292452716827395 and perplexity is 37.68436442474533
At time: 745.3534934520721 and batch: 650, loss is 3.471293125152588 and perplexity is 32.178326155407206
At time: 745.9713995456696 and batch: 700, loss is 3.466319689750671 and perplexity is 32.01868663640676
At time: 746.5900542736053 and batch: 750, loss is 3.570381426811218 and perplexity is 35.53014271641901
At time: 747.208044052124 and batch: 800, loss is 3.514934191703796 and perplexity is 33.61371578976997
At time: 747.8262300491333 and batch: 850, loss is 3.575355820655823 and perplexity is 35.70732395893021
At time: 748.4576315879822 and batch: 900, loss is 3.5549633073806763 and perplexity is 34.986536196487144
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452440653761772 and perplexity of 85.83618496369144
finished 61 epochs...
Completing Train Step...
At time: 750.07794880867 and batch: 50, loss is 3.8173219776153564 and perplexity is 45.48224261646764
At time: 750.7178192138672 and batch: 100, loss is 3.6807386112213134 and perplexity is 39.67568816125875
At time: 751.3427269458771 and batch: 150, loss is 3.6952641820907592 and perplexity is 40.256206152018486
At time: 751.9661946296692 and batch: 200, loss is 3.585754270553589 and perplexity is 36.08056196333509
At time: 752.5900144577026 and batch: 250, loss is 3.7369006967544554 and perplexity is 41.96771770865062
At time: 753.2164633274078 and batch: 300, loss is 3.708142890930176 and perplexity is 40.778006958170664
At time: 753.8421449661255 and batch: 350, loss is 3.685032048225403 and perplexity is 39.84639943578934
At time: 754.4672553539276 and batch: 400, loss is 3.6354878091812135 and perplexity is 37.92034627922443
At time: 755.0924866199493 and batch: 450, loss is 3.6423932838439943 and perplexity is 38.18311048125808
At time: 755.7185566425323 and batch: 500, loss is 3.528662824630737 and perplexity is 34.07836837963985
At time: 756.3448467254639 and batch: 550, loss is 3.6035649585723877 and perplexity is 36.72893847208984
At time: 756.9697830677032 and batch: 600, loss is 3.6292306280136106 and perplexity is 37.68381259142181
At time: 757.5980408191681 and batch: 650, loss is 3.471285171508789 and perplexity is 32.178070221480716
At time: 758.2260110378265 and batch: 700, loss is 3.4663154602050783 and perplexity is 32.018551212198204
At time: 758.8801987171173 and batch: 750, loss is 3.5703805017471315 and perplexity is 35.53010984877519
At time: 759.5063326358795 and batch: 800, loss is 3.514937605857849 and perplexity is 33.613830552369876
At time: 760.131796836853 and batch: 850, loss is 3.5753693628311156 and perplexity is 35.7078075170447
At time: 760.7548592090607 and batch: 900, loss is 3.554982690811157 and perplexity is 34.98721436215184
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452439817663741 and perplexity of 85.83611319625622
finished 62 epochs...
Completing Train Step...
At time: 762.370992898941 and batch: 50, loss is 3.8172977924346925 and perplexity is 45.48114263351465
At time: 763.0157504081726 and batch: 100, loss is 3.6807111740112304 and perplexity is 39.67459958600128
At time: 763.6457123756409 and batch: 150, loss is 3.695233473777771 and perplexity is 40.25496997082085
At time: 764.2725846767426 and batch: 200, loss is 3.585727095603943 and perplexity is 36.07958148920276
At time: 764.8939247131348 and batch: 250, loss is 3.736872000694275 and perplexity is 41.966513417776895
At time: 765.5240602493286 and batch: 300, loss is 3.7081139945983885 and perplexity is 40.776828640376586
At time: 766.1565613746643 and batch: 350, loss is 3.685004987716675 and perplexity is 39.845321186538676
At time: 766.7798874378204 and batch: 400, loss is 3.635464038848877 and perplexity is 37.91944491070401
At time: 767.4011816978455 and batch: 450, loss is 3.6423722743988036 and perplexity is 38.18230828371811
At time: 768.0228679180145 and batch: 500, loss is 3.5286444997787476 and perplexity is 34.077743904304974
At time: 768.6450078487396 and batch: 550, loss is 3.603547549247742 and perplexity is 36.72829905164203
At time: 769.2728865146637 and batch: 600, loss is 3.6292160654067995 and perplexity is 37.68326382087167
At time: 769.9058406352997 and batch: 650, loss is 3.471277189254761 and perplexity is 32.17781336897521
At time: 770.5279469490051 and batch: 700, loss is 3.4663112354278565 and perplexity is 32.01841594123811
At time: 771.1630356311798 and batch: 750, loss is 3.570379672050476 and perplexity is 35.53008036957411
At time: 771.7906947135925 and batch: 800, loss is 3.514940981864929 and perplexity is 33.613944033091364
At time: 772.4186294078827 and batch: 850, loss is 3.575382881164551 and perplexity is 35.70829023035569
At time: 773.0489897727966 and batch: 900, loss is 3.55500214099884 and perplexity is 34.98789487665574
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452438563516695 and perplexity of 85.83600554521594
finished 63 epochs...
Completing Train Step...
At time: 774.6666345596313 and batch: 50, loss is 3.8172736310958864 and perplexity is 45.480043761493356
At time: 775.2872061729431 and batch: 100, loss is 3.6806838655471803 and perplexity is 39.67351614841836
At time: 775.9103181362152 and batch: 150, loss is 3.695202865600586 and perplexity is 40.253737858423875
At time: 776.5342192649841 and batch: 200, loss is 3.5857000160217285 and perplexity is 36.078604482438095
At time: 777.1539068222046 and batch: 250, loss is 3.7368433141708373 and perplexity is 41.96530956167345
At time: 777.7729065418243 and batch: 300, loss is 3.7080851793289185 and perplexity is 40.77565366199992
At time: 778.3935310840607 and batch: 350, loss is 3.6849779891967773 and perplexity is 39.84424543636369
At time: 779.016678571701 and batch: 400, loss is 3.6354401540756225 and perplexity is 37.91853922417649
At time: 779.6373624801636 and batch: 450, loss is 3.642351198196411 and perplexity is 38.18150355414125
At time: 780.2576341629028 and batch: 500, loss is 3.528626275062561 and perplexity is 34.077122852753305
At time: 780.8791573047638 and batch: 550, loss is 3.6035301589965822 and perplexity is 36.72766034285053
At time: 781.5049724578857 and batch: 600, loss is 3.62920156955719 and perplexity is 37.68271757390569
At time: 782.1296598911285 and batch: 650, loss is 3.4712692070007325 and perplexity is 32.17755651851994
At time: 782.753187417984 and batch: 700, loss is 3.46630699634552 and perplexity is 32.01828021282434
At time: 783.3762419223785 and batch: 750, loss is 3.5703787136077882 and perplexity is 35.5300463160447
At time: 784.001234292984 and batch: 800, loss is 3.5149443531036377 and perplexity is 33.614057353911655
At time: 784.6243586540222 and batch: 850, loss is 3.575396304130554 and perplexity is 35.70876954473839
At time: 785.2488212585449 and batch: 900, loss is 3.5550212907791137 and perplexity is 34.98856489357018
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452437727418665 and perplexity of 85.83593377793073
finished 64 epochs...
Completing Train Step...
At time: 786.8886976242065 and batch: 50, loss is 3.817249574661255 and perplexity is 45.47894968695339
At time: 787.5086541175842 and batch: 100, loss is 3.6806565618515013 and perplexity is 39.67243292959493
At time: 788.1285171508789 and batch: 150, loss is 3.695172390937805 and perplexity is 40.2525111580287
At time: 788.7472004890442 and batch: 200, loss is 3.585673007965088 and perplexity is 36.0776300826031
At time: 789.3655123710632 and batch: 250, loss is 3.736814799308777 and perplexity is 41.96411294372075
At time: 789.9838790893555 and batch: 300, loss is 3.7080565118789672 and perplexity is 40.77448474474435
At time: 790.6178534030914 and batch: 350, loss is 3.6849511241912842 and perplexity is 39.84317503486941
At time: 791.237051486969 and batch: 400, loss is 3.635416407585144 and perplexity is 37.917638802636816
At time: 791.855384349823 and batch: 450, loss is 3.6423303365707396 and perplexity is 38.18070703421492
At time: 792.4711706638336 and batch: 500, loss is 3.528607954978943 and perplexity is 34.07649856273172
At time: 793.0879192352295 and batch: 550, loss is 3.6035129833221435 and perplexity is 36.72702952593095
At time: 793.7049334049225 and batch: 600, loss is 3.629187111854553 and perplexity is 37.68217277231876
At time: 794.3239743709564 and batch: 650, loss is 3.471261348724365 and perplexity is 32.177303659381515
At time: 794.9397649765015 and batch: 700, loss is 3.4663027906417847 and perplexity is 32.01814555370681
At time: 795.5570304393768 and batch: 750, loss is 3.5703777503967284 and perplexity is 35.53001209312762
At time: 796.1724286079407 and batch: 800, loss is 3.514947872161865 and perplexity is 33.61417564394488
At time: 796.7899551391602 and batch: 850, loss is 3.575409665107727 and perplexity is 35.70924665198046
At time: 797.4065611362457 and batch: 900, loss is 3.555040144920349 and perplexity is 34.989224579133186
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452436055222603 and perplexity of 85.83579024354033
finished 65 epochs...
Completing Train Step...
At time: 798.9660496711731 and batch: 50, loss is 3.81722562789917 and perplexity is 45.47786062640516
At time: 799.5999212265015 and batch: 100, loss is 3.680629358291626 and perplexity is 39.67135371286967
At time: 800.2190206050873 and batch: 150, loss is 3.695142025947571 and perplexity is 40.25128890947737
At time: 800.8392117023468 and batch: 200, loss is 3.5856461191177367 and perplexity is 36.07666000975714
At time: 801.4589817523956 and batch: 250, loss is 3.736786365509033 and perplexity is 41.96291976150032
At time: 802.0780324935913 and batch: 300, loss is 3.70802770614624 and perplexity is 40.77331022275127
At time: 802.697610616684 and batch: 350, loss is 3.6849241924285887 and perplexity is 39.84210200238373
At time: 803.3175184726715 and batch: 400, loss is 3.6353928327560423 and perplexity is 37.91674491131881
At time: 803.9379906654358 and batch: 450, loss is 3.6423093271255493 and perplexity is 38.17990488716952
At time: 804.5564699172974 and batch: 500, loss is 3.528589758872986 and perplexity is 34.075878508794524
At time: 805.1779623031616 and batch: 550, loss is 3.603495707511902 and perplexity is 36.726395042218755
At time: 805.7985303401947 and batch: 600, loss is 3.6291728591918946 and perplexity is 37.68163570484933
At time: 806.4447865486145 and batch: 650, loss is 3.4712534666061403 and perplexity is 32.17705003506946
At time: 807.065507888794 and batch: 700, loss is 3.466298608779907 and perplexity is 32.01801165852449
At time: 807.68603682518 and batch: 750, loss is 3.570376868247986 and perplexity is 35.52998075038595
At time: 808.3058862686157 and batch: 800, loss is 3.514951243400574 and perplexity is 33.61428896554599
At time: 808.9250655174255 and batch: 850, loss is 3.5754229640960693 and perplexity is 35.709721551993226
At time: 809.5454351902008 and batch: 900, loss is 3.555059008598328 and perplexity is 34.989884610823665
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452435637173587 and perplexity of 85.83575435998019
finished 66 epochs...
Completing Train Step...
At time: 811.1183290481567 and batch: 50, loss is 3.8172017765045165 and perplexity is 45.47677592893918
At time: 811.753155708313 and batch: 100, loss is 3.6806024265289308 and perplexity is 39.67028530777276
At time: 812.3730404376984 and batch: 150, loss is 3.6951118993759153 and perplexity is 40.25007629440386
At time: 812.9932928085327 and batch: 200, loss is 3.58561927318573 and perplexity is 36.07569151119567
At time: 813.6143820285797 and batch: 250, loss is 3.7367579746246338 and perplexity is 41.9617284140081
At time: 814.2337970733643 and batch: 300, loss is 3.7079991340637206 and perplexity is 40.77214526100977
At time: 814.8547370433807 and batch: 350, loss is 3.68489755153656 and perplexity is 39.84104058738467
At time: 815.4752852916718 and batch: 400, loss is 3.6353692245483398 and perplexity is 37.91584977549586
At time: 816.0999705791473 and batch: 450, loss is 3.642288451194763 and perplexity is 38.1791078544371
At time: 816.7210245132446 and batch: 500, loss is 3.528571572303772 and perplexity is 34.075258791106776
At time: 817.3424346446991 and batch: 550, loss is 3.60347861289978 and perplexity is 36.725767224107045
At time: 817.9647817611694 and batch: 600, loss is 3.6291585111618043 and perplexity is 37.681095051485045
At time: 818.5866808891296 and batch: 650, loss is 3.471245665550232 and perplexity is 32.17679902108227
At time: 819.2074460983276 and batch: 700, loss is 3.466294345855713 and perplexity is 32.01787516845887
At time: 819.8283920288086 and batch: 750, loss is 3.57037588596344 and perplexity is 35.529945849852076
At time: 820.4476909637451 and batch: 800, loss is 3.514954786300659 and perplexity is 33.61440805782419
At time: 821.0659720897675 and batch: 850, loss is 3.57543607711792 and perplexity is 35.710189817422396
At time: 821.68425822258 and batch: 900, loss is 3.5550777959823607 and perplexity is 34.99054198539827
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524339649775255 and perplexity of 85.83561082588982
finished 67 epochs...
Completing Train Step...
At time: 823.3090884685516 and batch: 50, loss is 3.8171779108047486 and perplexity is 45.475690606809486
At time: 823.941061258316 and batch: 100, loss is 3.680575337409973 and perplexity is 39.669210689250264
At time: 824.5706012248993 and batch: 150, loss is 3.6950817823410036 and perplexity is 40.24886409970485
At time: 825.1946382522583 and batch: 200, loss is 3.585592551231384 and perplexity is 36.07472751109416
At time: 825.8179404735565 and batch: 250, loss is 3.736729736328125 and perplexity is 41.96054350300914
At time: 826.4445641040802 and batch: 300, loss is 3.7079706239700316 and perplexity is 40.77098285989864
At time: 827.0709419250488 and batch: 350, loss is 3.68487078666687 and perplexity is 39.839974261395135
At time: 827.695990562439 and batch: 400, loss is 3.6353458070755007 and perplexity is 37.9149618925096
At time: 828.3198363780975 and batch: 450, loss is 3.6422676134109495 and perplexity is 38.17831229473031
At time: 828.9455626010895 and batch: 500, loss is 3.528553566932678 and perplexity is 34.07464525895058
At time: 829.5699541568756 and batch: 550, loss is 3.6034615087509154 and perplexity is 36.725139066489334
At time: 830.193062543869 and batch: 600, loss is 3.62914430141449 and perplexity is 37.68055961645005
At time: 830.815182685852 and batch: 650, loss is 3.471237859725952 and perplexity is 32.1765478556235
At time: 831.4375839233398 and batch: 700, loss is 3.4662901306152345 and perplexity is 32.01774020569988
At time: 832.0610318183899 and batch: 750, loss is 3.5703750705718993 and perplexity is 35.5299168790466
At time: 832.6850078105927 and batch: 800, loss is 3.514958357810974 and perplexity is 33.6145281122437
At time: 833.3069977760315 and batch: 850, loss is 3.575449161529541 and perplexity is 35.71065706730188
At time: 833.9333736896515 and batch: 900, loss is 3.555096354484558 and perplexity is 34.991191363474314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524339649775255 and perplexity of 85.83561082588982
finished 68 epochs...
Completing Train Step...
At time: 835.5514061450958 and batch: 50, loss is 3.8171542263031006 and perplexity is 45.47461355049519
At time: 836.17595911026 and batch: 100, loss is 3.680548596382141 and perplexity is 39.6681499079664
At time: 836.8028953075409 and batch: 150, loss is 3.695051712989807 and perplexity is 40.24765386067062
At time: 837.4276313781738 and batch: 200, loss is 3.585565891265869 and perplexity is 36.073765772922755
At time: 838.0669512748718 and batch: 250, loss is 3.736701683998108 and perplexity is 41.959366428505014
At time: 838.691556930542 and batch: 300, loss is 3.7079422903060912 and perplexity is 40.76982768493701
At time: 839.4790649414062 and batch: 350, loss is 3.684844241142273 and perplexity is 39.838916702415226
At time: 840.1050548553467 and batch: 400, loss is 3.635322217941284 and perplexity is 37.914067521933454
At time: 840.7352049350739 and batch: 450, loss is 3.6422468423843384 and perplexity is 38.17751930022535
At time: 841.3619012832642 and batch: 500, loss is 3.528535499572754 and perplexity is 34.07402962563186
At time: 841.9869709014893 and batch: 550, loss is 3.6034445858001707 and perplexity is 36.72451757402858
At time: 842.6080536842346 and batch: 600, loss is 3.629130177497864 and perplexity is 37.680027423125935
At time: 843.2281758785248 and batch: 650, loss is 3.4712300395965574 and perplexity is 32.17629623183966
At time: 843.8500380516052 and batch: 700, loss is 3.4662858390808107 and perplexity is 32.01760280076045
At time: 844.4677104949951 and batch: 750, loss is 3.5703741788864134 and perplexity is 35.52988519754953
At time: 845.0885932445526 and batch: 800, loss is 3.5149618005752563 and perplexity is 33.61464383933965
At time: 845.7109880447388 and batch: 850, loss is 3.5754622554779054 and perplexity is 35.71112466386291
At time: 846.3351457118988 and batch: 900, loss is 3.5551147508621215 and perplexity is 34.99183508056305
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452431456683433 and perplexity of 85.83539552520428
finished 69 epochs...
Completing Train Step...
At time: 847.9261674880981 and batch: 50, loss is 3.8171304845809937 and perplexity is 45.47353391767358
At time: 848.5478513240814 and batch: 100, loss is 3.6805218982696535 and perplexity is 39.66709085737538
At time: 849.1770689487457 and batch: 150, loss is 3.695021948814392 and perplexity is 40.24645594026871
At time: 849.8066158294678 and batch: 200, loss is 3.5855392837524414 and perplexity is 36.07280595248484
At time: 850.4379768371582 and batch: 250, loss is 3.7366735792160033 and perplexity is 41.95818718622554
At time: 851.0638229846954 and batch: 300, loss is 3.7079139041900633 and perplexity is 40.768670404303336
At time: 851.6942319869995 and batch: 350, loss is 3.6848176288604737 and perplexity is 39.837856512044475
At time: 852.3221189975739 and batch: 400, loss is 3.6352989149093626 and perplexity is 37.9131840195019
At time: 852.9534645080566 and batch: 450, loss is 3.6422261667251585 and perplexity is 38.17672996300803
At time: 853.5881624221802 and batch: 500, loss is 3.528517599105835 and perplexity is 34.07341969005083
At time: 854.2463941574097 and batch: 550, loss is 3.6034275245666505 and perplexity is 36.72389101380329
At time: 854.8672626018524 and batch: 600, loss is 3.629116005897522 and perplexity is 37.67949344062012
At time: 855.4949197769165 and batch: 650, loss is 3.471222267150879 and perplexity is 32.176046144296954
At time: 856.1233644485474 and batch: 700, loss is 3.466281633377075 and perplexity is 32.017468144491914
At time: 856.7549383640289 and batch: 750, loss is 3.570373330116272 and perplexity is 35.52985504085665
At time: 857.3856647014618 and batch: 800, loss is 3.5149653482437135 and perplexity is 33.614763093162836
At time: 858.0099365711212 and batch: 850, loss is 3.5754751777648925 and perplexity is 35.71158613624608
At time: 858.6314017772675 and batch: 900, loss is 3.555133008956909 and perplexity is 34.99247397063718
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452430202536387 and perplexity of 85.83528787506407
finished 70 epochs...
Completing Train Step...
At time: 860.2365350723267 and batch: 50, loss is 3.817107057571411 and perplexity is 45.47246862123714
At time: 860.8569800853729 and batch: 100, loss is 3.680495276451111 and perplexity is 39.6660348613368
At time: 861.4760575294495 and batch: 150, loss is 3.6949921607971192 and perplexity is 40.245257095999676
At time: 862.0982437133789 and batch: 200, loss is 3.585512819290161 and perplexity is 36.07185131770437
At time: 862.7184290885925 and batch: 250, loss is 3.736645750999451 and perplexity is 41.95701958095263
At time: 863.3422205448151 and batch: 300, loss is 3.707885751724243 and perplexity is 40.76752268185892
At time: 863.9680452346802 and batch: 350, loss is 3.6847911739349364 and perplexity is 39.83680261845728
At time: 864.5925869941711 and batch: 400, loss is 3.6352755069732665 and perplexity is 37.9122965605
At time: 865.2202229499817 and batch: 450, loss is 3.6422053289413454 and perplexity is 38.17593445285073
At time: 865.8493876457214 and batch: 500, loss is 3.528499598503113 and perplexity is 34.072806353479834
At time: 866.5057175159454 and batch: 550, loss is 3.603410773277283 and perplexity is 36.72327584643065
At time: 867.1375713348389 and batch: 600, loss is 3.6291020059585573 and perplexity is 37.678965933704276
At time: 867.7709517478943 and batch: 650, loss is 3.4712145376205443 and perplexity is 32.175797439533426
At time: 868.40092420578 and batch: 700, loss is 3.466277446746826 and perplexity is 32.017334099471874
At time: 869.0261054039001 and batch: 750, loss is 3.5703724098205565 and perplexity is 35.52982234289833
At time: 869.6578304767609 and batch: 800, loss is 3.5149688863754274 and perplexity is 33.61488202683259
At time: 870.2919816970825 and batch: 850, loss is 3.5754880332946777 and perplexity is 35.71204523055627
At time: 870.9211564064026 and batch: 900, loss is 3.5551511001586915 and perplexity is 34.993107032271055
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452428948389341 and perplexity of 85.83518022505878
finished 71 epochs...
Completing Train Step...
At time: 872.525384426117 and batch: 50, loss is 3.8170834970474243 and perplexity is 45.471397278670196
At time: 873.1645197868347 and batch: 100, loss is 3.680468788146973 and perplexity is 39.66498418925676
At time: 873.783016204834 and batch: 150, loss is 3.69496253490448 and perplexity is 40.244064811995045
At time: 874.4076681137085 and batch: 200, loss is 3.5854863834381105 and perplexity is 36.070897740184115
At time: 875.0337393283844 and batch: 250, loss is 3.7366179037094116 and perplexity is 41.95585120792727
At time: 875.6576147079468 and batch: 300, loss is 3.707857484817505 and perplexity is 40.76637032638418
At time: 876.2833006381989 and batch: 350, loss is 3.684764852523804 and perplexity is 39.835754071397034
At time: 876.9154093265533 and batch: 400, loss is 3.6352523040771483 and perplexity is 37.911416895626736
At time: 877.5379667282104 and batch: 450, loss is 3.642184772491455 and perplexity is 38.17514969923304
At time: 878.161209821701 and batch: 500, loss is 3.528481707572937 and perplexity is 34.07219676473353
At time: 878.785722732544 and batch: 550, loss is 3.6033938789367674 and perplexity is 36.722655436144386
At time: 879.4078242778778 and batch: 600, loss is 3.6290879154205324 and perplexity is 37.67843502054249
At time: 880.0290849208832 and batch: 650, loss is 3.4712069702148436 and perplexity is 32.17555395314174
At time: 880.648199558258 and batch: 700, loss is 3.4662733125686644 and perplexity is 32.01720173438206
At time: 881.2705359458923 and batch: 750, loss is 3.5703716230392457 and perplexity is 35.52979438870913
At time: 881.8914685249329 and batch: 800, loss is 3.5149724435806275 and perplexity is 33.615001602078415
At time: 882.5403234958649 and batch: 850, loss is 3.5755007886886596 and perplexity is 35.71250075466828
At time: 883.1739253997803 and batch: 900, loss is 3.5551691913604735 and perplexity is 34.993740105357865
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452427694242295 and perplexity of 85.83507257518858
finished 72 epochs...
Completing Train Step...
At time: 884.7970144748688 and batch: 50, loss is 3.817060089111328 and perplexity is 45.470332899566
At time: 885.4409654140472 and batch: 100, loss is 3.6804423093795777 and perplexity is 39.663933923271614
At time: 886.0626361370087 and batch: 150, loss is 3.694933066368103 and perplexity is 40.242878895780855
At time: 886.68465924263 and batch: 200, loss is 3.585460066795349 and perplexity is 36.06994848774483
At time: 887.3059811592102 and batch: 250, loss is 3.7365901947021483 and perplexity is 41.95468866904788
At time: 887.9281373023987 and batch: 300, loss is 3.7078294277191164 and perplexity is 40.76522655636649
At time: 888.5491681098938 and batch: 350, loss is 3.6847385501861574 and perplexity is 39.83470631172237
At time: 889.1772232055664 and batch: 400, loss is 3.6352291440963747 and perplexity is 37.9105388781078
At time: 889.8042376041412 and batch: 450, loss is 3.6421641540527343 and perplexity is 38.17436259536276
At time: 890.4276933670044 and batch: 500, loss is 3.5284639501571657 and perplexity is 34.07159173594123
At time: 891.0511996746063 and batch: 550, loss is 3.603377137184143 and perplexity is 36.72204063967777
At time: 891.6743314266205 and batch: 600, loss is 3.6290741300582887 and perplexity is 37.677915613247066
At time: 892.3109610080719 and batch: 650, loss is 3.4711992168426513 and perplexity is 32.175304485063556
At time: 892.9439649581909 and batch: 700, loss is 3.4662691259384157 and perplexity is 32.0170676904774
At time: 893.5679914951324 and batch: 750, loss is 3.570370659828186 and perplexity is 35.529760166034706
At time: 894.1894850730896 and batch: 800, loss is 3.5149760484695434 and perplexity is 33.61512278064352
At time: 894.8121864795685 and batch: 850, loss is 3.575513620376587 and perplexity is 35.71295900927315
At time: 895.4327991008759 and batch: 900, loss is 3.555187029838562 and perplexity is 34.9943643459917
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452426440095248 and perplexity of 85.83496492545332
finished 73 epochs...
Completing Train Step...
At time: 897.030035495758 and batch: 50, loss is 3.8170366144180297 and perplexity is 45.46926550997537
At time: 897.6552784442902 and batch: 100, loss is 3.680415916442871 and perplexity is 39.662887089388605
At time: 898.2863426208496 and batch: 150, loss is 3.694903540611267 and perplexity is 40.2416907118653
At time: 898.9044778347015 and batch: 200, loss is 3.585433702468872 and perplexity is 36.06899754038248
At time: 899.5222382545471 and batch: 250, loss is 3.73656259059906 and perplexity is 41.95353056348113
At time: 900.1401264667511 and batch: 300, loss is 3.7078014373779298 and perplexity is 40.764085539735426
At time: 900.7585718631744 and batch: 350, loss is 3.684712357521057 and perplexity is 39.833662948264866
At time: 901.3772356510162 and batch: 400, loss is 3.6352059841156006 and perplexity is 37.90966088092349
At time: 901.999089717865 and batch: 450, loss is 3.642143530845642 and perplexity is 38.17357532569538
At time: 902.6216645240784 and batch: 500, loss is 3.5284460544586183 and perplexity is 34.07098200646228
At time: 903.2465240955353 and batch: 550, loss is 3.6033604097366334 and perplexity is 36.72142637880804
At time: 903.8702218532562 and batch: 600, loss is 3.629060263633728 and perplexity is 37.67739315889491
At time: 904.5041964054108 and batch: 650, loss is 3.4711915254592896 and perplexity is 32.17505701341368
At time: 905.1278884410858 and batch: 700, loss is 3.4662648963928224 and perplexity is 32.01693227311621
At time: 905.7528991699219 and batch: 750, loss is 3.5703698635101317 and perplexity is 35.52973187305649
At time: 906.377402305603 and batch: 800, loss is 3.514979591369629 and perplexity is 33.61524187587587
At time: 907.020560503006 and batch: 850, loss is 3.5755262756347657 and perplexity is 35.713410968849566
At time: 907.649822473526 and batch: 900, loss is 3.5552047634124757 and perplexity is 34.99498492664094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452426440095248 and perplexity of 85.83496492545332
finished 74 epochs...
Completing Train Step...
At time: 909.2759506702423 and batch: 50, loss is 3.817013363838196 and perplexity is 45.468208335477655
At time: 909.8988645076752 and batch: 100, loss is 3.680389795303345 and perplexity is 39.66185106311209
At time: 910.5219492912292 and batch: 150, loss is 3.694874382019043 and perplexity is 40.24051733792248
At time: 911.1435406208038 and batch: 200, loss is 3.585407562255859 and perplexity is 36.06805470142668
At time: 911.7738935947418 and batch: 250, loss is 3.736535015106201 and perplexity is 41.95237369014942
At time: 912.4032607078552 and batch: 300, loss is 3.7077734899520873 and perplexity is 40.76294630439719
At time: 913.0317187309265 and batch: 350, loss is 3.684686093330383 and perplexity is 39.83261676308462
At time: 913.6719918251038 and batch: 400, loss is 3.6351828241348265 and perplexity is 37.90878290407334
At time: 914.3396804332733 and batch: 450, loss is 3.6421230459213256 and perplexity is 38.172793350903326
At time: 914.9721610546112 and batch: 500, loss is 3.5284283113479615 and perplexity is 34.0703774866214
At time: 915.6060059070587 and batch: 550, loss is 3.6033438444137573 and perplexity is 36.72081808156194
At time: 916.23122215271 and batch: 600, loss is 3.6290464639663695 and perplexity is 37.676873226989834
At time: 916.8551235198975 and batch: 650, loss is 3.4711838483810427 and perplexity is 32.17481000393155
At time: 917.47727394104 and batch: 700, loss is 3.4662607097625733 and perplexity is 32.01679823033967
At time: 918.0965292453766 and batch: 750, loss is 3.5703690385818483 and perplexity is 35.529702563587854
At time: 918.7202990055084 and batch: 800, loss is 3.514983115196228 and perplexity is 33.61536033036803
At time: 919.342000246048 and batch: 850, loss is 3.575538878440857 and perplexity is 35.71386106087907
At time: 919.9721930027008 and batch: 900, loss is 3.555222415924072 and perplexity is 34.99560268147062
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452426022046233 and perplexity of 85.83492904223829
finished 75 epochs...
Completing Train Step...
At time: 921.6038432121277 and batch: 50, loss is 3.8169902181625366 and perplexity is 45.46715595525379
At time: 922.2439916133881 and batch: 100, loss is 3.6803636074066164 and perplexity is 39.66081241625244
At time: 922.8733396530151 and batch: 150, loss is 3.6948452043533324 and perplexity is 40.23934323068851
At time: 923.5143051147461 and batch: 200, loss is 3.585381422042847 and perplexity is 36.06711188711658
At time: 924.1370203495026 and batch: 250, loss is 3.736507577896118 and perplexity is 41.95122264984975
At time: 924.7668554782867 and batch: 300, loss is 3.707745633125305 and perplexity is 40.76181079387879
At time: 925.3947985172272 and batch: 350, loss is 3.6846601390838623 and perplexity is 39.831582950945545
At time: 926.0191938877106 and batch: 400, loss is 3.635159740447998 and perplexity is 37.90790783970062
At time: 926.6420531272888 and batch: 450, loss is 3.64210253238678 and perplexity is 38.172010300019814
At time: 927.266350030899 and batch: 500, loss is 3.5284106302261353 and perplexity is 34.06977508945194
At time: 927.8887403011322 and batch: 550, loss is 3.6033271741867066 and perplexity is 36.72020594228929
At time: 928.5125155448914 and batch: 600, loss is 3.629032850265503 and perplexity is 37.676360308799495
At time: 929.1400923728943 and batch: 650, loss is 3.4711763715744017 and perplexity is 32.17456943999776
At time: 929.7615079879761 and batch: 700, loss is 3.4662565422058105 and perplexity is 32.01666479879372
At time: 930.4082980155945 and batch: 750, loss is 3.570368094444275 and perplexity is 35.52966901867652
At time: 931.0293588638306 and batch: 800, loss is 3.5149868011474608 and perplexity is 33.61548423517523
At time: 931.6516780853271 and batch: 850, loss is 3.5755512428283693 and perplexity is 35.71430264362674
At time: 932.2737417221069 and batch: 900, loss is 3.5552398633956908 and perplexity is 34.99621327158179
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452423931801156 and perplexity of 85.83474962638789
finished 76 epochs...
Completing Train Step...
At time: 933.8755967617035 and batch: 50, loss is 3.816966915130615 and perplexity is 45.466096445012134
At time: 934.517294883728 and batch: 100, loss is 3.6803375816345216 and perplexity is 39.65978022641922
At time: 935.140743970871 and batch: 150, loss is 3.6948160123825073 and perplexity is 40.238168582100144
At time: 935.7637498378754 and batch: 200, loss is 3.585355463027954 and perplexity is 36.06617563257414
At time: 936.3869621753693 and batch: 250, loss is 3.736480221748352 and perplexity is 41.950075041701105
At time: 937.0098721981049 and batch: 300, loss is 3.7077179479599 and perplexity is 40.76068231202593
At time: 937.6330211162567 and batch: 350, loss is 3.6846340703964233 and perplexity is 39.830544607393584
At time: 938.256404876709 and batch: 400, loss is 3.6351367902755736 and perplexity is 37.907037856662626
At time: 938.8785583972931 and batch: 450, loss is 3.6420820569992065 and perplexity is 38.171228721316055
At time: 939.5027363300323 and batch: 500, loss is 3.5283929347991942 and perplexity is 34.06917221557002
At time: 940.1259381771088 and batch: 550, loss is 3.603310718536377 and perplexity is 36.719601692391954
At time: 940.7512702941895 and batch: 600, loss is 3.629019088745117 and perplexity is 37.675841828366586
At time: 941.375 and batch: 650, loss is 3.4711687850952146 and perplexity is 32.17432534922225
At time: 941.9992287158966 and batch: 700, loss is 3.4662523460388184 and perplexity is 32.01653045180357
At time: 942.6197557449341 and batch: 750, loss is 3.5703673219680785 and perplexity is 35.52964157286355
At time: 943.2430121898651 and batch: 800, loss is 3.514990382194519 and perplexity is 33.615604614021706
At time: 943.8751528263092 and batch: 850, loss is 3.575563669204712 and perplexity is 35.71474644574962
At time: 944.5038626194 and batch: 900, loss is 3.555257215499878 and perplexity is 34.99682053478927
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452423931801156 and perplexity of 85.83474962638789
finished 77 epochs...
Completing Train Step...
At time: 946.1171653270721 and batch: 50, loss is 3.8169437885284423 and perplexity is 45.46504498084574
At time: 946.7449178695679 and batch: 100, loss is 3.680311608314514 and perplexity is 39.65875014363336
At time: 947.3713393211365 and batch: 150, loss is 3.6947871446609497 and perplexity is 40.23700701461951
At time: 947.9941446781158 and batch: 200, loss is 3.5853294229507444 and perplexity is 36.06523647880388
At time: 948.6161367893219 and batch: 250, loss is 3.7364529514312745 and perplexity is 41.94893106545167
At time: 949.2383253574371 and batch: 300, loss is 3.707690258026123 and perplexity is 40.75955366705813
At time: 949.8608772754669 and batch: 350, loss is 3.6846082067489623 and perplexity is 39.829514457551454
At time: 950.4837620258331 and batch: 400, loss is 3.6351138734817505 and perplexity is 37.90616915884554
At time: 951.1220915317535 and batch: 450, loss is 3.642061696052551 and perplexity is 38.170451526876526
At time: 951.7528412342072 and batch: 500, loss is 3.528375310897827 and perplexity is 34.068571789130175
At time: 952.3762953281403 and batch: 550, loss is 3.603294363021851 and perplexity is 36.71900112932435
At time: 953.0011596679688 and batch: 600, loss is 3.6290055465698243 and perplexity is 37.67533161896693
At time: 953.6235616207123 and batch: 650, loss is 3.471161184310913 and perplexity is 32.17408080004461
At time: 954.2456822395325 and batch: 700, loss is 3.466248188018799 and perplexity is 32.016397326705764
At time: 954.8684244155884 and batch: 750, loss is 3.5703665351867677 and perplexity is 35.529613618816576
At time: 955.4925715923309 and batch: 800, loss is 3.51499397277832 and perplexity is 33.615725313883786
At time: 956.1167352199554 and batch: 850, loss is 3.575576033592224 and perplexity is 35.71518803944458
At time: 956.7399797439575 and batch: 900, loss is 3.555274329185486 and perplexity is 34.99741946449811
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452423095703125 and perplexity of 85.83467786015275
finished 78 epochs...
Completing Train Step...
At time: 958.3530786037445 and batch: 50, loss is 3.816920747756958 and perplexity is 45.463997443201905
At time: 958.9747405052185 and batch: 100, loss is 3.6802856588363646 and perplexity is 39.65772103311558
At time: 959.5962533950806 and batch: 150, loss is 3.6947582149505616 and perplexity is 40.23584298649727
At time: 960.2169692516327 and batch: 200, loss is 3.5853035116195677 and perplexity is 36.06430199262446
At time: 960.8396732807159 and batch: 250, loss is 3.7364256715774538 and perplexity is 41.947786720353115
At time: 961.4606535434723 and batch: 300, loss is 3.707662682533264 and perplexity is 40.75842971777384
At time: 962.0901730060577 and batch: 350, loss is 3.6845824193954466 and perplexity is 39.82848737302473
At time: 962.7093114852905 and batch: 400, loss is 3.635091013908386 and perplexity is 37.90530264989474
At time: 963.3297328948975 and batch: 450, loss is 3.642041382789612 and perplexity is 38.169676168333204
At time: 963.9483563899994 and batch: 500, loss is 3.528357677459717 and perplexity is 34.06797104837461
At time: 964.5677478313446 and batch: 550, loss is 3.6032780170440675 and perplexity is 36.71840092625313
At time: 965.1870641708374 and batch: 600, loss is 3.6289918994903565 and perplexity is 37.67481746423071
At time: 965.8129587173462 and batch: 650, loss is 3.471153712272644 and perplexity is 32.17384039497976
At time: 966.4421038627625 and batch: 700, loss is 3.4662440538406374 and perplexity is 32.01626496548873
At time: 967.0718312263489 and batch: 750, loss is 3.5703656339645384 and perplexity is 35.52958159875341
At time: 967.7013945579529 and batch: 800, loss is 3.5149976301193235 and perplexity is 33.61584825827916
At time: 968.3319277763367 and batch: 850, loss is 3.575588321685791 and perplexity is 35.715626913713436
At time: 968.9557993412018 and batch: 900, loss is 3.5552915000915526 and perplexity is 34.99802040705966
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452421423507063 and perplexity of 85.83453432786249
finished 79 epochs...
Completing Train Step...
At time: 970.5763511657715 and batch: 50, loss is 3.8168977069854737 and perplexity is 45.46294992969385
At time: 971.2114441394806 and batch: 100, loss is 3.6802599143981936 and perplexity is 39.65670008051042
At time: 971.8352615833282 and batch: 150, loss is 3.69472948551178 and perplexity is 40.23468704991414
At time: 972.4568684101105 and batch: 200, loss is 3.585277752876282 and perplexity is 36.063373033492105
At time: 973.0751521587372 and batch: 250, loss is 3.7363985919952394 and perplexity is 41.94665080719401
At time: 973.692608833313 and batch: 300, loss is 3.7076351547241213 and perplexity is 40.75730774294244
At time: 974.3100581169128 and batch: 350, loss is 3.6845566558837892 and perplexity is 39.827461264544134
At time: 974.9285006523132 and batch: 400, loss is 3.635068206787109 and perplexity is 37.904438148918594
At time: 975.5471575260162 and batch: 450, loss is 3.6420209884643553 and perplexity is 38.168897731480364
At time: 976.1647019386292 and batch: 500, loss is 3.528340072631836 and perplexity is 34.067371292887366
At time: 976.7834100723267 and batch: 550, loss is 3.603261709213257 and perplexity is 36.71780213366571
At time: 977.3973841667175 and batch: 600, loss is 3.62897846698761 and perplexity is 37.6743114005405
At time: 978.0353634357452 and batch: 650, loss is 3.4711461591720583 and perplexity is 32.17359738364477
At time: 978.6529746055603 and batch: 700, loss is 3.4662398004531862 and perplexity is 32.0161287881987
At time: 979.2754032611847 and batch: 750, loss is 3.5703647422790525 and perplexity is 35.5295499175553
At time: 979.8985764980316 and batch: 800, loss is 3.5150012302398683 and perplexity is 33.61596927960295
At time: 980.5277116298676 and batch: 850, loss is 3.5756005096435546 and perplexity is 35.71606221691848
At time: 981.1606771945953 and batch: 900, loss is 3.5553085565567017 and perplexity is 34.99861735466592
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 80 epochs...
Completing Train Step...
At time: 982.87579870224 and batch: 50, loss is 3.816874861717224 and perplexity is 45.4619113282709
At time: 983.5175457000732 and batch: 100, loss is 3.6802341508865357 and perplexity is 39.65567839781671
At time: 984.1511061191559 and batch: 150, loss is 3.694700779914856 and perplexity is 40.23353210578208
At time: 984.7796919345856 and batch: 200, loss is 3.585252032279968 and perplexity is 36.06244547396134
At time: 985.4190964698792 and batch: 250, loss is 3.736371612548828 and perplexity is 41.94551912504257
At time: 986.0499165058136 and batch: 300, loss is 3.7076075506210326 and perplexity is 40.756182689546
At time: 986.6811389923096 and batch: 350, loss is 3.6845308685302736 and perplexity is 39.826434232963145
At time: 987.31130027771 and batch: 400, loss is 3.635045495033264 and perplexity is 37.90357728242565
At time: 987.9424238204956 and batch: 450, loss is 3.6420008945465088 and perplexity is 38.16813077649064
At time: 988.5705969333649 and batch: 500, loss is 3.5283226442337035 and perplexity is 34.06677755835107
At time: 989.2003040313721 and batch: 550, loss is 3.603245348930359 and perplexity is 36.71720142494931
At time: 989.8275375366211 and batch: 600, loss is 3.6289649534225465 and perplexity is 37.67380228972213
At time: 990.4696433544159 and batch: 650, loss is 3.4711386585235595 and perplexity is 32.17335606170489
At time: 991.1008167266846 and batch: 700, loss is 3.4662355518341066 and perplexity is 32.01599276415203
At time: 991.7263009548187 and batch: 750, loss is 3.570363988876343 and perplexity is 35.52952314950621
At time: 992.3662211894989 and batch: 800, loss is 3.515004916191101 and perplexity is 33.61609318665471
At time: 993.0131392478943 and batch: 850, loss is 3.5756126117706297 and perplexity is 35.71649445985757
At time: 993.6607656478882 and batch: 900, loss is 3.555325360298157 and perplexity is 34.999205467324465
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524210054580475 and perplexity of 85.83449844482739
Annealing...
finished 81 epochs...
Completing Train Step...
At time: 995.4681804180145 and batch: 50, loss is 3.8168570518493654 and perplexity is 45.46110166484756
At time: 996.099545955658 and batch: 100, loss is 3.68021635055542 and perplexity is 39.65497251989306
At time: 996.7344081401825 and batch: 150, loss is 3.6946862077713014 and perplexity is 40.23294582124826
At time: 997.3709237575531 and batch: 200, loss is 3.585237421989441 and perplexity is 36.061918595004784
At time: 998.0013794898987 and batch: 250, loss is 3.7363583278656005 and perplexity is 41.94496189580949
At time: 998.6298542022705 and batch: 300, loss is 3.7075864124298095 and perplexity is 40.75532118666812
At time: 999.2580227851868 and batch: 350, loss is 3.6845117235183715 and perplexity is 39.825671762704516
At time: 999.8847670555115 and batch: 400, loss is 3.63502272605896 and perplexity is 37.9027142666735
At time: 1000.5113184452057 and batch: 450, loss is 3.641980404853821 and perplexity is 38.16734873123253
At time: 1001.1403126716614 and batch: 500, loss is 3.5283010482788084 and perplexity is 34.06604186170357
At time: 1001.7712781429291 and batch: 550, loss is 3.6032226848602296 and perplexity is 36.716369273151265
At time: 1002.4092185497284 and batch: 600, loss is 3.628945846557617 and perplexity is 37.6730824683472
At time: 1003.0452394485474 and batch: 650, loss is 3.471116695404053 and perplexity is 32.17264944220059
At time: 1003.6724631786346 and batch: 700, loss is 3.466209306716919 and perplexity is 32.01515251169637
At time: 1004.3032660484314 and batch: 750, loss is 3.5703384399414064 and perplexity is 35.52861541962675
At time: 1004.936007976532 and batch: 800, loss is 3.5149823379516603 and perplexity is 33.61533420302197
At time: 1005.5717530250549 and batch: 850, loss is 3.5755824708938597 and perplexity is 35.71541794962297
At time: 1006.2066965103149 and batch: 900, loss is 3.5552901077270507 and perplexity is 34.99797167709234
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524210054580475 and perplexity of 85.83449844482739
Annealing...
finished 82 epochs...
Completing Train Step...
At time: 1007.9006612300873 and batch: 50, loss is 3.8168534088134765 and perplexity is 45.460936048724314
At time: 1008.5355367660522 and batch: 100, loss is 3.6802127742767334 and perplexity is 39.65483070291361
At time: 1009.1677496433258 and batch: 150, loss is 3.6946835851669313 and perplexity is 40.23284030628709
At time: 1009.795318365097 and batch: 200, loss is 3.5852346801757813 and perplexity is 36.061819720079335
At time: 1010.4348163604736 and batch: 250, loss is 3.7363557386398316 and perplexity is 41.944853290973874
At time: 1011.0609667301178 and batch: 300, loss is 3.7075821590423583 and perplexity is 40.75514783886508
At time: 1011.6880815029144 and batch: 350, loss is 3.684507842063904 and perplexity is 39.825517181472925
At time: 1012.3132183551788 and batch: 400, loss is 3.6350181484222412 and perplexity is 37.90254076221406
At time: 1012.9377601146698 and batch: 450, loss is 3.6419762182235718 and perplexity is 38.16718893899029
At time: 1013.5630564689636 and batch: 500, loss is 3.5282965803146364 and perplexity is 34.06588965618907
At time: 1014.1893591880798 and batch: 550, loss is 3.603217921257019 and perplexity is 36.716194371353296
At time: 1014.8184652328491 and batch: 600, loss is 3.6289418840408327 and perplexity is 37.672933188421354
At time: 1015.4457008838654 and batch: 650, loss is 3.471112093925476 and perplexity is 32.17250140078403
At time: 1016.081057548523 and batch: 700, loss is 3.4662037086486817 and perplexity is 32.014973289189626
At time: 1016.7152059078217 and batch: 750, loss is 3.5703330039978027 and perplexity is 35.52842228860194
At time: 1017.3462917804718 and batch: 800, loss is 3.514977536201477 and perplexity is 33.61517279097233
At time: 1017.9838798046112 and batch: 850, loss is 3.5755759143829344 and perplexity is 35.71518378186264
At time: 1018.6212303638458 and batch: 900, loss is 3.555282349586487 and perplexity is 34.997700158961855
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 83 epochs...
Completing Train Step...
At time: 1020.2644488811493 and batch: 50, loss is 3.8168528366088865 and perplexity is 45.46091003577549
At time: 1020.9177775382996 and batch: 100, loss is 3.6802120447158813 and perplexity is 39.65480177231208
At time: 1021.5579941272736 and batch: 150, loss is 3.694682698249817 and perplexity is 40.23280462310828
At time: 1022.1899580955505 and batch: 200, loss is 3.5852338218688966 and perplexity is 36.06178876798448
At time: 1022.8238599300385 and batch: 250, loss is 3.736354937553406 and perplexity is 41.94481968953474
At time: 1023.4634499549866 and batch: 300, loss is 3.707581377029419 and perplexity is 40.75511596782459
At time: 1024.094208240509 and batch: 350, loss is 3.684507131576538 and perplexity is 39.82548888595618
At time: 1024.7260966300964 and batch: 400, loss is 3.635017433166504 and perplexity is 37.90251365221401
At time: 1025.3652093410492 and batch: 450, loss is 3.6419756603240967 and perplexity is 38.16716764554156
At time: 1026.0008165836334 and batch: 500, loss is 3.5282959651947023 and perplexity is 34.06586870158772
At time: 1026.6578810214996 and batch: 550, loss is 3.603217487335205 and perplexity is 36.71617843939909
At time: 1027.2933146953583 and batch: 600, loss is 3.6289415550231934 and perplexity is 37.67292079336385
At time: 1027.9304220676422 and batch: 650, loss is 3.4711119031906126 and perplexity is 32.17249526436695
At time: 1028.563740491867 and batch: 700, loss is 3.4662035274505616 and perplexity is 32.01496748813718
At time: 1029.1934354305267 and batch: 750, loss is 3.570333118438721 and perplexity is 35.52842635450744
At time: 1029.8226699829102 and batch: 800, loss is 3.5149776840209963 and perplexity is 33.61517775995138
At time: 1030.4592034816742 and batch: 850, loss is 3.575576424598694 and perplexity is 35.71520200431691
At time: 1031.093538761139 and batch: 900, loss is 3.55528311252594 and perplexity is 34.997726860098254
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 84 epochs...
Completing Train Step...
At time: 1032.7243568897247 and batch: 50, loss is 3.816852192878723 and perplexity is 45.46088077122586
At time: 1033.3789336681366 and batch: 100, loss is 3.680211305618286 and perplexity is 39.65477246355429
At time: 1034.0083334445953 and batch: 150, loss is 3.694681878089905 and perplexity is 40.23277162578831
At time: 1034.6417303085327 and batch: 200, loss is 3.585233016014099 and perplexity is 36.0617597074307
At time: 1035.2856171131134 and batch: 250, loss is 3.7363541173934935 and perplexity is 41.94478528808921
At time: 1035.918597459793 and batch: 300, loss is 3.7075806570053103 and perplexity is 40.7550866231691
At time: 1036.557898759842 and batch: 350, loss is 3.6845064306259157 and perplexity is 39.82546097026474
At time: 1037.1971337795258 and batch: 400, loss is 3.635016770362854 and perplexity is 37.90248853029796
At time: 1037.8294565677643 and batch: 450, loss is 3.6419751024246216 and perplexity is 38.16714635210471
At time: 1038.4589431285858 and batch: 500, loss is 3.528295431137085 and perplexity is 34.06585050845591
At time: 1039.0885224342346 and batch: 550, loss is 3.6032170248031616 and perplexity is 36.71616145699398
At time: 1039.7171638011932 and batch: 600, loss is 3.628941307067871 and perplexity is 37.67291145216378
At time: 1040.3404138088226 and batch: 650, loss is 3.4711117935180664 and perplexity is 32.17249173592767
At time: 1040.9652979373932 and batch: 700, loss is 3.4662034940719604 and perplexity is 32.014966419522366
At time: 1041.5937995910645 and batch: 750, loss is 3.570333185195923 and perplexity is 35.528428726285846
At time: 1042.2326550483704 and batch: 800, loss is 3.5149778652191164 and perplexity is 33.61518385095895
At time: 1042.8550417423248 and batch: 850, loss is 3.5755769395828247 and perplexity is 35.715220397083904
At time: 1043.4750163555145 and batch: 900, loss is 3.5552838039398194 and perplexity is 34.997751058020725
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524210054580475 and perplexity of 85.83449844482739
Annealing...
finished 85 epochs...
Completing Train Step...
At time: 1045.0920639038086 and batch: 50, loss is 3.816851749420166 and perplexity is 45.46086061121375
At time: 1045.7189574241638 and batch: 100, loss is 3.6802109479904175 and perplexity is 39.65475828190506
At time: 1046.3376970291138 and batch: 150, loss is 3.694681601524353 and perplexity is 40.232760498791166
At time: 1046.9546060562134 and batch: 200, loss is 3.5852326726913453 and perplexity is 36.061747326610174
At time: 1047.5716819763184 and batch: 250, loss is 3.7363538265228273 and perplexity is 41.944773087583336
At time: 1048.1924858093262 and batch: 300, loss is 3.707579984664917 and perplexity is 40.75505922188735
At time: 1048.818175792694 and batch: 350, loss is 3.684505972862244 and perplexity is 39.82544273961967
At time: 1049.4350516796112 and batch: 400, loss is 3.6350161027908325 and perplexity is 37.90246322766551
At time: 1050.0524241924286 and batch: 450, loss is 3.641974620819092 and perplexity is 38.1671279706004
At time: 1050.6758847236633 and batch: 500, loss is 3.5282947492599486 and perplexity is 34.06582727973923
At time: 1051.3098907470703 and batch: 550, loss is 3.603216333389282 and perplexity is 36.71613607093912
At time: 1051.9269301891327 and batch: 600, loss is 3.6289406061172484 and perplexity is 37.6728850453223
At time: 1052.5415861606598 and batch: 650, loss is 3.471111035346985 and perplexity is 32.17246734368406
At time: 1053.1547393798828 and batch: 700, loss is 3.466202530860901 and perplexity is 32.0149355823675
At time: 1053.7731294631958 and batch: 750, loss is 3.570332236289978 and perplexity is 35.52839501316461
At time: 1054.3889985084534 and batch: 800, loss is 3.514977078437805 and perplexity is 33.615157403170926
At time: 1055.0073809623718 and batch: 850, loss is 3.575575785636902 and perplexity is 35.71517918367473
At time: 1055.6201062202454 and batch: 900, loss is 3.5552823162078857 and perplexity is 34.9976989907876
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 86 epochs...
Completing Train Step...
At time: 1057.2662580013275 and batch: 50, loss is 3.8168517255783083 and perplexity is 45.460859527342386
At time: 1057.8959393501282 and batch: 100, loss is 3.6802109146118163 and perplexity is 39.65475695828472
At time: 1058.5301487445831 and batch: 150, loss is 3.6946815347671507 and perplexity is 40.23275781296472
At time: 1059.1612374782562 and batch: 200, loss is 3.5852326059341433 and perplexity is 36.06174491922891
At time: 1059.7949903011322 and batch: 250, loss is 3.7363538265228273 and perplexity is 41.944773087583336
At time: 1060.4306509494781 and batch: 300, loss is 3.707579941749573 and perplexity is 40.75505747286999
At time: 1061.060949087143 and batch: 350, loss is 3.684505944252014 and perplexity is 39.82544160020462
At time: 1061.6932282447815 and batch: 400, loss is 3.6350160694122313 and perplexity is 37.902461962534325
At time: 1062.324610710144 and batch: 450, loss is 3.6419745683670044 and perplexity is 38.16712596865491
At time: 1062.963109254837 and batch: 500, loss is 3.5282947301864622 and perplexity is 34.065826629985146
At time: 1063.5989472866058 and batch: 550, loss is 3.6032162141799926 and perplexity is 36.716131694034885
At time: 1064.236783027649 and batch: 600, loss is 3.6289405965805055 and perplexity is 37.672884686045684
At time: 1064.8767004013062 and batch: 650, loss is 3.4711110305786135 and perplexity is 32.17246719027379
At time: 1065.5134756565094 and batch: 700, loss is 3.4662025499343874 and perplexity is 32.01493619300394
At time: 1066.135882139206 and batch: 750, loss is 3.5703322505950927 and perplexity is 35.52839552140239
At time: 1066.7756111621857 and batch: 800, loss is 3.514977078437805 and perplexity is 33.615157403170926
At time: 1067.4249258041382 and batch: 850, loss is 3.5755757761001585 and perplexity is 35.71517884306823
At time: 1068.0605854988098 and batch: 900, loss is 3.5552823638916013 and perplexity is 34.997700659607965
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 87 epochs...
Completing Train Step...
At time: 1069.6814432144165 and batch: 50, loss is 3.816851692199707 and perplexity is 45.46085800992251
At time: 1070.3282232284546 and batch: 100, loss is 3.680210814476013 and perplexity is 39.65475298742398
At time: 1070.9490463733673 and batch: 150, loss is 3.6946815156936648 and perplexity is 40.232757045585785
At time: 1071.5791029930115 and batch: 200, loss is 3.5852326011657714 and perplexity is 36.061744747273096
At time: 1072.2061281204224 and batch: 250, loss is 3.7363537549972534 and perplexity is 41.944770087459474
At time: 1072.8395652770996 and batch: 300, loss is 3.707579894065857 and perplexity is 40.75505552951745
At time: 1073.4617428779602 and batch: 350, loss is 3.6845058488845823 and perplexity is 39.82543780215472
At time: 1074.0874423980713 and batch: 400, loss is 3.6350160455703735 and perplexity is 37.90246105886922
At time: 1074.7328946590424 and batch: 450, loss is 3.6419745254516602 and perplexity is 38.1671243306996
At time: 1075.3662736415863 and batch: 500, loss is 3.5282946729660036 and perplexity is 34.065824680722976
At time: 1076.0036189556122 and batch: 550, loss is 3.603216190338135 and perplexity is 36.71613081865411
At time: 1076.6391191482544 and batch: 600, loss is 3.6289406394958497 and perplexity is 37.67288630279053
At time: 1077.278347492218 and batch: 650, loss is 3.4711110639572142 and perplexity is 32.17246826414575
At time: 1077.913581609726 and batch: 700, loss is 3.4662025451660154 and perplexity is 32.014936040344814
At time: 1078.5384335517883 and batch: 750, loss is 3.570332236289978 and perplexity is 35.52839501316461
At time: 1079.1728355884552 and batch: 800, loss is 3.51497709274292 and perplexity is 33.61515788403962
At time: 1079.8062336444855 and batch: 850, loss is 3.5755758762359617 and perplexity is 35.71518241943652
At time: 1080.442787885666 and batch: 900, loss is 3.5552824449539187 and perplexity is 34.997703496602796
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 88 epochs...
Completing Train Step...
At time: 1082.0688314437866 and batch: 50, loss is 3.8168516874313356 and perplexity is 45.46085779314825
At time: 1082.7102904319763 and batch: 100, loss is 3.6802107906341552 and perplexity is 39.65475204198101
At time: 1083.3342010974884 and batch: 150, loss is 3.6946814584732057 and perplexity is 40.23275474344903
At time: 1083.9607665538788 and batch: 200, loss is 3.5852325201034545 and perplexity is 36.06174182402464
At time: 1084.5893731117249 and batch: 250, loss is 3.7363536882400514 and perplexity is 41.94476728734408
At time: 1085.2282490730286 and batch: 300, loss is 3.7075798416137697 and perplexity is 40.75505339182978
At time: 1085.8614847660065 and batch: 350, loss is 3.6845058012008667 and perplexity is 39.82543590312991
At time: 1086.4989211559296 and batch: 400, loss is 3.635016050338745 and perplexity is 37.90246123960224
At time: 1087.1241688728333 and batch: 450, loss is 3.641974468231201 and perplexity is 38.16712214675929
At time: 1087.7430331707 and batch: 500, loss is 3.5282946395874024 and perplexity is 34.06582354365342
At time: 1088.3579950332642 and batch: 550, loss is 3.603216209411621 and perplexity is 36.71613151895873
At time: 1088.9740552902222 and batch: 600, loss is 3.6289405250549316 and perplexity is 37.67288199147109
At time: 1089.5907464027405 and batch: 650, loss is 3.4711110258102416 and perplexity is 32.1724670368635
At time: 1090.2323279380798 and batch: 700, loss is 3.4662025451660154 and perplexity is 32.014936040344814
At time: 1090.8479895591736 and batch: 750, loss is 3.5703322505950927 and perplexity is 35.52839552140239
At time: 1091.462964296341 and batch: 800, loss is 3.514977149963379 and perplexity is 33.61515980751444
At time: 1092.0769839286804 and batch: 850, loss is 3.575575895309448 and perplexity is 35.715183100649575
At time: 1092.6917686462402 and batch: 900, loss is 3.555282464027405 and perplexity is 34.99770416413102
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 89 epochs...
Completing Train Step...
At time: 1094.260708808899 and batch: 50, loss is 3.8168516778945922 and perplexity is 45.460857359599714
At time: 1094.8799984455109 and batch: 100, loss is 3.6802106857299806 and perplexity is 39.65474788203219
At time: 1095.505496263504 and batch: 150, loss is 3.694681406021118 and perplexity is 40.232752633157105
At time: 1096.1356799602509 and batch: 200, loss is 3.5852324676513674 and perplexity is 36.06173993251106
At time: 1096.7570741176605 and batch: 250, loss is 3.7363536405563353 and perplexity is 41.94476528726175
At time: 1097.3867688179016 and batch: 300, loss is 3.7075797843933107 and perplexity is 40.75505105980699
At time: 1098.0201334953308 and batch: 350, loss is 3.6845057916641237 and perplexity is 39.82543552332497
At time: 1098.6409442424774 and batch: 400, loss is 3.6350159454345703 and perplexity is 37.90245726347603
At time: 1099.264859676361 and batch: 450, loss is 3.6419744539260863 and perplexity is 38.16712160077422
At time: 1099.891508102417 and batch: 500, loss is 3.5282946252822875 and perplexity is 34.065823056337905
At time: 1100.5155568122864 and batch: 550, loss is 3.6032161140441894 and perplexity is 36.71612801743573
At time: 1101.1399977207184 and batch: 600, loss is 3.6289405536651613 and perplexity is 37.67288306930091
At time: 1101.7671728134155 and batch: 650, loss is 3.471110992431641 and perplexity is 32.17246596299159
At time: 1102.3933355808258 and batch: 700, loss is 3.466202621459961 and perplexity is 32.01493848289069
At time: 1103.020781993866 and batch: 750, loss is 3.5703323030471803 and perplexity is 35.52839738494095
At time: 1103.6484384536743 and batch: 800, loss is 3.5149771690368654 and perplexity is 33.61516044867273
At time: 1104.2700307369232 and batch: 850, loss is 3.5755759286880493 and perplexity is 35.71518429277245
At time: 1104.8914804458618 and batch: 900, loss is 3.555282459259033 and perplexity is 34.997703997248955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524210054580475 and perplexity of 85.83449844482739
Annealing...
finished 90 epochs...
Completing Train Step...
At time: 1106.4773032665253 and batch: 50, loss is 3.8168516397476195 and perplexity is 45.460855625405664
At time: 1107.0943524837494 and batch: 100, loss is 3.6802107191085813 and perplexity is 39.654749205652216
At time: 1107.7184076309204 and batch: 150, loss is 3.694681363105774 and perplexity is 40.23275090655472
At time: 1108.3431944847107 and batch: 200, loss is 3.585232434272766 and perplexity is 36.06173872882064
At time: 1108.9693965911865 and batch: 250, loss is 3.736353678703308 and perplexity is 41.9447668873276
At time: 1109.5883467197418 and batch: 300, loss is 3.7075797748565673 and perplexity is 40.75505067113652
At time: 1110.2034947872162 and batch: 350, loss is 3.6845058012008667 and perplexity is 39.82543590312991
At time: 1110.8169412612915 and batch: 400, loss is 3.6350158786773683 and perplexity is 37.90245473321412
At time: 1111.4311492443085 and batch: 450, loss is 3.641974368095398 and perplexity is 38.16711832486405
At time: 1112.0450358390808 and batch: 500, loss is 3.528294620513916 and perplexity is 34.06582289389941
At time: 1112.6594109535217 and batch: 550, loss is 3.6032161045074464 and perplexity is 36.71612766728346
At time: 1113.2791135311127 and batch: 600, loss is 3.6289405488967894 and perplexity is 37.67288288966259
At time: 1113.9088923931122 and batch: 650, loss is 3.4711109590530396 and perplexity is 32.172464889119695
At time: 1114.5298852920532 and batch: 700, loss is 3.4662025117874147 and perplexity is 32.014934971731066
At time: 1115.1471102237701 and batch: 750, loss is 3.57033224105835 and perplexity is 35.52839518257721
At time: 1115.76544880867 and batch: 800, loss is 3.5149771213531493 and perplexity is 33.615158845777
At time: 1116.3933699131012 and batch: 850, loss is 3.5755758476257324 and perplexity is 35.71518139761698
At time: 1117.0168471336365 and batch: 900, loss is 3.5552824354171753 and perplexity is 34.99770316283868
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 91 epochs...
Completing Train Step...
At time: 1118.5948910713196 and batch: 50, loss is 3.816851716041565 and perplexity is 45.460859093793836
At time: 1119.2287719249725 and batch: 100, loss is 3.6802107191085813 and perplexity is 39.654749205652216
At time: 1119.8463830947876 and batch: 150, loss is 3.694681377410889 and perplexity is 40.232751482088844
At time: 1120.4608664512634 and batch: 200, loss is 3.585232400894165 and perplexity is 36.06173752513027
At time: 1121.079018354416 and batch: 250, loss is 3.7363536882400514 and perplexity is 41.94476728734408
At time: 1121.706068277359 and batch: 300, loss is 3.7075797605514524 and perplexity is 40.755050088130844
At time: 1122.3445479869843 and batch: 350, loss is 3.684505753517151 and perplexity is 39.8254340041052
At time: 1122.9587879180908 and batch: 400, loss is 3.6350158786773683 and perplexity is 37.90245473321412
At time: 1123.5725073814392 and batch: 450, loss is 3.641974391937256 and perplexity is 38.16711923483906
At time: 1124.188678264618 and batch: 500, loss is 3.528294630050659 and perplexity is 34.0658232187764
At time: 1124.8132255077362 and batch: 550, loss is 3.603216061592102 and perplexity is 36.71612609159822
At time: 1125.436527967453 and batch: 600, loss is 3.6289405059814452 and perplexity is 37.67288127291789
At time: 1126.0617105960846 and batch: 650, loss is 3.471110978126526 and perplexity is 32.172465502760765
At time: 1126.687733888626 and batch: 700, loss is 3.466202516555786 and perplexity is 32.01493512439017
At time: 1127.3102152347565 and batch: 750, loss is 3.5703322219848634 and perplexity is 35.52839450492686
At time: 1127.9335086345673 and batch: 800, loss is 3.5149771404266357 and perplexity is 33.61515948693528
At time: 1128.5611708164215 and batch: 850, loss is 3.5755758094787597 and perplexity is 35.71518003519096
At time: 1129.1865000724792 and batch: 900, loss is 3.555282406806946 and perplexity is 34.997702161546385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 92 epochs...
Completing Train Step...
At time: 1130.7611210346222 and batch: 50, loss is 3.8168516731262208 and perplexity is 45.46085714282546
At time: 1131.396790266037 and batch: 100, loss is 3.680210690498352 and perplexity is 39.654748071120764
At time: 1132.0207839012146 and batch: 150, loss is 3.6946813535690306 and perplexity is 40.2327505228653
At time: 1132.6509172916412 and batch: 200, loss is 3.585232391357422 and perplexity is 36.061737181218746
At time: 1133.2770006656647 and batch: 250, loss is 3.736353597640991 and perplexity is 41.94476348718776
At time: 1133.9052753448486 and batch: 300, loss is 3.7075797939300537 and perplexity is 40.75505144847743
At time: 1134.5413675308228 and batch: 350, loss is 3.684505748748779 and perplexity is 39.82543381420271
At time: 1135.183477640152 and batch: 400, loss is 3.635015845298767 and perplexity is 37.90245346808322
At time: 1135.823533296585 and batch: 450, loss is 3.641974382400513 and perplexity is 38.16711887084906
At time: 1136.4589593410492 and batch: 500, loss is 3.528294606208801 and perplexity is 34.0658224065839
At time: 1137.0965802669525 and batch: 550, loss is 3.6032160997390745 and perplexity is 36.716127492207306
At time: 1137.731169462204 and batch: 600, loss is 3.6289404821395874 and perplexity is 37.672880374726425
At time: 1138.358614206314 and batch: 650, loss is 3.4711109828948974 and perplexity is 32.172465656171035
At time: 1138.9886395931244 and batch: 700, loss is 3.4662025022506713 and perplexity is 32.01493466641285
At time: 1139.6310367584229 and batch: 750, loss is 3.570332255363464 and perplexity is 35.528395690814975
At time: 1140.2778267860413 and batch: 800, loss is 3.5149771118164064 and perplexity is 33.61515852519788
At time: 1140.924025297165 and batch: 850, loss is 3.5755758047103883 and perplexity is 35.715179864887716
At time: 1141.5661947727203 and batch: 900, loss is 3.555282392501831 and perplexity is 34.99770166090023
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4524210054580475 and perplexity of 85.83449844482739
Annealing...
finished 93 epochs...
Completing Train Step...
At time: 1143.2007637023926 and batch: 50, loss is 3.8168516397476195 and perplexity is 45.460855625405664
At time: 1143.8348984718323 and batch: 100, loss is 3.6802107191085813 and perplexity is 39.654749205652216
At time: 1144.4734179973602 and batch: 150, loss is 3.6946814107894896 and perplexity is 40.23275282500182
At time: 1145.119280576706 and batch: 200, loss is 3.585232391357422 and perplexity is 36.061737181218746
At time: 1145.7585589885712 and batch: 250, loss is 3.7363536739349366 and perplexity is 41.94476668731937
At time: 1146.3923149108887 and batch: 300, loss is 3.7075797367095946 and perplexity is 40.75504911645475
At time: 1147.022224187851 and batch: 350, loss is 3.684505739212036 and perplexity is 39.82543343439779
At time: 1147.6532638072968 and batch: 400, loss is 3.635015869140625 and perplexity is 37.90245437174814
At time: 1148.2874057292938 and batch: 450, loss is 3.641974325180054 and perplexity is 38.16711668690906
At time: 1148.9181418418884 and batch: 500, loss is 3.5282945919036863 and perplexity is 34.0658219192684
At time: 1149.539894580841 and batch: 550, loss is 3.603216052055359 and perplexity is 36.716125741445964
At time: 1150.1799478530884 and batch: 600, loss is 3.628940472602844 and perplexity is 37.672880015449834
At time: 1150.8149631023407 and batch: 650, loss is 3.471110906600952 and perplexity is 32.17246320160679
At time: 1151.4602301120758 and batch: 700, loss is 3.466202564239502 and perplexity is 32.01493665098127
At time: 1152.103312253952 and batch: 750, loss is 3.5703322315216064 and perplexity is 35.528394843752025
At time: 1152.732689857483 and batch: 800, loss is 3.514977126121521 and perplexity is 33.61515900606657
At time: 1153.361210346222 and batch: 850, loss is 3.5755758237838746 and perplexity is 35.71518054610072
At time: 1154.0230889320374 and batch: 900, loss is 3.555282430648804 and perplexity is 34.99770299595663
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 94 epochs...
Completing Train Step...
At time: 1155.6438598632812 and batch: 50, loss is 3.816851649284363 and perplexity is 45.460856058954185
At time: 1156.289421081543 and batch: 100, loss is 3.6802107095718384 and perplexity is 39.65474882747507
At time: 1156.914385318756 and batch: 150, loss is 3.694681386947632 and perplexity is 40.232751865778255
At time: 1157.5413913726807 and batch: 200, loss is 3.585232367515564 and perplexity is 36.061736321439945
At time: 1158.1767060756683 and batch: 250, loss is 3.7363536596298217 and perplexity is 41.94476608729467
At time: 1158.8005239963531 and batch: 300, loss is 3.7075797939300537 and perplexity is 40.75505144847743
At time: 1159.4237689971924 and batch: 350, loss is 3.6845057725906374 and perplexity is 39.82543476371507
At time: 1160.046302318573 and batch: 400, loss is 3.6350158786773683 and perplexity is 37.90245473321412
At time: 1160.669703245163 and batch: 450, loss is 3.6419743490219116 and perplexity is 38.16711759688404
At time: 1161.2922217845917 and batch: 500, loss is 3.5282946014404297 and perplexity is 34.0658222441454
At time: 1161.9141368865967 and batch: 550, loss is 3.603216094970703 and perplexity is 36.716127317131175
At time: 1162.5362002849579 and batch: 600, loss is 3.628940529823303 and perplexity is 37.67288217110938
At time: 1163.1585483551025 and batch: 650, loss is 3.4711109685897825 and perplexity is 32.17246519594022
At time: 1163.779714345932 and batch: 700, loss is 3.466202526092529 and perplexity is 32.01493542970837
At time: 1164.4029035568237 and batch: 750, loss is 3.5703321886062622 and perplexity is 35.52839331903877
At time: 1165.0333890914917 and batch: 800, loss is 3.514977126121521 and perplexity is 33.61515900606657
At time: 1165.6601684093475 and batch: 850, loss is 3.575575852394104 and perplexity is 35.71518156792023
At time: 1166.2813522815704 and batch: 900, loss is 3.5552823877334596 and perplexity is 34.997701494018195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 95 epochs...
Completing Train Step...
At time: 1167.8512711524963 and batch: 50, loss is 3.8168516778945922 and perplexity is 45.460857359599714
At time: 1168.4925141334534 and batch: 100, loss is 3.68021071434021 and perplexity is 39.65474901656364
At time: 1169.1168503761292 and batch: 150, loss is 3.6946813678741455 and perplexity is 40.23275109839942
At time: 1169.751338481903 and batch: 200, loss is 3.585232400894165 and perplexity is 36.06173752513027
At time: 1170.373328447342 and batch: 250, loss is 3.736353597640991 and perplexity is 41.94476348718776
At time: 1170.9953849315643 and batch: 300, loss is 3.7075797653198244 and perplexity is 40.75505028246608
At time: 1171.6167912483215 and batch: 350, loss is 3.6845057439804076 and perplexity is 39.82543362430025
At time: 1172.2362473011017 and batch: 400, loss is 3.6350158786773683 and perplexity is 37.90245473321412
At time: 1172.8561618328094 and batch: 450, loss is 3.641974401473999 and perplexity is 38.16711959882909
At time: 1173.480919122696 and batch: 500, loss is 3.5282946109771727 and perplexity is 34.065822569022394
At time: 1174.102950334549 and batch: 550, loss is 3.6032160997390745 and perplexity is 36.716127492207306
At time: 1174.7275779247284 and batch: 600, loss is 3.628940544128418 and perplexity is 37.67288271002429
At time: 1175.3573732376099 and batch: 650, loss is 3.471110949516296 and perplexity is 32.17246458229915
At time: 1175.983737707138 and batch: 700, loss is 3.4662025070190428 and perplexity is 32.01493481907195
At time: 1176.62011384964 and batch: 750, loss is 3.570332255363464 and perplexity is 35.528395690814975
At time: 1177.2451326847076 and batch: 800, loss is 3.51497709274292 and perplexity is 33.61515788403962
At time: 1177.876719713211 and batch: 850, loss is 3.575575842857361 and perplexity is 35.71518122731373
At time: 1178.5059478282928 and batch: 900, loss is 3.5552824258804323 and perplexity is 34.99770282907458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 96 epochs...
Completing Train Step...
At time: 1180.0878503322601 and batch: 50, loss is 3.8168516397476195 and perplexity is 45.460855625405664
At time: 1180.725129365921 and batch: 100, loss is 3.6802107095718384 and perplexity is 39.65474882747507
At time: 1181.349395275116 and batch: 150, loss is 3.6946813917160033 and perplexity is 40.23275205762297
At time: 1181.9726753234863 and batch: 200, loss is 3.585232377052307 and perplexity is 36.06173666535146
At time: 1182.595120191574 and batch: 250, loss is 3.7363536596298217 and perplexity is 41.94476608729467
At time: 1183.2244882583618 and batch: 300, loss is 3.7075797653198244 and perplexity is 40.75505028246608
At time: 1183.853536605835 and batch: 350, loss is 3.6845057725906374 and perplexity is 39.82543476371507
At time: 1184.478504896164 and batch: 400, loss is 3.635015845298767 and perplexity is 37.90245346808322
At time: 1185.1043226718903 and batch: 450, loss is 3.6419743585586546 and perplexity is 38.16711796087404
At time: 1185.7428183555603 and batch: 500, loss is 3.528294634819031 and perplexity is 34.06582338121492
At time: 1186.3672406673431 and batch: 550, loss is 3.6032160663604738 and perplexity is 36.716126266674365
At time: 1187.0020554065704 and batch: 600, loss is 3.628940534591675 and perplexity is 37.67288235074769
At time: 1187.6361467838287 and batch: 650, loss is 3.4711109447479247 and perplexity is 32.17246442888889
At time: 1188.2723653316498 and batch: 700, loss is 3.4662025451660154 and perplexity is 32.014936040344814
At time: 1188.8996484279633 and batch: 750, loss is 3.5703322219848634 and perplexity is 35.52839450492686
At time: 1189.5246188640594 and batch: 800, loss is 3.514977078437805 and perplexity is 33.615157403170926
At time: 1190.149733543396 and batch: 850, loss is 3.575575799942017 and perplexity is 35.71517969458447
At time: 1190.7779519557953 and batch: 900, loss is 3.55528244972229 and perplexity is 34.99770366348485
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 97 epochs...
Completing Train Step...
At time: 1192.4660940170288 and batch: 50, loss is 3.8168516731262208 and perplexity is 45.46085714282546
At time: 1193.0948195457458 and batch: 100, loss is 3.6802107286453247 and perplexity is 39.654749583829386
At time: 1193.7228853702545 and batch: 150, loss is 3.6946813344955443 and perplexity is 40.23274975548649
At time: 1194.348872423172 and batch: 200, loss is 3.585232348442078 and perplexity is 36.06173563361692
At time: 1194.9789545536041 and batch: 250, loss is 3.736353645324707 and perplexity is 41.944765487269976
At time: 1195.6058475971222 and batch: 300, loss is 3.707579770088196 and perplexity is 40.7550504768013
At time: 1196.233206987381 and batch: 350, loss is 3.684505763053894 and perplexity is 39.82543438391012
At time: 1196.8586068153381 and batch: 400, loss is 3.635015916824341 and perplexity is 37.90245617907806
At time: 1197.4798212051392 and batch: 450, loss is 3.641974368095398 and perplexity is 38.16711832486405
At time: 1198.110106229782 and batch: 500, loss is 3.5282945966720582 and perplexity is 34.06582208170691
At time: 1198.7391300201416 and batch: 550, loss is 3.6032160663604738 and perplexity is 36.716126266674365
At time: 1199.3696258068085 and batch: 600, loss is 3.6289404964447023 and perplexity is 37.67288091364131
At time: 1199.9965920448303 and batch: 650, loss is 3.4711109352111817 and perplexity is 32.17246412206837
At time: 1200.6200113296509 and batch: 700, loss is 3.466202540397644 and perplexity is 32.01493588768571
At time: 1201.2458491325378 and batch: 750, loss is 3.5703322505950927 and perplexity is 35.52839552140239
At time: 1201.8857555389404 and batch: 800, loss is 3.5149771404266357 and perplexity is 33.61515948693528
At time: 1202.5136005878448 and batch: 850, loss is 3.575575842857361 and perplexity is 35.71518122731373
At time: 1203.1448900699615 and batch: 900, loss is 3.555282406806946 and perplexity is 34.997702161546385
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 98 epochs...
Completing Train Step...
At time: 1204.7415437698364 and batch: 50, loss is 3.8168516778945922 and perplexity is 45.460857359599714
At time: 1205.3643131256104 and batch: 100, loss is 3.68021071434021 and perplexity is 39.65474901656364
At time: 1205.9848947525024 and batch: 150, loss is 3.694681329727173 and perplexity is 40.232749563641796
At time: 1206.6051535606384 and batch: 200, loss is 3.585232367515564 and perplexity is 36.061736321439945
At time: 1207.228078365326 and batch: 250, loss is 3.736353693008423 and perplexity is 41.944767487352316
At time: 1207.8532660007477 and batch: 300, loss is 3.707579779624939 and perplexity is 40.755050865471745
At time: 1208.4791712760925 and batch: 350, loss is 3.6845057916641237 and perplexity is 39.82543552332497
At time: 1209.1133201122284 and batch: 400, loss is 3.635015869140625 and perplexity is 37.90245437174814
At time: 1209.752004623413 and batch: 450, loss is 3.641974411010742 and perplexity is 38.1671199628191
At time: 1210.3814697265625 and batch: 500, loss is 3.5282945966720582 and perplexity is 34.06582208170691
At time: 1211.0063695907593 and batch: 550, loss is 3.6032161140441894 and perplexity is 36.71612801743573
At time: 1211.628905057907 and batch: 600, loss is 3.628940534591675 and perplexity is 37.67288235074769
At time: 1212.2619235515594 and batch: 650, loss is 3.4711109256744384 and perplexity is 32.17246381524784
At time: 1212.9004204273224 and batch: 700, loss is 3.4662025356292725 and perplexity is 32.014935735026604
At time: 1213.5286357402802 and batch: 750, loss is 3.5703322649002076 and perplexity is 35.52839602964017
At time: 1214.1586000919342 and batch: 800, loss is 3.5149771308898927 and perplexity is 33.61515916635615
At time: 1214.7970266342163 and batch: 850, loss is 3.575575838088989 and perplexity is 35.71518105701046
At time: 1215.442549943924 and batch: 900, loss is 3.5552824211120604 and perplexity is 34.99770266219252
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
finished 99 epochs...
Completing Train Step...
At time: 1217.0546164512634 and batch: 50, loss is 3.816851692199707 and perplexity is 45.46085800992251
At time: 1217.715075969696 and batch: 100, loss is 3.6802106857299806 and perplexity is 39.65474788203219
At time: 1218.3570024967194 and batch: 150, loss is 3.6946814012527467 and perplexity is 40.2327524413124
At time: 1218.9963459968567 and batch: 200, loss is 3.5852324056625364 and perplexity is 36.06173769708603
At time: 1219.62864279747 and batch: 250, loss is 3.73635365486145 and perplexity is 41.944765887286444
At time: 1220.2676243782043 and batch: 300, loss is 3.7075797271728517 and perplexity is 40.75504872778432
At time: 1220.9012877941132 and batch: 350, loss is 3.6845057916641237 and perplexity is 39.82543552332497
At time: 1221.5301096439362 and batch: 400, loss is 3.6350159072875976 and perplexity is 37.90245581761206
At time: 1222.171109676361 and batch: 450, loss is 3.641974425315857 and perplexity is 38.16712050880414
At time: 1222.8051600456238 and batch: 500, loss is 3.528294634819031 and perplexity is 34.06582338121492
At time: 1223.4440050125122 and batch: 550, loss is 3.6032161140441894 and perplexity is 36.71612801743573
At time: 1224.0855824947357 and batch: 600, loss is 3.628940486907959 and perplexity is 37.67288055436471
At time: 1224.7175402641296 and batch: 650, loss is 3.4711109304428103 and perplexity is 32.17246396865811
At time: 1225.3583309650421 and batch: 700, loss is 3.4662025356292725 and perplexity is 32.014935735026604
At time: 1225.9914255142212 and batch: 750, loss is 3.570332269668579 and perplexity is 35.52839619905276
At time: 1226.6268348693848 and batch: 800, loss is 3.5149771070480345 and perplexity is 33.615158364908304
At time: 1227.2675430774689 and batch: 850, loss is 3.5755758619308473 and perplexity is 35.71518190852676
At time: 1227.91481590271 and batch: 900, loss is 3.5552823877334596 and perplexity is 34.997701494018195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.452420587409033 and perplexity of 85.83446256180737
Finished Training.
langmodel
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.6867840398358628, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.05034803884462, 'wordvec_source': 'None', 'dropout': 0.20082065557742956, 'lr': 4.125119577239014, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.40782479654432197, 'tune_wordvecs': True}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.07880911032358805 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8766500949859619 and batch: 50, loss is 6.330573835372925 and perplexity is 561.478697764764
At time: 1.524566650390625 and batch: 100, loss is 5.4563214969635006 and perplexity is 234.2342064241242
At time: 2.1643178462982178 and batch: 150, loss is 5.314175415039062 and perplexity is 203.19689092860375
At time: 2.800161600112915 and batch: 200, loss is 5.155654182434082 and perplexity is 173.409210921028
At time: 3.433198928833008 and batch: 250, loss is 5.215643272399903 and perplexity is 184.13022878549225
At time: 4.07580304145813 and batch: 300, loss is 5.156374607086182 and perplexity is 173.534184202988
At time: 4.713138103485107 and batch: 350, loss is 5.1349103641510006 and perplexity is 169.84909462453066
At time: 5.356498718261719 and batch: 400, loss is 5.005508861541748 and perplexity is 149.23300277633518
At time: 5.999378681182861 and batch: 450, loss is 5.004020080566407 and perplexity is 149.0109928240119
At time: 6.638638734817505 and batch: 500, loss is 4.929109535217285 and perplexity is 138.2563451037543
At time: 7.273680210113525 and batch: 550, loss is 5.006149787902832 and perplexity is 149.32868079976663
At time: 7.929722785949707 and batch: 600, loss is 4.9521361923217775 and perplexity is 141.47686313802214
At time: 8.566602945327759 and batch: 650, loss is 4.827260971069336 and perplexity is 124.86847346637377
At time: 9.203725814819336 and batch: 700, loss is 4.8853022003173825 and perplexity is 132.33044959622947
At time: 9.841334581375122 and batch: 750, loss is 4.924798784255981 and perplexity is 137.66163916671712
At time: 10.476248264312744 and batch: 800, loss is 4.867646064758301 and perplexity is 130.01451066913742
At time: 11.112579822540283 and batch: 850, loss is 4.92973484992981 and perplexity is 138.3428258664763
At time: 11.744417667388916 and batch: 900, loss is 4.87084415435791 and perplexity is 130.43097431257203
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.83072401072881 and perplexity of 125.30164755882815
finished 1 epochs...
Completing Train Step...
At time: 13.361703872680664 and batch: 50, loss is 4.806535005569458 and perplexity is 122.30708906522464
At time: 13.980090618133545 and batch: 100, loss is 4.652153148651123 and perplexity is 104.81041520239215
At time: 14.606304168701172 and batch: 150, loss is 4.640538578033447 and perplexity is 103.60012931443619
At time: 15.22748327255249 and batch: 200, loss is 4.533015518188477 and perplexity is 93.03869837746431
At time: 15.851320743560791 and batch: 250, loss is 4.664989070892334 and perplexity is 106.16442493320432
At time: 16.48102116584778 and batch: 300, loss is 4.649182348251343 and perplexity is 104.49950643162248
At time: 17.1082661151886 and batch: 350, loss is 4.624876632690429 and perplexity is 101.99019005119906
At time: 17.748809576034546 and batch: 400, loss is 4.535258054733276 and perplexity is 93.24757517804944
At time: 18.378406047821045 and batch: 450, loss is 4.555368785858154 and perplexity is 95.14183573012136
At time: 19.018211603164673 and batch: 500, loss is 4.450830554962158 and perplexity is 85.69809142726956
At time: 19.648584842681885 and batch: 550, loss is 4.537509031295777 and perplexity is 93.45770969949912
At time: 20.277458429336548 and batch: 600, loss is 4.514395580291748 and perplexity is 91.32235230840162
At time: 20.90401792526245 and batch: 650, loss is 4.386548805236816 and perplexity is 80.36259288054083
At time: 21.534239530563354 and batch: 700, loss is 4.4052403450012205 and perplexity is 81.87881964346974
At time: 22.176349878311157 and batch: 750, loss is 4.500476369857788 and perplexity is 90.0600229638946
At time: 22.808032035827637 and batch: 800, loss is 4.445819997787476 and perplexity is 85.26977020051771
At time: 23.437865734100342 and batch: 850, loss is 4.515225296020508 and perplexity is 91.39815534363576
At time: 24.076752424240112 and batch: 900, loss is 4.46737717628479 and perplexity is 87.12790193848487
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.674412557523545 and perplexity of 107.16959262602442
finished 2 epochs...
Completing Train Step...
At time: 25.67321467399597 and batch: 50, loss is 4.483563566207886 and perplexity is 88.54966368588298
At time: 26.295809745788574 and batch: 100, loss is 4.332678852081298 and perplexity is 76.14800282035111
At time: 26.91718339920044 and batch: 150, loss is 4.325949764251709 and perplexity is 75.63731637531139
At time: 27.537577390670776 and batch: 200, loss is 4.232925882339478 and perplexity is 68.91858513377146
At time: 28.16172957420349 and batch: 250, loss is 4.3748891735076905 and perplexity is 79.43103600133647
At time: 28.796281337738037 and batch: 300, loss is 4.36669882774353 and perplexity is 78.78312528040652
At time: 29.4342143535614 and batch: 350, loss is 4.3375066375732425 and perplexity is 76.51651788368203
At time: 30.06963038444519 and batch: 400, loss is 4.263963222503662 and perplexity is 71.09117602047897
At time: 30.700483083724976 and batch: 450, loss is 4.285614581108093 and perplexity is 72.64718058752887
At time: 31.33923888206482 and batch: 500, loss is 4.182833590507507 and perplexity is 65.5513359421341
At time: 31.97313165664673 and batch: 550, loss is 4.268450870513916 and perplexity is 71.4109251191788
At time: 32.614803314208984 and batch: 600, loss is 4.26475977897644 and perplexity is 71.14782671663944
At time: 33.257614612579346 and batch: 650, loss is 4.1332079744338985 and perplexity is 62.377708403892726
At time: 33.897661209106445 and batch: 700, loss is 4.142480459213257 and perplexity is 62.9587946488929
At time: 34.530683755874634 and batch: 750, loss is 4.243754458427429 and perplexity is 69.66893053088947
At time: 35.16423201560974 and batch: 800, loss is 4.1893114614486695 and perplexity is 65.97734737019448
At time: 35.79847455024719 and batch: 850, loss is 4.259177112579346 and perplexity is 70.75173877732799
At time: 36.43449020385742 and batch: 900, loss is 4.221134219169617 and perplexity is 68.11069294897031
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.618719231592466 and perplexity of 101.36412498788026
finished 3 epochs...
Completing Train Step...
At time: 38.08978247642517 and batch: 50, loss is 4.265805263519287 and perplexity is 71.22224956691969
At time: 38.73869490623474 and batch: 100, loss is 4.124614982604981 and perplexity is 61.843993652618664
At time: 39.381449937820435 and batch: 150, loss is 4.121998834609985 and perplexity is 61.68241206545295
At time: 40.0338032245636 and batch: 200, loss is 4.0302714824676515 and perplexity is 56.27618717162455
At time: 40.67845845222473 and batch: 250, loss is 4.178050312995911 and perplexity is 65.23853441572174
At time: 41.31072974205017 and batch: 300, loss is 4.165825071334839 and perplexity is 64.44583292337346
At time: 41.954299211502075 and batch: 350, loss is 4.135941953659057 and perplexity is 62.548481100912866
At time: 42.59587836265564 and batch: 400, loss is 4.0700497055053715 and perplexity is 58.55987326764872
At time: 43.23064637184143 and batch: 450, loss is 4.099738526344299 and perplexity is 60.32451226428473
At time: 43.87294340133667 and batch: 500, loss is 3.9924289655685423 and perplexity is 54.18634641609727
At time: 44.51164770126343 and batch: 550, loss is 4.073023185729981 and perplexity is 58.73425903058729
At time: 45.13754987716675 and batch: 600, loss is 4.075782566070557 and perplexity is 58.89655300269467
At time: 45.762779712677 and batch: 650, loss is 3.9466030836105346 and perplexity is 51.75924603920129
At time: 46.38662600517273 and batch: 700, loss is 3.95853551864624 and perplexity is 52.380559398689456
At time: 47.010210275650024 and batch: 750, loss is 4.057463207244873 and perplexity is 57.827428650540426
At time: 47.63774800300598 and batch: 800, loss is 4.003125085830688 and perplexity is 54.769040823309176
At time: 48.267531871795654 and batch: 850, loss is 4.0805421686172485 and perplexity is 59.17754536186957
At time: 48.9044029712677 and batch: 900, loss is 4.044211807250977 and perplexity is 57.06618914686435
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.611454375802654 and perplexity of 100.63039767619716
finished 4 epochs...
Completing Train Step...
At time: 50.520678758621216 and batch: 50, loss is 4.094766383171081 and perplexity is 60.025314595676875
At time: 51.170167684555054 and batch: 100, loss is 3.9652152681350707 and perplexity is 52.7316196053453
At time: 51.805737257003784 and batch: 150, loss is 3.958802638053894 and perplexity is 52.39455313160424
At time: 52.43137979507446 and batch: 200, loss is 3.8716519689559936 and perplexity is 48.021650867758545
At time: 53.062049865722656 and batch: 250, loss is 4.012378072738647 and perplexity is 55.278169890770606
At time: 53.70513415336609 and batch: 300, loss is 4.010013027191162 and perplexity is 55.14758897696808
At time: 54.345202684402466 and batch: 350, loss is 3.9715418910980222 and perplexity is 53.066290231952216
At time: 54.98479771614075 and batch: 400, loss is 3.9119041204452514 and perplexity is 49.99405610418224
At time: 55.621912479400635 and batch: 450, loss is 3.95110258102417 and perplexity is 51.9926613649133
At time: 56.281176805496216 and batch: 500, loss is 3.837055969238281 and perplexity is 46.38870343963723
At time: 56.92337203025818 and batch: 550, loss is 3.91454363822937 and perplexity is 50.12619061334719
At time: 57.557823181152344 and batch: 600, loss is 3.92106812953949 and perplexity is 50.45430774312737
At time: 58.19767737388611 and batch: 650, loss is 3.796117582321167 and perplexity is 44.52797228300431
At time: 58.83817720413208 and batch: 700, loss is 3.8084091091156007 and perplexity is 45.0786665507985
At time: 59.48489809036255 and batch: 750, loss is 3.910802626609802 and perplexity is 49.93901827705277
At time: 60.12572193145752 and batch: 800, loss is 3.854248185157776 and perplexity is 47.19312311014692
At time: 60.769932985305786 and batch: 850, loss is 3.936129717826843 and perplexity is 51.219981409984236
At time: 61.417301654815674 and batch: 900, loss is 3.8973433208465575 and perplexity is 49.27137683443933
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.617522357261344 and perplexity of 101.24287744209731
Annealing...
finished 5 epochs...
Completing Train Step...
At time: 63.11266469955444 and batch: 50, loss is 3.9476772022247313 and perplexity is 51.81487147764014
At time: 63.76066207885742 and batch: 100, loss is 3.803393478393555 and perplexity is 44.85313467105005
At time: 64.39295434951782 and batch: 150, loss is 3.7820783472061157 and perplexity is 43.90720138138186
At time: 65.02010202407837 and batch: 200, loss is 3.6620020484924316 and perplexity is 38.939223098224026
At time: 65.64763379096985 and batch: 250, loss is 3.792803249359131 and perplexity is 44.38063605226872
At time: 66.27865743637085 and batch: 300, loss is 3.774976282119751 and perplexity is 43.59647428750463
At time: 66.90582752227783 and batch: 350, loss is 3.7188526296615603 and perplexity is 41.21707571782697
At time: 67.54120516777039 and batch: 400, loss is 3.644218325614929 and perplexity is 38.25285988125487
At time: 68.18066215515137 and batch: 450, loss is 3.6588837194442747 and perplexity is 38.81798691312448
At time: 68.81642842292786 and batch: 500, loss is 3.525169610977173 and perplexity is 33.95953303738602
At time: 69.4561038017273 and batch: 550, loss is 3.5776939296722414 and perplexity is 35.79090925271693
At time: 70.08566212654114 and batch: 600, loss is 3.5650400161743163 and perplexity is 35.34086758230217
At time: 70.71417665481567 and batch: 650, loss is 3.4207596015930175 and perplexity is 30.592644418824655
At time: 71.34145426750183 and batch: 700, loss is 3.4071957588195803 and perplexity is 30.18049210339468
At time: 71.96532535552979 and batch: 750, loss is 3.4856126737594604 and perplexity is 32.64242013889628
At time: 72.61402535438538 and batch: 800, loss is 3.396279377937317 and perplexity is 29.852822095642043
At time: 73.25575733184814 and batch: 850, loss is 3.443502435684204 and perplexity is 31.296379993074822
At time: 73.88943195343018 and batch: 900, loss is 3.3850925016403197 and perplexity is 29.520723305186717
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.499857445285745 and perplexity of 90.00429984870489
finished 6 epochs...
Completing Train Step...
At time: 75.51652002334595 and batch: 50, loss is 3.7541139936447143 and perplexity is 42.69637378859585
At time: 76.14816880226135 and batch: 100, loss is 3.6218026542663573 and perplexity is 37.40493524983114
At time: 76.78321933746338 and batch: 150, loss is 3.6081184434890745 and perplexity is 36.89656449107009
At time: 77.41812920570374 and batch: 200, loss is 3.5024309253692625 and perplexity is 33.196051076502386
At time: 78.05692601203918 and batch: 250, loss is 3.635570640563965 and perplexity is 37.92348740403121
At time: 78.68619132041931 and batch: 300, loss is 3.627883505821228 and perplexity is 37.63308206896453
At time: 79.31484198570251 and batch: 350, loss is 3.578375382423401 and perplexity is 35.815307378434476
At time: 79.94458842277527 and batch: 400, loss is 3.512475361824036 and perplexity is 33.531166909432876
At time: 80.57346963882446 and batch: 450, loss is 3.537311806678772 and perplexity is 34.374389872953444
At time: 81.19818496704102 and batch: 500, loss is 3.40760769367218 and perplexity is 30.192927060981056
At time: 81.82497811317444 and batch: 550, loss is 3.4662659263610838 and perplexity is 32.01696524955726
At time: 82.45388889312744 and batch: 600, loss is 3.4658486652374267 and perplexity is 32.00360860145993
At time: 83.0902795791626 and batch: 650, loss is 3.329876103401184 and perplexity is 27.934880452145965
At time: 83.724942445755 and batch: 700, loss is 3.326009364128113 and perplexity is 27.827072120494293
At time: 84.35417532920837 and batch: 750, loss is 3.415310597419739 and perplexity is 30.426398320898706
At time: 84.98882937431335 and batch: 800, loss is 3.3352455997467043 and perplexity is 28.085280114511324
At time: 85.62355041503906 and batch: 850, loss is 3.39801353931427 and perplexity is 29.904636621102547
At time: 86.25502848625183 and batch: 900, loss is 3.3546316194534302 and perplexity is 28.63505365080416
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.506312958181721 and perplexity of 90.58720321285604
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 87.89392232894897 and batch: 50, loss is 3.6746143960952757 and perplexity is 39.43344823352105
At time: 88.54187417030334 and batch: 100, loss is 3.5476274919509887 and perplexity is 34.73082051271491
At time: 89.17354273796082 and batch: 150, loss is 3.535715322494507 and perplexity is 34.319555485939965
At time: 89.8070776462555 and batch: 200, loss is 3.428180799484253 and perplexity is 30.820523007271763
At time: 90.43789267539978 and batch: 250, loss is 3.557438025474548 and perplexity is 35.073225231874744
At time: 91.07723021507263 and batch: 300, loss is 3.5456282234191896 and perplexity is 34.66145364078775
At time: 91.70721912384033 and batch: 350, loss is 3.4916534328460695 and perplexity is 32.84020190953959
At time: 92.33559536933899 and batch: 400, loss is 3.4211733198165892 and perplexity is 30.605303771849925
At time: 92.96466636657715 and batch: 450, loss is 3.4362716722488402 and perplexity is 31.07089945359849
At time: 93.60012865066528 and batch: 500, loss is 3.300400524139404 and perplexity is 27.123500362018085
At time: 94.24302887916565 and batch: 550, loss is 3.3484273052215574 and perplexity is 28.457942773810064
At time: 94.87518906593323 and batch: 600, loss is 3.3428228092193604 and perplexity is 28.29889645085259
At time: 95.50700831413269 and batch: 650, loss is 3.200938310623169 and perplexity is 24.555560133733916
At time: 96.13681983947754 and batch: 700, loss is 3.184817171096802 and perplexity is 24.162870330900788
At time: 96.76832509040833 and batch: 750, loss is 3.268219451904297 and perplexity is 26.264532421456785
At time: 97.39565110206604 and batch: 800, loss is 3.1814163303375245 and perplexity is 24.080835828823133
At time: 98.03039240837097 and batch: 850, loss is 3.233315143585205 and perplexity is 25.36360173119243
At time: 98.66617488861084 and batch: 900, loss is 3.185686249732971 and perplexity is 24.183878893017848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.478562864538741 and perplexity of 88.107958573111
finished 8 epochs...
Completing Train Step...
At time: 100.31899046897888 and batch: 50, loss is 3.6224316501617433 and perplexity is 37.42847020148852
At time: 100.97736358642578 and batch: 100, loss is 3.4930226802825928 and perplexity is 32.88519907091286
At time: 101.61772799491882 and batch: 150, loss is 3.4804266119003295 and perplexity is 32.47357273380044
At time: 102.25632190704346 and batch: 200, loss is 3.376215810775757 and perplexity is 29.259836588733677
At time: 102.89068102836609 and batch: 250, loss is 3.505125889778137 and perplexity is 33.28563390972357
At time: 103.5327775478363 and batch: 300, loss is 3.496778483390808 and perplexity is 33.00894163469057
At time: 104.16346383094788 and batch: 350, loss is 3.4450698566436766 and perplexity is 31.34547305973139
At time: 104.81762766838074 and batch: 400, loss is 3.3792715787887575 and perplexity is 29.349384610756644
At time: 105.45358371734619 and batch: 450, loss is 3.398260669708252 and perplexity is 29.912027878997232
At time: 106.09707021713257 and batch: 500, loss is 3.2651762437820433 and perplexity is 26.184725479223793
At time: 106.73566055297852 and batch: 550, loss is 3.3167041683197023 and perplexity is 27.569336764317462
At time: 107.37121272087097 and batch: 600, loss is 3.3164094257354737 and perplexity is 27.561212104154563
At time: 108.0091700553894 and batch: 650, loss is 3.179024910926819 and perplexity is 24.023317253526642
At time: 108.64621877670288 and batch: 700, loss is 3.1678008127212522 and perplexity is 23.755184769065217
At time: 109.28446173667908 and batch: 750, loss is 3.2559572792053224 and perplexity is 25.944438721946156
At time: 109.9255359172821 and batch: 800, loss is 3.173540358543396 and perplexity is 23.891920766454735
At time: 110.55999445915222 and batch: 850, loss is 3.232318849563599 and perplexity is 25.338344710219914
At time: 111.20413637161255 and batch: 900, loss is 3.189615216255188 and perplexity is 24.27908344885124
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.481093315229024 and perplexity of 88.33119374137662
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 112.85232734680176 and batch: 50, loss is 3.597701835632324 and perplexity is 36.51422225924031
At time: 113.49342036247253 and batch: 100, loss is 3.469876561164856 and perplexity is 32.132775767526496
At time: 114.13090395927429 and batch: 150, loss is 3.459243130683899 and perplexity is 31.792904332309885
At time: 114.77491688728333 and batch: 200, loss is 3.3553072118759157 and perplexity is 28.654405812418915
At time: 115.42073035240173 and batch: 250, loss is 3.4828110218048094 and perplexity is 32.55109542847223
At time: 116.06707501411438 and batch: 300, loss is 3.4740168333053587 and perplexity is 32.26608999198084
At time: 116.70545959472656 and batch: 350, loss is 3.4202956533432007 and perplexity is 30.578454306982582
At time: 117.34909224510193 and batch: 400, loss is 3.355795922279358 and perplexity is 28.66841294107986
At time: 117.98776912689209 and batch: 450, loss is 3.3693722677230835 and perplexity is 29.060279255764073
At time: 118.62736940383911 and batch: 500, loss is 3.234172658920288 and perplexity is 25.385360736636944
At time: 119.26047706604004 and batch: 550, loss is 3.282917356491089 and perplexity is 26.653416905710642
At time: 119.89558362960815 and batch: 600, loss is 3.2813206624984743 and perplexity is 26.61089351251208
At time: 120.53060746192932 and batch: 650, loss is 3.13995858669281 and perplexity is 23.102910070998693
At time: 121.17424654960632 and batch: 700, loss is 3.124925775527954 and perplexity is 22.758205815023267
At time: 121.81105756759644 and batch: 750, loss is 3.2083442783355713 and perplexity is 24.738092900832992
At time: 122.44211673736572 and batch: 800, loss is 3.1240142393112182 and perplexity is 22.73747033819975
At time: 123.06563591957092 and batch: 850, loss is 3.1789617252349855 and perplexity is 24.02179937156056
At time: 123.69446206092834 and batch: 900, loss is 3.135429654121399 and perplexity is 22.998515126355315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477991391534674 and perplexity of 88.05762163781277
finished 10 epochs...
Completing Train Step...
At time: 125.29940915107727 and batch: 50, loss is 3.58341477394104 and perplexity is 35.996250272755134
At time: 125.92307472229004 and batch: 100, loss is 3.4550673580169677 and perplexity is 31.66042119364114
At time: 126.54629063606262 and batch: 150, loss is 3.4444423151016235 and perplexity is 31.325808643995323
At time: 127.17480492591858 and batch: 200, loss is 3.3404803609848024 and perplexity is 28.2326853291593
At time: 127.80841708183289 and batch: 250, loss is 3.4676934337615966 and perplexity is 32.06270234164485
At time: 128.44287538528442 and batch: 300, loss is 3.45945348739624 and perplexity is 31.799592886607485
At time: 129.07747316360474 and batch: 350, loss is 3.4066022396087647 and perplexity is 30.16258471625427
At time: 129.70703077316284 and batch: 400, loss is 3.3431909465789795 and perplexity is 28.30931624971309
At time: 130.33655667304993 and batch: 450, loss is 3.3583395051956177 and perplexity is 28.741426244803876
At time: 130.9750108718872 and batch: 500, loss is 3.224404354095459 and perplexity is 25.1385959936712
At time: 131.61196398735046 and batch: 550, loss is 3.274186539649963 and perplexity is 26.421723711711156
At time: 132.2491636276245 and batch: 600, loss is 3.2745071697235106 and perplexity is 26.430196669197993
At time: 132.88591504096985 and batch: 650, loss is 3.1346174621582032 and perplexity is 22.979843500702803
At time: 133.51755166053772 and batch: 700, loss is 3.1216832256317137 and perplexity is 22.684530709274856
At time: 134.15591716766357 and batch: 750, loss is 3.2074702644348143 and perplexity is 24.716480909725554
At time: 134.7853889465332 and batch: 800, loss is 3.1249946928024293 and perplexity is 22.759774302587306
At time: 135.41402554512024 and batch: 850, loss is 3.1821549129486084 and perplexity is 24.098628085143424
At time: 136.04322457313538 and batch: 900, loss is 3.1403407144546507 and perplexity is 23.111740021293258
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.478318723913741 and perplexity of 88.0864504666456
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.72582411766052 and batch: 50, loss is 3.5757247972488404 and perplexity is 35.720501556633394
At time: 138.38165426254272 and batch: 100, loss is 3.447423186302185 and perplexity is 31.419326157401198
At time: 139.02857565879822 and batch: 150, loss is 3.437572441101074 and perplexity is 31.11134180919449
At time: 139.6664683818817 and batch: 200, loss is 3.333207950592041 and perplexity is 28.028110432884997
At time: 140.304425239563 and batch: 250, loss is 3.4602142858505247 and perplexity is 31.82379517308618
At time: 140.9376106262207 and batch: 300, loss is 3.4513586950302124 and perplexity is 31.5432208240844
At time: 141.57729196548462 and batch: 350, loss is 3.398107271194458 and perplexity is 29.907439770289553
At time: 142.2165343761444 and batch: 400, loss is 3.3350476121902464 and perplexity is 28.079720128951156
At time: 142.86277627944946 and batch: 450, loss is 3.3493569898605347 and perplexity is 28.484411988168294
At time: 143.50573873519897 and batch: 500, loss is 3.21489963054657 and perplexity is 24.900792506801633
At time: 144.14867496490479 and batch: 550, loss is 3.264452042579651 and perplexity is 26.165769334407347
At time: 144.77965331077576 and batch: 600, loss is 3.2645484828948974 and perplexity is 26.168292891134975
At time: 145.41401624679565 and batch: 650, loss is 3.1231266117095946 and perplexity is 22.717296886517257
At time: 146.04346299171448 and batch: 700, loss is 3.109391574859619 and perplexity is 22.407407022741836
At time: 146.67171931266785 and batch: 750, loss is 3.193491644859314 and perplexity is 24.37338223558819
At time: 147.3028621673584 and batch: 800, loss is 3.110148115158081 and perplexity is 22.4243655432318
At time: 147.95141577720642 and batch: 850, loss is 3.166210570335388 and perplexity is 23.717438288342862
At time: 148.594007730484 and batch: 900, loss is 3.1247038555145266 and perplexity is 22.753155874045422
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477990973485659 and perplexity of 88.05758482541847
finished 12 epochs...
Completing Train Step...
At time: 150.2304720878601 and batch: 50, loss is 3.572414307594299 and perplexity is 35.60244472655119
At time: 150.87977647781372 and batch: 100, loss is 3.444058051109314 and perplexity is 31.31377357617316
At time: 151.5095603466034 and batch: 150, loss is 3.4341749286651613 and perplexity is 31.005819995840348
At time: 152.14359974861145 and batch: 200, loss is 3.3299037075042723 and perplexity is 27.93565158010882
At time: 152.77280640602112 and batch: 250, loss is 3.4568733978271484 and perplexity is 31.717652840483826
At time: 153.4218292236328 and batch: 300, loss is 3.448087387084961 and perplexity is 31.44020183048098
At time: 154.06052327156067 and batch: 350, loss is 3.395032172203064 and perplexity is 29.81561269345395
At time: 154.69935202598572 and batch: 400, loss is 3.3321876907348633 and perplexity is 27.99952905963095
At time: 155.33009266853333 and batch: 450, loss is 3.3468602085113526 and perplexity is 28.413381350459463
At time: 155.9601480960846 and batch: 500, loss is 3.212706003189087 and perplexity is 24.84622931467103
At time: 156.5938422679901 and batch: 550, loss is 3.2624287843704223 and perplexity is 26.112882746453604
At time: 157.22659945487976 and batch: 600, loss is 3.2630076217651367 and perplexity is 26.12800223491155
At time: 157.85568761825562 and batch: 650, loss is 3.12203013420105 and perplexity is 22.692401532518353
At time: 158.48376631736755 and batch: 700, loss is 3.1088799476623534 and perplexity is 22.39594571609691
At time: 159.11766910552979 and batch: 750, loss is 3.1935325479507446 and perplexity is 24.37437920265963
At time: 159.7523536682129 and batch: 800, loss is 3.1106974267959595 and perplexity is 22.436686892017057
At time: 160.39145016670227 and batch: 850, loss is 3.1673073291778566 and perplexity is 23.743464868338272
At time: 161.02831268310547 and batch: 900, loss is 3.12621365070343 and perplexity is 22.78753442507146
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4780340325342465 and perplexity of 88.0613765828761
Annealing...
finished 13 epochs...
Completing Train Step...
At time: 162.67959475517273 and batch: 50, loss is 3.57012357711792 and perplexity is 35.520982461052
At time: 163.30494046211243 and batch: 100, loss is 3.4417838430404664 and perplexity is 31.242640456056524
At time: 163.9335699081421 and batch: 150, loss is 3.432133049964905 and perplexity is 30.942574464232763
At time: 164.56192350387573 and batch: 200, loss is 3.3277052068710327 and perplexity is 27.874302495131488
At time: 165.1876232624054 and batch: 250, loss is 3.454880323410034 and perplexity is 31.654500152944852
At time: 165.8166172504425 and batch: 300, loss is 3.445708827972412 and perplexity is 31.36550831859853
At time: 166.454026222229 and batch: 350, loss is 3.3926597929000852 and perplexity is 29.74496258855366
At time: 167.09181761741638 and batch: 400, loss is 3.3297192430496216 and perplexity is 27.930498920630658
At time: 167.72298860549927 and batch: 450, loss is 3.3443284797668458 and perplexity is 28.341537359289223
At time: 168.35906147956848 and batch: 500, loss is 3.210056390762329 and perplexity is 24.78048357554819
At time: 169.00395011901855 and batch: 550, loss is 3.2597129726409912 and perplexity is 26.042061285607982
At time: 169.66578650474548 and batch: 600, loss is 3.2604047775268556 and perplexity is 26.060083544067794
At time: 170.3038740158081 and batch: 650, loss is 3.1189864587783815 and perplexity is 22.62343823188354
At time: 170.9481348991394 and batch: 700, loss is 3.1056070518493653 and perplexity is 22.32276593935144
At time: 171.58685064315796 and batch: 750, loss is 3.189979844093323 and perplexity is 24.287937892751362
At time: 172.2232165336609 and batch: 800, loss is 3.1068447113037108 and perplexity is 22.35041102573579
At time: 172.8574333190918 and batch: 850, loss is 3.163201665878296 and perplexity is 23.646182037950858
At time: 173.48640632629395 and batch: 900, loss is 3.1221333742141724 and perplexity is 22.694744417287996
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477979686162243 and perplexity of 88.0565908965887
finished 14 epochs...
Completing Train Step...
At time: 175.13025069236755 and batch: 50, loss is 3.569342908859253 and perplexity is 35.49326317871771
At time: 175.76811456680298 and batch: 100, loss is 3.4410248947143556 and perplexity is 31.2189379020237
At time: 176.40915703773499 and batch: 150, loss is 3.4313254308700563 and perplexity is 30.917594738661098
At time: 177.0554232597351 and batch: 200, loss is 3.3269443273544312 and perplexity is 27.85310157601877
At time: 177.69047689437866 and batch: 250, loss is 3.4540386390686035 and perplexity is 31.627868265223988
At time: 178.32863116264343 and batch: 300, loss is 3.444944729804993 and perplexity is 31.341551145153883
At time: 178.96699571609497 and batch: 350, loss is 3.3919184494018553 and perplexity is 29.722919525683764
At time: 179.60552048683167 and batch: 400, loss is 3.3290782308578493 and perplexity is 27.91260086734828
At time: 180.24986338615417 and batch: 450, loss is 3.3437528514862063 and perplexity is 28.325227863422292
At time: 180.89472317695618 and batch: 500, loss is 3.209566426277161 and perplexity is 24.768344992650974
At time: 181.5381565093994 and batch: 550, loss is 3.259241762161255 and perplexity is 26.02979288414294
At time: 182.17880272865295 and batch: 600, loss is 3.2600530576705933 and perplexity is 26.050919306946046
At time: 182.82036566734314 and batch: 650, loss is 3.1187537002563475 and perplexity is 22.6181730466195
At time: 183.4574568271637 and batch: 700, loss is 3.1054915761947632 and perplexity is 22.320188352169218
At time: 184.08806371688843 and batch: 750, loss is 3.189997272491455 and perplexity is 24.288361196291508
At time: 184.71757793426514 and batch: 800, loss is 3.1069905233383177 and perplexity is 22.35367022225102
At time: 185.35924124717712 and batch: 850, loss is 3.1634763956069945 and perplexity is 23.652679239573253
At time: 186.01879262924194 and batch: 900, loss is 3.1225177907943724 and perplexity is 22.703470330410788
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.47797634177012 and perplexity of 88.05629640131215
finished 15 epochs...
Completing Train Step...
At time: 187.69482445716858 and batch: 50, loss is 3.56864013671875 and perplexity is 35.46832826498777
At time: 188.3334732055664 and batch: 100, loss is 3.4403000497817993 and perplexity is 31.19631721232219
At time: 188.96405816078186 and batch: 150, loss is 3.430576753616333 and perplexity is 30.894456101506897
At time: 189.592036485672 and batch: 200, loss is 3.3262171363830566 and perplexity is 27.832854414696598
At time: 190.23074626922607 and batch: 250, loss is 3.4532861614227297 and perplexity is 31.604077953329263
At time: 190.86721754074097 and batch: 300, loss is 3.4442186880111696 and perplexity is 31.31880412778113
At time: 191.5022382736206 and batch: 350, loss is 3.391244306564331 and perplexity is 29.70288878492404
At time: 192.14240288734436 and batch: 400, loss is 3.328452076911926 and perplexity is 27.895128752861787
At time: 192.77259373664856 and batch: 450, loss is 3.3431961917877198 and perplexity is 28.30946473837554
At time: 193.41041374206543 and batch: 500, loss is 3.209088068008423 and perplexity is 24.756499683397475
At time: 194.04841494560242 and batch: 550, loss is 3.2587988233566283 and perplexity is 26.018265831876
At time: 194.68338775634766 and batch: 600, loss is 3.2597221899032593 and perplexity is 26.042301323223096
At time: 195.32033371925354 and batch: 650, loss is 3.118523473739624 and perplexity is 22.61296634280797
At time: 195.96567034721375 and batch: 700, loss is 3.1053784704208374 and perplexity is 22.317663952756238
At time: 196.60777926445007 and batch: 750, loss is 3.1900103998184206 and perplexity is 24.28868003964316
At time: 197.24696588516235 and batch: 800, loss is 3.1071164178848267 and perplexity is 22.356484604580437
At time: 197.8906798362732 and batch: 850, loss is 3.1637271642684937 and perplexity is 23.658611334047638
At time: 198.5270493030548 and batch: 900, loss is 3.1228580045700074 and perplexity is 22.711195677832208
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477988047142551 and perplexity of 88.05732713908903
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 200.13232803344727 and batch: 50, loss is 3.568057379722595 and perplexity is 35.44766487000321
At time: 200.77760982513428 and batch: 100, loss is 3.4397337436676025 and perplexity is 31.178655548570525
At time: 201.4034068584442 and batch: 150, loss is 3.4300522232055664 and perplexity is 30.878255269043656
At time: 202.04885601997375 and batch: 200, loss is 3.3256605958938597 and perplexity is 27.817368613922174
At time: 202.6704306602478 and batch: 250, loss is 3.452795262336731 and perplexity is 31.58856734773068
At time: 203.2922306060791 and batch: 300, loss is 3.4436357259750365 and perplexity is 31.300551774688923
At time: 203.91388821601868 and batch: 350, loss is 3.39065420627594 and perplexity is 29.685366272214377
At time: 204.5354974269867 and batch: 400, loss is 3.327828760147095 and perplexity is 27.877746669287827
At time: 205.1718623638153 and batch: 450, loss is 3.342543501853943 and perplexity is 28.290993464373226
At time: 205.79759645462036 and batch: 500, loss is 3.2083928871154783 and perplexity is 24.73929541857235
At time: 206.4251265525818 and batch: 550, loss is 3.2581116151809693 and perplexity is 26.00039200910185
At time: 207.04618310928345 and batch: 600, loss is 3.259063386917114 and perplexity is 26.02515022756211
At time: 207.67510890960693 and batch: 650, loss is 3.1177469968795775 and perplexity is 22.59541471280489
At time: 208.3135929107666 and batch: 700, loss is 3.10453351020813 and perplexity is 22.29881437936684
At time: 208.95029187202454 and batch: 750, loss is 3.1891145944595336 and perplexity is 24.266931852429035
At time: 209.57487297058105 and batch: 800, loss is 3.1061447715759276 and perplexity is 22.334772558757216
At time: 210.20819282531738 and batch: 850, loss is 3.1626981925964355 and perplexity is 23.634279813554148
At time: 210.83998584747314 and batch: 900, loss is 3.1218025970458982 and perplexity is 22.687238755414167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4779859568974745 and perplexity of 88.05714307788689
Annealing...
finished 17 epochs...
Completing Train Step...
At time: 212.44290685653687 and batch: 50, loss is 3.5679006004333496 and perplexity is 35.442107845924
At time: 213.08196568489075 and batch: 100, loss is 3.439588646888733 and perplexity is 31.174131954268358
At time: 213.70465350151062 and batch: 150, loss is 3.429920001029968 and perplexity is 30.874172748859106
At time: 214.32738041877747 and batch: 200, loss is 3.3255178546905517 and perplexity is 27.813398212630034
At time: 214.95225024223328 and batch: 250, loss is 3.452653656005859 and perplexity is 31.584094523308668
At time: 215.58924269676208 and batch: 300, loss is 3.4434858894348146 and perplexity is 31.295862159650586
At time: 216.2204179763794 and batch: 350, loss is 3.3904979467391967 and perplexity is 29.68072801302818
At time: 216.85124826431274 and batch: 400, loss is 3.3276740217208864 and perplexity is 27.873433244376884
At time: 217.48340487480164 and batch: 450, loss is 3.3423835706710814 and perplexity is 28.286469214118135
At time: 218.1297504901886 and batch: 500, loss is 3.2082231664657592 and perplexity is 24.735097005569088
At time: 218.77354168891907 and batch: 550, loss is 3.2579386854171752 and perplexity is 25.99589615619776
At time: 219.41708087921143 and batch: 600, loss is 3.2588974046707153 and perplexity is 26.020830873142465
At time: 220.0602011680603 and batch: 650, loss is 3.117553687095642 and perplexity is 22.591047220221952
At time: 220.70118975639343 and batch: 700, loss is 3.1043245124816896 and perplexity is 22.29415446483197
At time: 221.34031748771667 and batch: 750, loss is 3.188889126777649 and perplexity is 24.261461060324397
At time: 221.9757981300354 and batch: 800, loss is 3.1059044933319093 and perplexity is 22.329406643508413
At time: 222.61065888404846 and batch: 850, loss is 3.162443528175354 and perplexity is 23.6282617696911
At time: 223.24207854270935 and batch: 900, loss is 3.121540937423706 and perplexity is 22.68130319767461
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477987629093536 and perplexity of 88.05729032681785
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 224.85512256622314 and batch: 50, loss is 3.567861695289612 and perplexity is 35.44072899244631
At time: 225.50183820724487 and batch: 100, loss is 3.4395525312423705 and perplexity is 31.173006100673632
At time: 226.14838409423828 and batch: 150, loss is 3.429887251853943 and perplexity is 30.873161661697345
At time: 226.78277897834778 and batch: 200, loss is 3.325482659339905 and perplexity is 27.812419327553464
At time: 227.40955209732056 and batch: 250, loss is 3.4526179695129393 and perplexity is 31.582967417854423
At time: 228.0404486656189 and batch: 300, loss is 3.4434487771987916 and perplexity is 31.29470072177949
At time: 228.67857122421265 and batch: 350, loss is 3.3904593420028686 and perplexity is 29.6795822184659
At time: 229.3155107498169 and batch: 400, loss is 3.3276357221603394 and perplexity is 27.872365724575587
At time: 229.95269989967346 and batch: 450, loss is 3.342344408035278 and perplexity is 28.28536146311751
At time: 230.59222745895386 and batch: 500, loss is 3.2081815719604494 and perplexity is 24.734068182842183
At time: 231.23078298568726 and batch: 550, loss is 3.2578958654403687 and perplexity is 25.99478303635934
At time: 231.8695981502533 and batch: 600, loss is 3.258856315612793 and perplexity is 26.01976172368085
At time: 232.50633668899536 and batch: 650, loss is 3.117505865097046 and perplexity is 22.589966897025313
At time: 233.14355611801147 and batch: 700, loss is 3.1042725658416748 and perplexity is 22.2929963884949
At time: 233.7732012271881 and batch: 750, loss is 3.1888329935073854 and perplexity is 24.260099223396246
At time: 234.43668222427368 and batch: 800, loss is 3.1058448266983034 and perplexity is 22.328074362730327
At time: 235.07891058921814 and batch: 850, loss is 3.162380156517029 and perplexity is 23.626764455003578
At time: 235.7115397453308 and batch: 900, loss is 3.1214757537841797 and perplexity is 22.679824795967306
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477987629093536 and perplexity of 88.05729032681785
Annealing...
finished 19 epochs...
Completing Train Step...
At time: 237.36851406097412 and batch: 50, loss is 3.5678530740737915 and perplexity is 35.4404234515899
At time: 238.0042290687561 and batch: 100, loss is 3.439544553756714 and perplexity is 31.17275741945652
At time: 238.6442391872406 and batch: 150, loss is 3.4298803091049193 and perplexity is 30.87294731782843
At time: 239.28040099143982 and batch: 200, loss is 3.325475034713745 and perplexity is 27.81220726906193
At time: 239.9201409816742 and batch: 250, loss is 3.4526102542877197 and perplexity is 31.582723749087673
At time: 240.54970860481262 and batch: 300, loss is 3.443440523147583 and perplexity is 31.29444241478321
At time: 241.18194794654846 and batch: 350, loss is 3.3904504680633547 and perplexity is 29.67931884481708
At time: 241.81210207939148 and batch: 400, loss is 3.3276269960403444 and perplexity is 27.8721225080289
At time: 242.44823718070984 and batch: 450, loss is 3.342335414886475 and perplexity is 28.285107089796725
At time: 243.07611870765686 and batch: 500, loss is 3.2081717252731323 and perplexity is 24.733824635405778
At time: 243.71776342391968 and batch: 550, loss is 3.257885766029358 and perplexity is 25.994520505687028
At time: 244.36471366882324 and batch: 600, loss is 3.258846483230591 and perplexity is 26.01950588869651
At time: 245.00740456581116 and batch: 650, loss is 3.117494282722473 and perplexity is 22.589705253082357
At time: 245.6448450088501 and batch: 700, loss is 3.104259738922119 and perplexity is 22.292710439857487
At time: 246.28666257858276 and batch: 750, loss is 3.188819074630737 and perplexity is 24.25976155241769
At time: 246.926127910614 and batch: 800, loss is 3.105829792022705 and perplexity is 22.327738669899073
At time: 247.56754684448242 and batch: 850, loss is 3.1623640298843383 and perplexity is 23.626383437923813
At time: 248.20996475219727 and batch: 900, loss is 3.12145911693573 and perplexity is 22.67944747829801
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.477988465191567 and perplexity of 88.05736395137568
Annealing...
Model not improving. Stopping early with 88.05629640131215 lossat 19 epochs.
Finished Training.
langmodel
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.8814291672926791, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.561040963230379, 'wordvec_source': 'None', 'dropout': 0.766026158306164, 'lr': 35.32622074923119, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.07615678351470623, 'tune_wordvecs': True}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.08304220040639242 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8681082725524902 and batch: 50, loss is 6.883526115417481 and perplexity is 976.0620062778606
At time: 1.5140604972839355 and batch: 100, loss is 6.177148418426514 and perplexity is 481.6166272534258
At time: 2.1555097103118896 and batch: 150, loss is 6.100323610305786 and perplexity is 446.0020776002709
At time: 2.791281223297119 and batch: 200, loss is 5.953015394210816 and perplexity is 384.9122530712679
At time: 3.4285197257995605 and batch: 250, loss is 6.008348569869995 and perplexity is 406.8109453842122
At time: 4.097621917724609 and batch: 300, loss is 5.9291283321380615 and perplexity is 375.82677488441726
At time: 4.737701416015625 and batch: 350, loss is 5.935469779968262 and perplexity is 378.21763351101174
At time: 5.379105567932129 and batch: 400, loss is 5.8065151023864745 and perplexity is 332.4585206491013
At time: 6.014073610305786 and batch: 450, loss is 5.8057904624938965 and perplexity is 332.21769520881026
At time: 6.657130718231201 and batch: 500, loss is 5.77527099609375 and perplexity is 322.2317464319406
At time: 7.294671535491943 and batch: 550, loss is 5.826255884170532 and perplexity is 339.0867194217726
At time: 7.925474643707275 and batch: 600, loss is 5.7612083053588865 and perplexity is 317.73201436222314
At time: 8.562066793441772 and batch: 650, loss is 5.679929618835449 and perplexity is 292.92881252606475
At time: 9.193809270858765 and batch: 700, loss is 5.790955514907837 and perplexity is 327.32563952625225
At time: 9.835699796676636 and batch: 750, loss is 5.743496866226196 and perplexity is 312.15406569233915
At time: 10.482227802276611 and batch: 800, loss is 5.736647634506226 and perplexity is 310.0233553581311
At time: 11.12726640701294 and batch: 850, loss is 5.779894523620605 and perplexity is 323.7250432709321
At time: 11.771918296813965 and batch: 900, loss is 5.6713293933868405 and perplexity is 290.4203607844157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.442407895440924 and perplexity of 230.99773272744596
finished 1 epochs...
Completing Train Step...
At time: 13.456090450286865 and batch: 50, loss is 5.376174736022949 and perplexity is 216.1936937147923
At time: 14.083009719848633 and batch: 100, loss is 5.15056809425354 and perplexity is 172.52947548540297
At time: 14.710610628128052 and batch: 150, loss is 5.07701247215271 and perplexity is 160.2944557255953
At time: 15.347628831863403 and batch: 200, loss is 4.928089914321899 and perplexity is 138.11544788844594
At time: 15.97405481338501 and batch: 250, loss is 4.9926878929138185 and perplexity is 147.33190413388854
At time: 16.60117197036743 and batch: 300, loss is 4.933549137115478 and perplexity is 138.8715127752794
At time: 17.22703456878662 and batch: 350, loss is 4.903859243392945 and perplexity is 134.80903795747784
At time: 17.86102056503296 and batch: 400, loss is 4.776585912704467 and perplexity is 118.69841074688512
At time: 18.496721029281616 and batch: 450, loss is 4.778342409133911 and perplexity is 118.90708729769874
At time: 19.126659154891968 and batch: 500, loss is 4.678871431350708 and perplexity is 107.64851525195635
At time: 19.77040958404541 and batch: 550, loss is 4.758286657333374 and perplexity is 116.54607138151854
At time: 20.419078588485718 and batch: 600, loss is 4.708787002563477 and perplexity is 110.91753558639705
At time: 21.05389714241028 and batch: 650, loss is 4.575691900253296 and perplexity is 97.09519609206697
At time: 21.685469388961792 and batch: 700, loss is 4.611912517547608 and perplexity is 100.67651122464811
At time: 22.316189765930176 and batch: 750, loss is 4.658846464157104 and perplexity is 105.51429740463136
At time: 22.942753314971924 and batch: 800, loss is 4.593153600692749 and perplexity is 98.80553255366479
At time: 23.56609296798706 and batch: 850, loss is 4.6578122329711915 and perplexity is 105.40522763909436
At time: 24.191107749938965 and batch: 900, loss is 4.597965660095215 and perplexity is 99.28213644905041
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.705752856110873 and perplexity of 110.58150557918549
finished 2 epochs...
Completing Train Step...
At time: 25.81285262107849 and batch: 50, loss is 4.621905956268311 and perplexity is 101.68765978058123
At time: 26.435909748077393 and batch: 100, loss is 4.463368487358093 and perplexity is 86.7793324029769
At time: 27.05844235420227 and batch: 150, loss is 4.462001552581787 and perplexity is 86.6607917527771
At time: 27.6798415184021 and batch: 200, loss is 4.355263810157776 and perplexity is 77.88737010614598
At time: 28.315150499343872 and batch: 250, loss is 4.490452394485474 and perplexity is 89.16177305051892
At time: 28.944549798965454 and batch: 300, loss is 4.470671081542969 and perplexity is 87.41536617287971
At time: 29.57275366783142 and batch: 350, loss is 4.437814416885376 and perplexity is 84.58986132068372
At time: 30.200275421142578 and batch: 400, loss is 4.361930084228516 and perplexity is 78.40832314054832
At time: 30.841618537902832 and batch: 450, loss is 4.383173351287842 and perplexity is 80.09178994758742
At time: 31.479905605316162 and batch: 500, loss is 4.2788047599792485 and perplexity is 72.154146923059
At time: 32.12198877334595 and batch: 550, loss is 4.3666166305542 and perplexity is 78.7766497950788
At time: 32.763752698898315 and batch: 600, loss is 4.355282287597657 and perplexity is 77.88880927864062
At time: 33.399654150009155 and batch: 650, loss is 4.214706964492798 and perplexity is 67.67433198285993
At time: 34.031615018844604 and batch: 700, loss is 4.231631317138672 and perplexity is 68.82942325715206
At time: 34.6658730506897 and batch: 750, loss is 4.320393853187561 and perplexity is 75.21824740535244
At time: 35.300841093063354 and batch: 800, loss is 4.2686205577850345 and perplexity is 71.42304367234337
At time: 35.940802574157715 and batch: 850, loss is 4.342252855300903 and perplexity is 76.88054513024869
At time: 36.612515687942505 and batch: 900, loss is 4.299642205238342 and perplexity is 73.67342901632887
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.597404584492723 and perplexity of 99.226447288901
finished 3 epochs...
Completing Train Step...
At time: 38.27688765525818 and batch: 50, loss is 4.354080247879028 and perplexity is 77.79524008446144
At time: 38.914539098739624 and batch: 100, loss is 4.202332367897034 and perplexity is 66.84204962453703
At time: 39.558295011520386 and batch: 150, loss is 4.20494776725769 and perplexity is 67.01709708812054
At time: 40.19397187232971 and batch: 200, loss is 4.1022123670578 and perplexity is 60.4739302406811
At time: 40.83173727989197 and batch: 250, loss is 4.240952787399292 and perplexity is 69.47401427976389
At time: 41.469181299209595 and batch: 300, loss is 4.23808132648468 and perplexity is 69.27480850585101
At time: 42.10455822944641 and batch: 350, loss is 4.200446724891663 and perplexity is 66.71612813999216
At time: 42.73368978500366 and batch: 400, loss is 4.140217776298523 and perplexity is 62.81649990456131
At time: 43.370670557022095 and batch: 450, loss is 4.162938055992126 and perplexity is 64.26004513009747
At time: 44.011098861694336 and batch: 500, loss is 4.06636079788208 and perplexity is 58.344249257777875
At time: 44.63962125778198 and batch: 550, loss is 4.146366248130798 and perplexity is 63.20391517027397
At time: 45.27752614021301 and batch: 600, loss is 4.149249262809754 and perplexity is 63.38639590640291
At time: 45.902695417404175 and batch: 650, loss is 4.000921478271485 and perplexity is 54.648484229435475
At time: 46.53226947784424 and batch: 700, loss is 4.020557975769043 and perplexity is 55.73219436667524
At time: 47.1602418422699 and batch: 750, loss is 4.11276990890503 and perplexity is 61.11576844502781
At time: 47.787713050842285 and batch: 800, loss is 4.066143088340759 and perplexity is 58.33154854061707
At time: 48.41714906692505 and batch: 850, loss is 4.141542115211487 and perplexity is 62.89974535018576
At time: 49.06005549430847 and batch: 900, loss is 4.10956193447113 and perplexity is 60.92002476058381
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.551206823897688 and perplexity of 94.74668190695597
finished 4 epochs...
Completing Train Step...
At time: 50.64221668243408 and batch: 50, loss is 4.170513625144959 and perplexity is 64.7487001286517
At time: 51.27999997138977 and batch: 100, loss is 4.029483199119568 and perplexity is 56.231843070538005
At time: 51.91146731376648 and batch: 150, loss is 4.029660758972168 and perplexity is 56.24182847478224
At time: 52.56017065048218 and batch: 200, loss is 3.9317132568359376 and perplexity is 50.994269151702824
At time: 53.20322012901306 and batch: 250, loss is 4.070972723960876 and perplexity is 58.61395006452827
At time: 53.83815097808838 and batch: 300, loss is 4.0689070463180546 and perplexity is 58.49299750582963
At time: 54.47500538825989 and batch: 350, loss is 4.0298898935317995 and perplexity is 56.25471689791789
At time: 55.10727882385254 and batch: 400, loss is 3.98154363155365 and perplexity is 53.5997086031536
At time: 55.743587493896484 and batch: 450, loss is 3.999498462677002 and perplexity is 54.5707738887889
At time: 56.37964391708374 and batch: 500, loss is 3.906270685195923 and perplexity is 49.713209634205214
At time: 57.01816391944885 and batch: 550, loss is 3.986264190673828 and perplexity is 53.85332733658867
At time: 57.65264630317688 and batch: 600, loss is 3.9959956932067873 and perplexity is 54.37995943266905
At time: 58.28583359718323 and batch: 650, loss is 3.848433656692505 and perplexity is 46.91951357852863
At time: 58.924657106399536 and batch: 700, loss is 3.864732894897461 and perplexity is 47.690532346760854
At time: 59.56149935722351 and batch: 750, loss is 3.9550931787490846 and perplexity is 52.20055770049685
At time: 60.188469886779785 and batch: 800, loss is 3.916497950553894 and perplexity is 50.22424863223199
At time: 60.81596374511719 and batch: 850, loss is 3.993695397377014 and perplexity is 54.25501320050587
At time: 61.44658422470093 and batch: 900, loss is 3.9620559549331666 and perplexity is 52.565286790448255
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.538313356164384 and perplexity of 93.53291029837523
finished 5 epochs...
Completing Train Step...
At time: 63.09567189216614 and batch: 50, loss is 4.024474582672119 and perplexity is 55.950903483093306
At time: 63.74824905395508 and batch: 100, loss is 3.8860467433929444 and perplexity is 48.71791093076625
At time: 64.37647843360901 and batch: 150, loss is 3.8901083421707154 and perplexity is 48.91618592241848
At time: 65.01059293746948 and batch: 200, loss is 3.7942023181915285 and perplexity is 44.442771072381234
At time: 65.65107703208923 and batch: 250, loss is 3.932614288330078 and perplexity is 51.040237300494105
At time: 66.2733838558197 and batch: 300, loss is 3.934670205116272 and perplexity is 51.14527972337236
At time: 66.90195202827454 and batch: 350, loss is 3.891514639854431 and perplexity is 48.98502503417398
At time: 67.53538680076599 and batch: 400, loss is 3.846856031417847 and perplexity is 46.84555052636329
At time: 68.17553472518921 and batch: 450, loss is 3.865598373413086 and perplexity is 47.7318253444293
At time: 68.82851672172546 and batch: 500, loss is 3.773364095687866 and perplexity is 43.526245269526534
At time: 69.4702615737915 and batch: 550, loss is 3.8513423681259153 and perplexity is 47.056187580378456
At time: 70.11217403411865 and batch: 600, loss is 3.867105174064636 and perplexity is 47.80380190350276
At time: 70.75235223770142 and batch: 650, loss is 3.720833420753479 and perplexity is 41.2987990459436
At time: 71.38159608840942 and batch: 700, loss is 3.7320128965377806 and perplexity is 41.76308838994786
At time: 72.01127076148987 and batch: 750, loss is 3.827589583396912 and perplexity is 45.9516420339887
At time: 72.64575958251953 and batch: 800, loss is 3.7890150499343873 and perplexity is 44.21283139173774
At time: 73.28250288963318 and batch: 850, loss is 3.8653851985931396 and perplexity is 47.72165120562942
At time: 73.9180166721344 and batch: 900, loss is 3.835691418647766 and perplexity is 46.32544687517195
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.543039400283605 and perplexity of 93.97599715835563
Annealing...
finished 6 epochs...
Completing Train Step...
At time: 75.56692743301392 and batch: 50, loss is 3.9203622055053713 and perplexity is 50.41870340312491
At time: 76.21224975585938 and batch: 100, loss is 3.7766158103942873 and perplexity is 43.66801056662079
At time: 76.85423755645752 and batch: 150, loss is 3.769890398979187 and perplexity is 43.375310597002155
At time: 77.49898838996887 and batch: 200, loss is 3.6597751140594483 and perplexity is 38.85260448429677
At time: 78.13636088371277 and batch: 250, loss is 3.782203907966614 and perplexity is 43.91271474910273
At time: 78.7676215171814 and batch: 300, loss is 3.7705483055114746 and perplexity is 43.403856886549434
At time: 79.40635418891907 and batch: 350, loss is 3.7107855129241942 and perplexity is 40.88591032734404
At time: 80.03921270370483 and batch: 400, loss is 3.6615612506866455 and perplexity is 38.92206255656573
At time: 80.68553471565247 and batch: 450, loss is 3.6624697160720827 and perplexity is 38.95743796936423
At time: 81.32761549949646 and batch: 500, loss is 3.5468205547332765 and perplexity is 34.70280622544715
At time: 81.95580458641052 and batch: 550, loss is 3.6070574426651003 and perplexity is 36.85743796605136
At time: 82.57938408851624 and batch: 600, loss is 3.6093335437774656 and perplexity is 36.94142476657111
At time: 83.2107892036438 and batch: 650, loss is 3.4430872106552126 and perplexity is 31.283387650344487
At time: 83.84773135185242 and batch: 700, loss is 3.4404757165908815 and perplexity is 31.20179785119107
At time: 84.47618412971497 and batch: 750, loss is 3.513569169044495 and perplexity is 33.56786360780695
At time: 85.12325692176819 and batch: 800, loss is 3.450435175895691 and perplexity is 31.514103503359827
At time: 85.76305294036865 and batch: 850, loss is 3.4986725759506228 and perplexity is 33.0715228740685
At time: 86.39935541152954 and batch: 900, loss is 3.454452805519104 and perplexity is 31.640970180159815
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.462965037724743 and perplexity of 86.7443283747925
finished 7 epochs...
Completing Train Step...
At time: 88.04658126831055 and batch: 50, loss is 3.7851328372955324 and perplexity is 44.04152052667226
At time: 88.67970061302185 and batch: 100, loss is 3.6467463207244872 and perplexity is 38.34968525943692
At time: 89.31401205062866 and batch: 150, loss is 3.647915620803833 and perplexity is 38.39455377672182
At time: 89.94289588928223 and batch: 200, loss is 3.5420665407180785 and perplexity is 34.538220130564824
At time: 90.56830716133118 and batch: 250, loss is 3.67050865650177 and perplexity is 39.271876676139286
At time: 91.20352077484131 and batch: 300, loss is 3.667269434928894 and perplexity is 39.14487217491285
At time: 91.83707284927368 and batch: 350, loss is 3.6129660415649414 and perplexity is 37.07585842746834
At time: 92.46676206588745 and batch: 400, loss is 3.569686551094055 and perplexity is 35.50546225893641
At time: 93.10193943977356 and batch: 450, loss is 3.577201833724976 and perplexity is 35.773301024149006
At time: 93.72872853279114 and batch: 500, loss is 3.4675165033340454 and perplexity is 32.05702997583295
At time: 94.36097836494446 and batch: 550, loss is 3.531915488243103 and perplexity is 34.18939431556181
At time: 94.99118995666504 and batch: 600, loss is 3.543597502708435 and perplexity is 34.59113732957364
At time: 95.611248254776 and batch: 650, loss is 3.3830333375930786 and perplexity is 29.459997836416267
At time: 96.24157094955444 and batch: 700, loss is 3.3860595417022705 and perplexity is 29.549284835126624
At time: 96.87623882293701 and batch: 750, loss is 3.469807715415955 and perplexity is 32.13056363866321
At time: 97.51419949531555 and batch: 800, loss is 3.4157501697540282 and perplexity is 30.43977586381612
At time: 98.14362263679504 and batch: 850, loss is 3.47358446598053 and perplexity is 32.25214220446939
At time: 98.76533770561218 and batch: 900, loss is 3.4420679426193237 and perplexity is 31.251517738008804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.468289728034033 and perplexity of 87.20744694675477
Annealing...
finished 8 epochs...
Completing Train Step...
At time: 100.37268710136414 and batch: 50, loss is 3.7344687604904174 and perplexity is 41.86577889862196
At time: 101.01000142097473 and batch: 100, loss is 3.6042709064483645 and perplexity is 36.75487634250615
At time: 101.64075231552124 and batch: 150, loss is 3.6064859294891356 and perplexity is 36.83637947279782
At time: 102.26172304153442 and batch: 200, loss is 3.4958651876449585 and perplexity is 32.97880847106021
At time: 102.88196325302124 and batch: 250, loss is 3.6247318744659425 and perplexity is 37.514663171893524
At time: 103.5109338760376 and batch: 300, loss is 3.618336548805237 and perplexity is 37.27551022944861
At time: 104.13540530204773 and batch: 350, loss is 3.5591032981872557 and perplexity is 35.13168037516889
At time: 104.75580954551697 and batch: 400, loss is 3.513463954925537 and perplexity is 33.56433198040395
At time: 105.37656021118164 and batch: 450, loss is 3.5167170476913454 and perplexity is 33.6736976578578
At time: 105.99579787254333 and batch: 500, loss is 3.400495367050171 and perplexity is 29.97894695227865
At time: 106.6168942451477 and batch: 550, loss is 3.4586776876449585 and perplexity is 31.774932337416022
At time: 107.25112271308899 and batch: 600, loss is 3.466129059791565 and perplexity is 32.0125834972217
At time: 107.88189840316772 and batch: 650, loss is 3.2996090269088745 and perplexity is 27.102040680358897
At time: 108.51029586791992 and batch: 700, loss is 3.2952128505706786 and perplexity is 26.983156839034187
At time: 109.15281820297241 and batch: 750, loss is 3.3720836639404297 and perplexity is 29.139180104406712
At time: 109.79224824905396 and batch: 800, loss is 3.3134665870666504 and perplexity is 27.480223130605427
At time: 110.42484521865845 and batch: 850, loss is 3.361877579689026 and perplexity is 28.843295656421734
At time: 111.05053472518921 and batch: 900, loss is 3.3271372938156127 and perplexity is 27.85847680906599
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.45534316807577 and perplexity of 86.08568763658417
finished 9 epochs...
Completing Train Step...
At time: 112.69510984420776 and batch: 50, loss is 3.7017444229125975 and perplexity is 40.517923141147975
At time: 113.34714317321777 and batch: 100, loss is 3.569224214553833 and perplexity is 35.489050580508284
At time: 113.97987127304077 and batch: 150, loss is 3.5716374921798706 and perplexity is 35.574798937923866
At time: 114.60782599449158 and batch: 200, loss is 3.462759590148926 and perplexity is 31.90489958965499
At time: 115.24495339393616 and batch: 250, loss is 3.5914544820785523 and perplexity is 36.28681608605517
At time: 115.87992024421692 and batch: 300, loss is 3.5873832511901855 and perplexity is 36.13938439742646
At time: 116.50780534744263 and batch: 350, loss is 3.530163826942444 and perplexity is 34.12955849798889
At time: 117.17765760421753 and batch: 400, loss is 3.4863220167160036 and perplexity is 32.66558302394978
At time: 117.82492280006409 and batch: 450, loss is 3.4926741456985475 and perplexity is 32.873739438888485
At time: 118.46757650375366 and batch: 500, loss is 3.378437991142273 and perplexity is 29.324929520459868
At time: 119.11074829101562 and batch: 550, loss is 3.4389385271072386 and perplexity is 31.153871620949307
At time: 119.75528144836426 and batch: 600, loss is 3.449816584587097 and perplexity is 31.494615181109342
At time: 120.39972233772278 and batch: 650, loss is 3.2863626146316527 and perplexity is 26.745403174511356
At time: 121.04179835319519 and batch: 700, loss is 3.285060477256775 and perplexity is 26.710599649834414
At time: 121.6822292804718 and batch: 750, loss is 3.3657235050201417 and perplexity is 28.95443840418014
At time: 122.32482433319092 and batch: 800, loss is 3.3109845304489136 and perplexity is 27.41210023833819
At time: 122.95660305023193 and batch: 850, loss is 3.3636020755767824 and perplexity is 28.893078714172336
At time: 123.58045959472656 and batch: 900, loss is 3.332849311828613 and perplexity is 28.01806026831515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.456716877140411 and perplexity of 86.20402558837552
Annealing...
finished 10 epochs...
Completing Train Step...
At time: 125.21536326408386 and batch: 50, loss is 3.6866850090026855 and perplexity is 39.91231843692522
At time: 125.84041905403137 and batch: 100, loss is 3.555973143577576 and perplexity is 35.02188471224391
At time: 126.47143459320068 and batch: 150, loss is 3.5592453289031982 and perplexity is 35.136670507252475
At time: 127.1107873916626 and batch: 200, loss is 3.4498363065719606 and perplexity is 31.495236323558284
At time: 127.74584889411926 and batch: 250, loss is 3.5780160522460935 and perplexity is 35.802440169610534
At time: 128.37581872940063 and batch: 300, loss is 3.5728870105743407 and perplexity is 35.619278086546466
At time: 129.01135969161987 and batch: 350, loss is 3.5146082258224487 and perplexity is 33.60276065087495
At time: 129.6500961780548 and batch: 400, loss is 3.4718353939056397 and perplexity is 32.19578018816737
At time: 130.28607940673828 and batch: 450, loss is 3.4756624507904053 and perplexity is 32.31923134702476
At time: 130.91400122642517 and batch: 500, loss is 3.359566578865051 and perplexity is 28.776715739147516
At time: 131.5378062725067 and batch: 550, loss is 3.4184397315979003 and perplexity is 30.521755719042318
At time: 132.16374826431274 and batch: 600, loss is 3.4290929555892946 and perplexity is 30.848648961173573
At time: 132.78848123550415 and batch: 650, loss is 3.264407687187195 and perplexity is 26.164608767178507
At time: 133.43074345588684 and batch: 700, loss is 3.2600814723968505 and perplexity is 26.051659547203712
At time: 134.06529092788696 and batch: 750, loss is 3.3386231422424317 and perplexity is 28.180299717645934
At time: 134.69676780700684 and batch: 800, loss is 3.282250761985779 and perplexity is 26.63565580483736
At time: 135.32701015472412 and batch: 850, loss is 3.331360993385315 and perplexity is 27.97639148837091
At time: 135.96020102500916 and batch: 900, loss is 3.3006636333465575 and perplexity is 27.130637743607792
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455643745317851 and perplexity of 86.11156692432425
Annealing...
finished 11 epochs...
Completing Train Step...
At time: 137.59336519241333 and batch: 50, loss is 3.6803602743148804 and perplexity is 39.66068022334664
At time: 138.2299928665161 and batch: 100, loss is 3.549815230369568 and perplexity is 34.80688563801926
At time: 138.86169123649597 and batch: 150, loss is 3.5537225437164306 and perplexity is 34.94315309329144
At time: 139.48894572257996 and batch: 200, loss is 3.4439470291137697 and perplexity is 31.310297251520524
At time: 140.12973618507385 and batch: 250, loss is 3.572805242538452 and perplexity is 35.61636568720978
At time: 140.759783744812 and batch: 300, loss is 3.567459850311279 and perplexity is 35.42649017456258
At time: 141.3843433856964 and batch: 350, loss is 3.508789749145508 and perplexity is 33.407811475357185
At time: 142.01631355285645 and batch: 400, loss is 3.466159315109253 and perplexity is 32.01355206275748
At time: 142.66359210014343 and batch: 450, loss is 3.470301923751831 and perplexity is 32.146446755510944
At time: 143.29714226722717 and batch: 500, loss is 3.3540877532958984 and perplexity is 28.619484248423678
At time: 143.93135738372803 and batch: 550, loss is 3.4126356983184816 and perplexity is 30.345119530120886
At time: 144.56051063537598 and batch: 600, loss is 3.4237449073791506 and perplexity is 30.684109274574556
At time: 145.20244884490967 and batch: 650, loss is 3.2586868047714233 and perplexity is 26.015351465782654
At time: 145.84045314788818 and batch: 700, loss is 3.254058322906494 and perplexity is 25.895218115297
At time: 146.47582459449768 and batch: 750, loss is 3.3323951387405395 and perplexity is 28.005338108611223
At time: 147.11472940444946 and batch: 800, loss is 3.275474681854248 and perplexity is 26.455780579469277
At time: 147.7470896244049 and batch: 850, loss is 3.3240369081497194 and perplexity is 27.772238541916856
At time: 148.38434410095215 and batch: 900, loss is 3.2930758666992186 and perplexity is 26.92555583619566
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455307215860445 and perplexity of 86.08259272104074
finished 12 epochs...
Completing Train Step...
At time: 150.02853775024414 and batch: 50, loss is 3.678749094009399 and perplexity is 39.596831166452446
At time: 150.65474581718445 and batch: 100, loss is 3.548032512664795 and perplexity is 34.744890063467466
At time: 151.28385829925537 and batch: 150, loss is 3.5517616653442383 and perplexity is 34.87470095522659
At time: 151.9198009967804 and batch: 200, loss is 3.442098922729492 and perplexity is 31.2524859284685
At time: 152.5534291267395 and batch: 250, loss is 3.570930490493774 and perplexity is 35.5496563840535
At time: 153.17835474014282 and batch: 300, loss is 3.565723705291748 and perplexity is 35.36503801045662
At time: 153.81621193885803 and batch: 350, loss is 3.50709753036499 and perplexity is 33.35132595577047
At time: 154.4583203792572 and batch: 400, loss is 3.464548830986023 and perplexity is 31.96203623927465
At time: 155.10090017318726 and batch: 450, loss is 3.468951563835144 and perplexity is 32.10306677823148
At time: 155.73572373390198 and batch: 500, loss is 3.3528460931777953 and perplexity is 28.58397062871702
At time: 156.3600356578827 and batch: 550, loss is 3.4115408182144167 and perplexity is 30.311913444150502
At time: 156.98251867294312 and batch: 600, loss is 3.42285364151001 and perplexity is 30.656773758668827
At time: 157.61484479904175 and batch: 650, loss is 3.258015694618225 and perplexity is 25.997898156476555
At time: 158.24515509605408 and batch: 700, loss is 3.2536682987213137 and perplexity is 25.88512032327107
At time: 158.87581753730774 and batch: 750, loss is 3.3323186111450194 and perplexity is 28.003195009428204
At time: 159.5057029724121 and batch: 800, loss is 3.275811929702759 and perplexity is 26.464704239208316
At time: 160.14575290679932 and batch: 850, loss is 3.324669051170349 and perplexity is 27.789800118807708
At time: 160.78420281410217 and batch: 900, loss is 3.2940070056915283 and perplexity is 26.95063894724343
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455137487960188 and perplexity of 86.0679833431741
finished 13 epochs...
Completing Train Step...
At time: 162.45622992515564 and batch: 50, loss is 3.677353901863098 and perplexity is 39.54162449949994
At time: 163.09764766693115 and batch: 100, loss is 3.546451654434204 and perplexity is 34.690006710869454
At time: 163.73442840576172 and batch: 150, loss is 3.5500732135772703 and perplexity is 34.815866388417874
At time: 164.37728667259216 and batch: 200, loss is 3.4404625129699706 and perplexity is 31.201385877200284
At time: 165.00866627693176 and batch: 250, loss is 3.569294366836548 and perplexity is 35.491540305746824
At time: 165.6585614681244 and batch: 300, loss is 3.564177966117859 and perplexity is 35.31041511306985
At time: 166.29801106452942 and batch: 350, loss is 3.505597553253174 and perplexity is 33.30133723053254
At time: 166.93209624290466 and batch: 400, loss is 3.4631191873550415 and perplexity is 31.9163745654698
At time: 167.56825232505798 and batch: 450, loss is 3.4677347707748414 and perplexity is 32.064027745390135
At time: 168.2129762172699 and batch: 500, loss is 3.351762237548828 and perplexity is 28.553006514598785
At time: 168.84797763824463 and batch: 550, loss is 3.4105854272842406 and perplexity is 30.282967546495474
At time: 169.4818470478058 and batch: 600, loss is 3.422083897590637 and perplexity is 30.63318497330726
At time: 170.11635279655457 and batch: 650, loss is 3.25743757724762 and perplexity is 25.982872663620768
At time: 170.75082778930664 and batch: 700, loss is 3.2533368062973023 and perplexity is 25.87654102405419
At time: 171.38713455200195 and batch: 750, loss is 3.3322424507141113 and perplexity is 28.001062355242443
At time: 172.0178964138031 and batch: 800, loss is 3.2760697269439696 and perplexity is 26.471527646440634
At time: 172.64679741859436 and batch: 850, loss is 3.3252054405212403 and perplexity is 27.804710270120545
At time: 173.27828001976013 and batch: 900, loss is 3.2947939682006835 and perplexity is 26.97185643728227
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455050533764983 and perplexity of 86.06049969632153
finished 14 epochs...
Completing Train Step...
At time: 174.8951554298401 and batch: 50, loss is 3.676053409576416 and perplexity is 39.49023434532794
At time: 175.54277658462524 and batch: 100, loss is 3.5450013399124147 and perplexity is 34.639731756436376
At time: 176.17222476005554 and batch: 150, loss is 3.548525195121765 and perplexity is 34.76201247888158
At time: 176.8034155368805 and batch: 200, loss is 3.438957986831665 and perplexity is 31.154477872604595
At time: 177.43713808059692 and batch: 250, loss is 3.5677862977981567 and perplexity is 35.43805695111881
At time: 178.06993746757507 and batch: 300, loss is 3.562750163078308 and perplexity is 35.26003477020581
At time: 178.70072197914124 and batch: 350, loss is 3.5042178297042845 and perplexity is 33.25542227360352
At time: 179.3222312927246 and batch: 400, loss is 3.461812710762024 and perplexity is 31.874703796035725
At time: 179.94367909431458 and batch: 450, loss is 3.4666047048568727 and perplexity is 32.0278137463992
At time: 180.5650725364685 and batch: 500, loss is 3.3507724046707152 and perplexity is 28.524757793044778
At time: 181.18611764907837 and batch: 550, loss is 3.4097093677520753 and perplexity is 30.256449481511922
At time: 181.82064843177795 and batch: 600, loss is 3.4213840198516845 and perplexity is 30.611752989830126
At time: 182.44236588478088 and batch: 650, loss is 3.2569080543518067 and perplexity is 25.969117779730755
At time: 183.06818914413452 and batch: 700, loss is 3.253026342391968 and perplexity is 25.868508539034174
At time: 183.6996772289276 and batch: 750, loss is 3.332153797149658 and perplexity is 27.998580071289464
At time: 184.32562518119812 and batch: 800, loss is 3.2762669038772585 and perplexity is 26.476747735705118
At time: 184.96266293525696 and batch: 850, loss is 3.325662155151367 and perplexity is 27.81741198839684
At time: 185.59229445457458 and batch: 900, loss is 3.295468277931213 and perplexity is 26.990049955872315
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.4550191800888275 and perplexity of 86.05780142558483
finished 15 epochs...
Completing Train Step...
At time: 187.21960854530334 and batch: 50, loss is 3.6748200941085813 and perplexity is 39.44156044978525
At time: 187.85417485237122 and batch: 100, loss is 3.543642673492432 and perplexity is 34.592699873656564
At time: 188.49971532821655 and batch: 150, loss is 3.5470757246017457 and perplexity is 34.711662465822
At time: 189.13443732261658 and batch: 200, loss is 3.437549214363098 and perplexity is 31.11061920260213
At time: 189.7647144794464 and batch: 250, loss is 3.5663706159591673 and perplexity is 35.38792343241296
At time: 190.39425230026245 and batch: 300, loss is 3.561409821510315 and perplexity is 35.2128059383683
At time: 191.02918362617493 and batch: 350, loss is 3.5029268932342528 and perplexity is 33.21251933460673
At time: 191.66454529762268 and batch: 400, loss is 3.4605968141555787 and perplexity is 31.835971004165668
At time: 192.29773592948914 and batch: 450, loss is 3.4655406284332275 and perplexity is 31.99375183034557
At time: 192.9341094493866 and batch: 500, loss is 3.349846715927124 and perplexity is 28.498364963499373
At time: 193.56948971748352 and batch: 550, loss is 3.4088870763778685 and perplexity is 30.231580090433347
At time: 194.2011013031006 and batch: 600, loss is 3.42072922706604 and perplexity is 30.591715195833864
At time: 194.82527351379395 and batch: 650, loss is 3.2564089584350584 and perplexity is 25.956159932958524
At time: 195.45130014419556 and batch: 700, loss is 3.2527239751815795 and perplexity is 25.860687932677365
At time: 196.08578658103943 and batch: 750, loss is 3.332050738334656 and perplexity is 27.99569471948859
At time: 196.7177791595459 and batch: 800, loss is 3.276416425704956 and perplexity is 26.480706883400167
At time: 197.34670495986938 and batch: 850, loss is 3.326052203178406 and perplexity is 27.828264231370632
At time: 198.0017614364624 and batch: 900, loss is 3.296051959991455 and perplexity is 27.0058081622804
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455022942529966 and perplexity of 86.05812521360629
Annealing...
finished 16 epochs...
Completing Train Step...
At time: 199.61626815795898 and batch: 50, loss is 3.6738250923156737 and perplexity is 39.402335544084536
At time: 200.25572514533997 and batch: 100, loss is 3.5426664400100707 and perplexity is 34.55894580040359
At time: 200.88552045822144 and batch: 150, loss is 3.546192178726196 and perplexity is 34.68100666451455
At time: 201.52057528495789 and batch: 200, loss is 3.4365436267852782 and perplexity is 31.07935047475079
At time: 202.14815878868103 and batch: 250, loss is 3.565478830337524 and perplexity is 35.356379058616184
At time: 202.78340458869934 and batch: 300, loss is 3.5604657125473023 and perplexity is 35.17957690105324
At time: 203.43043994903564 and batch: 350, loss is 3.5018742990493776 and perplexity is 33.177578422416644
At time: 204.07094645500183 and batch: 400, loss is 3.459491081237793 and perplexity is 31.800788377935316
At time: 204.69695949554443 and batch: 450, loss is 3.4644186115264892 and perplexity is 31.95787443117006
At time: 205.33597135543823 and batch: 500, loss is 3.34867148399353 and perplexity is 28.46489244777634
At time: 205.9725694656372 and batch: 550, loss is 3.407618021965027 and perplexity is 30.193238903984042
At time: 206.6191017627716 and batch: 600, loss is 3.4195469665527343 and perplexity is 30.555569190131393
At time: 207.2540807723999 and batch: 650, loss is 3.255128378868103 and perplexity is 25.922942278376038
At time: 207.88573694229126 and batch: 700, loss is 3.251241192817688 and perplexity is 25.82237057586366
At time: 208.5214765071869 and batch: 750, loss is 3.3305618143081666 and perplexity is 27.954042273341326
At time: 209.15025162696838 and batch: 800, loss is 3.2747310304641726 and perplexity is 26.436114014906188
At time: 209.78981924057007 and batch: 850, loss is 3.324279131889343 and perplexity is 27.7789664521911
At time: 210.41670489311218 and batch: 900, loss is 3.2941955089569093 and perplexity is 26.955719709544187
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.455009146912457 and perplexity of 86.05693799681653
finished 17 epochs...
Completing Train Step...
At time: 212.02698802947998 and batch: 50, loss is 3.673543343544006 and perplexity is 39.39123554822269
At time: 212.68456077575684 and batch: 100, loss is 3.5423664808273316 and perplexity is 34.548581081838954
At time: 213.3227093219757 and batch: 150, loss is 3.5458653211593627 and perplexity is 34.66967276744677
At time: 213.9703459739685 and batch: 200, loss is 3.4362386417388917 and perplexity is 31.069873182894195
At time: 214.59989380836487 and batch: 250, loss is 3.565154166221619 and perplexity is 35.34490197426704
At time: 215.23879027366638 and batch: 300, loss is 3.5601630449295043 and perplexity is 35.16893079351473
At time: 215.8719310760498 and batch: 350, loss is 3.50158522605896 and perplexity is 33.167989066685905
At time: 216.51068878173828 and batch: 400, loss is 3.459230589866638 and perplexity is 31.79250562580651
At time: 217.14155507087708 and batch: 450, loss is 3.464190459251404 and perplexity is 31.950584001107426
At time: 217.77086186408997 and batch: 500, loss is 3.3484747552871705 and perplexity is 28.459293137099056
At time: 218.40076994895935 and batch: 550, loss is 3.407439360618591 and perplexity is 30.18784502112173
At time: 219.0278673171997 and batch: 600, loss is 3.419403715133667 and perplexity is 30.55119237498447
At time: 219.66630816459656 and batch: 650, loss is 3.255022530555725 and perplexity is 25.920198523897447
At time: 220.29523539543152 and batch: 700, loss is 3.251179175376892 and perplexity is 25.820769188182762
At time: 220.92580580711365 and batch: 750, loss is 3.330544352531433 and perplexity is 27.95355415035811
At time: 221.55570912361145 and batch: 800, loss is 3.2747791385650635 and perplexity is 26.437385836738603
At time: 222.1910948753357 and batch: 850, loss is 3.3243718910217286 and perplexity is 27.78154332453014
At time: 222.83008670806885 and batch: 900, loss is 3.294331250190735 and perplexity is 26.95937896054622
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454998695687072 and perplexity of 86.0560386010615
finished 18 epochs...
Completing Train Step...
At time: 224.48930096626282 and batch: 50, loss is 3.673269257545471 and perplexity is 39.380440441555294
At time: 225.13524556159973 and batch: 100, loss is 3.5420693874359133 and perplexity is 34.538318451272
At time: 225.77054381370544 and batch: 150, loss is 3.5455439615249635 and perplexity is 34.65853312409321
At time: 226.41137146949768 and batch: 200, loss is 3.435936155319214 and perplexity is 31.060476389468104
At time: 227.04524683952332 and batch: 250, loss is 3.564840440750122 and perplexity is 35.333815117435776
At time: 227.68373131752014 and batch: 300, loss is 3.559867887496948 and perplexity is 35.158551953967134
At time: 228.32079529762268 and batch: 350, loss is 3.5013058090209963 and perplexity is 33.158722660081914
At time: 228.95259141921997 and batch: 400, loss is 3.458973178863525 and perplexity is 31.784322938243992
At time: 229.5873806476593 and batch: 450, loss is 3.4639648246765136 and perplexity is 31.943375657925145
At time: 230.24082159996033 and batch: 500, loss is 3.3482789182662964 and perplexity is 28.453720299616663
At time: 230.8681981563568 and batch: 550, loss is 3.407263660430908 and perplexity is 30.182541477016265
At time: 231.49840235710144 and batch: 600, loss is 3.4192634534835817 and perplexity is 30.546907514837688
At time: 232.13044714927673 and batch: 650, loss is 3.254918351173401 and perplexity is 25.91749831428104
At time: 232.76559162139893 and batch: 700, loss is 3.2511174154281615 and perplexity is 25.819174548044465
At time: 233.4015200138092 and batch: 750, loss is 3.330527310371399 and perplexity is 27.953077765474077
At time: 234.03908348083496 and batch: 800, loss is 3.274823794364929 and perplexity is 26.438566445709817
At time: 234.67235445976257 and batch: 850, loss is 3.324461612701416 and perplexity is 27.784036043085308
At time: 235.30990171432495 and batch: 900, loss is 3.2944605016708373 and perplexity is 26.962863725380167
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454989916657748 and perplexity of 86.05528311589134
finished 19 epochs...
Completing Train Step...
At time: 236.96684432029724 and batch: 50, loss is 3.67300076007843 and perplexity is 39.36986831240424
At time: 237.61025166511536 and batch: 100, loss is 3.541776099205017 and perplexity is 34.52819025426839
At time: 238.2422423362732 and batch: 150, loss is 3.5452284097671507 and perplexity is 34.64759828838674
At time: 238.87745666503906 and batch: 200, loss is 3.4356369495391847 and perplexity is 31.051184305593843
At time: 239.52026963233948 and batch: 250, loss is 3.5645342111587524 and perplexity is 35.3229965142436
At time: 240.16046237945557 and batch: 300, loss is 3.5595784759521485 and perplexity is 35.148378135414504
At time: 240.7952253818512 and batch: 350, loss is 3.5010322523117066 and perplexity is 33.14965310960152
At time: 241.42795658111572 and batch: 400, loss is 3.4587189197540282 and perplexity is 31.776242511902286
At time: 242.06721878051758 and batch: 450, loss is 3.4637416219711303 and perplexity is 31.936246605700692
At time: 242.7051739692688 and batch: 500, loss is 3.3480846643447877 and perplexity is 28.448193589676865
At time: 243.34226512908936 and batch: 550, loss is 3.407090258598328 and perplexity is 30.177308222753275
At time: 243.98014426231384 and batch: 600, loss is 3.4191252994537353 and perplexity is 30.542687627969066
At time: 244.6066963672638 and batch: 650, loss is 3.2548152732849123 and perplexity is 25.914826930962537
At time: 245.23638701438904 and batch: 700, loss is 3.2510555934906007 and perplexity is 25.81757840598649
At time: 245.86588859558105 and batch: 750, loss is 3.3305093097686767 and perplexity is 27.952574597755035
At time: 246.5165994167328 and batch: 800, loss is 3.274864773750305 and perplexity is 26.439649904112567
At time: 247.1475167274475 and batch: 850, loss is 3.324547600746155 and perplexity is 27.786425240739348
At time: 247.78027200698853 and batch: 900, loss is 3.2945847654342653 and perplexity is 26.9662144404814
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454986154216609 and perplexity of 86.05495933856301
finished 20 epochs...
Completing Train Step...
At time: 249.41635990142822 and batch: 50, loss is 3.6727363300323486 and perplexity is 39.359459112625416
At time: 250.05589866638184 and batch: 100, loss is 3.541486802101135 and perplexity is 34.51820279356693
At time: 250.69759035110474 and batch: 150, loss is 3.5449179649353026 and perplexity is 34.63684378998776
At time: 251.33500480651855 and batch: 200, loss is 3.4353412532806398 and perplexity is 31.042003943937477
At time: 251.96822309494019 and batch: 250, loss is 3.564233593940735 and perplexity is 35.31237940922169
At time: 252.5994358062744 and batch: 300, loss is 3.559293909072876 and perplexity is 35.13837749413014
At time: 253.2444896697998 and batch: 350, loss is 3.5007628440856933 and perplexity is 33.140723523270324
At time: 253.8818097114563 and batch: 400, loss is 3.458468108177185 and perplexity is 31.768273661793422
At time: 254.519859790802 and batch: 450, loss is 3.4635208511352538 and perplexity is 31.929196792068048
At time: 255.15843677520752 and batch: 500, loss is 3.347892374992371 and perplexity is 28.442723830859123
At time: 255.80120372772217 and batch: 550, loss is 3.406919074058533 and perplexity is 30.172142776267815
At time: 256.44131112098694 and batch: 600, loss is 3.4189889287948607 and perplexity is 30.53852278552105
At time: 257.07383584976196 and batch: 650, loss is 3.2547129678726194 and perplexity is 25.91217583952168
At time: 257.69964957237244 and batch: 700, loss is 3.250993638038635 and perplexity is 25.815978915796773
At time: 258.32446575164795 and batch: 750, loss is 3.330490131378174 and perplexity is 27.952038517504434
At time: 258.9503126144409 and batch: 800, loss is 3.2749022531509397 and perplexity is 26.440640864914155
At time: 259.57580614089966 and batch: 850, loss is 3.324629855155945 and perplexity is 27.78871089074881
At time: 260.20070791244507 and batch: 900, loss is 3.2947047185897826 and perplexity is 26.96944931700884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454984063971533 and perplexity of 86.05477946279593
finished 21 epochs...
Completing Train Step...
At time: 261.8172519207001 and batch: 50, loss is 3.672475538253784 and perplexity is 39.34919582762844
At time: 262.445583820343 and batch: 100, loss is 3.5412013387680052 and perplexity is 34.508350518642345
At time: 263.08260703086853 and batch: 150, loss is 3.5446125221252442 and perplexity is 34.62626583065204
At time: 263.71214842796326 and batch: 200, loss is 3.435049171447754 and perplexity is 31.032938462524747
At time: 264.3421347141266 and batch: 250, loss is 3.5639375495910643 and perplexity is 35.30192692609979
At time: 264.98032999038696 and batch: 300, loss is 3.5590137100219725 and perplexity is 35.12853313336068
At time: 265.6122441291809 and batch: 350, loss is 3.5004970264434814 and perplexity is 33.13191530502394
At time: 266.25094866752625 and batch: 400, loss is 3.4582204675674437 and perplexity is 31.76040752116152
At time: 266.88852858543396 and batch: 450, loss is 3.463302435874939 and perplexity is 31.92222372977839
At time: 267.52288460731506 and batch: 500, loss is 3.3477020931243895 and perplexity is 28.437312211120993
At time: 268.1462171077728 and batch: 550, loss is 3.4067497634887696 and perplexity is 30.167034746017343
At time: 268.78066062927246 and batch: 600, loss is 3.418854236602783 and perplexity is 30.534409761946556
At time: 269.4126284122467 and batch: 650, loss is 3.2546113681793214 and perplexity is 25.909543304138356
At time: 270.04599356651306 and batch: 700, loss is 3.2509315395355225 and perplexity is 25.814375831924785
At time: 270.67996191978455 and batch: 750, loss is 3.3304697847366334 and perplexity is 27.951469793182227
At time: 271.3102345466614 and batch: 800, loss is 3.2749365758895874 and perplexity is 26.441548395694618
At time: 271.9372503757477 and batch: 850, loss is 3.3247087812423706 and perplexity is 27.79090423150099
At time: 272.5617094039917 and batch: 900, loss is 3.2948208379745485 and perplexity is 26.97258117470218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454984900069563 and perplexity of 86.05485141305766
Annealing...
finished 22 epochs...
Completing Train Step...
At time: 274.174601316452 and batch: 50, loss is 3.6722582817077636 and perplexity is 39.34064788583602
At time: 274.8226182460785 and batch: 100, loss is 3.5409882164001463 and perplexity is 34.500996800917605
At time: 275.44674468040466 and batch: 150, loss is 3.5444208097457888 and perplexity is 34.61962818311792
At time: 276.0705146789551 and batch: 200, loss is 3.434829664230347 and perplexity is 31.0261272561367
At time: 276.694669008255 and batch: 250, loss is 3.5637400579452514 and perplexity is 35.294955778845114
At time: 277.31993198394775 and batch: 300, loss is 3.5588083934783934 and perplexity is 35.12132140472567
At time: 277.9579803943634 and batch: 350, loss is 3.5002599143981934 and perplexity is 33.12406026012134
At time: 278.6010890007019 and batch: 400, loss is 3.457979097366333 and perplexity is 31.75274243031026
At time: 279.22565150260925 and batch: 450, loss is 3.4630549860000612 and perplexity is 31.91432555675191
At time: 279.8504567146301 and batch: 500, loss is 3.3474402904510496 and perplexity is 28.429868221232265
At time: 280.4806730747223 and batch: 550, loss is 3.4064674043655394 and perplexity is 30.15851801098151
At time: 281.1087369918823 and batch: 600, loss is 3.418584794998169 and perplexity is 30.526183629865233
At time: 281.7359068393707 and batch: 650, loss is 3.254321517944336 and perplexity is 25.902034505189427
At time: 282.37082028388977 and batch: 700, loss is 3.250599946975708 and perplexity is 25.805817395997916
At time: 282.99866366386414 and batch: 750, loss is 3.3301321411132814 and perplexity is 27.942033750742667
At time: 283.63285875320435 and batch: 800, loss is 3.2745589923858645 and perplexity is 26.431566387846853
At time: 284.2634563446045 and batch: 850, loss is 3.3243127965927126 and perplexity is 27.77990163859792
At time: 284.9036273956299 and batch: 900, loss is 3.2944056177139283 and perplexity is 26.961383937338002
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454983645922517 and perplexity of 86.05474348768759
finished 23 epochs...
Completing Train Step...
At time: 286.5803973674774 and batch: 50, loss is 3.672199320793152 and perplexity is 39.33832839363562
At time: 287.2207028865814 and batch: 100, loss is 3.5409243297576904 and perplexity is 34.498792718477084
At time: 287.848792552948 and batch: 150, loss is 3.5443520164489746 and perplexity is 34.61724666667784
At time: 288.4826850891113 and batch: 200, loss is 3.4347651195526123 and perplexity is 31.024124749377872
At time: 289.1061131954193 and batch: 250, loss is 3.563673176765442 and perplexity is 35.29259528949834
At time: 289.7280955314636 and batch: 300, loss is 3.558745517730713 and perplexity is 35.119113194804974
At time: 290.3485631942749 and batch: 350, loss is 3.5002010250091553 and perplexity is 33.122109661885496
At time: 290.98300218582153 and batch: 400, loss is 3.4579238080978394 and perplexity is 31.750986892940254
At time: 291.6170291900635 and batch: 450, loss is 3.4630065727233887 and perplexity is 31.91278051707943
At time: 292.25228238105774 and batch: 500, loss is 3.347397971153259 and perplexity is 28.42866511463036
At time: 292.88382267951965 and batch: 550, loss is 3.4064293050765992 and perplexity is 30.15736901477791
At time: 293.5220699310303 and batch: 600, loss is 3.4185547685623168 and perplexity is 30.52526705113152
At time: 294.16122221946716 and batch: 650, loss is 3.254299273490906 and perplexity is 25.901458334997454
At time: 294.8220958709717 and batch: 700, loss is 3.2505853033065795 and perplexity is 25.80543950691323
At time: 295.4562532901764 and batch: 750, loss is 3.3301275777816772 and perplexity is 27.9419062422679
At time: 296.0985429286957 and batch: 800, loss is 3.2745662212371824 and perplexity is 26.431757458400984
At time: 296.7391839027405 and batch: 850, loss is 3.3243296480178834 and perplexity is 27.780369773475993
At time: 297.38019013404846 and batch: 900, loss is 3.29443058013916 and perplexity is 26.96205696726888
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454984063971533 and perplexity of 86.05477946279593
Annealing...
finished 24 epochs...
Completing Train Step...
At time: 299.0269305706024 and batch: 50, loss is 3.6721493864059447 and perplexity is 39.33636410735665
At time: 299.66880321502686 and batch: 100, loss is 3.540875906944275 and perplexity is 34.49712223031941
At time: 300.3072724342346 and batch: 150, loss is 3.5443079376220705 and perplexity is 34.61572081268331
At time: 300.9456262588501 and batch: 200, loss is 3.4347160863876343 and perplexity is 31.022603575645025
At time: 301.58832144737244 and batch: 250, loss is 3.563627634048462 and perplexity is 35.290988005419905
At time: 302.22697734832764 and batch: 300, loss is 3.5586992168426512 and perplexity is 35.117487186319224
At time: 302.8594341278076 and batch: 350, loss is 3.5001477003097534 and perplexity is 33.12034348243513
At time: 303.48752522468567 and batch: 400, loss is 3.4578698539733885 and perplexity is 31.74927384245548
At time: 304.11579847335815 and batch: 450, loss is 3.462951726913452 and perplexity is 31.911030282781585
At time: 304.75865173339844 and batch: 500, loss is 3.3473401403427125 and perplexity is 28.42702110942156
At time: 305.3946828842163 and batch: 550, loss is 3.4063666963577273 and perplexity is 30.15548095964432
At time: 306.0312469005585 and batch: 600, loss is 3.418494553565979 and perplexity is 30.523429027626676
At time: 306.6697266101837 and batch: 650, loss is 3.2542347145080566 and perplexity is 25.899786217168725
At time: 307.303751707077 and batch: 700, loss is 3.250511727333069 and perplexity is 25.803540916425817
At time: 307.94488978385925 and batch: 750, loss is 3.3300524806976317 and perplexity is 27.93980796537466
At time: 308.5809783935547 and batch: 800, loss is 3.274482545852661 and perplexity is 26.4295458634614
At time: 309.2158236503601 and batch: 850, loss is 3.3242418718338014 and perplexity is 27.77793142564089
At time: 309.8542945384979 and batch: 900, loss is 3.2943386840820312 and perplexity is 26.959579374383758
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454984900069563 and perplexity of 86.05485141305766
Annealing...
finished 25 epochs...
Completing Train Step...
At time: 311.5090386867523 and batch: 50, loss is 3.672138476371765 and perplexity is 39.33593494862081
At time: 312.1467387676239 and batch: 100, loss is 3.540865225791931 and perplexity is 34.496753763269275
At time: 312.78027987480164 and batch: 150, loss is 3.544298310279846 and perplexity is 34.615387556896884
At time: 313.4158079624176 and batch: 200, loss is 3.434705152511597 and perplexity is 31.022264380197534
At time: 314.04569816589355 and batch: 250, loss is 3.5636173057556153 and perplexity is 35.29062351164323
At time: 314.67568159103394 and batch: 300, loss is 3.5586889743804933 and perplexity is 35.11712749862768
At time: 315.3163483142853 and batch: 350, loss is 3.500135841369629 and perplexity is 33.11995071259379
At time: 315.96440982818604 and batch: 400, loss is 3.457857913970947 and perplexity is 31.748894758311422
At time: 316.6068916320801 and batch: 450, loss is 3.462939600944519 and perplexity is 31.910643332965833
At time: 317.2455883026123 and batch: 500, loss is 3.3473274755477904 and perplexity is 28.426661089308755
At time: 317.8846778869629 and batch: 550, loss is 3.4063526010513305 and perplexity is 30.155055911896245
At time: 318.5184106826782 and batch: 600, loss is 3.4184812259674073 and perplexity is 30.523022226328408
At time: 319.15532970428467 and batch: 650, loss is 3.254220333099365 and perplexity is 25.899413744436462
At time: 319.78923749923706 and batch: 700, loss is 3.250495195388794 and perplexity is 25.803114337251394
At time: 320.4234309196472 and batch: 750, loss is 3.330035753250122 and perplexity is 27.939340607612344
At time: 321.0524787902832 and batch: 800, loss is 3.2744638776779174 and perplexity is 26.42905247668615
At time: 321.6838080883026 and batch: 850, loss is 3.3242222356796263 and perplexity is 27.777385979252003
At time: 322.3091199398041 and batch: 900, loss is 3.294318299293518 and perplexity is 26.959029814661154
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454985318118578 and perplexity of 86.05488738821104
Annealing...
finished 26 epochs...
Completing Train Step...
At time: 323.9249494075775 and batch: 50, loss is 3.6721365308761595 and perplexity is 39.33585842080667
At time: 324.5809733867645 and batch: 100, loss is 3.540863151550293 and perplexity is 34.49668220874045
At time: 325.2064199447632 and batch: 150, loss is 3.544296593666077 and perplexity is 34.61532813569698
At time: 325.83793210983276 and batch: 200, loss is 3.4347030305862427 and perplexity is 31.022198553338043
At time: 326.46639800071716 and batch: 250, loss is 3.563615403175354 and perplexity is 35.290556368463406
At time: 327.09144830703735 and batch: 300, loss is 3.5586870241165163 and perplexity is 35.11705901102573
At time: 327.7347984313965 and batch: 350, loss is 3.5001336240768435 and perplexity is 33.11987727604744
At time: 328.36304092407227 and batch: 400, loss is 3.4578554677963256 and perplexity is 31.748817095065792
At time: 328.99735498428345 and batch: 450, loss is 3.4629370260238646 and perplexity is 31.910561165697004
At time: 329.62369108200073 and batch: 500, loss is 3.347324833869934 and perplexity is 28.42658599532681
At time: 330.2587876319885 and batch: 550, loss is 3.40634973526001 and perplexity is 30.154969493922568
At time: 330.8960008621216 and batch: 600, loss is 3.4184783411026003 and perplexity is 30.522934171662797
At time: 331.53759241104126 and batch: 650, loss is 3.2542172479629516 and perplexity is 25.899333841335288
At time: 332.17659521102905 and batch: 700, loss is 3.250491533279419 and perplexity is 25.803019843597504
At time: 332.80985856056213 and batch: 750, loss is 3.330032172203064 and perplexity is 27.939240555698007
At time: 333.442745923996 and batch: 800, loss is 3.2744595575332642 and perplexity is 26.428938299603036
At time: 334.07536220550537 and batch: 850, loss is 3.3242177867889406 and perplexity is 27.777262400973143
At time: 334.7035665512085 and batch: 900, loss is 3.2943136930465697 and perplexity is 26.95890563499834
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454985318118578 and perplexity of 86.05488738821104
Annealing...
finished 27 epochs...
Completing Train Step...
At time: 336.30473136901855 and batch: 50, loss is 3.6721363162994383 and perplexity is 39.33584998024805
At time: 336.9591088294983 and batch: 100, loss is 3.540862889289856 and perplexity is 34.49667316162669
At time: 337.60037446022034 and batch: 150, loss is 3.544296479225159 and perplexity is 34.61532417428727
At time: 338.2363667488098 and batch: 200, loss is 3.4347028732299805 and perplexity is 31.022193671801215
At time: 338.87224102020264 and batch: 250, loss is 3.563615155220032 and perplexity is 35.29054761798322
At time: 339.51153230667114 and batch: 300, loss is 3.5586868810653685 and perplexity is 35.11705398749049
At time: 340.1443703174591 and batch: 350, loss is 3.5001334238052366 and perplexity is 33.11987064307706
At time: 340.77604365348816 and batch: 400, loss is 3.457855200767517 and perplexity is 31.748808617218128
At time: 341.41356587409973 and batch: 450, loss is 3.4629367542266847 and perplexity is 31.91055249249765
At time: 342.051917552948 and batch: 500, loss is 3.3473244285583497 and perplexity is 28.42657447370454
At time: 342.6897165775299 and batch: 550, loss is 3.406349287033081 and perplexity is 30.15495597765623
At time: 343.3437147140503 and batch: 600, loss is 3.418478026390076 and perplexity is 30.52292456571464
At time: 343.98434352874756 and batch: 650, loss is 3.2542169094085693 and perplexity is 25.8993250730038
At time: 344.6209924221039 and batch: 700, loss is 3.250491099357605 and perplexity is 25.803008647106754
At time: 345.2533047199249 and batch: 750, loss is 3.330031633377075 and perplexity is 27.939225501313146
At time: 345.88319206237793 and batch: 800, loss is 3.274459095001221 and perplexity is 26.428926075375028
At time: 346.51426458358765 and batch: 850, loss is 3.324217190742493 and perplexity is 27.777245844439495
At time: 347.1563720703125 and batch: 900, loss is 3.2943130111694336 and perplexity is 26.958887252343242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 4.454985318118578 and perplexity of 86.05488738821104
Annealing...
Model not improving. Stopping early with 86.05474348768759 lossat 27 epochs.
Finished Training.
langmodel
SETTINGS FOR THIS RUN
{'rnn_dropout': 0.3967156699045209, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.311110015148611, 'wordvec_source': 'None', 'dropout': 0.29446936213168917, 'lr': 19.677092011683996, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.4376707521124406, 'tune_wordvecs': True}
AVAILABLE TIME:None
Preparing Data Loaders

[93m    Warning: no model found for 'en'[0m

    Only loading the 'en' tokenizer.

Retrieving Train Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/train.txt...
Downloaded in 0.08049900929133097 minutes
Got Train Dataset with 1042946 words
Retrieving Valid Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/valid.txt...
Retrieving Test Data from file: /home-nfs/siddsach/ml/Interpreting-Attention/data/penn/test.txt...
Building Vocab...
Loading Vectors From Memory...
Found 9600 tokens
Getting Batches...
Created Iterator with 932 batches
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Initializing Model parameters...
Constructing LSTM with 1 layers and 300 hidden size...
Using Cross Entropy Loss ...
Begin Training...
finished 0 epochs...
Completing Train Step...
At time: 0.8946356773376465 and batch: 50, loss is 6.379149484634399 and perplexity is 589.4261783879199
At time: 1.5214512348175049 and batch: 100, loss is 5.778053369522095 and perplexity is 323.12956393354995
At time: 2.146465539932251 and batch: 150, loss is 5.759424381256103 and perplexity is 317.16570983590674
At time: 2.7692723274230957 and batch: 200, loss is 5.651718320846558 and perplexity is 284.78038971990526
At time: 3.4020473957061768 and batch: 250, loss is 5.741762771606445 and perplexity is 311.6132300720864
At time: 4.038011074066162 and batch: 300, loss is 5.698050632476806 and perplexity is 298.2853660461725
At time: 4.677754640579224 and batch: 350, loss is 5.717830429077148 and perplexity is 304.244127182675
At time: 5.311655759811401 and batch: 400, loss is 5.5902303695678714 and perplexity is 267.7973049575931
At time: 5.951633453369141 and batch: 450, loss is 5.600243406295776 and perplexity is 270.49223892784033
At time: 6.578338623046875 and batch: 500, loss is 5.551948547363281 and perplexity is 257.7392841289267
At time: 7.242146253585815 and batch: 550, loss is 5.620492486953736 and perplexity is 276.02528861976725
At time: 7.871548414230347 and batch: 600, loss is 5.582707061767578 and perplexity is 265.79014311959537
At time: 8.502991676330566 and batch: 650, loss is 5.490276584625244 and perplexity is 242.32422074380358
At time: 9.133293628692627 and batch: 700, loss is 5.577953214645386 and perplexity is 264.5296159632102
At time: 9.76346468925476 and batch: 750, loss is 5.566400632858277 and perplexity is 261.49120050394293
At time: 10.404986143112183 and batch: 800, loss is 5.549402208328247 and perplexity is 257.08382739058317
At time: 11.037714958190918 and batch: 850, loss is 5.586843767166138 and perplexity is 266.89191592311636
At time: 11.665393829345703 and batch: 900, loss is 5.502629833221436 and perplexity is 245.3362781264242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.455778775149828 and perplexity of 234.10711690103736
finished 1 epochs...
Completing Train Step...
At time: 13.274514436721802 and batch: 50, loss is 5.472219095230103 and perplexity is 237.987724647528
At time: 13.925017356872559 and batch: 100, loss is 5.467448053359985 and perplexity is 236.85497958442977
At time: 14.546316385269165 and batch: 150, loss is 5.483622961044311 and perplexity is 240.7172386499235
At time: 15.174326419830322 and batch: 200, loss is 5.374265213012695 and perplexity is 215.7812607823798
At time: 15.797512769699097 and batch: 250, loss is 5.505535707473755 and perplexity is 246.05023132691176
At time: 16.442641258239746 and batch: 300, loss is 5.463195524215698 and perplexity is 235.84988549141588
At time: 17.06344485282898 and batch: 350, loss is 5.450978622436524 and perplexity is 232.986059763936
At time: 17.681193351745605 and batch: 400, loss is 5.34668197631836 and perplexity is 209.91065258862747
At time: 18.30069899559021 and batch: 450, loss is 5.404458274841309 and perplexity is 222.39571049355678
At time: 18.9382483959198 and batch: 500, loss is 5.381090984344483 and perplexity is 217.2591725318604
At time: 19.564148664474487 and batch: 550, loss is 5.447020988464356 and perplexity is 232.06580842889346
At time: 20.189111471176147 and batch: 600, loss is 5.37515718460083 and perplexity is 215.97381740099948
At time: 20.815348386764526 and batch: 650, loss is 5.304372777938843 and perplexity is 201.21475649141948
At time: 21.44081425666809 and batch: 700, loss is 5.425457782745362 and perplexity is 227.1152919830737
At time: 22.067455053329468 and batch: 750, loss is 5.4103596401214595 and perplexity is 223.71202903600403
At time: 22.70189881324768 and batch: 800, loss is 5.370398244857788 and perplexity is 214.9484527773047
At time: 23.33359146118164 and batch: 850, loss is 5.460894060134888 and perplexity is 235.30770958998903
At time: 23.960954666137695 and batch: 900, loss is 5.3719476699829105 and perplexity is 215.2817574592875
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.4844728234696065 and perplexity of 240.92190214176023
Annealing...
finished 2 epochs...
Completing Train Step...
At time: 25.575441122055054 and batch: 50, loss is 5.349853601455688 and perplexity is 210.5774673752714
At time: 26.196983814239502 and batch: 100, loss is 5.230318832397461 and perplexity is 186.85236861267887
At time: 26.822702884674072 and batch: 150, loss is 5.251043949127197 and perplexity is 190.7653138268027
At time: 27.449432611465454 and batch: 200, loss is 5.1428945446014405 and perplexity is 171.21062857893077
At time: 28.07071328163147 and batch: 250, loss is 5.275835876464844 and perplexity is 195.55386709097857
At time: 28.693285942077637 and batch: 300, loss is 5.256117935180664 and perplexity is 191.73571418564939
At time: 29.34126329421997 and batch: 350, loss is 5.263998565673828 and perplexity is 193.25268198197728
At time: 29.962623834609985 and batch: 400, loss is 5.149012908935547 and perplexity is 172.26136871010434
At time: 30.593117237091064 and batch: 450, loss is 5.151733160018921 and perplexity is 172.7306008101751
At time: 31.21726155281067 and batch: 500, loss is 5.080479793548584 and perplexity is 160.85121279162388
At time: 31.839017868041992 and batch: 550, loss is 5.176269550323486 and perplexity is 177.02120901227192
At time: 32.46118927001953 and batch: 600, loss is 5.110624217987061 and perplexity is 165.77380157245065
At time: 33.08446502685547 and batch: 650, loss is 5.010910577774048 and perplexity is 150.04129823577318
At time: 33.7064254283905 and batch: 700, loss is 5.093108644485474 and perplexity is 162.89545985741032
At time: 34.32923412322998 and batch: 750, loss is 5.133789606094361 and perplexity is 169.65884151708343
At time: 34.952131271362305 and batch: 800, loss is 5.0795403003692625 and perplexity is 160.7001651395502
At time: 35.574254274368286 and batch: 850, loss is 5.119751081466675 and perplexity is 167.29372192998076
At time: 36.197319746017456 and batch: 900, loss is 5.07309928894043 and perplexity is 159.6684198478458
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.301041694536601 and perplexity of 200.54560846846653
finished 3 epochs...
Completing Train Step...
At time: 37.82360577583313 and batch: 50, loss is 5.155792179107666 and perplexity is 173.4331424665028
At time: 38.455233097076416 and batch: 100, loss is 5.08498031616211 and perplexity is 161.5767587555718
At time: 39.07757902145386 and batch: 150, loss is 5.127767477035523 and perplexity is 168.64020433826482
At time: 39.70674657821655 and batch: 200, loss is 5.024245672225952 and perplexity is 152.0555131444445
At time: 40.33019804954529 and batch: 250, loss is 5.149961824417114 and perplexity is 172.42490776985068
At time: 40.95822334289551 and batch: 300, loss is 5.128272762298584 and perplexity is 168.7254372799379
At time: 41.591832399368286 and batch: 350, loss is 5.137708292007447 and perplexity is 170.32498558160012
At time: 42.229796171188354 and batch: 400, loss is 5.04690224647522 and perplexity is 155.53989317647475
At time: 42.864036321640015 and batch: 450, loss is 5.057593040466308 and perplexity is 157.211658458366
At time: 43.492706060409546 and batch: 500, loss is 4.9966043186187745 and perplexity is 147.91004998404765
At time: 44.12604904174805 and batch: 550, loss is 5.087566995620728 and perplexity is 161.99524705263684
At time: 44.75971865653992 and batch: 600, loss is 5.051994142532348 and perplexity is 156.3339059449519
At time: 45.40752410888672 and batch: 650, loss is 4.934449892044068 and perplexity is 138.99665832912814
At time: 46.03917980194092 and batch: 700, loss is 5.015294933319092 and perplexity is 150.70057683348097
At time: 46.66939377784729 and batch: 750, loss is 5.052856492996216 and perplexity is 156.46877870669792
At time: 47.29607105255127 and batch: 800, loss is 4.99838059425354 and perplexity is 148.1730124797676
At time: 47.92716670036316 and batch: 850, loss is 5.034760913848877 and perplexity is 153.6628495603682
At time: 48.55101203918457 and batch: 900, loss is 4.979861249923706 and perplexity is 145.45419847229812
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.244723385327483 and perplexity of 189.56337196650838
finished 4 epochs...
Completing Train Step...
At time: 50.25595569610596 and batch: 50, loss is 5.086179056167603 and perplexity is 161.77056341774315
At time: 50.90999221801758 and batch: 100, loss is 4.996050605773926 and perplexity is 147.8281729597612
At time: 51.54008388519287 and batch: 150, loss is 5.027248582839966 and perplexity is 152.51280852372204
At time: 52.172744274139404 and batch: 200, loss is 4.9193644618988035 and perplexity is 136.9155704682372
At time: 52.809635162353516 and batch: 250, loss is 5.070756206512451 and perplexity is 159.29474152950678
At time: 53.45519804954529 and batch: 300, loss is 5.053945989608764 and perplexity is 156.63934380924746
At time: 54.09058976173401 and batch: 350, loss is 5.072031621932983 and perplexity is 159.4980381155053
At time: 54.73342776298523 and batch: 400, loss is 4.973628044128418 and perplexity is 144.55037231300886
At time: 55.36450386047363 and batch: 450, loss is 4.990247325897217 and perplexity is 146.97276917289804
At time: 55.99701404571533 and batch: 500, loss is 4.9082242870330814 and perplexity is 135.39877146110794
At time: 56.64086890220642 and batch: 550, loss is 5.019870939254761 and perplexity is 151.3917637992978
At time: 57.27608108520508 and batch: 600, loss is 4.983178281784058 and perplexity is 145.93747576282482
At time: 57.917688846588135 and batch: 650, loss is 4.870275354385376 and perplexity is 130.35680627335435
At time: 58.55992317199707 and batch: 700, loss is 4.939011116027832 and perplexity is 139.6321013179761
At time: 59.19879221916199 and batch: 750, loss is 5.006934299468994 and perplexity is 149.44587684182827
At time: 59.83250832557678 and batch: 800, loss is 4.9393695449829105 and perplexity is 139.68215847657856
At time: 60.47123098373413 and batch: 850, loss is 4.977192945480347 and perplexity is 145.0665997338975
At time: 61.10257053375244 and batch: 900, loss is 4.937476644515991 and perplexity is 139.41800414138382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.222167132651969 and perplexity of 185.3353955616866
finished 5 epochs...
Completing Train Step...
At time: 62.735779762268066 and batch: 50, loss is 5.035471963882446 and perplexity is 153.7721503892803
At time: 63.38399529457092 and batch: 100, loss is 4.991448421478271 and perplexity is 147.14940357265056
At time: 64.01814770698547 and batch: 150, loss is 5.011050958633422 and perplexity is 150.06236264064617
At time: 64.64499402046204 and batch: 200, loss is 4.9061438083648685 and perplexity is 135.11737003171768
At time: 65.2725522518158 and batch: 250, loss is 5.035455865859985 and perplexity is 153.76967498167414
At time: 65.89889740943909 and batch: 300, loss is 5.022546215057373 and perplexity is 151.7973207681932
At time: 66.524258852005 and batch: 350, loss is 5.018776454925537 and perplexity is 151.22615852896396
At time: 67.164794921875 and batch: 400, loss is 4.940241785049438 and perplexity is 139.8040480025172
At time: 67.8078031539917 and batch: 450, loss is 4.956646194458008 and perplexity is 142.11636508661715
At time: 68.44903802871704 and batch: 500, loss is 4.882769823074341 and perplexity is 131.9957629324163
At time: 69.0910291671753 and batch: 550, loss is 4.989197473526001 and perplexity is 146.81855043029893
At time: 69.73341155052185 and batch: 600, loss is 4.960671796798706 and perplexity is 142.6896221368646
At time: 70.37743663787842 and batch: 650, loss is 4.859845237731934 and perplexity is 129.00423557429744
At time: 71.01655793190002 and batch: 700, loss is 4.889278869628907 and perplexity is 132.85773175263859
At time: 71.65445303916931 and batch: 750, loss is 4.9534164142608645 and perplexity is 141.65810090961676
At time: 72.2930691242218 and batch: 800, loss is 4.913281755447388 and perplexity is 136.08528100798938
At time: 72.93022060394287 and batch: 850, loss is 4.966240730285644 and perplexity is 143.4864678829191
At time: 73.5696222782135 and batch: 900, loss is 4.919475288391113 and perplexity is 136.93074518151937
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.206342723271618 and perplexity of 182.42565556729443
finished 6 epochs...
Completing Train Step...
At time: 75.20516681671143 and batch: 50, loss is 4.97113673210144 and perplexity is 144.19070044541547
At time: 75.84918689727783 and batch: 100, loss is 4.91356011390686 and perplexity is 136.12316676983417
At time: 76.4889464378357 and batch: 150, loss is 4.957820386886596 and perplexity is 142.2833350547327
At time: 77.12654495239258 and batch: 200, loss is 4.8672709465026855 and perplexity is 129.96574899895867
At time: 77.76950907707214 and batch: 250, loss is 5.011664485931396 and perplexity is 150.1544582452195
At time: 78.42974376678467 and batch: 300, loss is 5.003409986495972 and perplexity is 148.92010982726805
At time: 79.06542921066284 and batch: 350, loss is 5.011333417892456 and perplexity is 150.1047551312004
At time: 79.70625638961792 and batch: 400, loss is 4.9097303199768065 and perplexity is 135.60284009994342
At time: 80.33728790283203 and batch: 450, loss is 4.912079486846924 and perplexity is 135.92176826046503
At time: 80.98002362251282 and batch: 500, loss is 4.868931264877319 and perplexity is 130.18171275477917
At time: 81.62238883972168 and batch: 550, loss is 4.955698986053466 and perplexity is 141.9818150047283
At time: 82.26289796829224 and batch: 600, loss is 4.917151346206665 and perplexity is 136.61289552161165
At time: 82.90658116340637 and batch: 650, loss is 4.838128032684327 and perplexity is 126.23282668988439
At time: 83.5329999923706 and batch: 700, loss is 4.892993154525757 and perplexity is 133.35212080162728
At time: 84.16377186775208 and batch: 750, loss is 4.943713693618775 and perplexity is 140.29027846022703
At time: 84.79008674621582 and batch: 800, loss is 4.902336769104004 and perplexity is 134.60395082283023
At time: 85.41412043571472 and batch: 850, loss is 4.917190227508545 and perplexity is 136.61820731210716
At time: 86.03969931602478 and batch: 900, loss is 4.871838731765747 and perplexity is 130.56076254442831
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.21492568760702 and perplexity of 183.9981471289676
Annealing...
finished 7 epochs...
Completing Train Step...
At time: 87.67125654220581 and batch: 50, loss is 4.942931203842163 and perplexity is 140.18054568955856
At time: 88.3116226196289 and batch: 100, loss is 4.836670761108398 and perplexity is 126.04900515108282
At time: 88.93731951713562 and batch: 150, loss is 4.833759422302246 and perplexity is 125.6825674618658
At time: 89.57099080085754 and batch: 200, loss is 4.724759559631348 and perplexity is 112.70339666978424
At time: 90.197505235672 and batch: 250, loss is 4.865932455062866 and perplexity is 129.79190732520397
At time: 90.82499265670776 and batch: 300, loss is 4.8379559898376465 and perplexity is 126.21111110309545
At time: 91.45197057723999 and batch: 350, loss is 4.8248992347717286 and perplexity is 124.57391503175045
At time: 92.07668709754944 and batch: 400, loss is 4.752065114974975 and perplexity is 115.82322599992551
At time: 92.69676613807678 and batch: 450, loss is 4.747430353164673 and perplexity is 115.28765501600365
At time: 93.3151524066925 and batch: 500, loss is 4.70091513633728 and perplexity is 110.04783515784037
At time: 93.93564105033875 and batch: 550, loss is 4.78328540802002 and perplexity is 119.49629993677378
At time: 94.56962084770203 and batch: 600, loss is 4.7546243000030515 and perplexity is 116.12001867751128
At time: 95.19058966636658 and batch: 650, loss is 4.647312574386596 and perplexity is 104.30429853979632
At time: 95.81515383720398 and batch: 700, loss is 4.703158636093139 and perplexity is 110.2950046077771
At time: 96.43741464614868 and batch: 750, loss is 4.751071996688843 and perplexity is 115.70825693461605
At time: 97.05804133415222 and batch: 800, loss is 4.716966419219971 and perplexity is 111.82849681135812
At time: 97.67797517776489 and batch: 850, loss is 4.761326951980591 and perplexity is 116.90094496526952
At time: 98.29794788360596 and batch: 900, loss is 4.728239583969116 and perplexity is 113.09629047663532
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.083207639929366 and perplexity of 161.29058919349052
finished 8 epochs...
Completing Train Step...
At time: 99.89339399337769 and batch: 50, loss is 4.863186550140381 and perplexity is 129.43599995550838
At time: 100.51241612434387 and batch: 100, loss is 4.7786028194427494 and perplexity is 118.9380559611296
At time: 101.13340210914612 and batch: 150, loss is 4.789503707885742 and perplexity is 120.24167885562902
At time: 101.75252532958984 and batch: 200, loss is 4.685558395385742 and perplexity is 108.37076915347687
At time: 102.37005281448364 and batch: 250, loss is 4.8294288921356205 and perplexity is 125.13947210620799
At time: 102.98995065689087 and batch: 300, loss is 4.812678880691529 and perplexity is 123.06084165688063
At time: 103.61102676391602 and batch: 350, loss is 4.8053123474121096 and perplexity is 122.15764068583503
At time: 104.23112940788269 and batch: 400, loss is 4.735805215835572 and perplexity is 113.95518030114268
At time: 104.85831809043884 and batch: 450, loss is 4.731975288391113 and perplexity is 113.51957492962009
At time: 105.49036812782288 and batch: 500, loss is 4.682665719985962 and perplexity is 108.05774065867845
At time: 106.11404037475586 and batch: 550, loss is 4.767358350753784 and perplexity is 117.60815176053423
At time: 106.74173402786255 and batch: 600, loss is 4.7389599895477295 and perplexity is 114.31525078012541
At time: 107.36618494987488 and batch: 650, loss is 4.639966287612915 and perplexity is 103.54085691499458
At time: 107.9938178062439 and batch: 700, loss is 4.694137868881225 and perplexity is 109.30453316927444
At time: 108.63033390045166 and batch: 750, loss is 4.741340618133545 and perplexity is 114.58771712583179
At time: 109.2559723854065 and batch: 800, loss is 4.711446676254273 and perplexity is 111.21293269346789
At time: 109.88867259025574 and batch: 850, loss is 4.761933650970459 and perplexity is 116.97189016951029
At time: 110.5350992679596 and batch: 900, loss is 4.734729175567627 and perplexity is 113.83262588696516
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.090477094258348 and perplexity of 162.4673558087317
Annealing...
finished 9 epochs...
Completing Train Step...
At time: 112.15047192573547 and batch: 50, loss is 4.845232849121094 and perplexity is 127.13288132719293
At time: 112.78616857528687 and batch: 100, loss is 4.741162004470826 and perplexity is 114.56725202170125
At time: 113.4296064376831 and batch: 150, loss is 4.750076341629028 and perplexity is 115.59310875658225
At time: 114.07250547409058 and batch: 200, loss is 4.632900896072388 and perplexity is 102.81187851190258
At time: 114.71124649047852 and batch: 250, loss is 4.771293354034424 and perplexity is 118.07185195609998
At time: 115.35433101654053 and batch: 300, loss is 4.746039972305298 and perplexity is 115.12747265019972
At time: 115.987300157547 and batch: 350, loss is 4.730036382675171 and perplexity is 113.29968441928861
At time: 116.61518096923828 and batch: 400, loss is 4.6596278381347656 and perplexity is 105.59677574991451
At time: 117.25603413581848 and batch: 450, loss is 4.651315155029297 and perplexity is 104.72262153335664
At time: 117.89996194839478 and batch: 500, loss is 4.590708169937134 and perplexity is 98.56420565990109
At time: 118.540602684021 and batch: 550, loss is 4.674369192123413 and perplexity is 107.16494527452605
At time: 119.17349004745483 and batch: 600, loss is 4.648812856674194 and perplexity is 104.46090187664794
At time: 119.80714893341064 and batch: 650, loss is 4.538231029510498 and perplexity is 93.5252103638014
At time: 120.43996262550354 and batch: 700, loss is 4.581491060256958 and perplexity is 97.65990249859422
At time: 121.06640124320984 and batch: 750, loss is 4.627392406463623 and perplexity is 102.2470973212393
At time: 121.68874764442444 and batch: 800, loss is 4.599408683776855 and perplexity is 99.42550634130662
At time: 122.30811762809753 and batch: 850, loss is 4.65238055229187 and perplexity is 104.83425218260273
At time: 122.92894101142883 and batch: 900, loss is 4.644981994628906 and perplexity is 104.06149210276527
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.037958850599315 and perplexity of 154.15504021197967
finished 10 epochs...
Completing Train Step...
At time: 124.53172779083252 and batch: 50, loss is 4.791691665649414 and perplexity is 120.50505058844507
At time: 125.15123319625854 and batch: 100, loss is 4.705081558227539 and perplexity is 110.50729735925148
At time: 125.7756781578064 and batch: 150, loss is 4.719487142562866 and perplexity is 112.11074109409796
At time: 126.40576648712158 and batch: 200, loss is 4.608180932998657 and perplexity is 100.30152838613071
At time: 127.04637885093689 and batch: 250, loss is 4.749694366455078 and perplexity is 115.54896349049208
At time: 127.68179202079773 and batch: 300, loss is 4.7257287883758545 and perplexity is 112.81268499557801
At time: 128.31158804893494 and batch: 350, loss is 4.712618856430054 and perplexity is 111.34337072196205
At time: 128.94438862800598 and batch: 400, loss is 4.644290657043457 and perplexity is 103.98957534432016
At time: 129.5803906917572 and batch: 450, loss is 4.638596096038818 and perplexity is 103.39908325599218
At time: 130.2183539867401 and batch: 500, loss is 4.5804292011260985 and perplexity is 97.55625647788274
At time: 130.85259532928467 and batch: 550, loss is 4.663902101516723 and perplexity is 106.04909014856771
At time: 131.48770236968994 and batch: 600, loss is 4.642836589813232 and perplexity is 103.83847739044097
At time: 132.12546348571777 and batch: 650, loss is 4.53675687789917 and perplexity is 93.38744159521896
At time: 132.75792503356934 and batch: 700, loss is 4.585683422088623 and perplexity is 98.07018757713793
At time: 133.38410186767578 and batch: 750, loss is 4.63272349357605 and perplexity is 102.79364104573517
At time: 134.01304268836975 and batch: 800, loss is 4.6037658596038815 and perplexity is 99.85966592206626
At time: 134.64882254600525 and batch: 850, loss is 4.654592704772949 and perplexity is 105.06641823234439
At time: 135.28900837898254 and batch: 900, loss is 4.642139434814453 and perplexity is 103.76611110505243
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.034073503050085 and perplexity of 153.55725635532005
finished 11 epochs...
Completing Train Step...
At time: 136.89618372917175 and batch: 50, loss is 4.775895013809204 and perplexity is 118.6164304693332
At time: 137.53597855567932 and batch: 100, loss is 4.691060934066773 and perplexity is 108.96872713736137
At time: 138.1674542427063 and batch: 150, loss is 4.706200199127197 and perplexity is 110.63098450964281
At time: 138.79693794250488 and batch: 200, loss is 4.597367315292359 and perplexity is 99.22274926746715
At time: 139.42227911949158 and batch: 250, loss is 4.7391344165802005 and perplexity is 114.33519218919501
At time: 140.05085802078247 and batch: 300, loss is 4.716866064071655 and perplexity is 111.81727480907695
At time: 140.68037486076355 and batch: 350, loss is 4.704280195236206 and perplexity is 110.4187763743334
At time: 141.3170599937439 and batch: 400, loss is 4.636701908111572 and perplexity is 103.20341133899683
At time: 141.94327235221863 and batch: 450, loss is 4.632684183120728 and perplexity is 102.78960026032453
At time: 142.56958603858948 and batch: 500, loss is 4.575978336334228 and perplexity is 97.12301164301145
At time: 143.2223961353302 and batch: 550, loss is 4.659181680679321 and perplexity is 105.54967346944093
At time: 143.85628271102905 and batch: 600, loss is 4.640408306121826 and perplexity is 103.5866340065951
At time: 144.4844901561737 and batch: 650, loss is 4.535708179473877 and perplexity is 93.28955766660765
At time: 145.10996222496033 and batch: 700, loss is 4.587194805145264 and perplexity is 98.21852126327725
At time: 145.74667811393738 and batch: 750, loss is 4.63345890045166 and perplexity is 102.86926399954109
At time: 146.380756855011 and batch: 800, loss is 4.603842992782592 and perplexity is 99.86736871259036
At time: 147.0092670917511 and batch: 850, loss is 4.653747072219849 and perplexity is 104.97760820446705
At time: 147.64695572853088 and batch: 900, loss is 4.638710088729859 and perplexity is 103.41087066757025
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.032498294360017 and perplexity of 153.31556204015484
finished 12 epochs...
Completing Train Step...
At time: 149.31386876106262 and batch: 50, loss is 4.76573938369751 and perplexity is 117.41790208281958
At time: 149.96013069152832 and batch: 100, loss is 4.68171555519104 and perplexity is 107.95511676021441
At time: 150.5999732017517 and batch: 150, loss is 4.696981010437011 and perplexity is 109.61574362781953
At time: 151.23894238471985 and batch: 200, loss is 4.589838228225708 and perplexity is 98.47849783195653
At time: 151.88695883750916 and batch: 250, loss is 4.732195777893066 and perplexity is 113.54460756377338
At time: 152.53582334518433 and batch: 300, loss is 4.710495462417603 and perplexity is 111.1071957102866
At time: 153.1700394153595 and batch: 350, loss is 4.697898616790772 and perplexity is 109.71637389306908
At time: 153.8070170879364 and batch: 400, loss is 4.630681619644165 and perplexity is 102.58396353009687
At time: 154.45204997062683 and batch: 450, loss is 4.627643175125122 and perplexity is 102.2727409041458
At time: 155.0979905128479 and batch: 500, loss is 4.571970748901367 and perplexity is 96.73456157581315
At time: 155.72871232032776 and batch: 550, loss is 4.6550438022613525 and perplexity is 105.11382412125934
At time: 156.36943936347961 and batch: 600, loss is 4.638172817230225 and perplexity is 103.35532587666093
At time: 157.00483083724976 and batch: 650, loss is 4.534283437728882 and perplexity is 93.15673877820261
At time: 157.63948321342468 and batch: 700, loss is 4.586951875686646 and perplexity is 98.19466398901523
At time: 158.27777314186096 and batch: 750, loss is 4.632735605239868 and perplexity is 102.79488605529777
At time: 158.91394758224487 and batch: 800, loss is 4.602040243148804 and perplexity is 99.68749503253613
At time: 159.5642762184143 and batch: 850, loss is 4.651284198760987 and perplexity is 104.71937976196311
At time: 160.1968126296997 and batch: 900, loss is 4.634326086044312 and perplexity is 102.9585094337928
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.031641293878424 and perplexity of 153.18422681486473
finished 13 epochs...
Completing Train Step...
At time: 161.8367874622345 and batch: 50, loss is 4.757084684371948 and perplexity is 116.40607031056386
At time: 162.4865882396698 and batch: 100, loss is 4.6733348846435545 and perplexity is 107.0541610723908
At time: 163.11760091781616 and batch: 150, loss is 4.689447660446167 and perplexity is 108.79307249199981
At time: 163.75869798660278 and batch: 200, loss is 4.58322919845581 and perplexity is 97.82979651248826
At time: 164.4019546508789 and batch: 250, loss is 4.72644998550415 and perplexity is 112.894074525452
At time: 165.0473644733429 and batch: 300, loss is 4.704827327728271 and perplexity is 110.47920660478569
At time: 165.68588280677795 and batch: 350, loss is 4.692659788131714 and perplexity is 109.1430915842042
At time: 166.3188591003418 and batch: 400, loss is 4.625600328445435 and perplexity is 102.06402663317921
At time: 166.94740343093872 and batch: 450, loss is 4.623169679641723 and perplexity is 101.8162460846799
At time: 167.5854823589325 and batch: 500, loss is 4.568151178359986 and perplexity is 96.36578183255406
At time: 168.2210922241211 and batch: 550, loss is 4.65103178024292 and perplexity is 104.69294998713333
At time: 168.86356210708618 and batch: 600, loss is 4.635694637298584 and perplexity is 103.09950989218643
At time: 169.501695394516 and batch: 650, loss is 4.532140111923217 and perplexity is 92.95728735705079
At time: 170.1467685699463 and batch: 700, loss is 4.5854034328460695 and perplexity is 98.04273282329834
At time: 170.79409337043762 and batch: 750, loss is 4.630546875 and perplexity is 102.57014182165547
At time: 171.4301266670227 and batch: 800, loss is 4.599320297241211 and perplexity is 99.41671885359995
At time: 172.05606126785278 and batch: 850, loss is 4.6481296348571775 and perplexity is 104.38955628466836
At time: 172.68335223197937 and batch: 900, loss is 4.629890584945679 and perplexity is 102.50284814220925
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.030734127514983 and perplexity of 153.0453262493747
finished 14 epochs...
Completing Train Step...
At time: 174.30183339118958 and batch: 50, loss is 4.749407014846802 and perplexity is 115.51576508003522
At time: 174.944917678833 and batch: 100, loss is 4.666007223129273 and perplexity is 106.27257152543841
At time: 175.57659554481506 and batch: 150, loss is 4.682615356445313 and perplexity is 108.05229862530553
At time: 176.21144247055054 and batch: 200, loss is 4.577348585128784 and perplexity is 97.25618555249635
At time: 176.85590744018555 and batch: 250, loss is 4.721026678085327 and perplexity is 112.28347249140579
At time: 177.48420810699463 and batch: 300, loss is 4.699549913406372 and perplexity is 109.89769783856084
At time: 178.1102638244629 and batch: 350, loss is 4.687902240753174 and perplexity is 108.62507138485596
At time: 178.7294180393219 and batch: 400, loss is 4.620891456604004 and perplexity is 101.5845499951345
At time: 179.3472774028778 and batch: 450, loss is 4.618929176330567 and perplexity is 101.38540808661278
At time: 179.96645259857178 and batch: 500, loss is 4.564329795837402 and perplexity is 95.99823403572138
At time: 180.59020686149597 and batch: 550, loss is 4.647121887207032 and perplexity is 104.28441094350637
At time: 181.21877789497375 and batch: 600, loss is 4.632925033569336 and perplexity is 102.81436016325667
At time: 181.84826850891113 and batch: 650, loss is 4.5298755168914795 and perplexity is 92.74701492675192
At time: 182.48312878608704 and batch: 700, loss is 4.583439168930053 and perplexity is 97.85034003794851
At time: 183.11014819145203 and batch: 750, loss is 4.628114423751831 and perplexity is 102.32094815075386
At time: 183.7331624031067 and batch: 800, loss is 4.596506023406983 and perplexity is 99.13732631100875
At time: 184.35392951965332 and batch: 850, loss is 4.6449747657775875 and perplexity is 104.06073986042982
At time: 184.9767279624939 and batch: 900, loss is 4.625960521697998 and perplexity is 102.1007960285484
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.030380458047945 and perplexity of 152.99120836089395
finished 15 epochs...
Completing Train Step...
At time: 186.57183527946472 and batch: 50, loss is 4.7431707859039305 and perplexity is 114.79762389631007
At time: 187.19535517692566 and batch: 100, loss is 4.659328308105469 and perplexity is 105.56515108108592
At time: 187.81826448440552 and batch: 150, loss is 4.6766904354095455 and perplexity is 107.41399011926556
At time: 188.44207978248596 and batch: 200, loss is 4.572389545440674 and perplexity is 96.77508215978041
At time: 189.0676884651184 and batch: 250, loss is 4.716308193206787 and perplexity is 111.75491260591751
At time: 189.6879448890686 and batch: 300, loss is 4.695248537063598 and perplexity is 109.42600167957268
At time: 190.30232739448547 and batch: 350, loss is 4.683333148956299 and perplexity is 108.12988559839653
At time: 190.91665029525757 and batch: 400, loss is 4.6164718437194825 and perplexity is 101.13657627351188
At time: 191.53217458724976 and batch: 450, loss is 4.615142078399658 and perplexity is 101.00217774087591
At time: 192.16690707206726 and batch: 500, loss is 4.561050195693969 and perplexity is 95.68391391749067
At time: 192.78279757499695 and batch: 550, loss is 4.643676223754883 and perplexity is 103.92570031304629
At time: 193.39893007278442 and batch: 600, loss is 4.630219049453736 and perplexity is 102.53652221986559
At time: 194.0147762298584 and batch: 650, loss is 4.5276085567474365 and perplexity is 92.53699927892671
At time: 194.62884831428528 and batch: 700, loss is 4.5813672065734865 and perplexity is 97.64780770894986
At time: 195.24433994293213 and batch: 750, loss is 4.625747566223144 and perplexity is 102.07905542002021
At time: 195.860365152359 and batch: 800, loss is 4.5937339973449705 and perplexity is 98.86289559902937
At time: 196.47418093681335 and batch: 850, loss is 4.641699647903442 and perplexity is 103.72048616094389
At time: 197.09334897994995 and batch: 900, loss is 4.6224026870727535 and perplexity is 101.73818372098586
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.029967843669734 and perplexity of 152.9280950102177
finished 16 epochs...
Completing Train Step...
At time: 198.69841361045837 and batch: 50, loss is 4.737148990631104 and perplexity is 114.10841333253624
At time: 199.31307172775269 and batch: 100, loss is 4.653363752365112 and perplexity is 104.93737591434993
At time: 199.9318323135376 and batch: 150, loss is 4.671109752655029 and perplexity is 106.8162162615652
At time: 200.5532386302948 and batch: 200, loss is 4.567702560424805 and perplexity is 96.32256011023107
At time: 201.17032289505005 and batch: 250, loss is 4.7119791793823245 and perplexity is 111.27216969855247
At time: 201.78644347190857 and batch: 300, loss is 4.690762586593628 and perplexity is 108.9362214422053
At time: 202.40322256088257 and batch: 350, loss is 4.6791528224945065 and perplexity is 107.67881085304833
At time: 203.01785492897034 and batch: 400, loss is 4.612423400878907 and perplexity is 100.72795831669804
At time: 203.63341879844666 and batch: 450, loss is 4.611372175216675 and perplexity is 100.62212613850818
At time: 204.2497639656067 and batch: 500, loss is 4.557707834243774 and perplexity is 95.36463755793687
At time: 204.8665373325348 and batch: 550, loss is 4.639969348907471 and perplexity is 103.5411738845413
At time: 205.48369789123535 and batch: 600, loss is 4.627554788589477 and perplexity is 102.26370177036111
At time: 206.1003499031067 and batch: 650, loss is 4.525218114852906 and perplexity is 92.31605913655069
At time: 206.7162892818451 and batch: 700, loss is 4.579370565414429 and perplexity is 97.45303458770819
At time: 207.3326871395111 and batch: 750, loss is 4.6230244636535645 and perplexity is 101.80146181137651
At time: 207.9683895111084 and batch: 800, loss is 4.590788307189942 and perplexity is 98.57210464106497
At time: 208.58264136314392 and batch: 850, loss is 4.638747816085815 and perplexity is 103.41477215989374
At time: 209.19710683822632 and batch: 900, loss is 4.618747844696045 and perplexity is 101.36702537158202
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.029755056720891 and perplexity of 152.8955573694035
finished 17 epochs...
Completing Train Step...
At time: 210.80106234550476 and batch: 50, loss is 4.731687297821045 and perplexity is 113.48688706964589
At time: 211.41578078269958 and batch: 100, loss is 4.647816028594971 and perplexity is 104.3568241988703
At time: 212.0347285270691 and batch: 150, loss is 4.665854444503784 and perplexity is 106.25633658824091
At time: 212.67073607444763 and batch: 200, loss is 4.563319711685181 and perplexity is 95.90131669645507
At time: 213.29312252998352 and batch: 250, loss is 4.707908000946045 and perplexity is 110.82008173053828
At time: 213.91384196281433 and batch: 300, loss is 4.6866806125640865 and perplexity is 108.49245295730836
At time: 214.52847456932068 and batch: 350, loss is 4.67521445274353 and perplexity is 107.2555658762412
At time: 215.1433846950531 and batch: 400, loss is 4.60841661453247 and perplexity is 100.32517039006702
At time: 215.7588415145874 and batch: 450, loss is 4.607882099151611 and perplexity is 100.27155937263936
At time: 216.3748471736908 and batch: 500, loss is 4.554569120407105 and perplexity is 95.06578450296065
At time: 216.99105668067932 and batch: 550, loss is 4.636531915664673 and perplexity is 103.18586902964724
At time: 217.60739302635193 and batch: 600, loss is 4.624941368103027 and perplexity is 101.99679264194134
At time: 218.22338461875916 and batch: 650, loss is 4.522862634658813 and perplexity is 92.09886638469119
At time: 218.83949542045593 and batch: 700, loss is 4.577086248397827 and perplexity is 97.23067502903312
At time: 219.45825862884521 and batch: 750, loss is 4.620237121582031 and perplexity is 101.51810140857087
At time: 220.08472180366516 and batch: 800, loss is 4.587813653945923 and perplexity is 98.27932248880587
At time: 220.7026867866516 and batch: 850, loss is 4.635653734207153 and perplexity is 103.0952928897517
At time: 221.32108211517334 and batch: 900, loss is 4.615567073822022 and perplexity is 101.04511232691958
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.029763835750214 and perplexity of 152.89689964987696
Annealing...
finished 18 epochs...
Completing Train Step...
At time: 222.9240381717682 and batch: 50, loss is 4.723870658874512 and perplexity is 112.60325904798398
At time: 223.56907200813293 and batch: 100, loss is 4.6354883098602295 and perplexity is 103.07823982878912
At time: 224.19694209098816 and batch: 150, loss is 4.6520013332366945 and perplexity is 104.79450457354172
At time: 224.82089018821716 and batch: 200, loss is 4.549935331344605 and perplexity is 94.62628876219529
At time: 225.44747805595398 and batch: 250, loss is 4.690058422088623 and perplexity is 108.85953942330282
At time: 226.07346606254578 and batch: 300, loss is 4.665403757095337 and perplexity is 106.20845898499971
At time: 226.70348620414734 and batch: 350, loss is 4.653945827484131 and perplexity is 104.99847513036644
At time: 227.33118653297424 and batch: 400, loss is 4.584111747741699 and perplexity is 97.91617424023336
At time: 227.95537328720093 and batch: 450, loss is 4.58357723236084 and perplexity is 97.86385052422837
At time: 228.59431719779968 and batch: 500, loss is 4.525618553161621 and perplexity is 92.35303342560543
At time: 229.22174835205078 and batch: 550, loss is 4.605903186798096 and perplexity is 100.07332695207492
At time: 229.843852519989 and batch: 600, loss is 4.596402158737183 and perplexity is 99.12702998006837
At time: 230.46630334854126 and batch: 650, loss is 4.494094285964966 and perplexity is 89.48708256350139
At time: 231.0871286392212 and batch: 700, loss is 4.541648359298706 and perplexity is 93.84536357411106
At time: 231.715398311615 and batch: 750, loss is 4.586281900405884 and perplexity is 98.12889802468506
At time: 232.34183192253113 and batch: 800, loss is 4.555813989639282 and perplexity is 95.18420266539198
At time: 232.96392941474915 and batch: 850, loss is 4.603222160339356 and perplexity is 99.80538705217657
At time: 233.58671307563782 and batch: 900, loss is 4.5920884323120115 and perplexity is 98.70034405622287
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.019811760889341 and perplexity of 151.38280494726573
finished 19 epochs...
Completing Train Step...
At time: 235.179354429245 and batch: 50, loss is 4.71241455078125 and perplexity is 111.320624965989
At time: 235.81668281555176 and batch: 100, loss is 4.627752685546875 and perplexity is 102.28394144841303
At time: 236.44897985458374 and batch: 150, loss is 4.644711799621582 and perplexity is 104.03337900532479
At time: 237.07584357261658 and batch: 200, loss is 4.544271593093872 and perplexity is 94.09186507754015
At time: 237.69933676719666 and batch: 250, loss is 4.684854764938354 and perplexity is 108.2945430013593
At time: 238.32942295074463 and batch: 300, loss is 4.661083030700683 and perplexity is 105.75055125231859
At time: 238.9563910961151 and batch: 350, loss is 4.649621143341064 and perplexity is 104.54537036362115
At time: 239.59764456748962 and batch: 400, loss is 4.581094379425049 and perplexity is 97.62117036988131
At time: 240.21562147140503 and batch: 450, loss is 4.581430282592773 and perplexity is 97.65396713820633
At time: 240.83690094947815 and batch: 500, loss is 4.5250817489624025 and perplexity is 92.30347123323862
At time: 241.4545316696167 and batch: 550, loss is 4.606018972396851 and perplexity is 100.08491467298812
At time: 242.07702803611755 and batch: 600, loss is 4.5967685508728025 and perplexity is 99.16335599865778
At time: 242.70737648010254 and batch: 650, loss is 4.4962629795074465 and perplexity is 89.68136321304175
At time: 243.3345217704773 and batch: 700, loss is 4.54281171798706 and perplexity is 93.95460292313929
At time: 243.95757293701172 and batch: 750, loss is 4.587151975631714 and perplexity is 98.21431470187308
At time: 244.5868365764618 and batch: 800, loss is 4.556327028274536 and perplexity is 95.23304836761997
At time: 245.21058893203735 and batch: 850, loss is 4.604316892623902 and perplexity is 99.91470705870667
At time: 245.8278045654297 and batch: 900, loss is 4.5921916580200195 and perplexity is 98.71053299498988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.018720234910103 and perplexity of 151.21765683098246
finished 20 epochs...
Completing Train Step...
At time: 247.43177962303162 and batch: 50, loss is 4.708210372924805 and perplexity is 110.85359568452246
At time: 248.07293057441711 and batch: 100, loss is 4.624522476196289 and perplexity is 101.95407595845155
At time: 248.69370365142822 and batch: 150, loss is 4.641098260879517 and perplexity is 103.65812875879709
At time: 249.3155312538147 and batch: 200, loss is 4.5416122245788575 and perplexity is 93.8419725594563
At time: 249.93775963783264 and batch: 250, loss is 4.6817854881286625 and perplexity is 107.96266664265045
At time: 250.5587887763977 and batch: 300, loss is 4.6585592269897464 and perplexity is 105.48399412905097
At time: 251.17866444587708 and batch: 350, loss is 4.647480134963989 and perplexity is 104.32177729261826
At time: 251.79795241355896 and batch: 400, loss is 4.579545888900757 and perplexity is 97.4701218913444
At time: 252.41701245307922 and batch: 450, loss is 4.580501585006714 and perplexity is 97.56331823388051
At time: 253.03749561309814 and batch: 500, loss is 4.525168256759644 and perplexity is 92.31145654860376
At time: 253.65756034851074 and batch: 550, loss is 4.606575393676758 and perplexity is 100.14061954556094
At time: 254.27508759498596 and batch: 600, loss is 4.5973233795166015 and perplexity is 99.21838993477137
At time: 254.89286708831787 and batch: 650, loss is 4.497890434265137 and perplexity is 89.82743440416598
At time: 255.521666765213 and batch: 700, loss is 4.543266487121582 and perplexity is 93.99734029367693
At time: 256.14015007019043 and batch: 750, loss is 4.587300157546997 and perplexity is 98.22886936547616
At time: 256.75749373435974 and batch: 800, loss is 4.556209497451782 and perplexity is 95.22185620681677
At time: 257.3737280368805 and batch: 850, loss is 4.604500150680542 and perplexity is 99.93301891159803
At time: 257.9909052848816 and batch: 900, loss is 4.592126483917236 and perplexity is 98.70409983420672
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.018318071757277 and perplexity of 151.15685488831835
finished 21 epochs...
Completing Train Step...
At time: 259.5795567035675 and batch: 50, loss is 4.705146284103393 and perplexity is 110.51445027234817
At time: 260.21035742759705 and batch: 100, loss is 4.622116060256958 and perplexity is 101.70902700808857
At time: 260.8268268108368 and batch: 150, loss is 4.638248176574707 and perplexity is 103.36311495975411
At time: 261.4439420700073 and batch: 200, loss is 4.5395583152771 and perplexity is 93.64942746189291
At time: 262.0611500740051 and batch: 250, loss is 4.67940128326416 and perplexity is 107.70556813719831
At time: 262.67854046821594 and batch: 300, loss is 4.656612167358398 and perplexity is 105.2788103197067
At time: 263.2961690425873 and batch: 350, loss is 4.645862636566162 and perplexity is 104.15317338002806
At time: 263.9180681705475 and batch: 400, loss is 4.578418235778809 and perplexity is 97.36027135238909
At time: 264.54012846946716 and batch: 450, loss is 4.579831657409668 and perplexity is 97.49797976300228
At time: 265.16252756118774 and batch: 500, loss is 4.525205354690552 and perplexity is 92.31488117616378
At time: 265.78362226486206 and batch: 550, loss is 4.606827249526978 and perplexity is 100.16584372273306
At time: 266.4060592651367 and batch: 600, loss is 4.597753353118897 and perplexity is 99.26106039623417
At time: 267.0276596546173 and batch: 650, loss is 4.499060192108154 and perplexity is 89.93257223098854
At time: 267.648446559906 and batch: 700, loss is 4.543405437469483 and perplexity is 94.01040216426718
At time: 268.27031326293945 and batch: 750, loss is 4.587170143127441 and perplexity is 98.2160990262241
At time: 268.89195585250854 and batch: 800, loss is 4.555912008285523 and perplexity is 95.19353294934415
At time: 269.51319456100464 and batch: 850, loss is 4.6044008159637455 and perplexity is 99.92309258648837
At time: 270.1360604763031 and batch: 900, loss is 4.591531677246094 and perplexity is 98.64540743420214
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.018089816994863 and perplexity of 151.12235655367286
finished 22 epochs...
Completing Train Step...
At time: 271.7230176925659 and batch: 50, loss is 4.702655258178711 and perplexity is 110.23949850982979
At time: 272.3466305732727 and batch: 100, loss is 4.620121374130249 and perplexity is 101.50635162703979
At time: 272.97078680992126 and batch: 150, loss is 4.635952272415161 and perplexity is 103.12607536838944
At time: 273.59432435035706 and batch: 200, loss is 4.537887945175171 and perplexity is 93.49312883281746
At time: 274.21856689453125 and batch: 250, loss is 4.677366037368774 and perplexity is 107.48658374087397
At time: 274.84357714653015 and batch: 300, loss is 4.6550861263275145 and perplexity is 105.1182730598539
At time: 275.4660875797272 and batch: 350, loss is 4.644517469406128 and perplexity is 104.01316414061134
At time: 276.0851936340332 and batch: 400, loss is 4.577333641052246 and perplexity is 97.2547321594755
At time: 276.7063937187195 and batch: 450, loss is 4.579194040298462 and perplexity is 97.43583319775958
At time: 277.33166241645813 and batch: 500, loss is 4.525110197067261 and perplexity is 92.30609712941778
At time: 277.95337557792664 and batch: 550, loss is 4.6071585750579835 and perplexity is 100.19903672263364
At time: 278.5757005214691 and batch: 600, loss is 4.598051357269287 and perplexity is 99.2906450121546
At time: 279.1971626281738 and batch: 650, loss is 4.499966478347778 and perplexity is 90.01411382812795
At time: 279.81614995002747 and batch: 700, loss is 4.543369598388672 and perplexity is 94.0070329782416
At time: 280.4350206851959 and batch: 750, loss is 4.586902256011963 and perplexity is 98.18979172261363
At time: 281.06494665145874 and batch: 800, loss is 4.555404767990113 and perplexity is 95.14525919780084
At time: 281.68436336517334 and batch: 850, loss is 4.604096212387085 and perplexity is 99.89266029022399
At time: 282.30350518226624 and batch: 900, loss is 4.591061592102051 and perplexity is 98.59904659126477
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.018013314025043 and perplexity of 151.11079568681635
finished 23 epochs...
Completing Train Step...
At time: 283.9047656059265 and batch: 50, loss is 4.700460176467896 and perplexity is 109.9977791967211
At time: 284.5303518772125 and batch: 100, loss is 4.618440456390381 and perplexity is 101.33587112187422
At time: 285.1507306098938 and batch: 150, loss is 4.6337971591949465 and perplexity is 102.904066313266
At time: 285.7704541683197 and batch: 200, loss is 4.53633828163147 and perplexity is 93.34835814138181
At time: 286.39019441604614 and batch: 250, loss is 4.675559625625611 and perplexity is 107.29259397921476
At time: 287.01085662841797 and batch: 300, loss is 4.653603715896606 and perplexity is 104.96256007917978
At time: 287.6442265510559 and batch: 350, loss is 4.643231601715088 and perplexity is 103.87950292713495
At time: 288.26024198532104 and batch: 400, loss is 4.5764057826995845 and perplexity is 97.16453539528699
At time: 288.8779797554016 and batch: 450, loss is 4.578593339920044 and perplexity is 97.37732103178581
At time: 289.4982006549835 and batch: 500, loss is 4.524991273880005 and perplexity is 92.29512044684795
At time: 290.1171998977661 and batch: 550, loss is 4.607307481765747 and perplexity is 100.21395814223523
At time: 290.7365221977234 and batch: 600, loss is 4.598201551437378 and perplexity is 99.30555900795099
At time: 291.35569310188293 and batch: 650, loss is 4.500700359344482 and perplexity is 90.08019772159243
At time: 291.9817819595337 and batch: 700, loss is 4.54314079284668 and perplexity is 93.98552610864925
At time: 292.60943245887756 and batch: 750, loss is 4.586440553665161 and perplexity is 98.14446772924552
At time: 293.22806334495544 and batch: 800, loss is 4.554825296401978 and perplexity is 95.09014119455074
At time: 293.8462858200073 and batch: 850, loss is 4.6036052417755124 and perplexity is 99.84362796740939
At time: 294.4657185077667 and batch: 900, loss is 4.5904660892486575 and perplexity is 98.5403480569848
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0178770300460185 and perplexity of 151.09020310955762
finished 24 epochs...
Completing Train Step...
At time: 296.12603664398193 and batch: 50, loss is 4.698416023254395 and perplexity is 109.77315654267483
At time: 296.74657917022705 and batch: 100, loss is 4.6168757343292235 and perplexity is 101.17743263716551
At time: 297.3686797618866 and batch: 150, loss is 4.631869106292725 and perplexity is 102.70585297386543
At time: 297.9935653209686 and batch: 200, loss is 4.534864845275879 and perplexity is 93.2109165573406
At time: 298.6144919395447 and batch: 250, loss is 4.673932437896728 and perplexity is 107.11815075132175
At time: 299.2352750301361 and batch: 300, loss is 4.6522478675842285 and perplexity is 104.8203432032759
At time: 299.85617184638977 and batch: 350, loss is 4.64211441040039 and perplexity is 103.76351445141246
At time: 300.47205686569214 and batch: 400, loss is 4.575516347885132 and perplexity is 97.07815229653876
At time: 301.0886449813843 and batch: 450, loss is 4.578050651550293 and perplexity is 97.32448982892066
At time: 301.70580887794495 and batch: 500, loss is 4.524916305541992 and perplexity is 92.28820149441576
At time: 302.3283317089081 and batch: 550, loss is 4.607340650558472 and perplexity is 100.21728217336774
At time: 302.9530358314514 and batch: 600, loss is 4.598332214355469 and perplexity is 99.31853540982239
At time: 303.5892572402954 and batch: 650, loss is 4.501238107681274 and perplexity is 90.12865122482289
At time: 304.22077918052673 and batch: 700, loss is 4.54293272972107 and perplexity is 93.965973220513
At time: 304.8412940502167 and batch: 750, loss is 4.585971374511718 and perplexity is 98.09843119149761
At time: 305.4581413269043 and batch: 800, loss is 4.55411322593689 and perplexity is 95.0224544152233
At time: 306.07531547546387 and batch: 850, loss is 4.603012819290161 and perplexity is 99.78449587450257
At time: 306.6958055496216 and batch: 900, loss is 4.589840211868286 and perplexity is 98.47869317829162
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.017657136263913 and perplexity of 151.0569829659421
finished 25 epochs...
Completing Train Step...
At time: 308.24652647972107 and batch: 50, loss is 4.696472215652466 and perplexity is 109.55998589498228
At time: 308.8734619617462 and batch: 100, loss is 4.615420646667481 and perplexity is 101.03031766183805
At time: 309.4906508922577 and batch: 150, loss is 4.62999321937561 and perplexity is 102.51336900348667
At time: 310.10871267318726 and batch: 200, loss is 4.533539037704468 and perplexity is 93.08741870371541
At time: 310.7281885147095 and batch: 250, loss is 4.672332105636596 and perplexity is 106.94686321407104
At time: 311.3437502384186 and batch: 300, loss is 4.65088116645813 and perplexity is 104.6771829730896
At time: 311.96433901786804 and batch: 350, loss is 4.640965614318848 and perplexity is 103.64437977642964
At time: 312.5754177570343 and batch: 400, loss is 4.574688138961792 and perplexity is 96.9977845897653
At time: 313.1892774105072 and batch: 450, loss is 4.577420082092285 and perplexity is 97.2631393230288
At time: 313.8039586544037 and batch: 500, loss is 4.524654388427734 and perplexity is 92.26403280023581
At time: 314.416925907135 and batch: 550, loss is 4.607321405410767 and perplexity is 100.21535349552863
At time: 315.0306737422943 and batch: 600, loss is 4.598557806015014 and perplexity is 99.34094337047863
At time: 315.64548563957214 and batch: 650, loss is 4.501927986145019 and perplexity is 90.19085049277008
At time: 316.2581875324249 and batch: 700, loss is 4.54273720741272 and perplexity is 93.94760257251684
At time: 316.8720963001251 and batch: 750, loss is 4.58557391166687 and perplexity is 98.05944845756845
At time: 317.49025678634644 and batch: 800, loss is 4.553550806045532 and perplexity is 94.96902692248521
At time: 318.1135141849518 and batch: 850, loss is 4.60248875617981 and perplexity is 99.73221620135033
At time: 318.73735332489014 and batch: 900, loss is 4.589253826141357 and perplexity is 98.42096360575813
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.017575616705908 and perplexity of 151.04466936936342
finished 26 epochs...
Completing Train Step...
At time: 320.32563877105713 and batch: 50, loss is 4.694731426239014 and perplexity is 109.3694309375453
At time: 320.95356607437134 and batch: 100, loss is 4.613972454071045 and perplexity is 100.88411219617372
At time: 321.5695378780365 and batch: 150, loss is 4.628350849151611 and perplexity is 102.34514228176717
At time: 322.19359374046326 and batch: 200, loss is 4.532276124954223 and perplexity is 92.96993161933104
At time: 322.8244831562042 and batch: 250, loss is 4.670898895263672 and perplexity is 106.7936956472521
At time: 323.4536032676697 and batch: 300, loss is 4.649744157791138 and perplexity is 104.55823174591586
At time: 324.078351020813 and batch: 350, loss is 4.639943389892578 and perplexity is 103.53848609255283
At time: 324.6985619068146 and batch: 400, loss is 4.573888626098633 and perplexity is 96.92026460652903
At time: 325.32723450660706 and batch: 450, loss is 4.576806716918945 and perplexity is 97.20349979299345
At time: 325.95127058029175 and batch: 500, loss is 4.524336786270141 and perplexity is 92.23473419724627
At time: 326.57190561294556 and batch: 550, loss is 4.607334928512573 and perplexity is 100.21670872711996
At time: 327.1986963748932 and batch: 600, loss is 4.598446712493897 and perplexity is 99.32990784828732
At time: 327.8180043697357 and batch: 650, loss is 4.50226321220398 and perplexity is 90.22108988436793
At time: 328.43732476234436 and batch: 700, loss is 4.5423564434051515 and perplexity is 93.91183751631509
At time: 329.0584511756897 and batch: 750, loss is 4.5849924659729 and perplexity is 98.0024487862223
At time: 329.678049325943 and batch: 800, loss is 4.552956895828247 and perplexity is 94.91264059293637
At time: 330.3025963306427 and batch: 850, loss is 4.602051334381104 and perplexity is 99.68860069583248
At time: 330.9286141395569 and batch: 900, loss is 4.58882360458374 and perplexity is 98.37862989258444
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.017631635273973 and perplexity of 151.05313091245492
Annealing...
finished 27 epochs...
Completing Train Step...
At time: 332.5048830509186 and batch: 50, loss is 4.6925475215911865 and perplexity is 109.13083915467142
At time: 333.1390926837921 and batch: 100, loss is 4.610568351745606 and perplexity is 100.54127621069725
At time: 333.76468420028687 and batch: 150, loss is 4.624769945144653 and perplexity is 101.9793095485466
At time: 334.39444756507874 and batch: 200, loss is 4.528165874481201 and perplexity is 92.58858616346173
At time: 335.0180037021637 and batch: 250, loss is 4.665698347091674 and perplexity is 106.23975154355415
At time: 335.65312695503235 and batch: 300, loss is 4.643809003829956 and perplexity is 103.93950049150993
At time: 336.2739498615265 and batch: 350, loss is 4.634066581726074 and perplexity is 102.93179472243682
At time: 336.8937587738037 and batch: 400, loss is 4.567203016281128 and perplexity is 96.2744547557984
At time: 337.51873564720154 and batch: 450, loss is 4.570191869735718 and perplexity is 96.56263544287273
At time: 338.14298844337463 and batch: 500, loss is 4.516066579818726 and perplexity is 91.47507948389286
At time: 338.7787013053894 and batch: 550, loss is 4.59996883392334 and perplexity is 99.48121515444129
At time: 339.4098057746887 and batch: 600, loss is 4.592444648742676 and perplexity is 98.73550900328206
At time: 340.03315019607544 and batch: 650, loss is 4.493278398513794 and perplexity is 89.41410095223469
At time: 340.65759778022766 and batch: 700, loss is 4.533443927764893 and perplexity is 93.07856558596369
At time: 341.2745518684387 and batch: 750, loss is 4.574402198791504 and perplexity is 96.97005299170002
At time: 341.89879274368286 and batch: 800, loss is 4.544393568038941 and perplexity is 94.10334262758693
At time: 342.52351784706116 and batch: 850, loss is 4.594913930892944 and perplexity is 98.97961609386833
At time: 343.14641523361206 and batch: 900, loss is 4.5834462928771975 and perplexity is 97.85103712108197
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.014690242401541 and perplexity of 150.60947710976376
finished 28 epochs...
Completing Train Step...
At time: 344.71951818466187 and batch: 50, loss is 4.690290689468384 and perplexity is 108.88482687989857
At time: 345.3547806739807 and batch: 100, loss is 4.609552068710327 and perplexity is 100.43914972083856
At time: 345.97786593437195 and batch: 150, loss is 4.624308786392212 and perplexity is 101.93229173955073
At time: 346.61084842681885 and batch: 200, loss is 4.527445182800293 and perplexity is 92.52188237898517
At time: 347.2342505455017 and batch: 250, loss is 4.665447130203247 and perplexity is 106.21306567585472
At time: 347.8570237159729 and batch: 300, loss is 4.643250970840454 and perplexity is 103.88151500173619
At time: 348.47847867012024 and batch: 350, loss is 4.633251533508301 and perplexity is 102.8479345262902
At time: 349.1014394760132 and batch: 400, loss is 4.566392574310303 and perplexity is 96.19646150571663
At time: 349.72231364250183 and batch: 450, loss is 4.56976201057434 and perplexity is 96.52113602947142
At time: 350.3460443019867 and batch: 500, loss is 4.516081743240356 and perplexity is 91.47646656960819
At time: 350.97079515457153 and batch: 550, loss is 4.599532737731933 and perplexity is 99.43784123368417
At time: 351.61555004119873 and batch: 600, loss is 4.592104158401489 and perplexity is 98.70189623886984
At time: 352.24015522003174 and batch: 650, loss is 4.49262451171875 and perplexity is 89.35565336346711
At time: 352.8610632419586 and batch: 700, loss is 4.5333599090576175 and perplexity is 93.0707455737264
At time: 353.47472310066223 and batch: 750, loss is 4.574343156814575 and perplexity is 96.96432785708178
At time: 354.09415006637573 and batch: 800, loss is 4.545007658004761 and perplexity is 94.1611482931677
At time: 354.7124001979828 and batch: 850, loss is 4.595062103271484 and perplexity is 98.99428322561702
At time: 355.3294892311096 and batch: 900, loss is 4.58299693107605 and perplexity is 97.80707648065307
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.014370434904752 and perplexity of 150.56131877100583
finished 29 epochs...
Completing Train Step...
At time: 356.9155423641205 and batch: 50, loss is 4.689706392288208 and perplexity is 108.82122436578308
At time: 357.5351173877716 and batch: 100, loss is 4.609353351593017 and perplexity is 100.41919272550496
At time: 358.15754294395447 and batch: 150, loss is 4.624285268783569 and perplexity is 101.92989456399357
At time: 358.7756142616272 and batch: 200, loss is 4.527062149047851 and perplexity is 92.48645016149536
At time: 359.3941602706909 and batch: 250, loss is 4.665324621200561 and perplexity is 106.2000544161211
At time: 360.0133180618286 and batch: 300, loss is 4.642985181808472 and perplexity is 103.8539081033909
At time: 360.6319878101349 and batch: 350, loss is 4.632702016830445 and perplexity is 102.79143339656326
At time: 361.24911093711853 and batch: 400, loss is 4.565902919769287 and perplexity is 96.14937000173596
At time: 361.8684742450714 and batch: 450, loss is 4.5695350646972654 and perplexity is 96.49923344104403
At time: 362.4899215698242 and batch: 500, loss is 4.516059427261353 and perplexity is 91.47442520547852
At time: 363.1087749004364 and batch: 550, loss is 4.599209079742431 and perplexity is 99.4056625896286
At time: 363.73211193084717 and batch: 600, loss is 4.591817359924317 and perplexity is 98.67359274422809
At time: 364.3595428466797 and batch: 650, loss is 4.492112913131714 and perplexity is 89.30995082913547
At time: 364.9809536933899 and batch: 700, loss is 4.533255281448365 and perplexity is 93.06100831352764
At time: 365.60594487190247 and batch: 750, loss is 4.574246740341186 and perplexity is 96.95497934922757
At time: 366.2268087863922 and batch: 800, loss is 4.545431938171387 and perplexity is 94.20110747720038
At time: 366.8471164703369 and batch: 850, loss is 4.595189123153687 and perplexity is 99.00685826643425
At time: 367.4882004261017 and batch: 900, loss is 4.58267427444458 and perplexity is 97.7755234694898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.014203215298587 and perplexity of 150.53614407148766
finished 30 epochs...
Completing Train Step...
At time: 369.09524607658386 and batch: 50, loss is 4.689252424240112 and perplexity is 108.7718342185889
At time: 369.7148869037628 and batch: 100, loss is 4.609247303009033 and perplexity is 100.40854397696388
At time: 370.33466601371765 and batch: 150, loss is 4.624306354522705 and perplexity is 101.93204385382012
At time: 370.95266604423523 and batch: 200, loss is 4.526743793487549 and perplexity is 92.45701127209954
At time: 371.5723421573639 and batch: 250, loss is 4.66527717590332 and perplexity is 106.19501584250155
At time: 372.1939618587494 and batch: 300, loss is 4.6427906799316405 and perplexity is 103.83371028766874
At time: 372.8122570514679 and batch: 350, loss is 4.632206010818481 and perplexity is 102.74046087000507
At time: 373.4290494918823 and batch: 400, loss is 4.565425624847412 and perplexity is 96.10348934586573
At time: 374.04581689834595 and batch: 450, loss is 4.569293937683105 and perplexity is 96.47596767413083
At time: 374.66829228401184 and batch: 500, loss is 4.5160235404968265 and perplexity is 91.47114254322342
At time: 375.30256605148315 and batch: 550, loss is 4.598968257904053 and perplexity is 99.3817264175107
At time: 375.9344937801361 and batch: 600, loss is 4.59154896736145 and perplexity is 98.64711303942106
At time: 376.5604751110077 and batch: 650, loss is 4.4916652488708495 and perplexity is 89.26997890367849
At time: 377.18527722358704 and batch: 700, loss is 4.533151159286499 and perplexity is 93.05131910459606
At time: 377.80571937561035 and batch: 750, loss is 4.574102039337158 and perplexity is 96.9409508813614
At time: 378.4316830635071 and batch: 800, loss is 4.54573959350586 and perplexity is 94.23009340903867
At time: 379.05756855010986 and batch: 850, loss is 4.595244817733764 and perplexity is 99.01237256538707
At time: 379.68475890159607 and batch: 900, loss is 4.582372102737427 and perplexity is 97.74598293602673
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01408908791738 and perplexity of 150.5189647559216
finished 31 epochs...
Completing Train Step...
At time: 381.31831908226013 and batch: 50, loss is 4.688871908187866 and perplexity is 108.73045266330881
At time: 381.9439506530762 and batch: 100, loss is 4.609167251586914 and perplexity is 100.4005064519376
At time: 382.5679295063019 and batch: 150, loss is 4.624364175796509 and perplexity is 101.93793786483512
At time: 383.1914002895355 and batch: 200, loss is 4.526460247039795 and perplexity is 92.43079913133869
At time: 383.83098125457764 and batch: 250, loss is 4.665213079452514 and perplexity is 106.18820933703155
At time: 384.4677948951721 and batch: 300, loss is 4.642615003585815 and perplexity is 103.81547076304537
At time: 385.10243916511536 and batch: 350, loss is 4.63175048828125 and perplexity is 102.69367093233794
At time: 385.7354156970978 and batch: 400, loss is 4.565008630752564 and perplexity is 96.06342311258607
At time: 386.3646750450134 and batch: 450, loss is 4.569099388122559 and perplexity is 96.45720014268319
At time: 387.0003423690796 and batch: 500, loss is 4.515985403060913 and perplexity is 91.46765413490667
At time: 387.62790727615356 and batch: 550, loss is 4.598680877685547 and perplexity is 99.35317017870297
At time: 388.2524118423462 and batch: 600, loss is 4.5912900733947755 and perplexity is 98.62157720270496
At time: 388.8759226799011 and batch: 650, loss is 4.491230907440186 and perplexity is 89.23121367260856
At time: 389.5016121864319 and batch: 700, loss is 4.533011207580566 and perplexity is 93.03829732497948
At time: 390.1205770969391 and batch: 750, loss is 4.5739086151123045 and perplexity is 96.92220196638623
At time: 390.75078320503235 and batch: 800, loss is 4.545994472503662 and perplexity is 94.25411374181874
At time: 391.37863874435425 and batch: 850, loss is 4.595274295806885 and perplexity is 99.01529130236452
At time: 392.0014407634735 and batch: 900, loss is 4.582111673355103 and perplexity is 97.72053032451383
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.014006732261344 and perplexity of 150.50656917826322
finished 32 epochs...
Completing Train Step...
At time: 393.5847144126892 and batch: 50, loss is 4.688546504974365 and perplexity is 108.69507718056678
At time: 394.2203345298767 and batch: 100, loss is 4.609139595031738 and perplexity is 100.3977297581883
At time: 394.8428885936737 and batch: 150, loss is 4.624364442825318 and perplexity is 101.93796508520488
At time: 395.46352529525757 and batch: 200, loss is 4.526206769943237 and perplexity is 92.40737300986042
At time: 396.0872824192047 and batch: 250, loss is 4.665142192840576 and perplexity is 106.18068228143072
At time: 396.7104334831238 and batch: 300, loss is 4.642439985275269 and perplexity is 103.79730274465815
At time: 397.3325459957123 and batch: 350, loss is 4.63131706237793 and perplexity is 102.64917047976984
At time: 397.95475482940674 and batch: 400, loss is 4.564594755172729 and perplexity is 96.02367303400528
At time: 398.57686161994934 and batch: 450, loss is 4.568884210586548 and perplexity is 96.4364469529162
At time: 399.19970512390137 and batch: 500, loss is 4.51593240737915 and perplexity is 91.46280687265964
At time: 399.8478503227234 and batch: 550, loss is 4.598447179794311 and perplexity is 99.32995426520526
At time: 400.4718360900879 and batch: 600, loss is 4.59105263710022 and perplexity is 98.59816364057545
At time: 401.093416929245 and batch: 650, loss is 4.490835599899292 and perplexity is 89.19594687203599
At time: 401.7161982059479 and batch: 700, loss is 4.53290714263916 and perplexity is 93.02861580378202
At time: 402.3342297077179 and batch: 750, loss is 4.573704519271851 and perplexity is 96.90242256663251
At time: 402.95565938949585 and batch: 800, loss is 4.546203117370606 and perplexity is 94.27378143054909
At time: 403.57840967178345 and batch: 850, loss is 4.5953170204162594 and perplexity is 99.0195217823797
At time: 404.2024302482605 and batch: 900, loss is 4.581837482452393 and perplexity is 97.69373991710172
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013944024909033 and perplexity of 150.49713160571022
finished 33 epochs...
Completing Train Step...
At time: 405.79519844055176 and batch: 50, loss is 4.688247117996216 and perplexity is 108.66254016069307
At time: 406.4322738647461 and batch: 100, loss is 4.609092559814453 and perplexity is 100.39300764020797
At time: 407.05644607543945 and batch: 150, loss is 4.624387140274048 and perplexity is 101.94027884319918
At time: 407.6764233112335 and batch: 200, loss is 4.525974111557007 and perplexity is 92.38587616038828
At time: 408.2962908744812 and batch: 250, loss is 4.665054626464844 and perplexity is 106.17138483098856
At time: 408.9273524284363 and batch: 300, loss is 4.642279529571534 and perplexity is 103.78064921151335
At time: 409.55345606803894 and batch: 350, loss is 4.63089506149292 and perplexity is 102.60586157782205
At time: 410.1734194755554 and batch: 400, loss is 4.564227600097656 and perplexity is 95.988423926464
At time: 410.79586601257324 and batch: 450, loss is 4.568697280883789 and perplexity is 96.4184218013228
At time: 411.4200756549835 and batch: 500, loss is 4.5158712196350095 and perplexity is 91.45721064104646
At time: 412.0406470298767 and batch: 550, loss is 4.598179302215576 and perplexity is 99.30334956112179
At time: 412.66323375701904 and batch: 600, loss is 4.590821151733398 and perplexity is 98.57534225000805
At time: 413.28350496292114 and batch: 650, loss is 4.490433855056763 and perplexity is 89.1601200575065
At time: 413.904155254364 and batch: 700, loss is 4.532763862609864 and perplexity is 93.01528761583864
At time: 414.5201427936554 and batch: 750, loss is 4.573476543426514 and perplexity is 96.88033367289549
At time: 415.14083075523376 and batch: 800, loss is 4.546377544403076 and perplexity is 94.29022676069714
At time: 415.77988934516907 and batch: 850, loss is 4.595352373123169 and perplexity is 99.02302245239031
At time: 416.4052610397339 and batch: 900, loss is 4.581598482131958 and perplexity is 97.67039387192432
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013902638056507 and perplexity of 150.4909031320083
finished 34 epochs...
Completing Train Step...
At time: 417.996693611145 and batch: 50, loss is 4.687947559356689 and perplexity is 108.62999423294532
At time: 418.64888620376587 and batch: 100, loss is 4.609067420959473 and perplexity is 100.39048390666991
At time: 419.2804355621338 and batch: 150, loss is 4.6243776512145995 and perplexity is 101.9393115304225
At time: 419.90476298332214 and batch: 200, loss is 4.5257447242736815 and perplexity is 92.36468644565667
At time: 420.53110432624817 and batch: 250, loss is 4.664967727661133 and perplexity is 106.1621590655182
At time: 421.1575162410736 and batch: 300, loss is 4.642106370925903 and perplexity is 103.76268025063862
At time: 421.7833185195923 and batch: 350, loss is 4.63045599937439 and perplexity is 102.56082111936722
At time: 422.4078860282898 and batch: 400, loss is 4.5638398265838624 and perplexity is 95.95120937390976
At time: 423.02967953681946 and batch: 450, loss is 4.568488311767578 and perplexity is 96.39827543399011
At time: 423.651061296463 and batch: 500, loss is 4.515797348022461 and perplexity is 91.45045479895273
At time: 424.2709422111511 and batch: 550, loss is 4.597915153503418 and perplexity is 99.27712217334012
At time: 424.89093041419983 and batch: 600, loss is 4.590561304092407 and perplexity is 98.54973100751909
At time: 425.51162815093994 and batch: 650, loss is 4.490083532333374 and perplexity is 89.12889071192438
At time: 426.1328365802765 and batch: 700, loss is 4.532608108520508 and perplexity is 93.00080123260588
At time: 426.76399064064026 and batch: 750, loss is 4.573222227096558 and perplexity is 96.85569855468009
At time: 427.3904666900635 and batch: 800, loss is 4.546448011398315 and perplexity is 94.29687134376651
At time: 428.01221537590027 and batch: 850, loss is 4.595290269851684 and perplexity is 99.01687298969658
At time: 428.6330072879791 and batch: 900, loss is 4.581349973678589 and perplexity is 97.64612496904189
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013899711713399 and perplexity of 150.4904627446354
finished 35 epochs...
Completing Train Step...
At time: 430.2128484249115 and batch: 50, loss is 4.687688961029052 and perplexity is 108.60190633000437
At time: 430.85018157958984 and batch: 100, loss is 4.608908023834228 and perplexity is 100.37448322739829
At time: 431.46868348121643 and batch: 150, loss is 4.624392747879028 and perplexity is 101.94085048561729
At time: 432.10214829444885 and batch: 200, loss is 4.525457096099854 and perplexity is 92.33812357986555
At time: 432.7202889919281 and batch: 250, loss is 4.664803457260132 and perplexity is 106.14472119737938
At time: 433.3401868343353 and batch: 300, loss is 4.641936855316162 and perplexity is 103.74509234738174
At time: 433.9593403339386 and batch: 350, loss is 4.63007550239563 and perplexity is 102.52180446012335
At time: 434.57814931869507 and batch: 400, loss is 4.563489055633545 and perplexity is 95.91755837925426
At time: 435.19690346717834 and batch: 450, loss is 4.568331089019775 and perplexity is 96.38312062361459
At time: 435.81738209724426 and batch: 500, loss is 4.515644435882568 and perplexity is 91.43647198331367
At time: 436.43917894363403 and batch: 550, loss is 4.5976043319702145 and perplexity is 99.24626950109996
At time: 437.0620422363281 and batch: 600, loss is 4.590378932952881 and perplexity is 98.53176001951978
At time: 437.68479919433594 and batch: 650, loss is 4.489783544540405 and perplexity is 89.10215714278262
At time: 438.30926418304443 and batch: 700, loss is 4.532393922805786 and perplexity is 92.98088392260199
At time: 438.9291024208069 and batch: 750, loss is 4.573024635314941 and perplexity is 96.83656255526331
At time: 439.5519244670868 and batch: 800, loss is 4.5465413856506345 and perplexity is 94.30567665471257
At time: 440.17525267601013 and batch: 850, loss is 4.595274829864502 and perplexity is 99.01534418224922
At time: 440.798704624176 and batch: 900, loss is 4.581140012741089 and perplexity is 97.62562524924564
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0138917687821065 and perplexity of 150.48926741397693
finished 36 epochs...
Completing Train Step...
At time: 442.4092061519623 and batch: 50, loss is 4.687450046539307 and perplexity is 108.57596286022695
At time: 443.0355906486511 and batch: 100, loss is 4.6088871574401855 and perplexity is 100.37238879573111
At time: 443.66055965423584 and batch: 150, loss is 4.624357881546021 and perplexity is 101.93729624393924
At time: 444.2828800678253 and batch: 200, loss is 4.525243797302246 and perplexity is 92.31843006950834
At time: 444.90641379356384 and batch: 250, loss is 4.66470778465271 and perplexity is 106.13456654090731
At time: 445.52753472328186 and batch: 300, loss is 4.641769599914551 and perplexity is 103.72774187131685
At time: 446.14867758750916 and batch: 350, loss is 4.629699916839599 and perplexity is 102.48330598137879
At time: 446.76907324790955 and batch: 400, loss is 4.563135385513306 and perplexity is 95.8836412029487
At time: 447.3917133808136 and batch: 450, loss is 4.568144626617432 and perplexity is 96.36515047082875
At time: 448.02720046043396 and batch: 500, loss is 4.515564098358154 and perplexity is 91.42912649857634
At time: 448.65167927742004 and batch: 550, loss is 4.5973881816864015 and perplexity is 99.22481971005264
At time: 449.2802367210388 and batch: 600, loss is 4.590148153305054 and perplexity is 98.50902351830453
At time: 449.9014232158661 and batch: 650, loss is 4.48946442604065 and perplexity is 89.07372753251882
At time: 450.5317118167877 and batch: 700, loss is 4.532237606048584 and perplexity is 92.96635058827684
At time: 451.15566539764404 and batch: 750, loss is 4.572804841995239 and perplexity is 96.81528086458317
At time: 451.77686405181885 and batch: 800, loss is 4.546636877059936 and perplexity is 94.31468246666347
At time: 452.39722895622253 and batch: 850, loss is 4.595238599777222 and perplexity is 99.01175691267133
At time: 453.01808071136475 and batch: 900, loss is 4.580918550491333 and perplexity is 97.6040072525176
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013866685841181 and perplexity of 150.48549274791256
finished 37 epochs...
Completing Train Step...
At time: 454.61869597435 and batch: 50, loss is 4.687209615707397 and perplexity is 108.54986098912413
At time: 455.2414081096649 and batch: 100, loss is 4.608852205276489 and perplexity is 100.36888062487674
At time: 455.8636910915375 and batch: 150, loss is 4.624381370544434 and perplexity is 101.93969067705025
At time: 456.4828186035156 and batch: 200, loss is 4.525038557052612 and perplexity is 92.29948455613253
At time: 457.1046929359436 and batch: 250, loss is 4.664614925384521 and perplexity is 106.12471142030553
At time: 457.72672939300537 and batch: 300, loss is 4.641614999771118 and perplexity is 103.71170678709068
At time: 458.35098361968994 and batch: 350, loss is 4.6293423557281494 and perplexity is 102.44666848704897
At time: 458.9746947288513 and batch: 400, loss is 4.562805528640747 and perplexity is 95.85201854069493
At time: 459.59937357902527 and batch: 450, loss is 4.567976732254028 and perplexity is 96.34897266335547
At time: 460.225136756897 and batch: 500, loss is 4.515474891662597 and perplexity is 91.42097077210188
At time: 460.8499929904938 and batch: 550, loss is 4.597142505645752 and perplexity is 99.20044554340907
At time: 461.47484040260315 and batch: 600, loss is 4.589930582046509 and perplexity is 98.48759311748374
At time: 462.09896326065063 and batch: 650, loss is 4.489140663146973 and perplexity is 89.04489343269996
At time: 462.7243537902832 and batch: 700, loss is 4.532057905197144 and perplexity is 92.94964595688404
At time: 463.34473156929016 and batch: 750, loss is 4.572598543167114 and perplexity is 96.7953100456453
At time: 463.98364996910095 and batch: 800, loss is 4.546738805770874 and perplexity is 94.3242963306259
At time: 464.60844349861145 and batch: 850, loss is 4.595192575454712 and perplexity is 99.00720006850258
At time: 465.2337827682495 and batch: 900, loss is 4.580717687606811 and perplexity is 97.58440419890884
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013847873635488 and perplexity of 150.48266181049726
finished 38 epochs...
Completing Train Step...
At time: 466.83552503585815 and batch: 50, loss is 4.68697714805603 and perplexity is 108.52462959073937
At time: 467.46015548706055 and batch: 100, loss is 4.608852157592773 and perplexity is 100.36887583891568
At time: 468.0845010280609 and batch: 150, loss is 4.624347810745239 and perplexity is 101.93626965890581
At time: 468.7067394256592 and batch: 200, loss is 4.524844312667847 and perplexity is 92.28155764069845
At time: 469.329017162323 and batch: 250, loss is 4.664520864486694 and perplexity is 106.11472970411965
At time: 469.9599528312683 and batch: 300, loss is 4.64147292137146 and perplexity is 103.69697264049137
At time: 470.5854637622833 and batch: 350, loss is 4.628973827362061 and perplexity is 102.40892093964841
At time: 471.2081608772278 and batch: 400, loss is 4.56246787071228 and perplexity is 95.8196588102423
At time: 471.82998061180115 and batch: 450, loss is 4.567788352966309 and perplexity is 96.33082422196158
At time: 472.4533779621124 and batch: 500, loss is 4.515383739471435 and perplexity is 91.412637930082
At time: 473.0752754211426 and batch: 550, loss is 4.59693060874939 and perplexity is 99.17942750378803
At time: 473.6984407901764 and batch: 600, loss is 4.589725608825684 and perplexity is 98.46740786709972
At time: 474.3201608657837 and batch: 650, loss is 4.488833093643189 and perplexity is 89.01751015035953
At time: 474.9404513835907 and batch: 700, loss is 4.531897134780884 and perplexity is 92.93470360478861
At time: 475.5574769973755 and batch: 750, loss is 4.572392587661743 and perplexity is 96.77537657142213
At time: 476.1800172328949 and batch: 800, loss is 4.546817388534546 and perplexity is 94.33170888575864
At time: 476.8027081489563 and batch: 850, loss is 4.595154094696045 and perplexity is 99.00339026963285
At time: 477.42463278770447 and batch: 900, loss is 4.580498714447021 and perplexity is 97.56303817295364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013832405821918 and perplexity of 150.48033419074056
finished 39 epochs...
Completing Train Step...
At time: 479.03214502334595 and batch: 50, loss is 4.686734571456909 and perplexity is 108.4983072478936
At time: 479.6687915325165 and batch: 100, loss is 4.608807401657105 and perplexity is 100.36438383648814
At time: 480.2901167869568 and batch: 150, loss is 4.624350290298462 and perplexity is 101.93652241562516
At time: 480.9113585948944 and batch: 200, loss is 4.524656152725219 and perplexity is 92.26419558158044
At time: 481.539470911026 and batch: 250, loss is 4.664404497146607 and perplexity is 106.10238213372078
At time: 482.1701691150665 and batch: 300, loss is 4.641332454681397 and perplexity is 103.68240769294388
At time: 482.7913293838501 and batch: 350, loss is 4.628630895614624 and perplexity is 102.37380769050547
At time: 483.41375732421875 and batch: 400, loss is 4.562151975631714 and perplexity is 95.78939463180687
At time: 484.0351753234863 and batch: 450, loss is 4.567624425888061 and perplexity is 96.31503428563612
At time: 484.6577248573303 and batch: 500, loss is 4.515294561386108 and perplexity is 91.40448628953597
At time: 485.2773869037628 and batch: 550, loss is 4.596693725585937 and perplexity is 99.1559363496908
At time: 485.89735531806946 and batch: 600, loss is 4.589520778656006 and perplexity is 98.4472408367172
At time: 486.51811385154724 and batch: 650, loss is 4.488539810180664 and perplexity is 88.99140661481205
At time: 487.1429092884064 and batch: 700, loss is 4.531731262207031 and perplexity is 92.91928956472
At time: 487.7649269104004 and batch: 750, loss is 4.5721937274932865 and perplexity is 96.75613371711673
At time: 488.3892343044281 and batch: 800, loss is 4.546903915405274 and perplexity is 94.33987146647513
At time: 489.01563453674316 and batch: 850, loss is 4.59511890411377 and perplexity is 98.99990634398311
At time: 489.64208245277405 and batch: 900, loss is 4.580302314758301 and perplexity is 97.5438787041443
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.013834496066995 and perplexity of 150.48064873184697
Annealing...
finished 40 epochs...
Completing Train Step...
At time: 491.22685980796814 and batch: 50, loss is 4.686403284072876 and perplexity is 108.4623690807728
At time: 491.86631894111633 and batch: 100, loss is 4.608426609039307 and perplexity is 100.32617309567911
At time: 492.49179673194885 and batch: 150, loss is 4.624679355621338 and perplexity is 101.97007170993865
At time: 493.11582589149475 and batch: 200, loss is 4.524638280868531 and perplexity is 92.26254666383416
At time: 493.74107217788696 and batch: 250, loss is 4.663349628448486 and perplexity is 105.99051706386534
At time: 494.3643202781677 and batch: 300, loss is 4.6392564868927 and perplexity is 103.4673896168397
At time: 494.9892475605011 and batch: 350, loss is 4.627028312683105 and perplexity is 102.20987656536266
At time: 495.62254095077515 and batch: 400, loss is 4.560326881408692 and perplexity is 95.61472939977422
At time: 496.246470451355 and batch: 450, loss is 4.566124963760376 and perplexity is 96.17072176201914
At time: 496.87678575515747 and batch: 500, loss is 4.5134458827972415 and perplexity is 91.23566486915924
At time: 497.50075912475586 and batch: 550, loss is 4.594731731414795 and perplexity is 98.96158370226468
At time: 498.12496614456177 and batch: 600, loss is 4.5870245170593265 and perplexity is 98.20179724327885
At time: 498.750910282135 and batch: 650, loss is 4.486388492584228 and perplexity is 88.80016362179677
At time: 499.3746302127838 and batch: 700, loss is 4.529240474700928 and perplexity is 92.68813535671003
At time: 500.00367498397827 and batch: 750, loss is 4.569458770751953 and perplexity is 96.49187141464776
At time: 500.6285090446472 and batch: 800, loss is 4.544745149612427 and perplexity is 94.13643344557833
At time: 501.254362821579 and batch: 850, loss is 4.59332615852356 and perplexity is 98.82258369314589
At time: 501.88812351226807 and batch: 900, loss is 4.5793165111541745 and perplexity is 97.44776697838364
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012894303831335 and perplexity of 150.33923448299873
finished 41 epochs...
Completing Train Step...
At time: 503.5354833602905 and batch: 50, loss is 4.686447076797485 and perplexity is 108.46711904743867
At time: 504.1725854873657 and batch: 100, loss is 4.607918968200684 and perplexity is 100.27525635783418
At time: 504.79423475265503 and batch: 150, loss is 4.6240965843200685 and perplexity is 101.9106637908542
At time: 505.41695952415466 and batch: 200, loss is 4.52418173789978 and perplexity is 92.22043446062234
At time: 506.03933668136597 and batch: 250, loss is 4.663034229278565 and perplexity is 105.95709301399971
At time: 506.6603593826294 and batch: 300, loss is 4.638936977386475 and perplexity is 103.43433608301316
At time: 507.2829341888428 and batch: 350, loss is 4.626943187713623 and perplexity is 102.20117632304844
At time: 507.90519094467163 and batch: 400, loss is 4.56022310256958 and perplexity is 95.6048071290249
At time: 508.52811002731323 and batch: 450, loss is 4.565953159332276 and perplexity is 96.1542006254097
At time: 509.14967370033264 and batch: 500, loss is 4.513330020904541 and perplexity is 91.22509474469486
At time: 509.77306747436523 and batch: 550, loss is 4.5946620655059816 and perplexity is 98.95468969373992
At time: 510.39843702316284 and batch: 600, loss is 4.586994304656982 and perplexity is 98.19883037588791
At time: 511.02268958091736 and batch: 650, loss is 4.486494750976562 and perplexity is 88.80959988575445
At time: 511.66008019447327 and batch: 700, loss is 4.52938307762146 and perplexity is 92.70135389798942
At time: 512.2789211273193 and batch: 750, loss is 4.569399499893189 and perplexity is 96.48615242805155
At time: 512.8984470367432 and batch: 800, loss is 4.5447459888458255 and perplexity is 94.13651244805044
At time: 513.5238544940948 and batch: 850, loss is 4.593442916870117 and perplexity is 98.83412272824664
At time: 514.1492402553558 and batch: 900, loss is 4.579473848342896 and perplexity is 97.46310034230973
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0128412116063785 and perplexity of 150.33125285042493
finished 42 epochs...
Completing Train Step...
At time: 515.7406930923462 and batch: 50, loss is 4.686330795288086 and perplexity is 108.45450706040032
At time: 516.3764078617096 and batch: 100, loss is 4.607651977539063 and perplexity is 100.24848737448828
At time: 516.997810125351 and batch: 150, loss is 4.6240341854095455 and perplexity is 101.90430487485975
At time: 517.6199855804443 and batch: 200, loss is 4.524146518707275 and perplexity is 92.21718658858228
At time: 518.2397749423981 and batch: 250, loss is 4.662811765670776 and perplexity is 105.93352403853389
At time: 518.8580706119537 and batch: 300, loss is 4.638672590255737 and perplexity is 103.4069929904166
At time: 519.479216337204 and batch: 350, loss is 4.6268805885314945 and perplexity is 102.19477881323957
At time: 520.0979108810425 and batch: 400, loss is 4.560119466781616 and perplexity is 95.59489956290304
At time: 520.7258775234222 and batch: 450, loss is 4.565827417373657 and perplexity is 96.14211076801074
At time: 521.345939874649 and batch: 500, loss is 4.513170680999756 and perplexity is 91.21056010478904
At time: 521.9670503139496 and batch: 550, loss is 4.594620838165283 and perplexity is 98.95061013912937
At time: 522.589262008667 and batch: 600, loss is 4.587005910873413 and perplexity is 98.19997009938041
At time: 523.2126626968384 and batch: 650, loss is 4.486590232849121 and perplexity is 88.81807999749476
At time: 523.8345293998718 and batch: 700, loss is 4.529463005065918 and perplexity is 92.70876357641872
At time: 524.4509780406952 and batch: 750, loss is 4.569342412948608 and perplexity is 96.48064448563242
At time: 525.0678522586823 and batch: 800, loss is 4.544736595153808 and perplexity is 94.13562816279828
At time: 525.6878747940063 and batch: 850, loss is 4.5935326671600345 and perplexity is 98.84299351748722
At time: 526.309291601181 and batch: 900, loss is 4.579603538513184 and perplexity is 97.47574116806766
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012823235498716 and perplexity of 150.3285505039275
finished 43 epochs...
Completing Train Step...
At time: 527.9028966426849 and batch: 50, loss is 4.686248559951782 and perplexity is 108.44558863424841
At time: 528.5290150642395 and batch: 100, loss is 4.60741268157959 and perplexity is 100.22450118652972
At time: 529.154458284378 and batch: 150, loss is 4.623997106552124 and perplexity is 101.90052644971891
At time: 529.7789061069489 and batch: 200, loss is 4.524116344451905 and perplexity is 92.21440404562537
At time: 530.4159138202667 and batch: 250, loss is 4.662610826492309 and perplexity is 105.91223998171353
At time: 531.0414457321167 and batch: 300, loss is 4.638449411392212 and perplexity is 103.38391731033826
At time: 531.6659314632416 and batch: 350, loss is 4.626822261810303 and perplexity is 102.18881830069877
At time: 532.2897846698761 and batch: 400, loss is 4.5600149536132815 and perplexity is 95.58490915914655
At time: 532.9127209186554 and batch: 450, loss is 4.565712041854859 and perplexity is 96.13101896197625
At time: 533.5360896587372 and batch: 500, loss is 4.513027124404907 and perplexity is 91.1974671671774
At time: 534.1602628231049 and batch: 550, loss is 4.594586448669434 and perplexity is 98.94720733604338
At time: 534.7885191440582 and batch: 600, loss is 4.587016458511353 and perplexity is 98.20100588257326
At time: 535.4149751663208 and batch: 650, loss is 4.486673517227173 and perplexity is 88.82547746408954
At time: 536.0398843288422 and batch: 700, loss is 4.529528932571411 and perplexity is 92.71487583541943
At time: 536.6620643138885 and batch: 750, loss is 4.569285469055176 and perplexity is 96.47515065851596
At time: 537.2841329574585 and batch: 800, loss is 4.544726190567016 and perplexity is 94.13464872558016
At time: 537.9048192501068 and batch: 850, loss is 4.593609104156494 and perplexity is 98.85054906779087
At time: 538.5362524986267 and batch: 900, loss is 4.579723930358886 and perplexity is 97.48747715890272
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012814456469392 and perplexity of 150.3272307709674
finished 44 epochs...
Completing Train Step...
At time: 540.1406309604645 and batch: 50, loss is 4.686174325942993 and perplexity is 108.43753858226613
At time: 540.7646942138672 and batch: 100, loss is 4.607207822799682 and perplexity is 100.203971420423
At time: 541.3851780891418 and batch: 150, loss is 4.623954725265503 and perplexity is 101.89620786581487
At time: 542.0063056945801 and batch: 200, loss is 4.524089040756226 and perplexity is 92.21188628597235
At time: 542.6277046203613 and batch: 250, loss is 4.6624247837066655 and perplexity is 105.89253760635279
At time: 543.2468540668488 and batch: 300, loss is 4.638249235153198 and perplexity is 103.36322437778234
At time: 543.8815217018127 and batch: 350, loss is 4.626767168045044 and perplexity is 102.18318848901637
At time: 544.5022926330566 and batch: 400, loss is 4.559912462234497 and perplexity is 95.57511303203367
At time: 545.1301717758179 and batch: 450, loss is 4.565601921081543 and perplexity is 96.12043352267757
At time: 545.7517399787903 and batch: 500, loss is 4.512893743515015 and perplexity is 91.1853039790372
At time: 546.3750114440918 and batch: 550, loss is 4.5945541954040525 and perplexity is 98.94401601697196
At time: 546.9979484081268 and batch: 600, loss is 4.587025194168091 and perplexity is 98.20186373659897
At time: 547.6347723007202 and batch: 650, loss is 4.486748094558716 and perplexity is 88.83210207819181
At time: 548.262143611908 and batch: 700, loss is 4.529586982727051 and perplexity is 92.72025810461108
At time: 548.8814654350281 and batch: 750, loss is 4.569233198165893 and perplexity is 96.47010794839197
At time: 549.5002369880676 and batch: 800, loss is 4.544718208312989 and perplexity is 94.13389732190019
At time: 550.1316959857941 and batch: 850, loss is 4.593678712844849 and perplexity is 98.85743016434388
At time: 550.7567639350891 and batch: 900, loss is 4.579834852218628 and perplexity is 97.49829125091922
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012808185734161 and perplexity of 150.32628811166086
finished 45 epochs...
Completing Train Step...
At time: 552.3463082313538 and batch: 50, loss is 4.686098947525024 and perplexity is 108.42936504021758
At time: 552.9669458866119 and batch: 100, loss is 4.6070279788970945 and perplexity is 100.18595196754087
At time: 553.6016478538513 and batch: 150, loss is 4.623910856246948 and perplexity is 101.89173787722908
At time: 554.2287490367889 and batch: 200, loss is 4.5240606021881105 and perplexity is 92.20926394925107
At time: 554.8543992042542 and batch: 250, loss is 4.66224720954895 and perplexity is 105.87373549761276
At time: 555.4778807163239 and batch: 300, loss is 4.638061532974243 and perplexity is 103.343824696081
At time: 556.1034896373749 and batch: 350, loss is 4.626713533401489 and perplexity is 102.17770807709574
At time: 556.7280633449554 and batch: 400, loss is 4.559811573028565 and perplexity is 95.56547102116862
At time: 557.3547549247742 and batch: 450, loss is 4.565496606826782 and perplexity is 96.11031120387547
At time: 557.978090763092 and batch: 500, loss is 4.512767324447632 and perplexity is 91.17377714657016
At time: 558.6030876636505 and batch: 550, loss is 4.594522333145141 and perplexity is 98.94086348733953
At time: 559.2336361408234 and batch: 600, loss is 4.587031555175781 and perplexity is 98.20248840139615
At time: 559.8820250034332 and batch: 650, loss is 4.486815013885498 and perplexity is 88.83804686156776
At time: 560.5071458816528 and batch: 700, loss is 4.529638118743897 and perplexity is 92.72499957052027
At time: 561.1304960250854 and batch: 750, loss is 4.569182271957398 and perplexity is 96.46519521665547
At time: 561.7512395381927 and batch: 800, loss is 4.544712066650391 and perplexity is 94.13331918503917
At time: 562.3841879367828 and batch: 850, loss is 4.593744373321533 and perplexity is 98.86392140343885
At time: 563.0131013393402 and batch: 900, loss is 4.579938955307007 and perplexity is 97.50844165248503
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012800660851884 and perplexity of 150.32515692829566
finished 46 epochs...
Completing Train Step...
At time: 564.614682674408 and batch: 50, loss is 4.6860269260406495 and perplexity is 108.4215560776075
At time: 565.2514653205872 and batch: 100, loss is 4.606862115859985 and perplexity is 100.16933619928083
At time: 565.8731179237366 and batch: 150, loss is 4.623868217468262 and perplexity is 101.88739343058936
At time: 566.4948303699493 and batch: 200, loss is 4.52403263092041 and perplexity is 92.20668477531625
At time: 567.1165177822113 and batch: 250, loss is 4.662076997756958 and perplexity is 105.85571607297155
At time: 567.7367854118347 and batch: 300, loss is 4.63788330078125 and perplexity is 103.32540714092254
At time: 568.357138633728 and batch: 350, loss is 4.626659879684448 and perplexity is 102.17222601032664
At time: 568.986780166626 and batch: 400, loss is 4.559712352752686 and perplexity is 95.55598945915877
At time: 569.6093535423279 and batch: 450, loss is 4.565395021438599 and perplexity is 96.10054829649619
At time: 570.227373123169 and batch: 500, loss is 4.512645864486695 and perplexity is 91.16270385565382
At time: 570.8542091846466 and batch: 550, loss is 4.594490756988526 and perplexity is 98.93773936446252
At time: 571.4819066524506 and batch: 600, loss is 4.587034978866577 and perplexity is 98.20282461692736
At time: 572.1083254814148 and batch: 650, loss is 4.486876068115234 and perplexity is 88.8434709656707
At time: 572.7312273979187 and batch: 700, loss is 4.529683084487915 and perplexity is 92.7291691128576
At time: 573.3504791259766 and batch: 750, loss is 4.56913200378418 and perplexity is 96.46034620938923
At time: 573.9679696559906 and batch: 800, loss is 4.544707078933715 and perplexity is 94.13284967588426
At time: 574.589097738266 and batch: 850, loss is 4.593806667327881 and perplexity is 98.87008022501318
At time: 575.2102954387665 and batch: 900, loss is 4.58003722190857 and perplexity is 97.5180239464719
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012794390116652 and perplexity of 150.32421428199348
finished 47 epochs...
Completing Train Step...
At time: 576.7977356910706 and batch: 50, loss is 4.685957555770874 and perplexity is 108.41403510588184
At time: 577.4411866664886 and batch: 100, loss is 4.606706533432007 and perplexity is 100.15375282302718
At time: 578.0630147457123 and batch: 150, loss is 4.623827915191651 and perplexity is 101.88328721942152
At time: 578.6850666999817 and batch: 200, loss is 4.524005355834961 and perplexity is 92.20416986440742
At time: 579.3066782951355 and batch: 250, loss is 4.661913385391236 and perplexity is 105.83839818558852
At time: 579.9318211078644 and batch: 300, loss is 4.63771318435669 and perplexity is 103.30783128710657
At time: 580.5570089817047 and batch: 350, loss is 4.626607284545899 and perplexity is 102.16685238925815
At time: 581.1881890296936 and batch: 400, loss is 4.559613819122315 and perplexity is 95.54657444446903
At time: 581.8074278831482 and batch: 450, loss is 4.565295667648315 and perplexity is 96.09100081707156
At time: 582.4355878829956 and batch: 500, loss is 4.512528800964356 and perplexity is 91.15203265305101
At time: 583.0702800750732 and batch: 550, loss is 4.594459819793701 and perplexity is 98.93467855569097
At time: 583.7057869434357 and batch: 600, loss is 4.5870363140106205 and perplexity is 98.20295573193121
At time: 584.3401381969452 and batch: 650, loss is 4.486932029724121 and perplexity is 88.8484429283633
At time: 584.9658041000366 and batch: 700, loss is 4.529722986221313 and perplexity is 92.73286924126208
At time: 585.5888378620148 and batch: 750, loss is 4.569082546234131 and perplexity is 96.45557563496025
At time: 586.2118954658508 and batch: 800, loss is 4.544702777862549 and perplexity is 94.13244480466936
At time: 586.8375866413116 and batch: 850, loss is 4.593866558074951 and perplexity is 98.8760018053029
At time: 587.4640567302704 and batch: 900, loss is 4.580130262374878 and perplexity is 97.5270974909902
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012788537430437 and perplexity of 150.32333448411126
finished 48 epochs...
Completing Train Step...
At time: 589.0751802921295 and batch: 50, loss is 4.6858895778656 and perplexity is 108.40666559735779
At time: 589.7120244503021 and batch: 100, loss is 4.606558580398559 and perplexity is 100.13893586761961
At time: 590.3365888595581 and batch: 150, loss is 4.623789081573486 and perplexity is 101.87933079956981
At time: 590.9590895175934 and batch: 200, loss is 4.523979368209839 and perplexity is 92.20177372814136
At time: 591.5813660621643 and batch: 250, loss is 4.66175555229187 and perplexity is 105.82169470138705
At time: 592.2174007892609 and batch: 300, loss is 4.637549457550048 and perplexity is 103.29091841037233
At time: 592.8389229774475 and batch: 350, loss is 4.626554689407349 and perplexity is 102.16147905080848
At time: 593.4603326320648 and batch: 400, loss is 4.559516706466675 and perplexity is 95.53729611341643
At time: 594.081787109375 and batch: 450, loss is 4.565198316574096 and perplexity is 96.08164671024282
At time: 594.7033815383911 and batch: 500, loss is 4.512414836883545 and perplexity is 91.14164518734593
At time: 595.3256447315216 and batch: 550, loss is 4.594430160522461 and perplexity is 98.93174426873927
At time: 595.9468724727631 and batch: 600, loss is 4.587036247253418 and perplexity is 98.20294917617684
At time: 596.5758516788483 and batch: 650, loss is 4.486983280181885 and perplexity is 88.85299656842204
At time: 597.2004618644714 and batch: 700, loss is 4.5297583484649655 and perplexity is 92.73614854156014
At time: 597.832985162735 and batch: 750, loss is 4.569033737182617 and perplexity is 96.45086784469265
At time: 598.4568617343903 and batch: 800, loss is 4.544697999954224 and perplexity is 94.13199504955213
At time: 599.0843212604523 and batch: 850, loss is 4.59392409324646 and perplexity is 98.88169081668244
At time: 599.7127356529236 and batch: 900, loss is 4.580218935012818 and perplexity is 97.5357458594265
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012781430597174 and perplexity of 150.32226616503382
finished 49 epochs...
Completing Train Step...
At time: 601.3149461746216 and batch: 50, loss is 4.685822992324829 and perplexity is 108.39944752121824
At time: 601.9596145153046 and batch: 100, loss is 4.606417388916015 and perplexity is 100.12479810089371
At time: 602.5845420360565 and batch: 150, loss is 4.623751831054688 and perplexity is 101.87553581232575
At time: 603.2095320224762 and batch: 200, loss is 4.523954200744629 and perplexity is 92.19945327240883
At time: 603.838963508606 and batch: 250, loss is 4.661602001190186 and perplexity is 105.80544691104852
At time: 604.4681589603424 and batch: 300, loss is 4.637391366958618 and perplexity is 103.27459037867955
At time: 605.0903434753418 and batch: 350, loss is 4.626502141952515 and perplexity is 102.15611086614578
At time: 605.7099947929382 and batch: 400, loss is 4.559420776367188 and perplexity is 95.52813165067647
At time: 606.3293344974518 and batch: 450, loss is 4.5651030921936036 and perplexity is 96.07249783056322
At time: 606.9504399299622 and batch: 500, loss is 4.5123036670684815 and perplexity is 91.13151355068236
At time: 607.5857236385345 and batch: 550, loss is 4.59440131187439 and perplexity is 98.92889026283308
At time: 608.2209589481354 and batch: 600, loss is 4.587034721374511 and perplexity is 98.20279933048243
At time: 608.8433246612549 and batch: 650, loss is 4.487029981613159 and perplexity is 88.85714622743154
At time: 609.4720540046692 and batch: 700, loss is 4.529789123535156 and perplexity is 92.73900254695661
At time: 610.0952816009521 and batch: 750, loss is 4.568985204696656 and perplexity is 96.44618695789146
At time: 610.713663816452 and batch: 800, loss is 4.544692659378052 and perplexity is 94.1314923318048
At time: 611.3347890377045 and batch: 850, loss is 4.59398115158081 and perplexity is 98.88733300222354
At time: 611.9569404125214 and batch: 900, loss is 4.5803040599823 and perplexity is 97.54404894021094
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012774323763913 and perplexity of 150.32119785354888
finished 50 epochs...
Completing Train Step...
At time: 613.5354306697845 and batch: 50, loss is 4.685757122039795 and perplexity is 108.39230745387415
At time: 614.1546802520752 and batch: 100, loss is 4.606282024383545 and perplexity is 100.11124567168991
At time: 614.7771804332733 and batch: 150, loss is 4.623715658187866 and perplexity is 101.87185074878646
At time: 615.3997960090637 and batch: 200, loss is 4.523929471969605 and perplexity is 92.19717332106183
At time: 616.0228040218353 and batch: 250, loss is 4.661452693939209 and perplexity is 105.78965056991531
At time: 616.6422438621521 and batch: 300, loss is 4.63723822593689 and perplexity is 103.25877601333531
At time: 617.262570142746 and batch: 350, loss is 4.62644907951355 and perplexity is 102.15069035756197
At time: 617.8816843032837 and batch: 400, loss is 4.559325714111328 and perplexity is 95.51905096260586
At time: 618.5020444393158 and batch: 450, loss is 4.565009965896606 and perplexity is 96.06355137117875
At time: 619.1198589801788 and batch: 500, loss is 4.512194366455078 and perplexity is 91.12155336468801
At time: 619.7384769916534 and batch: 550, loss is 4.594372644424438 and perplexity is 98.92605426447331
At time: 620.357768535614 and batch: 600, loss is 4.587032241821289 and perplexity is 98.2025558317168
At time: 620.9767451286316 and batch: 650, loss is 4.487073459625244 and perplexity is 88.86100964349527
At time: 621.5947651863098 and batch: 700, loss is 4.52981725692749 and perplexity is 92.74161164640117
At time: 622.2117114067078 and batch: 750, loss is 4.56893702507019 and perplexity is 96.44154032856714
At time: 622.8289964199066 and batch: 800, loss is 4.544686508178711 and perplexity is 94.13091331201203
At time: 623.4504718780518 and batch: 850, loss is 4.594038763046265 and perplexity is 98.89303021050334
At time: 624.1004922389984 and batch: 900, loss is 4.580385637283325 and perplexity is 97.5520066450342
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012772233518835 and perplexity of 150.3208836457334
finished 51 epochs...
Completing Train Step...
At time: 625.7120275497437 and batch: 50, loss is 4.685692014694214 and perplexity is 108.38525054818517
At time: 626.3323693275452 and batch: 100, loss is 4.606151094436646 and perplexity is 100.09813896965872
At time: 626.9645938873291 and batch: 150, loss is 4.623680667877197 and perplexity is 101.86828628344163
At time: 627.5950634479523 and batch: 200, loss is 4.523904943466187 and perplexity is 92.19491189011579
At time: 628.2218506336212 and batch: 250, loss is 4.661306400299072 and perplexity is 105.77417534883541
At time: 628.8418061733246 and batch: 300, loss is 4.637089204788208 and perplexity is 103.24338941841485
At time: 629.4645748138428 and batch: 350, loss is 4.626395750045776 and perplexity is 102.14524286086987
At time: 630.0857620239258 and batch: 400, loss is 4.559231376647949 and perplexity is 95.51004036265894
At time: 630.7174005508423 and batch: 450, loss is 4.564919137954712 and perplexity is 96.05482651275315
At time: 631.338837146759 and batch: 500, loss is 4.512086753845215 and perplexity is 91.1117480641121
At time: 631.9616005420685 and batch: 550, loss is 4.594343805313111 and perplexity is 98.92320136611892
At time: 632.5818295478821 and batch: 600, loss is 4.587029228210449 and perplexity is 98.20225988787595
At time: 633.2033252716064 and batch: 650, loss is 4.487114458084107 and perplexity is 88.8646528826267
At time: 633.8236720561981 and batch: 700, loss is 4.529843721389771 and perplexity is 92.7440660357613
At time: 634.4435305595398 and batch: 750, loss is 4.5688893699646 and perplexity is 96.43694450628755
At time: 635.0612282752991 and batch: 800, loss is 4.544680252075195 and perplexity is 94.13032442111637
At time: 635.6836326122284 and batch: 850, loss is 4.594095220565796 and perplexity is 98.89861362329931
At time: 636.30566573143 and batch: 900, loss is 4.5804642391204835 and perplexity is 97.55967471333314
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0127672169306505 and perplexity of 150.32012954965606
finished 52 epochs...
Completing Train Step...
At time: 637.8886070251465 and batch: 50, loss is 4.6856271839141845 and perplexity is 108.37822407561671
At time: 638.5079584121704 and batch: 100, loss is 4.606024169921875 and perplexity is 100.08543486818859
At time: 639.128413438797 and batch: 150, loss is 4.623646554946899 and perplexity is 101.86481131696301
At time: 639.7554750442505 and batch: 200, loss is 4.523880529403686 and perplexity is 92.19266106525068
At time: 640.3988647460938 and batch: 250, loss is 4.661163368225098 and perplexity is 105.75904733108403
At time: 641.0202434062958 and batch: 300, loss is 4.636944141387939 and perplexity is 103.22841366753347
At time: 641.6460280418396 and batch: 350, loss is 4.626342086791992 and perplexity is 102.13976156185291
At time: 642.2693917751312 and batch: 400, loss is 4.55913782119751 and perplexity is 95.50110529577998
At time: 642.8940703868866 and batch: 450, loss is 4.564829797744751 and perplexity is 96.04624533771239
At time: 643.5158126354218 and batch: 500, loss is 4.51198034286499 and perplexity is 91.102053289515
At time: 644.1407585144043 and batch: 550, loss is 4.594314680099488 and perplexity is 98.92032024870363
At time: 644.7645690441132 and batch: 600, loss is 4.587025604248047 and perplexity is 98.20190400722316
At time: 645.3889057636261 and batch: 650, loss is 4.487152643203736 and perplexity is 88.86804625481555
At time: 646.0145151615143 and batch: 700, loss is 4.5298683643341064 and perplexity is 92.7463515507789
At time: 646.6372425556183 and batch: 750, loss is 4.568842420578003 and perplexity is 96.43241695718133
At time: 647.2587914466858 and batch: 800, loss is 4.544673709869385 and perplexity is 94.12970860317544
At time: 647.8837842941284 and batch: 850, loss is 4.594150009155274 and perplexity is 98.90403228728015
At time: 648.5190010070801 and batch: 900, loss is 4.580540294647217 and perplexity is 97.56709494795275
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012763872538527 and perplexity of 150.3196268210395
finished 53 epochs...
Completing Train Step...
At time: 650.1926567554474 and batch: 50, loss is 4.685562086105347 and perplexity is 108.37116912033723
At time: 650.8265087604523 and batch: 100, loss is 4.6059008312225345 and perplexity is 100.07309122206925
At time: 651.447338104248 and batch: 150, loss is 4.623613328933716 and perplexity is 101.86142681162639
At time: 652.0680429935455 and batch: 200, loss is 4.523855905532837 and perplexity is 92.19039095302097
At time: 652.6901428699493 and batch: 250, loss is 4.661023035049438 and perplexity is 105.74420686944624
At time: 653.3102090358734 and batch: 300, loss is 4.636802654266358 and perplexity is 103.21380920961396
At time: 653.9333264827728 and batch: 350, loss is 4.626288471221923 and perplexity is 102.1342854271144
At time: 654.5533590316772 and batch: 400, loss is 4.559045162200928 and perplexity is 95.49225666914955
At time: 655.1771056652069 and batch: 450, loss is 4.564742450714111 and perplexity is 96.03785634975999
At time: 655.7974359989166 and batch: 500, loss is 4.511874647140503 and perplexity is 91.09242470084956
At time: 656.4427833557129 and batch: 550, loss is 4.594285144805908 and perplexity is 98.91739865114943
At time: 657.0648806095123 and batch: 600, loss is 4.587021446228027 and perplexity is 98.20149568258923
At time: 657.6892273426056 and batch: 650, loss is 4.487187633514404 and perplexity is 88.87115582976469
At time: 658.3180594444275 and batch: 700, loss is 4.529891500473022 and perplexity is 92.74849736807518
At time: 658.9404804706573 and batch: 750, loss is 4.56879602432251 and perplexity is 96.42794295791562
At time: 659.5628407001495 and batch: 800, loss is 4.544666976928711 and perplexity is 94.12907483556529
At time: 660.1891672611237 and batch: 850, loss is 4.594202861785889 and perplexity is 98.9092597637067
At time: 660.8163206577301 and batch: 900, loss is 4.5806138229370115 and perplexity is 97.5742691533348
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012762200342466 and perplexity of 150.3193754573617
finished 54 epochs...
Completing Train Step...
At time: 662.4190719127655 and batch: 50, loss is 4.685496921539307 and perplexity is 108.36410739022004
At time: 663.0548553466797 and batch: 100, loss is 4.605780601501465 and perplexity is 100.06106018548363
At time: 663.6802995204926 and batch: 150, loss is 4.623580780029297 and perplexity is 101.85811138773813
At time: 664.3057403564453 and batch: 200, loss is 4.523830890655518 and perplexity is 92.18808485054477
At time: 664.9327025413513 and batch: 250, loss is 4.660885457992554 and perplexity is 105.72965989337058
At time: 665.559713602066 and batch: 300, loss is 4.636664667129517 and perplexity is 103.19956801417213
At time: 666.1875009536743 and batch: 350, loss is 4.626234397888184 and perplexity is 102.12876283512608
At time: 666.8117215633392 and batch: 400, loss is 4.558953046798706 and perplexity is 95.4834607666427
At time: 667.4390773773193 and batch: 450, loss is 4.564656906127929 and perplexity is 96.02964118246732
At time: 668.071263551712 and batch: 500, loss is 4.511769151687622 and perplexity is 91.08281537113093
At time: 668.7009670734406 and batch: 550, loss is 4.59425539970398 and perplexity is 98.91445638680327
At time: 669.3271522521973 and batch: 600, loss is 4.587016830444336 and perplexity is 98.20104240677311
At time: 669.9551773071289 and batch: 650, loss is 4.487218818664551 and perplexity is 88.8739273333176
At time: 670.5831978321075 and batch: 700, loss is 4.529912576675415 and perplexity is 92.75045217477724
At time: 671.207647562027 and batch: 750, loss is 4.568749809265137 and perplexity is 96.42348663797479
At time: 671.8309528827667 and batch: 800, loss is 4.544659929275513 and perplexity is 94.12841144882768
At time: 672.4810659885406 and batch: 850, loss is 4.594254627227783 and perplexity is 98.9143799777697
At time: 673.1070601940155 and batch: 900, loss is 4.580684719085693 and perplexity is 97.58118703845099
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012759273999358 and perplexity of 150.31893557193706
finished 55 epochs...
Completing Train Step...
At time: 674.697591304779 and batch: 50, loss is 4.685430755615235 and perplexity is 108.35693761611829
At time: 675.3439667224884 and batch: 100, loss is 4.605663366317749 and perplexity is 100.04933019630717
At time: 675.96803855896 and batch: 150, loss is 4.623549165725708 and perplexity is 101.85489126538302
At time: 676.5925397872925 and batch: 200, loss is 4.52380560874939 and perplexity is 92.18575418949939
At time: 677.2139835357666 and batch: 250, loss is 4.66075026512146 and perplexity is 105.71536696326275
At time: 677.845799446106 and batch: 300, loss is 4.636529817581176 and perplexity is 103.18565253730549
At time: 678.4721531867981 and batch: 350, loss is 4.62618013381958 and perplexity is 102.12322106329408
At time: 679.0989243984222 and batch: 400, loss is 4.558862419128418 and perplexity is 95.47480771515121
At time: 679.7214753627777 and batch: 450, loss is 4.56457335472107 and perplexity is 96.02161810602061
At time: 680.3456952571869 and batch: 500, loss is 4.51166332244873 and perplexity is 91.07317665614205
At time: 680.9709844589233 and batch: 550, loss is 4.594225425720214 and perplexity is 98.91149157092727
At time: 681.5935871601105 and batch: 600, loss is 4.587011880874634 and perplexity is 98.20055635507183
At time: 682.2176697254181 and batch: 650, loss is 4.487245893478393 and perplexity is 88.8763336109302
At time: 682.8421995639801 and batch: 700, loss is 4.529932146072388 and perplexity is 92.75226726295523
At time: 683.4664087295532 and batch: 750, loss is 4.568703908920288 and perplexity is 96.41906086825948
At time: 684.0872943401337 and batch: 800, loss is 4.544652318954467 and perplexity is 94.12769510412286
At time: 684.7112672328949 and batch: 850, loss is 4.594304885864258 and perplexity is 98.91935140456265
At time: 685.33695936203 and batch: 900, loss is 4.580753450393677 and perplexity is 97.58789415156242
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01275634765625 and perplexity of 150.31849568779953
finished 56 epochs...
Completing Train Step...
At time: 686.9654145240784 and batch: 50, loss is 4.6853639507293705 and perplexity is 108.34969908505558
At time: 687.6016638278961 and batch: 100, loss is 4.605548629760742 and perplexity is 100.03785153915294
At time: 688.2237446308136 and batch: 150, loss is 4.623517694473267 and perplexity is 101.85168581482765
At time: 688.8805587291718 and batch: 200, loss is 4.5237800407409665 and perplexity is 92.18339721349147
At time: 689.5085327625275 and batch: 250, loss is 4.660617532730103 and perplexity is 105.70133604100205
At time: 690.1274108886719 and batch: 300, loss is 4.636397914886475 and perplexity is 103.17204296927034
At time: 690.762455701828 and batch: 350, loss is 4.62612566947937 and perplexity is 102.11765914090331
At time: 691.3953249454498 and batch: 400, loss is 4.558772859573364 and perplexity is 95.46625741673958
At time: 692.0199115276337 and batch: 450, loss is 4.564491348266602 and perplexity is 96.01374403643413
At time: 692.6404430866241 and batch: 500, loss is 4.511557235717773 and perplexity is 91.06351551302133
At time: 693.2629284858704 and batch: 550, loss is 4.594195261001587 and perplexity is 98.90850797861481
At time: 693.889410495758 and batch: 600, loss is 4.587006254196167 and perplexity is 98.2000038136704
At time: 694.5174734592438 and batch: 650, loss is 4.487269897460937 and perplexity is 88.87846702249585
At time: 695.1396520137787 and batch: 700, loss is 4.529950399398803 and perplexity is 92.75396031581725
At time: 695.758139371872 and batch: 750, loss is 4.568658380508423 and perplexity is 96.41467116147363
At time: 696.3764941692352 and batch: 800, loss is 4.544644899368286 and perplexity is 94.1269967181679
At time: 696.9996609687805 and batch: 850, loss is 4.59435435295105 and perplexity is 98.92424477773346
At time: 697.6313650608063 and batch: 900, loss is 4.580819940567016 and perplexity is 97.59438300328046
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012752585215111 and perplexity of 150.31793012437134
finished 57 epochs...
Completing Train Step...
At time: 699.2793519496918 and batch: 50, loss is 4.685298004150391 and perplexity is 108.34255402866602
At time: 699.908933877945 and batch: 100, loss is 4.6054362869262695 and perplexity is 100.02661363461733
At time: 700.5468151569366 and batch: 150, loss is 4.623486137390136 and perplexity is 101.84847172342533
At time: 701.1712143421173 and batch: 200, loss is 4.523754081726074 and perplexity is 92.18100425436995
At time: 701.7972893714905 and batch: 250, loss is 4.660486879348755 and perplexity is 105.68752670617312
At time: 702.4249784946442 and batch: 300, loss is 4.636268358230591 and perplexity is 103.1586772102328
At time: 703.0558876991272 and batch: 350, loss is 4.626070728302002 and perplexity is 102.11204883060002
At time: 703.6831984519958 and batch: 400, loss is 4.558684644699096 and perplexity is 95.45783624428655
At time: 704.3184204101562 and batch: 450, loss is 4.564410448074341 and perplexity is 96.00597682027079
At time: 704.969712972641 and batch: 500, loss is 4.511452016830444 and perplexity is 91.05393441530782
At time: 705.6021978855133 and batch: 550, loss is 4.5941648769378665 and perplexity is 98.9055027818612
At time: 706.2337071895599 and batch: 600, loss is 4.5869997215271 and perplexity is 98.19936230763848
At time: 706.8636448383331 and batch: 650, loss is 4.487292585372924 and perplexity is 88.88048351220812
At time: 707.4935629367828 and batch: 700, loss is 4.529967355728149 and perplexity is 92.75553309585077
At time: 708.1210465431213 and batch: 750, loss is 4.568612804412842 and perplexity is 96.41027705733917
At time: 708.7551746368408 and batch: 800, loss is 4.544638595581055 and perplexity is 94.12640336347802
At time: 709.3871080875397 and batch: 850, loss is 4.59440276145935 and perplexity is 98.92903366876851
At time: 710.026615858078 and batch: 900, loss is 4.58088454246521 and perplexity is 97.60068798933034
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012748404724957 and perplexity of 150.317301723058
finished 58 epochs...
Completing Train Step...
At time: 711.670729637146 and batch: 50, loss is 4.685233306884766 and perplexity is 108.33554478841134
At time: 712.3001866340637 and batch: 100, loss is 4.605325946807861 and perplexity is 100.01557729511161
At time: 712.9354319572449 and batch: 150, loss is 4.623453741073608 and perplexity is 101.84517226154298
At time: 713.5698273181915 and batch: 200, loss is 4.5237275409698485 and perplexity is 92.1785577332738
At time: 714.1926798820496 and batch: 250, loss is 4.660358390808105 and perplexity is 105.67394794247821
At time: 714.8127391338348 and batch: 300, loss is 4.636141080856323 and perplexity is 103.14554828019007
At time: 715.4368805885315 and batch: 350, loss is 4.626015787124634 and perplexity is 102.10643882852523
At time: 716.0569612979889 and batch: 400, loss is 4.558597393035889 and perplexity is 95.44950775265073
At time: 716.6772141456604 and batch: 450, loss is 4.56432988166809 and perplexity is 95.99824227531624
At time: 717.2971403598785 and batch: 500, loss is 4.511350030899048 and perplexity is 91.04464866851502
At time: 717.9247908592224 and batch: 550, loss is 4.594134883880615 and perplexity is 98.90253634794024
At time: 718.5560970306396 and batch: 600, loss is 4.58698974609375 and perplexity is 98.19838273133067
At time: 719.1845943927765 and batch: 650, loss is 4.487318410873413 and perplexity is 88.88277892481842
At time: 719.8079919815063 and batch: 700, loss is 4.529982442855835 and perplexity is 92.75693252097878
At time: 720.4281220436096 and batch: 750, loss is 4.568566665649414 and perplexity is 96.40582890899086
At time: 721.07080245018 and batch: 800, loss is 4.544633283615112 and perplexity is 94.12590336855702
At time: 721.6929047107697 and batch: 850, loss is 4.594449987411499 and perplexity is 98.93370579690067
At time: 722.3244526386261 and batch: 900, loss is 4.580946750640869 and perplexity is 97.60675973892755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012745060332834 and perplexity of 150.31679900389875
finished 59 epochs...
Completing Train Step...
At time: 723.9484498500824 and batch: 50, loss is 4.685170402526856 and perplexity is 108.3287302248627
At time: 724.5675234794617 and batch: 100, loss is 4.605217056274414 and perplexity is 100.00468713847518
At time: 725.1896693706512 and batch: 150, loss is 4.6234198093414305 and perplexity is 101.84171653706389
At time: 725.810304403305 and batch: 200, loss is 4.523699607849121 and perplexity is 92.1759829344534
At time: 726.431841135025 and batch: 250, loss is 4.660231370925903 and perplexity is 105.66052610249699
At time: 727.0513405799866 and batch: 300, loss is 4.636015691757202 and perplexity is 103.13261576362815
At time: 727.6740863323212 and batch: 350, loss is 4.625960369110107 and perplexity is 102.10078044920448
At time: 728.3055107593536 and batch: 400, loss is 4.558511018753052 and perplexity is 95.4412637259125
At time: 728.9305374622345 and batch: 450, loss is 4.5642492294311525 and perplexity is 95.99050011455012
At time: 729.559455871582 and batch: 500, loss is 4.511252956390381 and perplexity is 91.03581098294269
At time: 730.1946914196014 and batch: 550, loss is 4.5941062259674075 and perplexity is 98.8997020482503
At time: 730.8213369846344 and batch: 600, loss is 4.5869788074493405 and perplexity is 98.19730858001525
At time: 731.451324224472 and batch: 650, loss is 4.4873488330841065 and perplexity is 88.88548297657732
At time: 732.0769855976105 and batch: 700, loss is 4.52999547958374 and perplexity is 92.75814177575171
At time: 732.70112657547 and batch: 750, loss is 4.568519105911255 and perplexity is 96.40124398204074
At time: 733.3243496417999 and batch: 800, loss is 4.544628047943116 and perplexity is 94.12541055749072
At time: 733.9509446620941 and batch: 850, loss is 4.594494485855103 and perplexity is 98.93810829077991
At time: 734.5790863037109 and batch: 900, loss is 4.581005525588989 and perplexity is 97.61249673976168
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012742552038741 and perplexity of 150.31642196563268
finished 60 epochs...
Completing Train Step...
At time: 736.1801717281342 and batch: 50, loss is 4.685107889175415 and perplexity is 108.32195844454462
At time: 736.8269562721252 and batch: 100, loss is 4.605108213424683 and perplexity is 99.99380293568511
At time: 737.4513990879059 and batch: 150, loss is 4.623386964797974 and perplexity is 101.8383716473104
At time: 738.0757665634155 and batch: 200, loss is 4.523673048019409 and perplexity is 92.17353478855448
At time: 738.7016940116882 and batch: 250, loss is 4.660104389190674 and perplexity is 105.64710999736546
At time: 739.3264768123627 and batch: 300, loss is 4.635891218185424 and perplexity is 103.11977927749548
At time: 739.9526944160461 and batch: 350, loss is 4.625904741287232 and perplexity is 102.09510096304443
At time: 740.5770652294159 and batch: 400, loss is 4.5584255027771 and perplexity is 95.43310232206906
At time: 741.2022001743317 and batch: 450, loss is 4.5641696071624756 and perplexity is 95.98285743742724
At time: 741.8355662822723 and batch: 500, loss is 4.511156969070434 and perplexity is 91.02707311879607
At time: 742.4648809432983 and batch: 550, loss is 4.594081420898437 and perplexity is 98.89724886474566
At time: 743.0913231372833 and batch: 600, loss is 4.586988372802734 and perplexity is 98.19824787646647
At time: 743.7180941104889 and batch: 650, loss is 4.487361173629761 and perplexity is 88.88657987870621
At time: 744.3409779071808 and batch: 700, loss is 4.53000997543335 and perplexity is 92.75948639357063
At time: 744.9595861434937 and batch: 750, loss is 4.5684729671478275 and perplexity is 96.39679625045773
At time: 745.5767161846161 and batch: 800, loss is 4.544627246856689 and perplexity is 94.12533515493213
At time: 746.1983740329742 and batch: 850, loss is 4.594542636871338 and perplexity is 98.9428723759353
At time: 746.8207354545593 and batch: 900, loss is 4.581064720153808 and perplexity is 97.61827504004737
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01274087984268 and perplexity of 150.31617060731404
finished 61 epochs...
Completing Train Step...
At time: 748.4116790294647 and batch: 50, loss is 4.685047330856324 and perplexity is 108.31539884744159
At time: 749.0455672740936 and batch: 100, loss is 4.605002555847168 and perplexity is 99.98323839082238
At time: 749.674733877182 and batch: 150, loss is 4.623352117538452 and perplexity is 101.83482292097639
At time: 750.302708864212 and batch: 200, loss is 4.5236443996429445 and perplexity is 92.17089420425415
At time: 750.9248728752136 and batch: 250, loss is 4.659981269836425 and perplexity is 105.63410359409077
At time: 751.5453934669495 and batch: 300, loss is 4.635769805908203 and perplexity is 103.10726003027737
At time: 752.1682648658752 and batch: 350, loss is 4.625849323272705 and perplexity is 102.08944321202827
At time: 752.802882194519 and batch: 400, loss is 4.558340167999267 and perplexity is 95.42495890694775
At time: 753.4225525856018 and batch: 450, loss is 4.564088592529297 and perplexity is 95.97508173641752
At time: 754.0418787002563 and batch: 500, loss is 4.511064033508301 and perplexity is 91.01861385967553
At time: 754.6631741523743 and batch: 550, loss is 4.594051189422608 and perplexity is 98.89425910014974
At time: 755.2823443412781 and batch: 600, loss is 4.586973171234131 and perplexity is 98.19675512041083
At time: 755.9039521217346 and batch: 650, loss is 4.48739254951477 and perplexity is 88.88936881756783
At time: 756.5265791416168 and batch: 700, loss is 4.5300212001800535 and perplexity is 92.76052760115338
At time: 757.1472315788269 and batch: 750, loss is 4.568425512313842 and perplexity is 96.39222186503417
At time: 757.7683579921722 and batch: 800, loss is 4.544625015258789 and perplexity is 94.1251251052662
At time: 758.3985686302185 and batch: 850, loss is 4.594586238861084 and perplexity is 98.94718657609532
At time: 759.0240437984467 and batch: 900, loss is 4.581119995117188 and perplexity is 97.62367103575575
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012737535450556 and perplexity of 150.31566789193772
finished 62 epochs...
Completing Train Step...
At time: 760.5977039337158 and batch: 50, loss is 4.684986181259156 and perplexity is 108.30877560694127
At time: 761.2597303390503 and batch: 100, loss is 4.604895877838135 and perplexity is 99.9725729469084
At time: 761.8923966884613 and batch: 150, loss is 4.623317432403565 and perplexity is 101.83129082766303
At time: 762.514488697052 and batch: 200, loss is 4.523616256713868 and perplexity is 92.16830028181607
At time: 763.1349606513977 and batch: 250, loss is 4.659857273101807 and perplexity is 105.62100612221938
At time: 763.7552630901337 and batch: 300, loss is 4.635648612976074 and perplexity is 103.09476491628557
At time: 764.377037525177 and batch: 350, loss is 4.625792665481567 and perplexity is 102.08365921353322
At time: 764.9966595172882 and batch: 400, loss is 4.558255615234375 and perplexity is 95.41689080392749
At time: 765.6185052394867 and batch: 450, loss is 4.564009389877319 and perplexity is 95.96748055644105
At time: 766.2415661811829 and batch: 500, loss is 4.510971536636353 and perplexity is 91.01019531195509
At time: 766.8641941547394 and batch: 550, loss is 4.594025745391845 and perplexity is 98.89174286359072
At time: 767.4870405197144 and batch: 600, loss is 4.586980257034302 and perplexity is 98.19745092546017
At time: 768.1138770580292 and batch: 650, loss is 4.487403898239136 and perplexity is 88.89037760423778
At time: 768.7478604316711 and batch: 700, loss is 4.530034227371216 and perplexity is 92.76173601814989
At time: 769.3668010234833 and batch: 750, loss is 4.568379135131836 and perplexity is 96.38775156907738
At time: 769.9865763187408 and batch: 800, loss is 4.54462779045105 and perplexity is 94.12538632094743
At time: 770.6129560470581 and batch: 850, loss is 4.59463228225708 and perplexity is 98.95174254547481
At time: 771.239706993103 and batch: 900, loss is 4.581175498962402 and perplexity is 97.62908967525843
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01273544520548 and perplexity of 150.31535369568135
finished 63 epochs...
Completing Train Step...
At time: 772.8305974006653 and batch: 50, loss is 4.684925422668457 and perplexity is 108.30219511828777
At time: 773.4655139446259 and batch: 100, loss is 4.604791593551636 and perplexity is 99.96214792206177
At time: 774.0885779857635 and batch: 150, loss is 4.623280124664307 and perplexity is 101.82749180328348
At time: 774.7114973068237 and batch: 200, loss is 4.523585414886474 and perplexity is 92.16545768684328
At time: 775.3343732357025 and batch: 250, loss is 4.659736700057984 and perplexity is 105.60827184374037
At time: 775.9581677913666 and batch: 300, loss is 4.635530099868775 and perplexity is 103.08254755932181
At time: 776.5831577777863 and batch: 350, loss is 4.625735673904419 and perplexity is 102.07784147057633
At time: 777.2041709423065 and batch: 400, loss is 4.558171243667602 and perplexity is 95.40884067095969
At time: 777.8253116607666 and batch: 450, loss is 4.563929672241211 and perplexity is 95.95983056067166
At time: 778.4496991634369 and batch: 500, loss is 4.510881175994873 and perplexity is 91.00197194386553
At time: 779.0702233314514 and batch: 550, loss is 4.5939950752258305 and perplexity is 98.88870988393083
At time: 779.6924018859863 and batch: 600, loss is 4.586963758468628 and perplexity is 98.19583082173183
At time: 780.3132636547089 and batch: 650, loss is 4.487433710098267 and perplexity is 88.89302763115391
At time: 780.935712814331 and batch: 700, loss is 4.530044746398926 and perplexity is 92.76271178655357
At time: 781.554053068161 and batch: 750, loss is 4.568331661224366 and perplexity is 96.38317577449442
At time: 782.1720325946808 and batch: 800, loss is 4.544630432128907 and perplexity is 94.12563497022467
At time: 782.7975766658783 and batch: 850, loss is 4.594667930603027 and perplexity is 98.95527007430007
At time: 783.4305019378662 and batch: 900, loss is 4.581226329803467 and perplexity is 97.63405237012697
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01273084666631 and perplexity of 150.31466246622887
finished 64 epochs...
Completing Train Step...
At time: 785.0349357128143 and batch: 50, loss is 4.6848619842529295 and perplexity is 108.29532481655411
At time: 785.6547963619232 and batch: 100, loss is 4.604684972763062 and perplexity is 99.95149044718706
At time: 786.2761371135712 and batch: 150, loss is 4.623240537643433 and perplexity is 101.82346083602745
At time: 786.8959534168243 and batch: 200, loss is 4.523553800582886 and perplexity is 92.16254398614116
At time: 787.5201616287231 and batch: 250, loss is 4.659614419937133 and perplexity is 105.59535884101439
At time: 788.1457402706146 and batch: 300, loss is 4.63541054725647 and perplexity is 103.0702245081192
At time: 788.771714925766 and batch: 350, loss is 4.6256759738922115 and perplexity is 102.07174760409818
At time: 789.4006381034851 and batch: 400, loss is 4.558087053298951 and perplexity is 95.40080850361127
At time: 790.0251197814941 and batch: 450, loss is 4.563852710723877 and perplexity is 95.95244563068994
At time: 790.6487169265747 and batch: 500, loss is 4.510789566040039 and perplexity is 90.99363563917599
At time: 791.2734084129333 and batch: 550, loss is 4.593969793319702 and perplexity is 98.88620982045367
At time: 791.8996982574463 and batch: 600, loss is 4.586970109939575 and perplexity is 98.19645451167904
At time: 792.5270509719849 and batch: 650, loss is 4.487446279525757 and perplexity is 88.89414497264124
At time: 793.1535024642944 and batch: 700, loss is 4.530060958862305 and perplexity is 92.76421571081245
At time: 793.775022983551 and batch: 750, loss is 4.568289031982422 and perplexity is 96.37906712035007
At time: 794.3963298797607 and batch: 800, loss is 4.544644746780396 and perplexity is 94.1269823555291
At time: 795.0223834514618 and batch: 850, loss is 4.594661607742309 and perplexity is 98.95464439588815
At time: 795.6484587192535 and batch: 900, loss is 4.5812785434722905 and perplexity is 97.63915033529386
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012727502274187 and perplexity of 150.31415975589633
finished 65 epochs...
Completing Train Step...
At time: 797.2552878856659 and batch: 50, loss is 4.684797573089599 and perplexity is 108.28834961334236
At time: 797.8798418045044 and batch: 100, loss is 4.604580993652344 and perplexity is 99.94109812039723
At time: 798.5063881874084 and batch: 150, loss is 4.6231983947753905 and perplexity is 101.81916979377287
At time: 799.1314628124237 and batch: 200, loss is 4.523520984649658 and perplexity is 92.15951963587533
At time: 799.7578639984131 and batch: 250, loss is 4.659492349624633 and perplexity is 105.58246956927684
At time: 800.3791120052338 and batch: 300, loss is 4.635293264389038 and perplexity is 103.05813684549373
At time: 801.0258157253265 and batch: 350, loss is 4.625615997314453 and perplexity is 102.06562587357311
At time: 801.6463198661804 and batch: 400, loss is 4.558001337051391 and perplexity is 95.3926314547502
At time: 802.2679915428162 and batch: 450, loss is 4.563771800994873 and perplexity is 95.94468245837902
At time: 802.895430803299 and batch: 500, loss is 4.5106985378265385 and perplexity is 90.98535302806522
At time: 803.5159044265747 and batch: 550, loss is 4.593937044143677 and perplexity is 98.88297143158937
At time: 804.1455047130585 and batch: 600, loss is 4.586953792572022 and perplexity is 98.194852217111
At time: 804.7721824645996 and batch: 650, loss is 4.487477588653564 and perplexity is 88.89692821435774
At time: 805.412727355957 and batch: 700, loss is 4.530075035095215 and perplexity is 92.76552149070872
At time: 806.0322525501251 and batch: 750, loss is 4.5682455253601075 and perplexity is 96.37487408389093
At time: 806.649703502655 and batch: 800, loss is 4.544655237197876 and perplexity is 94.12796979204948
At time: 807.2796750068665 and batch: 850, loss is 4.594658002853394 and perplexity is 98.9542876760304
At time: 807.9003159999847 and batch: 900, loss is 4.581332120895386 and perplexity is 97.64438172950311
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012724157882063 and perplexity of 150.31365704724504
finished 66 epochs...
Completing Train Step...
At time: 809.4914708137512 and batch: 50, loss is 4.684736318588257 and perplexity is 108.28171666763657
At time: 810.1092009544373 and batch: 100, loss is 4.604479961395263 and perplexity is 99.93100135573708
At time: 810.7294616699219 and batch: 150, loss is 4.62316294670105 and perplexity is 101.8155605642432
At time: 811.3488759994507 and batch: 200, loss is 4.523498802185059 and perplexity is 92.15747533326743
At time: 811.9701039791107 and batch: 250, loss is 4.659367027282715 and perplexity is 105.56923855601302
At time: 812.5899584293365 and batch: 300, loss is 4.635184850692749 and perplexity is 103.04696453757222
At time: 813.211688041687 and batch: 350, loss is 4.6255628204345705 and perplexity is 102.06019848635293
At time: 813.833349943161 and batch: 400, loss is 4.5579185295104985 and perplexity is 95.38473255256899
At time: 814.4591603279114 and batch: 450, loss is 4.563694486618042 and perplexity is 95.93726484179237
At time: 815.0781056880951 and batch: 500, loss is 4.510608930587768 and perplexity is 90.97720044708242
At time: 815.7005016803741 and batch: 550, loss is 4.593913354873657 and perplexity is 98.88062899392423
At time: 816.3218381404877 and batch: 600, loss is 4.586965398788452 and perplexity is 98.19599189443187
At time: 816.9554309844971 and batch: 650, loss is 4.487484073638916 and perplexity is 88.89750471150428
At time: 817.5758011341095 and batch: 700, loss is 4.530079412460327 and perplexity is 92.76592756015486
At time: 818.1941542625427 and batch: 750, loss is 4.568186368942261 and perplexity is 96.36917306019741
At time: 818.8126564025879 and batch: 800, loss is 4.544640340805054 and perplexity is 94.12656763527946
At time: 819.4327156543732 and batch: 850, loss is 4.5946870708465575 and perplexity is 98.95716412039411
At time: 820.0537905693054 and batch: 900, loss is 4.5813839149475095 and perplexity is 97.64943925867382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0127237398330475 and perplexity of 150.3135942087818
finished 67 epochs...
Completing Train Step...
At time: 821.6327333450317 and batch: 50, loss is 4.684683513641358 and perplexity is 108.27599900829944
At time: 822.2686684131622 and batch: 100, loss is 4.604376678466797 and perplexity is 99.92068072225426
At time: 822.8925793170929 and batch: 150, loss is 4.623123483657837 and perplexity is 101.81154269165619
At time: 823.5159440040588 and batch: 200, loss is 4.523470211029053 and perplexity is 92.15484048217994
At time: 824.1404869556427 and batch: 250, loss is 4.659245777130127 and perplexity is 105.55643904571642
At time: 824.760552406311 and batch: 300, loss is 4.635077848434448 and perplexity is 103.03593886955186
At time: 825.3811872005463 and batch: 350, loss is 4.625510492324829 and perplexity is 102.05485800881608
At time: 826.0028190612793 and batch: 400, loss is 4.5578379058837895 and perplexity is 95.37704259949804
At time: 826.6226823329926 and batch: 450, loss is 4.563615131378174 and perplexity is 95.92965201919124
At time: 827.2405850887299 and batch: 500, loss is 4.510520582199097 and perplexity is 90.96916311306505
At time: 827.8599188327789 and batch: 550, loss is 4.5938849449157715 and perplexity is 98.87781983932297
At time: 828.4802670478821 and batch: 600, loss is 4.586951990127563 and perplexity is 98.19467522650328
At time: 829.1009635925293 and batch: 650, loss is 4.487509479522705 and perplexity is 88.89976325986827
At time: 829.7273375988007 and batch: 700, loss is 4.530080404281616 and perplexity is 92.76601956742235
At time: 830.3444459438324 and batch: 750, loss is 4.5681267261505125 and perplexity is 96.36342550507946
At time: 830.9616198539734 and batch: 800, loss is 4.544624614715576 and perplexity is 94.12508740409375
At time: 831.5851082801819 and batch: 850, loss is 4.594712266921997 and perplexity is 98.95965748397792
At time: 832.2098543643951 and batch: 900, loss is 4.581433115005493 and perplexity is 97.65424373493667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012723321784033 and perplexity of 150.31353137034495
finished 68 epochs...
Completing Train Step...
At time: 833.9035875797272 and batch: 50, loss is 4.684630298614502 and perplexity is 108.2702372514118
At time: 834.5463514328003 and batch: 100, loss is 4.604274129867553 and perplexity is 99.91043452178647
At time: 835.1628186702728 and batch: 150, loss is 4.623089027404785 and perplexity is 101.80803470781396
At time: 835.7824683189392 and batch: 200, loss is 4.523443832397461 and perplexity is 92.1524095956553
At time: 836.4011278152466 and batch: 250, loss is 4.659126510620117 and perplexity is 105.54385044833637
At time: 837.0175518989563 and batch: 300, loss is 4.634970531463623 and perplexity is 103.02488195801405
At time: 837.6511192321777 and batch: 350, loss is 4.6254580783843995 and perplexity is 102.04950905174903
At time: 838.2711391448975 and batch: 400, loss is 4.557757768630982 and perplexity is 95.3693996515697
At time: 838.8906493186951 and batch: 450, loss is 4.563536710739136 and perplexity is 95.92212944954345
At time: 839.5096821784973 and batch: 500, loss is 4.510432319641113 and perplexity is 90.96113429635827
At time: 840.1317846775055 and batch: 550, loss is 4.593862161636353 and perplexity is 98.87556710398772
At time: 840.7531983852386 and batch: 600, loss is 4.58695954322815 and perplexity is 98.19541690356333
At time: 841.3744428157806 and batch: 650, loss is 4.487514362335205 and perplexity is 88.90019734180333
At time: 841.995777130127 and batch: 700, loss is 4.530084161758423 and perplexity is 92.76636813424422
At time: 842.6142544746399 and batch: 750, loss is 4.568067741394043 and perplexity is 96.35774169952408
At time: 843.233208656311 and batch: 800, loss is 4.544612512588501 and perplexity is 94.1239482972179
At time: 843.8642859458923 and batch: 850, loss is 4.594739456176757 and perplexity is 98.96234815989483
At time: 844.4903209209442 and batch: 900, loss is 4.5814833831787105 and perplexity is 97.65915275875898
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012720395440924 and perplexity of 150.31309150202193
finished 69 epochs...
Completing Train Step...
At time: 846.1496658325195 and batch: 50, loss is 4.684576644897461 and perplexity is 108.26442830657541
At time: 846.7927207946777 and batch: 100, loss is 4.604178142547608 and perplexity is 99.90084484719306
At time: 847.4146838188171 and batch: 150, loss is 4.623054008483887 and perplexity is 101.80446956272385
At time: 848.034688949585 and batch: 200, loss is 4.52341630935669 and perplexity is 92.14987331603209
At time: 848.6524066925049 and batch: 250, loss is 4.65901123046875 and perplexity is 105.53168403856708
At time: 849.2854473590851 and batch: 300, loss is 4.634865198135376 and perplexity is 103.01403057582127
At time: 849.9045135974884 and batch: 350, loss is 4.625405921936035 and perplexity is 102.04418665059951
At time: 850.5229704380035 and batch: 400, loss is 4.557673530578613 and perplexity is 95.36136625745101
At time: 851.1404628753662 and batch: 450, loss is 4.563456764221192 and perplexity is 95.91446111583252
At time: 851.7566246986389 and batch: 500, loss is 4.510347986221314 and perplexity is 90.95346355628863
At time: 852.3756551742554 and batch: 550, loss is 4.59383394241333 and perplexity is 98.87277695167622
At time: 852.9947202205658 and batch: 600, loss is 4.586940631866455 and perplexity is 98.1935599120767
At time: 853.6128118038177 and batch: 650, loss is 4.48753809928894 and perplexity is 88.90230758671997
At time: 854.2317843437195 and batch: 700, loss is 4.530085830688477 and perplexity is 92.76652295495316
At time: 854.847368478775 and batch: 750, loss is 4.568003816604614 and perplexity is 96.35158224804901
At time: 855.4626104831696 and batch: 800, loss is 4.544598093032837 and perplexity is 94.12259108149134
At time: 856.0817446708679 and batch: 850, loss is 4.59475869178772 and perplexity is 98.96425177943259
At time: 856.701470375061 and batch: 900, loss is 4.58153058052063 and perplexity is 97.66376211995721
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012712452509632 and perplexity of 150.31189758020543
finished 70 epochs...
Completing Train Step...
At time: 858.3091018199921 and batch: 50, loss is 4.684521074295044 and perplexity is 108.25841215423628
At time: 858.9387135505676 and batch: 100, loss is 4.604081983566284 and perplexity is 99.89123894557346
At time: 859.5556447505951 and batch: 150, loss is 4.623018732070923 and perplexity is 101.80087832955726
At time: 860.1745274066925 and batch: 200, loss is 4.523393020629883 and perplexity is 92.1477272877964
At time: 860.7927463054657 and batch: 250, loss is 4.6588954162597656 and perplexity is 105.5194626777746
At time: 861.4095973968506 and batch: 300, loss is 4.63475959777832 and perplexity is 103.00315283176761
At time: 862.0269284248352 and batch: 350, loss is 4.625352993011474 and perplexity is 102.03878570447684
At time: 862.6433815956116 and batch: 400, loss is 4.557582893371582 and perplexity is 95.35272336124466
At time: 863.2609579563141 and batch: 450, loss is 4.563380002975464 and perplexity is 95.90709888488459
At time: 863.8767547607422 and batch: 500, loss is 4.510263624191285 and perplexity is 90.94579086111145
At time: 864.4938244819641 and batch: 550, loss is 4.593810911178589 and perplexity is 98.8704998157635
At time: 865.1347732543945 and batch: 600, loss is 4.586942615509034 and perplexity is 98.19375469319623
At time: 865.7530901432037 and batch: 650, loss is 4.487539463043213 and perplexity is 88.90242882770441
At time: 866.3718955516815 and batch: 700, loss is 4.530089311599731 and perplexity is 92.76684586754895
At time: 866.987330198288 and batch: 750, loss is 4.567939071655274 and perplexity is 96.34534417168163
At time: 867.6019103527069 and batch: 800, loss is 4.544586849212647 and perplexity is 94.12153278995098
At time: 868.219938993454 and batch: 850, loss is 4.5947802448272705 and perplexity is 98.96638478285153
At time: 868.8388376235962 and batch: 900, loss is 4.581580038070679 and perplexity is 97.6685924498074
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012704091529324 and perplexity of 150.31064083064354
finished 71 epochs...
Completing Train Step...
At time: 870.458206653595 and batch: 50, loss is 4.684466342926026 and perplexity is 108.25248718527365
At time: 871.074019908905 and batch: 100, loss is 4.603988704681396 and perplexity is 99.88192163675537
At time: 871.6917912960052 and batch: 150, loss is 4.622981586456299 and perplexity is 101.79709694359381
At time: 872.3112409114838 and batch: 200, loss is 4.523371114730835 and perplexity is 92.14570873109412
At time: 872.9286179542542 and batch: 250, loss is 4.658781929016113 and perplexity is 105.5074882442892
At time: 873.545069694519 and batch: 300, loss is 4.634655876159668 and perplexity is 102.9924697320734
At time: 874.1640741825104 and batch: 350, loss is 4.625302495956421 and perplexity is 102.03363317639241
At time: 874.780396938324 and batch: 400, loss is 4.557478523254394 and perplexity is 95.34277190565957
At time: 875.3965451717377 and batch: 450, loss is 4.563301572799682 and perplexity is 95.89957716922895
At time: 876.0105493068695 and batch: 500, loss is 4.510177888870239 and perplexity is 90.93799392877527
At time: 876.6267523765564 and batch: 550, loss is 4.593780813217163 and perplexity is 98.86752406005617
At time: 877.2445514202118 and batch: 600, loss is 4.586915273666381 and perplexity is 98.19106993170931
At time: 877.8623871803284 and batch: 650, loss is 4.4875539112091065 and perplexity is 88.9037133140237
At time: 878.4815621376038 and batch: 700, loss is 4.530085258483886 and perplexity is 92.76646987353809
At time: 879.0974626541138 and batch: 750, loss is 4.567876501083374 and perplexity is 96.3393159769927
At time: 879.7139568328857 and batch: 800, loss is 4.544571876525879 and perplexity is 94.1201235482725
At time: 880.3319430351257 and batch: 850, loss is 4.594798898696899 and perplexity is 98.9682309061095
At time: 880.9639446735382 and batch: 900, loss is 4.581625843048096 and perplexity is 97.67306625993955
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012694894450984 and perplexity of 150.30925841826158
finished 72 epochs...
Completing Train Step...
At time: 882.5605564117432 and batch: 50, loss is 4.6844117641448975 and perplexity is 108.24657905769963
At time: 883.1776022911072 and batch: 100, loss is 4.6038921928405765 and perplexity is 99.8722823137955
At time: 883.7961678504944 and batch: 150, loss is 4.6229446983337406 and perplexity is 101.7933419090642
At time: 884.4143567085266 and batch: 200, loss is 4.523351545333862 and perplexity is 92.14390551278468
At time: 885.0332581996918 and batch: 250, loss is 4.658667287826538 and perplexity is 105.49539343362278
At time: 885.6504175662994 and batch: 300, loss is 4.634554443359375 and perplexity is 102.98202344726627
At time: 886.268607378006 and batch: 350, loss is 4.625261640548706 and perplexity is 102.02946463586258
At time: 886.8859288692474 and batch: 400, loss is 4.557360944747924 and perplexity is 95.33156230395325
At time: 887.5044658184052 and batch: 450, loss is 4.563227386474609 and perplexity is 95.89246299591314
At time: 888.1221733093262 and batch: 500, loss is 4.510095243453979 and perplexity is 90.93047863096993
At time: 888.7415461540222 and batch: 550, loss is 4.593758430480957 and perplexity is 98.86531115911129
At time: 889.3615226745605 and batch: 600, loss is 4.586913537979126 and perplexity is 98.19089950286856
At time: 889.9815232753754 and batch: 650, loss is 4.487553939819336 and perplexity is 88.9037158575794
At time: 890.6008629798889 and batch: 700, loss is 4.530089082717896 and perplexity is 92.76682463490545
At time: 891.219037771225 and batch: 750, loss is 4.567821941375732 and perplexity is 96.33405987546558
At time: 891.8348519802094 and batch: 800, loss is 4.544561014175415 and perplexity is 94.11910118805748
At time: 892.4544606208801 and batch: 850, loss is 4.594825143814087 and perplexity is 98.97082837301278
At time: 893.0753977298737 and batch: 900, loss is 4.581675539016723 and perplexity is 97.6779203381892
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012689459813784 and perplexity of 150.308441544194
finished 73 epochs...
Completing Train Step...
At time: 894.6703815460205 and batch: 50, loss is 4.684362201690674 and perplexity is 108.2412142245285
At time: 895.2870004177094 and batch: 100, loss is 4.603801679611206 and perplexity is 99.8632429600954
At time: 895.9039175510406 and batch: 150, loss is 4.6229087829589846 and perplexity is 101.78968602869341
At time: 896.5195899009705 and batch: 200, loss is 4.523328742980957 and perplexity is 92.1418044388879
At time: 897.1511764526367 and batch: 250, loss is 4.658556537628174 and perplexity is 105.4837104448321
At time: 897.7731029987335 and batch: 300, loss is 4.634452838897705 and perplexity is 102.97156054575997
At time: 898.3919532299042 and batch: 350, loss is 4.6252116775512695 and perplexity is 102.02436706532858
At time: 899.0119552612305 and batch: 400, loss is 4.55727783203125 and perplexity is 95.32363936807829
At time: 899.6323554515839 and batch: 450, loss is 4.56315396308899 and perplexity is 95.88542250509609
At time: 900.2511324882507 and batch: 500, loss is 4.510015993118286 and perplexity is 90.92327264555585
At time: 900.8730728626251 and batch: 550, loss is 4.59373046875 and perplexity is 98.86254675252862
At time: 901.4941489696503 and batch: 600, loss is 4.586889867782593 and perplexity is 98.18857533248642
At time: 902.1168344020844 and batch: 650, loss is 4.4875766563415525 and perplexity is 88.90573546375491
At time: 902.7381961345673 and batch: 700, loss is 4.530090236663819 and perplexity is 92.76693168286627
At time: 903.3565611839294 and batch: 750, loss is 4.567769384384155 and perplexity is 96.32899698013853
At time: 903.9724507331848 and batch: 800, loss is 4.5445481300354 and perplexity is 94.11788855219156
At time: 904.5891375541687 and batch: 850, loss is 4.594849004745483 and perplexity is 98.97318993733325
At time: 905.215603351593 and batch: 900, loss is 4.581723165512085 and perplexity is 97.68257250599147
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012687787617723 and perplexity of 150.3081901992202
finished 74 epochs...
Completing Train Step...
At time: 906.8425154685974 and batch: 50, loss is 4.684307126998902 and perplexity is 108.2352530371748
At time: 907.4852359294891 and batch: 100, loss is 4.603710737228393 and perplexity is 99.854161571773
At time: 908.1122629642487 and batch: 150, loss is 4.6228726196289065 and perplexity is 101.7860050412378
At time: 908.73885679245 and batch: 200, loss is 4.523304986953735 and perplexity is 92.13961554167324
At time: 909.3664834499359 and batch: 250, loss is 4.658445615768432 and perplexity is 105.47201064439092
At time: 909.9946150779724 and batch: 300, loss is 4.634350118637085 and perplexity is 102.9609838234554
At time: 910.6236433982849 and batch: 350, loss is 4.625155925750732 and perplexity is 102.01867918172238
At time: 911.2520473003387 and batch: 400, loss is 4.5571994209289555 and perplexity is 95.31616522947225
At time: 911.8708643913269 and batch: 450, loss is 4.563081693649292 and perplexity is 95.87849316972908
At time: 912.4900138378143 and batch: 500, loss is 4.509935159683227 and perplexity is 90.91592330214142
At time: 913.1260004043579 and batch: 550, loss is 4.593707513809204 and perplexity is 98.86027739466763
At time: 913.746964931488 and batch: 600, loss is 4.586891078948975 and perplexity is 98.18869425526
At time: 914.3665416240692 and batch: 650, loss is 4.487578659057617 and perplexity is 88.90591351687789
At time: 914.9862666130066 and batch: 700, loss is 4.530092220306397 and perplexity is 92.76711569948432
At time: 915.6028747558594 and batch: 750, loss is 4.567719316482544 and perplexity is 96.3241741101319
At time: 916.2183573246002 and batch: 800, loss is 4.544538536071777 and perplexity is 94.11698559292404
At time: 916.8371534347534 and batch: 850, loss is 4.59487753868103 and perplexity is 98.97601407224742
At time: 917.4556448459625 and batch: 900, loss is 4.581772155761719 and perplexity is 97.6873581168266
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012684861274614 and perplexity of 150.30775034652726
finished 75 epochs...
Completing Train Step...
At time: 919.0339946746826 and batch: 50, loss is 4.684254121780396 and perplexity is 108.22951615598117
At time: 919.6611249446869 and batch: 100, loss is 4.603622455596923 and perplexity is 99.84534667258295
At time: 920.2773525714874 and batch: 150, loss is 4.622830867767334 and perplexity is 101.78175537476166
At time: 920.8937239646912 and batch: 200, loss is 4.523277368545532 and perplexity is 92.13707082730016
At time: 921.5098764896393 and batch: 250, loss is 4.658338088989257 and perplexity is 105.4606701885055
At time: 922.1258535385132 and batch: 300, loss is 4.63424898147583 and perplexity is 102.95057116839355
At time: 922.7434265613556 and batch: 350, loss is 4.625101432800293 and perplexity is 102.01312003436242
At time: 923.3590779304504 and batch: 400, loss is 4.557121267318726 and perplexity is 95.30871621813368
At time: 923.9786348342896 and batch: 450, loss is 4.563008499145508 and perplexity is 95.8714756478231
At time: 924.5985286235809 and batch: 500, loss is 4.509853439331055 and perplexity is 90.9084939244409
At time: 925.2193765640259 and batch: 550, loss is 4.593678741455078 and perplexity is 98.85743299267772
At time: 925.8384084701538 and batch: 600, loss is 4.586867456436157 and perplexity is 98.18637481896695
At time: 926.4570031166077 and batch: 650, loss is 4.487600412368774 and perplexity is 88.90784753591389
At time: 927.0763740539551 and batch: 700, loss is 4.530090408325195 and perplexity is 92.76694760736684
At time: 927.6913509368896 and batch: 750, loss is 4.567668294906616 and perplexity is 96.31925962434234
At time: 928.3060789108276 and batch: 800, loss is 4.544525766372681 and perplexity is 94.11578375501173
At time: 928.9480278491974 and batch: 850, loss is 4.594902429580689 and perplexity is 98.97847770494322
At time: 929.5651118755341 and batch: 900, loss is 4.581817846298218 and perplexity is 97.69182160659699
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012684861274614 and perplexity of 150.30775034652726
finished 76 epochs...
Completing Train Step...
At time: 931.2515633106232 and batch: 50, loss is 4.6841997623443605 and perplexity is 108.22363302042399
At time: 931.8960609436035 and batch: 100, loss is 4.60353310585022 and perplexity is 99.83642591468788
At time: 932.523463010788 and batch: 150, loss is 4.62278115272522 and perplexity is 101.77669541628582
At time: 933.1501965522766 and batch: 200, loss is 4.523250885009766 and perplexity is 92.13463074420068
At time: 933.7743861675262 and batch: 250, loss is 4.658229389190674 and perplexity is 105.44920725791795
At time: 934.3976700305939 and batch: 300, loss is 4.6341471767425535 and perplexity is 102.94009084643734
At time: 935.0221111774445 and batch: 350, loss is 4.625046415328979 and perplexity is 102.00750768484734
At time: 935.6434874534607 and batch: 400, loss is 4.557043991088867 and perplexity is 95.30135140443785
At time: 936.2670712471008 and batch: 450, loss is 4.562937259674072 and perplexity is 95.86464605784327
At time: 936.8881719112396 and batch: 500, loss is 4.5097645568847655 and perplexity is 90.9004141141944
At time: 937.5106890201569 and batch: 550, loss is 4.593655328750611 and perplexity is 98.85511849990901
At time: 938.1320757865906 and batch: 600, loss is 4.586867923736572 and perplexity is 98.18642070151141
At time: 938.7546854019165 and batch: 650, loss is 4.487600803375244 and perplexity is 88.90788229946429
At time: 939.3774509429932 and batch: 700, loss is 4.530089683532715 and perplexity is 92.76688037060516
At time: 939.9993011951447 and batch: 750, loss is 4.567618618011474 and perplexity is 96.31447490142794
At time: 940.6197988986969 and batch: 800, loss is 4.544516105651855 and perplexity is 94.11487453309142
At time: 941.2427530288696 and batch: 850, loss is 4.594933528900146 and perplexity is 98.98155591610565
At time: 941.8668427467346 and batch: 900, loss is 4.581864881515503 and perplexity is 97.69641667071726
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012684861274614 and perplexity of 150.30775034652726
finished 77 epochs...
Completing Train Step...
At time: 943.4797277450562 and batch: 50, loss is 4.684146366119385 and perplexity is 108.21785444124606
At time: 944.1223745346069 and batch: 100, loss is 4.603447008132934 and perplexity is 99.82783059633864
At time: 944.7481775283813 and batch: 150, loss is 4.622719640731812 and perplexity is 101.77043512141184
At time: 945.4007494449615 and batch: 200, loss is 4.523221464157104 and perplexity is 92.13192010467938
At time: 946.0256850719452 and batch: 250, loss is 4.658124341964721 and perplexity is 105.438130693008
At time: 946.6416501998901 and batch: 300, loss is 4.634047431945801 and perplexity is 102.92982362005787
At time: 947.2595252990723 and batch: 350, loss is 4.6249922752380375 and perplexity is 102.00198513860144
At time: 947.8744978904724 and batch: 400, loss is 4.556967391967773 and perplexity is 95.29405168426085
At time: 948.4913697242737 and batch: 450, loss is 4.562866001129151 and perplexity is 95.85781512603978
At time: 949.1073899269104 and batch: 500, loss is 4.509673023223877 and perplexity is 90.89209404730317
At time: 949.7243995666504 and batch: 550, loss is 4.593626890182495 and perplexity is 98.85230724186225
At time: 950.3417866230011 and batch: 600, loss is 4.586843614578247 and perplexity is 98.18403390127584
At time: 950.9590067863464 and batch: 650, loss is 4.487621402740478 and perplexity is 88.90971376426741
At time: 951.5770888328552 and batch: 700, loss is 4.5300854969024655 and perplexity is 92.76649199079066
At time: 952.1940610408783 and batch: 750, loss is 4.56756778717041 and perplexity is 96.30957928008738
At time: 952.8099458217621 and batch: 800, loss is 4.544503335952759 and perplexity is 94.11367272213656
At time: 953.4287049770355 and batch: 850, loss is 4.59496262550354 and perplexity is 98.9844359850814
At time: 954.04705119133 and batch: 900, loss is 4.5819088745117185 and perplexity is 97.70071472334755
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.0126844432255995 and perplexity of 150.3076875105334
finished 78 epochs...
Completing Train Step...
At time: 955.6336259841919 and batch: 50, loss is 4.684091854095459 and perplexity is 108.2119554277606
At time: 956.2495622634888 and batch: 100, loss is 4.603359527587891 and perplexity is 99.81909798527992
At time: 956.8664529323578 and batch: 150, loss is 4.622666053771972 and perplexity is 101.76498169930963
At time: 957.4813241958618 and batch: 200, loss is 4.523193941116333 and perplexity is 92.12938438898149
At time: 958.1116604804993 and batch: 250, loss is 4.658018655776978 and perplexity is 105.42698792776082
At time: 958.7288715839386 and batch: 300, loss is 4.633948192596436 and perplexity is 102.91960943816434
At time: 959.3479554653168 and batch: 350, loss is 4.624938869476319 and perplexity is 101.99653779034948
At time: 959.9656803607941 and batch: 400, loss is 4.556892404556274 and perplexity is 95.28690609791165
At time: 960.5835652351379 and batch: 450, loss is 4.5627970695495605 and perplexity is 95.85120772315906
At time: 961.2237334251404 and batch: 500, loss is 4.50959098815918 and perplexity is 90.88463801431975
At time: 961.8415727615356 and batch: 550, loss is 4.593604793548584 and perplexity is 98.85012296275056
At time: 962.4602627754211 and batch: 600, loss is 4.586844606399536 and perplexity is 98.1841312823392
At time: 963.0792324542999 and batch: 650, loss is 4.487621927261353 and perplexity is 88.90976039928047
At time: 963.6982007026672 and batch: 700, loss is 4.530084247589111 and perplexity is 92.7663760964458
At time: 964.3139202594757 and batch: 750, loss is 4.5675192546844485 and perplexity is 96.30490525020505
At time: 964.9294490814209 and batch: 800, loss is 4.54449460029602 and perplexity is 94.11285058098824
At time: 965.5474700927734 and batch: 850, loss is 4.594993047714233 and perplexity is 98.9874473562543
At time: 966.16636967659 and batch: 900, loss is 4.581954574584961 and perplexity is 97.70517975519158
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012686115421661 and perplexity of 150.30793885466667
Annealing...
finished 79 epochs...
Completing Train Step...
At time: 967.7636938095093 and batch: 50, loss is 4.6840986728668215 and perplexity is 108.21269330285907
At time: 968.3831386566162 and batch: 100, loss is 4.603217306137085 and perplexity is 99.80490257781601
At time: 969.0029497146606 and batch: 150, loss is 4.622375946044922 and perplexity is 101.73546317375876
At time: 969.6214382648468 and batch: 200, loss is 4.522935161590576 and perplexity is 92.10554627512192
At time: 970.2408018112183 and batch: 250, loss is 4.657658700942993 and perplexity is 105.38904580295906
At time: 970.8596432209015 and batch: 300, loss is 4.633530225753784 and perplexity is 102.8766014425447
At time: 971.4796757698059 and batch: 350, loss is 4.624583806991577 and perplexity is 101.96032907476526
At time: 972.0975542068481 and batch: 400, loss is 4.556358671188354 and perplexity is 95.2360618664398
At time: 972.7168061733246 and batch: 450, loss is 4.5624165439605715 and perplexity is 95.81474082461895
At time: 973.3308486938477 and batch: 500, loss is 4.508998289108276 and perplexity is 90.83078673600419
At time: 973.9503998756409 and batch: 550, loss is 4.593010320663452 and perplexity is 98.79137670821498
At time: 974.5701298713684 and batch: 600, loss is 4.586430015563965 and perplexity is 98.14343347836228
At time: 975.1896917819977 and batch: 650, loss is 4.487112035751343 and perplexity is 88.8644376231272
At time: 975.809148311615 and batch: 700, loss is 4.5293699073791505 and perplexity is 92.70013300673592
At time: 976.4254012107849 and batch: 750, loss is 4.5669386768341065 and perplexity is 96.24900898297491
At time: 977.0554692745209 and batch: 800, loss is 4.543937501907348 and perplexity is 94.06043506523156
At time: 977.6745748519897 and batch: 850, loss is 4.594351606369019 and perplexity is 98.92397307455343
At time: 978.2935194969177 and batch: 900, loss is 4.581422052383423 and perplexity is 97.65316342892022
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012672737853168 and perplexity of 150.30592811336905
finished 80 epochs...
Completing Train Step...
At time: 979.8837251663208 and batch: 50, loss is 4.684063711166382 and perplexity is 108.20891006922658
At time: 980.5022871494293 and batch: 100, loss is 4.603177318572998 and perplexity is 99.80091170267114
At time: 981.1213042736053 and batch: 150, loss is 4.622351732254028 and perplexity is 101.73299980235085
At time: 981.7399764060974 and batch: 200, loss is 4.522919912338256 and perplexity is 92.1041417451158
At time: 982.358638048172 and batch: 250, loss is 4.657626714706421 and perplexity is 105.38567485792008
At time: 982.9756925106049 and batch: 300, loss is 4.6334688854217525 and perplexity is 102.8702911511936
At time: 983.5936832427979 and batch: 350, loss is 4.624564371109009 and perplexity is 101.95834740504054
At time: 984.2100636959076 and batch: 400, loss is 4.556346397399903 and perplexity is 95.23489296633694
At time: 984.8263833522797 and batch: 450, loss is 4.5623924446105955 and perplexity is 95.81243177947032
At time: 985.4412226676941 and batch: 500, loss is 4.508982172012329 and perplexity is 90.82932281929652
At time: 986.058756351471 and batch: 550, loss is 4.593023347854614 and perplexity is 98.7926636907474
At time: 986.676855802536 and batch: 600, loss is 4.586433162689209 and perplexity is 98.14374234852534
At time: 987.295416355133 and batch: 650, loss is 4.48712010383606 and perplexity is 88.86515459183053
At time: 987.9136917591095 and batch: 700, loss is 4.529399747848511 and perplexity is 92.70289926348761
At time: 988.530134677887 and batch: 750, loss is 4.56694860458374 and perplexity is 96.2499645237818
At time: 989.1440010070801 and batch: 800, loss is 4.543935861587524 and perplexity is 94.0602807761618
At time: 989.7624161243439 and batch: 850, loss is 4.594335203170776 and perplexity is 98.92235041832058
At time: 990.3880524635315 and batch: 900, loss is 4.581409378051758 and perplexity is 97.65192574818218
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012667303215967 and perplexity of 150.30511125740028
finished 81 epochs...
Completing Train Step...
At time: 991.9811971187592 and batch: 50, loss is 4.6840509510040285 and perplexity is 108.20752931477534
At time: 992.6125495433807 and batch: 100, loss is 4.603154945373535 and perplexity is 99.79867886194504
At time: 993.2310569286346 and batch: 150, loss is 4.622322187423706 and perplexity is 101.72999416253434
At time: 993.848742723465 and batch: 200, loss is 4.522903137207031 and perplexity is 92.10259669901082
At time: 994.466127872467 and batch: 250, loss is 4.6575861740112305 and perplexity is 105.38140253600028
At time: 995.0828492641449 and batch: 300, loss is 4.633419790267944 and perplexity is 102.8652408424011
At time: 995.7004358768463 and batch: 350, loss is 4.62454460144043 and perplexity is 101.95633174222809
At time: 996.3179585933685 and batch: 400, loss is 4.556335334777832 and perplexity is 95.23383942453557
At time: 996.9370641708374 and batch: 450, loss is 4.562375583648682 and perplexity is 95.81081630332649
At time: 997.5533256530762 and batch: 500, loss is 4.508965034484863 and perplexity is 90.82776624261992
At time: 998.1716413497925 and batch: 550, loss is 4.593030252456665 and perplexity is 98.79334581713064
At time: 998.7901272773743 and batch: 600, loss is 4.586442346572876 and perplexity is 98.14464369337662
At time: 999.4093716144562 and batch: 650, loss is 4.487126121520996 and perplexity is 88.86568935594168
At time: 1000.0284683704376 and batch: 700, loss is 4.5294193840026855 and perplexity is 92.70471960978223
At time: 1000.6451969146729 and batch: 750, loss is 4.566956491470337 and perplexity is 96.25072363933042
At time: 1001.2605993747711 and batch: 800, loss is 4.543933610916138 and perplexity is 94.06006907761753
At time: 1001.8816306591034 and batch: 850, loss is 4.59432279586792 and perplexity is 98.92112306637377
At time: 1002.5007078647614 and batch: 900, loss is 4.581397457122803 and perplexity is 97.65076165345157
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012661868578767 and perplexity of 150.30429440587085
finished 82 epochs...
Completing Train Step...
At time: 1004.0715484619141 and batch: 50, loss is 4.684042100906372 and perplexity is 108.20657167181136
At time: 1004.7023792266846 and batch: 100, loss is 4.603134689331054 and perplexity is 99.79665735614041
At time: 1005.3195195198059 and batch: 150, loss is 4.622296695709228 and perplexity is 101.7274009236225
At time: 1005.9372692108154 and batch: 200, loss is 4.522885065078736 and perplexity is 92.10093222410731
At time: 1006.5548868179321 and batch: 250, loss is 4.6575495052337645 and perplexity is 105.37753839964866
At time: 1007.1723392009735 and batch: 300, loss is 4.633376722335815 and perplexity is 102.86081074458832
At time: 1007.7912759780884 and batch: 350, loss is 4.624525423049927 and perplexity is 101.95437640263384
At time: 1008.4233026504517 and batch: 400, loss is 4.556322803497315 and perplexity is 95.2326460300564
At time: 1009.0438346862793 and batch: 450, loss is 4.562359867095947 and perplexity is 95.80931049941255
At time: 1009.6616203784943 and batch: 500, loss is 4.508946962356568 and perplexity is 90.82612480640779
At time: 1010.2826890945435 and batch: 550, loss is 4.59303544998169 and perplexity is 98.79385929935222
At time: 1010.9020221233368 and batch: 600, loss is 4.586453123092651 and perplexity is 98.14570135676917
At time: 1011.5226504802704 and batch: 650, loss is 4.4871305656433105 and perplexity is 88.86608428681231
At time: 1012.1428046226501 and batch: 700, loss is 4.529434804916382 and perplexity is 92.70614921228542
At time: 1012.7609541416168 and batch: 750, loss is 4.566963005065918 and perplexity is 96.25135057966041
At time: 1013.3761768341064 and batch: 800, loss is 4.543930644989014 and perplexity is 94.05979010272104
At time: 1013.9951772689819 and batch: 850, loss is 4.594310722351074 and perplexity is 98.91992874773784
At time: 1014.6125872135162 and batch: 900, loss is 4.5813866806030275 and perplexity is 97.64970932375779
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012658942235659 and perplexity of 150.30385456457847
finished 83 epochs...
Completing Train Step...
At time: 1016.1922402381897 and batch: 50, loss is 4.684034776687622 and perplexity is 108.2057791461126
At time: 1016.8213081359863 and batch: 100, loss is 4.60311559677124 and perplexity is 99.79475200067972
At time: 1017.4382276535034 and batch: 150, loss is 4.622273921966553 and perplexity is 101.72508423635071
At time: 1018.064624786377 and batch: 200, loss is 4.522867593765259 and perplexity is 92.09932311390558
At time: 1018.6833102703094 and batch: 250, loss is 4.657515182495117 and perplexity is 105.37392161600813
At time: 1019.300724029541 and batch: 300, loss is 4.633337955474854 and perplexity is 102.856823231132
At time: 1019.9192616939545 and batch: 350, loss is 4.624505987167359 and perplexity is 101.95239484860345
At time: 1020.536408662796 and batch: 400, loss is 4.556309537887573 and perplexity is 95.23138271931884
At time: 1021.1553542613983 and batch: 450, loss is 4.562344369888305 and perplexity is 95.80782573413863
At time: 1021.7736220359802 and batch: 500, loss is 4.508928432464599 and perplexity is 90.82444182372001
At time: 1022.3933618068695 and batch: 550, loss is 4.593039503097534 and perplexity is 98.79425972312018
At time: 1023.0124726295471 and batch: 600, loss is 4.586463470458984 and perplexity is 98.14671691154923
At time: 1023.6324017047882 and batch: 650, loss is 4.487135229110717 and perplexity is 88.86649871186629
At time: 1024.2653143405914 and batch: 700, loss is 4.529447870254517 and perplexity is 92.70736045738472
At time: 1024.8832132816315 and batch: 750, loss is 4.566968364715576 and perplexity is 96.2518664545611
At time: 1025.5002579689026 and batch: 800, loss is 4.5439270496368405 and perplexity is 94.0594519252582
At time: 1026.1220455169678 and batch: 850, loss is 4.594298934936523 and perplexity is 98.91876274440243
At time: 1026.7416841983795 and batch: 900, loss is 4.581376571655273 and perplexity is 97.64872219293743
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012656851990582 and perplexity of 150.30354039301469
finished 84 epochs...
Completing Train Step...
At time: 1028.4294571876526 and batch: 50, loss is 4.684028291702271 and perplexity is 108.2050774354952
At time: 1029.061255455017 and batch: 100, loss is 4.603098096847535 and perplexity is 99.79300561541433
At time: 1029.677407503128 and batch: 150, loss is 4.622253398895264 and perplexity is 101.72299654661806
At time: 1030.2939267158508 and batch: 200, loss is 4.5228508186340335 and perplexity is 92.09777814863311
At time: 1030.9117381572723 and batch: 250, loss is 4.657482032775879 and perplexity is 105.37042855798882
At time: 1031.5295717716217 and batch: 300, loss is 4.633302412033081 and perplexity is 102.85316741059515
At time: 1032.1476802825928 and batch: 350, loss is 4.624486780166626 and perplexity is 101.95043666768632
At time: 1032.7638466358185 and batch: 400, loss is 4.556295528411865 and perplexity is 95.23004858692123
At time: 1033.3808233737946 and batch: 450, loss is 4.5623290061950685 and perplexity is 95.8063537834017
At time: 1033.9964640140533 and batch: 500, loss is 4.508909740447998 and perplexity is 90.82274414761218
At time: 1034.6139876842499 and batch: 550, loss is 4.593043060302734 and perplexity is 98.79461115519963
At time: 1035.23144865036 and batch: 600, loss is 4.586472768783569 and perplexity is 98.14762951582286
At time: 1035.8496901988983 and batch: 650, loss is 4.487139863967895 and perplexity is 88.86691059635025
At time: 1036.467693567276 and batch: 700, loss is 4.529459276199341 and perplexity is 92.70841787845333
At time: 1037.0829701423645 and batch: 750, loss is 4.5669723320007325 and perplexity is 96.25224831391964
At time: 1037.697914838791 and batch: 800, loss is 4.543922958374023 and perplexity is 94.05906710410711
At time: 1038.3153066635132 and batch: 850, loss is 4.59428708076477 and perplexity is 98.91759015134937
At time: 1038.9328076839447 and batch: 900, loss is 4.581366901397705 and perplexity is 97.64777790920837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012654761745505 and perplexity of 150.30322622210772
finished 85 epochs...
Completing Train Step...
At time: 1040.5203940868378 and batch: 50, loss is 4.684022598266601 and perplexity is 108.20446137860144
At time: 1041.1360671520233 and batch: 100, loss is 4.603081550598144 and perplexity is 99.79135442911652
At time: 1041.753011226654 and batch: 150, loss is 4.622234840393066 and perplexity is 101.72110873768057
At time: 1042.3702828884125 and batch: 200, loss is 4.5228346157073975 and perplexity is 92.0962859071798
At time: 1042.9877326488495 and batch: 250, loss is 4.6574496650695805 and perplexity is 105.36701801410076
At time: 1043.6048700809479 and batch: 300, loss is 4.633268737792969 and perplexity is 102.84970396665423
At time: 1044.223958492279 and batch: 350, loss is 4.6244680690765385 and perplexity is 101.94852908172797
At time: 1044.8423986434937 and batch: 400, loss is 4.556280727386475 and perplexity is 95.22863909498517
At time: 1045.4618515968323 and batch: 450, loss is 4.5623133087158205 and perplexity is 95.80484987695516
At time: 1046.0780813694 and batch: 500, loss is 4.508890943527222 and perplexity is 90.82103697573059
At time: 1046.6966230869293 and batch: 550, loss is 4.593045997619629 and perplexity is 98.79490134670627
At time: 1047.3156485557556 and batch: 600, loss is 4.586481294631958 and perplexity is 98.14846631119904
At time: 1047.9330139160156 and batch: 650, loss is 4.4871446323394775 and perplexity is 88.86733434781162
At time: 1048.5515549182892 and batch: 700, loss is 4.529469509124755 and perplexity is 92.70936656163268
At time: 1049.1665952205658 and batch: 750, loss is 4.566975650787353 and perplexity is 96.25256775512362
At time: 1049.781394958496 and batch: 800, loss is 4.5439183712005615 and perplexity is 94.05863563984029
At time: 1050.3977599143982 and batch: 850, loss is 4.594275341033936 and perplexity is 98.91642889228264
At time: 1051.015234708786 and batch: 900, loss is 4.581357688903808 and perplexity is 97.64687833379404
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012654343696489 and perplexity of 150.30316338800506
finished 86 epochs...
Completing Train Step...
At time: 1052.6029381752014 and batch: 50, loss is 4.6840175533294675 and perplexity is 108.20391549527316
At time: 1053.219568014145 and batch: 100, loss is 4.603065643310547 and perplexity is 99.78976703196751
At time: 1053.8381395339966 and batch: 150, loss is 4.622218017578125 and perplexity is 101.71939751668648
At time: 1054.4559178352356 and batch: 200, loss is 4.522819156646729 and perplexity is 92.09486219611323
At time: 1055.0742573738098 and batch: 250, loss is 4.657417621612549 and perplexity is 105.36364174468042
At time: 1055.6932723522186 and batch: 300, loss is 4.633236379623413 and perplexity is 102.84637599233834
At time: 1056.3266820907593 and batch: 350, loss is 4.624449796676636 and perplexity is 101.94666625445433
At time: 1056.9441215991974 and batch: 400, loss is 4.55626503944397 and perplexity is 95.22714516528859
At time: 1057.5631656646729 and batch: 450, loss is 4.56229793548584 and perplexity is 95.80337705828576
At time: 1058.1794738769531 and batch: 500, loss is 4.508872365951538 and perplexity is 90.81934975671474
At time: 1058.7995307445526 and batch: 550, loss is 4.593048458099365 and perplexity is 98.79514442985814
At time: 1059.4203300476074 and batch: 600, loss is 4.586489305496216 and perplexity is 98.14925256838912
At time: 1060.042461156845 and batch: 650, loss is 4.487149572372436 and perplexity is 88.86777335645661
At time: 1060.6629583835602 and batch: 700, loss is 4.529478883743286 and perplexity is 92.7102356806522
At time: 1061.2791781425476 and batch: 750, loss is 4.56697832107544 and perplexity is 96.25282477755177
At time: 1061.8961672782898 and batch: 800, loss is 4.5439134693145755 and perplexity is 94.05817457626242
At time: 1062.5139138698578 and batch: 850, loss is 4.5942638397216795 and perplexity is 98.91529123008898
At time: 1063.1326854228973 and batch: 900, loss is 4.581348476409912 and perplexity is 97.645978766667
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012651835402397 and perplexity of 150.30278638394108
finished 87 epochs...
Completing Train Step...
At time: 1064.755247592926 and batch: 50, loss is 4.684012622833252 and perplexity is 108.20338199759249
At time: 1065.3723170757294 and batch: 100, loss is 4.6030505275726314 and perplexity is 99.78825864740261
At time: 1065.9902484416962 and batch: 150, loss is 4.622202606201172 and perplexity is 101.71782989278755
At time: 1066.6077842712402 and batch: 200, loss is 4.522803707122803 and perplexity is 92.09343938532717
At time: 1067.2264029979706 and batch: 250, loss is 4.657385988235474 and perplexity is 105.36030878958769
At time: 1067.8450016975403 and batch: 300, loss is 4.633204936981201 and perplexity is 102.84314228137373
At time: 1068.465262413025 and batch: 350, loss is 4.624432191848755 and perplexity is 101.94487151673997
At time: 1069.083485364914 and batch: 400, loss is 4.556248826980591 and perplexity is 95.2256013111998
At time: 1069.7028431892395 and batch: 450, loss is 4.562282485961914 and perplexity is 95.8018969531532
At time: 1070.320163011551 and batch: 500, loss is 4.508853578567505 and perplexity is 90.81764351474125
At time: 1070.9393746852875 and batch: 550, loss is 4.593050575256347 and perplexity is 98.79535359490939
At time: 1071.5580813884735 and batch: 600, loss is 4.586496524810791 and perplexity is 98.14996114127636
At time: 1072.1911871433258 and batch: 650, loss is 4.487154512405396 and perplexity is 88.86821236727039
At time: 1072.8098142147064 and batch: 700, loss is 4.529487476348877 and perplexity is 92.71103230656423
At time: 1073.424275636673 and batch: 750, loss is 4.566980228424073 and perplexity is 96.2530083654206
At time: 1074.0404226779938 and batch: 800, loss is 4.543908615112304 and perplexity is 94.05771799996592
At time: 1074.6593644618988 and batch: 850, loss is 4.594252605438232 and perplexity is 98.91417999391199
At time: 1075.2783269882202 and batch: 900, loss is 4.581339769363403 and perplexity is 97.64512856228988
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012649327108305 and perplexity of 150.30240938082272
finished 88 epochs...
Completing Train Step...
At time: 1076.8528509140015 and batch: 50, loss is 4.684008274078369 and perplexity is 108.20291144862989
At time: 1077.4825990200043 and batch: 100, loss is 4.603036155700684 and perplexity is 99.78682451363306
At time: 1078.1056582927704 and batch: 150, loss is 4.622187986373901 and perplexity is 101.71634280655468
At time: 1078.7339313030243 and batch: 200, loss is 4.522789068222046 and perplexity is 92.09209124847533
At time: 1079.3599848747253 and batch: 250, loss is 4.6573543453216555 and perplexity is 105.35697493516354
At time: 1079.9819328784943 and batch: 300, loss is 4.63317385673523 and perplexity is 102.83994594088692
At time: 1080.603420972824 and batch: 350, loss is 4.62441466331482 and perplexity is 101.9430845882612
At time: 1081.2220513820648 and batch: 400, loss is 4.55623230934143 and perplexity is 95.22402842206874
At time: 1081.8424055576324 and batch: 450, loss is 4.562267379760742 and perplexity is 95.800449761356
At time: 1082.4611983299255 and batch: 500, loss is 4.508835039138794 and perplexity is 90.81595982312096
At time: 1083.0753238201141 and batch: 550, loss is 4.593052577972412 and perplexity is 98.7955514541493
At time: 1083.6935858726501 and batch: 600, loss is 4.58650354385376 and perplexity is 98.15065006248881
At time: 1084.3124408721924 and batch: 650, loss is 4.487159690856934 and perplexity is 88.86867256819295
At time: 1084.9290635585785 and batch: 700, loss is 4.529495668411255 and perplexity is 92.71179180423492
At time: 1085.5450909137726 and batch: 750, loss is 4.566981611251831 and perplexity is 96.2531414668444
At time: 1086.1606822013855 and batch: 800, loss is 4.543903465270996 and perplexity is 94.05723361889166
At time: 1086.779542684555 and batch: 850, loss is 4.594241418838501 and perplexity is 98.91307348676172
At time: 1087.3993957042694 and batch: 900, loss is 4.581331396102906 and perplexity is 97.64431095761515
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012647654912243 and perplexity of 150.30215804593587
finished 89 epochs...
Completing Train Step...
At time: 1088.9906766414642 and batch: 50, loss is 4.684003849029541 and perplexity is 108.20243264652268
At time: 1089.621494293213 and batch: 100, loss is 4.603021850585938 and perplexity is 99.78539706186821
At time: 1090.243272304535 and batch: 150, loss is 4.622173948287964 and perplexity is 101.7149149138156
At time: 1090.8643226623535 and batch: 200, loss is 4.522774648666382 and perplexity is 92.09076333101336
At time: 1091.483346939087 and batch: 250, loss is 4.657322816848755 and perplexity is 105.35365324299859
At time: 1092.1017327308655 and batch: 300, loss is 4.633143396377563 and perplexity is 102.83681344705984
At time: 1092.722175836563 and batch: 350, loss is 4.624397506713867 and perplexity is 101.94133560644241
At time: 1093.3416702747345 and batch: 400, loss is 4.556215400695801 and perplexity is 95.22241832632905
At time: 1093.9600982666016 and batch: 450, loss is 4.562252225875855 and perplexity is 95.79899802336793
At time: 1094.5759932994843 and batch: 500, loss is 4.50881649017334 and perplexity is 90.81427529664266
At time: 1095.1926691532135 and batch: 550, loss is 4.593054323196411 and perplexity is 98.79572387466716
At time: 1095.811712026596 and batch: 600, loss is 4.586510171890259 and perplexity is 98.15130061073577
At time: 1096.430514574051 and batch: 650, loss is 4.487164602279663 and perplexity is 88.86910904088323
At time: 1097.0488593578339 and batch: 700, loss is 4.529503259658814 and perplexity is 92.71249560506946
At time: 1097.6650025844574 and batch: 750, loss is 4.566982526779174 and perplexity is 96.25322958926766
At time: 1098.281044960022 and batch: 800, loss is 4.543898277282715 and perplexity is 94.05674565233163
At time: 1098.8998472690582 and batch: 850, loss is 4.59423059463501 and perplexity is 98.91200283732087
At time: 1099.5171792507172 and batch: 900, loss is 4.581323156356811 and perplexity is 97.64350639659999
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012647236863228 and perplexity of 150.30209521227988
finished 90 epochs...
Completing Train Step...
At time: 1101.09117603302 and batch: 50, loss is 4.683999786376953 and perplexity is 108.20199305852265
At time: 1101.7227692604065 and batch: 100, loss is 4.603007793426514 and perplexity is 99.78399437249246
At time: 1102.3415913581848 and batch: 150, loss is 4.622160358428955 and perplexity is 101.71353263185536
At time: 1102.960780620575 and batch: 200, loss is 4.522760457992554 and perplexity is 92.0894565103007
At time: 1103.579933166504 and batch: 250, loss is 4.657291440963745 and perplexity is 105.3503477307461
At time: 1104.2094140052795 and batch: 300, loss is 4.633113107681274 and perplexity is 102.83369870122107
At time: 1104.828703403473 and batch: 350, loss is 4.624380435943603 and perplexity is 101.9395954041752
At time: 1105.446788072586 and batch: 400, loss is 4.556198444366455 and perplexity is 95.22080371733175
At time: 1106.0623540878296 and batch: 450, loss is 4.562237215042114 and perplexity is 95.79756001132901
At time: 1106.6768505573273 and batch: 500, loss is 4.508798112869263 and perplexity is 90.81260639042607
At time: 1107.2959661483765 and batch: 550, loss is 4.59305585861206 and perplexity is 98.7958755672841
At time: 1107.914981842041 and batch: 600, loss is 4.586516485214234 and perplexity is 98.15192027365113
At time: 1108.534353017807 and batch: 650, loss is 4.487169542312622 and perplexity is 88.86954805829528
At time: 1109.1534123420715 and batch: 700, loss is 4.529510478973389 and perplexity is 92.71316492815635
At time: 1109.7696542739868 and batch: 750, loss is 4.566983013153076 and perplexity is 96.25327640433791
At time: 1110.3860630989075 and batch: 800, loss is 4.543892965316773 and perplexity is 94.05624602742913
At time: 1111.0050556659698 and batch: 850, loss is 4.594220056533813 and perplexity is 98.91096049811752
At time: 1111.6239256858826 and batch: 900, loss is 4.581315393447876 and perplexity is 97.64274840189384
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012646818814212 and perplexity of 150.30203237865
finished 91 epochs...
Completing Train Step...
At time: 1113.1960031986237 and batch: 50, loss is 4.683995761871338 and perplexity is 108.20155759987028
At time: 1113.8265779018402 and batch: 100, loss is 4.602994155883789 and perplexity is 99.78263357328498
At time: 1114.444322347641 and batch: 150, loss is 4.6221471118927 and perplexity is 101.7121852887816
At time: 1115.0623106956482 and batch: 200, loss is 4.5227463722229 and perplexity is 92.0881593685644
At time: 1115.680408000946 and batch: 250, loss is 4.65726001739502 and perplexity is 105.34703729886695
At time: 1116.2982349395752 and batch: 300, loss is 4.633083238601684 and perplexity is 102.83062719916171
At time: 1116.917239665985 and batch: 350, loss is 4.624363441467285 and perplexity is 101.93786300885579
At time: 1117.5343227386475 and batch: 400, loss is 4.556181488037109 and perplexity is 95.21918913571206
At time: 1118.152631521225 and batch: 450, loss is 4.562222423553467 and perplexity is 95.7961430332873
At time: 1118.7672736644745 and batch: 500, loss is 4.508779802322388 and perplexity is 90.81094357716346
At time: 1119.3851099014282 and batch: 550, loss is 4.593057241439819 and perplexity is 98.79601218505776
At time: 1120.016230583191 and batch: 600, loss is 4.586522455215454 and perplexity is 98.15250624248407
At time: 1120.6344470977783 and batch: 650, loss is 4.487174568176269 and perplexity is 88.86999470564858
At time: 1121.2530736923218 and batch: 700, loss is 4.529517164230347 and perplexity is 92.71378474155907
At time: 1121.868807554245 and batch: 750, loss is 4.566983098983765 and perplexity is 96.25328466582323
At time: 1122.484251499176 and batch: 800, loss is 4.543887748718261 and perplexity is 94.05575537503589
At time: 1123.1024346351624 and batch: 850, loss is 4.594209613800049 and perplexity is 98.90992760268384
At time: 1123.7197024822235 and batch: 900, loss is 4.58130744934082 and perplexity is 97.64197272052837
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012645982716181 and perplexity of 150.30190671146926
finished 92 epochs...
Completing Train Step...
At time: 1125.303817987442 and batch: 50, loss is 4.683991889953614 and perplexity is 108.20113865315268
At time: 1125.9229135513306 and batch: 100, loss is 4.602980642318726 and perplexity is 99.78128516328493
At time: 1126.54185795784 and batch: 150, loss is 4.622134275436402 and perplexity is 101.71087967313984
At time: 1127.1602716445923 and batch: 200, loss is 4.522732725143433 and perplexity is 92.08690264271084
At time: 1127.7789494991302 and batch: 250, loss is 4.657228956222534 and perplexity is 105.34376514718926
At time: 1128.3947718143463 and batch: 300, loss is 4.633053455352783 and perplexity is 102.82756461460428
At time: 1129.0124871730804 and batch: 350, loss is 4.624346694946289 and perplexity is 101.9361559185866
At time: 1129.6301546096802 and batch: 400, loss is 4.5561643409729005 and perplexity is 95.21755642016025
At time: 1130.247737646103 and batch: 450, loss is 4.562207717895507 and perplexity is 95.7947342983322
At time: 1130.8642418384552 and batch: 500, loss is 4.508761539459228 and perplexity is 90.80928512447161
At time: 1131.4822008609772 and batch: 550, loss is 4.593058471679687 and perplexity is 98.79613372792554
At time: 1132.1007542610168 and batch: 600, loss is 4.5865281963348385 and perplexity is 98.15306974935785
At time: 1132.7191302776337 and batch: 650, loss is 4.48717960357666 and perplexity is 88.87044220278132
At time: 1133.337599515915 and batch: 700, loss is 4.529523801803589 and perplexity is 92.71440013813822
At time: 1133.952919960022 and batch: 750, loss is 4.5669827556610105 and perplexity is 96.25325161988611
At time: 1134.570946931839 and batch: 800, loss is 4.543882389068603 and perplexity is 94.05525127048963
At time: 1135.1901636123657 and batch: 850, loss is 4.594199476242065 and perplexity is 98.90892490264002
At time: 1135.8230550289154 and batch: 900, loss is 4.581299934387207 and perplexity is 97.64123894838981
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.01264431052012 and perplexity of 150.30165537742295
finished 93 epochs...
Completing Train Step...
At time: 1137.4072239398956 and batch: 50, loss is 4.683987884521485 and perplexity is 108.20070526170348
At time: 1138.025821685791 and batch: 100, loss is 4.602967166900635 and perplexity is 99.77994057780917
At time: 1138.6563827991486 and batch: 150, loss is 4.622121553421021 and perplexity is 101.70958571399517
At time: 1139.2811379432678 and batch: 200, loss is 4.522719249725342 and perplexity is 92.0856617415579
At time: 1139.912040233612 and batch: 250, loss is 4.657197704315186 and perplexity is 105.3404730050443
At time: 1140.536462545395 and batch: 300, loss is 4.633024110794067 and perplexity is 102.82454722936893
At time: 1141.1559612751007 and batch: 350, loss is 4.624329900741577 and perplexity is 101.93444399629176
At time: 1141.773945093155 and batch: 400, loss is 4.556147289276123 and perplexity is 95.21593281310297
At time: 1142.3931074142456 and batch: 450, loss is 4.56219298362732 and perplexity is 95.79332284342448
At time: 1143.010065317154 and batch: 500, loss is 4.5087434101104735 and perplexity is 90.80763882619463
At time: 1143.6310648918152 and batch: 550, loss is 4.593059568405152 and perplexity is 98.79624208022058
At time: 1144.2501714229584 and batch: 600, loss is 4.586533555984497 and perplexity is 98.15359581683437
At time: 1144.8713788986206 and batch: 650, loss is 4.487184381484985 and perplexity is 88.87086681862135
At time: 1145.4922904968262 and batch: 700, loss is 4.529530086517334 and perplexity is 92.71498282343411
At time: 1146.1105949878693 and batch: 750, loss is 4.566982183456421 and perplexity is 96.25319654334953
At time: 1146.7279782295227 and batch: 800, loss is 4.54387713432312 and perplexity is 94.0547570353814
At time: 1147.3489468097687 and batch: 850, loss is 4.594189414978027 and perplexity is 98.90792975883708
At time: 1147.9684104919434 and batch: 900, loss is 4.581292791366577 and perplexity is 97.64054149749661
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012643056373074 and perplexity of 150.30146687716407
finished 94 epochs...
Completing Train Step...
At time: 1149.5592563152313 and batch: 50, loss is 4.683983983993531 and perplexity is 108.20028322265105
At time: 1150.1747858524323 and batch: 100, loss is 4.602953815460205 and perplexity is 99.77860838076987
At time: 1150.793547153473 and batch: 150, loss is 4.622109088897705 and perplexity is 101.70831796039359
At time: 1151.4116597175598 and batch: 200, loss is 4.522705783843994 and perplexity is 92.08442173531196
At time: 1152.0505952835083 and batch: 250, loss is 4.657166795730591 and perplexity is 105.33721713044065
At time: 1152.6730751991272 and batch: 300, loss is 4.63299485206604 and perplexity is 102.82153875791928
At time: 1153.2924530506134 and batch: 350, loss is 4.624313249588012 and perplexity is 101.9327466843424
At time: 1153.9114346504211 and batch: 400, loss is 4.556130390167237 and perplexity is 95.21432376228243
At time: 1154.529685497284 and batch: 450, loss is 4.56217845916748 and perplexity is 95.79193150725818
At time: 1155.1457796096802 and batch: 500, loss is 4.5087251949310305 and perplexity is 90.80598476382319
At time: 1155.7631692886353 and batch: 550, loss is 4.593060531616211 and perplexity is 98.79633724189941
At time: 1156.3813667297363 and batch: 600, loss is 4.5865387439727785 and perplexity is 98.15410503786019
At time: 1157.00203871727 and batch: 650, loss is 4.487189102172851 and perplexity is 88.87128635123426
At time: 1157.6223471164703 and batch: 700, loss is 4.529536170959473 and perplexity is 92.71554694409866
At time: 1158.2399311065674 and batch: 750, loss is 4.566981239318848 and perplexity is 96.25310566713301
At time: 1158.856693983078 and batch: 800, loss is 4.543871774673462 and perplexity is 94.05425293618589
At time: 1159.4764099121094 and batch: 850, loss is 4.594179620742798 and perplexity is 98.90696103605096
At time: 1160.0961229801178 and batch: 900, loss is 4.5812857055664065 and perplexity is 97.6398496385822
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012642220275043 and perplexity of 150.30134121045614
finished 95 epochs...
Completing Train Step...
At time: 1161.667682170868 and batch: 50, loss is 4.68398006439209 and perplexity is 108.1998591214962
At time: 1162.2965812683105 and batch: 100, loss is 4.60294056892395 and perplexity is 99.7772866685705
At time: 1162.9143533706665 and batch: 150, loss is 4.622096843719483 and perplexity is 101.70707253153871
At time: 1163.532948732376 and batch: 200, loss is 4.522692594528198 and perplexity is 92.08320721280319
At time: 1164.1519718170166 and batch: 250, loss is 4.657135763168335 and perplexity is 105.33394829741259
At time: 1164.7693917751312 and batch: 300, loss is 4.632965784072876 and perplexity is 102.81854998557262
At time: 1165.388569355011 and batch: 350, loss is 4.624296712875366 and perplexity is 101.93106106573858
At time: 1166.0066542625427 and batch: 400, loss is 4.556113233566284 and perplexity is 95.21269022213774
At time: 1166.6257514953613 and batch: 450, loss is 4.562163810729981 and perplexity is 95.79052831541387
At time: 1167.2418749332428 and batch: 500, loss is 4.508707084655762 and perplexity is 90.80434025733435
At time: 1167.8740541934967 and batch: 550, loss is 4.59306134223938 and perplexity is 98.7964173285318
At time: 1168.4937105178833 and batch: 600, loss is 4.586543788909912 and perplexity is 98.1546002203986
At time: 1169.114175081253 and batch: 650, loss is 4.487193546295166 and perplexity is 88.87168130697869
At time: 1169.7351248264313 and batch: 700, loss is 4.529542036056519 and perplexity is 92.71609073137387
At time: 1170.353534936905 and batch: 750, loss is 4.566979913711548 and perplexity is 96.25297807339811
At time: 1170.9702425003052 and batch: 800, loss is 4.543866491317749 and perplexity is 94.05375601542401
At time: 1171.5900161266327 and batch: 850, loss is 4.594170055389404 and perplexity is 98.90601496054033
At time: 1172.2078652381897 and batch: 900, loss is 4.581278896331787 and perplexity is 97.63918478820135
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012642638324058 and perplexity of 150.3014040437969
Annealing...
finished 96 epochs...
Completing Train Step...
At time: 1173.7790749073029 and batch: 50, loss is 4.683987054824829 and perplexity is 108.20061548797746
At time: 1174.4078443050385 and batch: 100, loss is 4.602917947769165 and perplexity is 99.77502961665341
At time: 1175.0289378166199 and batch: 150, loss is 4.622039785385132 and perplexity is 101.70126946094673
At time: 1175.6477053165436 and batch: 200, loss is 4.522634649276734 and perplexity is 92.07787158279427
At time: 1176.265139579773 and batch: 250, loss is 4.657037162780762 and perplexity is 105.32356284129929
At time: 1176.8824000358582 and batch: 300, loss is 4.632870121002197 and perplexity is 102.80871451781124
At time: 1177.5009429454803 and batch: 350, loss is 4.62416675567627 and perplexity is 101.91781525125468
At time: 1178.1187992095947 and batch: 400, loss is 4.5560047054290775 and perplexity is 95.2023575269337
At time: 1178.7362592220306 and batch: 450, loss is 4.562072687149048 and perplexity is 95.78179993714085
At time: 1179.3520777225494 and batch: 500, loss is 4.5085555267333985 and perplexity is 90.79057918300964
At time: 1179.9700846672058 and batch: 550, loss is 4.592918767929077 and perplexity is 98.78233250156191
At time: 1180.5875244140625 and batch: 600, loss is 4.586451368331909 and perplexity is 98.14552913469653
At time: 1181.2059862613678 and batch: 650, loss is 4.487035627365112 and perplexity is 88.85764789425454
At time: 1181.8249309062958 and batch: 700, loss is 4.529333791732788 and perplexity is 92.69678514197003
At time: 1182.442699432373 and batch: 750, loss is 4.566840353012085 and perplexity is 96.23954587777793
At time: 1183.0573997497559 and batch: 800, loss is 4.54373685836792 and perplexity is 94.04156433982777
At time: 1183.6902389526367 and batch: 850, loss is 4.594022779464722 and perplexity is 98.89144955832323
At time: 1184.3088369369507 and batch: 900, loss is 4.581154518127441 and perplexity is 97.62704135692836
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012647654912243 and perplexity of 150.30215804593587
Annealing...
finished 97 epochs...
Completing Train Step...
At time: 1185.879739522934 and batch: 50, loss is 4.68397738456726 and perplexity is 108.19956916521569
At time: 1186.509531736374 and batch: 100, loss is 4.602903928756714 and perplexity is 99.77363087907537
At time: 1187.1276395320892 and batch: 150, loss is 4.622025756835938 and perplexity is 101.69984274969234
At time: 1187.7437210083008 and batch: 200, loss is 4.52261947631836 and perplexity is 92.07647449968053
At time: 1188.3609523773193 and batch: 250, loss is 4.657016067504883 and perplexity is 105.32134103511947
At time: 1188.976506471634 and batch: 300, loss is 4.632844505310058 and perplexity is 102.80608103516033
At time: 1189.593421459198 and batch: 350, loss is 4.624144172668457 and perplexity is 101.91551366642501
At time: 1190.2096590995789 and batch: 400, loss is 4.555977420806885 and perplexity is 95.19976000201312
At time: 1190.8258407115936 and batch: 450, loss is 4.562052450180054 and perplexity is 95.77986162343821
At time: 1191.440839290619 and batch: 500, loss is 4.508522281646728 and perplexity is 90.7875608925078
At time: 1192.0575857162476 and batch: 550, loss is 4.592887439727783 and perplexity is 98.77923787723981
At time: 1192.6790091991425 and batch: 600, loss is 4.586428680419922 and perplexity is 98.14330244282917
At time: 1193.3007588386536 and batch: 650, loss is 4.486996517181397 and perplexity is 88.8541727232786
At time: 1193.921061038971 and batch: 700, loss is 4.529284725189209 and perplexity is 92.69223694270532
At time: 1194.5384018421173 and batch: 750, loss is 4.56680811882019 and perplexity is 96.23644372378617
At time: 1195.1541147232056 and batch: 800, loss is 4.543707046508789 and perplexity is 94.0387608277484
At time: 1195.773848772049 and batch: 850, loss is 4.5939884281158445 and perplexity is 98.8880525619845
At time: 1196.392060995102 and batch: 900, loss is 4.581126594543457 and perplexity is 97.62431529810078
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012648491010274 and perplexity of 150.30228371332677
Annealing...
finished 98 epochs...
Completing Train Step...
At time: 1197.9648962020874 and batch: 50, loss is 4.6839754772186275 and perplexity is 108.19936279111218
At time: 1198.5919544696808 and batch: 100, loss is 4.602900724411011 and perplexity is 99.77331117038226
At time: 1199.207394361496 and batch: 150, loss is 4.622022848129273 and perplexity is 101.69954693511214
At time: 1199.8363134860992 and batch: 200, loss is 4.52261643409729 and perplexity is 92.07619438311588
At time: 1200.451636314392 and batch: 250, loss is 4.657011861801148 and perplexity is 105.32089808569351
At time: 1201.0663967132568 and batch: 300, loss is 4.632839012145996 and perplexity is 102.80551630604168
At time: 1201.6822836399078 and batch: 350, loss is 4.624140157699585 and perplexity is 101.91510447963151
At time: 1202.2984189987183 and batch: 400, loss is 4.555972089767456 and perplexity is 95.19925248969174
At time: 1202.916656255722 and batch: 450, loss is 4.5620487022399905 and perplexity is 95.7795026469303
At time: 1203.533272743225 and batch: 500, loss is 4.508515663146973 and perplexity is 90.7869600170467
At time: 1204.1507012844086 and batch: 550, loss is 4.5928810024261475 and perplexity is 98.77860200753689
At time: 1204.7687418460846 and batch: 600, loss is 4.58642430305481 and perplexity is 98.14287283470134
At time: 1205.3857562541962 and batch: 650, loss is 4.486988105773926 and perplexity is 88.85342533776961
At time: 1206.013550043106 and batch: 700, loss is 4.529274110794067 and perplexity is 92.69125307589745
At time: 1206.637209892273 and batch: 750, loss is 4.566801738739014 and perplexity is 96.23582972942177
At time: 1207.2550489902496 and batch: 800, loss is 4.543700952529907 and perplexity is 94.03818775927192
At time: 1207.875189781189 and batch: 850, loss is 4.5939812660217285 and perplexity is 98.88734431898136
At time: 1208.4966740608215 and batch: 900, loss is 4.581120767593384 and perplexity is 97.62374644774691
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012648491010274 and perplexity of 150.30228371332677
Annealing...
finished 99 epochs...
Completing Train Step...
At time: 1210.1414086818695 and batch: 50, loss is 4.6839749622344975 and perplexity is 108.19930707017181
At time: 1210.7621109485626 and batch: 100, loss is 4.602900123596191 and perplexity is 99.77325122511627
At time: 1211.3825697898865 and batch: 150, loss is 4.62202226638794 and perplexity is 101.6994877722993
At time: 1212.003681898117 and batch: 200, loss is 4.522615833282471 and perplexity is 92.07613906239041
At time: 1212.6257328987122 and batch: 250, loss is 4.657011041641235 and perplexity is 105.32081170575039
At time: 1213.2473104000092 and batch: 300, loss is 4.632838268280029 and perplexity is 102.80543983254532
At time: 1213.86913895607 and batch: 350, loss is 4.624139366149902 and perplexity is 101.91502380879481
At time: 1214.4902894496918 and batch: 400, loss is 4.555971260070801 and perplexity is 95.19917350322314
At time: 1215.1115069389343 and batch: 450, loss is 4.562048177719117 and perplexity is 95.77945240859503
At time: 1215.750675201416 and batch: 500, loss is 4.508514757156372 and perplexity is 90.7868777649515
At time: 1216.3711371421814 and batch: 550, loss is 4.592880077362061 and perplexity is 98.77851063104191
At time: 1216.9923899173737 and batch: 600, loss is 4.586423559188843 and perplexity is 98.14279982958547
At time: 1217.613697052002 and batch: 650, loss is 4.486986684799194 and perplexity is 88.85329907938709
At time: 1218.2408061027527 and batch: 700, loss is 4.529272480010986 and perplexity is 92.69110191669343
At time: 1218.8603661060333 and batch: 750, loss is 4.566800680160522 and perplexity is 96.23572785629622
At time: 1219.478770017624 and batch: 800, loss is 4.543700189590454 and perplexity is 94.03811601385577
At time: 1220.100445508957 and batch: 850, loss is 4.593980302810669 and perplexity is 98.88724906964356
At time: 1220.7237794399261 and batch: 900, loss is 4.581119985580444 and perplexity is 97.62367010474382
Finished Train Step
Begin Evaluating...
Getting Batches...
Created Iterator with 74 batches
Done Evaluating: Achieved loss of 5.012648491010274 and perplexity of 150.30228371332677
Annealing...
Model not improving. Stopping early with 150.30134121045614 lossat 99 epochs.
Finished Training.
langmodel



RESULTS:
[{'params': {'rnn_dropout': 0.20498933070666836, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.206438412426412, 'wordvec_source': 'None', 'dropout': 0.0105454069243337, 'lr': 33.092705634289025, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.08760933524890213, 'tune_wordvecs': True}, 'best_accuracy': -85.78750454786196}, {'params': {'rnn_dropout': 0.4153279496866563, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.8252968048649985, 'wordvec_source': 'None', 'dropout': 0.7488427949660501, 'lr': 18.815293339150582, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.045940316742695886, 'tune_wordvecs': True}, 'best_accuracy': -85.83446256180737}, {'params': {'rnn_dropout': 0.6867840398358628, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.05034803884462, 'wordvec_source': 'None', 'dropout': 0.20082065557742956, 'lr': 4.125119577239014, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.40782479654432197, 'tune_wordvecs': True}, 'best_accuracy': -88.05629640131215}, {'params': {'rnn_dropout': 0.8814291672926791, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.561040963230379, 'wordvec_source': 'None', 'dropout': 0.766026158306164, 'lr': 35.32622074923119, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.07615678351470623, 'tune_wordvecs': True}, 'best_accuracy': -86.05474348768759}, {'params': {'rnn_dropout': 0.3967156699045209, 'num_layers': 1, 'wordvec_dim': 300, 'anneal': 4.311110015148611, 'wordvec_source': 'None', 'dropout': 0.29446936213168917, 'lr': 19.677092011683996, 'seq_len': 35, 'hidden_size': 300, 'batch_size': 32, 'clip': 0.4376707521124406, 'tune_wordvecs': True}, 'best_accuracy': -150.30134121045614}]
Traceback (most recent call last):
  File "bayesian_optimization.py", line 226, in <module>
    fix_pretrained = args.fix_pretrained, savepath = args.savepath)
  File "bayesian_optimization.py", line 79, in __init__
    self.save()
  File "bayesian_optimization.py", line 89, in save
    self.best_model.save_checkpoint(savepath, name = name)
TypeError: save_checkpoint() got multiple values for argument 'name'
